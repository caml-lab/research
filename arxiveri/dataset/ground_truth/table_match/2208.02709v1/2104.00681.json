{
    "target_title": "Globally Consistent Video Depth and Pose Estimation with Efficient Test-Time Training",
    "target_table": "<table><tbody><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">knownpose?</td><td colspan=\"4\">Depth Metrics</td><td colspan=\"3\">Pose Metrics</td></tr><tr><td rowspan=\"2\"><p>AbsRel\\downarrow</p></td><td rowspan=\"2\"><p>SqRel\\downarrow</p></td><td rowspan=\"2\"><p>RMSE\\downarrow</p></td><td><p>\\delta&lt;</p></td><td><p>ATE</p></td><td><p>RPE Trans</p></td><td><p>RPE Rot</p></td></tr><tr><td><p>1.25\\uparrow</p></td><td><p>(m)\\downarrow</p></td><td><p>(m) \\downarrow</p></td><td><p>(deg) \\downarrow</p></td></tr><tr><td>DPSNet\\dagger (Im et al. 2019)</td><td>\u2713</td><td><p>0.199</p></td><td><p>0.142</p></td><td><p>0.438</p></td><td><p>0.710</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>CNMNet\\dagger (Long et al. 2020)</td><td>\u2713</td><td><p>0.161</p></td><td><p>0.083</p></td><td><p>0.361</p></td><td><p>0.766</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>NeuralRecon\\dagger (Sun et al. 2021)</td><td>\u2713</td><td><p>0.155</p></td><td><p>0.104</p></td><td><p>0.347</p></td><td><p>0.820</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>DeepV2D (Teed and Deng 2019)</td><td></td><td><p>0.162</p></td><td><p>0.092</p></td><td><p>0.380</p></td><td><p>0.767</p></td><td><p>0.471</p></td><td><p>1.018</p></td><td><p>60.979</p></td></tr><tr><td>DROID-SLAM (Teed and Deng 2021)</td><td></td><td><p>0.209</p></td><td><p>0.132</p></td><td><p>0.462</p></td><td><p>0.665</p></td><td><p>0.463</p></td><td><p>0.928</p></td><td><p>40.143</p></td></tr><tr><td>Deep3D (Lee et al. 2021)</td><td></td><td><p>0.172</p></td><td><p>0.105</p></td><td><p>0.406</p></td><td><p>0.748</p></td><td><p>0.310</p></td><td><p>0.306</p></td><td><p>8.665</p></td></tr><tr><td>CVD2 (Kopf, Rong, and Huang 2021)</td><td></td><td><p>0.154</p></td><td><p>0.085</p></td><td><p>0.379</p></td><td><p>0.795</p></td><td><p>0.375</p></td><td><p>0.517</p></td><td><p>31.102</p></td></tr><tr><td>Ours (GCVD)</td><td></td><td>0.124</td><td>0.054</td><td>0.307</td><td>0.858</td><td>0.249</td><td>0.257</td><td>8.155</td></tr></tbody></table>",
    "target_caption": "Table 1: Quantitative evaluations of depth and pose on 7-Scenes dataset (Shotton et al. 2013). The standard depth evaluation measures per-frame errors and accuracy metrics. The pose evaluation computes the per-sequence pose errors.\\daggerWe also refer to the approaches of multi-view stereo and 3D reconstruction with known camera pose and compare with the results reported in NeuralRecon (Sun et al. 2021).",
    "source_title": "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video",
    "source_table": "",
    "source_caption": ""
}