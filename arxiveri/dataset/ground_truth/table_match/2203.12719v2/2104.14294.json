{"target_title": "What to Hide from Your Students: Attention-Guided Masked Image Modeling", "target_table": "<table><thead><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">(a) Full</td><td colspan=\"4\">(b) Few Examples</td></tr><tr><td>k-NN</td><td>Linear</td><td>\\nu=1</td><td>5</td><td>10</td><td>20</td></tr></thead><tbody><tr><td>SimCLR [9]</td><td>-</td><td>69.0</td><td></td><td></td><td></td><td></td></tr><tr><td>BYOL [23]</td><td>66.6</td><td>71.4</td><td></td><td></td><td></td><td></td></tr><tr><td>MoBY [71]</td><td>-</td><td>72.8</td><td></td><td></td><td></td><td></td></tr><tr><td>DINO [8]</td><td>72.8</td><td>76.1</td><td></td><td></td><td></td><td></td></tr><tr><td>MST [38]</td><td>75.0</td><td>76.9</td><td></td><td></td><td></td><td></td></tr><tr><td>iBOT [78]</td><td>74.6</td><td>77.4</td><td>38.9</td><td>54.1</td><td>58.5</td><td>61.9</td></tr><tr><td>iBOT+AttMask (Ours)</td><td>75.0</td><td>77.5</td><td>40.4</td><td>55.5</td><td>59.9</td><td>63.1</td></tr></tbody></table>", "target_caption": "Table A13: Top-1 accuracy on ImageNet validation set. (a) k-NN and linear probing using the full ImageNet training set; (b) k-NN using only \\nu\\in\\{1,5,10,20\\} examples per class. Pre-training on 100% ImageNet-1k for 300 epochs.", "source_title": "Emerging properties in self-supervised vision transformers", "source_table": "<table><tbody><tr><td>Method</td><td>Arch.</td><td>Param.</td><td>im/s</td><td>Linear</td><td>k-NN</td></tr><tr><td>Supervised</td><td>RN50</td><td>23</td><td>1237</td><td>79.3</td><td>79.3</td></tr><tr><td>SCLR [12]</td><td>RN50</td><td>23</td><td>1237</td><td>69.1</td><td>60.7</td></tr><tr><td>MoCov2 [15]</td><td>RN50</td><td>23</td><td>1237</td><td>71.1</td><td>61.9</td></tr><tr><td>InfoMin [67]</td><td>RN50</td><td>23</td><td>1237</td><td>73.0</td><td>65.3</td></tr><tr><td>BarlowT [81]</td><td>RN50</td><td>23</td><td>1237</td><td>73.2</td><td>66.0</td></tr><tr><td>OBoW [27]</td><td>RN50</td><td>23</td><td>1237</td><td>73.8</td><td>61.9</td></tr><tr><td>BYOL [30]</td><td>RN50</td><td>23</td><td>1237</td><td>74.4</td><td>64.8</td></tr><tr><td>DCv2 [10]</td><td>RN50</td><td>23</td><td>1237</td><td>75.2</td><td>67.1</td></tr><tr><td>SwAV [10]</td><td>RN50</td><td>23</td><td>1237</td><td>75.3</td><td>65.7</td></tr><tr><td>DINO</td><td>RN50</td><td>23</td><td>1237</td><td>75.3</td><td>67.5</td></tr><tr><td>Supervised</td><td>ViT-S</td><td>21</td><td>1007</td><td>79.8</td><td>79.8</td></tr><tr><td>BYOL{}^{*} [30]</td><td>ViT-S</td><td>21</td><td>1007</td><td>71.4</td><td>66.6</td></tr><tr><td>MoCov2{}^{*} [15]</td><td>ViT-S</td><td>21</td><td>1007</td><td>72.7</td><td>64.4</td></tr><tr><td>SwAV{}^{*} [10]</td><td>ViT-S</td><td>21</td><td>1007</td><td>73.5</td><td>66.3</td></tr><tr><td>DINO</td><td>ViT-S</td><td>21</td><td>1007</td><td>77.0</td><td>74.5</td></tr><tr><td colspan=\"5\">Comparison across architectures</td><td></td></tr><tr><td>SCLR [12]</td><td>RN50w4</td><td>375</td><td>117</td><td>76.8</td><td>69.3</td></tr><tr><td>SwAV [10]</td><td>RN50w2</td><td>93</td><td>384</td><td>77.3</td><td>67.3</td></tr><tr><td>BYOL [30]</td><td>RN50w2</td><td>93</td><td>384</td><td>77.4</td><td>\u2013</td></tr><tr><td>DINO</td><td>ViT-B/16</td><td>85</td><td>312</td><td>78.2</td><td>76.1</td></tr><tr><td>SwAV [10]</td><td>RN50w5</td><td>586</td><td>76</td><td>78.5</td><td>67.1</td></tr><tr><td>BYOL [30]</td><td>RN50w4</td><td>375</td><td>117</td><td>78.6</td><td>\u2013</td></tr><tr><td>BYOL [30]</td><td>RN200w2</td><td>250</td><td>123</td><td>79.6</td><td>73.9</td></tr><tr><td>DINO</td><td>ViT-S/8</td><td>21</td><td>180</td><td>79.7</td><td>78.3</td></tr><tr><td>SCLRv2 [13]</td><td>RN152w3+SK</td><td>794</td><td>46</td><td>79.8</td><td>73.1</td></tr><tr><td>DINO</td><td>ViT-B/8</td><td>85</td><td>63</td><td>80.1</td><td>77.4</td></tr></tbody></table>", "source_caption": "Table 2: Linear and k-NN classification on ImageNet.We report top-1 accuracy for linear and k-NN evaluations on the validation set of ImageNet for different self-supervised methods.We focus on ResNet-50 and ViT-small architectures, but also report the best results obtained across architectures.{}^{*} are run by us.We run the k-NN evaluation for models with official released weights.The throughput (im/s) is calculated on a NVIDIA V100 GPU with 128 samples per forward.Parameters (M) are of the feature extractor."}