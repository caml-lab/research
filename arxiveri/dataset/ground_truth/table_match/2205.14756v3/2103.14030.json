{
    "target_title": "EfficientViT: Lightweight Multi-Scale Attention for On-Device Semantic Segmentation",
    "target_table": "<table><tbody><tr><td colspan=\"2\" rowspan=\"2\">Models</td><td rowspan=\"2\">Params</td><td rowspan=\"2\">MACs</td><td colspan=\"2\">Accuracy</td></tr><tr><td>Top1 (%)</td><td>Top5 (%)</td></tr><tr><td rowspan=\"9\">CNN-based</td><td>MobileNetV2 [22]</td><td>3.4M</td><td>300M</td><td>72.0</td><td>-</td></tr><tr><td>ShuffleNetV2 1.5x [75]</td><td>-</td><td>299M</td><td>72.6</td><td>-</td></tr><tr><td>FBNet-B [76]</td><td>4.5M</td><td>295M</td><td>74.1</td><td>-</td></tr><tr><td>ProxylessNAS-Mobile [77]</td><td>4.1M</td><td>320M</td><td>74.6</td><td>92.2</td></tr><tr><td>MnasNet-A1 [78]</td><td>3.9M</td><td>312M</td><td>75.2</td><td>92.5</td></tr><tr><td>MobileNetV3-Large 1.25x [48]</td><td>7.5M</td><td>356M</td><td>76.6</td><td>-</td></tr><tr><td>EfficientNet-B0 [39]</td><td>5.3M</td><td>390M</td><td>77.1</td><td>93.3</td></tr><tr><td>EfficientNetV2-B0 [79]</td><td>7.4M</td><td>700M</td><td>78.7</td><td>-</td></tr><tr><td>EfficientNet-B1 [39]</td><td>7.8M</td><td>700M</td><td>79.1</td><td>94.4</td></tr><tr><td rowspan=\"13\">ViT-based</td><td>T2T-ViT-7 [80]</td><td>4.3M</td><td>1.2G</td><td>71.7</td><td>-</td></tr><tr><td>QuadTree-B-b0 [34]</td><td>3.5M</td><td>0.7G</td><td>72.0</td><td>-</td></tr><tr><td>ConViT-Tiny [81]</td><td>6.0M</td><td>1.0G</td><td>73.1</td><td>-</td></tr><tr><td>PVT-Tiny [5]</td><td>13.2M</td><td>1.9G</td><td>75.1</td><td>-</td></tr><tr><td>CeiT-T [82]</td><td>6.4M</td><td>1.2G</td><td>76.4</td><td>93.4</td></tr><tr><td>ViL-Tiny-RPB [46]</td><td>6.7M</td><td>1.3G</td><td>76.7</td><td></td></tr><tr><td>Swin-1G [4] {}^{{\\ddagger}}</td><td>7.3M</td><td>1.0G</td><td>77.3</td><td>-</td></tr><tr><td>HVT-S-1 [83]</td><td>22.1M</td><td>2.4G</td><td>78.0</td><td>93.8</td></tr><tr><td>PiT-XS [84]</td><td>10.6M</td><td>1.4G</td><td>78.1</td><td>-</td></tr><tr><td>CoaT Tiny [47]</td><td>5.5M</td><td>4.4G</td><td>78.3</td><td></td></tr><tr><td>HRFormer-T [10]</td><td>8.0M</td><td>1.8G</td><td>78.5</td><td></td></tr><tr><td>MobileViT-XS [41]</td><td>2.3M</td><td>700M</td><td>74.8</td><td>-</td></tr><tr><td>MobileFormer w/o DY-ReLU [40]</td><td>10.1M</td><td>290M</td><td>76.8</td><td>93.2</td></tr><tr><td rowspan=\"3\">ViT-based</td><td>EfficientViT-Base-r192 (ours)</td><td>7.9M</td><td>304M</td><td>77.7</td><td>93.6</td></tr><tr><td>EfficientViT-Base-r224 (ours)</td><td>7.9M</td><td>406M</td><td>78.6</td><td>94.2</td></tr><tr><td>EfficientViT-Base-r224-w1.2 (ours)</td><td>10.9M</td><td>584M</td><td>79.7</td><td>94.8</td></tr></tbody></table>",
    "target_caption": "Table 3: Results on ImageNet classification. \u2018r224\u2019 denotes the input resolution is 224x224. \u2018w1.2\u2019 denotes the width multiplier [22] is 1.2. {}^{{\\ddagger}} denotes the result is from [40]. While EfficientViT is not specifically designed for image classification, it still provides highly competitive performances on ImageNet. With 584M MACs, EfficientViT achieves 79.7% ImageNet top1 accuracy, outperforming EfficientNet-B1 by 0.6% while saving the computational cost by 1.2\\times. It demonstrates the strong capacity of EfficientViT in visual feature learning. ",
    "source_title": "Swin transformer: Hierarchical vision transformer using shifted windows",
    "source_table": "",
    "source_caption": ""
}