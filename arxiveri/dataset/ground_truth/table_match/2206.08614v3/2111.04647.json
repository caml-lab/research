{"target_title": "Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment", "target_table": "<table><tr><td>Model</td><td>SRCC</td><td>LCC</td><td>Accuracy (%)</td></tr><tr><td>Murray et al. Murray et al. (2012)</td><td>\u2013</td><td>\u2013</td><td>66.70</td></tr><tr><td>Lu et al. Lu et al. (2014)</td><td>\u2013</td><td>\u2013</td><td>74.46</td></tr><tr><td>Ma et al. Ma et al. (2017)</td><td>\u2013</td><td>\u2013</td><td>81.70</td></tr><tr><td>Kong et al. Kong et al. (2016)</td><td>0.558</td><td>\u2013</td><td>77.33</td></tr><tr><td>Talebi et al. Talebi and Milanfar (2018)</td><td>0.612</td><td>0.636</td><td>81.51</td></tr><tr><td>Chen et al. Chen et al. (2020)</td><td>0.649</td><td>0.671</td><td>83.20</td></tr><tr><td>Xu et al. Xu et al. (2020)</td><td>0.724</td><td>0.725</td><td>80.90</td></tr><tr><td>Ke et al. Ke et al. (2021)</td><td>0.726</td><td>0.738</td><td>81.15</td></tr><tr><td>Celona et al. Celona et al. (2022)</td><td>0.731</td><td>0.732</td><td>80.75</td></tr><tr><td>Hosu et al. Hosu et al. (2019)</td><td>0.756</td><td>0.757</td><td>81.72</td></tr><tr><td>ViT-L/16 - 21k</td><td>0.793</td><td>0.793</td><td>82.85</td></tr></table>", "target_caption": "Table 7: Comparison of our baseline with state-of-the-art methods on the AVA dataset for image aesthetic assessment. In each column, the best and second-best results are marked in boldface and underlined, respectively. The \u201c\u2013\u201d means that the result is not available.", "source_title": "Composition and style attributes guided image aesthetic assessment", "source_table": "<table><tr><td>Method</td><td>Network architecture</td><td>MTL</td><td>Accuracy (%) \\uparrow</td><td>SROCC \\uparrow</td><td>PLCC \\uparrow</td><td>MAE \\downarrow</td><td>RMSE \\downarrow</td><td>EMD \\downarrow</td></tr><tr><td>Baseline</td><td></td><td></td><td>71.28</td><td>-0.0003</td><td>-0.0021</td><td>0.6230</td><td>0.7550</td><td>0.0743</td></tr><tr><td>RAPID [12]</td><td>AlexNet</td><td></td><td>74.20</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>DMA-Net [29]</td><td>AlexNet</td><td></td><td>75.42</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>MNA-CNN [30]</td><td>VGG16</td><td></td><td>76.10</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Reg-Net [11]</td><td>AlexNet</td><td></td><td>77.33</td><td>0.5581</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>MTCNN [13]</td><td>VGG16</td><td>\u2713</td><td>78.56</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Multimodal DBM Model [28]</td><td>VGG16</td><td></td><td>78.88</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>NIMA [19]</td><td>VGG16</td><td></td><td>80.60</td><td>0.5920</td><td>0.6100</td><td>\u2013</td><td>\u2013</td><td>0.0520</td></tr><tr><td>GPF-CNN [20]</td><td>VGG16</td><td></td><td>80.70</td><td>0.6762</td><td>0.6868</td><td>0.4144</td><td>0.5347</td><td>0.0460</td></tr><tr><td>NIMA [19]</td><td>InceptionNet</td><td></td><td>81.51</td><td>0.6120</td><td>0.6360</td><td>\u2013</td><td>\u2013</td><td>0.0500</td></tr><tr><td>MLSP [16]</td><td>InceptionNet</td><td></td><td>81.68</td><td>0.7524</td><td>0.7545</td><td>0.3831</td><td>0.4943</td><td>\u2013</td></tr><tr><td>GPF-CNN [20]</td><td>InceptionNet</td><td></td><td>81.81</td><td>0.6900</td><td>0.7042</td><td>0.4072</td><td>0.5246</td><td>0.0450</td></tr><tr><td>MULTIGAP [27]</td><td>InceptionNet</td><td></td><td>82.27</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>A-Lamp [14]</td><td>VGG16</td><td></td><td>82.50</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>AFDC+SPP [21]</td><td>ResNet-50</td><td></td><td>83.24</td><td>0.6489</td><td>0.6711</td><td>\u2013</td><td>\u2013</td><td>0.0447</td></tr><tr><td>PI-DCNN [17]</td><td>ResNet-50</td><td>\u2713</td><td>\u2013</td><td>0.6578</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Pan et al.  [18]</td><td>ResNet-50</td><td>\u2713</td><td>\u2013</td><td>0.7041</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>RGNet [4]</td><td>DenseNet-121</td><td></td><td>83.59</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Proposed</td><td>EfficentNet-B4</td><td>\u2713</td><td>80.75</td><td>0.7318</td><td>0.7329</td><td>0.4011</td><td>0.5128</td><td>0.0439</td></tr></table>", "source_caption": "TABLE III: Comparison of the proposed method with state-of-the-art methods on the AVA dataset. In each column, the best and second-best results are marked in boldface and underlined, respectively. The \u201c\u2013\u201d means that the result is not available. The network architecture and whether it uses Multi-Task Learning (MTL) is indicated for each method."}