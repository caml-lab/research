{"target_title": "Deep Non-rigid Structure-from-Motion: A Sequence-to-Sequence Translation Perspective", "target_table": "<table><tbody><tr><td></td><td>Methods</td><td>S07</td><td>S20</td><td>S23</td><td>S33</td><td>S32</td><td>S38</td><td>S39</td><td>S43</td><td>S93</td><td>Mean</td></tr><tr><td rowspan=\"5\">All</td><td>CSF[19]</td><td>1.231</td><td>1.164</td><td>1.238</td><td>1.156</td><td>1.165</td><td>1.188</td><td>1.172</td><td>1.267</td><td>1.117</td><td>1.189</td></tr><tr><td>URN[13]</td><td>1.504</td><td>1.770</td><td>1.329</td><td>1.205</td><td>1.305</td><td>1.303</td><td>1.550</td><td>1.434</td><td>1.601</td><td>1.445</td></tr><tr><td>CNS[12]</td><td>0.310</td><td>0.217</td><td>0.184</td><td>0.177</td><td>0.249</td><td>0.223</td><td>0.312</td><td>0.266</td><td>0.245</td><td>0.243</td></tr><tr><td>C3DPO[28]</td><td>0.226</td><td>0.235</td><td>0.342</td><td>0.357</td><td>0.354</td><td>0.391</td><td>0.189</td><td>0.351</td><td>0.246</td><td>0.299</td></tr><tr><td>Ours</td><td><p>0.072</p></td><td><p>0.122</p></td><td><p>0.137</p></td><td><p>0.158</p></td><td><p>0.142</p></td><td><p>0.093</p></td><td><p>0.090</p></td><td><p>0.108</p></td><td><p>0.129</p></td><td><p>0.117</p></td></tr><tr><td rowspan=\"4\">Unseen</td><td>DNRSFM</td><td>0.097</td><td>0.219</td><td>0.264</td><td>0.219</td><td>0.209</td><td>0.137</td><td>0.127</td><td>0.223</td><td>0.164</td><td>0.184</td></tr><tr><td>PR-RRN</td><td><p>0.061</p></td><td>0.167</td><td>0.249</td><td>0.254</td><td>0.265</td><td>0.108</td><td><p>0.028</p></td><td><p>0.080</p></td><td>0.242</td><td>0.162</td></tr><tr><td>C3DPO</td><td>0.286</td><td>0.361</td><td>0.413</td><td>0.421</td><td>0.401</td><td>0.263</td><td>0.330</td><td>0.491</td><td>0.325</td><td>0.366</td></tr><tr><td><p>Ours</p></td><td>0.081</td><td><p>0.139</p></td><td><p>0.196</p></td><td><p>0.191</p></td><td><p>0.195</p></td><td><p>0.097</p></td><td>0.089</td><td>0.139</td><td><p>0.151</p></td><td><p>0.142</p></td></tr></tbody></table>", "target_caption": "Table 1: Results on the long sequences of the CMU motion capture dataset. We follow the comparison in [41]. Ours result surpasses the state-of-the-art on both All and Unseen dataset which is not available during training. DNRSFM and PR-RRN train a model for each subject separately for testing while we train only one model for different subjects. Notice that many methods have significant gaps in performance on the unseen set versus the training set, while our method achieves consistent performance on both datasets", "source_title": "Pr-rrn: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion", "source_table": "<table><thead><tr><td>Methods</td><td>Subj. 07</td><td>Subj. 20</td><td>Subj. 23</td><td>Subj. 33</td><td>Subj. 34</td><td>Subj. 38</td><td>Subj. 39</td><td>Subj. 43</td><td>Subj. 93</td></tr></thead><tbody><tr><td>CSF [15]</td><td>1.231</td><td>1.164</td><td>1.238</td><td>1.156</td><td>1.165</td><td>1.188</td><td>1.172</td><td>1.267</td><td>1.117</td></tr><tr><td>URN [7]</td><td>1.504</td><td>1.770</td><td>1.329</td><td>1.205</td><td>1.305</td><td>1.303</td><td>1.550</td><td>1.434</td><td>1.601</td></tr><tr><td>CNS [6]</td><td>0.310</td><td>0.217</td><td>0.184</td><td>0.177</td><td>0.249</td><td>0.223</td><td>0.312</td><td>0.266</td><td>0.245</td></tr><tr><td>C3DPO [30]</td><td>0.226</td><td>0.235</td><td>0.342</td><td>0.357</td><td>0.354</td><td>0.391</td><td>0.189</td><td>0.351</td><td>0.246</td></tr><tr><td>DNRSFM [20]</td><td>0.045</td><td>0.137</td><td>0.053</td><td>0.137</td><td>0.062</td><td>0.053</td><td>0.041</td><td>0.125</td><td>0.214</td></tr><tr><td>PR-RRN (Ours)</td><td>0.024</td><td>0.034</td><td>0.039</td><td>0.043</td><td>0.039</td><td>0.034</td><td>0.025</td><td>0.028</td><td>0.152</td></tr><tr><td>PR-RRN (Unseen)</td><td>0.061</td><td>0.167</td><td>0.249</td><td>0.254</td><td>0.265</td><td>0.108</td><td>0.028</td><td>0.080</td><td>0.242</td></tr></tbody></table>", "source_caption": "Table 1: The reconstruction error \\mathrm{e_{3D}} on CMU MOCAP."}