{
    "target_title": "Zero-Shot Video Captioning with Evolving Pseudo-Tokens",
    "target_table": "<table><tbody><tr><td></td><td></td><td colspan=\"5\">Supervised Metrics</td><td colspan=\"4\">Unsupervised Metrics</td></tr><tr><td>Dataset</td><td>Method</td><td>B@4</td><td>M</td><td>C</td><td>R</td><td>\\text{CLIP-S}^{\\text{Ref}}</td><td>CLIP-S</td><td>BLIP-S</td><td>Retrieval</td><td>PP</td></tr><tr><td rowspan=\"5\">MSR-VTT</td><td>VNS-GRU [6]</td><td>0.453</td><td>0.299</td><td>0.530</td><td>0.634</td><td>0.739</td><td>0.626</td><td>0.623</td><td>0.446</td><td>118.81</td></tr><tr><td>SemSynAN [34]</td><td>0.464</td><td>0.304</td><td>0.519</td><td>0.647</td><td>0.733</td><td>0.619</td><td>0.608</td><td>0.437</td><td>155.01</td></tr><tr><td colspan=\"9\">Zero-Shot Methods</td><td></td></tr><tr><td>ZeroCap* [47]</td><td>0.023</td><td>0.129</td><td>0.058</td><td>0.304</td><td>0.739</td><td>0.710</td><td>0.575</td><td>0.442</td><td>54.71</td></tr><tr><td>MAGIC* [44]</td><td>0.055</td><td>0.133</td><td>0.074</td><td>0.354</td><td>0.628</td><td>0.566</td><td>0.434</td><td>0.392</td><td>30.48</td></tr><tr><td></td><td>Ours</td><td>0.030</td><td>0.146</td><td>0.113</td><td>0.277</td><td>0.785</td><td>0.775</td><td>0.675</td><td>0.504</td><td>18.35</td></tr><tr><td rowspan=\"4\">MSVD</td><td>VNS-GRU [6]</td><td>0.665</td><td>0.421</td><td>1.215</td><td>0.797</td><td>0.780</td><td>0.673</td><td>0.646</td><td>0.557</td><td>418.72</td></tr><tr><td>SemSynAN [34]</td><td>0.644</td><td>0.419</td><td>1.115</td><td>0.795</td><td>0.767</td><td>0.660</td><td>0.633</td><td>0.546</td><td>242.46</td></tr><tr><td colspan=\"9\">Zero-Shot Methods</td><td></td></tr><tr><td>ZeroCap* [47]</td><td>0.029</td><td>0.163</td><td>0.096</td><td>0.354</td><td>0.762</td><td>0.765</td><td>0.642</td><td>0.500</td><td>28.44</td></tr><tr><td></td><td>MAGIC* [44]</td><td>0.066</td><td>0.161</td><td>0.140</td><td>0.401</td><td>0.670</td><td>0.623</td><td>0.497</td><td>0.469</td><td>29.84</td></tr><tr><td></td><td>Ours</td><td>0.030</td><td>0.178</td><td>0.174</td><td>0.314</td><td>0.805</td><td>0.822</td><td>0.743</td><td>0.569</td><td>18.94</td></tr></tbody></table>",
    "target_caption": "Table 1: Quantitative results for video captioning. We separate the results into two categories: (i) supervised metrics that require human references, B@4 = BLEU-4, M = METEOR, C = CIDEr, S = SPICE, and \\text{CLIP-S}^{\\text{Ref}}. (ii) Unsupervised metrics that use a pre-trained model, CLIP-S = CLIP-based image-text similarity, BLIP-S = BLIP-based image-text similarity [20], Retrieval = VideoCLIP-based video-text similarity [53], and PP = caption perplexity computed with BERT [8]. (*) denotes that the model is adapted from image captioning to video captioning.",
    "source_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
    "source_table": "",
    "source_caption": ""
}