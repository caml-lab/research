{"target_title": "Zero-Shot Video Captioning with Evolving Pseudo-Tokens", "target_table": "<table><tbody><tr><td></td><td colspan=\"5\">Supervised Metrics</td><td colspan=\"2\">Unsupervised Metrics</td></tr><tr><td>Method</td><td>B@4</td><td>M</td><td>C</td><td>S</td><td>\\text{CLIP-S}^{\\text{Ref}}</td><td>CLIP-S</td><td>PP</td></tr><tr><td>VinVL [60]</td><td>0.41</td><td>0.311</td><td>1.409</td><td>0.252</td><td>0.83</td><td>0.780</td><td>24.16</td></tr><tr><td></td><td colspan=\"6\">Zero-Shot Methods</td><td></td></tr><tr><td>ZeroCap [47]</td><td>0.029</td><td>0.12</td><td>0.131</td><td>0.055</td><td>0.778</td><td>0.870</td><td>25.737</td></tr><tr><td>MAGIC [44]</td><td>0.129</td><td>0.174</td><td>0.493</td><td>0.113</td><td>0.763</td><td>0.737</td><td>37.126</td></tr><tr><td>Ours</td><td>0.022</td><td>0.127</td><td>0.172</td><td>0.073</td><td>0.798</td><td>0.885</td><td>19.049</td></tr></tbody></table>", "target_caption": "Table 2: Quantitative results for image captioning methods. We evaluate supervised metrics that measure text correspondence to human references and unsupervised metrics that are computed without referring to the human annotation.", "source_title": "Zero-shot image-to-text generation for visual-semantic arithmetic", "source_table": "<table><tbody><tr><td></td><td colspan=\"5\">Supervised Metrics</td><td colspan=\"2\">Diversity Metrics</td><td>Unsupervised Metric</td></tr><tr><td>Method</td><td>B@4</td><td>M</td><td>C</td><td>S</td><td>\\text{CLIP-S}^{\\text{Ref}}</td><td>Vocab</td><td>%Novel</td><td>CLIP-S</td></tr><tr><td>ClipCap [51]</td><td>32.15</td><td>27.1</td><td>108.35</td><td>20.12</td><td>0.81</td><td>1650</td><td>66.4%</td><td>0.77</td></tr><tr><td>CLIP-VL [64]</td><td>40.2</td><td>29.7</td><td>134.2</td><td>23.8</td><td>0.82</td><td>2464</td><td>85.1%</td><td>0.77</td></tr><tr><td>VinVL [78]</td><td>41.0</td><td>31.1</td><td>140.9</td><td>25.2</td><td>0.83</td><td>1125</td><td>77.9%</td><td>0.78</td></tr><tr><td>Ours</td><td>2.6</td><td>11.5</td><td>14.6</td><td>5.5</td><td>0.79</td><td>8681</td><td>100%</td><td>0.87</td></tr></tbody></table>", "source_caption": "Table 1: For each method, we report supervised metrics (i.e., ones requiring human references): B@1 = BLEU-1, M = METEOR, C = CIDEr, S = SPICE. We also report diversity metrics, which measures the vocabulary size (Vocab), and the number of novel sentences w.r.t the training set (%Novel). Finally, we report semantic relatedness to the image (CLIP-S), and to the human references (\\text{CLIP-S}^{\\text{Ref}}) based on CLIP\u2019s embeddings."}