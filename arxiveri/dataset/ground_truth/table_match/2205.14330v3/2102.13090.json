{"target_title": "Differentiable Point-Based Radiance Fields for Efficient View Synthesis", "target_table": "<table><thead><tr><td rowspan=\"2\">Synthetic Dataset</td><td rowspan=\"2\">Pretraining</td><td rowspan=\"2\">Training</td><td rowspan=\"2\">Inference</td><td rowspan=\"2\">Model Size</td><td colspan=\"3\">Rendering Quality</td></tr><tr><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td></tr></thead><tbody><tr><td>NeRF [Mildenhall et al., 2020]</td><td>None</td><td>20 h</td><td>1/12 fps</td><td>14 MB</td><td>31.0 dB</td><td>0.947</td><td>0.081</td></tr><tr><td>IBRNet [Wang et al., 2021]</td><td>1 day</td><td>30 min</td><td>1/25 fps</td><td>15 MB</td><td>28.1 dB</td><td>0.942</td><td>0.072</td></tr><tr><td>MVSNeRF [Chenet al., 2021b]</td><td>20 h</td><td>15 min</td><td>1/14 fps</td><td>14 MB</td><td>27.0 dB</td><td>0.931</td><td>0.168</td></tr><tr><td>Plenoxels [Yu et al., 2021a]</td><td>None</td><td>11 min</td><td>15 fps</td><td>1.1 GB</td><td>31.7 dB</td><td>0.958</td><td>0.050</td></tr><tr><td>Plenoxels_s [Yu et al., 2021a]</td><td>None</td><td>8.5 min</td><td>18 fps</td><td>234 MB</td><td>28.5 dB</td><td>0.926</td><td>0.100</td></tr><tr><td>Pulsar [Lassner andZollhofer, 2021]</td><td>None</td><td>95 min</td><td>4 fps</td><td>228 MB</td><td>26.9 dB</td><td>0.923</td><td>0.184</td></tr><tr><td>PBNR [Kopanas et al., 2021]</td><td>None</td><td>3 h</td><td>4 fps</td><td>2.96 GB</td><td>27.4 dB</td><td>0.932</td><td>0.164</td></tr><tr><td>Ours</td><td>None</td><td>3 min</td><td>32 fps</td><td>9 MB</td><td>30.3 dB</td><td>0.945</td><td>0.078</td></tr></tbody></table>", "target_caption": "Table 1. Static Novel View Synthesis Evaluation on the Synthetic Blender Dataset. We evaluate the proposed method and comparable baseline approaches for novel view synthesis on the static Blender scenes from [Mildenhall et al., 2020]. The proposed model does not require an extra dataset for pretraining and improves on existing methods in training, inference speed, model size, at cost of only a small reduction in quality. Specifically, although the concurrent Plenoxels [2021a] achieves better quality, our model is two magnitudes smaller than theirs. We also compare here to the Plenoxels_s model from [Yu et al., 2021a] (Plenoxels with smaller volume resolution), which achieves worse rendering quality with larger model size.", "source_title": "IBRNet: Learning Multi-View Image-Based Rendering", "source_table": "<table><tr><td></td><td></td><td colspan=\"3\">Diffuse Synthetic 360^{\\circ} [59]</td><td colspan=\"3\">Realistic Synthetic 360^{\\circ} [42]</td><td colspan=\"3\">Real Forward-Facing [41]</td></tr><tr><td>Method</td><td>Settings</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td></tr><tr><td>LLFF [41]</td><td rowspan=\"2\"> No per-sceneoptimization </td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr><tr><td>Ours</td><td>\\mathbf{37.17}</td><td>\\mathbf{0.990}</td><td>\\mathbf{0.017}</td><td>\\mathbf{25.49}</td><td>\\mathbf{0.916}</td><td>\\mathbf{0.100}</td><td>\\mathbf{25.13}</td><td>\\mathbf{0.817}</td><td>\\mathbf{0.205}</td></tr><tr><td>SRN [60]</td><td rowspan=\"2\"> Per-sceneoptimization </td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr><tr><td>NV [36]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NeRF [42]</td><td></td><td>40.15</td><td>0.991</td><td>0.023</td><td>\\mathbf{31.01}</td><td>\\mathbf{0.947}</td><td>0.081</td><td>26.50</td><td>0.811</td><td>0.250</td></tr><tr><td>Ours{}_{\\text{ft}}</td><td></td><td>\\mathbf{42.93}</td><td>\\mathbf{0.997}</td><td>\\mathbf{0.009}</td><td>28.14</td><td>0.942</td><td>\\mathbf{0.072}</td><td>\\mathbf{26.73}</td><td>\\mathbf{0.851}</td><td>\\mathbf{0.175}</td></tr></table>", "source_caption": "Table 1: Quantitative comparison on datasets of synthetic and real images. Our evaluation metrics are PSNR/SSIM (higher is better) and LPIPS [73] (lower is better).Both Ours and LLFF [41] are trained on large amounts of training data and then evaluated on all test scenes without any per-scene tuning. Ours consistently outperforms LLFF [41] on all datasets. We also compare our method with neural rendering methods (SRN [60], NV [36], and NeRF [42]) that train a separate network for each scene. To compete fairly with these methods, we also fine-tune our pretrained model on each scene (Ours{}_{\\text{ft}}). After fine-tuning, Ours{}_{\\text{ft}} is competitive with the state-of-the-art method NeRF [42]."}