{"target_title": "Differentiable Point-Based Radiance Fields for Efficient View Synthesis", "target_table": "<table><thead><tr><td rowspan=\"2\">Synthetic Dataset</td><td rowspan=\"2\">Pretraining</td><td rowspan=\"2\">Training</td><td rowspan=\"2\">Inference</td><td rowspan=\"2\">Model Size</td><td colspan=\"3\">Rendering Quality</td></tr><tr><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td></tr></thead><tbody><tr><td>NeRF [Mildenhall et al., 2020]</td><td>None</td><td>20 h</td><td>1/12 fps</td><td>14 MB</td><td>31.0 dB</td><td>0.947</td><td>0.081</td></tr><tr><td>IBRNet [Wang et al., 2021]</td><td>1 day</td><td>30 min</td><td>1/25 fps</td><td>15 MB</td><td>28.1 dB</td><td>0.942</td><td>0.072</td></tr><tr><td>MVSNeRF [Chenet al., 2021b]</td><td>20 h</td><td>15 min</td><td>1/14 fps</td><td>14 MB</td><td>27.0 dB</td><td>0.931</td><td>0.168</td></tr><tr><td>Plenoxels [Yu et al., 2021a]</td><td>None</td><td>11 min</td><td>15 fps</td><td>1.1 GB</td><td>31.7 dB</td><td>0.958</td><td>0.050</td></tr><tr><td>Plenoxels_s [Yu et al., 2021a]</td><td>None</td><td>8.5 min</td><td>18 fps</td><td>234 MB</td><td>28.5 dB</td><td>0.926</td><td>0.100</td></tr><tr><td>Pulsar [Lassner andZollhofer, 2021]</td><td>None</td><td>95 min</td><td>4 fps</td><td>228 MB</td><td>26.9 dB</td><td>0.923</td><td>0.184</td></tr><tr><td>PBNR [Kopanas et al., 2021]</td><td>None</td><td>3 h</td><td>4 fps</td><td>2.96 GB</td><td>27.4 dB</td><td>0.932</td><td>0.164</td></tr><tr><td>Ours</td><td>None</td><td>3 min</td><td>32 fps</td><td>9 MB</td><td>30.3 dB</td><td>0.945</td><td>0.078</td></tr></tbody></table>", "target_caption": "Table 1. Static Novel View Synthesis Evaluation on the Synthetic Blender Dataset. We evaluate the proposed method and comparable baseline approaches for novel view synthesis on the static Blender scenes from [Mildenhall et al., 2020]. The proposed model does not require an extra dataset for pretraining and improves on existing methods in training, inference speed, model size, at cost of only a small reduction in quality. Specifically, although the concurrent Plenoxels [2021a] achieves better quality, our model is two magnitudes smaller than theirs. We also compare here to the Plenoxels_s model from [Yu et al., 2021a] (Plenoxels with smaller volume resolution), which achieves worse rendering quality with larger model size.", "source_title": "Plenoxels: Radiance Fields without Neural Networks", "source_table": "<table><tbody><tr><td></td><td>PSNR \\uparrow</td><td>SSIM \\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><td>Ours: 100 images (low TV)</td><td>31.71</td><td>0.958</td><td>0.050</td></tr><tr><td>NeRF: 100 images [26]</td><td>31.01</td><td>0.947</td><td>0.081</td></tr><tr><td>Ours: 25 images (low TV)</td><td>26.88</td><td>0.911</td><td>0.099</td></tr><tr><td>Ours: 25 images (high TV)</td><td>28.25</td><td>0.932</td><td>0.078</td></tr><tr><td>NeRF: 25 images [26]</td><td>27.78</td><td>0.925</td><td>0.108</td></tr></tbody></table>", "source_caption": "Table 3: Ablation over the number of views. By increasing our TV regularization, we exceed NeRF fidelity even when the number of training views is only a quarter of the full dataset. Results are averaged over the 8 synthetic scenes from NeRF."}