{"target_title": "Learning Where to Learn in Cross-View Self-Supervised Learning", "target_table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Object Det.</td><td colspan=\"3\">Instance Seg.</td></tr><tr><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><td colspan=\"7\">ResNet50-C4:</td></tr><tr><td>Supervised{}^{\\dagger}</td><td>38.2</td><td>58.2</td><td>41.2</td><td>33.3</td><td>54.7</td><td>35.2</td></tr><tr><td>SimCLR [7]{}^{\\dagger}</td><td>37.9</td><td>57.7</td><td>40.9</td><td>33.3</td><td>54.6</td><td>35.3</td></tr><tr><td>SwAV [3]{}^{\\dagger}</td><td>37.6</td><td>57.6</td><td>40.3</td><td>33.1</td><td>54.2</td><td>35.1</td></tr><tr><td>BYOL [14]{}^{\\dagger}</td><td>37.9</td><td>57.8</td><td>40.9</td><td>33.2</td><td>54.3</td><td>35.0</td></tr><tr><td>SimSiam [9]{}^{\\dagger}</td><td>37.9</td><td>57.5</td><td>40.9</td><td>33.2</td><td>54.2</td><td>35.2</td></tr><tr><td>MoCov2 [15]{}^{*}</td><td>38.8</td><td>58.0</td><td>42.0</td><td>34.0</td><td>55.2</td><td>36.3</td></tr><tr><td>BYOL [14]{}^{*}</td><td>38.1</td><td>58.4</td><td>40.9</td><td>33.3</td><td>55.0</td><td>35.3</td></tr><tr><td>LEWEL{}_{M}</td><td>38.9</td><td>58.6</td><td>42.0</td><td>34.1</td><td>55.3</td><td>36.3</td></tr><tr><td>LEWEL{}_{B}</td><td>38.5</td><td>58.9</td><td>41.2</td><td>33.7</td><td>55.5</td><td>35.5</td></tr><tr><td colspan=\"7\">ResNet50-FPN:</td></tr><tr><td>DenseCL [40]</td><td>40.3</td><td>59.9</td><td>44.3</td><td>36.4</td><td>57.0</td><td>39.2</td></tr><tr><td>ReSim [43]</td><td>39.8</td><td>60.2</td><td>43.5</td><td>36.0</td><td>57.1</td><td>38.6</td></tr><tr><td>LEWEL{}_{M}</td><td>40.0</td><td>59.8</td><td>43.7</td><td>36.1</td><td>57.0</td><td>38.7</td></tr><tr><td>LEWEL{}_{B}</td><td>41.3</td><td>61.2</td><td>45.4</td><td>37.4</td><td>58.3</td><td>40.3</td></tr><tr><td>PixelPro [44] (400 ep)</td><td>41.4</td><td>61.6</td><td>45.4</td><td>37.4</td><td>-</td><td>-</td></tr><tr><td>LEWEL{}_{B} (400 ep)</td><td>41.9</td><td>62.4</td><td>46.0</td><td>37.9</td><td>59.3</td><td>40.7</td></tr></tbody></table>", "target_caption": "Table 4: Transfer learning to MS-COCO Object Detection and Instance Segmentation with models pre-trained for 200 epochs on IN-1K dataset. All entries are based on the Mask R-CNN [16] architecture. {\\dagger}: results from [9]. *: our reproduction.", "source_title": "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning", "source_table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\">#. Epoch</td><td colspan=\"3\">Pascal VOC (R50-C4)</td><td colspan=\"3\">COCO (R50-FPN)</td><td colspan=\"3\">COCO (R50-C4)</td><td>Cityscapes (R50)</td></tr><tr><td>AP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mAP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mAP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mIoU</td></tr><tr><td>scratch</td><td>-</td><td>33.8</td><td>60.2</td><td>33.1</td><td>32.8</td><td>51.0</td><td>35.3</td><td>26.4</td><td>44.0</td><td>27.8</td><td>65.3</td></tr><tr><td>supervised</td><td>100</td><td>53.5</td><td>81.3</td><td>58.8</td><td>39.7</td><td>59.5</td><td>43.3</td><td>38.2</td><td>58.2</td><td>41.2</td><td>74.6</td></tr><tr><td>MoCo [19]</td><td>200</td><td>55.9</td><td>81.5</td><td>62.6</td><td>39.4</td><td>59.1</td><td>43.0</td><td>38.5</td><td>58.3</td><td>41.6</td><td>75.3</td></tr><tr><td>SimCLR [9]</td><td>1000</td><td>56.3</td><td>81.9</td><td>62.5</td><td>39.8</td><td>59.5</td><td>43.6</td><td>38.4</td><td>58.3</td><td>41.6</td><td>75.8</td></tr><tr><td>MoCo v2 [10]</td><td>800</td><td>57.6</td><td>82.7</td><td>64.4</td><td>40.4</td><td>60.1</td><td>44.3</td><td>39.5</td><td>59.0</td><td>42.6</td><td>76.2</td></tr><tr><td>InfoMin [35]</td><td>200</td><td>57.6</td><td>82.7</td><td>64.6</td><td>40.6</td><td>60.6</td><td>44.6</td><td>39.0</td><td>58.5</td><td>42.0</td><td>75.6</td></tr><tr><td>InfoMin [35]</td><td>800</td><td>57.5</td><td>82.5</td><td>64.0</td><td>40.4</td><td>60.4</td><td>44.3</td><td>38.8</td><td>58.2</td><td>41.7</td><td>75.6</td></tr><tr><td>PixPro (ours)</td><td>100</td><td>58.8</td><td>83.0</td><td>66.5</td><td>41.3</td><td>61.3</td><td>45.4</td><td>40.0</td><td>59.3</td><td>43.4</td><td>76.8</td></tr><tr><td>PixPro (ours)</td><td>400</td><td>60.2</td><td>83.8</td><td>67.7</td><td>41.4</td><td>61.6</td><td>45.4</td><td>40.5</td><td>59.8</td><td>44.0</td><td>77.2</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "source_caption": "Table 1: Comparing the proposed pixel-level pre-training method, PixPro, to previous supervised/unsupervised pre-training methods. For Pascal VOC object detection, a Faster R-CNN (R50-C4) detector is adopted for all methods. For COCO object detection, a Mask R-CNN detector (R50-FPN and R50-C4) with 1\\times setting is adopted for all methods. For Cityscapes semantic segmentation, an FCN method (R50) is used. Only a pixel-level pretext task is involved in PixPro pre-training. For Pascal VOC (R50-C4), COCO (R50-C4) and Cityscapes (R50), a regular backbone network of R50 with output feature map of C5 is adopted for PixPro pre-training. For COCO (R50-FPN), an FPN network with P_{3}-P_{6} feature maps is used. Note that InfoMin [35] reports results for only its 200 epoch model, so we reproduce it with longer training lengths, where saturation is observed."}