{
    "target_title": "Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion",
    "target_table": "<table><thead><tr><td>Method</td><td>RMSE\\downarrow</td><td>Rel\\downarrow</td><td>\\delta_{1}\\uparrow</td><td>\\delta_{2}\\uparrow</td><td>\\delta_{3}\\uparrow</td></tr></thead><tbody><tr><td>Eigen et al. [1]</td><td>0.641</td><td>0.158</td><td>0.769</td><td>0.95</td><td>0.988</td></tr><tr><td>DORN [2]</td><td>0.509</td><td>0.115</td><td>0.828</td><td>0.965</td><td>0.992</td></tr><tr><td>Chen et al. [5]</td><td>0.514</td><td>0.111</td><td>0.878</td><td>0.977</td><td>0.994</td></tr><tr><td>VNL [23]</td><td>0.416</td><td>0.108</td><td>0.875</td><td>0.976</td><td>0.994</td></tr><tr><td>BTS [3]</td><td>0.392</td><td>0.110</td><td>0.885</td><td>0.978</td><td>0.994</td></tr><tr><td>DAV [4]</td><td>0.412</td><td>0.108</td><td>0.882</td><td>0.980</td><td>0.996</td></tr><tr><td>DPT-Hybrid [18]</td><td>0.357</td><td>0.110</td><td>0.904</td><td>0.988</td><td>0.998</td></tr><tr><td>Adabins [19]</td><td>0.364</td><td>0.103</td><td>0.903</td><td>0.984</td><td>0.997</td></tr><tr><td>Depthformer (ours)</td><td>0.345</td><td>0.100</td><td>0.911</td><td>0.988</td><td>0.997</td></tr></tbody></table>",
    "target_caption": "Table 1: Results on NYUV2 Dataset. The best results are in bold and second best are underlined. Our method outperforms the previous SoTA methodsin most of the metrics. ",
    "source_title": "Pyramid Vision Transformer: A Versatile Backbone For Dense Prediction Without Convolutions",
    "source_table": "",
    "source_caption": ""
}