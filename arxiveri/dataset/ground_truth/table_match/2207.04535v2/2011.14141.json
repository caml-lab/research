{"target_title": "Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion", "target_table": "<table><thead><tr><td>Method</td><td>RMSE\\downarrow</td><td>Rel\\downarrow</td><td>\\delta_{1}\\uparrow</td><td>\\delta_{2}\\uparrow</td><td>\\delta_{3}\\uparrow</td></tr></thead><tbody><tr><td>Eigen et al. [1]</td><td>0.641</td><td>0.158</td><td>0.769</td><td>0.95</td><td>0.988</td></tr><tr><td>DORN [2]</td><td>0.509</td><td>0.115</td><td>0.828</td><td>0.965</td><td>0.992</td></tr><tr><td>Chen et al. [5]</td><td>0.514</td><td>0.111</td><td>0.878</td><td>0.977</td><td>0.994</td></tr><tr><td>VNL [23]</td><td>0.416</td><td>0.108</td><td>0.875</td><td>0.976</td><td>0.994</td></tr><tr><td>BTS [3]</td><td>0.392</td><td>0.110</td><td>0.885</td><td>0.978</td><td>0.994</td></tr><tr><td>DAV [4]</td><td>0.412</td><td>0.108</td><td>0.882</td><td>0.980</td><td>0.996</td></tr><tr><td>DPT-Hybrid [18]</td><td>0.357</td><td>0.110</td><td>0.904</td><td>0.988</td><td>0.998</td></tr><tr><td>Adabins [19]</td><td>0.364</td><td>0.103</td><td>0.903</td><td>0.984</td><td>0.997</td></tr><tr><td>Depthformer (ours)</td><td>0.345</td><td>0.100</td><td>0.911</td><td>0.988</td><td>0.997</td></tr></tbody></table>", "target_caption": "Table 1: Results on NYUV2 Dataset. The best results are in bold and second best are underlined. Our method outperforms the previous SoTA methodsin most of the metrics. ", "source_title": "AdaBins: Depth Estimation Using Adaptive Bins", "source_table": "<table><thead><tr><td>Method</td><td><p>\\delta_{1}\\uparrow</p></td><td><p>\\delta_{2}\\uparrow</p></td><td><p>\\delta_{3} \\uparrow</p></td><td><p>REL \\downarrow</p></td><td><p>RMS \\downarrow</p></td><td>log_{10} \\downarrow</td></tr></thead><tbody><tr><td>Eigen et al. [8]</td><td><p>0.769</p></td><td><p>0.950</p></td><td><p>0.988</p></td><td><p>0.158</p></td><td><p>0.641</p></td><td>\u2013</td></tr><tr><td>Laina et al. [25]</td><td><p>0.811</p></td><td><p>0.953</p></td><td><p>0.988</p></td><td><p>0.127</p></td><td><p>0.573</p></td><td>0.055</td></tr><tr><td>Hao et al. [16]</td><td><p>0.841</p></td><td><p>0.966</p></td><td><p>0.991</p></td><td><p>0.127</p></td><td><p>0.555</p></td><td>0.053</td></tr><tr><td>Lee et al. [27]</td><td><p>0.837</p></td><td><p>0.971</p></td><td><p>0.994</p></td><td><p>0.131</p></td><td><p>0.538</p></td><td>\u2013</td></tr><tr><td>Fu et al. [11]</td><td><p>0.828</p></td><td><p>0.965</p></td><td><p>0.992</p></td><td><p>0.115</p></td><td><p>0.509</p></td><td>0.051</td></tr><tr><td>SharpNet [34]</td><td><p>0.836</p></td><td><p>0.966</p></td><td><p>0.993</p></td><td><p>0.139</p></td><td><p>0.502</p></td><td>0.047</td></tr><tr><td>Hu et al. [19]</td><td><p>0.866</p></td><td><p>0.975</p></td><td><p>0.993</p></td><td><p>0.115</p></td><td><p>0.530</p></td><td>0.050</td></tr><tr><td>Chen et al. [4]</td><td><p>0.878</p></td><td><p>0.977</p></td><td><p>0.994</p></td><td><p>0.111</p></td><td><p>0.514</p></td><td>0.048</td></tr><tr><td>Yin et al. [47]</td><td><p>0.875</p></td><td><p>0.976</p></td><td><p>0.994</p></td><td>0.108</td><td><p>0.416</p></td><td>0.048</td></tr><tr><td>BTS [26]</td><td>0.885</td><td><p>0.978</p></td><td><p>0.994</p></td><td><p>0.110</p></td><td>0.392</td><td>0.047</td></tr><tr><td>DAV [22]</td><td><p>0.882</p></td><td>0.980</td><td>0.996</td><td>0.108</td><td><p>0.412</p></td><td>\u2013</td></tr><tr><td>AdaBins (Ours)</td><td>0.903</td><td>0.984</td><td>0.997</td><td>0.103</td><td>0.364</td><td>0.044</td></tr></tbody></table>", "source_caption": "Table 2: Comparison of performances on the NYU-Depth-v2 dataset. The reported numbers are from the corresponding original papers. Best results are in bold, second best are underlined."}