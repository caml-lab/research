{
    "target_title": "Video Question Answering: Datasets, Algorithms and Challenges",
    "target_table": "<table><tbody><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Techniques &amp; Insights</td><td colspan=\"2\">Encoder</td><td rowspan=\"2\">Pre-trainingDataset</td><td rowspan=\"2\">TGIF-QA(Frame-QA)</td><td rowspan=\"2\">MSVD-QA</td><td rowspan=\"2\">MSRVTT-QA</td></tr><tr><td>Video</td><td>Text</td></tr><tr><td>STVQA(Jang et al., 2019)</td><td>Att</td><td>RN, Flow</td><td>GV</td><td>/</td><td>52.0</td><td>/</td><td>/</td></tr><tr><td>PSAC(Li et al., 2019)</td><td>Att</td><td>RN</td><td>GV</td><td>/</td><td>55.7</td><td>/</td><td>/</td></tr><tr><td>QueST(Jiang et al., 2020)</td><td>Att</td><td>RN, C3D</td><td>GV</td><td>/</td><td>59.7</td><td>36.1</td><td>34.6</td></tr><tr><td>CoMem(Gao et al., 2018)</td><td>Mem</td><td>RN, Flow</td><td>GV</td><td>/</td><td>51.5</td><td>/</td><td>/</td></tr><tr><td>HME(Fan et al., 2019)</td><td>Mem</td><td>RN, VGG, C3D</td><td>GV</td><td>/</td><td>53.8</td><td>33.7</td><td>33.0</td></tr><tr><td>LGCN(Huang et al., 2020)</td><td>GNN</td><td>RN, RoI</td><td>GV</td><td>/</td><td>56.3</td><td>34.3</td><td>/</td></tr><tr><td>HGA(Jiang and Han, 2020)</td><td>GNN</td><td>RN, VGG, C3D</td><td>GV</td><td>/</td><td>55.1</td><td>34.7</td><td>35.5</td></tr><tr><td>B2A(Park et al., 2021)</td><td>GNN, MG</td><td>RN, RX(3D)</td><td>GV</td><td>/</td><td>57.5</td><td>37.2</td><td>36.9</td></tr><tr><td>HAIR(Liu et al., 2021a)</td><td>GNN, Mem, HL</td><td>RoI</td><td>GV</td><td>/</td><td>60.2</td><td>37.5</td><td>36.9</td></tr><tr><td>MASN(Seo et al., 2021a)</td><td>GNN</td><td>RN, I3D, RoI</td><td>GV</td><td></td><td>59.5</td><td>38.0</td><td>35.2</td></tr><tr><td>DualVGR(Wang et al., 2021)</td><td>GNN</td><td>RN, RX(3D)</td><td>GV</td><td>/</td><td>/</td><td>39.0</td><td>35.5</td></tr><tr><td>PGAT(Peng et al., 2021)</td><td>GNN, MG, HL</td><td>RN, RX(3D), RoI</td><td>GV</td><td>/</td><td>61.1</td><td>39.0</td><td>38.1</td></tr><tr><td>HCRN(Le et al., 2020)</td><td>MN, HL</td><td>RN, RX(3D)</td><td>GV</td><td>/</td><td>55.9</td><td>36.1</td><td>35.6</td></tr><tr><td>HOSTR(Dang et al., 2021)</td><td>MN, GNN, HL</td><td>RN, RX(3D), RoI</td><td>GV</td><td>/</td><td>58.2</td><td>39.4</td><td>35.9</td></tr><tr><td>HQGA(Xiao et al., 2022a)</td><td>MN, GNN, HL, MG</td><td>RN, RX(3D), RoI</td><td>BT</td><td>/</td><td>61.3</td><td>41.2</td><td>38.6</td></tr><tr><td>MHN(Peng et al., 2022)</td><td>TF, HL, MG</td><td>RN, RX(3D)</td><td>GV</td><td>/</td><td>58.1</td><td>40.4</td><td>38.6</td></tr><tr><td>VGT(Xiao et al., 2022b)</td><td>TF, GNN</td><td>RN, RoI</td><td>BT</td><td>/</td><td>61.6</td><td>/</td><td>39.7</td></tr><tr><td>ClipBERT(Lei et al., 2021)</td><td>TF, CM-PF</td><td>RN (E2E)</td><td>BT</td><td>VG&amp;COCO</td><td>60.3</td><td>/</td><td>37.4</td></tr><tr><td>CoMVT(Seo et al., 2021b)</td><td>TF, CM-PF</td><td>S3D</td><td>BT</td><td>HowTo100M</td><td>/</td><td>42.6</td><td>39.5</td></tr><tr><td>VQA-T(Yang et al., 2021a)</td><td>TF, CM-PF</td><td>S3D</td><td>BT</td><td>H2VQA69M</td><td>/</td><td>46.3</td><td>41.5</td></tr><tr><td>SiaSRea(Yu et al., 2021)</td><td>TF, GNN, CM-PF</td><td>RN (E2E)</td><td>BT</td><td>VG&amp;COCO</td><td>60.2</td><td>45.5</td><td>41.6</td></tr><tr><td>MERLOT(Zellers et al., 2021)</td><td>TF, CM-PF</td><td>ViT(E2E)</td><td>BT</td><td>YT-T &amp; CC</td><td>69.5</td><td>/</td><td>43.1</td></tr><tr><td>VIOLET(Fu et al., 2021)</td><td>TF, CM-PF</td><td>VSwin (E2E)</td><td>BT</td><td>Web&amp;YT-T&amp;CC</td><td>68.9</td><td>47.9</td><td>43.9</td></tr></tbody></table>",
    "target_caption": "Table 2: Performance on Factoid VideoQA tasks. (Att: Attention, MG: Multi-Granularity, HL: Hierarchical Learning, CM-PF: Cross-modal Pre-training and Fine-tuning, Mem: Memory, GNN: Graph Neural Networks, MN: Modular Networks, TF: Transformer. RN: ResNet at frame-level, RX(3D): 3D ResNeXt at clip-level, RoI: Region-of-interest features from Faster R-CNN, GV: GloVe, BT: BERT, VG: Visual Genome (Krishna et al., 2017), YT-T: Youtube-Temporal-180M (Zellers et al., 2021), Web: WebVid2M (Bain et al., 2021), CC: Conceptual Captions-3M (Sharma et al., 2018). ViT (Dosovitskiy et al., 2020)and VSwin (Liu et al., 2021b)are Transformer-style visual encoders. Attention is found in all methods, but we omit it for those methods that do not emphasize attention.) ",
    "source_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
    "source_table": "",
    "source_caption": ""
}