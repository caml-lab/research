{"target_title": "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment", "target_table": "<table><tbody><tr><td>Method</td><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><td>ClipBERT [21]</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>VLM [45]</td><td>28.1</td><td>55.5</td><td>67.4</td><td>4.0</td></tr><tr><td>MMT [9]</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>Support Set [34]</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>Frozen [2]</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>VideoCLIP [46]</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><td>HD-VILA [48]</td><td>35.6</td><td>65.3</td><td>78.0</td><td>3.0</td></tr><tr><td>Florence [52]</td><td>37.6</td><td>63.8</td><td>72.6</td><td>-</td></tr><tr><td>All-in-One [40]</td><td>37.9</td><td>68.1</td><td>77.1</td><td>-</td></tr><tr><td>BridgeFormer [11]</td><td>37.6</td><td>64.8</td><td>75.1</td><td>3.0</td></tr><tr><td>CLIP-ViT-B/32</td><td colspan=\"4\"></td></tr><tr><td>CLIP4Clip [29]</td><td>44.5</td><td>71.4</td><td>81.6</td><td>2.0</td></tr><tr><td>CenterCLIP [57]</td><td>44.2</td><td>71.6</td><td>82.1</td><td>2.0</td></tr><tr><td>XPool [12]</td><td>46.9</td><td>72.8</td><td>82.2</td><td>2.0</td></tr><tr><td>CLIP2Video [7]</td><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td></tr><tr><td>CLIP2Video\u2020[3]</td><td>47.2</td><td>73.0</td><td>83.0</td><td>2.0</td></tr><tr><td>CLIP2TV [10]</td><td>46.1</td><td>72.5</td><td>82.9</td><td>2.0</td></tr><tr><td>DRL [43]</td><td>47.4</td><td>74.6</td><td>83.8</td><td>2.0</td></tr><tr><td>CAMoE* [6]</td><td>47.3</td><td>74.2</td><td>84.5</td><td>2.0</td></tr><tr><td>Ours</td><td>50.1</td><td>74.8</td><td>84.6</td><td>1.0</td></tr><tr><td>Ours*</td><td>55.9</td><td>77.0</td><td>86.8</td><td>1.0</td></tr><tr><td>CLIP-ViT-B/16</td><td colspan=\"4\"></td></tr><tr><td>CenterCLIP [57]</td><td>48.4</td><td>73.8</td><td>82.0</td><td>2.0</td></tr><tr><td>CLIP2TV [10]</td><td>49.3</td><td>74.7</td><td>83.6</td><td>2.0</td></tr><tr><td>DRL [43]</td><td>50.2</td><td>76.5</td><td>84.7</td><td>1.0</td></tr><tr><td>DRL\u2020[43]</td><td>53.3</td><td>80.3</td><td>87.6</td><td>1.0</td></tr><tr><td>Ours</td><td>54.2</td><td>77.2</td><td>84.8</td><td>1.0</td></tr><tr><td>Ours*</td><td>57.7</td><td>80.5</td><td>88.2</td><td>1.0</td></tr></tbody></table>", "target_caption": "Table 5: Comparison of text-to-video retrieval in MSR-VTT [47]. * and \u2020 respectively denotes that the method uses DSL [6] and QB-Norm [3] as post-processing operations.", "source_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "source_table": "<table><tbody><tr><td>Method</td><td>E2E\\dagger</td><td>Vis Enc. Init.</td><td>Visual-Text PT</td><td>#pairs PT</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><td>JSFusion [75]</td><td>\\checkmark</td><td>-</td><td>-</td><td>-</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td></tr><tr><td>ActBERT [80]</td><td>\\checkmark</td><td>VisGenome</td><td>HowTo100M</td><td>136M</td><td>16.3</td><td>42.8</td><td>56.9</td><td>10.0</td></tr><tr><td>HERO [34]</td><td>\\checkmark</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><td>VidTranslate [28]</td><td>\\checkmark</td><td>IG65M</td><td>HowTo100M</td><td>136M</td><td>14.7</td><td>-</td><td>52.8</td><td></td></tr><tr><td>NoiseEst. [2]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8.0</td></tr><tr><td>\\rowcoloraliceblue CE [38]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td></tr><tr><td>UniVL [40]</td><td>\u2717</td><td>-</td><td>HowTo100M</td><td>136M</td><td>21.2</td><td>49.6</td><td>63.1</td><td>6.0</td></tr><tr><td>ClipBERT [32]</td><td>\u2713</td><td>-</td><td>COCO, VisGenome</td><td>5.6M</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>AVLnet [55]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue MMT [21]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>HowTo100M</td><td>136M</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue T2VLAD [69]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>29.5</td><td>59.0</td><td>70.1</td><td>4.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>-</td><td>-</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M</td><td>3M</td><td>25.5</td><td>54.5</td><td>66.1</td><td>4.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>32.5</td><td>61.5</td><td>71.2</td><td>3.0</td></tr><tr><td>Zero-shot</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>7.5</td><td>21.2</td><td>29.6</td><td>38.0</td></tr><tr><td>SupportSet [48]</td><td></td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>23.2</td><td>44.6</td><td>56.6</td><td>7.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>24.7</td><td>46.9</td><td>57.2</td><td>7.0</td></tr></tbody></table>", "source_caption": "Table 4: Comparison to state-of-the-art results on MSR-VTT for text-to-video retrieval, 1k-A split. \\daggerE2E: Works trained on pixels directly, without using pre-extracted expert features trained for other tasks. Vis Enc. Init.: Datasets used for pretraining visual encoders for tasks other than visual-text retrieval, eg object classification. Visual-Text PT: Visual-text pretraining data. Rows highlighted in blue use additional modalities such as sound and speech from the MSR-VTT test videos. \\dagger Object, Motion, Face, Scene, Speech, OCR and Sound classification features."}