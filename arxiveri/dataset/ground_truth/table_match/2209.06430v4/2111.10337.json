{"target_title": "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment", "target_table": "<table><tbody><tr><td>Method</td><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><td>ClipBERT [21]</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>VLM [45]</td><td>28.1</td><td>55.5</td><td>67.4</td><td>4.0</td></tr><tr><td>MMT [9]</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>Support Set [34]</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>Frozen [2]</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>VideoCLIP [46]</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><td>HD-VILA [48]</td><td>35.6</td><td>65.3</td><td>78.0</td><td>3.0</td></tr><tr><td>Florence [52]</td><td>37.6</td><td>63.8</td><td>72.6</td><td>-</td></tr><tr><td>All-in-One [40]</td><td>37.9</td><td>68.1</td><td>77.1</td><td>-</td></tr><tr><td>BridgeFormer [11]</td><td>37.6</td><td>64.8</td><td>75.1</td><td>3.0</td></tr><tr><td>CLIP-ViT-B/32</td><td colspan=\"4\"></td></tr><tr><td>CLIP4Clip [29]</td><td>44.5</td><td>71.4</td><td>81.6</td><td>2.0</td></tr><tr><td>CenterCLIP [57]</td><td>44.2</td><td>71.6</td><td>82.1</td><td>2.0</td></tr><tr><td>XPool [12]</td><td>46.9</td><td>72.8</td><td>82.2</td><td>2.0</td></tr><tr><td>CLIP2Video [7]</td><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td></tr><tr><td>CLIP2Video\u2020[3]</td><td>47.2</td><td>73.0</td><td>83.0</td><td>2.0</td></tr><tr><td>CLIP2TV [10]</td><td>46.1</td><td>72.5</td><td>82.9</td><td>2.0</td></tr><tr><td>DRL [43]</td><td>47.4</td><td>74.6</td><td>83.8</td><td>2.0</td></tr><tr><td>CAMoE* [6]</td><td>47.3</td><td>74.2</td><td>84.5</td><td>2.0</td></tr><tr><td>Ours</td><td>50.1</td><td>74.8</td><td>84.6</td><td>1.0</td></tr><tr><td>Ours*</td><td>55.9</td><td>77.0</td><td>86.8</td><td>1.0</td></tr><tr><td>CLIP-ViT-B/16</td><td colspan=\"4\"></td></tr><tr><td>CenterCLIP [57]</td><td>48.4</td><td>73.8</td><td>82.0</td><td>2.0</td></tr><tr><td>CLIP2TV [10]</td><td>49.3</td><td>74.7</td><td>83.6</td><td>2.0</td></tr><tr><td>DRL [43]</td><td>50.2</td><td>76.5</td><td>84.7</td><td>1.0</td></tr><tr><td>DRL\u2020[43]</td><td>53.3</td><td>80.3</td><td>87.6</td><td>1.0</td></tr><tr><td>Ours</td><td>54.2</td><td>77.2</td><td>84.8</td><td>1.0</td></tr><tr><td>Ours*</td><td>57.7</td><td>80.5</td><td>88.2</td><td>1.0</td></tr></tbody></table>", "target_caption": "Table 5: Comparison of text-to-video retrieval in MSR-VTT [47]. * and \u2020 respectively denotes that the method uses DSL [6] and QB-Norm [3] as post-processing operations.", "source_title": "Advancing high-resolution video-language representation with large-scale video transcriptions", "source_table": "<table><thead><tr><td>Method</td><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MedR \\downarrow</td></tr></thead><tbody><tr><td>HowTo100M [41]</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td></tr><tr><td>CE [35]</td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td></tr><tr><td>DECEMBERT [50]</td><td>17.5</td><td>44.3</td><td>58.6</td><td>9.0</td></tr><tr><td>HERO [32]</td><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><td>ClipBERT [30]</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>VLM [57]</td><td>28.1</td><td>55.5</td><td>67.4</td><td>4.0</td></tr><tr><td>MMT [15]</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>Support Set [43]</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>VideoCLIP [58]</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><td>Ours</td><td>35.6</td><td>65.3</td><td>78.0</td><td>3.0</td></tr><tr><td>Zero-shot</td><td></td><td></td><td></td><td></td></tr><tr><td>HT MIL-NCE [39]</td><td>9.9</td><td>24.0</td><td>32.4</td><td>29.5</td></tr><tr><td>Support Set [43]</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31.0</td></tr><tr><td>VideoCLIP [58]</td><td>10.4</td><td>22.2</td><td>30.0</td><td>-</td></tr><tr><td>Ours</td><td>14.6</td><td>34.4</td><td>44.1</td><td>15.0</td></tr></tbody></table>", "source_caption": "Table 3: Comparison of text-to-video retrieval in MSR-VTT [59]. We gray out some lines to highlight fair comparisons with traditional retrieval models and general pre-training models. This mark is also applicable to Table 5, 6."}