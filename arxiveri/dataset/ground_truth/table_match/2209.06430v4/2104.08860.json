{"target_title": "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment", "target_table": "<table><tbody><tr><td>Method</td><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><td>MMT [9]</td><td>12.9</td><td>29.9</td><td>40.1</td><td>19.3</td></tr><tr><td>Frozen [2]</td><td>15.0</td><td>30.8</td><td>40.3</td><td>20.0</td></tr><tr><td>HD-VILA [48]</td><td>17.4</td><td>34.1</td><td>44.1</td><td>15.0</td></tr><tr><td>BridgeFormer [11]</td><td>17.9</td><td>35.4</td><td>44.5</td><td>15.0</td></tr><tr><td>CLIP-ViT-B/32</td><td colspan=\"4\"></td></tr><tr><td>CLIP4Clip [29]</td><td>21.6</td><td>41.8</td><td>49.8</td><td>11.0</td></tr><tr><td>CenterCLIP [57]</td><td>21.7</td><td>39.8</td><td>49.8</td><td>11.0</td></tr><tr><td>XPool [12]</td><td>22.7</td><td>42.6</td><td>51.2</td><td>10.0</td></tr><tr><td>DRL [43]</td><td>24.9</td><td>45.7</td><td>55.3</td><td>7.0</td></tr><tr><td>CAMoE* [6]</td><td>25.9</td><td>46.1</td><td>53.7</td><td>-</td></tr><tr><td>Ours</td><td>25.6</td><td>45.3</td><td>54.4</td><td>8.0</td></tr><tr><td>Ours*</td><td>26.0</td><td>46.4</td><td>54.9</td><td>8.0</td></tr><tr><td>CLIP-ViT-B/16</td><td colspan=\"4\"></td></tr><tr><td>CLIP4Clip by [57]</td><td>24.1</td><td>45.0</td><td>55.1</td><td>8.0</td></tr><tr><td>CenterCLIP [57]</td><td>24.2</td><td>46.2</td><td>55.9</td><td>8.0</td></tr><tr><td>DRL [43]</td><td>26.5</td><td>47.6</td><td>56.8</td><td>7.0</td></tr><tr><td>Ours</td><td>29.4</td><td>50.6</td><td>59.0</td><td>5.0</td></tr><tr><td>Ours*</td><td>30.7</td><td>51.4</td><td>60.6</td><td>5.0</td></tr></tbody></table>", "target_caption": "Table 8: Comparison of text-to-video retrieval in LSMDC [37]. * denotes that the method uses post-processing operations DSL [6].", "source_title": "Clip4clip: An empirical study of clip for end to end video clip retrieval", "source_table": "<table><tbody><tr><td>Methods</td><td>TrainD</td><td>E2E</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td><td>MnR\\downarrow</td></tr><tr><td>CT-SAN{}^{a}</td><td>L</td><td>\u2713</td><td>5.1</td><td>16.3</td><td>25.2</td><td>46.0</td><td>-</td></tr><tr><td>JSFusion{}^{b}</td><td>L</td><td>\u2713</td><td>9.1</td><td>21.2</td><td>34.1</td><td>36.0</td><td>-</td></tr><tr><td>CE{}^{c}</td><td>L</td><td></td><td>11.2</td><td>26.9</td><td>34.8</td><td>25.3</td><td>96.8</td></tr><tr><td>MMT{}^{d}</td><td>H+L</td><td></td><td>12.9</td><td>29.9</td><td>40.1</td><td>19.3</td><td>75.0</td></tr><tr><td>NoiseE{}^{e}</td><td>H+L</td><td></td><td>6.4</td><td>19.8</td><td>28.4</td><td>39.0</td><td>-</td></tr><tr><td>CLIP-straight{}^{f}</td><td>L</td><td>\u2713</td><td>11.3</td><td>22.7</td><td>29.2</td><td>56.5</td><td>-</td></tr><tr><td>MDMMT{}^{g}</td><td>MD+L</td><td></td><td>18.8</td><td>38.5</td><td>47.9</td><td>12.3</td><td>58.0</td></tr><tr><td>Frozen{}^{h}</td><td>CW+L</td><td>\u2713</td><td>15.0</td><td>30.8</td><td>39.8</td><td>20.0</td><td>-</td></tr><tr><td>HiT{}^{i}</td><td>H+L</td><td></td><td>14.0</td><td>31.2</td><td>41.6</td><td>18.5</td><td>-</td></tr><tr><td>TT-CE+{}^{j}</td><td>L</td><td></td><td>17.2</td><td>36.5</td><td>46.3</td><td>13.7</td><td>-</td></tr><tr><td>(Ours)-meanP</td><td>W+L</td><td>\u2713</td><td>20.7</td><td>38.9</td><td>47.2</td><td>13.0</td><td>65.3</td></tr><tr><td>(Ours)-seqLSTM</td><td>W+L</td><td>\u2713</td><td>21.6</td><td>41.8</td><td>49.8</td><td>11.0</td><td>58.0</td></tr><tr><td>(Ours)-seqTransf</td><td>W+L</td><td>\u2713</td><td>22.6</td><td>41.0</td><td>49.1</td><td>11.0</td><td>61.0</td></tr><tr><td>(Ours)-tightTransf</td><td>W+L</td><td>\u2713</td><td>18.9</td><td>37.8</td><td>46.7</td><td>13.0</td><td>61.6</td></tr></tbody></table>", "source_caption": "Table 3: Results of text-to-video retrieval on LSMDC dataset. In the column \u2018TrainD\u2019, L, H, and W denote training on LSMDC, HowTo100M Miech et al. (2019), and WIT Radford et al. (2021), MD used in Dzabraev et al. (2021) denotes a combined multidomain dataset containing MSR-VTT, LSMDC, HowTo100M, etc., and CW means CC3M Sharma et al. (2018) plus WebVid-2M Bain et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline methods are {}^{a}CT-SAN Yu et al. (2017), {}^{b}JSFusion Yu et al. (2018), {}^{c}CE Liu et al. (2019), {}^{d}MMT Gabeur et al. (2020), {}^{e}NoiseE Amrani et al. (2021), {}^{f}CLIP-straight Portillo-Quintero et al. (2021), {}^{g}MDMMT Dzabraev et al. (2021), {}^{h}Frozen Bain et al. (2021), {}^{i}HiT Liu et al. (2021), {}^{j}TT-CE+ Croitoru et al. (2021)."}