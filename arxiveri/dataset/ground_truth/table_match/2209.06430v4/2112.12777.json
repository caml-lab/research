{"target_title": "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment", "target_table": "<table><tbody><tr><td>Method</td><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><td>ClipBERT [21]</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>VLM [45]</td><td>28.1</td><td>55.5</td><td>67.4</td><td>4.0</td></tr><tr><td>MMT [9]</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>Support Set [34]</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>Frozen [2]</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>VideoCLIP [46]</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><td>HD-VILA [48]</td><td>35.6</td><td>65.3</td><td>78.0</td><td>3.0</td></tr><tr><td>Florence [52]</td><td>37.6</td><td>63.8</td><td>72.6</td><td>-</td></tr><tr><td>All-in-One [40]</td><td>37.9</td><td>68.1</td><td>77.1</td><td>-</td></tr><tr><td>BridgeFormer [11]</td><td>37.6</td><td>64.8</td><td>75.1</td><td>3.0</td></tr><tr><td>CLIP-ViT-B/32</td><td colspan=\"4\"></td></tr><tr><td>CLIP4Clip [29]</td><td>44.5</td><td>71.4</td><td>81.6</td><td>2.0</td></tr><tr><td>CenterCLIP [57]</td><td>44.2</td><td>71.6</td><td>82.1</td><td>2.0</td></tr><tr><td>XPool [12]</td><td>46.9</td><td>72.8</td><td>82.2</td><td>2.0</td></tr><tr><td>CLIP2Video [7]</td><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td></tr><tr><td>CLIP2Video\u2020[3]</td><td>47.2</td><td>73.0</td><td>83.0</td><td>2.0</td></tr><tr><td>CLIP2TV [10]</td><td>46.1</td><td>72.5</td><td>82.9</td><td>2.0</td></tr><tr><td>DRL [43]</td><td>47.4</td><td>74.6</td><td>83.8</td><td>2.0</td></tr><tr><td>CAMoE* [6]</td><td>47.3</td><td>74.2</td><td>84.5</td><td>2.0</td></tr><tr><td>Ours</td><td>50.1</td><td>74.8</td><td>84.6</td><td>1.0</td></tr><tr><td>Ours*</td><td>55.9</td><td>77.0</td><td>86.8</td><td>1.0</td></tr><tr><td>CLIP-ViT-B/16</td><td colspan=\"4\"></td></tr><tr><td>CenterCLIP [57]</td><td>48.4</td><td>73.8</td><td>82.0</td><td>2.0</td></tr><tr><td>CLIP2TV [10]</td><td>49.3</td><td>74.7</td><td>83.6</td><td>2.0</td></tr><tr><td>DRL [43]</td><td>50.2</td><td>76.5</td><td>84.7</td><td>1.0</td></tr><tr><td>DRL\u2020[43]</td><td>53.3</td><td>80.3</td><td>87.6</td><td>1.0</td></tr><tr><td>Ours</td><td>54.2</td><td>77.2</td><td>84.8</td><td>1.0</td></tr><tr><td>Ours*</td><td>57.7</td><td>80.5</td><td>88.2</td><td>1.0</td></tr></tbody></table>", "target_caption": "Table 5: Comparison of text-to-video retrieval in MSR-VTT [47]. * and \u2020 respectively denotes that the method uses DSL [6] and QB-Norm [3] as post-processing operations.", "source_title": "Cross modal retrieval with querybank normalisation", "source_table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>CE[73]</td><td>21.7_{\\pm 1.3}</td><td>51.8_{\\pm 0.5}</td><td>65.7_{\\pm 0.6}</td><td>5.0_{\\pm 0.0}</td></tr><tr><td>MMT[41]</td><td>24.6_{\\pm 0.4}</td><td>54.0_{\\pm 0.2}</td><td>67.1_{\\pm 0.5}</td><td>4.0_{\\pm 0.0}</td></tr><tr><td>SSB[89]</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td></tr><tr><td>Frozen[6]</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>CLIP4Clip [76]</td><td>44.5</td><td>71.4</td><td>81.6</td><td>\\mathbf{2.0}</td></tr><tr><td>TT-CE+[27]</td><td>29.6_{\\pm 0.3}</td><td>61.6_{\\pm 0.5}</td><td>74.2_{\\pm 0.3}</td><td>3.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>33.3_{\\pm 0.7}</td><td>63.7_{\\pm 0.1}</td><td>76.3_{\\pm 0.4}</td><td>3.0_{\\pm 0.0}</td></tr><tr><td>CLIP2Video[35]</td><td>45.6</td><td>72.5</td><td>81.7</td><td>\\mathbf{2.0}</td></tr><tr><td>CLIP2Video (+QB-Norm)</td><td>\\mathbf{47.2}</td><td>\\mathbf{73.0}</td><td>\\mathbf{83.0}</td><td>\\mathbf{2.0}</td></tr></table>", "source_caption": "Table 4: MSR-VTT 1k-A split: Comparison to state of the art. "}