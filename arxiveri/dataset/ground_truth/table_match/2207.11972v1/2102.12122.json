{"target_title": "TransCL: Transformer Makes Strong and Flexible Compressive Learning", "target_table": "<table><tr><td>Method</td><td> Meas.</td><td>Param./Flops</td><td>Top-1 ACC</td></tr><tr><td>ResNet-18[72]</td><td>3\\times50176</td><td>12M/1.9G</td><td>69.83</td></tr><tr><td>ResNet-50[72]</td><td>3\\times50176</td><td>25M/4.2G</td><td>76.20</td></tr><tr><td>ResNet-101[72]</td><td>3\\times50176</td><td>45M/7.8G</td><td>77.41</td></tr><tr><td>ResNet-152[72]</td><td>3\\times50176</td><td>60M/11.58G</td><td>78.33</td></tr><tr><td>ViT-B-16[49]</td><td>3\\times147456</td><td>86M/49.3G</td><td>83.97</td></tr><tr><td>ViT-B-32[49]</td><td>3\\times147456</td><td>88M/12.3G</td><td>81.28</td></tr><tr><td>PVT-Large[53]</td><td>3\\times50176</td><td>62M/9.9G</td><td>81.71</td></tr><tr><td>DeiT-B \\uparrow 384[50]</td><td>3\\times147456</td><td>86M/49.4G</td><td>83.12</td></tr><tr><td>Twins[73]</td><td>3\\times50176</td><td>99M/14.8G</td><td>83.70</td></tr><tr><td>VCL-T-10</td><td>3\\times14745</td><td>589M/50.8</td><td>67.59</td></tr><tr><td>VCL-T-1</td><td>3\\times1474</td><td>136M/49.6</td><td>59.89</td></tr><tr><td>MCL-T-10</td><td>3\\times14770</td><td>87M/49.4</td><td>74.07</td></tr><tr><td>MCL-T-1</td><td>3\\times1477</td><td>86M/49.3</td><td>68.92</td></tr><tr><td>TransCL-16,32-10</td><td>3\\times14745</td><td>86,88M/49.3,12.3G</td><td>83.86,81.82</td></tr><tr><td>TransCL-16,32-5</td><td>3\\times7372</td><td>86,88M/49.3,12.3G</td><td>83.05,81.53</td></tr><tr><td>TransCL-16,32-2.5</td><td>3\\times3686</td><td>86,88M/49.3,12.3G</td><td>81.34,80.50</td></tr><tr><td>TransCL-16,32-1</td><td>3\\times1474</td><td>86,88M/49.3,12.3G</td><td>78.86,78.00</td></tr><tr><td>TransCL-16-10<img/></td><td>3\\times14745</td><td>86M/49.3G</td><td>83.29</td></tr><tr><td>TransCL-16-5<img/></td><td>3\\times7372</td><td>86M/49.3G</td><td>82.46</td></tr><tr><td>TransCL-16-2.5<img/></td><td>3\\times3686</td><td>86M/49.3G</td><td>81.24</td></tr><tr><td>TransCL-16-1<img/></td><td>3\\times1474</td><td>86M/49.3G</td><td>78.58</td></tr></table>", "target_caption": "TABLE I: Image classification performance on validation dataset of ImageNet. \u201cTop-1 ACC\u201d denotes the top-1 (\\%) accuracy. Performances of different methods, model parameters, and the number of measurements for each image are reported.", "source_title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions", "source_table": "<table><tr><td>Method</td><td>#Param (M)</td><td>GFLOPs</td><td>Top-1 Err (%)</td></tr><tr><td>ResNet18* [22]</td><td>11.7</td><td>1.8</td><td>30.2</td></tr><tr><td>ResNet18 [22]</td><td>11.7</td><td>1.8</td><td>31.5</td></tr><tr><td>DeiT-Tiny/16 [63]</td><td>5.7</td><td>1.3</td><td>27.8</td></tr><tr><td>PVT-Tiny (ours)</td><td>13.2</td><td>1.9</td><td>24.9</td></tr><tr><td>ResNet50* [22]</td><td>25.6</td><td>4.1</td><td>23.9</td></tr><tr><td>ResNet50 [22]</td><td>25.6</td><td>4.1</td><td>21.5</td></tr><tr><td>ResNeXt50-32x4d* [73]</td><td>25.0</td><td>4.3</td><td>22.4</td></tr><tr><td>ResNeXt50-32x4d [73]</td><td>25.0</td><td>4.3</td><td>20.5</td></tr><tr><td>T2T-ViT{}_{t}-14 [75]</td><td>22.0</td><td>6.1</td><td>19.3</td></tr><tr><td>TNT-S [19]</td><td>23.8</td><td>5.2</td><td>18.7</td></tr><tr><td>DeiT-Small/16 [63]</td><td>22.1</td><td>4.6</td><td>20.1</td></tr><tr><td>PVT-Small (ours)</td><td>24.5</td><td>3.8</td><td>20.2</td></tr><tr><td>ResNet101* [22]</td><td>44.7</td><td>7.9</td><td>22.6</td></tr><tr><td>ResNet101 [22]</td><td>44.7</td><td>7.9</td><td>20.2</td></tr><tr><td>ResNeXt101-32x4d* [73]</td><td>44.2</td><td>8.0</td><td>21.2</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>44.2</td><td>8.0</td><td>19.4</td></tr><tr><td>T2T-ViT{}_{t}-19 [75]</td><td>39.0</td><td>9.8</td><td>18.6</td></tr><tr><td>ViT-Small/16 [13]</td><td>48.8</td><td>9.9</td><td>19.2</td></tr><tr><td>PVT-Medium (ours)</td><td>44.2</td><td>6.7</td><td>18.8</td></tr><tr><td>ResNeXt101-64x4d* [73]</td><td>83.5</td><td>15.6</td><td>20.4</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>83.5</td><td>15.6</td><td>18.5</td></tr><tr><td>ViT-Base/16 [13]</td><td>86.6</td><td>17.6</td><td>18.2</td></tr><tr><td>T2T-ViT{}_{t}-24 [75]</td><td>64.0</td><td>15.0</td><td>17.8</td></tr><tr><td>TNT-B [19]</td><td>66.0</td><td>14.1</td><td>17.2</td></tr><tr><td>DeiT-Base/16 [63]</td><td>86.6</td><td>17.6</td><td>18.2</td></tr><tr><td>PVT-Large (ours)</td><td>61.4</td><td>9.8</td><td>18.3</td></tr></table>", "source_caption": "Table 2: Image classification performance on the ImageNet validation set.\u201c#Param\u201d refers to the number of parameters.\u201cGFLOPs\u201d is calculated under the input scale of 224\\times 224. \u201c*\u201d indicates the performance of the method trained under the strategy of its original paper."}