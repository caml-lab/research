{
    "target_title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
    "target_table": "<table><tr><td rowspan=\"2\">Method</td><td>Pre-train</td><td colspan=\"6\">COCO (5K test set)</td><td colspan=\"6\">Flickr30K (1K test set)</td></tr><tr><td># Images</td><td colspan=\"3\">TR</td><td colspan=\"3\">IR</td><td colspan=\"3\">TR</td><td colspan=\"3\">IR</td></tr><tr><td></td><td></td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><td>UNITER (Chen et al., 2020)</td><td>4M</td><td>65.7</td><td>88.6</td><td>93.8</td><td>52.9</td><td>79.9</td><td>88.0</td><td>87.3</td><td>98.0</td><td>99.2</td><td>75.6</td><td>94.1</td><td>96.8</td></tr><tr><td>VILLA (Gan et al., 2020)</td><td>4M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>87.9</td><td>97.5</td><td>98.8</td><td>76.3</td><td>94.2</td><td>96.8</td></tr><tr><td>OSCAR (Li et al., 2020)</td><td>4M</td><td>70.0</td><td>91.1</td><td>95.5</td><td>54.0</td><td>80.8</td><td>88.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UNIMO (Li et al., 2021b)</td><td>5.7M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>89.4</td><td>98.9</td><td>99.8</td><td>78.0</td><td>94.2</td><td>97.1</td></tr><tr><td>ALIGN (Jia et al., 2021)</td><td>1.8B</td><td>77.0</td><td>93.5</td><td>96.9</td><td>59.9</td><td>83.3</td><td>89.8</td><td>95.3</td><td>99.8</td><td>100.0</td><td>84.9</td><td>97.4</td><td>98.6</td></tr><tr><td>ALBEF (Li et al., 2021a)</td><td>14M</td><td>77.6</td><td>94.3</td><td>97.2</td><td>60.7</td><td>84.3</td><td>90.5</td><td>95.9</td><td>99.8</td><td>100.0</td><td>85.6</td><td>97.5</td><td>98.9</td></tr><tr><td>BLIP</td><td>14M</td><td>80.6</td><td>95.2</td><td>97.6</td><td>63.1</td><td>85.3</td><td>91.1</td><td>96.6</td><td>99.8</td><td>100.0</td><td>87.2</td><td>97.5</td><td>98.8</td></tr><tr><td>BLIP</td><td>129M</td><td>81.9</td><td>95.4</td><td>97.8</td><td>64.3</td><td>85.7</td><td>91.5</td><td>97.3</td><td>99.9</td><td>100.0</td><td>87.3</td><td>97.6</td><td>98.9</td></tr><tr><td>BLIP{}_{\\text{CapFilt-L}}</td><td>129M</td><td>81.2</td><td>95.7</td><td>97.9</td><td>64.1</td><td>85.8</td><td>91.6</td><td>97.2</td><td>99.9</td><td>100.0</td><td>87.5</td><td>97.7</td><td>98.9</td></tr><tr><td>BLIP{}_{\\text{ViT-L}}</td><td>129M</td><td>82.4</td><td>95.4</td><td>97.9</td><td>65.1</td><td>86.3</td><td>91.8</td><td>97.4</td><td>99.8</td><td>99.9</td><td>87.6</td><td>97.7</td><td>99.0</td></tr></table>",
    "target_caption": "Table 5: Comparison with state-of-the-art image-text retrieval methods,finetuned on COCO and Flickr30K datasets.BLIP{}_{\\text{CapFilt-L}} pre-trains a model with ViT-B backbone using a dataset bootstrapped by captioner and filter with ViT-L.",
    "source_title": "Align before fuse: Vision and language representation learning with momentum distillation",
    "source_table": "",
    "source_caption": ""
}