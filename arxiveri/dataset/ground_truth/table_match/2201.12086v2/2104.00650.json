{"target_title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "target_table": "<table><tr><td>Method</td><td>R1\\uparrow</td><td>R5\\uparrow</td><td>R10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td colspan=\"3\">zero-shot</td><td></td><td></td></tr><tr><td>ActBERT (Zhu &amp; Yang, 2020)</td><td>8.6</td><td>23.4</td><td>33.1</td><td>36</td></tr><tr><td>SupportSet (Patrick et al., 2021)</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31</td></tr><tr><td>MIL-NCE (Miech et al., 2020)</td><td>9.9</td><td>24.0</td><td>32.4</td><td>29.5</td></tr><tr><td>VideoCLIP (Xu et al., 2021)</td><td>10.4</td><td>22.2</td><td>30.0</td><td>-</td></tr><tr><td>FiT (Bain et al., 2021)</td><td>18.7</td><td>39.5</td><td>51.6</td><td>10</td></tr><tr><td>BLIP</td><td>43.3</td><td>65.6</td><td>74.7</td><td>2</td></tr><tr><td colspan=\"3\">finetuning</td><td></td><td></td></tr><tr><td>ClipBERT (Lei et al., 2021)</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6</td></tr><tr><td>VideoCLIP (Xu et al., 2021)</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr></table>", "target_caption": "Table 10: Comparisons with state-of-the-art methods for text-to-video retrieval on the 1k test split of the MSRVTT dataset.", "source_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "source_table": "<table><tbody><tr><td>Method</td><td>E2E\\dagger</td><td>Vis Enc. Init.</td><td>Visual-Text PT</td><td>#pairs PT</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><td>JSFusion [75]</td><td>\\checkmark</td><td>-</td><td>-</td><td>-</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td></tr><tr><td>ActBERT [80]</td><td>\\checkmark</td><td>VisGenome</td><td>HowTo100M</td><td>136M</td><td>16.3</td><td>42.8</td><td>56.9</td><td>10.0</td></tr><tr><td>HERO [34]</td><td>\\checkmark</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><td>VidTranslate [28]</td><td>\\checkmark</td><td>IG65M</td><td>HowTo100M</td><td>136M</td><td>14.7</td><td>-</td><td>52.8</td><td></td></tr><tr><td>NoiseEst. [2]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8.0</td></tr><tr><td>\\rowcoloraliceblue CE [38]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td></tr><tr><td>UniVL [40]</td><td>\u2717</td><td>-</td><td>HowTo100M</td><td>136M</td><td>21.2</td><td>49.6</td><td>63.1</td><td>6.0</td></tr><tr><td>ClipBERT [32]</td><td>\u2713</td><td>-</td><td>COCO, VisGenome</td><td>5.6M</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>AVLnet [55]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue MMT [21]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>HowTo100M</td><td>136M</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue T2VLAD [69]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>29.5</td><td>59.0</td><td>70.1</td><td>4.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>-</td><td>-</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M</td><td>3M</td><td>25.5</td><td>54.5</td><td>66.1</td><td>4.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>32.5</td><td>61.5</td><td>71.2</td><td>3.0</td></tr><tr><td>Zero-shot</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>7.5</td><td>21.2</td><td>29.6</td><td>38.0</td></tr><tr><td>SupportSet [48]</td><td></td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>23.2</td><td>44.6</td><td>56.6</td><td>7.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>24.7</td><td>46.9</td><td>57.2</td><td>7.0</td></tr></tbody></table>", "source_caption": "Table 4: Comparison to state-of-the-art results on MSR-VTT for text-to-video retrieval, 1k-A split. \\daggerE2E: Works trained on pixels directly, without using pre-extracted expert features trained for other tasks. Vis Enc. Init.: Datasets used for pretraining visual encoders for tasks other than visual-text retrieval, eg object classification. Visual-Text PT: Visual-text pretraining data. Rows highlighted in blue use additional modalities such as sound and speech from the MSR-VTT test videos. \\dagger Object, Motion, Face, Scene, Speech, OCR and Sound classification features."}