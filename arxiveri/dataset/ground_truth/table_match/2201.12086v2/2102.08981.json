{"target_title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "target_table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">Pre-train#Images </td><td colspan=\"8\">NoCaps validation</td><td colspan=\"2\">COCO Caption</td></tr><tr><td colspan=\"2\">in-domain</td><td colspan=\"2\">near-domain</td><td colspan=\"2\">out-domain</td><td colspan=\"2\">overall</td><td colspan=\"2\">Karpathy test</td></tr><tr><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>B@4</td><td>C</td></tr><tr><td>Enc-Dec (Changpinyo et al., 2021)</td><td>15M</td><td>92.6</td><td>12.5</td><td>88.3</td><td>12.1</td><td>94.5</td><td>11.9</td><td>90.2</td><td>12.1</td><td>-</td><td>110.9</td></tr><tr><td>VinVL\u2020 (Zhang et al., 2021)</td><td>5.7M</td><td>103.1</td><td>14.2</td><td>96.1</td><td>13.8</td><td>88.3</td><td>12.1</td><td>95.5</td><td>13.5</td><td>38.2</td><td>129.3</td></tr><tr><td>LEMON{}_{\\mathrm{base}}\u2020 (Hu et al., 2021)</td><td>12M</td><td>104.5</td><td>14.6</td><td>100.7</td><td>14.0</td><td>96.7</td><td>12.4</td><td>100.4</td><td>13.8</td><td>-</td><td>-</td></tr><tr><td>LEMON{}_{\\mathrm{base}}\u2020 (Hu et al., 2021)</td><td>200M</td><td>107.7</td><td>14.7</td><td>106.2</td><td>14.3</td><td>107.9</td><td>13.1</td><td>106.8</td><td>14.1</td><td>40.3</td><td>133.3</td></tr><tr><td>BLIP</td><td>14M</td><td>111.3</td><td>15.1</td><td>104.5</td><td>14.4</td><td>102.4</td><td>13.7</td><td>105.1</td><td>14.4</td><td>38.6</td><td>129.7</td></tr><tr><td>BLIP</td><td>129M</td><td>109.1</td><td>14.8</td><td>105.8</td><td>14.4</td><td>105.7</td><td>13.7</td><td>106.3</td><td>14.3</td><td>39.4</td><td>131.4</td></tr><tr><td>BLIP{}_{\\text{CapFilt-L}}</td><td>129M</td><td>111.8</td><td>14.9</td><td>108.6</td><td>14.8</td><td>111.5</td><td>14.2</td><td>109.6</td><td>14.7</td><td>39.7</td><td>133.3</td></tr><tr><td>LEMON{}_{\\mathrm{large}}\u2020 (Hu et al., 2021)</td><td>200M</td><td>116.9</td><td>15.8</td><td>113.3</td><td>15.1</td><td>111.3</td><td>14.0</td><td>113.4</td><td>15.0</td><td>40.6</td><td>135.7</td></tr><tr><td>SimVLM{}_{\\mathrm{huge}} (Wang et al., 2021)</td><td>1.8B</td><td>113.7</td><td>-</td><td>110.9</td><td>-</td><td>115.2</td><td>-</td><td>112.2</td><td>-</td><td>40.6</td><td>143.3</td></tr><tr><td>BLIP{}_{\\text{ViT-L}}</td><td>129M</td><td>114.9</td><td>15.2</td><td>112.1</td><td>14.9</td><td>115.3</td><td>14.4</td><td>113.2</td><td>14.8</td><td>40.4</td><td>136.7</td></tr></table>", "target_caption": "Table 7: Comparison with state-of-the-art image captioning methods on NoCaps and COCO Caption.All methods optimize the cross-entropy loss during finetuning. C: CIDEr, S: SPICE, B@4: BLEU@4. BLIP{}_{\\text{CapFilt-L}} is pre-trained on a dataset bootstrapped by captioner and filter with ViT-L.VinVL\u2020 and LEMON\u2020 require an object detector pre-trained on 2.5M images with human-annotated bounding boxes and high resolution (800\\times1333) input images.SimVLM{}_{\\mathrm{huge}} uses 13\\times more training data and a larger vision backbone than ViT-L.", "source_title": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "source_table": "<table><tbody><tr><td></td><td colspan=\"8\">nocaps val</td></tr><tr><td>Method</td><td colspan=\"2\">in-domain</td><td colspan=\"2\">near-domain</td><td colspan=\"2\">out-of-domain</td><td colspan=\"2\">overall</td></tr><tr><td></td><td>CIDEr</td><td>SPICE</td><td>CIDEr</td><td>SPICE</td><td>CIDEr</td><td>SPICE</td><td>CIDEr</td><td>SPICE</td></tr><tr><td>UpDown [2]</td><td>78.1</td><td>11.6</td><td>57.7</td><td>10.3</td><td>31.3</td><td>8.3</td><td>55.3</td><td>10.1</td></tr><tr><td>UpDown + CBS [2]</td><td>80.0</td><td>12.0</td><td>73.6</td><td>11.3</td><td>66.4</td><td>9.7</td><td>73.1</td><td>11.1</td></tr><tr><td>UpDown + ELMo + CBS [2]</td><td>79.3</td><td>12.4</td><td>73.8</td><td>11.4</td><td>71.7</td><td>9.9</td><td>74.3</td><td>11.2</td></tr><tr><td>Oscar{}_{L} [50]</td><td>79.9</td><td>12.4</td><td>68.2</td><td>11.8</td><td>45.1</td><td>9.4</td><td>65.2</td><td>11.4</td></tr><tr><td>Oscar{}_{L} + CBS [50]</td><td>78.8</td><td>12.2</td><td>78.9</td><td>12.1</td><td>77.4</td><td>10.5</td><td>78.6</td><td>11.8</td></tr><tr><td>Oscar{}_{L} + SCST + CBS [50]</td><td>85.4</td><td>11.9</td><td>84.0</td><td>11.7</td><td>80.3</td><td>10.0</td><td>83.4</td><td>11.4</td></tr><tr><td>VIVO [32]</td><td>88.8</td><td>12.9</td><td>83.2</td><td>12.6</td><td>71.1</td><td>10.6</td><td>81.5</td><td>12.2</td></tr><tr><td>VIVO + CBS [32]</td><td>90.4</td><td>13.0</td><td>84.9</td><td>12.5</td><td>83.0</td><td>10.7</td><td>85.3</td><td>12.2</td></tr><tr><td>VIVO + SCST + CBS [32]</td><td>92.2</td><td>12.9</td><td>87.8</td><td>12.6</td><td>87.5</td><td>11.5</td><td>88.3</td><td>12.4</td></tr><tr><td>pretrain ic on CC12M</td><td>88.3</td><td>12.3</td><td>86.0</td><td>11.8</td><td>91.3</td><td>11.2</td><td>87.4</td><td>11.8</td></tr><tr><td>pretrain ic on CC3M+CC12M</td><td>92.6</td><td>12.5</td><td>88.3</td><td>12.1</td><td>94.5</td><td>11.9</td><td>90.2</td><td>12.1</td></tr><tr><td>Human</td><td>84.4</td><td>14.3</td><td>85.0</td><td>14.3</td><td>95.7</td><td>14.0</td><td>87.1</td><td>14.2</td></tr><tr><td></td><td colspan=\"8\">nocaps test</td></tr><tr><td>UpDown [2]</td><td>74.3</td><td>11.5</td><td>56.9</td><td>10.3</td><td>30.1</td><td>8.1</td><td>54.3</td><td>10.1</td></tr><tr><td>UpDown + ELMo + CBS [2]</td><td>76.0</td><td>11.8</td><td>74.2</td><td>11.5</td><td>66.7</td><td>9.7</td><td>73.1</td><td>11.2</td></tr><tr><td>VIVO + SCST + CBS [32]</td><td>89.0</td><td>12.9</td><td>87.8</td><td>12.6</td><td>80.1</td><td>11.1</td><td>86.6</td><td>12.4</td></tr><tr><td>pretrain ic on CC12M</td><td>82.9</td><td>11.9</td><td>85.7</td><td>12.0</td><td>85.3</td><td>11.3</td><td>85.3</td><td>11.8</td></tr><tr><td>pretrain ic on CC3M+CC12M</td><td>87.2</td><td>12.3</td><td>87.4</td><td>12.1</td><td>87.2</td><td>11.4</td><td>87.3</td><td>12.0</td></tr><tr><td>Human</td><td>80.6</td><td>15.0</td><td>84.6</td><td>14.7</td><td>91.6</td><td>14.2</td><td>85.3</td><td>14.7</td></tr></tbody></table>", "source_caption": "Table 4: Comparison between our best model (in italics, pre-trained on CC12M with ic and fine-tuned on COCO Captions) and existing models, on the nocaps val (top) and test (bottom) splits. Bold indicates best-to-date, underline indicates second-best."}