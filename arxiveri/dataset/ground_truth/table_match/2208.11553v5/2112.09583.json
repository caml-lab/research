{"target_title": "Improving video retrieval using multilingual knowledge transfer", "target_table": "<table><tbody><tr><td></td><td></td><td colspan=\"5\">Text-to-Video Retrieval</td><td colspan=\"5\">Video-to-Text Retrieval</td></tr><tr><td>Type</td><td>Model</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td></tr><tr><td rowspan=\"13\">Others</td><td>JsFusion (Yu, Kim, and Kim 2018)</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HT (Miech et al. 2019)</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HERO (Li et al. 2020)</td><td>20.5</td><td>46.8</td><td>60.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CE (Liu et al. 2019)</td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td><td>28.2</td><td>20.6</td><td>50.3</td><td>64.0</td><td>5.3</td><td>25.1</td></tr><tr><td>ClipBERT (Lei et al. 2021)</td><td>22.0</td><td>46.8</td><td>59.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SupportSET (Patrick et al. 2020)</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VideoCLIP (Xu et al. 2021)</td><td>30.9</td><td>55.4</td><td>66.8</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>FrozenInTime (Bain et al. 2021)</td><td>31</td><td>59.5</td><td>70.5</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CLIP (Radford et al. 2021)</td><td>31.2</td><td>53.7</td><td>2.6</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HIT (Liu et al. 2021)</td><td>30.7</td><td>60.9</td><td>73.2</td><td>2.6</td><td>-</td><td>32.1</td><td>62.7</td><td>74.1</td><td>3.0</td><td>-</td></tr><tr><td>AlignPrompt (Li et al. 2022)</td><td>33.9</td><td>60.7</td><td>73.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>All-in-one (Wang et al. 2022a)</td><td>34.4</td><td>65.4</td><td>75.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MDMMT (Dzabraev et al. 2021)</td><td>38.9</td><td>69.0</td><td>79.7</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td rowspan=\"7\">CLIP based</td><td>CLIP4Clip (Luo et al. 2021)</td><td>44.5</td><td>71.4</td><td>81.6</td><td>-</td><td>15.3</td><td>43.1</td><td>70.5</td><td>81.2</td><td>2.0</td><td>12.4</td></tr><tr><td>VCM (Cao et al. 2022)</td><td>43.8</td><td>71.0</td><td>80.9</td><td>2.0</td><td>14.3</td><td>45.1</td><td>72.3</td><td>82.3</td><td>2.0</td><td>10.7</td></tr><tr><td>MCQ (Ge et al. 2022a)</td><td>44.9</td><td>71.9</td><td>80.3</td><td>2.0</td><td>15.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MILES (Ge et al. 2022b)</td><td>44.3</td><td>71.1</td><td>80.8</td><td>2.0</td><td>14.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CAMoE (Cheng et al. 2021)</td><td>44.6</td><td>72.6</td><td>81.8</td><td>2.0</td><td>13.3</td><td>45.1</td><td>72.4</td><td>83.1</td><td>2.0</td><td>10.0</td></tr><tr><td>CLIP2Video (Fang et al. 2021)</td><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td><td>14.6</td><td>43.5</td><td>72.3</td><td>82.1</td><td>2.0</td><td>10.2</td></tr><tr><td>CLIP2TV (Gao et al. 2021)</td><td>46.1</td><td>72.5</td><td>82.9</td><td>2.0</td><td>15.2</td><td>43.9</td><td>70.9</td><td>82.2</td><td>2.0</td><td>12.0</td></tr><tr><td><p>Ours</p></td><td>MKTVR</td><td>46.6</td><td>72.6</td><td>82.2</td><td>2.0</td><td>13.9</td><td>45.5</td><td>73.4</td><td>84.7</td><td>2.0</td><td>8.07</td></tr></tbody></table>", "target_caption": "Table 1: Text-to-video and video-to-text retrieval results on MSR-VTT dataset 9k split. Recall at rank 1 (R@1)\\uparrow, rank 5 (R@5)\\uparrow, rank 10 (R@10)\\uparrow, Median Rank (MdR)\\downarrow and Mean Rank (MnR)\\downarrow are reported. Results of other methods taken from mentioned references. Our model surpasses previous state-of-the-art performance. In video-to-text retrieval, our model achieved 1.6 points boost in performance.", "source_title": "Align and Prompt: Video-and-Language Pre-training with Entity Prompts", "source_table": "<table><tr><td>Method</td><td>PT datasets</td><td>R1\\uparrow</td><td>R5\\uparrow</td><td>R10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td colspan=\"6\">Finetuning</td></tr><tr><td>JSFusion [55]</td><td>-</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13</td></tr><tr><td>HT100M [39]</td><td>HT (100M)</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9</td></tr><tr><td>ActBERT [57]</td><td>HT (100M)</td><td>16.3</td><td>42.8</td><td>56.9</td><td>10</td></tr><tr><td>NoiseEst. [1]</td><td>HT (100M)</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8</td></tr><tr><td>HERO [12]</td><td>HT (100M)</td><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><td>ClipBERT [26]</td><td> COCO +VG (5.6M) </td><td>22.0</td><td>46.8</td><td>59.9</td><td>6</td></tr><tr><td>AVLNet [25]</td><td>HT (100M)</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4</td></tr><tr><td>VideoClip [52]</td><td>HT (100M)</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><td>SupportSet [43]</td><td>HT (100M)</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3</td></tr><tr><td>FiT [3]</td><td> Web2M +CC3M (5.5M) </td><td>31.0</td><td>59.5</td><td>70.5</td><td>3</td></tr><tr><td>AlPro</td><td> Web2M +CC3M (5.5M) </td><td>33.9</td><td>60.7</td><td>73.2</td><td>3</td></tr><tr><td colspan=\"6\">Zero-shot</td></tr><tr><td>HT100M [39]</td><td>HT (100M)</td><td>7.5</td><td>21.2</td><td>29.6</td><td>38</td></tr><tr><td>ActBERT [57]</td><td>HT (100M)</td><td>8.6</td><td>23.4</td><td>33.1</td><td>36</td></tr><tr><td>SupportSet [43]</td><td>HT (100M)</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31</td></tr><tr><td>MIL-NCE [37]</td><td>HT (100M)</td><td>9.9</td><td>24.0</td><td>32.4</td><td>29.5</td></tr><tr><td>VideoCLIP [52]</td><td>HT (100M)</td><td>10.4</td><td>22.2</td><td>30.0</td><td>-</td></tr><tr><td>FiT [3]</td><td> Web2M +CC3M (5.5M) </td><td>18.7</td><td>39.5</td><td>51.6</td><td>10</td></tr><tr><td>AlPro</td><td> Web2M +CC3M (5.5M) </td><td>24.1</td><td>44.7</td><td>55.4</td><td>8</td></tr></table>", "source_caption": "Table 2: Comparisons with existing text-to-video retrieval methods with finetuning and zero-shot setups on MSRVTT. We follow the common partition with 7k training videos. Methods using 9k training videos are greyed out. Both partition protocols share the same 1k testing videos. R@k denotes recall (%) with k retrieval efforts; MdR denotes median ranking for retrieved videos. The pre-training datasets are HowTo100M (HT) [39], MS-COCO (COCO) [31], Visual Genome (VG) [22], WebVid2M (Web2M) [3] and Conceptual Captions (CC3M) [46]."}