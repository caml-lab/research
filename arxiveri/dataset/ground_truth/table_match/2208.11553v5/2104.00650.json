{"target_title": "Improving video retrieval using multilingual knowledge transfer", "target_table": "<table><tbody><tr><td></td><td></td><td colspan=\"5\">Text-to-Video Retrieval</td><td colspan=\"5\">Video-to-Text Retrieval</td></tr><tr><td>Type</td><td>Model</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td></tr><tr><td rowspan=\"13\">Others</td><td>JsFusion (Yu, Kim, and Kim 2018)</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HT (Miech et al. 2019)</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HERO (Li et al. 2020)</td><td>20.5</td><td>46.8</td><td>60.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CE (Liu et al. 2019)</td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td><td>28.2</td><td>20.6</td><td>50.3</td><td>64.0</td><td>5.3</td><td>25.1</td></tr><tr><td>ClipBERT (Lei et al. 2021)</td><td>22.0</td><td>46.8</td><td>59.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SupportSET (Patrick et al. 2020)</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VideoCLIP (Xu et al. 2021)</td><td>30.9</td><td>55.4</td><td>66.8</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>FrozenInTime (Bain et al. 2021)</td><td>31</td><td>59.5</td><td>70.5</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CLIP (Radford et al. 2021)</td><td>31.2</td><td>53.7</td><td>2.6</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HIT (Liu et al. 2021)</td><td>30.7</td><td>60.9</td><td>73.2</td><td>2.6</td><td>-</td><td>32.1</td><td>62.7</td><td>74.1</td><td>3.0</td><td>-</td></tr><tr><td>AlignPrompt (Li et al. 2022)</td><td>33.9</td><td>60.7</td><td>73.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>All-in-one (Wang et al. 2022a)</td><td>34.4</td><td>65.4</td><td>75.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MDMMT (Dzabraev et al. 2021)</td><td>38.9</td><td>69.0</td><td>79.7</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td rowspan=\"7\">CLIP based</td><td>CLIP4Clip (Luo et al. 2021)</td><td>44.5</td><td>71.4</td><td>81.6</td><td>-</td><td>15.3</td><td>43.1</td><td>70.5</td><td>81.2</td><td>2.0</td><td>12.4</td></tr><tr><td>VCM (Cao et al. 2022)</td><td>43.8</td><td>71.0</td><td>80.9</td><td>2.0</td><td>14.3</td><td>45.1</td><td>72.3</td><td>82.3</td><td>2.0</td><td>10.7</td></tr><tr><td>MCQ (Ge et al. 2022a)</td><td>44.9</td><td>71.9</td><td>80.3</td><td>2.0</td><td>15.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MILES (Ge et al. 2022b)</td><td>44.3</td><td>71.1</td><td>80.8</td><td>2.0</td><td>14.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CAMoE (Cheng et al. 2021)</td><td>44.6</td><td>72.6</td><td>81.8</td><td>2.0</td><td>13.3</td><td>45.1</td><td>72.4</td><td>83.1</td><td>2.0</td><td>10.0</td></tr><tr><td>CLIP2Video (Fang et al. 2021)</td><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td><td>14.6</td><td>43.5</td><td>72.3</td><td>82.1</td><td>2.0</td><td>10.2</td></tr><tr><td>CLIP2TV (Gao et al. 2021)</td><td>46.1</td><td>72.5</td><td>82.9</td><td>2.0</td><td>15.2</td><td>43.9</td><td>70.9</td><td>82.2</td><td>2.0</td><td>12.0</td></tr><tr><td><p>Ours</p></td><td>MKTVR</td><td>46.6</td><td>72.6</td><td>82.2</td><td>2.0</td><td>13.9</td><td>45.5</td><td>73.4</td><td>84.7</td><td>2.0</td><td>8.07</td></tr></tbody></table>", "target_caption": "Table 1: Text-to-video and video-to-text retrieval results on MSR-VTT dataset 9k split. Recall at rank 1 (R@1)\\uparrow, rank 5 (R@5)\\uparrow, rank 10 (R@10)\\uparrow, Median Rank (MdR)\\downarrow and Mean Rank (MnR)\\downarrow are reported. Results of other methods taken from mentioned references. Our model surpasses previous state-of-the-art performance. In video-to-text retrieval, our model achieved 1.6 points boost in performance.", "source_title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "source_table": "<table><tbody><tr><td>Method</td><td>E2E\\dagger</td><td>Vis Enc. Init.</td><td>Visual-Text PT</td><td>#pairs PT</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><td>JSFusion [75]</td><td>\\checkmark</td><td>-</td><td>-</td><td>-</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td></tr><tr><td>ActBERT [80]</td><td>\\checkmark</td><td>VisGenome</td><td>HowTo100M</td><td>136M</td><td>16.3</td><td>42.8</td><td>56.9</td><td>10.0</td></tr><tr><td>HERO [34]</td><td>\\checkmark</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><td>VidTranslate [28]</td><td>\\checkmark</td><td>IG65M</td><td>HowTo100M</td><td>136M</td><td>14.7</td><td>-</td><td>52.8</td><td></td></tr><tr><td>NoiseEst. [2]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8.0</td></tr><tr><td>\\rowcoloraliceblue CE [38]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td></tr><tr><td>UniVL [40]</td><td>\u2717</td><td>-</td><td>HowTo100M</td><td>136M</td><td>21.2</td><td>49.6</td><td>63.1</td><td>6.0</td></tr><tr><td>ClipBERT [32]</td><td>\u2713</td><td>-</td><td>COCO, VisGenome</td><td>5.6M</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>AVLnet [55]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue MMT [21]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>HowTo100M</td><td>136M</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue T2VLAD [69]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>29.5</td><td>59.0</td><td>70.1</td><td>4.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>-</td><td>-</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M</td><td>3M</td><td>25.5</td><td>54.5</td><td>66.1</td><td>4.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>32.5</td><td>61.5</td><td>71.2</td><td>3.0</td></tr><tr><td>Zero-shot</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>7.5</td><td>21.2</td><td>29.6</td><td>38.0</td></tr><tr><td>SupportSet [48]</td><td></td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>23.2</td><td>44.6</td><td>56.6</td><td>7.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>24.7</td><td>46.9</td><td>57.2</td><td>7.0</td></tr></tbody></table>", "source_caption": "Table 4: Comparison to state-of-the-art results on MSR-VTT for text-to-video retrieval, 1k-A split. \\daggerE2E: Works trained on pixels directly, without using pre-extracted expert features trained for other tasks. Vis Enc. Init.: Datasets used for pretraining visual encoders for tasks other than visual-text retrieval, eg object classification. Visual-Text PT: Visual-text pretraining data. Rows highlighted in blue use additional modalities such as sound and speech from the MSR-VTT test videos. \\dagger Object, Motion, Face, Scene, Speech, OCR and Sound classification features."}