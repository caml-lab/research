{
    "target_title": "K-LITE: Learning Transferable Visual Models with External Knowledge",
    "target_table": "<table><tbody><tr><td></td><td colspan=\"5\">Pre-training</td><td colspan=\"2\">Downstream</td></tr><tr><td>Task</td><td></td><td>#Instances</td><td>#Concepts</td><td>Vocab. Size</td><td>#Ins/#C.</td><td colspan=\"2\">Concept Overlap (%)</td></tr><tr><td rowspan=\"5\">IC</td><td>Dataset</td><td></td><td></td><td></td><td></td><td>ImageNet-1K</td><td>20-datasets</td></tr><tr><td>ImageNet-21K deng2009imagenet </td><td>13M</td><td>19.2K / 18.4K</td><td>13.5K / 12.9K</td><td>591 \\pm 537</td><td>11.82</td><td>13.26</td></tr><tr><td>GCC-3M sharma2018conceptual </td><td>3.3M</td><td>681K / 64.5K</td><td>29.6K / 13.0K</td><td>9.5 \\pm303</td><td>35.97</td><td>19.73</td></tr><tr><td>GCC-12M changpinyo2021conceptual </td><td>12M</td><td>10.2M / 728K</td><td>1.24M / 264K</td><td>5.6 \\pm 353</td><td>61.02</td><td>31.34</td></tr><tr><td>YFCC-14M thomee2016yfcc100m </td><td>14M</td><td>14.2M / 1.25M</td><td>2.41M / 473K</td><td>8.3 \\pm 1354</td><td>65.23</td><td>34.65</td></tr><tr><td rowspan=\"2\">OD</td><td>Dataset</td><td></td><td></td><td></td><td></td><td>LVIS</td><td>13-datasets</td></tr><tr><td>Object-365 shao2019objects365 </td><td>9.6M</td><td>365 / 365</td><td>452 / 452</td><td>26.3K \\pm 12.4K</td><td>13.46</td><td>21.26</td></tr></tbody></table>",
    "target_caption": "Table 1: Statistics of training and test datasets used in our experiments. #Instances indicates #Image for IC and #Regions for OD, respectively. For #Concept and Vocabulary size, we report numbers for the full set and for items with frequency larger than 5. #Ins/C. reports the mean and standard derivation for the numbers of instances per concept.",
    "source_title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
    "source_table": "",
    "source_caption": ""
}