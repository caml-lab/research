{"target_title": "Refine and Represent: Region-to-Object Representation Learning", "target_table": "<table><tbody><tr><td></td><td colspan=\"2\">COCO 1\\times</td><td colspan=\"2\">COCO 2\\times</td></tr><tr><td>Method</td><td>AP<sup>bb</sup></td><td>AP<sup>mk</sup></td><td>AP<sup>bb</sup></td><td>AP<sup>mk</sup></td></tr><tr><td>Supervised</td><td>38.9</td><td>35.4</td><td>40.6</td><td>36.8</td></tr><tr><td>MoCo v2 [10]</td><td>38.9</td><td>35.4</td><td>40.9</td><td>37.0</td></tr><tr><td>BYOL<sup>\u2020</sup> [23]</td><td>40.6</td><td>37.5</td><td>42.0</td><td>38.7</td></tr><tr><td>DenseCL [56]</td><td>40.3</td><td>36.4</td><td>41.2</td><td>37.3</td></tr><tr><td>ReSim [60]</td><td>39.3</td><td>35.7</td><td>41.1</td><td>37.1</td></tr><tr><td>PixPro [63]</td><td>41.4</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DetCon<sub>B</sub><sup>\u2020</sup> [28]</td><td>41.5</td><td>38.0</td><td>42.1</td><td>38.9</td></tr><tr><td>DetCo [61]</td><td>39.4</td><td>34.4</td><td>41.4</td><td>35.8</td></tr><tr><td>LEWEL [31]</td><td>41.3</td><td>37.4</td><td>42.2</td><td>38.2</td></tr><tr><td>R2O</td><td>41.7</td><td>38.3</td><td>42.3</td><td>39.0</td></tr></tbody></table>", "target_caption": "Table 1: Performance on COCO object detection and instance segmentation following ImageNet pretraining. All methods pretrained a ResNet-50 which later served as the backbone of a Mask R-CNN R50-FPN finetuned on train2017 for 12 epochs (1\\times schedule) or 24 epochs (2\\times schedule). We report Average Precision on bounding box (AP<sup>bb</sup>) and mask (AP<sup>mk</sup>) predictions for val2017. <sup>\u2020</sup>: Results from re-implementation.", "source_title": "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning", "source_table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\">#. Epoch</td><td colspan=\"3\">Pascal VOC (R50-C4)</td><td colspan=\"3\">COCO (R50-FPN)</td><td colspan=\"3\">COCO (R50-C4)</td><td>Cityscapes (R50)</td></tr><tr><td>AP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mAP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mAP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mIoU</td></tr><tr><td>scratch</td><td>-</td><td>33.8</td><td>60.2</td><td>33.1</td><td>32.8</td><td>51.0</td><td>35.3</td><td>26.4</td><td>44.0</td><td>27.8</td><td>65.3</td></tr><tr><td>supervised</td><td>100</td><td>53.5</td><td>81.3</td><td>58.8</td><td>39.7</td><td>59.5</td><td>43.3</td><td>38.2</td><td>58.2</td><td>41.2</td><td>74.6</td></tr><tr><td>MoCo [19]</td><td>200</td><td>55.9</td><td>81.5</td><td>62.6</td><td>39.4</td><td>59.1</td><td>43.0</td><td>38.5</td><td>58.3</td><td>41.6</td><td>75.3</td></tr><tr><td>SimCLR [9]</td><td>1000</td><td>56.3</td><td>81.9</td><td>62.5</td><td>39.8</td><td>59.5</td><td>43.6</td><td>38.4</td><td>58.3</td><td>41.6</td><td>75.8</td></tr><tr><td>MoCo v2 [10]</td><td>800</td><td>57.6</td><td>82.7</td><td>64.4</td><td>40.4</td><td>60.1</td><td>44.3</td><td>39.5</td><td>59.0</td><td>42.6</td><td>76.2</td></tr><tr><td>InfoMin [35]</td><td>200</td><td>57.6</td><td>82.7</td><td>64.6</td><td>40.6</td><td>60.6</td><td>44.6</td><td>39.0</td><td>58.5</td><td>42.0</td><td>75.6</td></tr><tr><td>InfoMin [35]</td><td>800</td><td>57.5</td><td>82.5</td><td>64.0</td><td>40.4</td><td>60.4</td><td>44.3</td><td>38.8</td><td>58.2</td><td>41.7</td><td>75.6</td></tr><tr><td>PixPro (ours)</td><td>100</td><td>58.8</td><td>83.0</td><td>66.5</td><td>41.3</td><td>61.3</td><td>45.4</td><td>40.0</td><td>59.3</td><td>43.4</td><td>76.8</td></tr><tr><td>PixPro (ours)</td><td>400</td><td>60.2</td><td>83.8</td><td>67.7</td><td>41.4</td><td>61.6</td><td>45.4</td><td>40.5</td><td>59.8</td><td>44.0</td><td>77.2</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "source_caption": "Table 1: Comparing the proposed pixel-level pre-training method, PixPro, to previous supervised/unsupervised pre-training methods. For Pascal VOC object detection, a Faster R-CNN (R50-C4) detector is adopted for all methods. For COCO object detection, a Mask R-CNN detector (R50-FPN and R50-C4) with 1\\times setting is adopted for all methods. For Cityscapes semantic segmentation, an FCN method (R50) is used. Only a pixel-level pretext task is involved in PixPro pre-training. For Pascal VOC (R50-C4), COCO (R50-C4) and Cityscapes (R50), a regular backbone network of R50 with output feature map of C5 is adopted for PixPro pre-training. For COCO (R50-FPN), an FPN network with P_{3}-P_{6} feature maps is used. Note that InfoMin [35] reports results for only its 200 epoch model, so we reproduce it with longer training lengths, where saturation is observed."}