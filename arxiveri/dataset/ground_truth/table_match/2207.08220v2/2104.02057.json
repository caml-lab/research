{"target_title": "Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches", "target_table": "<table><tbody><tr><td><p>Method</p></td><td><p>100 ep.</p></td><td><p>200 ep.</p></td><td><p>400 ep.</p></td><td><p>800 ep.</p></td><td><p>1000 ep.</p></td></tr><tr><td><p>\\hlineB2.5SimCLR [6]</p></td><td><p>64.8</p></td><td><p>67.0</p></td><td><p>68.3</p></td><td><p>69.1</p></td><td><p>-</p></td></tr><tr><td><p>MoCo v2 [7]</p></td><td><p>-</p></td><td><p>67.5</p></td><td><p>-</p></td><td><p>71.1</p></td><td><p>-</p></td></tr><tr><td><p>BYOL [15]</p></td><td><p>66.5</p></td><td><p>70.6</p></td><td><p>73.2</p></td><td><p>-</p></td><td><p>74.3</p></td></tr><tr><td><p>SwAV [3]</p></td><td><p>-</p></td><td><p>-</p></td><td><p>70.1</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>BarlowTwins [33]</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>73.2</p></td></tr><tr><td><p>SimSiam [8]</p></td><td><p>68.1</p></td><td><p>70.0</p></td><td><p>70.8</p></td><td><p>71.3</p></td><td><p>-</p></td></tr><tr><td><p>MoCo v3 [9]</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>73.8</p></td><td><p>-</p></td></tr><tr><td><p>NNCLR [11]</p></td><td><p>69.4</p></td><td><p>70.7</p></td><td><p>74.2</p></td><td><p>74.9</p></td><td><p>75.4</p></td></tr><tr><td><p>OBoW [13]</p></td><td><p>-</p></td><td><p>73.8</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>Fast-MoCo</td><td>73.5</td><td>75.1</td><td>75.5</td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>SwAV [3] (w/ multi-crop)</p></td><td><p>72.1</p></td><td><p>73.9</p></td><td><p>-</p></td><td><p>75.3</p></td><td><p>-</p></td></tr><tr><td><p>DINO [4] (w/ multi-crop)</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>75.3</p></td><td><p>-</p></td></tr><tr><td><p>NNCLR [11] (w/ multi-crop)</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>75.6</td><td><p>-</p></td></tr></tbody></table>", "target_caption": "Table 1: ImageNet-1k linear evaluation results for existing methods and our Fast-MoCo using ResNet-50. Best results are in bold. Fast-MoCo can achieve similar performance as MoCo v3 with only 100 epochs. When trained for 200 epochs, Fast-MoCo performances better than MoCo v3 trained for 800 epochs and is comparable with state-of-the-arts (multi-crop is not used in Fast-MoCo for a fair comparison).", "source_title": "An empirical study of training self-supervised vision transformers", "source_table": "<table><thead><tr><td>model</td><td><p>MoCo v3</p></td><td><p>SimCLR</p></td><td><p>BYOL</p></td><td><p>SwAV</p></td></tr></thead><tbody><tr><td>R-50, 800-ep</td><td>73.8</td><td>70.4</td><td>74.3</td><td>71.8</td></tr><tr><td>ViT-S, 300-ep</td><td>72.5</td><td><p>69.0</p></td><td><p>71.0</p></td><td><p>67.1</p></td></tr><tr><td>ViT-B, 300-ep</td><td>76.5</td><td><p>73.9</p></td><td><p>73.9</p></td><td><p>71.6</p></td></tr></tbody></table>", "source_caption": "Table 4: ViT-S/16 and ViT-B/16 in different self-supervised learning frameworks (ImageNet, linear probing).R-50 results of other frameworks are from the improved implementation in [13]. For fair comparisons, all are pre-trained with two 224\\times224 crops for each image (multi-crop training [7] could improve results, which is beyond the focus of this work)."}