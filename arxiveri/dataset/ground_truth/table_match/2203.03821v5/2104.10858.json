{"target_title": "CF-ViT: A General Coarse-to-Fine Method for Vision Transformer", "target_table": "<table><tbody><tr><td>Model</td><td>Top-1 Acc.(%)</td><td>FLOPs(G)</td></tr><tr><td colspan=\"3\">DeiT-S</td></tr><tr><td>Baseline (Touvron et al. 2021a)</td><td>79.8</td><td>4.6</td></tr><tr><td>DynamicViT (Rao et al. 2021)</td><td>79.3</td><td>2.9</td></tr><tr><td>IA-RED{}^{2} (Pan et al. 2021a)</td><td>79.1</td><td>3.2</td></tr><tr><td>PS-ViT (Tang et al. 2021)</td><td>79.4</td><td>2.6</td></tr><tr><td>EVIT (Liang et al. 2022)</td><td>79.5</td><td>3.0</td></tr><tr><td>Evo-ViT (Xu et al. 2022)</td><td>79.4</td><td>3.0</td></tr><tr><td>CF-ViT(\\eta=0.5)(Ours)</td><td>79.8</td><td>1.8</td></tr><tr><td>CF-ViT(\\eta=0.75)(Ours)</td><td>80.7</td><td>2.6</td></tr><tr><td colspan=\"3\">LV-ViT-S</td></tr><tr><td>Baseline (Jiang et al. 2021)</td><td>83.3</td><td>6.6</td></tr><tr><td>DynamicViT (Rao et al. 2021)</td><td>83.0</td><td>4.6</td></tr><tr><td>EVIT (Liang et al. 2022)</td><td>83.0</td><td>4.7</td></tr><tr><td>SiT (Zong et al. 2021)</td><td>83.2</td><td>4.0</td></tr><tr><td>CF-ViT(\\eta=0.63)(Ours)</td><td>83.3</td><td>3.1</td></tr><tr><td>CF-ViT(\\eta=0.75)(Ours)</td><td>83.5</td><td>4.0</td></tr></tbody></table>", "target_caption": "Table 3:  Comparisons between existing token slimming based ViT compression methods and our CF-ViT. ", "source_title": "All tokens matter: Token labeling for training better vision transformers", "source_table": "<table><tbody><tr><td></td><td>Network</td><td>Params</td><td>FLOPs</td><td>Train size</td><td>Test size</td><td>Top-1(%)</td><td>Real Top-1 (%)</td></tr><tr><td rowspan=\"6\"> CNNs</td><td>EfficientNet-B5 [34]</td><td>030M</td><td>009.9B</td><td>456</td><td>456</td><td>83.6</td><td>88.3</td></tr><tr><td>EfficientNet-B7 [34]</td><td>066M</td><td>037.0B</td><td>600</td><td>600</td><td>84.3</td><td>_</td></tr><tr><td>Fix-EfficientNet-B8 [34, 38]</td><td>087M</td><td>089.5B</td><td>672</td><td>800</td><td>85.7</td><td>90.0</td></tr><tr><td>NFNet-F3 [3]</td><td>255M</td><td>114.8B</td><td>320</td><td>416</td><td>85.7</td><td>89.4</td></tr><tr><td>NFNet-F4 [3]</td><td>316M</td><td>215.3B</td><td>384</td><td>512</td><td>85.9</td><td>89.4</td></tr><tr><td>NFNet-F5 [3]</td><td>377M</td><td>289.8B</td><td>416</td><td>544</td><td>86.0</td><td>89.2</td></tr><tr><td rowspan=\"16\"> Transformers</td><td>ViT-B/16 [16]</td><td>086M</td><td>055.4B</td><td>224</td><td>384</td><td>77.9</td><td>83.6</td></tr><tr><td>ViT-L/16 [16]</td><td>307M</td><td>190.7B</td><td>224</td><td>384</td><td>76.5</td><td>82.2</td></tr><tr><td>T2T-ViT-14 [46]</td><td>022M</td><td>005.2B</td><td>224</td><td>224</td><td>81.5</td><td>_</td></tr><tr><td>T2T-ViT-14\\uparrow384 [46]</td><td>022M</td><td>017.1B</td><td>224</td><td>384</td><td>83.3</td><td>_</td></tr><tr><td>CrossViT [7]</td><td>045M</td><td>056.6B</td><td>224</td><td>480</td><td>84.1</td><td>_</td></tr><tr><td>Swin-B [26]</td><td>088M</td><td>047.0B</td><td>224</td><td>384</td><td>84.2</td><td>_</td></tr><tr><td>TNT-B [17]</td><td>066M</td><td>014.1B</td><td>224</td><td>224</td><td>82.8</td><td>_</td></tr><tr><td>DeepViT-S [59]</td><td>027M</td><td>006.2B</td><td>224</td><td>224</td><td>82.3</td><td>_</td></tr><tr><td>DeepViT-L [59]</td><td>055M</td><td>012.5B</td><td>224</td><td>224</td><td>83.1</td><td>_</td></tr><tr><td>DeiT-S [36]</td><td>022M</td><td>004.6B</td><td>224</td><td>224</td><td>79.9</td><td>85.7</td></tr><tr><td>DeiT-B [36]</td><td>086M</td><td>017.5B</td><td>224</td><td>224</td><td>81.8</td><td>86.7</td></tr><tr><td>DeiT-B\\uparrow384 [36]</td><td>086M</td><td>055.4B</td><td>224</td><td>384</td><td>83.1</td><td>87.7</td></tr><tr><td>BoTNet-S1-128 [31]</td><td>79.1M</td><td>019.3B</td><td>256</td><td>256</td><td>84.2</td><td>-</td></tr><tr><td>BoTNet-S1-128\\uparrow384 [31]</td><td>79.1M</td><td>045.8B</td><td>256</td><td>384</td><td>84.7</td><td>-</td></tr><tr><td>CaiT-S36\\uparrow384 [37]</td><td>068M</td><td>048.0B</td><td>224</td><td>384</td><td>85.4</td><td>89.8</td></tr><tr><td>CaiT-M36 [37]</td><td>271M</td><td>053.7B</td><td>224</td><td>224</td><td>85.1</td><td>89.3</td></tr><tr><td></td><td>CaiT-M36\\uparrow448 [37]</td><td>271M</td><td>247.8B</td><td>224</td><td>448</td><td>86.3</td><td>90.2</td></tr><tr><td rowspan=\"7\"> Ours LV-ViT</td><td>LV-ViT-S</td><td>026M</td><td>006.6B</td><td>224</td><td>224</td><td>83.3</td><td>88.1</td></tr><tr><td>LV-ViT-S\\uparrow384</td><td>026M</td><td>022.2B</td><td>224</td><td>384</td><td>84.4</td><td>88.9</td></tr><tr><td>LV-ViT-M</td><td>056M</td><td>016.0B</td><td>224</td><td>224</td><td>84.1</td><td>88.4</td></tr><tr><td>LV-ViT-M\\uparrow384</td><td>056M</td><td>042.2B</td><td>224</td><td>384</td><td>85.4</td><td>89.5</td></tr><tr><td>LV-ViT-L</td><td>150M</td><td>059.0B</td><td>288</td><td>288</td><td>85.3</td><td>89.3</td></tr><tr><td>LV-ViT-L\\uparrow448</td><td>150M</td><td>157.2B</td><td>288</td><td>448</td><td>85.9</td><td>89.7</td></tr><tr><td>LV-ViT-L\\uparrow448</td><td>150M</td><td>157.2B</td><td>448</td><td>448</td><td>86.2</td><td>89.9</td></tr><tr><td></td><td>LV-ViT-L\\uparrow512</td><td>151M</td><td>214.8B</td><td>448</td><td>512</td><td>86.4</td><td>90.1</td></tr></tbody></table>", "source_caption": "Table 4: Top-1 accuracy comparison with other methods on ImageNet [14]and ImageNet Real [2]. All models are trained without external data.With the same computation and parameter constraint, our model consistently outperformsother CNN-based and transformer-based counterparts. The results of CNNs and ViT are referenced from [37]."}