{"target_title": "Multimodal Knowledge Alignment with Reinforcement Learning", "target_table": "<table><tbody><tr><td>Model</td><td>Style</td><td>B@4</td><td>M</td><td>C</td><td>Time (sec/image)</td></tr><tr><td>Pseudo-Align Laina et al. (2019)</td><td>\\checkmark</td><td>5.2</td><td>15.5</td><td>29.4</td><td>-</td></tr><tr><td>RSA Honda et al. (2021)</td><td>\\checkmark</td><td>7.6</td><td>13.5</td><td>31.8</td><td>-</td></tr><tr><td>Unpaired Laina et al. (2019)</td><td>\\checkmark</td><td>19.3</td><td>20.1</td><td>63.6</td><td>-</td></tr><tr><td>CLIP-Infer Tewel et al. (2021)</td><td></td><td>2.6</td><td>11.5</td><td>14.6</td><td>65s</td></tr><tr><td>CLIP-Infer-Style</td><td>\\checkmark</td><td>7.0</td><td>15.4</td><td>34.5</td><td>65s</td></tr><tr><td>CLIP-Retrieval</td><td>\\checkmark</td><td>4.8</td><td>11.2</td><td>13.4</td><td>0.37s</td></tr><tr><td><img/>\u2009ESPER-Free (GPT-2)</td><td></td><td>6.3</td><td>13.3</td><td>29.1</td><td>0.65s</td></tr><tr><td><img/>\u2009ESPER-Style (GPT-2)</td><td>\\checkmark</td><td>21.9</td><td>21.9</td><td>78.2</td><td>0.65s</td></tr></tbody></table>", "target_caption": "Table 1: Unpaired captioning experiments in COCO test split. B@4 denotes Bleu-4, M METEOR and C CIDEr score. Running time entails the whole time for each process needed to infer caption for an image, including image loading and feature extraction. We use greedy decoding for all results in this table.", "source_title": "Zero-shot image-to-text generation for visual-semantic arithmetic", "source_table": "<table><tbody><tr><td></td><td colspan=\"5\">Supervised Metrics</td><td colspan=\"2\">Diversity Metrics</td><td>Unsupervised Metric</td></tr><tr><td>Method</td><td>B@4</td><td>M</td><td>C</td><td>S</td><td>\\text{CLIP-S}^{\\text{Ref}}</td><td>Vocab</td><td>%Novel</td><td>CLIP-S</td></tr><tr><td>ClipCap [51]</td><td>32.15</td><td>27.1</td><td>108.35</td><td>20.12</td><td>0.81</td><td>1650</td><td>66.4%</td><td>0.77</td></tr><tr><td>CLIP-VL [64]</td><td>40.2</td><td>29.7</td><td>134.2</td><td>23.8</td><td>0.82</td><td>2464</td><td>85.1%</td><td>0.77</td></tr><tr><td>VinVL [78]</td><td>41.0</td><td>31.1</td><td>140.9</td><td>25.2</td><td>0.83</td><td>1125</td><td>77.9%</td><td>0.78</td></tr><tr><td>Ours</td><td>2.6</td><td>11.5</td><td>14.6</td><td>5.5</td><td>0.79</td><td>8681</td><td>100%</td><td>0.87</td></tr></tbody></table>", "source_caption": "Table 1: For each method, we report supervised metrics (i.e., ones requiring human references): B@1 = BLEU-1, M = METEOR, C = CIDEr, S = SPICE. We also report diversity metrics, which measures the vocabulary size (Vocab), and the number of novel sentences w.r.t the training set (%Novel). Finally, we report semantic relatedness to the image (CLIP-S), and to the human references (\\text{CLIP-S}^{\\text{Ref}}) based on CLIP\u2019s embeddings."}