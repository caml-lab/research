{"target_title": "LightViT: Towards Light-Weight Convolution-Free Vision Transformers", "target_table": "<table><tbody><tr><td>Model</td><td> Blocktype </td><td> Params(M) </td><td> FLOPs(G) </td><td> Throughput(image/s) </td><td> Top-1(%) </td></tr><tr><td>RegNetY-800M [22]</td><td>CNN</td><td>6.3</td><td>0.8</td><td>3321</td><td>76.3</td></tr><tr><td>PVTv2-B0 [31]</td><td>Hybrid</td><td>3.4</td><td>0.6</td><td>2324</td><td>70.5</td></tr><tr><td>SimViT-Micro [15]</td><td>Hybrid</td><td>3.3</td><td>0.7</td><td>1004</td><td>71.1</td></tr><tr><td>MobileViT-XS [21]</td><td>Hybrid</td><td>2.3</td><td>0.7</td><td>1581</td><td>74.8</td></tr><tr><td>LVT [36]</td><td>Hybrid</td><td>5.5</td><td>0.9</td><td>1545</td><td>74.8</td></tr><tr><td>LightViT-T</td><td>Transformer</td><td>9.4</td><td>0.7</td><td>2578</td><td>78.7</td></tr><tr><td>RegNetY-1.6G [22]</td><td>CNN</td><td>11.2</td><td>1.6</td><td>1845</td><td>78.0</td></tr><tr><td>MobileViT-S [21]</td><td>Hybrid</td><td>5.6</td><td>1.1</td><td>1219</td><td>78.4</td></tr><tr><td>PVTv2-B1 [31]</td><td>Hybrid</td><td>13.1</td><td>2.1</td><td>1231</td><td>78.7</td></tr><tr><td>ResT-Small [41]</td><td>Hybrid</td><td>13.7</td><td>1.9</td><td>1298</td><td>79.6</td></tr><tr><td>DeiT-Ti [29]</td><td>Transformer</td><td>5.7</td><td>1.3</td><td>2612</td><td>72.2</td></tr><tr><td>LightViT-S</td><td>Transformer</td><td>19.2</td><td>1.7</td><td>1467</td><td>80.8</td></tr><tr><td>RegNetY-4G{}^{\\dagger} [22]</td><td>CNN</td><td>21.0</td><td>4.0</td><td>1045</td><td>80.0</td></tr><tr><td>Twins-PCPVT-S [3]</td><td>Hybrid</td><td>24.1</td><td>3.8</td><td>807</td><td>81.2</td></tr><tr><td>ResT-Base [41]</td><td>Hybrid</td><td>30.3</td><td>4.3</td><td>735</td><td>81.6</td></tr><tr><td>PVTv2-B2 [31]</td><td>Hybrid</td><td>25.4</td><td>4.0</td><td>695</td><td>82.0</td></tr><tr><td>DeiT-S [29]</td><td>Transformer</td><td>22</td><td>4.6</td><td>961</td><td>79.8</td></tr><tr><td>Swin-T [20]</td><td>Transformer</td><td>29</td><td>4.9</td><td>765</td><td>81.3</td></tr><tr><td>LightViT-B</td><td>Transformer</td><td>35.2</td><td>3.9</td><td>827</td><td>82.1</td></tr></tbody></table>", "target_caption": "Table 3: Image classification performance on ImageNet validation dataset. Throughput is measured on a single V100 GPU following [29, 20]. Hybrid denotes using both attention and convolution in blocks. All models are trained and evaluated on 224\\times 224 resolution. \\dagger: accuracy reported by DeiT [29].", "source_title": "Rest: An efficient transformer for visual recognition", "source_table": "<table><tbody><tr><td>Model</td><td>#Params (M)</td><td>FLOPs (G)</td><td>Throughput</td><td>Top-1 (%)</td><td>Top-5 (%)</td></tr><tr><td colspan=\"6\">ConvNet</td></tr><tr><td>ResNet-18 DBLP:conf/cvpr/HeZRS16 </td><td>11.7</td><td>1.8</td><td>1852</td><td>69.7</td><td>89.1</td></tr><tr><td>ResNet-50 DBLP:conf/cvpr/HeZRS16 </td><td>25.6</td><td>4.1</td><td>871</td><td>79.0</td><td>94.4</td></tr><tr><td>ResNet-101 DBLP:conf/cvpr/HeZRS16 </td><td>44.7</td><td>7.9</td><td>635</td><td>80.3</td><td>95.2</td></tr><tr><td>RegNetY-4G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>20.6</td><td>4.0</td><td>1156</td><td>79.4</td><td>94.7</td></tr><tr><td>RegNetY-8G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>39.2</td><td>8.0</td><td>591</td><td>79.9</td><td>94.9</td></tr><tr><td>RegNetY-16G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>83.6</td><td>15.9</td><td>334</td><td>80.4</td><td>95.1</td></tr><tr><td colspan=\"6\">Transformer</td></tr><tr><td>DeiT-S DBLP:journals/corr/abs-2012-12877 </td><td>22.1</td><td>4.6</td><td>940</td><td>79.8</td><td>94.9</td></tr><tr><td>DeiT-B DBLP:journals/corr/abs-2012-12877 </td><td>86.6</td><td>17.6</td><td>292</td><td>81.8</td><td>95.6</td></tr><tr><td>PVT-T wang2021pyramid </td><td>13.2</td><td>1.9</td><td>1038</td><td>75.1</td><td>92.4</td></tr><tr><td>PVT-S wang2021pyramid </td><td>24.5</td><td>3.7</td><td>820</td><td>79.8</td><td>94.9</td></tr><tr><td>PVT-M wang2021pyramid </td><td>44.2</td><td>6.4</td><td>526</td><td>81.2</td><td>95.6</td></tr><tr><td>PVT-L wang2021pyramid </td><td>61.4</td><td>9.5</td><td>367</td><td>81.7</td><td>95.9</td></tr><tr><td>Swin-T DBLP:journals/corr/abs-2103-14030 </td><td>28.29</td><td>4.5</td><td>755</td><td>81.3</td><td>95.5</td></tr><tr><td>Swin-S DBLP:journals/corr/abs-2103-14030 </td><td>49.61</td><td>8.7</td><td>437</td><td>83.3</td><td>96.2</td></tr><tr><td>Swin-B DBLP:journals/corr/abs-2103-14030 </td><td>87.77</td><td>15.4</td><td>278</td><td>83.5</td><td>96.5</td></tr><tr><td>ResT-Lite (Ours)</td><td>10.49</td><td>1.4</td><td>1246</td><td>77.2 (\\uparrow 7.5)</td><td>93.7 (\\uparrow 4.6)</td></tr><tr><td>ResT-Small (Ours)</td><td>13.66</td><td>1.9</td><td>1043</td><td>79.6 (\\uparrow 9.9)</td><td>94.9 (\\uparrow 5.8)</td></tr><tr><td>ResT-Base (Ours)</td><td>30.28</td><td>4.3</td><td>673</td><td>81.6 (\\uparrow 2.6)</td><td>95.7 (\\uparrow 1.3)</td></tr><tr><td>ResT-Large (Ours)</td><td>51.63</td><td>7.9</td><td>429</td><td>83.6 (\\uparrow 3.3)</td><td>96.3 (\\uparrow 1.1)</td></tr></tbody></table>", "source_caption": "Table 2: Comparison with state-of-the-art backbones on ImageNet-1k benchmark. Throughput (images / s) is measured on a single V100 GPU, following  DBLP:journals/corr/abs-2012-12877 . All models are trained and evaluated on 224\\times224 resolution. The best records and the improvements over bench-marked ResNets are marked in bold and blue, respectively."}