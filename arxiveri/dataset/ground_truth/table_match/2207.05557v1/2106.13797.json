{"target_title": "LightViT: Towards Light-Weight Convolution-Free Vision Transformers", "target_table": "<table><tbody><tr><td>Model</td><td> Blocktype </td><td> Params(M) </td><td> FLOPs(G) </td><td> Throughput(image/s) </td><td> Top-1(%) </td></tr><tr><td>RegNetY-800M [22]</td><td>CNN</td><td>6.3</td><td>0.8</td><td>3321</td><td>76.3</td></tr><tr><td>PVTv2-B0 [31]</td><td>Hybrid</td><td>3.4</td><td>0.6</td><td>2324</td><td>70.5</td></tr><tr><td>SimViT-Micro [15]</td><td>Hybrid</td><td>3.3</td><td>0.7</td><td>1004</td><td>71.1</td></tr><tr><td>MobileViT-XS [21]</td><td>Hybrid</td><td>2.3</td><td>0.7</td><td>1581</td><td>74.8</td></tr><tr><td>LVT [36]</td><td>Hybrid</td><td>5.5</td><td>0.9</td><td>1545</td><td>74.8</td></tr><tr><td>LightViT-T</td><td>Transformer</td><td>9.4</td><td>0.7</td><td>2578</td><td>78.7</td></tr><tr><td>RegNetY-1.6G [22]</td><td>CNN</td><td>11.2</td><td>1.6</td><td>1845</td><td>78.0</td></tr><tr><td>MobileViT-S [21]</td><td>Hybrid</td><td>5.6</td><td>1.1</td><td>1219</td><td>78.4</td></tr><tr><td>PVTv2-B1 [31]</td><td>Hybrid</td><td>13.1</td><td>2.1</td><td>1231</td><td>78.7</td></tr><tr><td>ResT-Small [41]</td><td>Hybrid</td><td>13.7</td><td>1.9</td><td>1298</td><td>79.6</td></tr><tr><td>DeiT-Ti [29]</td><td>Transformer</td><td>5.7</td><td>1.3</td><td>2612</td><td>72.2</td></tr><tr><td>LightViT-S</td><td>Transformer</td><td>19.2</td><td>1.7</td><td>1467</td><td>80.8</td></tr><tr><td>RegNetY-4G{}^{\\dagger} [22]</td><td>CNN</td><td>21.0</td><td>4.0</td><td>1045</td><td>80.0</td></tr><tr><td>Twins-PCPVT-S [3]</td><td>Hybrid</td><td>24.1</td><td>3.8</td><td>807</td><td>81.2</td></tr><tr><td>ResT-Base [41]</td><td>Hybrid</td><td>30.3</td><td>4.3</td><td>735</td><td>81.6</td></tr><tr><td>PVTv2-B2 [31]</td><td>Hybrid</td><td>25.4</td><td>4.0</td><td>695</td><td>82.0</td></tr><tr><td>DeiT-S [29]</td><td>Transformer</td><td>22</td><td>4.6</td><td>961</td><td>79.8</td></tr><tr><td>Swin-T [20]</td><td>Transformer</td><td>29</td><td>4.9</td><td>765</td><td>81.3</td></tr><tr><td>LightViT-B</td><td>Transformer</td><td>35.2</td><td>3.9</td><td>827</td><td>82.1</td></tr></tbody></table>", "target_caption": "Table 3: Image classification performance on ImageNet validation dataset. Throughput is measured on a single V100 GPU following [29, 20]. Hybrid denotes using both attention and convolution in blocks. All models are trained and evaluated on 224\\times 224 resolution. \\dagger: accuracy reported by DeiT [29].", "source_title": "Pvt v2: Improved baselines with pyramid vision transformer", "source_table": "<table><tr><td>Method</td><td>#Param (M)</td><td>GFLOPs</td><td>Top-1 Acc (%)</td></tr><tr><td>PVTv2-B0 (ours)</td><td>3.4</td><td>0.6</td><td>70.5</td></tr><tr><td>ResNet18 [14]</td><td>11.7</td><td>1.8</td><td>69.8</td></tr><tr><td>DeiT-Tiny/16 [31]</td><td>5.7</td><td>1.3</td><td>72.2</td></tr><tr><td>PVTv1-Tiny [33]</td><td>13.2</td><td>1.9</td><td>75.1</td></tr><tr><td>PVTv2-B1 (ours)</td><td>13.1</td><td>2.1</td><td>78.7</td></tr><tr><td>ResNet50 [14]</td><td>25.6</td><td>4.1</td><td>76.1</td></tr><tr><td>ResNeXt50-32x4d [35]</td><td>25.0</td><td>4.3</td><td>77.6</td></tr><tr><td>RegNetY-4G [26]</td><td>21.0</td><td>4.0</td><td>80.0</td></tr><tr><td>DeiT-Small/16 [31]</td><td>22.1</td><td>4.6</td><td>79.9</td></tr><tr><td>T2T-ViT{}_{t}-14 [37]</td><td>22.0</td><td>6.1</td><td>80.7</td></tr><tr><td>PVTv1-Small [33]</td><td>24.5</td><td>3.8</td><td>79.8</td></tr><tr><td>TNT-S [11]</td><td>23.8</td><td>5.2</td><td>81.3</td></tr><tr><td>Swin-T [23]</td><td>29.0</td><td>4.5</td><td>81.3</td></tr><tr><td>CvT-13 [34]</td><td>20.0</td><td>4.5</td><td>81.6</td></tr><tr><td>CoaT-Lite Small [36]</td><td>20.0</td><td>4.0</td><td>81.9</td></tr><tr><td>Twins-SVT-S [5]</td><td>24.0</td><td>2.8</td><td>81.7</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>22.6</td><td>3.9</td><td>82.1</td></tr><tr><td>PVTv2-B2 (ours)</td><td>25.4</td><td>4.0</td><td>82.0</td></tr><tr><td>ResNet101 [14]</td><td>44.7</td><td>7.9</td><td>77.4</td></tr><tr><td>ResNeXt101-32x4d [35]</td><td>44.2</td><td>8.0</td><td>78.8</td></tr><tr><td>RegNetY-8G [26]</td><td>39.0</td><td>8.0</td><td>81.7</td></tr><tr><td>T2T-ViT{}_{t}-19 [37]</td><td>39.0</td><td>9.8</td><td>81.4</td></tr><tr><td>PVTv1-Medium [33]</td><td>44.2</td><td>6.7</td><td>81.2</td></tr><tr><td>CvT-21 [34]</td><td>32.0</td><td>7.1</td><td>82.5</td></tr><tr><td>PVTv2-B3 (ours)</td><td>45.2</td><td>6.9</td><td>83.2</td></tr><tr><td>ResNet152 [14]</td><td>60.2</td><td>11.6</td><td>78.3</td></tr><tr><td>T2T-ViT{}_{t}-24  [37]</td><td>64.0</td><td>15.0</td><td>82.2</td></tr><tr><td>PVTv1-Large  [33]</td><td>61.4</td><td>9.8</td><td>81.7</td></tr><tr><td>TNT-B  [11]</td><td>66.0</td><td>14.1</td><td>82.8</td></tr><tr><td>Swin-S  [23]</td><td>50.0</td><td>8.7</td><td>83.0</td></tr><tr><td>Twins-SVT-B [5]</td><td>56.0</td><td>8.3</td><td>83.2</td></tr><tr><td>PVTv2-B4 (ours)</td><td>62.6</td><td>10.1</td><td>83.6</td></tr><tr><td>ResNeXt101-64x4d [35]</td><td>83.5</td><td>15.6</td><td>79.6</td></tr><tr><td>RegNetY-16G [26]</td><td>84.0</td><td>16.0</td><td>82.9</td></tr><tr><td>ViT-Base/16 [8]</td><td>86.6</td><td>17.6</td><td>81.8</td></tr><tr><td>DeiT-Base/16 [31]</td><td>86.6</td><td>17.6</td><td>81.8</td></tr><tr><td>Swin-B [23]</td><td>88.0</td><td>15.4</td><td>83.3</td></tr><tr><td>Twins-SVT-L [5]</td><td>99.2</td><td>14.8</td><td>83.7</td></tr><tr><td>PVTv2-B5 (ours)</td><td>82.0</td><td>11.8</td><td>83.8</td></tr></table>", "source_caption": "Table 2: Image classification performance on the ImageNet validation set.\u201c#Param\u201d refers to the number of parameters.\u201cGFLOPs\u201d is calculated under the input scale of 224\\times 224. \u201c*\u201d indicates the performance of the method trained under the strategy of its original paper.\u201c-Li\u201d denotes PVT v2 with linear SRA."}