{"target_title": "Contrastive Masked Autoencoders are Stronger Vision Learners", "target_table": "<table><tbody><tr><td>Method</td><td>Pre-training epochs</td><td>Params.(M)</td><td>Supervision</td><td>Accuracy</td></tr><tr><td>MoCo-v3 [10]</td><td>300</td><td>88</td><td>RGB</td><td>83.0</td></tr><tr><td>DINO [6]</td><td>1600</td><td>88</td><td>RGB</td><td>82.8</td></tr><tr><td>CIM [17]</td><td>300</td><td>88</td><td>RGB</td><td>83.1</td></tr><tr><td>BEiT [3]</td><td>800</td><td>88</td><td>DALLE</td><td>83.2</td></tr><tr><td>SimMIM [42]</td><td>800</td><td>88</td><td>RGB</td><td>83.8</td></tr><tr><td>PeCo [15]</td><td>800</td><td>88</td><td>Perceptual Codebook</td><td>84.5</td></tr><tr><td>MaskFeat [39]</td><td>1600</td><td>88</td><td>HOG</td><td>84.0</td></tr><tr><td>CAE [11]</td><td>1600</td><td>88</td><td>DALLE+RGB</td><td>83.8</td></tr><tr><td>iBOT [50]</td><td>1600</td><td>88</td><td>RGB</td><td>84.0</td></tr><tr><td>SIM [37]</td><td>1600</td><td>88</td><td>RGB</td><td>83.8</td></tr><tr><td>MAE [24]</td><td>800</td><td>88</td><td>RGB</td><td>83.1</td></tr><tr><td>MAE [24]</td><td>1600</td><td>88</td><td>RGB</td><td>83.6</td></tr><tr><td>CMAE (ours)</td><td>800</td><td>88</td><td>RGB</td><td>84.4</td></tr><tr><td>CMAE (ours)</td><td>1600</td><td>88</td><td>RGB</td><td>84.7</td></tr><tr><td>ConvMAE<sup>*</sup> [18]</td><td>800</td><td>88</td><td>RGB</td><td>84.6</td></tr><tr><td>ConvMAE<sup>*</sup> [18]</td><td>1600</td><td>88</td><td>RGB</td><td>84.6</td></tr><tr><td>CMAE<sup>*</sup> (ours)</td><td>800</td><td>88</td><td>RGB</td><td>85.0</td></tr><tr><td>CMAE<sup>*</sup> (ours)</td><td>1600</td><td>88</td><td>RGB</td><td>85.3</td></tr></tbody></table>", "target_caption": "Table 2: Comparison of our model with existing methods on ViT-B. We evaluate them with the top-1 accuracy on ImageNet. The symbol of <sup>*</sup> throughout experiments denotes using convolutions instead of linear transformation as the tokenizer for visual patches. ", "source_title": "Masked autoencoders are scalable vision learners", "source_table": "<table><thead><tr><td>method</td><td>pre-train data</td><td>ViT-B</td><td>ViT-L</td><td>ViT-H</td><td>ViT-H{}_{\\text{448}}</td></tr></thead><tbody><tr><td>scratch, our impl.</td><td>-</td><td>82.3</td><td>82.6</td><td>83.1</td><td>-</td></tr><tr><td>DINO [5]</td><td>IN1K</td><td><p>82.8</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>MoCo v3 [9]</td><td>IN1K</td><td><p>83.2</p></td><td><p>84.1</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>BEiT [2]</td><td>IN1K+DALLE</td><td><p>83.2</p></td><td><p>85.2</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>MAE</td><td>IN1K</td><td>83.6</td><td>85.9</td><td>86.9</td><td>87.8</td></tr></tbody></table>", "source_caption": "Table 3: Comparisons with previous results on ImageNet-1K. The pre-training data is the ImageNet-1K training set (except the tokenizer in BEiT was pre-trained on 250M DALLE data [50]). All self-supervised methods are evaluated by end-to-end fine-tuning. The ViT models are B/16, L/16, H/14 [16]. The best for each column is underlined. All results are on an image size of 224, except for ViT-H with an extra result on 448. Here our MAE reconstructs normalized pixels and is pre-trained for 1600 epochs."}