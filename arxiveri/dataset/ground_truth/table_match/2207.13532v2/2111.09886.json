{"target_title": "Contrastive Masked Autoencoders are Stronger Vision Learners", "target_table": "<table><tbody><tr><td>Method</td><td>Pre-training epochs</td><td>Params.(M)</td><td>Supervision</td><td>Accuracy</td></tr><tr><td>MoCo-v3 [10]</td><td>300</td><td>88</td><td>RGB</td><td>83.0</td></tr><tr><td>DINO [6]</td><td>1600</td><td>88</td><td>RGB</td><td>82.8</td></tr><tr><td>CIM [17]</td><td>300</td><td>88</td><td>RGB</td><td>83.1</td></tr><tr><td>BEiT [3]</td><td>800</td><td>88</td><td>DALLE</td><td>83.2</td></tr><tr><td>SimMIM [42]</td><td>800</td><td>88</td><td>RGB</td><td>83.8</td></tr><tr><td>PeCo [15]</td><td>800</td><td>88</td><td>Perceptual Codebook</td><td>84.5</td></tr><tr><td>MaskFeat [39]</td><td>1600</td><td>88</td><td>HOG</td><td>84.0</td></tr><tr><td>CAE [11]</td><td>1600</td><td>88</td><td>DALLE+RGB</td><td>83.8</td></tr><tr><td>iBOT [50]</td><td>1600</td><td>88</td><td>RGB</td><td>84.0</td></tr><tr><td>SIM [37]</td><td>1600</td><td>88</td><td>RGB</td><td>83.8</td></tr><tr><td>MAE [24]</td><td>800</td><td>88</td><td>RGB</td><td>83.1</td></tr><tr><td>MAE [24]</td><td>1600</td><td>88</td><td>RGB</td><td>83.6</td></tr><tr><td>CMAE (ours)</td><td>800</td><td>88</td><td>RGB</td><td>84.4</td></tr><tr><td>CMAE (ours)</td><td>1600</td><td>88</td><td>RGB</td><td>84.7</td></tr><tr><td>ConvMAE<sup>*</sup> [18]</td><td>800</td><td>88</td><td>RGB</td><td>84.6</td></tr><tr><td>ConvMAE<sup>*</sup> [18]</td><td>1600</td><td>88</td><td>RGB</td><td>84.6</td></tr><tr><td>CMAE<sup>*</sup> (ours)</td><td>800</td><td>88</td><td>RGB</td><td>85.0</td></tr><tr><td>CMAE<sup>*</sup> (ours)</td><td>1600</td><td>88</td><td>RGB</td><td>85.3</td></tr></tbody></table>", "target_caption": "Table 2: Comparison of our model with existing methods on ViT-B. We evaluate them with the top-1 accuracy on ImageNet. The symbol of <sup>*</sup> throughout experiments denotes using convolutions instead of linear transformation as the tokenizer for visual patches. ", "source_title": "Simmim: A simple framework for masked image modeling", "source_table": "<table><tr><td rowspan=\"2\">Methods</td><td>Input</td><td>Fine-tuning</td><td>Linear eval</td><td>Pre-training</td></tr><tr><td>Size</td><td>Top-1 acc (%)</td><td>Top-1 acc (%)</td><td>costs</td></tr><tr><td>Sup. baseline [46]</td><td>224^{2}</td><td>81.8</td><td>-</td><td>-</td></tr><tr><td>DINO [5]</td><td>224^{2}</td><td>82.8</td><td>78.2</td><td>2.0\\times</td></tr><tr><td>MoCo v3 [9]</td><td>224^{2}</td><td>83.2</td><td>76.7</td><td>1.8\\times</td></tr><tr><td>ViT [15]</td><td>384^{2}</td><td>79.9</td><td>-</td><td>\\sim4.0\\times</td></tr><tr><td>BEiT [1]</td><td>224^{2}</td><td>83.2</td><td>56.7</td><td>1.5\\times^{\\dagger}</td></tr><tr><td>Ours</td><td>224^{2}</td><td>83.8</td><td>56.7</td><td>1.0\\times</td></tr></table>", "source_caption": "Table 6: System-level comparison using ViT-B as the encoder. Training costs are counted in relative to our approach. {}^{\\dagger} BEiT requires an additional stage to pre-train dVAE, which is not counted."}