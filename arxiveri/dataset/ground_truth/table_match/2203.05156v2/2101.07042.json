{"target_title": "End-to-End Semantic Video Transformer for Zero-Shot Action Recognition", "target_table": "<table><tbody><tr><td>Method</td><td>Protocol</td><td>UCF</td><td>HMDB</td><td>ActivityNet</td></tr><tr><td>DataAug [47]</td><td>OE</td><td>18.3</td><td>19.7</td><td>-</td></tr><tr><td>InfDem [32]</td><td>OE</td><td>17.8</td><td>21.3</td><td>-</td></tr><tr><td>Bidirectional [42]</td><td>OE</td><td>21.4</td><td>18.9</td><td>-</td></tr><tr><td>TARN [4]</td><td>OE</td><td>19</td><td>19.5</td><td>-</td></tr><tr><td>Action2Vec [21]</td><td>OE</td><td>22.1</td><td>23.5</td><td>-</td></tr><tr><td>OD [27]</td><td>OE</td><td>26.9</td><td>30.2</td><td>-</td></tr><tr><td>CLASTER [19]</td><td>OE</td><td>46.4</td><td>36.8</td><td>-</td></tr><tr><td>GGM [27]</td><td>OE</td><td>20.3</td><td>20.7</td><td>-</td></tr><tr><td>ViSET-96(600) + CD (Ours)</td><td>OE</td><td>68.3</td><td>40.2</td><td>44.8</td></tr><tr><td>E2E (605classes)</td><td>R</td><td>44.1</td><td>29.8</td><td>26.6</td></tr><tr><td>E2E (664classes)</td><td>R</td><td>48</td><td>32.7</td><td>-</td></tr><tr><td>ViSET-96(505) + CD (Ours)</td><td>R</td><td>45.6</td><td>31.3</td><td>35.8</td></tr><tr><td>ViSET-96(564) + CD (Ours)</td><td>R</td><td>53.2</td><td>34.5</td><td>-</td></tr></tbody></table>", "target_caption": "Table 2: Comparison with the state-of-the-art methods on standard benchmark datasets using the open-ended (OE) and Restrictive (R) protocols. All the methods are evaluated by randomly splitting the dataset in half and averaging the results over 10 trials.", "source_title": "Claster: Clustering with reinforcement learning for zero-shot action recognition", "source_table": "<table><thead><tr><td>Method</td><td>SE</td><td>Olympics</td><td>HMDB51</td><td>UCF101</td></tr></thead><tbody><tr><td>SJE [1]</td><td>M</td><td>47.5 \\pm 14.8</td><td>-</td><td>12.0 \\pm 1.2</td></tr><tr><td>Bi-Dir GAN [29]</td><td>M</td><td>53.2 \\pm 10.5</td><td>-</td><td>24.7 \\pm 3.7</td></tr><tr><td>IAF [29]</td><td>M</td><td>54.9 \\pm 11.7</td><td>-</td><td>26.1 \\pm 2.9</td></tr><tr><td>GGM [30]</td><td>M</td><td>57.9 \\pm 14.1</td><td>-</td><td>24.5 \\pm 2.9</td></tr><tr><td>OD [26]</td><td>M</td><td>65.9 \\pm 8.1</td><td>-</td><td>38.3 \\pm 3.0</td></tr><tr><td>WGAN [43]</td><td>M</td><td>64.7 \\pm 7.5</td><td>-</td><td>37.5 \\pm 3.1</td></tr><tr><td>CLASTER (ours)</td><td>M</td><td>67.4 \\pm 7.8</td><td>-</td><td>51.8 \\pm 2.8</td></tr><tr><td>SJE [1]</td><td>W</td><td>28.6 \\pm 4.9</td><td>13.3 \\pm 2.4</td><td>9.9 \\pm 1.4</td></tr><tr><td>IAF [29]</td><td>W</td><td>39.8 \\pm 11.6</td><td>19.2 \\pm 3.7</td><td>22.2 \\pm 2.7</td></tr><tr><td>Bi-Dir GAN [29]</td><td>W</td><td>40.2 \\pm 10.6</td><td>21.3 \\pm 3.2</td><td>21.8 \\pm 3.6</td></tr><tr><td>GGM [30]</td><td>W</td><td>41.3 \\pm 11.4</td><td>20.7 \\pm 3.1</td><td>20.3 \\pm 1.9</td></tr><tr><td>WGAN [43]</td><td>W</td><td>47.1 \\pm 6.4</td><td>29.1 \\pm 3.8</td><td>25.8 \\pm 3.2</td></tr><tr><td>OD [26]</td><td>W</td><td>50.5 \\pm 6.9</td><td>30.2 \\pm 2.7</td><td>26.9 \\pm 2.8</td></tr><tr><td>PS-GNN [13]</td><td>W</td><td>61.8 \\pm 6.8</td><td>32.6 \\pm 2.9</td><td>43.0 \\pm 4.9</td></tr><tr><td>E2E [4]*</td><td>W</td><td>61.4 \\pm 5.5</td><td>33.1 \\pm 3.4</td><td>46.2 \\pm 3.8</td></tr><tr><td>CLASTER (ours)</td><td>W</td><td>63.8 \\pm 5.7</td><td>36.6 \\pm 4.6</td><td>46.7 \\pm 5.4</td></tr><tr><td>CLASTER (ours)</td><td>S</td><td>64.2 \\pm 3.3</td><td>41.8 \\pm 2.1</td><td>50.2 \\pm 3.8</td></tr><tr><td>CLASTER (ours)</td><td>C</td><td>67.7 \\pm 2.7</td><td>42.6 \\pm 2.6</td><td>52.7 \\pm 2.2</td></tr><tr><td>ER [6]</td><td>ED</td><td>60.2 \\pm 8.9</td><td>35.3 \\pm 4.6</td><td>51.8 \\pm 2.9</td></tr><tr><td>CLASTER (ours)</td><td>ED</td><td>68.4 \\pm 4.1</td><td>43.2 \\pm 1.9</td><td>53.9 \\pm 2.5</td></tr></tbody></table>", "source_caption": "Table 2: Results on ZSL. SE: semantic embedding, M: manual representation, W: word2vec embedding, S: sentence2vec, C: Combination of embeddings. The proposed CLASTER outperforms previous state-of-the-art across tasks and datasets. "}