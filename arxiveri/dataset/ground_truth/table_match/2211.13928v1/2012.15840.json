{"target_title": "UperFormer: A Multi-scale Transformer-based Decoder for Semantic Segmentation", "target_table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">Params</td><td colspan=\"3\">ADE20K</td><td colspan=\"3\">Cityscapes</td></tr><tr><td>FLOPs**</td><td>mIoU</td><td> mIoU(ms+flip)* </td><td>FLOPs</td><td>mIoU</td><td> mIoU(ms+flip) </td></tr><tr><td>FPN[23]</td><td>ResNet-101[16]</td><td>47.51M</td><td>65.04G</td><td>39.35</td><td>40.72</td><td>145.72G</td><td>75.80</td><td>77.40</td></tr><tr><td>UperNet[37]</td><td>ResNet-101</td><td>83.11M</td><td>257.37G</td><td>40.66</td><td>40.44</td><td>576.55G</td><td>78.84</td><td>79.9</td></tr><tr><td>UperFormer(Ours)\u2020</td><td>ResNet-101</td><td>247.86M</td><td>248.3G</td><td>43.18</td><td>44.08</td><td>523.01G</td><td>79.94</td><td>-</td></tr><tr><td>FCN[26]</td><td>ResNet-101-D8</td><td>68.59M</td><td>275.69G</td><td>39.91</td><td>41.40</td><td>632.49G</td><td>75.52</td><td>76.61</td></tr><tr><td>DeepLabV3+[5]</td><td>ResNet-101-D8</td><td>62.68M</td><td>255.14G</td><td>45.47</td><td>46.35</td><td>583.18G</td><td>80.65</td><td>81.47</td></tr><tr><td>DMnet[15]</td><td>ResNet-101-D8</td><td>72.27M</td><td>273.64G</td><td>45.42</td><td>46.76</td><td>627.67G</td><td>79.19</td><td>80.65</td></tr><tr><td>PSPNet[43]</td><td>ResNet-101-D8</td><td>68.07M</td><td>256.44G</td><td>44.39</td><td>45.35</td><td>588.21G</td><td>79.77</td><td>81.06</td></tr><tr><td>PSANet[44]</td><td>ResNet-101-D8</td><td>73.06M</td><td>272.48G</td><td>43.74</td><td>45.38</td><td>637.68G</td><td>79.69</td><td>80.89</td></tr><tr><td>SETR-PUP[45]</td><td>ViT-Large</td><td>317.29M</td><td>270.55G</td><td>48.24</td><td>49.99</td><td>588.62G</td><td>79.21</td><td>81.02</td></tr><tr><td>SegFormer[39]</td><td>MiT-B5</td><td>82.01M</td><td>52.45G</td><td>49.13</td><td>50.22</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UperNet[37]</td><td>Swin-T-Base[25]</td><td>121.42M</td><td>299.81G</td><td>50.21</td><td>52.02</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UperFormer(Ours)</td><td>Swin-T-Base</td><td>138.94M</td><td>139.53G</td><td>50.18</td><td>51.8</td><td>285.85G</td><td>81.44</td><td>82.79</td></tr></table><p>*ms+flip means we use multi-scaled and flipped data when testing. On ADE20K, the 512\\times 512 images are augmented to 512\\times 512 and 1024\\times 1024. On Cityscapes, the 2048\\times 1024 images are augmented to 2048\\times 1024 and 4096\\times 2048.<br/>**On ADE20K, the FLOPs are calculated with input images of 512\\times 512. On Cityscapes, the FLOPs are calculated with input images of 768\\times 768.<br/>\u2020The maximum output channels of ResNet are 2048, while that of Swin-T are 1024. So the Params and FLOPs of UperFormer+ResNet are quite larger than UperFormer+Swin-T, but the FLOPs are still lower than most of the models.</p>", "target_caption": "Table 5: Comparison of segmentation results of different methods on ADE20K and Cityscapes dataset. ", "source_title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers", "source_table": "<table><tbody><tr><td>Method</td><td>Pre</td><td>Backbone</td><td>#Params</td><td>40k</td><td>80k</td></tr><tr><td>FCN [39]</td><td>1K</td><td>R-101</td><td>68.59M</td><td>73.93</td><td>75.52</td></tr><tr><td>Semantic FPN [39]</td><td>1K</td><td>R-101</td><td>47.51M</td><td>-</td><td>75.80</td></tr><tr><td>Hybrid-Base</td><td>R</td><td>T-Base</td><td>112.59M</td><td>74.48</td><td>77.36</td></tr><tr><td>Hybrid-Base</td><td>21K</td><td>T-Base</td><td>112.59M</td><td>76.76</td><td>76.57</td></tr><tr><td>Hybrid-DeiT</td><td>21K</td><td>T-Base</td><td>112.59M</td><td>77.42</td><td>78.28</td></tr><tr><td>SETR-Na\u00efve</td><td>21K</td><td>T-Large</td><td>305.67M</td><td>77.37</td><td>77.90</td></tr><tr><td>SETR-MLA</td><td>21K</td><td>T-Large</td><td>310.57M</td><td>76.65</td><td>77.24</td></tr><tr><td>SETR-PUP</td><td>21K</td><td>T-Large</td><td>318.31M</td><td>78.39</td><td>79.34</td></tr><tr><td>SETR-PUP</td><td>R</td><td>T-Large</td><td>318.31M</td><td>42.27</td><td>-</td></tr><tr><td>SETR-Na\u00efve-Base</td><td>21K</td><td>T-Base</td><td>87.69M</td><td>75.54</td><td>76.25</td></tr><tr><td>SETR-MLA-Base</td><td>21K</td><td>T-Base</td><td>92.59M</td><td>75.60</td><td>76.87</td></tr><tr><td>SETR-PUP-Base</td><td>21K</td><td>T-Base</td><td>97.64M</td><td>76.71</td><td>78.02</td></tr><tr><td>SETR-Na\u00efve-DeiT</td><td>1K</td><td>T-Base</td><td>87.69M</td><td>77.85</td><td>78.66</td></tr><tr><td>SETR-MLA-DeiT</td><td>1K</td><td>T-Base</td><td>92.59M</td><td>78.04</td><td>78.98</td></tr><tr><td>SETR-PUP-DeiT</td><td>1K</td><td>T-Base</td><td>97.64M</td><td>78.79</td><td>79.45</td></tr></tbody></table>", "source_caption": "Table 2: Comparing SETR variants on different pre-training strategies and backbones.All experiments are trained on Cityscapes train fine set with batch size 8, and evaluated using the single scale test protocol on the Cityscapes validation set in mean IoU (%) rate.\u201cPre\u201d denotes the pre-training of transformer part.\u201cR\u201d means the transformer part is randomly initialized."}