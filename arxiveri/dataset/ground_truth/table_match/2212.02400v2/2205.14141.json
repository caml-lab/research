{"target_title": "Location-Aware Self-Supervised Transformers for Semantic Segmentation", "target_table": "<table><thead><tr><td>Method</td><td>ADE20k</td></tr><tr><td colspan=\"2\">Supervised on ImageNet-1k</td></tr></thead><tbody><tr><td>Supervised used in MAE [31]</td><td>47.4</td></tr><tr><td>DeiT-III [59]</td><td>49.3</td></tr><tr><td colspan=\"2\">Self-supervised on ImageNet-1k without labels</td></tr><tr><td>DINO [11]</td><td>47.2</td></tr><tr><td>SimMIM [68]</td><td>47.6</td></tr><tr><td>FD-DINO [63]</td><td>47.7</td></tr><tr><td>MAE [31]</td><td>48.1</td></tr><tr><td>data2vec [5]</td><td>48.2</td></tr><tr><td>iBOT [74]</td><td>48.4</td></tr><tr><td>LOCA (Ours)</td><td>48.5</td></tr></tbody></table>", "target_caption": "Table 6: Comparison to previous results with UperNet.We report mean IoU for semantic segmentation on the validation set of ADE20k for different self-supervised and supervised representation learning methods.All models use the popular ViT-Base/16 architecture and are pre-trained on ImageNet-1k before being end-to-end finetuned on ADE20k.Results are reported in single-scale mode.", "source_title": "Contrastive learning rivals masked image modeling in fine-tuning via feature distillation", "source_table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">res.</td><td rowspan=\"2\">F. D.</td><td colspan=\"2\">IN-1K</td><td rowspan=\"2\">ADE20K</td></tr><tr><td>f.t.</td><td>linear</td></tr><tr><td>BEiT Bao et al., (2021)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>83.2</td><td>37.6</td><td>47.1</td></tr><tr><td>MAE He et al., (2021)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>83.6</td><td>68.0</td><td>48.1</td></tr><tr><td>SimMIM Xie et al., (2022)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>83.8</td><td>56.7</td><td>47.6</td></tr><tr><td>SimMIM Xie et al., (2022)</td><td>Swin-B</td><td>224^{2}</td><td></td><td>84.8</td><td>24.8</td><td>48.3</td></tr><tr><td>WiSE-FT CLIP Wortsman et al., (2021)</td><td>ViT-L</td><td>336{}^{2}</td><td></td><td>87.1</td><td>-</td><td>-</td></tr><tr><td>DINO Caron et al., (2021)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>82.8</td><td>78.2</td><td>46.2</td></tr><tr><td>FD-DINO</td><td>ViT-B</td><td>224^{2}</td><td>\\checkmark</td><td>83.8 (+1.0)</td><td>76.1</td><td>47.7 (+1.5)</td></tr><tr><td>EsViT Li et al., 2022a </td><td>Swin-B</td><td>224^{2}</td><td></td><td>83.9</td><td>81.3</td><td>47.3</td></tr><tr><td>FD-EsViT</td><td>Swin-B</td><td>224^{2}</td><td>\\checkmark</td><td>85.1 (+1.2)</td><td>80.4</td><td>48.9 (+1.6)</td></tr><tr><td>DeiT Touvron et al., (2020)</td><td rowspan=\"2\">ViT-B</td><td>224^{2}</td><td></td><td>81.8</td><td>-</td><td>47.0</td></tr><tr><td>FD-DeiT</td><td>224^{2}</td><td>\\checkmark</td><td>83.0 (+1.2)</td><td>-</td><td>48.0  (+1.0)</td></tr><tr><td>CLIP Radford et al., (2021)</td><td rowspan=\"2\">ViT-B</td><td>224^{2}</td><td></td><td>82.9</td><td>79.5</td><td>49.5</td></tr><tr><td>FD-CLIP</td><td>224^{2}</td><td>\\checkmark</td><td>84.9 (+2.0)</td><td>80.3</td><td>52.8 (+3.3)</td></tr><tr><td>CLIP Radford et al., (2021)</td><td rowspan=\"3\">ViT-L</td><td>224^{2}</td><td></td><td>86.1</td><td>83.5</td><td>53.5</td></tr><tr><td>FD-CLIP</td><td>224^{2}</td><td>\\checkmark</td><td>87.7 (+1.6)</td><td>84.8</td><td>55.7 (+2.2)</td></tr><tr><td>FD-CLIP*</td><td>336{}^{2}</td><td>\\checkmark</td><td>89.0</td><td>-</td><td>-</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "source_caption": "Table 1: Feature distillation improves fine-tuning performance. * uses the same model in the upper row, but with an additional inter-mediate fine-tuning step on ImageNet-22K image classification."}