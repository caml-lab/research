{
    "target_title": "Location-Aware Self-Supervised Transformers for Semantic Segmentation",
    "target_table": "<table><tbody><tr><td></td><td colspan=\"4\">Classification only (mAP)</td><td colspan=\"4\">Localization only (mIoU)</td><td colspan=\"4\">Both (mIoU)</td></tr><tr><td>Method</td><td>ADE20k</td><td>P. Cont.</td><td>P. VOC</td><td>Citysc.</td><td>ADE20k</td><td>P. Cont.</td><td>P. VOC</td><td>Citysc.</td><td>ADE20k</td><td>P. Cont.</td><td>P. VOC</td><td>Citysc.</td></tr><tr><td>Image-level pretrainings</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DINO [11]</td><td>61.6</td><td>67.7</td><td>89.9</td><td>81.5</td><td>64.5</td><td>71.6</td><td>78.7</td><td>79.6</td><td>44.1</td><td>50.7</td><td>74.7</td><td>78.4</td></tr><tr><td>MoCo-v3 [15]</td><td>61.1</td><td>69.3</td><td>93.6</td><td>82.1</td><td>66.2</td><td>73.7</td><td>79.0</td><td>79.9</td><td>45.4</td><td>51.6</td><td>74.3</td><td>78.6</td></tr><tr><td>Supervised (DeiT-III [59])</td><td>64.8</td><td>71.5</td><td>94.6</td><td>84.0</td><td>66.5</td><td>73.6</td><td>80.1</td><td>80.7</td><td>47.3</td><td>53.9</td><td>76.3</td><td>79.7</td></tr><tr><td>Spatially-aware pretrainings</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MAE [31]</td><td>59.0</td><td>67.6</td><td>92.8</td><td>84.3</td><td>67.0</td><td>74.3</td><td>79.9</td><td>81.1</td><td>45.5</td><td>51.7</td><td>75.4</td><td>79.7</td></tr><tr><td>LOCA (Ours)</td><td>62.2</td><td>69.9</td><td>93.7</td><td>83.6</td><td>67.9</td><td>75.4</td><td>80.5</td><td>81.4</td><td>47.9</td><td>54.9</td><td>76.6</td><td>79.8</td></tr></tbody></table>",
    "target_caption": "Table 4: Disentangling localization and classification on semantic segmentation.We report classification only (with a multi-label classification training) and localization only (with an oracle giving the class of the segmentation masks) evaluations on 4 popular semantic segmentation benchmarks: ADE20k [73], Pascal Context (\u201cP.Cont.\u201d) [44], Pascal VOC (\u201cP.VOC\u201d) [25] and Cityscapes (\u201cCity.\u201d) [19].Details about the classification/location only protocols are in Sec. 4.2.Best number is in bold and second best is underlined.We report performance for different methods pretrained on ImageNet-1k (with or without labels) with ViT-B.LOCA yields excellent locality and good semantic understanding but is still behind image-level pretraining methods on the pure semantic axis (classification only evaluation).",
    "source_title": "Emerging properties in self-supervised vision transformers",
    "source_table": "",
    "source_caption": ""
}