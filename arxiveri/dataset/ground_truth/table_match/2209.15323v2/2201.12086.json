{"target_title": "SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation", "target_table": "<table><tbody><tr><td>Model</td><td>|\\theta|</td><td>B@4</td><td>M</td><td>CIDEr</td><td>S</td></tr><tr><td colspan=\"6\">Large Models with V&amp;L pre-training</td></tr><tr><td>LEMON{}_{\\text{Huge}} [11]</td><td>675</td><td>41.5</td><td>30.8</td><td>139.1</td><td>24.1</td></tr><tr><td>SimVLM{}_{\\text{Huge}} [39]</td><td>632</td><td>40.6</td><td>33.7</td><td>143.3</td><td>25.4</td></tr><tr><td>OSCAR{}_{\\text{Large}} [19]</td><td>338</td><td>37.4</td><td>30.7</td><td>127.8</td><td>23.5</td></tr><tr><td>BLIP{}_{{\\text{CapFilt-L}}} [18]</td><td>224</td><td>39.7</td><td>-</td><td>133.3</td><td>-</td></tr><tr><td colspan=\"6\">Lightweight models</td></tr><tr><td>I-tuning{}_{\\text{Large}} [22]</td><td>95</td><td>34.8</td><td>29.3</td><td>119.4</td><td>22.4</td></tr><tr><td>CaMEL [4]</td><td>76</td><td>39.1</td><td>29.4</td><td>125.7</td><td>22.2</td></tr><tr><td>I-tuning{}_{\\text{Medium}} [22]</td><td>44</td><td>35.5</td><td>28.8</td><td>120.0</td><td>22.0</td></tr><tr><td>ClipCap [24]</td><td>43</td><td>33.5</td><td>27.5</td><td>113.1</td><td>21.1</td></tr><tr><td>I-tuning{}_{\\text{Base}} [22]</td><td>14</td><td>34.8</td><td>28.3</td><td>116.7</td><td>21.8</td></tr><tr><td>SmallCap</td><td>7</td><td>37.0</td><td>27.9</td><td>119.7</td><td>21.3</td></tr><tr><td>-SmallerCap</td><td>3.6</td><td>36.7</td><td>27.8</td><td>119.1</td><td>21.1</td></tr><tr><td>-SmallestCap</td><td>1.8</td><td>36.0</td><td>27.4</td><td>117.4</td><td>21.0</td></tr></tbody></table>", "target_caption": "Table 1: Results on the COCO test split with cross-entropy training.|\\theta|: number of trainable parameters in the model (in millions).", "source_title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "source_table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">Pre-train#Images </td><td colspan=\"8\">NoCaps validation</td><td colspan=\"2\">COCO Caption</td></tr><tr><td colspan=\"2\">in-domain</td><td colspan=\"2\">near-domain</td><td colspan=\"2\">out-domain</td><td colspan=\"2\">overall</td><td colspan=\"2\">Karpathy test</td></tr><tr><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>B@4</td><td>C</td></tr><tr><td>Enc-Dec (Changpinyo et al., 2021)</td><td>15M</td><td>92.6</td><td>12.5</td><td>88.3</td><td>12.1</td><td>94.5</td><td>11.9</td><td>90.2</td><td>12.1</td><td>-</td><td>110.9</td></tr><tr><td>VinVL\u2020 (Zhang et al., 2021)</td><td>5.7M</td><td>103.1</td><td>14.2</td><td>96.1</td><td>13.8</td><td>88.3</td><td>12.1</td><td>95.5</td><td>13.5</td><td>38.2</td><td>129.3</td></tr><tr><td>LEMON{}_{\\mathrm{base}}\u2020 (Hu et al., 2021)</td><td>12M</td><td>104.5</td><td>14.6</td><td>100.7</td><td>14.0</td><td>96.7</td><td>12.4</td><td>100.4</td><td>13.8</td><td>-</td><td>-</td></tr><tr><td>LEMON{}_{\\mathrm{base}}\u2020 (Hu et al., 2021)</td><td>200M</td><td>107.7</td><td>14.7</td><td>106.2</td><td>14.3</td><td>107.9</td><td>13.1</td><td>106.8</td><td>14.1</td><td>40.3</td><td>133.3</td></tr><tr><td>BLIP</td><td>14M</td><td>111.3</td><td>15.1</td><td>104.5</td><td>14.4</td><td>102.4</td><td>13.7</td><td>105.1</td><td>14.4</td><td>38.6</td><td>129.7</td></tr><tr><td>BLIP</td><td>129M</td><td>109.1</td><td>14.8</td><td>105.8</td><td>14.4</td><td>105.7</td><td>13.7</td><td>106.3</td><td>14.3</td><td>39.4</td><td>131.4</td></tr><tr><td>BLIP{}_{\\text{CapFilt-L}}</td><td>129M</td><td>111.8</td><td>14.9</td><td>108.6</td><td>14.8</td><td>111.5</td><td>14.2</td><td>109.6</td><td>14.7</td><td>39.7</td><td>133.3</td></tr><tr><td>LEMON{}_{\\mathrm{large}}\u2020 (Hu et al., 2021)</td><td>200M</td><td>116.9</td><td>15.8</td><td>113.3</td><td>15.1</td><td>111.3</td><td>14.0</td><td>113.4</td><td>15.0</td><td>40.6</td><td>135.7</td></tr><tr><td>SimVLM{}_{\\mathrm{huge}} (Wang et al., 2021)</td><td>1.8B</td><td>113.7</td><td>-</td><td>110.9</td><td>-</td><td>115.2</td><td>-</td><td>112.2</td><td>-</td><td>40.6</td><td>143.3</td></tr><tr><td>BLIP{}_{\\text{ViT-L}}</td><td>129M</td><td>114.9</td><td>15.2</td><td>112.1</td><td>14.9</td><td>115.3</td><td>14.4</td><td>113.2</td><td>14.8</td><td>40.4</td><td>136.7</td></tr></table>", "source_caption": "Table 7: Comparison with state-of-the-art image captioning methods on NoCaps and COCO Caption.All methods optimize the cross-entropy loss during finetuning. C: CIDEr, S: SPICE, B@4: BLEU@4. BLIP{}_{\\text{CapFilt-L}} is pre-trained on a dataset bootstrapped by captioner and filter with ViT-L.VinVL\u2020 and LEMON\u2020 require an object detector pre-trained on 2.5M images with human-annotated bounding boxes and high resolution (800\\times1333) input images.SimVLM{}_{\\mathrm{huge}} uses 13\\times more training data and a larger vision backbone than ViT-L."}