{"title": "Adamixer: A fast-converging query-based object detector", "abstract": "Traditional object detectors employ the dense paradigm of scanning over locations and scales in an image. The recent query-based object detectors break this convention by decoding image features with a set of learnable queries. However, this paradigm still suffers from slow convergence, limited performance, and design complexity of extra networks between backbone and decoder. In this paper, we find that the key to these issues is the adaptability of decoders for casting queries to varying objects. Accordingly, we propose a fast-converging query-based detector, named AdaMixer, by improving the adaptability of query-based decoding processes in two aspects. First, each query adaptively samples features over space and scales based on estimated offsets, which allows AdaMixer to efficiently attend to the coherent regions of objects. Then, we dynamically decode these sampled features with an adaptive MLP-Mixer under the guidance of each query. Thanks to these two critical designs, AdaMixer enjoys architectural simplicity without requiring dense attentional encoders or explicit pyramid networks. On the challenging MS COCO benchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs, reaches up to 45.0 AP on the validation set along with 27.9 APs in detecting small objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN and Swin-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple, accurate, and fast converging architecture for query-based object detectors. The code is made available at https://github.com/MCG-NJU/AdaMixer", "authors": ["Ziteng Gao", " Limin Wang", " Bing Han", " Sheng Guo"], "pdf_url": "https://arxiv.org/abs/2203.16507", "list_table_and_caption": [{"table": "<table><tr><td></td><td>adaptive to decode locations?</td><td>adaptive to decode content?</td><td>extra networks before the query decoder<sup>1</sup>?</td></tr><tr><td>DETR [4]</td><td>yes, multi-head attention aggregation</td><td>no, linear projection</td><td>TransformerEncoder</td></tr><tr><td>Deformable DETR [56]</td><td>yes, multi-scale multi-head adaptive sampling</td><td>no, linear projection<sup>2</sup></td><td>Multi-scale DeformTransEncoder</td></tr><tr><td>Sparse R-CNN [39]</td><td>restricted, RoIAlign [17]</td><td>partially yes, adaptive point-wise conv.</td><td>FPN</td></tr><tr><td>AdaMixer (ours)</td><td>yes, adaptive 3D sampling</td><td>yes, adaptive channel and spatial mixing</td><td>linear projection to form 3D feature space</td></tr></table>", "caption": "Table 1: Comparisons of the adaptability of decoders across different query-based object detectors. <sup>1</sup>We specify trainable networks introduced after pre-trained backbones before the query decoder. <sup>2</sup>We regard the softmax aggregation in deformable attention as one step in decoding locations as the softmax weights normalize to one.", "list_citation_info": ["[39] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse R-CNN: end-to-end object detection with learnable proposals. In CVPR, 2021.", "[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.", "[17] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask R-CNN. In ICCV, 2017.", "[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable transformers for end-to-end object detection. In ICLR, 2020."]}, {"table": "<table><tr><td>detector</td><td>epochs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{s}</td><td>AP{}_{m}</td><td>AP{}_{l}</td></tr><tr><td>FCOS [40]</td><td>12</td><td>38.7</td><td>57.4</td><td>41.8</td><td>22.9</td><td>42.5</td><td>50.1</td></tr><tr><td>Cascade R-CNN [3]</td><td>12</td><td>40.4</td><td>58.9</td><td>44.1</td><td>22.8</td><td>43.7</td><td>54.0</td></tr><tr><td>GFocalV2 [22]</td><td>12</td><td>41.1</td><td>58.8</td><td>44.9</td><td>23.5</td><td>44.9</td><td>53.3</td></tr><tr><td>BorderDet [33]</td><td>12</td><td>41.4</td><td>59.4</td><td>44.5</td><td>23.6</td><td>45.1</td><td>54.6</td></tr><tr><td>Dynamic Head [8]</td><td>12</td><td>42.6</td><td>60.1</td><td>46.4</td><td>26.1</td><td>46.8</td><td>56.0</td></tr><tr><td>DETR [4]</td><td>12</td><td>20.0</td><td>36.2</td><td>19.3</td><td>6.0</td><td>20.5</td><td>32.2</td></tr><tr><td>Deformable DETR [56]</td><td>12</td><td>35.1</td><td>53.6</td><td>37.7</td><td>18.2</td><td>38.5</td><td>48.7</td></tr><tr><td>Sparse R-CNN [39]</td><td>12</td><td>37.9</td><td>56.0</td><td>40.5</td><td>20.7</td><td>40.0</td><td>53.5</td></tr><tr><td>AdaMixer (N=100)</td><td>12</td><td>42.7</td><td>61.5</td><td>45.9</td><td>24.7</td><td>45.4</td><td>59.2</td></tr><tr><td>AdaMixer (N=300)</td><td>12</td><td>44.1</td><td>63.4</td><td>47.4</td><td>27.0</td><td>46.9</td><td>59.5</td></tr><tr><td>AdaMixer (N=500)</td><td>12</td><td>45.0</td><td>64.2</td><td>48.6</td><td>27.9</td><td>47.8</td><td>61.1</td></tr></table>", "caption": "Table 3: \\mathbf{1}\\times training scheme performance on COCO minival set with different detectors and ResNet-50 as backbone.", "list_citation_info": ["[3] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: delving into high quality object detection. In CVPR, 2018.", "[33] Han Qiu, Yuchen Ma, Zeming Li, Songtao Liu, and Jian Sun. Borderdet: Border feature for dense object detection. In ECCV, 2020.", "[22] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss V2: learning reliable localization quality estimation for dense object detection. In CVPR, 2021.", "[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable transformers for end-to-end object detection. In ICLR, 2020.", "[39] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse R-CNN: end-to-end object detection with learnable proposals. In CVPR, 2021.", "[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.", "[8] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. In CVPR, 2021.", "[40] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: fully convolutional one-stage object detection. In ICCV, 2019."]}, {"table": "<table><tr><td>detector</td><td>backbone</td><td>encoder/pyramid net</td><td>#epochs</td><td>GFLOPs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{s}</td><td>AP{}_{m}</td><td>AP{}_{l}</td></tr><tr><td>DETR [4]</td><td>ResNet-50-DC5</td><td>TransformerEnc</td><td>500</td><td>187</td><td>43.3</td><td>63.1</td><td>45.9</td><td>22.5</td><td>47.3</td><td>61.1</td></tr><tr><td>SMCA [13]</td><td>ResNet-50</td><td>TransformerEnc</td><td>50</td><td>152</td><td>43.7</td><td>63.6</td><td>47.2</td><td>24.2</td><td>47.0</td><td>60.4</td></tr><tr><td>Deformable DETR [56]</td><td>ResNet-50</td><td>DeformTransEnc</td><td>50</td><td>173</td><td>43.8</td><td>62.6</td><td>47.7</td><td>26.4</td><td>47.1</td><td>58.0</td></tr><tr><td>Sparse R-CNN [39]</td><td>ResNet-50</td><td>FPN</td><td>36</td><td>174</td><td>45.0</td><td>63.4</td><td>48.2</td><td>26.9</td><td>47.2</td><td>59.5</td></tr><tr><td>Efficient DETR [49]</td><td>ResNet-50</td><td>DeformTransEnc</td><td>36</td><td>210</td><td>45.1</td><td>63.1</td><td>49.1</td><td>28.3</td><td>48.4</td><td>59.0</td></tr><tr><td>Conditional DETR [31]</td><td>ResNet-50-DC5</td><td>TransformerEnc</td><td>108</td><td>195</td><td>45.1</td><td>65.4</td><td>48.5</td><td>25.3</td><td>49.0</td><td>62.2</td></tr><tr><td>Anchor DETR [46]</td><td>ResNet-50-DC5</td><td>DecoupTransEnc</td><td>50</td><td>151</td><td>44.2</td><td>64.7</td><td>47.5</td><td>24.7</td><td>48.2</td><td>60.6</td></tr><tr><td>AdaMixer (ours)</td><td>ResNet-50</td><td>-</td><td>12</td><td>132</td><td>44.1</td><td>63.1</td><td>47.8</td><td>29.5</td><td>47.0</td><td>58.8</td></tr><tr><td>AdaMixer (ours)</td><td>ResNet-50</td><td>-</td><td>24</td><td>132</td><td>46.7</td><td>65.9</td><td>50.5</td><td>29.7</td><td>49.7</td><td>61.5</td></tr><tr><td>AdaMixer (ours)</td><td>ResNet-50</td><td>-</td><td>36</td><td>132</td><td>47.0</td><td>66.0</td><td>51.1</td><td>30.1</td><td>50.2</td><td>61.8</td></tr><tr><td>DETR [4]</td><td>ResNet-101-DC5</td><td>TransformerEnc</td><td>500</td><td>253</td><td>44.9</td><td>64.7</td><td>47.7</td><td>23.7</td><td>49.5</td><td>62.3</td></tr><tr><td>SMCA [13]</td><td>ResNet-101</td><td>TransformerEnc</td><td>50</td><td>218</td><td>44.4</td><td>65.2</td><td>48.0</td><td>24.3</td><td>48.5</td><td>61.0</td></tr><tr><td>Sparse R-CNN [39]</td><td>ResNet-101</td><td>FPN</td><td>36</td><td>250</td><td>46.4</td><td>64.6</td><td>49.5</td><td>28.3</td><td>48.3</td><td>61.6</td></tr><tr><td>Efficient DETR [49]</td><td>ResNet-101</td><td>DeformTransEnc</td><td>36</td><td>289</td><td>45.7</td><td>64.1</td><td>49.5</td><td>28.2</td><td>49.1</td><td>60.2</td></tr><tr><td>Conditional DETR [31]</td><td>ResNet-101-DC5</td><td>TransformerEnc</td><td>108</td><td>262</td><td>45.9</td><td>66.8</td><td>49.5</td><td>27.2</td><td>50.3</td><td>63.3</td></tr><tr><td>AdaMixer (ours)</td><td>ResNet-101</td><td>-</td><td>36</td><td>208</td><td>48.0</td><td>67.0</td><td>52.4</td><td>30.0</td><td>51.2</td><td>63.7</td></tr><tr><td>AdaMixer (ours)</td><td>ResNeXt-101-DCN</td><td>-</td><td>36</td><td>214</td><td>49.5</td><td>68.9</td><td>53.9</td><td>31.3</td><td>52.3</td><td>66.3</td></tr><tr><td>AdaMixer (ours)</td><td>Swin-S</td><td>-</td><td>36</td><td>234</td><td>51.3</td><td>71.2</td><td>55.7</td><td>34.2</td><td>54.6</td><td>67.3</td></tr></table>", "caption": "Table 5: Different query-based detector performance on COCO minival set with longer training scheme and single scale testing.", "list_citation_info": ["[49] Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient DETR: improving end-to-end object detector with dense prior. ArXiv, 2021.", "[13] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fast convergence of DETR with spatially modulated co-attention. In ICCV, 2021.", "[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: deformable transformers for end-to-end object detection. In ICLR, 2020.", "[39] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse R-CNN: end-to-end object detection with learnable proposals. In CVPR, 2021.", "[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.", "[31] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional DETR for fast training convergence. In ICCV, 2021.", "[46] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor DETR: query design for transformer-based detector. arXiv, 2021."]}]}