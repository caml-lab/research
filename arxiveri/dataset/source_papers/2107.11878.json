{"title": "Spatio-temporal representation factorization for video-based person re-identification", "abstract": "Despite much recent progress in video-based person re-identification (re-ID), the current state-of-the-art still suffers from common real-world challenges such as appearance similarity among various people, occlusions, and frame misalignment. To alleviate these problems, we propose Spatio-Temporal Representation Factorization (STRF), a flexible new computational unit that can be used in conjunction with most existing 3D convolutional neural network architectures for re-ID. The key innovations of STRF over prior work include explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture complementary person-specific appearance and motion information. Specifically, temporal factorization comprises two branches, one each for static features (e.g., the color of clothes) that do not change much over time, and dynamic features (e.g., walking patterns) that change over time. Further, spatial factorization also comprises two branches to learn both global (coarse segments) as well as local (finer segments) appearance features, with the local features particularly useful in cases of occlusion or spatial misalignment. These two factorization operations taken together result in a modular architecture for our parameter-wise light STRF unit that can be plugged in between any two 3D convolutional layers, resulting in an end-to-end learning framework. We empirically show that STRF improves performance of various existing baseline architectures while demonstrating new state-of-the-art results using standard person re-ID evaluation protocols on three benchmarks.", "authors": ["Abhishek Aich", " Meng Zheng", " Srikrishna Karanam", " Terrence Chen", " Amit K. Roy-Chowdhury", " Ziyan Wu"], "pdf_url": "https://arxiv.org/abs/2107.11878", "list_table_and_caption": [{"table": "<table><tbody><tr><td colspan=\"2\"> METHODS</td><td colspan=\"2\">FACTORIZATION</td><td rowspan=\"2\">\\begin{subarray}{c}\\textbf{WITHOUT}\\\\[2.5pt]\\textbf{NON-LOCAL?}\\end{subarray}</td><td rowspan=\"2\">GENERIC?</td></tr><tr><td></td><td>T?</td><td>S?</td><td></td></tr><tr><td> AP3D [17]</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2713</td></tr><tr><td>MGH [51]</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td></tr><tr><td>AFA [5]</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2713</td></tr><tr><td> STRF (Ours)</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 1: Characteristic comparison with state-of-the-art works. We compare our STRF with few current state-of-the-art works. Unlike these methods, STRF uses factorized information from both spatial (S) and temporal (T) dimensions, does not require non-local operations, and is adaptable to multiple baselines.", "list_citation_info": ["[17] Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, and Xilin Chen. Appearance-Preserving 3D Convolution for Video-based Person Re-identification. In Proceedings of the European Conference of Computer Vision, 2020.", "[5] Guangyi Chen, Yongming Rao, Jiwen Lu, and Jie Zhou. Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification? In Proceedings of the European Conference of Computer Vision, 2020.", "[51] Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, and Ling Shao. Learning Multi-Granular Hypergraphs for Video-based Person Re-Identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2899\u20132908, 2020."]}]}