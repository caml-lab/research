{"title": "Mst: Masked self-supervised transformer for visual representation", "abstract": "Transformer has been widely used for self-supervised pre-training in Natural Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4% and its comparable variant DINO by 1.0\\%. For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch pre-training.", "authors": ["Zhaowen Li", " Zhiyang Chen", " Fan Yang", " Wei Li", " Yousong Zhu", " Chaoyang Zhao", " Rui Deng", " Liwei Wu", " Rui Zhao", " Ming Tang", " Jinqiao Wang"], "pdf_url": "https://arxiv.org/abs/2106.05656", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>Architecture</td><td>Parameters</td><td>epoch</td><td>im/s</td><td>Linear</td><td>k-NN</td></tr><tr><td>Supervised</td><td rowspan=\"4\"> Res50[16] </td><td rowspan=\"4\"> 23 </td><td>100</td><td>1237</td><td>76.5</td><td>-</td></tr><tr><td>MoCov2 [6]</td><td>800</td><td>1237</td><td>71.1</td><td>61.9</td></tr><tr><td>BYOL [13]</td><td>1000</td><td>1237</td><td>74.4</td><td>64.8</td></tr><tr><td>SwAV [1]</td><td>800</td><td>1237</td><td>75.3</td><td>65.7</td></tr><tr><td>Supervised</td><td rowspan=\"15\"> DeiT-S[26] </td><td rowspan=\"15\"> 21 </td><td>300</td><td>1007</td><td>76.4</td><td>-</td></tr><tr><td>SwAV [1]</td><td>300</td><td>1007</td><td>67.1</td><td>-</td></tr><tr><td>SimCLR [4]</td><td>300</td><td>1007</td><td>69.0</td><td>-</td></tr><tr><td>BYOL [13]</td><td>300</td><td>1007</td><td>71.0</td><td>-</td></tr><tr><td>MoCov3 [7]</td><td>300</td><td>1007</td><td>72.5</td><td>-</td></tr><tr><td>MOBY [30]</td><td>300</td><td>1007</td><td>72.8</td><td>-</td></tr><tr><td>BYOL [13]</td><td>800</td><td>1007</td><td>71.4</td><td>66.6</td></tr><tr><td>MoCov2 [6]</td><td>800</td><td>1007</td><td>72.7</td><td>64.4</td></tr><tr><td>SwAV [1]</td><td>800</td><td>1007</td><td>73.5</td><td>66.3</td></tr><tr><td>DINO [2]</td><td>300</td><td>1007</td><td>75.2</td><td>72.8</td></tr><tr><td>DINO{}^{{\\dagger}} [2]</td><td>300</td><td>1007</td><td>75.9</td><td>72.8</td></tr><tr><td>DINO{}^{{\\dagger}} [2]</td><td>800</td><td>1007</td><td>77.0</td><td>74.5</td></tr><tr><td>Ours {}^{{\\dagger}}</td><td>100</td><td>1007</td><td>75.0</td><td>72.1</td></tr><tr><td>Ours</td><td>300</td><td>1007</td><td>76.3</td><td>75.0</td></tr><tr><td>Ours {}^{{\\dagger}}</td><td>300</td><td>1007</td><td>76.9</td><td>75.0</td></tr><tr><td>Supervised</td><td rowspan=\"3\"> Swin-T[20] </td><td rowspan=\"3\"> 28 </td><td>300</td><td>755</td><td>81.2</td><td>-</td></tr><tr><td>MoBY [30]</td><td>100</td><td>755</td><td>70.9</td><td>57.34</td></tr><tr><td>Ours</td><td>100</td><td>755</td><td>73.8</td><td>66.20</td></tr></table>", "caption": "Table 1:  Comparison of popular self-supervise learning methods on ImageNet.Throughput (im/s) is calculated on a single NVIDIA V100 GPU with batch size 128. {}^{{\\dagger}} adopts the linear probing of DINO.", "list_citation_info": ["[1] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning of visual features by contrasting cluster assignments. In: Advances in Neural Information Processing Systems. vol. 33, pp. 9912\u20139924 (2020)", "[30] Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Self-supervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)", "[26] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877 (2020)", "[2] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. arXiv: Computer Vision and Pattern Recognition (2021)", "[13] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P.H., Buchatskaya, E., Doersch, C., Pires, B.A., Guo, Z.D., Azar, M.G., Piot, B., Kavukcuoglu, K., Munos, R., Valko, M.: Bootstrap your own latent: A new approach to self-supervised learning. In: Advances in Neural Information Processing Systems (NeurIPS). vol. 33, pp. 21271\u201321284 (2020)", "[4] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709 (2020)", "[20] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)", "[6] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)", "[7] Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057 (2021)", "[16] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) (2016)"]}, {"table": "<table><tr><td rowspan=\"2\"> Method </td><td rowspan=\"2\"> Backbone </td><td rowspan=\"2\"> Epoch </td><td colspan=\"3\">box AP</td><td colspan=\"3\">mask AP</td></tr><tr><td>AP{}^{\\text{bbox}}</td><td>AP{}^{\\text{bbox}}_{\\text{50}}</td><td>AP{}^{\\text{bbox}}_{\\text{75}}</td><td>AP{}^{\\text{mask}}</td><td>AP{}^{\\text{mask}}_{\\text{50}}</td><td>AP{}^{\\text{mask}}_{\\text{75}}</td></tr><tr><td rowspan=\"2\"> Supervised </td><td rowspan=\"2\"> Swin-T [20] </td><td>300</td><td>43.7</td><td>66.6</td><td>47.7</td><td>39.8</td><td>63.3</td><td>42.7</td></tr><tr><td>100</td><td>41.6</td><td>64.6</td><td>45.4</td><td>38.4</td><td>61.5</td><td>41.0</td></tr><tr><td>MoBY [30]</td><td rowspan=\"3\"> Swin-T [20] </td><td rowspan=\"3\"> 100 </td><td>41.5</td><td>64.1</td><td>45.2</td><td>38.3</td><td>61.0</td><td>40.8</td></tr><tr><td>DINO [2]</td><td>42.2</td><td>64.6</td><td>46.3</td><td>38.7</td><td>61.5</td><td>41.3</td></tr><tr><td>Ours</td><td>42.7</td><td>65.1</td><td>46.7</td><td>38.8</td><td>61.8</td><td>42.5</td></tr></table>", "caption": "Table 2:  Results of object detection and instance segmentation fine-tuned on MS COCO. ", "list_citation_info": ["[20] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)", "[30] Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Self-supervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)", "[2] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. arXiv: Computer Vision and Pattern Recognition (2021)"]}, {"table": "<table><tr><td>Method</td><td>Backbone</td><td>Pre-Epochs</td><td>Schedule</td><td>mIoU</td><td>mAcc</td><td>aAcc</td></tr><tr><td>Supervised</td><td>DeiT-S [26]</td><td>300</td><td> 40K </td><td>71.33</td><td>80.30</td><td>94.99</td></tr><tr><td>DINO [2]</td><td rowspan=\"2\"> DeiT-S [26] </td><td rowspan=\"2\"> 100 </td><td rowspan=\"2\"> 40K </td><td>72.96</td><td>81.32</td><td>95.37</td></tr><tr><td>Ours</td><td>74.04</td><td>82.35</td><td>95.42</td></tr></table>", "caption": "Table 3: Results of semantic segmentation fine-tuned on Cityscapes.", "list_citation_info": ["[26] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877 (2020)", "[2] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. arXiv: Computer Vision and Pattern Recognition (2021)"]}]}