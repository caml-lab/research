{"title": "Disentangled representation learning for text-video retrieval", "abstract": "Cross-modality interaction is a critical component in Text-Video Retrieval (TVR), yet there has been little examination of how different influencing factors for computing interaction affect performance. This paper first studies the interaction paradigm in depth, where we find that its computation can be split into two terms, the interaction contents at different granularity and the matching function to distinguish pairs with the same semantics. We also observe that the single-vector representation and implicit intensive function substantially hinder the optimization. Based on these findings, we propose a disentangled framework to capture a sequential and hierarchical representation. Firstly, considering the natural sequential structure in both text and video inputs, a Weighted Token-wise Interaction (WTI) module is performed to decouple the content and adaptively exploit the pair-wise correlations. This interaction can form a better disentangled manifold for sequential inputs. Secondly, we introduce a Channel DeCorrelation Regularization (CDCR) to minimize the redundancy between the components of the compared vectors, which facilitate learning a hierarchical representation. We demonstrate the effectiveness of the disentangled representation on various benchmarks, e.g., surpassing CLIP4Clip largely by +2.9%, +3.1%, +7.9%, +2.3%, +2.8% and +6.5% R@1 on the MSR-VTT, MSVD, VATEX, LSMDC, AcitivityNet, and DiDeMo, respectively.", "authors": ["Qiang Wang", " Yanhao Zhang", " Yun Zheng", " Pan Pan", " Xian-Sheng Hua"], "pdf_url": "https://arxiv.org/abs/2203.07111", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Interaction</td><td colspan=\"4\">InfoNCE [31]</td><td colspan=\"4\">+CDCL</td><td rowspan=\"2\">Time(ms)</td></tr><tr><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MnR\\downarrow</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MnR\\downarrow</td></tr><tr><td>DP</td><td>42.8</td><td>72.1</td><td>81.4</td><td>16.3</td><td>44.2</td><td>72.7</td><td>82.0</td><td>14.5</td><td>415</td></tr><tr><td>HI</td><td>43.5</td><td>72.9</td><td>81.7</td><td>16.1</td><td>44.1</td><td>72.6</td><td>82.8</td><td>14.1</td><td>531</td></tr><tr><td>MLP</td><td>29.3</td><td>54.8</td><td>64.2</td><td>33.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>25,304</td></tr><tr><td>XTI</td><td>41.8</td><td>71.2</td><td>82.7</td><td>16.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>80,453</td></tr><tr><td>TI</td><td>44.8</td><td>73.7</td><td>82.9</td><td>13.5</td><td>45.5</td><td>72.0</td><td>82.5</td><td>13.3</td><td>536</td></tr><tr><td>WTI</td><td>46.3</td><td>73.7</td><td>83.2</td><td>13.0</td><td>47.4</td><td>74.6</td><td>83.8</td><td>12.8</td><td>565</td></tr><tr><td>DP{}_{\\text{+ViT-B/16}}</td><td>45.9</td><td>73.8</td><td>82.3</td><td>13.8</td><td>46.6</td><td>73.3</td><td>82.8</td><td>13.4</td><td>415</td></tr><tr><td>TI{}_{\\text{+ViT-B/16}}</td><td>47.3</td><td>76.7</td><td>84.8</td><td>13.7</td><td>49.1</td><td>75.7</td><td>85.1</td><td>12.7</td><td>536</td></tr><tr><td>WTI{}_{\\text{+ViT-B/16}}</td><td>48.8</td><td>76.1</td><td>84.3</td><td>13.5</td><td>50.2</td><td>76.5</td><td>84.7</td><td>12.4</td><td>565</td></tr></tbody></table>", "caption": "Table 2: Ablation of Disentangled Representation on the 1K validation set of MSR-VTT [44]. Time shows inference speed for indexing 1 million video documents on a Tesla V100 GPU.", "list_citation_info": ["[44] Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for bridging video and language. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288\u20135296 (2016)", "[31] Van den Oord, A., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv e-prints pp. arXiv\u20131807 (2018)"]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Dual</th><td colspan=\"4\">t2v</td><td colspan=\"4\">v2t</td><td colspan=\"4\">t2v+v2t</td></tr><tr><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MnR\\downarrow</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MnR\\downarrow</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MnR\\downarrow</td></tr><tr><th>TI</th><td>43.4</td><td>71.0</td><td>80.2</td><td>16.1</td><td>43.5</td><td>71.8</td><td>82.7</td><td>14.0</td><td>44.8</td><td>73.7</td><td>82.9</td><td>13.5</td></tr><tr><th>WTI</th><td>45.4</td><td>72.3</td><td>81.7</td><td>13.4</td><td>45.3</td><td>74.6</td><td>83.3</td><td>13.6</td><td>46.3</td><td>73.7</td><td>83.2</td><td>13.0</td></tr><tr><th>Layers</th><td colspan=\"4\">1FC</td><td colspan=\"4\">2FC</td><td colspan=\"4\">3FC</td></tr><tr><th>WTI</th><td>46.3</td><td>73.9</td><td>82.9</td><td>13.7</td><td>46.3</td><td>73.7</td><td>83.2</td><td>13.0</td><td>45.7</td><td>72.9</td><td>81.4</td><td>14.0</td></tr></tbody></table>", "caption": "Table 3: Effect of dual-path and layers for weight model on MSR-VTT 1K [44]. ", "list_citation_info": ["[44] Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for bridging video and language. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288\u20135296 (2016)"]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"4\">Text\\rightarrow Video</td><td colspan=\"4\">Video \\rightarrowText</td></tr><tr><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><th>HERO [23]</th><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>UniVL [28]</th><td>21.2</td><td>49.6</td><td>63.1</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ClipBERT [21]</th><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MDMMT [12]</th><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td><td>27.0</td><td>57.5</td><td>69.7</td><td>3.7</td></tr><tr><th>SUPPORT [33]</th><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td><td>26.6</td><td>55.1</td><td>67.5</td><td>3.0</td></tr><tr><th>FROZEN [2]</th><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP4Clip [29]</th><td>44.5</td><td>71.4</td><td>81.6</td><td>2.0</td><td>42.7</td><td>70.9</td><td>80.6</td><td>2.0</td></tr><tr><th>Ours</th><td>47.4</td><td>74.6</td><td>83.8</td><td>2.0</td><td>45.3</td><td>73.9</td><td>83.3</td><td>2.0</td></tr><tr><th>+ViT-B/16</th><td>50.2</td><td>76.5</td><td>84.7</td><td>1.0</td><td>48.9</td><td>76.3</td><td>85.4</td><td>2.0</td></tr><tr><th>+QB-Norm [3]</th><td>53.3</td><td>80.3</td><td>87.6</td><td>1.0</td><td>56.2</td><td>79.9</td><td>87.4</td><td>1.0</td></tr></tbody></table>", "caption": "Table 4: Retrieval results on the validation set of MSR-VTT 1K [44].", "list_citation_info": ["[23] Li, L., Chen, Y.C., Cheng, Y., Gan, Z., Yu, L., Liu, J.: Hero: Hierarchical encoder for video+ language omni-representation pre-training. In: EMNLP (2020)", "[12] Gabeur, V., Sun, C., Alahari, K., Schmid, C.: Multi-modal transformer for video retrieval. In: European Conference on Computer Vision. pp. 214\u2013229. Springer (2020)", "[33] Patrick, M., Huang, P.Y., Asano, Y., Metze, F., Hauptmann, A., Henriques, J., Vedaldi, A.: Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824 (2020)", "[28] Luo, H., Ji, L., Shi, B., Huang, H., Duan, N., Li, T., Li, J., Bharti, T., Zhou, M.: Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353 (2020)", "[21] Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T.L., Bansal, M., Liu, J.: Less is more: Clipbert for video-and-language learning via sparse sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7331\u20137341 (2021)", "[3] Bogolin, S.V., Croitoru, I., Jin, H., Liu, Y., Albanie, S.: Cross modal retrieval with querybank normalisation. arXiv preprint arXiv:2112.12777 (2021)", "[2] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1728\u20131738 (2021)", "[44] Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for bridging video and language. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288\u20135296 (2016)", "[29] Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860 (2021)"]}, {"table": "<table><tbody><tr><th></th><td colspan=\"4\">Text\\rightarrow Video</td><td colspan=\"4\">Video \\rightarrowText</td></tr><tr><th>Method</th><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><th colspan=\"9\">Retrieval performance on MSVD [43]</th></tr><tr><th>CE [26]</th><td>19.8</td><td>49.0</td><td>63.8</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SUPPORT [33]</th><td>28.4</td><td>60.0</td><td>72.9</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>FROZEN[2]</th><td>33.7</td><td>64.7</td><td>76.3</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP [35]</th><td>37.0</td><td>64.1</td><td>73.8</td><td>3.0</td><td>54.9</td><td>82.9</td><td>89.6</td><td>1.0</td></tr><tr><th>CLIP4Clip[29]</th><td>45.2</td><td>75.5</td><td>84.3</td><td>2.0</td><td>48.4</td><td>70.3</td><td>77.2</td><td>2.0</td></tr><tr><th>Ours</th><td>48.3</td><td>79.1</td><td>87.3</td><td>2.0</td><td>62.3</td><td>86.3</td><td>92.2</td><td>1.0</td></tr><tr><th>Ours{}_{\\text{+ViT-B/16}}</th><td>50.0</td><td>81.5</td><td>89.5</td><td>2.0</td><td>68.7</td><td>92.5</td><td>95.6</td><td>1.0</td></tr><tr><th colspan=\"9\">Retrieval performance on VATEX [42]</th></tr><tr><th>CLIP [35]</th><td>39.7</td><td>72.3</td><td>82.2</td><td>2.0</td><td>52.7</td><td>88.8</td><td>94.9</td><td>1.0</td></tr><tr><th>SUPPORT [33]</th><td>44.9</td><td>82.1</td><td>89.7</td><td>1.0</td><td>58.4</td><td>84.4</td><td>91.0</td><td>1.0</td></tr><tr><th>CLIP4Clip [29]</th><td>55.9</td><td>89.2</td><td>95.0</td><td>1.0</td><td>73.2</td><td>97.1</td><td>99.1</td><td>1.0</td></tr><tr><th>Ours</th><td>63.5</td><td>91.7</td><td>96.5</td><td>1.0</td><td>77.0</td><td>98.0</td><td>99.4</td><td>1.0</td></tr><tr><th>Ours{}_{\\text{+ViT-B/16}}</th><td>65.7</td><td>92.6</td><td>96.7</td><td>1.0</td><td>80.1</td><td>98.5</td><td>99.5</td><td>1.0</td></tr><tr><th colspan=\"9\">Retrieval performance on LSMDC [39]</th></tr><tr><th>CE [26]</th><td>11.2</td><td>26.9</td><td>34.8</td><td>25.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MMT [12]</th><td>12.9</td><td>29.9</td><td>40.1</td><td>19.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP [35]</th><td>11.3</td><td>22.7</td><td>29.2</td><td>46.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MDMMT [12]</th><td>18.8</td><td>38.5</td><td>47.9</td><td>12.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP4Clip [29]</th><td>22.6</td><td>41.0</td><td>49.1</td><td>11.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>24.9</td><td>45.7</td><td>55.3</td><td>7.0</td><td>24.9</td><td>44.1</td><td>53.8</td><td>9.0</td></tr><tr><th>Ours{}_{\\text{+ViT-B/16}}</th><td>26.5</td><td>47.6</td><td>56.8</td><td>7.0</td><td>27.0</td><td>45.7</td><td>55.4</td><td>8.0</td></tr><tr><th colspan=\"9\">Retrieval performance on ActivityNet [10]</th></tr><tr><th>CE [26]</th><td>17.7</td><td>46.6</td><td>-</td><td>6.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MMT [12]</th><td>28.9</td><td>61.1</td><td>-</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SUPPORT[33]</th><td>28.7</td><td>60.8</td><td>-</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP4Clip [29]</th><td>41.4</td><td>73.7</td><td>85.3</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><th>Ours</th><td>44.2</td><td>74.5</td><td>86.1</td><td>2.0</td><td>42.2</td><td>74.0</td><td>86.2</td><td>2.0</td></tr><tr><th>Ours{}_{\\text{+ViT-B/16}}</th><td>46.2</td><td>77.3</td><td>88.2</td><td>2.0</td><td>45.7</td><td>76.5</td><td>87.8</td><td>2.0</td></tr><tr><th colspan=\"9\">Retrieval performance on DiDeMo [1]</th></tr><tr><th>CE [26]</th><td>15.6</td><td>40.9</td><td>-</td><td>8.2</td><td>27.2</td><td>51.7</td><td>62.6</td><td>5.0</td></tr><tr><th>ClipBERT [21]</th><td>21.1</td><td>47.3</td><td>61.1</td><td>6.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>FROZEN [2]</th><td>31.0</td><td>59.8</td><td>72.4</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP4Clip [29]</th><td>41.4</td><td>68.2</td><td>79.1</td><td>2.0</td><td>42.8</td><td>69.8</td><td>79.0</td><td>2.0</td></tr><tr><th>Ours</th><td>47.9</td><td>73.8</td><td>82.7</td><td>2.0</td><td>45.4</td><td>72.6</td><td>82.1</td><td>2.0</td></tr><tr><th>Ours{}_{\\text{+ViT-B/16}}</th><td>49.0</td><td>76.5</td><td>84.5</td><td>2.0</td><td>49.9</td><td>75.4</td><td>83.3</td><td>2.0</td></tr></tbody></table>", "caption": "Table 5: Retrieval results on the validation set of MSVD [43], VATEX [42], LSMDC [39], ActivityNet [10] and DiDeMo [1].", "list_citation_info": ["[39] Rohrbach, A., Rohrbach, M., Tandon, N., Schiele, B.: A dataset for movie description. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3202\u20133212 (2015)", "[42] Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.F., Wang, W.Y.: Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4581\u20134591 (2019)", "[10] Fabian Caba Heilbron, Victor Escorcia, B.G., Niebles, J.C.: Activitynet: A large-scale video benchmark for human activity understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 961\u2013970 (2015)", "[33] Patrick, M., Huang, P.Y., Asano, Y., Metze, F., Hauptmann, A., Henriques, J., Vedaldi, A.: Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824 (2020)", "[12] Gabeur, V., Sun, C., Alahari, K., Schmid, C.: Multi-modal transformer for video retrieval. In: European Conference on Computer Vision. pp. 214\u2013229. Springer (2020)", "[21] Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T.L., Bansal, M., Liu, J.: Less is more: Clipbert for video-and-language learning via sparse sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7331\u20137341 (2021)", "[43] Wu, Z., Yao, T., Fu, Y., Jiang, Y.G.: Deep learning for video classification and captioning. In: Frontiers of multimedia research, pp. 3\u201329 (2017)", "[26] Liu, Y., Albanie, S., Nagrani, A., Zisserman, A.: Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487 (2019)", "[2] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1728\u20131738 (2021)", "[1] Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.: Localizing moments in video with natural language. In: Proceedings of the IEEE international conference on computer vision. pp. 5803\u20135812 (2017)", "[35] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020 (2021)", "[29] Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860 (2021)"]}]}