{"title": "Cross modal retrieval with querybank normalisation", "abstract": "Profiting from large-scale training datasets, advances in neural architecture design and efficient inference, joint embeddings have become the dominant approach for tackling cross-modal retrieval. In this work we first show that, despite their effectiveness, state-of-the-art joint embeddings suffer significantly from the longstanding \"hubness problem\" in which a small number of gallery embeddings form the nearest neighbours of many queries. Drawing inspiration from the NLP literature, we formulate a simple but effective framework called Querybank Normalisation (QB-Norm) that re-normalises query similarities to account for hubs in the embedding space. QB-Norm improves retrieval performance without requiring retraining. Differently from prior work, we show that QB-Norm works effectively without concurrent access to any test set queries. Within the QB-Norm framework, we also propose a novel similarity normalisation method, the Dynamic Inverted Softmax, that is significantly more robust than existing approaches. We showcase QB-Norm across a range of cross modal retrieval models and benchmarks where it consistently enhances strong baselines beyond the state of the art. Code is available at https://vladbogo.github.io/QB-Norm/.", "authors": ["Simion-Vlad Bogolin", " Ioana Croitoru", " Hailin Jin", " Yang Liu", " Samuel Albanie"], "pdf_url": "https://arxiv.org/abs/2112.12777", "list_table_and_caption": [{"table": "<table><tr><td>Querybank Source</td><td>Size</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>No querybank</td><td>-</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.5_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>Training set</td><td>60k</td><td>17.3_{\\pm 0.0}</td><td>42.1_{\\pm 0.1}</td><td>54.9_{\\pm 0.0}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>Val set</td><td>10k</td><td>16.6_{\\pm 0.1}</td><td>40.8_{\\pm 0.1}</td><td>53.7_{\\pm 0.1}</td><td>9.0_{\\pm 0.0}</td></tr><tr><td>Test set</td><td>60k</td><td>17.5_{\\pm 0.0}</td><td>42.4_{\\pm 0.1}</td><td>55.1_{\\pm 0.0}</td><td>8.0_{\\pm 0.0}</td></tr></table>", "caption": "Table 1: Effective querybanks can be constructedfrom the training set.Performance is reported on MSR-VTTfull split [121].We observe that a querybank of 60K samples from the trainingset performs comparably to a test set querybank.", "list_citation_info": ["[121] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016."]}, {"table": "<table><tr><td> QB SourceData </td><td>Normalisation</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>No QB</td><td>-</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.5_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>In Domain</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MSR-VTT</td><td>QB-Norm (GC [28])</td><td>15.8_{\\pm 0.0}</td><td>39.1_{\\pm 0.0}</td><td>51.8_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>QB-Norm (CSLS [25])</td><td>16.8_{\\pm 0.1}</td><td>41.5_{\\pm 0.1}</td><td>54.4_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>QB-Norm (IS [102])</td><td>17.1_{\\pm 0.1}</td><td>41.9_{\\pm 0.2}</td><td>54.7_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>QB-Norm (DIS)</td><td>17.0_{\\pm 0.1}</td><td>41.3_{\\pm 0.1}</td><td>54.1_{\\pm 0.1}</td><td>8.6_{\\pm 0.5}</td></tr><tr><td>Close Domain</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MSVD</td><td>QB-Norm (GC [28])</td><td>15.2_{\\pm 0.1}</td><td>38.8_{\\pm 0.0}</td><td>51.7_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>MSVD</td><td>QB-Norm (CSLS [25])</td><td>16.5_{\\pm 0.0}</td><td>41.2_{\\pm 0.0}</td><td>54.1_{\\pm 0.1}</td><td>9.0_{\\pm 0.0}</td></tr><tr><td>MSVD</td><td>QB-Norm (IS [102])</td><td>16.4_{\\pm 0.2}</td><td>40.9_{\\pm 0.2}</td><td>53.9_{\\pm 0.1}</td><td>9.0_{\\pm 0.0}</td></tr><tr><td>MSVD</td><td>QB-Norm (DIS)</td><td>16.7_{\\pm 0.1}</td><td>41.1_{\\pm 0.1}</td><td>54.0_{\\pm 0.0}</td><td>9.0_{\\pm 0.0}</td></tr><tr><td>Far Domain</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LSMDC</td><td>QB-Norm (GC [28])</td><td>14.8_{\\pm 0.1}</td><td>38.2_{\\pm 0.0}</td><td>51.4_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>LSMDC</td><td>QB-Norm (CSLS [25])</td><td>13.4_{\\pm 0.0}</td><td>35.9_{\\pm 0.0}</td><td>48.5_{\\pm 0.0}</td><td>11.0_{\\pm 0.0}</td></tr><tr><td>LSMDC</td><td>QB-Norm (IS [102])</td><td>11.6_{\\pm 0.0}</td><td>32.5_{\\pm 0.0}</td><td>44.6_{\\pm 0.0}</td><td>14.0_{\\pm 0.0}</td></tr><tr><td>LSMDC</td><td>QB-Norm (DIS)</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.2_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>Adversarial</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MSR-VTT</td><td>QB-Norm (GC [28])</td><td>14.5_{\\pm 0.0}</td><td>38.1_{\\pm 0.0}</td><td>51.4_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>QB-Norm (CSLS [25])</td><td>14.4_{\\pm 0.1}</td><td>37.5_{\\pm 0.1}</td><td>50.4_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>QB-Norm (IS [102])</td><td>12.3_{\\pm 0.1}</td><td>32.9_{\\pm 0.1}</td><td>45.0_{\\pm 0.0}</td><td>14.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>QB-Norm (DIS)</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.5_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>Overall</td><td></td><td> GM(R@1) </td><td> GM(R@5) </td><td> GM(R@10) </td><td> GM(MdR) </td></tr><tr><td>Summary</td><td>QB-Norm (GC [28])</td><td>15.1_{\\pm 0.6}</td><td>38.5_{\\pm 0.5}</td><td>51.6_{\\pm 0.2}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>Summary</td><td>QB-Norm (CSLS [25])</td><td>15.2_{\\pm 1.6}</td><td>39.0_{\\pm 2.8}</td><td>51.8_{\\pm 2.9}</td><td>\\mathbf{9.4}_{\\pm 1.3}</td></tr><tr><td>Summary</td><td>QB-Norm (IS [102])</td><td>14.1_{\\pm 2.8}</td><td>36.8_{\\pm 5.0}</td><td>49.3_{\\pm 5.5}</td><td>10.9_{\\pm 3.2}</td></tr><tr><td>Summary</td><td>QB-Norm (DIS)</td><td>\\mathbf{15.8}_{\\pm 1.1}</td><td>\\mathbf{39.7}_{\\pm 1.7}</td><td>\\mathbf{52.7}_{\\pm 1.6}</td><td>\\mathbf{9.4}_{\\pm 0.7}</td></tr></table>", "caption": "Table 2: The influence of normalisation strategiesacrossquerybank source distributions.Performance is reported on MSR-VTT full split [121],while querybanks of 5,000 samplesare sampled from the training sets of different datasets.In the last block, we presented the overall performance reported as geometric mean (GM) for each method.We observe that DIS provides the best overall trade-off:it matches the high performance of IS and CSLSwith in domain and close domainquerybanks, and is more robust on far domainand adversarial querybanks.", "list_citation_info": ["[102] Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. arXiv preprint arXiv:1702.03859, 2017.", "[25] Alexis Conneau, Guillaume Lample, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. Word translation without parallel data. In International Conference on Learning Representations, 2018.", "[28] Georgiana Dinu, Angeliki Lazaridou, and Marco Baroni. Improving zero-shot learning by mitigating the hubness problem. arXiv preprint arXiv:1412.6568, 2014.", "[121] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>CE[73]</td><td>21.7_{\\pm 1.3}</td><td>51.8_{\\pm 0.5}</td><td>65.7_{\\pm 0.6}</td><td>5.0_{\\pm 0.0}</td></tr><tr><td>MMT[41]</td><td>24.6_{\\pm 0.4}</td><td>54.0_{\\pm 0.2}</td><td>67.1_{\\pm 0.5}</td><td>4.0_{\\pm 0.0}</td></tr><tr><td>SSB[89]</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td></tr><tr><td>Frozen[6]</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>CLIP4Clip [76]</td><td>44.5</td><td>71.4</td><td>81.6</td><td>\\mathbf{2.0}</td></tr><tr><td>TT-CE+[27]</td><td>29.6_{\\pm 0.3}</td><td>61.6_{\\pm 0.5}</td><td>74.2_{\\pm 0.3}</td><td>3.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>33.3_{\\pm 0.7}</td><td>63.7_{\\pm 0.1}</td><td>76.3_{\\pm 0.4}</td><td>3.0_{\\pm 0.0}</td></tr><tr><td>CLIP2Video[35]</td><td>45.6</td><td>72.5</td><td>81.7</td><td>\\mathbf{2.0}</td></tr><tr><td>CLIP2Video (+QB-Norm)</td><td>\\mathbf{47.2}</td><td>\\mathbf{73.0}</td><td>\\mathbf{83.0}</td><td>\\mathbf{2.0}</td></tr></table>", "caption": "Table 4: MSR-VTT 1k-A split: Comparison to state of the art. ", "list_citation_info": ["[35] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.", "[41] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. European Conference on Computer Vision, 2020.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.", "[6] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. 2021.", "[89] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>VSE++[33]</td><td>15.4</td><td>39.6</td><td>53.0</td><td>9.0</td></tr><tr><td>MoEE[79]</td><td>21.1_{\\pm 0.2}</td><td>52.0_{\\pm 0.7}</td><td>66.7_{\\pm 0.2}</td><td>5.0_{\\pm 0.0}</td></tr><tr><td>CE[73]</td><td>21.5_{\\pm 0.5}</td><td>52.3_{\\pm 0.8}</td><td>67.5_{\\pm 0.7}</td><td>5.0_{\\pm 0.0}</td></tr><tr><td>Frozen[6]</td><td>33.7</td><td>64.7</td><td>76.3</td><td>3.0</td></tr><tr><td>CLIP4Clip [76]</td><td>46.2</td><td>76.1</td><td>84.6</td><td>\\mathbf{2.0}</td></tr><tr><td>TT-CE+[27]</td><td>25.4_{\\pm 0.3}</td><td>56.9_{\\pm 0.4}</td><td>71.3_{\\pm 0.2}</td><td>4.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>26.6_{\\pm 0.9}</td><td>58.5_{\\pm 1.3}</td><td>71.8_{\\pm 1.1}</td><td>4.0_{\\pm 0.0}</td></tr><tr><td>CLIP2Video[35]</td><td>47.0</td><td>76.8</td><td>85.9</td><td>\\mathbf{2.0}</td></tr><tr><td>CLIP2Video (+QB-Norm)</td><td>\\mathbf{47.6}</td><td>\\mathbf{77.6}</td><td>\\mathbf{86.1}</td><td>\\mathbf{2.0}</td></tr></table>", "caption": "Table 5: MSVD: Comparison to state of the art methods. ", "list_citation_info": ["[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[35] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.", "[33] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[6] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>MoEE[79]</td><td>16.1_{\\pm 1.0}</td><td>41.2_{\\pm 1.6}</td><td>55.2_{\\pm 1.6}</td><td>8.3_{\\pm 0.5}</td></tr><tr><td>CE[73]</td><td>17.1_{\\pm 0.9}</td><td>41.9_{\\pm 0.2}</td><td>56.0_{\\pm 0.5}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>TT-CE</td><td>21.0_{\\pm 0.6}</td><td>47.5_{\\pm 0.9}</td><td>61.9_{\\pm 0.5}</td><td>6.0_{\\pm 0.0}</td></tr><tr><td>Frozen[6]</td><td>31.0</td><td>59.8</td><td>72.4</td><td>3.0</td></tr><tr><td>CLIP4Clip [76]</td><td>\\mathbf{43.4}</td><td>\\mathbf{70.2}</td><td>\\mathbf{80.6}</td><td>\\mathbf{2.0}</td></tr><tr><td>CE+[27]</td><td>18.2_{\\pm 0.2}</td><td>43.9_{\\pm 0.9}</td><td>57.1_{\\pm 0.8}</td><td>7.9_{\\pm 0.1}</td></tr><tr><td>CE+ (+QB-Norm)</td><td>20.7_{\\pm 0.6}</td><td>46.6_{\\pm 0.2}</td><td>59.8_{\\pm 0.2}</td><td>6.3_{\\pm 0.5}</td></tr><tr><td>TT-CE+[27]</td><td>21.6_{\\pm 0.7}</td><td>48.6_{\\pm 0.4}</td><td>62.9_{\\pm 0.6}</td><td>6.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>24.2_{\\pm 0.7}</td><td>50.8_{\\pm 0.7}</td><td>64.4_{\\pm 0.1}</td><td>5.3_{\\pm 0.5}</td></tr></table>", "caption": "Table 6: DiDeMo: Comparison to state of the art methods.", "list_citation_info": ["[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[6] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>MoEE[79]</td><td>12.1_{\\pm 0.7}</td><td>29.4_{\\pm 0.8}</td><td>37.7_{\\pm 0.2}</td><td>23.2_{\\pm 0.8}</td></tr><tr><td>CE[73]</td><td>12.4_{\\pm 0.7}</td><td>28.5_{\\pm 0.8}</td><td>37.9_{\\pm 0.6}</td><td>21.7_{\\pm 0.6}</td></tr><tr><td>MMT[41]</td><td>13.2_{\\pm 0.4}</td><td>29.2_{\\pm 0.8}</td><td>38.8_{\\pm 0.9}</td><td>21.0_{\\pm 1.4}</td></tr><tr><td>Frozen[6]</td><td>15.0</td><td>30.8</td><td>39.8</td><td>20.0</td></tr><tr><td>CLIP4Clip [76]</td><td>\\mathbf{21.6}</td><td>\\mathbf{41.8}</td><td>\\mathbf{49.8}</td><td>\\mathbf{11.0}</td></tr><tr><td>CE+[27]</td><td>14.9_{\\pm 0.6}</td><td>33.7_{\\pm 0.2}</td><td>44.1_{\\pm 0.6}</td><td>15.3_{\\pm 0.5}</td></tr><tr><td>CE+ (QB-Norm)</td><td>16.4_{\\pm 0.8}</td><td>34.8_{\\pm 0.4}</td><td>44.9_{\\pm 0.9}</td><td>14.5_{\\pm 0.4}</td></tr><tr><td>TT-CE+[27]</td><td>17.2_{\\pm 0.4}</td><td>36.5_{\\pm 0.6}</td><td>46.3_{\\pm 0.3}</td><td>13.7_{\\pm 0.5}</td></tr><tr><td>TT-CE+ (QB-Norm)</td><td>17.8_{\\pm 0.4}</td><td>37.7_{\\pm 0.5}</td><td>47.6_{\\pm 0.6}</td><td>12.7_{\\pm 0.5}</td></tr></table>", "caption": "Table 7: LSMDC: Comparison to state of the art methods.", "list_citation_info": ["[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[41] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. European Conference on Computer Vision, 2020.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.", "[6] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. 2021."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>HGR[19]</td><td>35.1</td><td>73.5</td><td>83.5</td><td>2.0</td></tr><tr><td>SSB[89]</td><td>44.6</td><td>81.8</td><td>89.5</td><td>\\mathbf{1.0}</td></tr><tr><td>CE[73]</td><td>47.9_{\\pm 0.1}</td><td>84.2_{\\pm 0.1}</td><td>91.3_{\\pm 0.1}</td><td>2.0_{\\pm 0.0}</td></tr><tr><td>Fast and Slow [78]</td><td>50.5</td><td>84.6</td><td>91.7</td><td>-</td></tr><tr><td>TT-CE+[27]</td><td>53.2_{\\pm 0.2}</td><td>87.4_{\\pm 0.1}</td><td>93.3_{\\pm 0.0}</td><td>\\mathbf{1.0}_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>54.8_{\\pm 0.1}</td><td>88.2_{\\pm 0.1}</td><td>\\mathbf{93.8}_{\\pm 0.1}</td><td>\\mathbf{1.0}_{\\pm 0.0}</td></tr><tr><td>CLIP2Video[35]</td><td>57.4</td><td>87.9</td><td>93.6</td><td>\\mathbf{1.0}</td></tr><tr><td>CLIP2Video (+QB-Norm)</td><td>\\mathbf{58.8}</td><td>\\mathbf{88.3}</td><td>\\mathbf{93.8}</td><td>\\mathbf{1.0}</td></tr></table>", "caption": "Table 8: VaTeX: Comparison to state of the art methods. ", "list_citation_info": ["[78] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. Thinking fast and slow: Efficient text-to-visual retrieval with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9826\u20139836, 2021.", "[35] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.", "[19] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10638\u201310647, 2020.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.", "[89] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>MoEE[79]</td><td>11.6_{\\pm 1.3}</td><td>30.2_{\\pm 3.0}</td><td>43.2_{\\pm 3.1}</td><td>14.2_{\\pm 1.6}</td></tr><tr><td>CE[73]</td><td>13.9_{\\pm 0.8}</td><td>37.6_{\\pm 1.2}</td><td>48.3_{\\pm 1.4}</td><td>11.3_{\\pm 0.6}</td></tr><tr><td>CE+[27]</td><td>13.2_{\\pm 2.0}</td><td>37.1_{\\pm 2.9}</td><td>50.5_{\\pm 1.9}</td><td>10.3_{\\pm 1.2}</td></tr><tr><td>CE+ (+QB-Norm)</td><td>14.1_{\\pm 1.8}</td><td>\\mathbf{38.6}_{\\pm 1.3}</td><td>51.1_{\\pm 1.6}</td><td>10.0_{\\pm 0.8}</td></tr><tr><td>TT-CE+[27]</td><td>14.4_{\\pm 0.5}</td><td>37.7_{\\pm 1.7}</td><td>50.9_{\\pm 1.6}</td><td>\\mathbf{9.8}_{\\pm 1.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>\\mathbf{15.1}_{\\pm 1.6}</td><td>38.3_{\\pm 2.4}</td><td>\\mathbf{51.2}_{\\pm 2.8}</td><td>10.3_{\\pm 1.7}</td></tr></table>", "caption": "Table 9: QuerYD: Comparison to state of the art methods. ", "list_citation_info": ["[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.", "[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>CLIP [91]</td><td>37.8</td><td>62.4</td><td>72.2</td><td>-</td></tr><tr><td>VSE++ [33]</td><td>43.9</td><td>59.4</td><td>72.4</td><td>-</td></tr><tr><td>OSCAR [69]</td><td>54.0</td><td>80.8</td><td>88.5</td><td>-</td></tr><tr><td>VinVL [130]</td><td>58.8</td><td>83.5</td><td>90.3</td><td>-</td></tr><tr><td>Fast and Slow [78]</td><td>\\mathbf{68.2}</td><td>\\mathbf{89.7}</td><td>\\mathbf{93.9}</td><td>-</td></tr><tr><td>CLIP[91]{}^{\\ddagger}</td><td>30.3</td><td>56.1</td><td>67.1</td><td>4.0</td></tr><tr><td>CLIP{}^{\\ddagger} (+QB-Norm)</td><td>34.8</td><td>59.9</td><td>70.4</td><td>3.0</td></tr><tr><td>MMT-OSCAR [42]</td><td>52.2</td><td>80.2</td><td>88.0</td><td>\\mathbf{1.0}</td></tr><tr><td>MMT-Oscar (+QB-Norm)</td><td>53.9</td><td>80.5</td><td>88.1</td><td>\\mathbf{1.0}</td></tr></table>", "caption": "Table 10: Text-image retrieval - MSCoCo 5k split: Comparison to other methods. {}^{\\ddagger} represents the results obtained using the official CLIP[91] ViT-B/32 model. ", "list_citation_info": ["[78] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. Thinking fast and slow: Efficient text-to-visual retrieval with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9826\u20139836, 2021.", "[130] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. CVPR 2021, 2021.", "[91] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.", "[33] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017.", "[69] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. ECCV 2020, 2020.", "[42] Gregor Geigle, Jonas Pfeiffer, Nils Reimers, Ivan Vuli\u0107, and Iryna Gurevych. Retrieve fast, rerank smart: Cooperative and joint approaches for improved cross-modal retrieval. arXiv preprint arXiv:2103.11920, 2021."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@2\\uparrow</td><td>R@4\\uparrow</td><td>R@8\\uparrow</td></tr><tr><td>MS [113]</td><td>57.4</td><td>69.8</td><td>80.0</td><td>-</td></tr><tr><td>EPS [68]</td><td>64.4</td><td>75.2</td><td>\\mathbf{84.3}</td><td>-</td></tr><tr><td>RDML[97]</td><td>64.4</td><td>75.3</td><td>83.4</td><td>90.0</td></tr><tr><td>RDML[97] (+QB-Norm)</td><td>\\mathbf{64.8}</td><td>\\mathbf{75.6}</td><td>84.0</td><td>\\mathbf{90.4}</td></tr></table>", "caption": "Table 11: Image to Image retrieval - CUB 200: Comparison to other methods. ", "list_citation_info": ["[68] Elad Levi, Tete Xiao, Xiaolong Wang, and Trevor Darrell. Rethinking preventing class-collapsing in metric learning with margin-based losses. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10316\u201310325, 2021.", "[113] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss with general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5022\u20135030, 2019.", "[97] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In International Conference on Machine Learning, pages 8242\u20138252. PMLR, 2020."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@10\\uparrow</td><td>R@100\\uparrow</td><td>R@1000\\uparrow</td></tr><tr><td>XBM [115]</td><td>\\mathbf{80.6}</td><td>\\mathbf{91.6}</td><td>96.2</td><td>98.7</td></tr><tr><td>Smooth-AP[11]</td><td>80.1</td><td>91.5</td><td>\\mathbf{96.6}</td><td>\\mathbf{99.0}</td></tr><tr><td>RDML[97]</td><td>77.8</td><td>89.5</td><td>95.4</td><td>98.4</td></tr><tr><td>RDML[97] (+QB-Norm)</td><td>78.1</td><td>89.8</td><td>95.6</td><td>98.5</td></tr></table>", "caption": "Table 12: Image to Image retrieval - Online Products: Comparison to other methods. ", "list_citation_info": ["[115] Xun Wang, Haozhi Zhang, Weilin Huang, and Matthew R Scott. Cross-batch memory for embedding learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6388\u20136397, 2020.", "[11] Andrew Brown, Weidi Xie, Vicky Kalogeiton, and Andrew Zisserman. Smooth-ap: Smoothing the path towards large-scale image retrieval. In European Conference on Computer Vision, pages 677\u2013694. Springer, 2020.", "[97] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In International Conference on Machine Learning, pages 8242\u20138252. PMLR, 2020."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>AR [86]-MoEE</td><td>22.5_{\\pm 0.3}</td><td>54.4_{\\pm 0.6}</td><td>69.5_{\\pm 0.9}</td><td>5.0_{\\pm 0.0}</td></tr><tr><td>AR [86]-CE</td><td>23.1_{\\pm 0.6}</td><td>55.1_{\\pm 0.7}</td><td>70.7_{\\pm 0.6}</td><td>4.7_{\\pm 0.5}</td></tr><tr><td>AR [86]-CE (+QB-Norm)</td><td>\\mathbf{23.9}_{\\pm 0.2}</td><td>\\mathbf{57.1}_{\\pm 0.3}</td><td>\\mathbf{71.6}_{\\pm 0.4}</td><td>\\mathbf{4.0}_{\\pm 0.0}</td></tr></table>", "caption": "Table 13: Text-audio retrieval - AudioCaps: Comparison to other methods. ", "list_citation_info": ["[86] Andreea-Maria Oncescu, A Koepke, Jo\u00e3o F Henriques, Zeynep Akata, and Samuel Albanie. Audio retrieval with natural language queries. Interspeech, 2021."]}, {"table": "<table><tr><td>Querybank Source Data</td><td>Topk</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>No querybank</td><td>-</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.5_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>In Domain</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MSR-VTT</td><td>1</td><td>17.0_{\\pm 0.1}</td><td>41.3_{\\pm 0.1}</td><td>54.1_{\\pm 0.1}</td><td>8.6_{\\pm 0.5}</td></tr><tr><td>MSR-VTT</td><td>2</td><td>17.1_{\\pm 0.1}</td><td>41.7_{\\pm 0.1}</td><td>54.5_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>3</td><td>17.1_{\\pm 0.1}</td><td>41.8_{\\pm 0.1}</td><td>54.6_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>5</td><td>17.1_{\\pm 0.1}</td><td>41.9_{\\pm 0.1}</td><td>54.7_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>MSR-VTT</td><td>10</td><td>17.1_{\\pm 0.1}</td><td>41.9_{\\pm 0.1}</td><td>54.7_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>Far Domain</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LSMDC</td><td>1</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.2_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>LSMDC</td><td>2</td><td>14.8_{\\pm 0.0}</td><td>38.0_{\\pm 0.0}</td><td>51.0_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>LSMDC</td><td>3</td><td>14.7_{\\pm 0.0}</td><td>37.9_{\\pm 0.0}</td><td>50.9_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>LSMDC</td><td>5</td><td>14.6_{\\pm 0.0}</td><td>37.8_{\\pm 0.0}</td><td>50.8_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>LSMDC</td><td>10</td><td>14.5_{\\pm 0.0}</td><td>37.5_{\\pm 0.0}</td><td>50.4_{\\pm 0.0}</td><td>10.0_{\\pm 0.0}</td></tr></table>", "caption": "Table 14: The influence of the k hyperparameter on DIS normalisation.Performance is reported on MSR-VTT full split [121],while querybanks of 5,000 samplesare sampled from the training sets of different datasets.We observe that for Far Domain querybanks,k=1 performs the best,while retaining good performance for In Domain querybanks.", "list_citation_info": ["[121] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016."]}, {"table": "<table><tr><td>Querybank Source</td><td>Size</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>No querybank</td><td>-</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.5_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>Training set</td><td>60k</td><td>17.3_{\\pm 0.1}</td><td>42.1_{\\pm 0.2}</td><td>54.9_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>Val set</td><td>10k</td><td>16.7_{\\pm 0.1}</td><td>41.2_{\\pm 0.1}</td><td>54.0_{\\pm 0.1}</td><td>8.7_{\\pm 0.5}</td></tr><tr><td>Test set</td><td>60k</td><td>17.5_{\\pm 0.0}</td><td>42.4_{\\pm 0.1}</td><td>55.1_{\\pm 0.0}</td><td>8.0_{\\pm 0.0}</td></tr></table>", "caption": "Table 15: Effective querybanks can be constructedfrom the training set.Performance is reported on MSR-VTTfull split [121] using IS normalisation.We observe that a querybank of 60K samples from the trainingset performs comparably to a test set querybank.", "list_citation_info": ["[121] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>Dual[30]</td><td>7.7</td><td>22.0</td><td>31.8</td><td>32.0</td></tr><tr><td>HGR[19]</td><td>9.2</td><td>26.2</td><td>36.5</td><td>24.0</td></tr><tr><td>MoEE[79]</td><td>11.1_{\\pm 0.1}</td><td>30.7_{\\pm 0.1}</td><td>42.9_{\\pm 0.1}</td><td>15.0_{\\pm 0.0}</td></tr><tr><td>CE[73]</td><td>11.0_{\\pm 0.0}</td><td>30.8_{\\pm 0.1}</td><td>43.3_{\\pm 0.3}</td><td>15.0_{\\pm 0.0}</td></tr><tr><td>CE+[27]</td><td>14.4_{\\pm 0.1}</td><td>37.4_{\\pm 0.1}</td><td>50.2_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>CE+ (+QB-Norm)</td><td>16.4_{\\pm 0.0}</td><td>40.3_{\\pm 0.1}</td><td>53.0_{\\pm 0.1}</td><td>9.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+[27]</td><td>14.9_{\\pm 0.1}</td><td>38.3_{\\pm 0.1}</td><td>51.5_{\\pm 0.1}</td><td>10.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>17.3_{\\pm 0.0}</td><td>42.1_{\\pm 0.1}</td><td>54.9_{\\pm 0.1}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>CLIP4Clip[76]{}^{\\ddagger}</td><td>27.9</td><td>52.7</td><td>63.6</td><td>5.0</td></tr><tr><td>CLIP4Clip (+QB-Norm)</td><td>\\mathbf{29.6}</td><td>\\mathbf{54.5}</td><td>\\mathbf{65.3}</td><td>\\mathbf{4.0}</td></tr></table>", "caption": "Table 16: MSR-VTT full split: comparison to state of the art.<br/>{}^{\\ddagger} denotes results obtained training using the official code.", "list_citation_info": ["[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[19] Shizhe Chen, Yida Zhao, Qin Jin, and Qi Wu. Fine-grained video-text retrieval with hierarchical graph reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10638\u201310647, 2020.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.", "[30] Jianfeng Dong, Xirong Li, Chaoxi Xu, Shouling Ji, and Xun Wang. Dual dense encoding for zero-example video retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>MoEE[79]</td><td>16.1_{\\pm 1.0}</td><td>41.2_{\\pm 1.6}</td><td>55.2_{\\pm 1.6}</td><td>8.3_{\\pm 0.5}</td></tr><tr><td>CE[73]</td><td>17.1_{\\pm 0.9}</td><td>41.9_{\\pm 0.2}</td><td>56.0_{\\pm 0.5}</td><td>8.0_{\\pm 0.0}</td></tr><tr><td>TT-CE</td><td>21.0_{\\pm 0.6}</td><td>47.5_{\\pm 0.9}</td><td>61.9_{\\pm 0.5}</td><td>6.0_{\\pm 0.0}</td></tr><tr><td>Frozen[6]</td><td>31.0</td><td>59.8</td><td>72.4</td><td>3.0</td></tr><tr><td>CLIP4Clip [76]</td><td>\\mathbf{43.4}</td><td>70.2</td><td>80.6</td><td>\\mathbf{2.0}</td></tr><tr><td>CE+[27]</td><td>18.2_{\\pm 0.2}</td><td>43.9_{\\pm 0.9}</td><td>57.1_{\\pm 0.8}</td><td>7.9_{\\pm 0.1}</td></tr><tr><td>CE+ (+QB-Norm)</td><td>20.7_{\\pm 0.6}</td><td>46.6_{\\pm 0.2}</td><td>59.8_{\\pm 0.2}</td><td>6.3_{\\pm 0.5}</td></tr><tr><td>TT-CE+[27]</td><td>21.6_{\\pm 0.7}</td><td>48.6_{\\pm 0.4}</td><td>62.9_{\\pm 0.6}</td><td>6.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>24.2_{\\pm 0.7}</td><td>50.8_{\\pm 0.7}</td><td>64.4_{\\pm 0.1}</td><td>5.3_{\\pm 0.5}</td></tr><tr><td>CLIP4Clip [76]{}^{\\ddagger}</td><td>43.0</td><td>70.5</td><td>80.0</td><td>\\mathbf{2.0}</td></tr><tr><td>CLIP4Clip (+QB-Norm)</td><td>43.3</td><td>\\mathbf{71.4}</td><td>\\mathbf{80.8}</td><td>\\mathbf{2.0}</td></tr></table>", "caption": "Table 17: DiDeMo: Comparison to state of the art methods.<br/>{}^{\\ddagger} denotes results obtained training using the official code.", "list_citation_info": ["[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[6] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>MoEE[79]</td><td>12.1_{\\pm 0.7}</td><td>29.4_{\\pm 0.8}</td><td>37.7_{\\pm 0.2}</td><td>23.2_{\\pm 0.8}</td></tr><tr><td>CE[73]</td><td>12.4_{\\pm 0.7}</td><td>28.5_{\\pm 0.8}</td><td>37.9_{\\pm 0.6}</td><td>21.7_{\\pm 0.6}</td></tr><tr><td>MMT[41]</td><td>13.2_{\\pm 0.4}</td><td>29.2_{\\pm 0.8}</td><td>38.8_{\\pm 0.9}</td><td>21.0_{\\pm 1.4}</td></tr><tr><td>Frozen[6]</td><td>15.0</td><td>30.8</td><td>39.8</td><td>20.0</td></tr><tr><td>CLIP4Clip [76]</td><td>21.6</td><td>\\mathbf{41.8}</td><td>\\mathbf{49.8}</td><td>\\mathbf{11.0}</td></tr><tr><td>CE+[27]</td><td>14.9_{\\pm 0.6}</td><td>33.7_{\\pm 0.2}</td><td>44.1_{\\pm 0.6}</td><td>15.3_{\\pm 0.5}</td></tr><tr><td>CE+ (QB-Norm)</td><td>16.4_{\\pm 0.8}</td><td>34.8_{\\pm 0.4}</td><td>44.9_{\\pm 0.9}</td><td>14.5_{\\pm 0.4}</td></tr><tr><td>TT-CE+[27]</td><td>17.2_{\\pm 0.4}</td><td>36.5_{\\pm 0.6}</td><td>46.3_{\\pm 0.3}</td><td>13.7_{\\pm 0.5}</td></tr><tr><td>TT-CE+ (QB-Norm)</td><td>17.8_{\\pm 0.4}</td><td>37.7_{\\pm 0.5}</td><td>47.6_{\\pm 0.6}</td><td>12.7_{\\pm 0.5}</td></tr><tr><td>CLIP4Clip [76]{}^{\\ddagger}</td><td>21.3</td><td>40.0</td><td>49.5</td><td>\\mathbf{11.0}</td></tr><tr><td>CLIP4Clip (+QB-Norm)</td><td>\\mathbf{22.4}</td><td>40.1</td><td>49.5</td><td>\\mathbf{11.0}</td></tr></table>", "caption": "Table 18: LSMDC: Comparison to state of the art methods.<br/>{}^{\\ddagger} denotes results obtained training using the official code.", "list_citation_info": ["[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[41] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. European Conference on Computer Vision, 2020.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.", "[6] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. 2021."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@50\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>MoEE[79]</td><td>19.7_{\\pm 0.3}</td><td>50.0_{\\pm 0.5}</td><td>92.0_{\\pm 0.2}</td><td>5.3_{\\pm 0.5}</td></tr><tr><td>CE[73]</td><td>19.9_{\\pm 0.3}</td><td>50.1_{\\pm 0.7}</td><td>92.2_{\\pm 0.6}</td><td>5.3_{\\pm 0.5}</td></tr><tr><td>HSE[128]</td><td>20.5</td><td>49.3</td><td>-</td><td>-</td></tr><tr><td>MMT[41]</td><td>22.7_{\\pm 0.2}</td><td>54.2_{\\pm 1.0}</td><td>93.2_{\\pm 0.4}</td><td>5.0_{\\pm 0.0}</td></tr><tr><td>SSB[89]</td><td>26.8</td><td>58.1</td><td>93.5</td><td>3.0</td></tr><tr><td>CLIP4Clip[76]</td><td>40.5</td><td>\\mathbf{72.4}</td><td>\\mathbf{98.1}</td><td>\\mathbf{2.0}</td></tr><tr><td>TT-CE+[27]</td><td>23.5_{\\pm 0.2}</td><td>57.2_{\\pm 0.5}</td><td>96.1_{\\pm 0.1}</td><td>4.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>27.0_{\\pm 0.2}</td><td>60.6_{\\pm 0.4}</td><td>96.8_{\\pm 0.0}</td><td>4.0_{\\pm 0.0}</td></tr><tr><td>CLIP4Clip [76]{}^{\\ddagger}</td><td>36.3</td><td>65.9</td><td>96.8</td><td>3.0</td></tr><tr><td>CLIP4Clip (+QB-Norm)</td><td>\\mathbf{41.4}</td><td>71.4</td><td>97.6</td><td>\\mathbf{2.0}</td></tr></table>", "caption": "Table 19: ActivityNet: Comparison to state of the art methods.<br/>{}^{\\ddagger} denotes results obtained training using the official code. ", "list_citation_info": ["[79] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[128] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In Proceedings of the European Conference on Computer Vision (ECCV), pages 374\u2013390, 2018.", "[41] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. European Conference on Computer Vision, 2020.", "[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[76] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[73] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487, 2019.", "[89] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020."]}, {"table": "<table><tr><td>Model</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>Baseline</td><td>15.0</td><td>38.4</td><td>51.5</td><td>10.0</td></tr><tr><td>CENT [107]</td><td>14.4</td><td>37.2</td><td>50.2</td><td>10.0</td></tr><tr><td>DIS</td><td>17.3</td><td>42.1</td><td>54.9</td><td>8.0</td></tr></table>", "caption": "Table 20: MSR-VTT full split Comparison with CENT for a seed of TT-CE+[27] model. ", "list_citation_info": ["[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021.", "[107] Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Marco Saerens, and Kenji Fukumizu. Centering similarity measures to reduce hubs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 613\u2013623, 2013."]}, {"table": "<table><tr><td>Model</td><td>Task</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td>CE+[27]</td><td>v2t</td><td>22.7_{\\pm 0.5}</td><td>52.6_{\\pm 0.6}</td><td>66.3_{\\pm 0.2}</td><td>5.0_{\\pm 0.0}</td></tr><tr><td>CE+ (+QB-Norm)</td><td>v2t</td><td>28.6_{\\pm 0.4}</td><td>58.9_{\\pm 0.5}</td><td>71.4_{\\pm 0.5}</td><td>4.0_{\\pm 0.0}</td></tr><tr><td>TT-CE+[27]</td><td>v2t</td><td>24.6_{\\pm 0.3}</td><td>54.1_{\\pm 0.3}</td><td>67.5_{\\pm 0.5}</td><td>4.7_{\\pm 0.5}</td></tr><tr><td>TT-CE+ (+QB-Norm)</td><td>v2t</td><td>30.1_{\\pm 0.4}</td><td>61.4_{\\pm 0.4}</td><td>73.2_{\\pm 0.4}</td><td>3.0_{\\pm 0.0}</td></tr></table>", "caption": "Table 21: MSR-VTT full split: Comparison to state of the art - v2t task.", "list_citation_info": ["[27] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021."]}]}