{"title": "Learning a no-reference quality metric for single-image super-resolution", "abstract": "Numerous single-image super-resolution algorithms have been proposed in the literature, but few studies address the problem of performance evaluation based on visual perception. While most super-resolution images are evaluated by fullreference metrics, the effectiveness is not clear and the required ground-truth images are not always available in practice. To address these problems, we conduct human subject studies using a large set of super-resolution images and propose a no-reference metric learned from visual perceptual scores. Specifically, we design three types of low-level statistical features in both spatial and frequency domains to quantify super-resolved artifacts, and learn a two-stage regression model to predict the quality scores of super-resolution images without referring to ground-truth images. Extensive experimental results show that the proposed metric is effective and efficient to assess the quality of super-resolution images based on human perception.", "authors": ["Chao Ma", " Chih-Yuan Yang", " Xiaokang Yang", " Ming-Hsuan Yang"], "pdf_url": "https://arxiv.org/abs/1612.05890", "list_table_and_caption": [{"table": "<table><tr><td>Dataset</td><td># Reference Images</td><td> # Distortions</td><td># Subject Scores</td></tr><tr><td>LIVE DBLP:journals/tip/SheikhSB06 </td><td>29</td><td>982</td><td>22,457</td></tr><tr><td>ASQA CVPR14_PengYe </td><td>20</td><td>120</td><td>35,700</td></tr><tr><td>SRAB Yang14_ECCV </td><td>10</td><td>540</td><td>16,200</td></tr><tr><td>Our study</td><td>30</td><td>1,620</td><td>81,000</td></tr></table>", "caption": "Table 4: Data sets used for image quality assessment based on subjectstudies.", "list_citation_info": ["(33) P. Ye, D. Doermann, Active sampling for subjective image quality assessment, in: CVPR, 2014.", "(34) H. R. Sheikh, M. F. Sabir, A. C. Bovik, A statistical evaluation of recent full reference image quality assessment algorithms, TIP 15 (11) (2006) 3440\u20133451.", "(17) C.-Y. Yang, C. Ma, M.-H. Yang, Single image super-resolution: A benchmark, in: ECCV, 2014."]}, {"table": "<table><tr><td></td><td><p>Ours</p></td><td><p>RFR-con</p></td><td><p>RFR-DCT</p></td><td><p>RFR-GSM</p></td><td><p>RFR-PCA</p></td><td><p>SVR-all</p></td><td><p>SVR-con</p></td><td><p>SRV-DCT</p></td><td><p>SVR-GSM</p></td><td><p>SVR-PCA</p></td></tr><tr><td><p>  Bicubic</p></td><td>0.933</td><td><p>0.922</p></td><td><p>0.910</p></td><td><p>0.898</p></td><td><p>\\ul0.923</p></td><td><p>0.851</p></td><td><p>0.772</p></td><td><p>0.630</p></td><td><p>0.713</p></td><td><p>0.862</p></td></tr><tr><td><p>  BP</p></td><td>0.966</td><td><p>\\ul0.962</p></td><td><p>0.956</p></td><td><p>0.952</p></td><td>0.966</td><td><p>0.881</p></td><td><p>0.876</p></td><td><p>0.776</p></td><td><p>0.838</p></td><td><p>0.889</p></td></tr><tr><td><p>  Shan08</p></td><td>0.891</td><td><p>\\ul0.887</p></td><td><p>0.830</p></td><td><p>0.870</p></td><td><p>0.874</p></td><td><p>0.504</p></td><td><p>0.373</p></td><td><p>0.499</p></td><td><p>0.522</p></td><td><p>0.044</p></td></tr><tr><td><p>  Glasner09</p></td><td>0.931</td><td><p>\\ul0.926</p></td><td><p>0.911</p></td><td><p>0.897</p></td><td><p>0.878</p></td><td><p>0.841</p></td><td><p>0.717</p></td><td><p>0.766</p></td><td><p>0.685</p></td><td><p>0.599</p></td></tr><tr><td><p>  Yang10</p></td><td><p>\\ul0.968</p></td><td><p>0.961</p></td><td><p>0.954</p></td><td><p>0.948</p></td><td>0.969</td><td><p>0.929</p></td><td><p>0.905</p></td><td><p>0.874</p></td><td><p>0.834</p></td><td><p>0.877</p></td></tr><tr><td><p>  Dong11</p></td><td><p>\\ul0.954</p></td><td><p>0.946</p></td><td><p>0.922</p></td><td><p>0.929</p></td><td>0.960</td><td><p>0.885</p></td><td><p>0.892</p></td><td><p>0.792</p></td><td><p>0.883</p></td><td><p>0.874</p></td></tr><tr><td><p>  Yang13</p></td><td>0.958</td><td><p>\\ul0.955</p></td><td><p>0.937</p></td><td><p>0.932</p></td><td>0.958</td><td><p>0.898</p></td><td><p>0.855</p></td><td><p>0.801</p></td><td><p>0.770</p></td><td><p>0.874</p></td></tr><tr><td><p>  Timofte13</p></td><td>0.930</td><td><p>\\ul0.928</p></td><td><p>0.911</p></td><td><p>0.880</p></td><td><p>0.927</p></td><td><p>0.883</p></td><td><p>0.814</p></td><td><p>0.859</p></td><td><p>0.628</p></td><td><p>0.839</p></td></tr><tr><td><p>  SRCNN</p></td><td>0.949</td><td><p>0.938</p></td><td><p>0.917</p></td><td><p>0.936</p></td><td><p>\\ul0.945</p></td><td><p>0.866</p></td><td><p>0.853</p></td><td><p>0.778</p></td><td><p>0.816</p></td><td><p>0.843</p></td></tr><tr><td><p>  Overall</p></td><td>0.931</td><td><p>\\ul0.921</p></td><td><p>0.909</p></td><td><p>0.913</p></td><td><p>\\ul0.921</p></td><td><p>0.752</p></td><td><p>0.696</p></td><td><p>0.711</p></td><td><p>0.616</p></td><td><p>0.663</p></td></tr></table>", "caption": "Table 6: Spearman rank correlation coefficients hogg2005introduction (metric with higher coefficient matches perceptual score better). The random forest regression (RFR) uniformly performs better than the support vector regression (SVR) for each type of features or the concatenation (-con) of three type of features. The proposed two-stage regression approach (-all) combining three types of features improves the accuracy for both RFR and SVR. Bold: best; underline: second best.", "list_citation_info": ["(40) R. Hogg, J. McKean, A. Craig, Introduction to Mathematical Statistics, Pearson Education, 2005."]}, {"table": "<table><tr><td></td><td><p>Ours</p></td><td><p>BRISQUE</p></td><td><p>BLIINDS</p></td><td><p>CORNIA</p></td><td><p>CNNIQA</p></td><td><p>NSSA</p></td><td><p>DIVINE</p></td><td><p>BIQI</p></td><td><p>IFC</p></td><td><p>SSIM</p></td><td><p>FSIM</p></td><td><p>PSNR</p></td></tr><tr><td></td><td></td><td><p>DBLP:journals/tip/MittalMB12 </p></td><td><p>DBLP:journals/tip/SaadBC12 </p></td><td><p>DBLP:conf/cvpr/YeKKD12 </p></td><td><p>DBLP:conf/cvpr/KangYLD14 </p></td><td><p>DBLP:conf/icip/YeganehRW12 </p></td><td><p>DBLP:journals/tip/MoorthyB11 </p></td><td><p>DBLP:journals/spl/MoorthyB10 </p></td><td><p>DBLP:journals/tip/SheikhBV05 </p></td><td><p>DBLP:journals/tip/WangBSS04 </p></td><td><p>DBLP:journals/tip/ZhangZMZ11 </p></td><td></td></tr><tr><td><p>  Bicubic</p></td><td>0.933</td><td><p>0.850</p></td><td><p>0.886</p></td><td><p>0.889</p></td><td><p>\\ul0.926</p></td><td><p>-0.007</p></td><td><p>0.784</p></td><td><p>0.770</p></td><td><p>0.884</p></td><td><p>0.588</p></td><td><p>0.706</p></td><td><p>0.572</p></td></tr><tr><td><p>  BP</p></td><td>0.966</td><td><p>0.917</p></td><td><p>0.931</p></td><td><p>0.932</p></td><td><p>\\ul0.956</p></td><td><p>0.022</p></td><td><p>0.842</p></td><td><p>0.740</p></td><td><p>0.880</p></td><td><p>0.657</p></td><td><p>0.770</p></td><td><p>0.620</p></td></tr><tr><td><p>  Shan08</p></td><td><p>\\ul0.891</p></td><td><p>0.667</p></td><td><p>0.664</p></td><td>0.907</td><td><p>0.832</p></td><td><p>-0.128</p></td><td><p>0.653</p></td><td><p>0.254</p></td><td><p>0.934</p></td><td><p>0.560</p></td><td><p>0.648</p></td><td><p>0.564</p></td></tr><tr><td><p>  Glasner09</p></td><td>0.931</td><td><p>0.738</p></td><td><p>0.862</p></td><td><p>\\ul0.918</p></td><td><p>0.914</p></td><td><p>0.325</p></td><td><p>0.426</p></td><td><p>0.523</p></td><td><p>0.890</p></td><td><p>0.648</p></td><td><p>0.778</p></td><td><p>0.605</p></td></tr><tr><td><p>  Yang10</p></td><td>0.968</td><td><p>0.886</p></td><td><p>0.901</p></td><td><p>0.908</p></td><td><p>\\ul0.943</p></td><td><p>0.036</p></td><td><p>0.525</p></td><td><p>0.556</p></td><td><p>0.866</p></td><td><p>0.649</p></td><td><p>0.757</p></td><td><p>0.625</p></td></tr><tr><td><p>  Dong11</p></td><td>0.954</td><td><p>0.783</p></td><td><p>0.811</p></td><td><p>0.912</p></td><td><p>\\ul0.921</p></td><td><p>0.027</p></td><td><p>0.763</p></td><td><p>0.236</p></td><td><p>0.865</p></td><td><p>0.649</p></td><td><p>0.765</p></td><td><p>0.634</p></td></tr><tr><td><p>  Yang13</p></td><td>0.958</td><td><p>0.784</p></td><td><p>0.864</p></td><td><p>0.923</p></td><td><p>\\ul0.927</p></td><td><p>0.168</p></td><td><p>0.537</p></td><td><p>0.646</p></td><td><p>0.870</p></td><td><p>0.652</p></td><td><p>0.768</p></td><td><p>0.631</p></td></tr><tr><td><p>  Timofte13</p></td><td>0.930</td><td><p>0.843</p></td><td><p>0.903</p></td><td><p>0.911</p></td><td><p>\\ul0.924</p></td><td><p>0.320</p></td><td><p>0.122</p></td><td><p>0.563</p></td><td><p>0.881</p></td><td><p>0.656</p></td><td><p>0.756</p></td><td><p>0.620</p></td></tr><tr><td><p>  SRCNN</p></td><td>0.949</td><td><p>0.812</p></td><td><p>0.843</p></td><td><p>0.898</p></td><td><p>\\ul0.908</p></td><td><p>0.165</p></td><td><p>0.625</p></td><td><p>0.617</p></td><td><p>0.885</p></td><td><p>0.660</p></td><td><p>0.780</p></td><td><p>0.645</p></td></tr><tr><td><p>  Overall</p></td><td>0.931</td><td><p>0.802</p></td><td><p>0.853</p></td><td><p>\\ul0.919</p></td><td><p>0.904</p></td><td><p>0.076</p></td><td><p>0.589</p></td><td><p>0.482</p></td><td><p>0.810</p></td><td><p>0.635</p></td><td><p>0.747</p></td><td><p>0.604</p></td></tr></table>", "caption": "Table 7: Spearman rank correlation coefficients hogg2005introduction (metric with higher coefficient matches perceptual score better). The compared no-reference metrics are re-trained on our SR dataset using the 5-fold cross validation. The proposed metric performs favorably against state-of-the-art methods. Bold: best; underline: second best.", "list_citation_info": ["(40) R. Hogg, J. McKean, A. Craig, Introduction to Mathematical Statistics, Pearson Education, 2005.", "(42) L. Kang, P. Ye, Y. Li, D. S. Doermann, Convolutional neural networks for no-reference image quality assessment, in: CVPR, 2014, pp. 1733\u20131740.", "(41) A. Mittal, A. K. Moorthy, A. C. Bovik, No-reference image quality assessment in the spatial domain, TIP 21 (12) (2012) 4695\u20134708.", "(16) Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli, Image quality assessment: from error visibility to structural similarity, TIP 13 (4) (2004) 600\u2013612.", "(43) L. Zhang, L. Zhang, X. Mou, D. Zhang, FSIM: A feature similarity index for image quality assessment, TIP 20 (8) (2011) 2378\u20132386.", "(23) M. A. Saad, A. C. Bovik, C. Charrier, Blind image quality assessment: A natural scene statistics approach in the dct domain, TIP 21 (8) (2012) 3339\u20133352.", "(24) P. Ye, J. Kumar, L. Kang, D. S. Doermann, Unsupervised feature learning framework for no-reference image quality assessment, in: CVPR, 2012.", "(21) A. K. Moorthy, A. C. Bovik, Blind image quality assessment: From natural scene statistics to perceptual quality, TIP 20 (12) (2011) 3350\u20133364.", "(20) A. K. Moorthy, A. C. Bovik, A two-step framework for constructing blind image quality indices, SPL 17 (5) (2010) 513\u2013516.", "(18) H. R. Sheikh, A. C. Bovik, G. de Veciana, An information fidelity criterion for image quality assessment using natural scene statistics, TIP 14 (12) (2005) 2117\u20132128.", "(28) H. Yeganeh, M. Rostami, Z. Wang, Objective quality assessment for image super-resolution: A natural scene statistics approach, in: ICIP, 2012."]}, {"table": "<table><tr><td></td><td><p>Ours</p></td><td><p>BRISQUE</p></td><td><p>BLIINDS</p></td><td><p>CORNIA</p></td><td><p>CNNIQA</p></td><td><p>NSSA</p></td></tr><tr><td></td><td></td><td><p>DBLP:journals/tip/MittalMB12 </p></td><td><p>DBLP:journals/tip/SaadBC12 </p></td><td><p>DBLP:conf/cvpr/YeKKD12 </p></td><td><p>DBLP:conf/cvpr/KangYLD14 </p></td><td><p>DBLP:conf/icip/YeganehRW12 </p></td></tr><tr><td><p>  Bicubic</p></td><td>0.805</td><td><p>0.423</p></td><td><p>0.522</p></td><td><p>\\ul0.761</p></td><td><p>0.736</p></td><td><p>0.093</p></td></tr><tr><td><p>  BP</p></td><td>0.893</td><td><p>0.539</p></td><td><p>0.476</p></td><td><p>\\ul0.873</p></td><td><p>0.853</p></td><td><p>-0.046</p></td></tr><tr><td><p>  Shan08</p></td><td><p>\\ul0.800</p></td><td><p>0.442</p></td><td><p>0.474</p></td><td>0.832</td><td><p>0.742</p></td><td><p>0.048</p></td></tr><tr><td><p>  Glasner09</p></td><td>0.867</td><td><p>0.277</p></td><td><p>0.399</p></td><td><p>\\ul0.859</p></td><td><p>0.803</p></td><td><p>0.023</p></td></tr><tr><td><p>  Yang10</p></td><td>0.904</td><td><p>0.625</p></td><td><p>0.442</p></td><td><p>0.843</p></td><td><p>\\ul0.867</p></td><td><p>0.012</p></td></tr><tr><td><p>  Dong11</p></td><td>0.875</td><td><p>0.527</p></td><td><p>0.411</p></td><td><p>0.819</p></td><td><p>\\ul0.849</p></td><td><p>-0.101</p></td></tr><tr><td><p>  Yang13</p></td><td>0.885</td><td><p>0.575</p></td><td><p>0.290</p></td><td><p>\\ul0.843</p></td><td><p>0.841</p></td><td><p>0.108</p></td></tr><tr><td><p>  Timofte13</p></td><td><p>\\ul0.815</p></td><td><p>0.500</p></td><td><p>0.406</p></td><td>0.828</td><td><p>0.740</p></td><td><p>-0.035</p></td></tr><tr><td><p>  SRCNN</p></td><td>0.904</td><td><p>0.563</p></td><td><p>0.383</p></td><td><p>0.827</p></td><td><p>\\ul0.850</p></td><td><p>0.042</p></td></tr><tr><td><p>  Overall</p></td><td>0.852</td><td><p>0.505</p></td><td><p>0.432</p></td><td><p>\\ul0.843</p></td><td><p>0.799</p></td><td><p>0.017</p></td></tr></table>", "caption": "Table 8: Spearman rank correlation coefficients hogg2005introduction (metric with higher coefficient matches perceptual score better). The compared metrics are retrained on our SR dataset under the leave-image-out validation. Bold: best; underline: second best.", "list_citation_info": ["(40) R. Hogg, J. McKean, A. Craig, Introduction to Mathematical Statistics, Pearson Education, 2005.", "(42) L. Kang, P. Ye, Y. Li, D. S. Doermann, Convolutional neural networks for no-reference image quality assessment, in: CVPR, 2014, pp. 1733\u20131740.", "(41) A. Mittal, A. K. Moorthy, A. C. Bovik, No-reference image quality assessment in the spatial domain, TIP 21 (12) (2012) 4695\u20134708.", "(23) M. A. Saad, A. C. Bovik, C. Charrier, Blind image quality assessment: A natural scene statistics approach in the dct domain, TIP 21 (8) (2012) 3339\u20133352.", "(24) P. Ye, J. Kumar, L. Kang, D. S. Doermann, Unsupervised feature learning framework for no-reference image quality assessment, in: CVPR, 2012.", "(28) H. Yeganeh, M. Rostami, Z. Wang, Objective quality assessment for image super-resolution: A natural scene statistics approach, in: ICIP, 2012."]}, {"table": "<table><tr><td></td><td><p>Ours</p></td><td><p>BRISQUE</p></td><td><p>BLIINDS</p></td><td><p>CORNIA</p></td><td><p>CNNIQA</p></td><td><p>NSSA</p></td></tr><tr><td></td><td></td><td><p>DBLP:journals/tip/MittalMB12 </p></td><td><p>DBLP:journals/tip/SaadBC12 </p></td><td><p>DBLP:conf/cvpr/YeKKD12 </p></td><td><p>DBLP:conf/cvpr/KangYLD14 </p></td><td><p>DBLP:conf/icip/YeganehRW12 </p></td></tr><tr><td><p>  Bicubic</p></td><td><p>\\ul0.932</p></td><td><p>0.850</p></td><td><p>0.929</p></td><td><p>0.893</p></td><td>0.941</td><td><p>0.036</p></td></tr><tr><td><p>  BP</p></td><td><p>\\ul0.967</p></td><td><p>0.934</p></td><td><p>0.953</p></td><td><p>0.938</p></td><td>0.971</td><td><p>0.021</p></td></tr><tr><td><p>  Shan08</p></td><td>0.803</td><td><p>0.534</p></td><td><p>0.471</p></td><td><p>\\ul0.799</p></td><td><p>0.767</p></td><td><p>-0.087</p></td></tr><tr><td><p>  Glasner09</p></td><td>0.913</td><td><p>0.677</p></td><td><p>0.805</p></td><td><p>0.817</p></td><td><p>\\ul0.883</p></td><td><p>0.393</p></td></tr><tr><td><p>  Yang10</p></td><td>0.965</td><td><p>0.834</p></td><td><p>0.895</p></td><td><p>0.914</p></td><td><p>\\ul0.930</p></td><td><p>-0.054</p></td></tr><tr><td><p>  Dong11</p></td><td>0.932</td><td><p>0.774</p></td><td><p>0.780</p></td><td><p>0.917</p></td><td><p>\\ul0.920</p></td><td><p>-0.062</p></td></tr><tr><td><p>  Yang13</p></td><td>0.944</td><td><p>0.716</p></td><td><p>0.845</p></td><td><p>\\ul0.911</p></td><td><p>0.906</p></td><td><p>0.147</p></td></tr><tr><td><p>  Timofte13</p></td><td><p>0.774</p></td><td><p>0.760</p></td><td><p>\\ul0.849</p></td><td>0.898</td><td><p>0.845</p></td><td><p>0.382</p></td></tr><tr><td><p>  SRCNN</p></td><td>0.933</td><td><p>0.771</p></td><td><p>0.806</p></td><td><p>\\ul0.908</p></td><td><p>0.890</p></td><td><p>0.149</p></td></tr><tr><td><p>  Overall</p></td><td>0.848</td><td><p>0.644</p></td><td><p>0.763</p></td><td><p>\\ul0.809</p></td><td><p>0.797</p></td><td><p>0.053</p></td></tr></table>", "caption": "Table 9: Spearman rank correlation coefficients hogg2005introduction (metric with higher coefficient matches perceptual score better). The compared metrics are retrained on our SR dataset under the leave-method-out validation. Bold: best; underline: second best.", "list_citation_info": ["(40) R. Hogg, J. McKean, A. Craig, Introduction to Mathematical Statistics, Pearson Education, 2005.", "(42) L. Kang, P. Ye, Y. Li, D. S. Doermann, Convolutional neural networks for no-reference image quality assessment, in: CVPR, 2014, pp. 1733\u20131740.", "(41) A. Mittal, A. K. Moorthy, A. C. Bovik, No-reference image quality assessment in the spatial domain, TIP 21 (12) (2012) 4695\u20134708.", "(23) M. A. Saad, A. C. Bovik, C. Charrier, Blind image quality assessment: A natural scene statistics approach in the dct domain, TIP 21 (8) (2012) 3339\u20133352.", "(24) P. Ye, J. Kumar, L. Kang, D. S. Doermann, Unsupervised feature learning framework for no-reference image quality assessment, in: CVPR, 2012.", "(28) H. Yeganeh, M. Rostami, Z. Wang, Objective quality assessment for image super-resolution: A natural scene statistics approach, in: ICIP, 2012."]}, {"table": "<table><tr><td></td><td><p>Ours</p></td><td><p>Ours</p></td><td><p>Ours</p></td><td><p>BRISQUE</p></td><td><p>BLIINDS</p></td><td><p>CORNIA</p></td><td><p>CNNIQA</p></td></tr><tr><td></td><td>5-fold CV</td><td>image-out</td><td>method-out</td><td><p>DBLP:journals/tip/MittalMB12 </p></td><td><p>DBLP:journals/tip/SaadBC12 </p></td><td><p>DBLP:conf/cvpr/YeKKD12 </p></td><td><p>DBLP:conf/cvpr/KangYLD14 </p></td></tr><tr><td><p>  Bicubic</p></td><td>0.933</td><td><p>0.805</p></td><td><p>\\ul0.932</p></td><td><p>0.850</p></td><td><p>0.929</p></td><td><p>0.893</p></td><td><p>0.927</p></td></tr><tr><td><p>  BP</p></td><td><p>\\ul0.966</p></td><td><p>0.893</p></td><td>0.967</td><td><p>0.934</p></td><td><p>0.953</p></td><td><p>0.938</p></td><td><p>0.931</p></td></tr><tr><td><p>  Shan08</p></td><td>0.891</td><td><p>0.800</p></td><td><p>\\ul0.803</p></td><td><p>0.534</p></td><td><p>0.471</p></td><td><p>0.799</p></td><td><p>0.842</p></td></tr><tr><td><p>  Glasner09</p></td><td>0.931</td><td><p>0.867</p></td><td><p>0.913</p></td><td><p>0.677</p></td><td><p>0.805</p></td><td><p>0.817</p></td><td><p>\\ul0.896</p></td></tr><tr><td><p>  Yang10</p></td><td>0.968</td><td><p>0.904</p></td><td><p>\\ul0.965</p></td><td><p>0.834</p></td><td><p>0.895</p></td><td><p>0.914</p></td><td><p>0.938</p></td></tr><tr><td><p>  Dong11</p></td><td>0.954</td><td><p>0.875</p></td><td><p>0.932</p></td><td><p>0.774</p></td><td><p>0.780</p></td><td><p>0.917</p></td><td><p>\\ul0.936</p></td></tr><tr><td><p>  Yang13</p></td><td>0.958</td><td><p>0.885</p></td><td><p>\\ul0.944</p></td><td><p>0.716</p></td><td><p>0.845</p></td><td><p>0.911</p></td><td><p>0.934</p></td></tr><tr><td><p>  Timofte13</p></td><td>0.930</td><td><p>0.815</p></td><td><p>0.774</p></td><td><p>0.760</p></td><td><p>0.849</p></td><td><p>0.898</p></td><td><p>\\ul0.906</p></td></tr><tr><td><p>  SRCNN</p></td><td>0.949</td><td><p>0.904</p></td><td><p>0.933</p></td><td><p>0.771</p></td><td><p>0.806</p></td><td><p>\\ul0.908</p></td><td><p>0.924</p></td></tr><tr><td><p>  Overall</p></td><td>0.931</td><td><p>\\ul0.852</p></td><td><p>0.848</p></td><td><p>0.644</p></td><td><p>0.763</p></td><td><p>0.809</p></td><td><p>0.833</p></td></tr></table>", "caption": "Table 10: Spearman rank correlation coefficients hogg2005introduction (metric with higher coefficient matches perceptual score better). The compared no-reference metrics are not retrained on our SR dataset. Bold: best; underline: second best.", "list_citation_info": ["(40) R. Hogg, J. McKean, A. Craig, Introduction to Mathematical Statistics, Pearson Education, 2005.", "(42) L. Kang, P. Ye, Y. Li, D. S. Doermann, Convolutional neural networks for no-reference image quality assessment, in: CVPR, 2014, pp. 1733\u20131740.", "(41) A. Mittal, A. K. Moorthy, A. C. Bovik, No-reference image quality assessment in the spatial domain, TIP 21 (12) (2012) 4695\u20134708.", "(23) M. A. Saad, A. C. Bovik, C. Charrier, Blind image quality assessment: A natural scene statistics approach in the dct domain, TIP 21 (8) (2012) 3339\u20133352.", "(24) P. Ye, J. Kumar, L. Kang, D. S. Doermann, Unsupervised feature learning framework for no-reference image quality assessment, in: CVPR, 2012."]}]}