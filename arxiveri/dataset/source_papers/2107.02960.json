{"title": "Glit: Neural Architecture Search for Global and Local Image Transformer", "abstract": "We introduce the first Neural Architecture Search (NAS) method to find a better transformer architecture for image recognition. Recently, transformers without CNN-based backbones are found to achieve impressive performance for image recognition. However, the transformer is designed for NLP tasks and thus could be sub-optimal when directly used for image recognition. In order to improve the visual representation ability for transformers, we propose a new search space and searching algorithm. Specifically, we introduce a locality module that models the local correlations in images explicitly with fewer computational cost. With the locality module, our search space is defined to let the search algorithm freely trade off between global and local information as well as optimizing the low-level design choice in each module. To tackle the problem caused by huge search space, a hierarchical neural architecture search method is proposed to search the optimal vision transformer from two levels separately with the evolutionary algorithm. Extensive experiments on the ImageNet dataset demonstrate that our method can find more discriminative and efficient transformer variants than the ResNet family (e.g., ResNet101) and the baseline ViT for image classification.", "authors": ["Boyu Chen", " Peixia Li", " Chuming Li", " Baopu Li", " Lei Bai", " Chen Lin", " Ming Sun", " Junjie yan", " Wanli Ouyang"], "pdf_url": "https://arxiv.org/abs/2107.02960", "list_table_and_caption": [{"table": "<table><tr><td>Self-attention</td><td>Conv1d</td><td>Acc</td></tr><tr><td>head number</td><td>head number</td><td>(%)</td></tr><tr><td>3</td><td>0</td><td>72.20</td></tr><tr><td>2</td><td>1</td><td>72.89</td></tr><tr><td>1</td><td>2</td><td>73.98</td></tr><tr><td>0</td><td>3</td><td>71.02</td></tr></table>", "caption": "Table 1: Performance comparisons of different head distributions in DeiT-Tiny model [28] on ImageNet dataset. All blocks utilize the same distribution of heads. Here, the total head number in each transformer block is 3. The first row with 3 self-attention heads and 0 Conv1d head is the baseline model corresponding the the ViT in [11]. In the 2nd, 3rd and 4th rows, we gradually replace self-attention heads (global sub-modules) with more Convolution heads (local sub-modules).", "list_citation_info": ["[28] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.", "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."]}, {"table": "<table><tr><td>Models</td><td>Params(M)</td><td>FLOPS(G)</td><td>Acc(%)</td></tr><tr><td>R18</td><td>11.7</td><td>1.8</td><td>69.8</td></tr><tr><td>R18{}^{s}</td><td>11.7</td><td>1.8</td><td>68.5</td></tr><tr><td>DeiT-Tiny{}^{s}</td><td>5.7</td><td>1.3</td><td>72.2</td></tr><tr><td>GLiT-Tiny{}^{s}</td><td>7.2</td><td>1.4</td><td>76.3</td></tr><tr><td>R50</td><td>25.6</td><td>4.1</td><td>76.1</td></tr><tr><td>R50{}^{s}</td><td>25.6</td><td>4.1</td><td>78.5</td></tr><tr><td>X50-32x4d</td><td>25.0</td><td>4.3</td><td>77.6</td></tr><tr><td>X50-32x4d{}^{s}</td><td>25.0</td><td>4.3</td><td>79.1</td></tr><tr><td>DeiT-Small{}^{s}</td><td>22.1</td><td>4.6</td><td>79.6</td></tr><tr><td>GLiT-Small{}^{s}</td><td>24.6</td><td>4.4</td><td>80.5</td></tr><tr><td>X101-64x4d</td><td>83.5</td><td>15.6</td><td>79.6</td></tr><tr><td>X101-64x4d{}^{s}</td><td>83.5</td><td>15.6</td><td>81.5</td></tr><tr><td>Vit-Base</td><td>86.6</td><td>17.6</td><td>77.9</td></tr><tr><td>DeiT-Base{}^{s}</td><td>86.6</td><td>17.6</td><td>81.8</td></tr><tr><td>GLiT-Base{}^{s}</td><td>96.1</td><td>17.0</td><td>82.3</td></tr></table>", "caption": "Table 3: Classification accuracy of different models on ImageNet. \u2018Acc\u2019 denotes the Top-1 accuracy and \u2018{}^{s}\u2019 denotes the models trained using the training configurations in DeiT [28]. R18 denotes Resnet-18, X50 denotes Resnext-50.", "list_citation_info": ["[28] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In ICML, 2021."]}]}