{"title": "Synergizing between self-training and adversarial learning for domain adaptive object detection", "abstract": "We study adapting trained object detectors to unseen domains manifesting significant variations of object appearance, viewpoints and backgrounds. Most current methods align domains by either using image or instance-level feature alignment in an adversarial fashion. This often suffers due to the presence of unwanted background and as such lacks class-specific alignment. A common remedy to promote class-level alignment is to use high confidence predictions on the unlabelled domain as pseudo labels. These high confidence predictions are often fallacious since the model is poorly calibrated under domain shift. In this paper, we propose to leverage model predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. Specifically, we measure predictive uncertainty on class assignments and the bounding box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-supervision, whereas the ones with higher uncertainty are used to generate tiles for an adversarial feature alignment stage. This synergy between tiling around the uncertain object regions and generating pseudo-labels from highly certain object regions allows us to capture both the image and instance level context during the model adaptation stage. We perform extensive experiments covering various domain shift scenarios. Our approach improves upon existing state-of-the-art methods with visible margins.", "authors": ["Muhammad Akhtar Munir", " Muhammad Haris Khan", " M. Saquib Sarfraz", " Mohsen Ali"], "pdf_url": "https://arxiv.org/abs/2110.00249", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>person</td><td>rider</td><td>car</td><td>truck</td><td>bus</td><td>train</td><td>mbike</td><td>bicycle</td><td>mAP@0.5</td><td>SO / Gain</td></tr><tr><td colspan=\"11\">Two Stage Object Detector</td></tr><tr><td>DAF Chenet al. (2018)</td><td>25.0</td><td>31.0</td><td>40.5</td><td>22.1</td><td>35.3</td><td>20.2</td><td>20.0</td><td>27.1</td><td>27.6</td><td>18.8 / 8.8</td></tr><tr><td>SW-DA Saitoet al. (2019)</td><td>29.9</td><td>42.3</td><td>43.5</td><td>24.5</td><td>36.2</td><td>32.6</td><td>30.0</td><td>35.3</td><td>34.3</td><td>20.3 / 14.0</td></tr><tr><td>DAM He andZhang (2019)</td><td>30.8</td><td>40.5</td><td>44.3</td><td>27.2</td><td>38.4</td><td>34.5</td><td>28.4</td><td>32.2</td><td>34.6</td><td>18.8 / 16.7</td></tr><tr><td>CR-DA Xuet al. (2020)</td><td>32.9</td><td>43.8</td><td>49.2</td><td>27.2</td><td>45.1</td><td>36.4</td><td>30.3</td><td>34.6</td><td></td><td></td></tr><tr><td>CF-DA Zhenget al. (2020)</td><td>43.2</td><td>37.4</td><td>52.1</td><td>34.7</td><td>34.0</td><td>46.9</td><td>29.9</td><td>30.8</td><td>38.6</td><td>20.8 / 17.8</td></tr><tr><td>HTCN Chenet al. (2020)</td><td>33.2</td><td>47.5</td><td>47.9</td><td>31.6</td><td>47.4</td><td>40.9</td><td>32.3</td><td>37.1</td><td>39.8</td><td>20.3 / 19.5</td></tr><tr><td>UADA Nguyenet al. (2020a)</td><td>34.2</td><td>48.9</td><td>52.4</td><td>30.3</td><td>42.7</td><td>46.0</td><td>33.2</td><td>36.2</td><td>40.5</td><td>20.3 / 20.2</td></tr><tr><td>SAPNet Liet al. (2020)</td><td>40.8</td><td>46.7</td><td>59.8</td><td>24.3</td><td>46.8</td><td>37.5</td><td>30.4</td><td>40.7</td><td>40.9</td><td>20.3 / 20.6</td></tr><tr><td colspan=\"11\">One Stage Object Detector</td></tr><tr><td>Source Only</td><td>31.7</td><td>31.7</td><td>34.6</td><td>5.9</td><td>20.3</td><td>2.5</td><td>10.6</td><td>25.8</td><td>20.4</td><td>-</td></tr><tr><td>Baseline Hsuet al. (2020)</td><td>38.7</td><td>36.1</td><td>53.1</td><td>21.9</td><td>35.4</td><td>25.7</td><td>20.6</td><td>33.9</td><td>33.2</td><td>18.4 / 14.8</td></tr><tr><td>EPM Hsuet al. (2020)</td><td>41.9</td><td>38.7</td><td>56.7</td><td>22.6</td><td>41.5</td><td>26.8</td><td>24.6</td><td>35.5</td><td>36.0</td><td>18.4 / 17.6</td></tr><tr><td>Ours</td><td>45.1</td><td>47.4</td><td>59.4</td><td>24.5</td><td>50.0</td><td>25.7</td><td>26.0</td><td>38.7</td><td>39.6</td><td>20.4 / 19.2</td></tr><tr><td>Oracle</td><td>47.4</td><td>40.8</td><td>66.8</td><td>27.2</td><td>48.2</td><td>32.4</td><td>31.2</td><td>38.3</td><td>41.5</td><td>-</td></tr></tbody></table>", "caption": "Table 1: Cityscapes \\rightarrow Foggy CityscapesOur method achieves an absolute gain of 19.2% over the source only model and out-performs most recent one-stage domain adaptive detector (EPM). SO refers to source only.The best results are bold-faced. ", "list_citation_info": ["Chen et al. (2020) Chen, C., Z. Zheng, X. Ding, Y. Huang, and Q. Dou (2020). Harmonizing transferability and discriminability for adapting object detectors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8869\u20138878.", "Xu et al. (2020) Xu, C.-D., X.-R. Zhao, X. Jin, and X.-S. Wei (2020). Exploring categorical regularization for domain adaptive object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11724\u201311733.", "Li et al. (2020) Li, C., D. Du, L. Zhang, L. Wen, T. Luo, Y. Wu, and P. Zhu (2020). Spatial attention pyramid network for unsupervised domain adaptation. In European Conference on Computer Vision, pp. 481\u2013497. Springer.", "Zheng et al. (2020) Zheng, Y., D. Huang, S. Liu, and Y. Wang (2020). Cross-domain object detection through coarse-to-fine feature adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13766\u201313775.", "Chen et al. (2018) Chen, Y., W. Li, C. Sakaridis, D. Dai, and L. Van Gool (2018). Domain adaptive faster r-cnn for object detection in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3339\u20133348.", "Hsu et al. (2020) Hsu, C.-C., Y.-H. Tsai, Y.-Y. Lin, and M.-H. Yang (2020). Every pixel matters: Center-aware feature alignment for domain adaptive object detector. In European Conference on Computer Vision, pp. 733\u2013748. Springer.", "Saito et al. (2019) Saito, K., Y. Ushiku, T. Harada, and K. Saenko (2019). Strong-weak distribution alignment for adaptive object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6956\u20136965.", "He and Zhang (2019) He, Z. and L. Zhang (2019). Multi-adversarial faster-rcnn for unrestricted object detection. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6668\u20136677.", "Nguyen et al. (2020a) Nguyen, D.-K., W.-L. Tseng, and H.-H. Shuai (2020a). Domain-adaptive object detection via uncertainty-aware distribution alignment. In Proceedings of the 28th ACM International Conference on Multimedia, pp. 2499\u20132507."]}]}