{"title": "Smoothnet: A Plug-and-Play Network for Refining Human Poses in Videos", "abstract": "When analyzing human motion videos, the output jitters from existing pose estimators are highly-unbalanced with varied estimation errors across frames. Most frames in a video are relatively easy to estimate and only suffer from slight jitters. In contrast, for rarely seen or occluded actions, the estimated positions of multiple joints largely deviate from the ground truth values for a consecutive sequence of frames, rendering significant jitters on them. To tackle this problem, we propose to attach a dedicated temporal-only refinement network to existing pose estimators for jitter mitigation, named SmoothNet. Unlike existing learning-based solutions that employ spatio-temporal models to co-optimize per-frame precision and temporal smoothness at all the joints, SmoothNet models the natural smoothness characteristics in body movements by learning the long-range temporal relations of every joint without considering the noisy correlations among joints. With a simple yet effective motion-aware fully-connected network, SmoothNet improves the temporal smoothness of existing pose estimators significantly and enhances the estimation accuracy of those challenging frames as a side-effect. Moreover, as a temporal-only model, a unique advantage of SmoothNet is its strong transferability across various types of estimators and datasets. Comprehensive experiments on five datasets with eleven popular backbone networks across 2D and 3D pose estimation and body recovery tasks demonstrate the efficacy of the proposed solution. Code is available at https://github.com/cure-lab/SmoothNet.", "authors": ["Ailing Zeng", " Lei Yang", " Xuan Ju", " Jiefeng Li", " Jianyi Wang", " Qiang Xu"], "pdf_url": "https://arxiv.org/abs/2112.13715", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th>Strategy</th><th>Accel</th><th>MPJPE</th><th>PA-MPJPE</th><th>MPJVE</th></tr></thead><tbody><tr><th rowspan=\"4\"><p>Backbones</p></th><th>In = 1</th><td>32.69</td><td>84.54</td><td>57.94</td><td>102.05</td></tr><tr><th>In = 16</th><td>23.21</td><td>83.03</td><td>56.77</td><td>99.76</td></tr><tr><th>In = 16 \\times</th><td>20.42</td><td>84.51</td><td>57.81</td><td>101.62</td></tr><tr><th>In = 16 w/ B</th><td>21.65</td><td>86.56</td><td>59.93</td><td>105.08</td></tr><tr><th rowspan=\"2\"><p>Ours</p></th><th>In = 1 w/ ours</th><td>6.12</td><td>82.98</td><td>57.27</td><td>100.67</td></tr><tr><th>In = 16 w/ ours</th><td>6.05</td><td>81.42</td><td>56.21</td><td>98.83</td></tr></tbody></table>", "caption": "Table 1: Comparison results of the body recovery from VIBE [21] of different training strategies on 3DPW. \\times means acceleration loss added in the loss function. B means to add SmoothNet behind the backbones trained in an end-to-end manner. ", "list_citation_info": ["[21] Kocabas, M., Athanasiou, N., Black, M.J.: Vibe: Video inference for human body pose and shape estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5253\u20135263 (2020)"]}, {"table": "<table><tbody><tr><th></th><th>Strategy</th><td>Accel</td><td>MPJPE</td><td>Params.</td></tr><tr><th rowspan=\"7\"><p>Backbones</p></th><th>In = 1</th><td>19.17</td><td>54.55</td><td>6.39M</td></tr><tr><th>In = 27</th><td>5.07</td><td>50.13</td><td>8.61M</td></tr><tr><th>In = 27 w/ \\times</th><td>4.12</td><td>51.48</td><td>8.61M</td></tr><tr><th>In = 27 w/ B</th><td>2.78</td><td>52.65</td><td>8.65M</td></tr><tr><th>In = 27 w/ B \\times</th><td>2.87</td><td>52.18</td><td>8.65M</td></tr><tr><th>In = 27 w/ B \\circ</th><td>5.46</td><td>51.06</td><td>8.65M</td></tr><tr><th>In = 27 w/ B \\circ \\times</th><td>2.69</td><td>50.94</td><td>8.65M</td></tr><tr><th rowspan=\"2\"><p>Ours</p></th><th>In = 1 w/ ours</th><td>1.03</td><td>52.72</td><td>0.03M</td></tr><tr><th>In = 27 w/ ours</th><td>0.88</td><td>50.04</td><td>0.03M</td></tr></tbody></table>", "caption": "Table 2: Comparison of the 3D pose estimation results from VPose [35] of different training strategies on Human3.6M. \\times means acceleration loss added in the loss function. B means to add SmoothNet behind the origin network trained in an end-to-end manner. \\circ adds an intermediate L1 supervision between the backbone and SmoothNet.", "list_citation_info": ["[35] Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.: 3d human pose estimation in video with temporal convolutions and semi-supervised training. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7753\u20137762 (2019)"]}, {"table": "<table><tbody><tr><th></th><th>Method</th><td>Accel</td><td>MPJPE</td><td>PA-MPJPE</td><td>Test FPS</td></tr><tr><th rowspan=\"8\"><p>2D Pose</p></th><th>CPN [5]</th><td>2.91</td><td>6.67</td><td>5.18</td><td>-</td></tr><tr><th>w/One-Euro [4]</th><td>0.51</td><td>7.86</td><td>5.47</td><td>2.28k</td></tr><tr><th>w/Savitzky-Golay [36]</th><td>0.20</td><td>6.52</td><td>4.99</td><td>67.39k</td></tr><tr><th>w/Gaussian1d [46]</th><td>0.51</td><td>6.55</td><td>5.00</td><td>35.97k</td></tr><tr><th>w/One-Euro [4]</th><td>0.19</td><td>9.21</td><td>6.01</td><td>3.93k</td></tr><tr><th>w/Savitzky-Golay [36]</th><td>0.15</td><td>8.23</td><td>5.89</td><td>65.10k</td></tr><tr><th>w/Gaussian1d [46]</th><td>0.14</td><td>6.73</td><td>4.99</td><td>43.74k</td></tr><tr><th>w/Ours</th><td>0.14</td><td>6.45</td><td>4.96</td><td>71.60k</td></tr><tr><th rowspan=\"8\"><p>3D Pose</p></th><th>FCN [29]</th><td>19.17</td><td>54.55</td><td>42.20</td><td>-</td></tr><tr><th>w/One-Euro [4]</th><td>3.80</td><td>55.20</td><td>42.73</td><td>2.27k</td></tr><tr><th>w/Savitzky-Golay [36]</th><td>1.34</td><td>53.48</td><td>41.49</td><td>66.37k</td></tr><tr><th>w/Gaussian1d [46]</th><td>2.43</td><td>53.67</td><td>41.60</td><td>29.54k</td></tr><tr><th>w/One-Euro [4]</th><td>0.94</td><td>143.24</td><td>85.35</td><td>3.72k</td></tr><tr><th>w/Savitzky-Golay [36]</th><td>0.92</td><td>74.38</td><td>57.25</td><td>65.56k</td></tr><tr><th>w/Gaussian1d [46]</th><td>0.95</td><td>83.54</td><td>68.53</td><td>28.93k</td></tr><tr><th>w/Ours</th><td>1.03</td><td>52.72</td><td>40.92</td><td>66.67k</td></tr></tbody></table>", "caption": "Table 3: Comparison of most used filters with different estimated poses from CPN [5] (2D) and FCN [29] (3D) on Human3.6M. ", "list_citation_info": ["[5] Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.: Cascaded pyramid network for multi-person pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7103\u20137112 (2018)", "[29] Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet effective baseline for 3d human pose estimation. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2640\u20132649 (2017)", "[36] Press, W.H., Teukolsky, S.A.: Savitzky-golay smoothing filters. Computers in Physics 4(6), 669\u2013672 (1990)", "[46] Young, I.T., Van Vliet, L.J.: Recursive implementation of the gaussian filter. Signal processing 44(2), 139\u2013151 (1995)", "[4] Casiez, G., Roussel, N., Vogel, D.: 1\u20ac filter: a simple speed-based low-pass filter for noisy input in interactive systems. In: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. pp. 2527\u20132530 (2012)"]}, {"table": "<table><tbody><tr><th>Method</th><td>Accel</td><td>MPJPE</td><td>PA-MPJPE</td></tr><tr><th>TPoseNet [43]</th><td>12.70</td><td>103.33</td><td>68.36</td></tr><tr><th>TPoseNet w/ RefineNet [43]</th><td>9.53</td><td>93.97</td><td>65.16</td></tr><tr><th>TPoseNet w/ Savitzky-Golay</th><td>8.29</td><td>102.79</td><td>68.30</td></tr><tr><th>TPoseNet w/ Gaussian1d</th><td>8.61</td><td>102.70</td><td>68.17</td></tr><tr><th>TPoseNet w/ Ours</th><td>7.23</td><td>100.78</td><td>68.10</td></tr><tr><th>RefineNet w/ Savitzky-Golay</th><td>7.22</td><td>93.75</td><td>65.34</td></tr><tr><th>RefineNet w/ Gaussian1d</th><td>8.40</td><td>93.65</td><td>65.19</td></tr><tr><th>RefineNet w/ Ours</th><td>7.21</td><td>91.78</td><td>65.06</td></tr></tbody></table>", "caption": "Table 6: Comparison results on multi-person MuPoTS-3D dataset [32]. SmoothNet is directly tested on it, while RefineNet [43] has been trained on in-domain datasets.", "list_citation_info": ["[43] V\u00e9ges, M., L\u0151rincz, A.: Temporal smoothing for 3d human pose estimation and localization for occluded people. In: International Conference on Neural Information Processing. pp. 557\u2013568. Springer (2020)", "[32] Mehta, D., Sotnychenko, O., Mueller, F., Xu, W., Sridhar, S., Pons-Moll, G., Theobalt, C.: Single-shot multi-person 3d pose estimation from monocular rgb. 2018 International Conference on 3D Vision (3DV) pp. 120\u2013130 (2018)"]}, {"table": "<table><thead><tr><th>Method</th><th>Accel</th><th>MPJPE</th><th>PA-MPJPE</th></tr><tr><th>EFT</th><th>32.71</th><th>90.32</th><th>52.19</th></tr></thead><tbody><tr><th>L_{pose}</th><td>6.42</td><td>86.63</td><td>50.82</td></tr><tr><th>L_{acc}</th><td>7.63</td><td>446.54</td><td>356.61</td></tr><tr><th>L_{pose}+L_{acc}</th><td>6.30</td><td>86.39</td><td>50.60</td></tr></tbody></table>", "caption": "Table 11: Comparison of refined results by different loss functions based on the outputs of the SMPL-based method EFT [16] on the 3DPW dataset. ", "list_citation_info": ["[16] Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In: 2021 International Conference on 3D Vision (3DV). pp. 42\u201352. IEEE (2021)"]}, {"table": "<table><thead><tr><th>Method</th><th>Accel</th><th>MPJPE</th><th>PA-MPJPE</th></tr><tr><th>EFT [16]</th><th>32.71</th><th>90.32</th><th>52.19</th></tr></thead><tbody><tr><th>Angle-Axis</th><td>77.89</td><td>172.17</td><td>51.38</td></tr><tr><th>Quaternion</th><td>28.50</td><td>91.23</td><td>51.03</td></tr><tr><th>6D Rotation</th><td>6.43</td><td>86.92</td><td>50.87</td></tr><tr><th>3D Position</th><td>6.30</td><td>86.39</td><td>50.60</td></tr></tbody></table>", "caption": "Table 12: Comparison of refined results trained by different motion modalities based on the outputs of EFT [16] on the 3DPW dataset. ", "list_citation_info": ["[16] Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In: 2021 International Conference on 3D Vision (3DV). pp. 42\u201352. IEEE (2021)"]}, {"table": "<table><thead><tr><th>Method</th><th>Accel</th><th>MPJPE</th><th>PA-MPJPE</th></tr><tr><th>EFT</th><th>32.71</th><th>90.32</th><th>52.19</th></tr></thead><tbody><tr><th>6D Rotation</th><td>6.43</td><td>86.92</td><td>50.87</td></tr><tr><th>Cross-Test on 3D Position</th><td>7.10</td><td>88.13</td><td>51.79</td></tr><tr><th>3D Position</th><td>6.30</td><td>86.39</td><td>50.60</td></tr><tr><th>Cross-Test on 6D Rotation</th><td>6.47</td><td>86.82</td><td>50.81</td></tr></tbody></table>", "caption": "Table 13: Comparison of refined results by cross motion representations testing based on the outputs of EFT [16] on the 3DPW dataset. Cross-Test means training the SmoothNet on a motion representation while testing it on another modality directly.", "list_citation_info": ["[16] Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In: 2021 International Conference on 3D Vision (3DV). pp. 42\u201352. IEEE (2021)"]}, {"table": "<table><thead><tr><th>Method</th><th>Accel</th><th>MPJPE</th><th>PA-MPJPE</th></tr><tr><th>EFT [16]</th><th>32.71</th><th>90.32</th><th>52.19</th></tr></thead><tbody><tr><th>w/o Norm.</th><td>5.80</td><td>85.16</td><td>50.31</td></tr><tr><th>Sequence Norm.</th><td>5.82</td><td>88.21</td><td>51.06</td></tr><tr><th>Sequence Norm. \\dagger</th><td>5.80</td><td>61.65</td><td>44.28</td></tr><tr><th>TCMR [6]</th><td>6.76</td><td>86.46</td><td>52.67</td></tr><tr><th>w/o Norm.</th><td>5.91</td><td>86.04</td><td>52.42</td></tr><tr><th>Sequence Norm</th><td>6.00</td><td>86.34</td><td>52.87</td></tr><tr><th>Sequence Norm \\dagger</th><td>5.92</td><td>68.51</td><td>49.15</td></tr></tbody></table>", "caption": "Table 14: Comparison of the results of different normalization based on the outputs of EFT [16] and cross-backbone testing on the outputs of TCMR [6] on 3DPW dataset. \\dagger means using the same mean and variance as the ground truth to explore the upper bound performance.", "list_citation_info": ["[6] Choi, H., Moon, G., Chang, J.Y., Lee, K.M.: Beyond static features for temporally consistent 3d human pose and shape from a video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1964\u20131973 (2021)", "[16] Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In: 2021 International Conference on 3D Vision (3DV). pp. 42\u201352. IEEE (2021)"]}]}