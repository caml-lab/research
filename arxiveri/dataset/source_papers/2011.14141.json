{"title": "Adabins: Depth estimation using adaptive bins", "abstract": "We address the problem of estimating a high quality dense depth map from a single RGB input image. We start out with a baseline encoder-decoder convolutional neural network architecture and pose the question of how the global processing of information can help improve overall depth estimation. To this end, we propose a transformer-based architecture block that divides the depth range into bins whose center value is estimated adaptively per image. The final depth values are estimated as linear combinations of the bin centers. We call our new building block AdaBins. Our results show a decisive improvement over the state-of-the-art on several popular depth datasets across all metrics. We also validate the effectiveness of the proposed block with an ablation study and provide the code and corresponding pre-trained weights of the new state-of-the-art model.", "authors": ["Shariq Farooq Bhat", " Ibraheem Alhashim", " Peter Wonka"], "pdf_url": "https://arxiv.org/abs/2011.14141", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th><p>\\delta_{1}\\uparrow</p></th><th><p>\\delta_{2}\\uparrow</p></th><th><p>\\delta_{3} \\uparrow</p></th><th><p>REL \\downarrow</p></th><th><p>RMS \\downarrow</p></th><th>log_{10} \\downarrow</th></tr></thead><tbody><tr><td>Eigen et al. [8]</td><td><p>0.769</p></td><td><p>0.950</p></td><td><p>0.988</p></td><td><p>0.158</p></td><td><p>0.641</p></td><td>\u2013</td></tr><tr><td>Laina et al. [25]</td><td><p>0.811</p></td><td><p>0.953</p></td><td><p>0.988</p></td><td><p>0.127</p></td><td><p>0.573</p></td><td>0.055</td></tr><tr><td>Hao et al. [16]</td><td><p>0.841</p></td><td><p>0.966</p></td><td><p>0.991</p></td><td><p>0.127</p></td><td><p>0.555</p></td><td>0.053</td></tr><tr><td>Lee et al. [27]</td><td><p>0.837</p></td><td><p>0.971</p></td><td><p>0.994</p></td><td><p>0.131</p></td><td><p>0.538</p></td><td>\u2013</td></tr><tr><td>Fu et al. [11]</td><td><p>0.828</p></td><td><p>0.965</p></td><td><p>0.992</p></td><td><p>0.115</p></td><td><p>0.509</p></td><td>0.051</td></tr><tr><td>SharpNet [34]</td><td><p>0.836</p></td><td><p>0.966</p></td><td><p>0.993</p></td><td><p>0.139</p></td><td><p>0.502</p></td><td>0.047</td></tr><tr><td>Hu et al. [19]</td><td><p>0.866</p></td><td><p>0.975</p></td><td><p>0.993</p></td><td><p>0.115</p></td><td><p>0.530</p></td><td>0.050</td></tr><tr><td>Chen et al. [4]</td><td><p>0.878</p></td><td><p>0.977</p></td><td><p>0.994</p></td><td><p>0.111</p></td><td><p>0.514</p></td><td>0.048</td></tr><tr><td>Yin et al. [47]</td><td><p>0.875</p></td><td><p>0.976</p></td><td><p>0.994</p></td><td>0.108</td><td><p>0.416</p></td><td>0.048</td></tr><tr><td>BTS [26]</td><td>0.885</td><td><p>0.978</p></td><td><p>0.994</p></td><td><p>0.110</p></td><td>0.392</td><td>0.047</td></tr><tr><td>DAV [22]</td><td><p>0.882</p></td><td>0.980</td><td>0.996</td><td>0.108</td><td><p>0.412</p></td><td>\u2013</td></tr><tr><td>AdaBins (Ours)</td><td>0.903</td><td>0.984</td><td>0.997</td><td>0.103</td><td>0.364</td><td>0.044</td></tr></tbody></table>", "caption": "Table 2: Comparison of performances on the NYU-Depth-v2 dataset. The reported numbers are from the corresponding original papers. Best results are in bold, second best are underlined.", "list_citation_info": ["[19] Junjie Hu, Mete Ozay, Yan Zhang, and Takayuki Okatani. Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1043\u20131051, 2018.", "[8] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NIPS, 2014.", "[4] Xiaotian Chen, Xuejin Chen, and Zheng-Jun Zha. Structure-aware residual pyramid network for monocular depth estimation. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 694\u2013700. International Joint Conferences on Artificial Intelligence Organization, 7 2019.", "[25] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. 2016 Fourth International Conference on 3D Vision (3DV), pages 239\u2013248, 2016.", "[26] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[27] Wonwoo Lee, Nohyoung Park, and Woontack Woo. Depth-assisted real-time 3d object detection for augmented reality. ICAT\u201911, 2:126\u2013132, 2011.", "[34] Michael Ramamonjisoa and Vincent Lepetit. Sharpnet: Fast and accurate recovery of occluding contours in monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, Oct 2019.", "[22] Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, and Janne Heikkila. Guiding monocular depth estimation using depth-attention volume. arXiv preprint arXiv:2004.02760, 2020.", "[16] Zhixiang Hao, Yu Li, Shaodi You, and Feng Lu. Detail preserving depth estimation from a single image using attention guided networks. 2018 International Conference on 3D Vision (3DV), pages 304\u2013313, 2018.", "[47] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.", "[11] Huan Fu, Mingming Gong, Chaohui Wang, Nematollah Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th><p>\\delta_{1}\\uparrow</p></th><th><p>\\delta_{2}\\uparrow</p></th><th><p>\\delta_{3}\\uparrow</p></th><th><p>REL \\downarrow</p></th><th><p>Sq Rel \\downarrow</p></th><th><p>RMS \\downarrow</p></th><th>RMS log \\downarrow</th></tr></thead><tbody><tr><td>Saxena et al. [36]</td><td><p>0.601</p></td><td><p>0.820</p></td><td><p>0.926</p></td><td><p>0.280</p></td><td><p>3.012</p></td><td><p>8.734</p></td><td>0.361</td></tr><tr><td>Eigen et al. [8]</td><td><p>0.702</p></td><td><p>0.898</p></td><td><p>0.967</p></td><td><p>0.203</p></td><td><p>1.548</p></td><td><p>6.307</p></td><td>0.282</td></tr><tr><td>Liu et al. [29]</td><td><p>0.680</p></td><td><p>0.898</p></td><td><p>0.967</p></td><td><p>0.201</p></td><td><p>1.584</p></td><td><p>6.471</p></td><td>0.273</td></tr><tr><td>Godard et al. [15]</td><td><p>0.861</p></td><td><p>0.949</p></td><td><p>0.976</p></td><td><p>0.114</p></td><td><p>0.898</p></td><td><p>4.935</p></td><td>0.206</td></tr><tr><td>Kuznietsov et al. [24]</td><td><p>0.862</p></td><td><p>0.960</p></td><td><p>0.986</p></td><td><p>0.113</p></td><td><p>0.741</p></td><td><p>4.621</p></td><td>0.189</td></tr><tr><td>Gan et al. [12]</td><td><p>0.890</p></td><td><p>0.964</p></td><td><p>0.985</p></td><td><p>0.098</p></td><td><p>0.666</p></td><td><p>3.933</p></td><td>0.173</td></tr><tr><td>Fu et al. [11]</td><td><p>0.932</p></td><td><p>0.984</p></td><td><p>0.994</p></td><td><p>0.072</p></td><td><p>0.307</p></td><td>2.727</td><td>0.120</td></tr><tr><td>Yin et al. [47]</td><td><p>0.938</p></td><td><p>0.990</p></td><td><p>0.998</p></td><td><p>0.072</p></td><td><p>\u2013</p></td><td><p>3.258</p></td><td>0.117</td></tr><tr><td>BTS[26]</td><td>0.956</td><td>0.993</td><td>0.998</td><td>0.059</td><td>0.245</td><td><p>2.756</p></td><td>0.096</td></tr><tr><td>AdaBins (Ours)</td><td>0.964</td><td>0.995</td><td>0.999</td><td>0.058</td><td>0.190</td><td>2.360</td><td>0.088</td></tr></tbody></table>", "caption": "Table 3: Comparison of performances on the KITTI dataset. We compare our network against the state-of-the-art on this dataset. The reported numbers are from the corresponding original papers. Measurements are made for the depth range from 0m to 80m. Best results are in bold, second best are underlined.", "list_citation_info": ["[8] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NIPS, 2014.", "[15] Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with left-right consistency. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6602\u20136611, 2017.", "[26] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[12] Yukang Gan, Xiangyu Xu, Wenxiu Sun, and Liang Lin. Monocular depth estimation with affinity, vertical pooling, and label enhancement. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision \u2013 ECCV 2018, pages 232\u2013247, Cham, 2018. Springer International Publishing.", "[29] Fayao Liu, Chunhua Shen, Guosheng Lin, and I. Reid. Learning depth from single monocular images using deep convolutional neural fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38:2024\u20132039, 2016.", "[47] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.", "[11] Huan Fu, Mingming Gong, Chaohui Wang, Nematollah Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[36] Ashutosh Saxena, Sung H. Chung, and Andrew Y. Ng. Learning depth from single monocular images. In Proceedings of the 18th International Conference on Neural Information Processing Systems, NIPS\u201905, page 1161\u20131168, Cambridge, MA, USA, 2005. MIT Press.", "[24] Yevhen Kuznietsov, J\u00f6rg St\u00fcckler, and Bastian Leibe. Semi-supervised deep learning for monocular depth map prediction. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2215\u20132223, 2017."]}, {"table": "<p>Method\\delta_{1}\\uparrow\\delta_{2}\\uparrow\\delta_{3}\\uparrowREL\\downarrowRMS\\downarrowlog_{10}\\downarrowChen [4]0.7570.9430.9840.1660.4940.071Yin [47]0.6960.9120.9730.1830.5410.082BTS [26]0.7400.9330.9800.1720.5150.075Ours0.7710.9440.9830.1590.4760.068</p>", "caption": "Table 5: Results of models trained on the NYU-Depth-v2 dataset and tested on the SUN RGB-D dataset [39] without fine-tuning.", "list_citation_info": ["[26] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[39] S. Song, S. P. Lichtenberg, and J. Xiao. Sun rgb-d: A rgb-d scene understanding benchmark suite. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 567\u2013576, 2015.", "[4] Xiaotian Chen, Xuejin Chen, and Zheng-Jun Zha. Structure-aware residual pyramid network for monocular depth estimation. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 694\u2013700. International Joint Conferences on Artificial Intelligence Organization, 7 2019.", "[47] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019."]}]}