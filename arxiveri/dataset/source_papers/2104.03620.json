{"title": "Open Domain Generalization with Domain-Augmented Meta-Learning", "abstract": "Leveraging datasets available to learn a model with high generalization ability to unseen domains is important for computer vision, especially when the unseen domain's annotated data are unavailable. We study a novel and practical problem of Open Domain Generalization (OpenDG), which learns from different source domains to achieve high performance on an unknown target domain, where the distributions and label sets of each individual source domain and the target domain can be different. The problem can be generally applied to diverse source domains and widely applicable to real-world applications. We propose a Domain-Augmented Meta-Learning framework to learn open-domain generalizable representations. We augment domains on both feature-level by a new Dirichlet mixup and label-level by distilled soft-labeling, which complements each domain with missing classes and other domain knowledge. We conduct meta-learning over domains by designing new meta-learning tasks and losses to preserve domain unique knowledge and generalize knowledge across domains simultaneously. Experiment results on various multi-domain datasets demonstrate that the proposed Domain-Augmented Meta-Learning (DAML) outperforms prior methods for unseen domain recognition.", "authors": ["Yang Shu", " Zhangjie Cao", " Chenyu Wang", " Jianmin Wang", " Mingsheng Long"], "pdf_url": "https://arxiv.org/abs/2104.03620", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Problem Setting</td><td colspan=\"2\">Label Set</td><td colspan=\"2\">Target Data for Training</td><td rowspan=\"2\">Post-Training on Target Labeled Data</td></tr><tr><td>Same for S Domains</td><td>Same between S&amp;T Domains</td><td>Labeled Data</td><td>Unlabeled Data</td></tr><tr><td>Domain Adaptation [33, 34]</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2717</td></tr><tr><td>Domain Adaptation with Category Shift [37, 2, 54]</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2717</td></tr><tr><td>Multi-Source Domain Adaptation [58]</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2717</td></tr><tr><td>Multi-Source Domain Adaptation with Category Shift [53]</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2717</td></tr><tr><td>Domain Generalization [36]</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2717</td></tr><tr><td>Heterogeneous Domain Generalization [32]</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>\u2713</td></tr><tr><td>The Proposed Open Domain Generalization</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>\u2717</td></tr></table>", "caption": "Table 1: Comparison of the proposed generalization setting with the previous settings related to cross-domain learning. The columns list assumptions made by the problem settings. Note that more \u201c\u2717\u201d means the method needs less assumption and thus is more widely-applicable. We can observe that the proposed open domain generalization problem requires no assumptions on the label set, no target data, and no post-training on target data, which is the most general problem setting. S means source while T means target. Note that \u201cSame between S&amp;T Domains\u201d means the union of all source domain label sets equals the target label set, i.e., whether there are open classes.", "list_citation_info": ["[33] M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning (ICML), 2015.", "[58] Han Zhao, Shanghang Zhang, Guanhang Wu, Jos\u00e9 MF Moura, Joao P Costeira, and Geoffrey J Gordon. Adversarial multiple source domain adaptation. In Advances in Neural Information Processing Systems (NeurIPS), pages 8559\u20138570, 2018.", "[32] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M. Hospedales. Feature-critic networks for heterogeneous domain generalization. In International Conference on Machine Learning (ICML), volume 97, pages 3915\u20133924, 2019.", "[37] Pau Panareda Busto and Juergen Gall. Open set domain adaptation. In IEEE International Conference on Computer Vision (ICCV), Oct 2017.", "[36] Krikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain generalization via invariant feature representation. In International Conference on Machine Learning (ICML), pages 10\u201318, 2013.", "[53] Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, and Liang Lin. Deep cocktail network: Multi-source unsupervised domain adaptation with category shift. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3964\u20133973, 2018."]}, {"table": "<table><tr><td></td><td colspan=\"2\">Art</td><td colspan=\"2\">Sketch</td><td colspan=\"2\">Photo</td><td colspan=\"2\">Cartoon</td><td colspan=\"2\">Avg</td></tr><tr><td>Method</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td></tr><tr><td>AGG</td><td>51.35</td><td>38.87</td><td>49.75</td><td>47.09</td><td>53.15</td><td>44.19</td><td>66.43</td><td>48.98</td><td>55.17\\pm{0.16}</td><td>44.78\\pm{0.33}</td></tr><tr><td>MLDG [27]</td><td>44.59</td><td>31.54</td><td>51.29</td><td>49.91</td><td>62.20</td><td>43.35</td><td>71.64</td><td>55.20</td><td>57.43\\pm{0.14}</td><td>45.00\\pm{0.31}</td></tr><tr><td>FC [32]</td><td>51.12</td><td>39.01</td><td>51.15</td><td>49.28</td><td>60.94</td><td>45.79</td><td>69.32</td><td>52.67</td><td>58.13\\pm{0.20}</td><td>46.69\\pm{0.25}</td></tr><tr><td>Epi-FCR [29]</td><td>\\mathbf{54.16}</td><td>41.16</td><td>46.35</td><td>46.14</td><td>70.03</td><td>48.38</td><td>72.00</td><td>\\mathbf{58.19}</td><td>60.64\\pm{0.22}</td><td>48.47\\pm{0.29}</td></tr><tr><td>PAR [51]</td><td>52.97</td><td>39.21</td><td>53.62</td><td>52.00</td><td>51.86</td><td>36.53</td><td>67.77</td><td>52.05</td><td>56.56\\pm{0.51}</td><td>44.95\\pm{0.57}</td></tr><tr><td>RSC [23]</td><td>50.47</td><td>38.43</td><td>50.17</td><td>44.59</td><td>67.53</td><td>49.82</td><td>67.51</td><td>47.35</td><td>58.92\\pm{0.46}</td><td>45.05\\pm{0.60}</td></tr><tr><td>CuMix [35]</td><td>53.85</td><td>38.67</td><td>37.70</td><td>28.71</td><td>65.67</td><td>49.28</td><td>\\mathbf{74.16}</td><td>47.53</td><td>57.85\\pm{0.32}</td><td>41.05\\pm{0.66}</td></tr><tr><td>DAML (ours)</td><td>54.10</td><td>\\mathbf{43.02}</td><td>\\mathbf{58.50}</td><td>\\mathbf{56.73}</td><td>\\mathbf{75.69}</td><td>\\mathbf{53.29}</td><td>73.65</td><td>54.47</td><td>\\mathbf{65.49}\\pm{0.36}</td><td>\\mathbf{51.88}\\pm{0.42}</td></tr></table>", "caption": "Table 2: Results of PACS dataset under the open-domain setting.", "list_citation_info": ["[35] Massimiliano Mancini, Zeynep Akata, Elisa Ricci, and Barbara Caputo. Towards recognizing unseen categories in unseen domains. In European Conference on Computer Vision (ECCV), August 2020.", "[23] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In European Conference on Computer Vision (ECCV), 2020.", "[29] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic training for domain generalization. In IEEE International Conference on Computer Vision (ICCV), October 2019.", "[32] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M. Hospedales. Feature-critic networks for heterogeneous domain generalization. In International Conference on Machine Learning (ICML), volume 97, pages 3915\u20133924, 2019.", "[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI Conference on Artificial Intelligence (AAAI), 2018.", "[51] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), pages 10506\u201310518, 2019."]}, {"table": "<table><tr><td></td><td colspan=\"2\">Clipart</td><td colspan=\"2\">Real-World</td><td colspan=\"2\">Product</td><td colspan=\"2\">Art</td><td colspan=\"2\">Avg</td></tr><tr><td>Method</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td></tr><tr><td>AGG</td><td>42.83</td><td>\\mathbf{44.98}</td><td>62.40</td><td>53.67</td><td>54.27</td><td>50.11</td><td>42.22</td><td>40.87</td><td>50.43\\pm{0.32}</td><td>47.41\\pm{0.53}</td></tr><tr><td>MLDG [27]</td><td>41.82</td><td>41.26</td><td>62.98</td><td>55.84</td><td>56.89</td><td>52.25</td><td>42.58</td><td>40.97</td><td>51.07\\pm{0.19}</td><td>47.58\\pm{0.42}</td></tr><tr><td>FC [32]</td><td>41.80</td><td>41.65</td><td>63.79</td><td>55.16</td><td>54.41</td><td>52.02</td><td>44.13</td><td>43.25</td><td>51.03\\pm{0.24}</td><td>48.02\\pm{0.57}</td></tr><tr><td>Epi-FCR [29]</td><td>37.13</td><td>42.05</td><td>62.60</td><td>54.73</td><td>54.95</td><td>52.68</td><td>46.33</td><td>44.46</td><td>50.25\\pm{0.50}</td><td>48.48\\pm{0.76}</td></tr><tr><td>PAR [51]</td><td>41.27</td><td>41.77</td><td>65.98</td><td>57.60</td><td>55.37</td><td>54.13</td><td>42.40</td><td>42.62</td><td>51.26\\pm{0.27}</td><td>49.03\\pm{0.41}</td></tr><tr><td>RSC [23]</td><td>38.60</td><td>38.39</td><td>60.85</td><td>53.73</td><td>54.61</td><td>54.66</td><td>44.19</td><td>44.77</td><td>49.56\\pm{0.44}</td><td>47.89\\pm{0.79}</td></tr><tr><td>CuMix [35]</td><td>41.54</td><td>43.07</td><td>64.63</td><td>58.02</td><td>57.74</td><td>55.79</td><td>42.76</td><td>40.72</td><td>51.67\\pm{0.12}</td><td>49.40\\pm{0.27}</td></tr><tr><td>DAML (ours)</td><td>\\mathbf{45.13}</td><td>43.12</td><td>\\mathbf{65.99}</td><td>\\mathbf{60.13}</td><td>\\mathbf{61.54}</td><td>\\mathbf{59.00}</td><td>\\mathbf{53.13}</td><td>\\mathbf{51.11}</td><td>\\mathbf{56.45}\\pm{0.21}</td><td>\\mathbf{53.34}\\pm{0.45}</td></tr></table>", "caption": "Table 3: Results of Office-Home dataset under the open-domain setting.", "list_citation_info": ["[35] Massimiliano Mancini, Zeynep Akata, Elisa Ricci, and Barbara Caputo. Towards recognizing unseen categories in unseen domains. In European Conference on Computer Vision (ECCV), August 2020.", "[23] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In European Conference on Computer Vision (ECCV), 2020.", "[29] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic training for domain generalization. In IEEE International Conference on Computer Vision (ICCV), October 2019.", "[32] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M. Hospedales. Feature-critic networks for heterogeneous domain generalization. In International Conference on Machine Learning (ICML), volume 97, pages 3915\u20133924, 2019.", "[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI Conference on Artificial Intelligence (AAAI), 2018.", "[51] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), pages 10506\u201310518, 2019."]}, {"table": "<table><tr><td>Method</td><td>A</td><td>S</td><td>P</td><td>C</td><td>Avg</td></tr><tr><td>AGG</td><td>77.6</td><td>70.3</td><td>94.4</td><td>73.9</td><td>79.1</td></tr><tr><td>CIDDG [31]</td><td>82.0</td><td>74.8</td><td>94.6</td><td>74.4</td><td>81.4</td></tr><tr><td>MLDG [27]</td><td>79.5</td><td>71.5</td><td>94.3</td><td>77.3</td><td>80.7</td></tr><tr><td>CrossGrad [46]</td><td>78.7</td><td>65.1</td><td>94.0</td><td>73.3</td><td>77.8</td></tr><tr><td>MetaReg [1]</td><td>79.5</td><td>72.2</td><td>94.3</td><td>75.4</td><td>80.4</td></tr><tr><td>JiGen [3]</td><td>79.4</td><td>71.4</td><td>\\mathbf{96.0}</td><td>75.3</td><td>80.4</td></tr><tr><td>MASF [10]</td><td>80.3</td><td>71.7</td><td>94.5</td><td>77.2</td><td>81.0</td></tr><tr><td>Epi-FCR [29]</td><td>82.1</td><td>73.0</td><td>93.9</td><td>77.0</td><td>81.5</td></tr><tr><td>CSD [41]</td><td>79.8</td><td>72.5</td><td>95.5</td><td>75.0</td><td>80.7</td></tr><tr><td>DMG [5]</td><td>76.9</td><td>\\mathbf{75.2}</td><td>93.4</td><td>\\mathbf{80.4}</td><td>81.5</td></tr><tr><td>CuMix [35]</td><td>82.3</td><td>72.6</td><td>95.1</td><td>76.5</td><td>81.6</td></tr><tr><td>DAML</td><td>\\mathbf{83.0}</td><td>74.1</td><td>95.6</td><td>78.1</td><td>\\mathbf{82.7}</td></tr></table>", "caption": "Table 4: Results on closed-set PACS dataset.", "list_citation_info": ["[3] Fabio Maria Carlucci, Antonio D\u2019Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[35] Massimiliano Mancini, Zeynep Akata, Elisa Ricci, and Barbara Caputo. Towards recognizing unseen categories in unseen domains. In European Conference on Computer Vision (ECCV), August 2020.", "[41] Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi. Efficient domain generalization via common-specific low-rank decomposition. In International Conference on Machine Learning (ICML), volume 119, pages 7728\u20137738, 2020.", "[1] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain generalization using meta-regularization. In Advances in Neural Information Processing Systems (NeurIPS), pages 998\u20131008, 2018.", "[29] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic training for domain generalization. In IEEE International Conference on Computer Vision (ICCV), October 2019.", "[46] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing across domains via cross-gradient training. In International Conference on Learning Representations (ICLR), 2018.", "[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI Conference on Artificial Intelligence (AAAI), 2018.", "[5] Prithvijit Chattopadhyay, Yogesh Balaji, and Judy Hoffman. Learning to balance specificity and invariance for in and out of domain generalization. In European Conference in Computer Vision (ECCV), 2020.", "[10] Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "[31] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In European Conference on Computer Vision (ECCV), pages 624\u2013639, 2018."]}, {"table": "<table><tr><td></td><td colspan=\"2\">Clipart</td><td colspan=\"2\">Real</td><td colspan=\"2\">Painting</td><td colspan=\"2\">Sketch</td><td colspan=\"2\">Avg</td></tr><tr><td>Method</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td><td>Acc</td><td>H-score</td></tr><tr><td>AGG</td><td>29.78</td><td>34.06</td><td>65.33</td><td>64.72</td><td>44.30</td><td>51.04</td><td>27.59</td><td>35.41</td><td>41.75\\pm{0.63}</td><td>46.31\\pm{0.57}</td></tr><tr><td>MLDG [27]</td><td>29.66</td><td>35.11</td><td>65.37</td><td>54.40</td><td>44.04</td><td>50.53</td><td>26.83</td><td>34.57</td><td>41.48\\pm{0.68}</td><td>43.65\\pm{0.71}</td></tr><tr><td>FC [32]</td><td>29.91</td><td>35.42</td><td>64.77</td><td>63.65</td><td>44.13</td><td>50.07</td><td>28.56</td><td>34.10</td><td>41.84\\pm{0.73}</td><td>45.81\\pm{0.69}</td></tr><tr><td>Epi-FCR [29]</td><td>27.70</td><td>37.62</td><td>60.31</td><td>64.95</td><td>39.57</td><td>50.24</td><td>26.76</td><td>33.74</td><td>38.59\\pm{1.13}</td><td>46.64\\pm{0.95}</td></tr><tr><td>PAR [51]</td><td>29.29</td><td>39.99</td><td>64.09</td><td>62.59</td><td>42.36</td><td>46.37</td><td>30.21</td><td>39.96</td><td>41.49\\pm{0.63}</td><td>47.23\\pm{0.55}</td></tr><tr><td>RSC [23]</td><td>27.57</td><td>34.98</td><td>60.36</td><td>60.02</td><td>37.76</td><td>42.21</td><td>26.21</td><td>30.44</td><td>37.98\\pm{0.77}</td><td>41.91\\pm{1.28}</td></tr><tr><td>CuMix [35]</td><td>30.03</td><td>40.18</td><td>64.61</td><td>65.07</td><td>44.37</td><td>48.70</td><td>29.72</td><td>33.70</td><td>42.18\\pm{0.45}</td><td>46.91\\pm{0.40}</td></tr><tr><td>DAML (ours)</td><td>\\mathbf{37.62}</td><td>\\mathbf{44.27}</td><td>\\mathbf{66.54}</td><td>\\mathbf{67.80}</td><td>\\mathbf{47.80}</td><td>\\mathbf{52.93}</td><td>\\mathbf{34.48}</td><td>\\mathbf{41.82}</td><td>\\mathbf{46.61}\\pm{0.59}</td><td>\\mathbf{51.71}\\pm{0.52}</td></tr></table>", "caption": "Table 5: Results on the Multi-Datasets scenario (naturally under the open-domain setting).", "list_citation_info": ["[35] Massimiliano Mancini, Zeynep Akata, Elisa Ricci, and Barbara Caputo. Towards recognizing unseen categories in unseen domains. In European Conference on Computer Vision (ECCV), August 2020.", "[23] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In European Conference on Computer Vision (ECCV), 2020.", "[29] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic training for domain generalization. In IEEE International Conference on Computer Vision (ICCV), October 2019.", "[32] Yiying Li, Yongxin Yang, Wei Zhou, and Timothy M. Hospedales. Feature-critic networks for heterogeneous domain generalization. In International Conference on Machine Learning (ICML), volume 97, pages 3915\u20133924, 2019.", "[27] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In AAAI Conference on Artificial Intelligence (AAAI), 2018.", "[51] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), pages 10506\u201310518, 2019."]}]}