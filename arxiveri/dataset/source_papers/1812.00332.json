{"title": "Proxylessnas: Direct Neural Architecture Search on Target Task and Hardware", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours) makes it difficult to \\emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet, our model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being 1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "authors": ["Han Cai", " Ligeng Zhu", " Song Han"], "pdf_url": "https://arxiv.org/abs/1812.00332", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td>Params</td><td>Test error (%)</td></tr><tr><th>DenseNet-BC (Huang et al., 2017)</th><td>25.6M</td><td>3.46</td></tr><tr><th>PyramidNet (Han et al., 2017)</th><td>26.0M</td><td>3.31</td></tr><tr><th>Shake-Shake + c/o (DeVries &amp; Taylor, 2017)</th><td>26.2M</td><td>2.56</td></tr><tr><th>PyramidNet + SD (Yamada et al., 2018)</th><td>26.0M</td><td>2.31</td></tr><tr><th>ENAS + c/o (Pham et al., 2018)</th><td>4.6M</td><td>2.89</td></tr><tr><th>DARTS + c/o (Liu et al., 2018c)</th><td>3.4M</td><td>2.83</td></tr><tr><th>NASNet-A + c/o (Zoph et al., 2018)</th><td>27.6M</td><td>2.40</td></tr><tr><th>PathLevel EAS + c/o (Cai et al., 2018b)</th><td>14.3M</td><td>2.30</td></tr><tr><th>AmoebaNet-B + c/o (Real et al., 2018)</th><td>34.9M</td><td>2.13</td></tr><tr><th>Proxyless-R + c/o (ours)</th><td>5.8M</td><td>2.30</td></tr><tr><th>Proxyless-G + c/o (ours)</th><td>5.7M</td><td>2.08</td></tr></tbody></table>", "caption": "Table 1:  ProxylessNAS achieves state-of-the-art performance on CIFAR-10.", "list_citation_info": ["Huang et al. (2017) Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.", "Real et al. (2018) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.", "DeVries & Taylor (2017) Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.", "Liu et al. (2018c) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018c.", "Cai et al. (2018b) Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation for efficient architecture search. In ICML, 2018b.", "Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018.", "Yamada et al. (2018) Yoshihiro Yamada, Masakazu Iwamura, and Koichi Kise. Shakedrop regularization. arXiv preprint arXiv:1802.02375, 2018.", "Han et al. (2017) Dongyoon Han, Jiwhan Kim, and Junmo Kim. Deep pyramidal residual networks. In CVPR, 2017.", "Pham et al. (2018) Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. In ICML, 2018."]}, {"table": "<table><tbody><tr><th>Model</th><td>Top-1</td><td>Top-5</td><td>GPU latency</td></tr><tr><th>MobileNetV2 (Sandler et al., 2018)</th><td>72.0</td><td>91.0</td><td>6.1ms</td></tr><tr><th>ShuffleNetV2 (1.5) (Ma et al., 2018)</th><td>72.6</td><td>-</td><td>7.3ms</td></tr><tr><th>ResNet-34 (He et al., 2016)</th><td>73.3</td><td>91.4</td><td>8.0ms</td></tr><tr><th>NASNet-A (Zoph et al., 2018)</th><td>74.0</td><td>91.3</td><td>38.3ms</td></tr><tr><th>DARTS (Liu et al., 2018c)</th><td>73.1</td><td>91.0</td><td>-</td></tr><tr><th>MnasNet (Tan et al., 2018)</th><td>74.0</td><td>91.8</td><td>6.1ms</td></tr><tr><th>Proxyless (GPU)</th><td>75.1</td><td>92.5</td><td>5.1ms</td></tr></tbody></table>", "caption": "Table 3: ImageNet Accuracy (%) and GPU latency (Tesla V100) on ImageNet.", "list_citation_info": ["Liu et al. (2018c) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018c.", "Tan et al. (2018) Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. arXiv preprint arXiv:1807.11626, 2018.", "Sandler et al. (2018) Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.", "Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018.", "Ma et al. (2018) Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, 2018.", "He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016."]}, {"table": "<table><tbody><tr><th>Model</th><td>Top-1 (%)</td><td>GPU latency</td><td>CPU latency</td><td>Mobile latency</td></tr><tr><th>Proxyless (GPU)</th><td>75.1</td><td>5.1ms</td><td>204.9ms</td><td>124ms</td></tr><tr><th>Proxyless (CPU)</th><td>75.3</td><td>7.4ms</td><td>138.7ms</td><td>116ms</td></tr><tr><th>Proxyless (mobile)</th><td>74.6</td><td>7.2ms</td><td>164.1ms</td><td>78ms</td></tr></tbody></table>", "caption": "Table 4: Hardware prefers specialized models. Models optimized for GPU does not run fast on CPU and mobile phone, vice versa. ProxylessNAS provides an efficient solution to search a specialized neural network architecture for a target hardware architecture, while cutting down the search cost by 200\\times compared with state-of-the-arts (Zoph &amp; Le, 2017; Tan et al., 2018). ", "list_citation_info": ["Zoph & Le (2017) Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2017."]}]}