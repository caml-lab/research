{"title": "Simultaneously localize, segment and rank the camouflaged objects", "abstract": "Camouflage is a key defence mechanism across species that is critical to survival. Common strategies for camouflage include background matching, imitating the color and pattern of the environment, and disruptive coloration, disguising body outlines [35]. Camouflaged object detection (COD) aims to segment camouflaged objects hiding in their surroundings. Existing COD models are built upon binary ground truth to segment the camouflaged objects without illustrating the level of camouflage. In this paper, we revisit this task and argue that explicitly modeling the conspicuousness of camouflaged objects against their particular backgrounds can not only lead to a better understanding about camouflage and evolution of animals, but also provide guidance to design more sophisticated camouflage techniques. Furthermore, we observe that it is some specific parts of the camouflaged objects that make them detectable by predators. With the above understanding about camouflaged objects, we present the first ranking based COD network (Rank-Net) to simultaneously localize, segment and rank camouflaged objects. The localization model is proposed to find the discriminative regions that make the camouflaged object obvious. The segmentation model segments the full scope of the camouflaged objects. And, the ranking model infers the detectability of different camouflaged objects. Moreover, we contribute a large COD testing set to evaluate the generalization ability of COD models. Experimental results show that our model achieves new state-of-the-art, leading to a more interpretable COD network.", "authors": ["Yunqiu Lv", " Jing Zhang", " Yuchao Dai", " Aixuan Li", " Bowen Liu", " Nick Barnes", " Deng-Ping Fan"], "pdf_url": "https://arxiv.org/abs/2103.04011", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"4\">CAMO</th><th colspan=\"4\">CHAMELEON</th><th colspan=\"4\">COD10K</th><th colspan=\"4\">NC4K</th></tr><tr><th>Method</th><th>S_{\\alpha}\\uparrow</th><th>F^{\\mathrm{mean}}_{\\beta}\\uparrow</th><th>E^{\\mathrm{mean}}_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F^{\\mathrm{mean}}_{\\beta}\\uparrow</th><th>E^{\\mathrm{mean}}_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F^{\\mathrm{mean}}_{\\beta}\\uparrow</th><th>E^{\\mathrm{mean}}_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F^{\\mathrm{mean}}_{\\beta}\\uparrow</th><th>E^{\\mathrm{mean}}_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th></tr></thead><tbody><tr><th>SCRN [53]</th><td>0.702</td><td>0.632</td><td>0.731</td><td>0.106</td><td>0.822</td><td>0.726</td><td>0.833</td><td>0.060</td><td>0.756</td><td>0.623</td><td>0.793</td><td>0.052</td><td>0.793</td><td>0.729</td><td>0.823</td><td>0.068</td></tr><tr><th>CSNet[14]</th><td>0.704</td><td>0.633</td><td>0.753</td><td>0.106</td><td>0.819</td><td>0.759</td><td>0.859</td><td>0.051</td><td>0.745</td><td>0.615</td><td>0.808</td><td>0.048</td><td>0.785</td><td>0.729</td><td>0.834</td><td>0.065</td></tr><tr><th>UCNet [59]</th><td>0.703</td><td>0.640</td><td>0.740</td><td>0.107</td><td>0.833</td><td>0.781</td><td>0.890</td><td>0.049</td><td>0.756</td><td>0.650</td><td>0.823</td><td>0.047</td><td>0.792</td><td>0.751</td><td>0.854</td><td>0.065</td></tr><tr><th>BASNet [36]</th><td>0.644</td><td>0.578</td><td>0.588</td><td>0.143</td><td>0.761</td><td>0.657</td><td>0.797</td><td>0.080</td><td>0.640</td><td>0.579</td><td>0.713</td><td>0.072</td><td>0.724</td><td>0.648</td><td>0.780</td><td>0.089</td></tr><tr><th>SINet [10]</th><td>0.697</td><td>0.579</td><td>0.693</td><td>0.130</td><td>0.820</td><td>0.731</td><td>0.835</td><td>0.069</td><td>0.733</td><td>0.588</td><td>0.768</td><td>0.069</td><td>0.779</td><td>0.696</td><td>0.800</td><td>0.086</td></tr><tr><th>Ours_cod_new</th><td>0.708</td><td>0.645</td><td>0.755</td><td>0.105</td><td>0.842</td><td>0.794</td><td>0.896</td><td>0.046</td><td>0.760</td><td>0.658</td><td>0.831</td><td>0.045</td><td>0.797</td><td>0.758</td><td>0.854</td><td>0.061</td></tr></tbody></table>", "caption": "Table 1: Performance of baseline models trained with our CAM-FR dataset on benchmark testing sets.", "list_citation_info": ["[10] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2777\u20132787, 2020.", "[53] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Int. Conf. Comput. Vis., pages 7264\u20137273, 2019.", "[36] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7479\u20137489, 2019.", "[59] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8582\u20138591, 2020.", "[14] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In Eur. Conf. Comput. Vis., 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>MAE</th><th>r_{MAE}</th></tr></thead><tbody><tr><th>Ours_rank_new</th><td>0.049</td><td>0.139</td></tr><tr><th>SOLOv2[48]</th><td>0.049</td><td>0.210</td></tr><tr><th>MS-RCNN[30]</th><td>0.053</td><td>0.142</td></tr><tr><th>RSDNet[2]</th><td>0.074</td><td>0.293</td></tr></tbody></table>", "caption": "Table 3: Comparison of camouflage ranking methods.", "list_citation_info": ["[48] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic, faster and stronger. arXiv preprint arXiv:2003.10152, 2020.", "[2] Md Amirul Islam, Mahmoud Kalash, and Neil DB Bruce. Revisiting salient object detection: Simultaneous detection, ranking, and subitizing of multiple salient objects. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7142\u20137150, 2018.", "[30] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8759\u20138768, 2018."]}, {"table": "<table><thead><tr><th></th><th colspan=\"4\">CAMO</th><th colspan=\"4\">CHAMELEON</th><th colspan=\"4\">COD10K</th><th colspan=\"4\">NC4K</th></tr><tr><th>Method</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}^{\\mathrm{mean}}\\uparrow</th><th>E_{\\xi}^{\\mathrm{mean}}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}^{\\mathrm{mean}}\\uparrow</th><th>E_{\\xi}^{\\mathrm{mean}}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}^{\\mathrm{mean}}\\uparrow</th><th>E_{\\xi}^{\\mathrm{mean}}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}^{\\mathbf{\\mathrm{mean}}}\\uparrow</th><th>E_{\\xi}^{\\mathbf{\\mathrm{mean}}}\\uparrow</th><th>\\mathcal{M}\\downarrow</th></tr></thead><tbody><tr><th>PiCANet[29]</th><td>0.701</td><td>0.573</td><td>0.716</td><td>0.125</td><td>0.765</td><td>0.618</td><td>0.779</td><td>0.085</td><td>0.696</td><td>0.489</td><td>0.712</td><td>0.081</td><td>0.758</td><td>0.639</td><td>0.773</td><td>0.088</td></tr><tr><th>CPD [52]</th><td>0.716</td><td>0.618</td><td>0.723</td><td>0.113</td><td>0.857</td><td>0.771</td><td>0.874</td><td>0.048</td><td>0.750</td><td>0.595</td><td>0.776</td><td>0.053</td><td>0.790</td><td>0.708</td><td>0.810</td><td>0.071</td></tr><tr><th>SCRN [53]</th><td>0.779</td><td>0.705</td><td>0.796</td><td>0.090</td><td>0.876</td><td>0.787</td><td>0.889</td><td>0.042</td><td>0.789</td><td>0.651</td><td>0.817</td><td>0.047</td><td>0.832</td><td>0.759</td><td>0.855</td><td>0.059</td></tr><tr><th>CSNet[14]</th><td>0.771</td><td>0.705</td><td>0.795</td><td>0.092</td><td>0.856</td><td>0.766</td><td>0.869</td><td>0.047</td><td>0.778</td><td>0.635</td><td>0.810</td><td>0.047</td><td>0.819</td><td>0.748</td><td>0.845</td><td>0.061</td></tr><tr><th>PoolNet [28]</th><td>0.730</td><td>0.643</td><td>0.746</td><td>0.105</td><td>0.845</td><td>0.749</td><td>0.864</td><td>0.054</td><td>0.740</td><td>0.576</td><td>0.776</td><td>0.056</td><td>0.785</td><td>0.699</td><td>0.814</td><td>0.073</td></tr><tr><th>UCNet [59]</th><td>0.739</td><td>0.700</td><td>0.787</td><td>0.094</td><td>0.880</td><td>0.836</td><td>0.930</td><td>0.036</td><td>0.776</td><td>0.681</td><td>0.857</td><td>0.042</td><td>0.813</td><td>0.777</td><td>0.872</td><td>0.055</td></tr><tr><th>F3Net [49]</th><td>0.711</td><td>0.616</td><td>0.741</td><td>0.109</td><td>0.848</td><td>0.770</td><td>0.894</td><td>0.047</td><td>0.739</td><td>0.593</td><td>0.795</td><td>0.051</td><td>0.782</td><td>0.706</td><td>0.825</td><td>0.069</td></tr><tr><th>ITSD[64]</th><td>0.750</td><td>0.663</td><td>0.779</td><td>0.102</td><td>0.814</td><td>0.705</td><td>0.844</td><td>0.057</td><td>0.767</td><td>0.615</td><td>0.808</td><td>0.051</td><td>0.811</td><td>0.729</td><td>0.845</td><td>0.064</td></tr><tr><th>BASNet [36]</th><td>0.615</td><td>0.503</td><td>0.671</td><td>0.124</td><td>0.847</td><td>0.795</td><td>0.883</td><td>0.044</td><td>0.661</td><td>0.486</td><td>0.729</td><td>0.071</td><td>0.698</td><td>0.613</td><td>0.761</td><td>0.094</td></tr><tr><th>NLDF[31]</th><td>0.665</td><td>0.564</td><td>0.664</td><td>0.123</td><td>0.798</td><td>0.714</td><td>0.809</td><td>0.063</td><td>0.701</td><td>0.539</td><td>0.709</td><td>0.059</td><td>0.738</td><td>0.657</td><td>0.748</td><td>0.083</td></tr><tr><th>EGNet [62]</th><td>0.737</td><td>0.655</td><td>0.758</td><td>0.102</td><td>0.856</td><td>0.766</td><td>0.883</td><td>0.049</td><td>0.751</td><td>0.595</td><td>0.793</td><td>0.053</td><td>0.796</td><td>0.718</td><td>0.830</td><td>0.067</td></tr><tr><th>SSAL[60]</th><td>0.644</td><td>0.579</td><td>0.721</td><td>0.126</td><td>0.757</td><td>0.702</td><td>0.849</td><td>0.071</td><td>0.668</td><td>0.527</td><td>0.768</td><td>0.066</td><td>0.699</td><td>0.647</td><td>0.778</td><td>0.092</td></tr><tr><th>SINet [10]</th><td>0.745</td><td>0.702</td><td>0.804</td><td>0.092</td><td>0.872</td><td>0.827</td><td>0.936</td><td>0.034</td><td>0.776</td><td>0.679</td><td>0.864</td><td>0.043</td><td>0.810</td><td>0.772</td><td>0.873</td><td>0.057</td></tr><tr><th>Ours_cod_full</th><td>0.793</td><td>0.725</td><td>0.826</td><td>0.085</td><td>0.893</td><td>0.839</td><td>0.938</td><td>0.033</td><td>0.793</td><td>0.685</td><td>0.868</td><td>0.041</td><td>0.839</td><td>0.779</td><td>0.883</td><td>0.053</td></tr></tbody></table>", "caption": "Table 4: Performance comparison with baseline models on benchmark dataset and our NC4K dataset.", "list_citation_info": ["[10] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2777\u20132787, 2020.", "[53] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Int. Conf. Comput. Vis., pages 7264\u20137273, 2019.", "[52] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3907\u20133916, 2019.", "[36] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7479\u20137489, 2019.", "[62] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for salient object detection. In Int. Conf. Comput. Vis., pages 8779\u20138788, 2019.", "[31] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep features for salient object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 6609\u20136617, 2017.", "[28] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3917\u20133926, 2019.", "[64] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for accurate and fast saliency detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 9141\u20139150, 2020.", "[59] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8582\u20138591, 2020.", "[49] Jun Wei, Shuhui Wang, and Qingming Huang. F33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTnet: Fusion, feedback and focus for salient object detection. In AAAI Conf. Art. Intell., pages 12321\u201312328, 2020.", "[14] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In Eur. Conf. Comput. Vis., 2020.", "[29] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3089\u20133098, 2018.", "[60] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-supervised salient object detection via scribble annotations. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12546\u201312555, 2020."]}]}