{"title": "Quadtree Attention for Vision Transformers", "abstract": "Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks, e.g. with 4.0% improvement in feature matching on ScanNet, about 50% flops reduction in stereo matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at https://github.com/Tangshitao/QuadtreeAttention.", "authors": ["Shitao Tang", " Jiahui Zhang", " Siyu Zhu", " Ping Tan"], "pdf_url": "https://arxiv.org/abs/2201.02767", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th></th><th>AUC@5{}^{\\circ}</th><th>AUC@10{}^{\\circ}</th><th>AUC@20{}^{\\circ}</th></tr></thead><tbody><tr><td rowspan=\"4\">Others</td><td>ContextDesc + SGMNet(Chen et al. (2021))</td><td>15.4</td><td>32.3</td><td>48.8</td></tr><tr><td>SuperPoint + OANet (Zhang et al. (2019b))</td><td>11.8</td><td>26.9</td><td>43.9</td></tr><tr><td>SuperPoint + SuperGlue (Sarlin et al. (2020))</td><td>16.2</td><td>33.8</td><td>51.9</td></tr><tr><td>DRC-Net (Li et al. (2020))</td><td>7.7</td><td>17.9</td><td>30.5</td></tr><tr><td rowspan=\"4\">LoFTR-lite</td><td>Linear Att. (LoFTR) (Katharopoulos et al. (2020))</td><td>16.1</td><td>32.6</td><td>49.0</td></tr><tr><td>PVT  (Wang et al., 2021c)</td><td>16.2</td><td>32.7</td><td>49.2</td></tr><tr><td>QuadTree-A (ours, K=8)</td><td>16.8</td><td>33.4</td><td>50.5</td></tr><tr><td>QuadTree-B (ours, K=8)</td><td>17.4</td><td>34.4</td><td>51.6</td></tr><tr><td rowspan=\"4\">LoFTR</td><td>Linear Att. (LoFTR) \\star (Sun et al. (2021), 64 GPUs)</td><td>22.1</td><td>40.8</td><td>57.6</td></tr><tr><td>Linear Att. (LoFTR) (Katharopoulos et al. (2020))</td><td>21.1</td><td>39.5</td><td>56.6</td></tr><tr><td>QuadTree-B (ours, K=8)</td><td>23.0</td><td>41.7</td><td>58.5</td></tr><tr><td>QuadTree-B\\ast (ours, K=16)</td><td>24.9</td><td>44.7</td><td>61.6</td></tr></tbody></table>", "caption": "Table 1: Results on feature matching. The symbol \\star indicates results cited from  (Sun et al., 2021), where the model is trained with a batch size of 64 on 64 GPUs (a more preferable setting than ours). The symbol \\ast indicates we use the ViT (Dosovitskiy et al., 2020)-like architecture for transformer blocks. For PVT and our method, we replace the original linear attention in LoFTR with corresponding attentions.", "list_citation_info": ["Sun et al. (2021) Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8922\u20138931, 2021.", "Zhang et al. (2019b) Jiahui Zhang, Dawei Sun, Zixin Luo, Anbang Yao, Lei Zhou, Tianwei Shen, Yurong Chen, Long Quan, and Hongen Liao. Learning two-view correspondences and geometry using order-aware network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5845\u20135854, 2019b.", "Chen et al. (2021) Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to match features with seeded graph matching network. arXiv preprint arXiv:2108.08771, 2021.", "Wang et al. (2021c) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021c.", "Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020.", "Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.", "Sarlin et al. (2020) Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4938\u20134947, 2020.", "Li et al. (2020) Xinghui Li, Kai Han, Shuda Li, and Victor Prisacariu. Dual-resolution correspondence networks. Advances in Neural Information Processing Systems, 33, 2020."]}, {"table": "<table><tbody><tr><td></td><td>EPE (px)</td><td>IOU</td><td>Flops (G)</td><td>Mem. (MB)</td></tr><tr><td>GA-Net (Zhang et al., 2019a)</td><td>0.89</td><td>/</td><td>/</td><td>/</td></tr><tr><td>GWC-Net (Guo et al., 2019)</td><td>0.97</td><td>/</td><td>305</td><td>4339</td></tr><tr><td>Bi3D (Badki et al., 2020)</td><td>1.16</td><td>/</td><td>897</td><td>10031</td></tr><tr><td>STTR (Vanilla Transformer) (Li et al., 2021)</td><td>0.45</td><td>0.92</td><td>490</td><td>8507</td></tr><tr><td>QuadTree-B (ours, K=6)</td><td>0.46</td><td>0.99</td><td>254 (52%)</td><td>5381 (63%)</td></tr></tbody></table>", "caption": "Table 2: Results of stereo matching. QuadTree-B achieves similar performance as STTR but with significantly lower flops and memory usage.", "list_citation_info": ["Li et al. (2021) Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X Creighton, Russell H Taylor, and Mathias Unberath. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. IEEE/CVF International Conference on Computer Vision, 2021.", "Badki et al. (2020) Abhishek Badki, Alejandro Troccoli, Kihwan Kim, Jan Kautz, Pradeep Sen, and Orazio Gallo. Bi3d: Stereo depth estimation via binary classifications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1600\u20131608, 2020.", "Zhang et al. (2019a) Feihu Zhang, Victor Prisacariu, Ruigang Yang, and Philip HS Torr. Ga-net: Guided aggregation net for end-to-end stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 185\u2013194, 2019a.", "Guo et al. (2019) Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3273\u20133282, 2019."]}, {"table": "<table><tbody><tr><th></th><td>Param (M)</td><td>Flops (G)</td><td>Top1 (%)</td></tr><tr><th>PVTv2-b0 (Wang et al., 2021b)</th><td>3.7</td><td>0.6</td><td>70.5</td></tr><tr><th>QuadTree-A-b0 (ours)</th><td>3.4</td><td>0.6</td><td>70.9</td></tr><tr><th>QuadTree-B-b0 (ours)</th><td>3.5</td><td>0.7</td><td>72.0</td></tr><tr><th>ResNet18 (He et al., 2016)</th><td>11.7</td><td>1.8</td><td>69.8</td></tr><tr><th>PVTv1-Tiny (Wang et al., 2021c)</th><td>13.2</td><td>2.1</td><td>75.1</td></tr><tr><th>PVTv2-b1 (Wang et al., 2021b)</th><td>14.0</td><td>2.1</td><td>78.7</td></tr><tr><th>QuadTree-B-b1 (ours)</th><td>13.6</td><td>2.3</td><td>80.0</td></tr><tr><th>ResNet50 (He et al., 2016)</th><td>25.1</td><td>4.1</td><td>76.4</td></tr><tr><th>ResNeXt50-32x4d (Xie et al., 2017)</th><td>25.0</td><td>4.3</td><td>77.6</td></tr><tr><th>RegNetY-4G (Radosavovic et al., 2020)</th><td>21.0</td><td>4.0</td><td>80.0</td></tr><tr><th>DeiT-Small/16 (Touvron et al., 2021)</th><td>22.1</td><td>4.6</td><td>79.9</td></tr><tr><th>Swin-T (Liu et al., 2021)</th><td>29.0</td><td>4.5</td><td>81.3</td></tr><tr><th>TNT-S (Han et al., 2021)</th><td>23.8</td><td>5.2</td><td>81.3</td></tr><tr><th>CeiT (Yuan et al., 2021a)</th><td>24.2</td><td>4.5</td><td>82.0</td></tr><tr><th>PVTv2-b2 (Wang et al., 2021c)</th><td>25.4</td><td>4.0</td><td>82.0</td></tr><tr><th>Focal-T (Yang et al., 2021)</th><td>29.1</td><td>4.9</td><td>82.2</td></tr><tr><th>QuadTree-B-b2 (ours)</th><td>24.2</td><td>4.5</td><td>82.7</td></tr><tr><th>ResNet101 (He et al., 2016)</th><td>44.7</td><td>7.9</td><td>77.4</td></tr><tr><th>ResNeXt101-32x4d (Xie et al., 2017)</th><td>44.2</td><td>8.0</td><td>78.8</td></tr><tr><th>RegNetY-8G (Radosavovic et al., 2020)</th><td>39.0</td><td>8.0</td><td>81.7</td></tr><tr><th>CvT-21 (Wu et al., 2021)</th><td>32.0</td><td>7.1</td><td>82.5</td></tr><tr><th>PVTv2-b3 (Wang et al., 2021c)</th><td>45.2</td><td>6.9</td><td>83.2</td></tr><tr><th>Quadtree-B-b3 (ours)</th><td>46.3</td><td>7.8</td><td>83.7</td></tr><tr><th>ResNet152 (He et al., 2016)</th><td>60.2</td><td>11.6</td><td>78.3</td></tr><tr><th>T2T-ViTt-24 (Yuan et al., 2021b)</th><td>64.0</td><td>15.0</td><td>82.2</td></tr><tr><th>Swin-S (Liu et al., 2021)</th><td>50.0</td><td>8.7</td><td>83.0</td></tr><tr><th>Focal-Small (Yang et al., 2021)</th><td>51.1</td><td>9.1</td><td>83.5</td></tr><tr><th>PVTv2-b4 (Wang et al., 2021c)</th><td>62.6</td><td>10.1</td><td>83.6</td></tr><tr><th>Quadtree-B-b4 (ours)</th><td>64.2</td><td>11.5</td><td>84.0</td></tr></tbody></table>", "caption": "Table 3: Image classification results. We report top-1 accuracy on the ImageNet validation set.", "list_citation_info": ["Wu et al. (2021) Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.", "Yuan et al. (2021b) Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021b.", "Yang et al. (2021) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.", "Yuan et al. (2021a) Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021a.", "Wang et al. (2021c) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021c.", "He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.", "Wang et al. (2021b) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021b.", "Radosavovic et al. (2020) Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10428\u201310436, 2020.", "Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "Han et al. (2021) Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.", "Touvron et al. (2021) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pp. 10347\u201310357. PMLR, 2021.", "Xie et al. (2017) Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492\u20131500, 2017."]}, {"table": "<table><tbody><tr><th></th><td>Flops (G)</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><th>PVTv2-b0 (Wang et al., 2021b)</th><td>28.3</td><td>37.2</td><td>57.2</td><td>39.5</td><td>23.1</td><td>40.4</td><td>49.7</td></tr><tr><th>QuadTree-A-b0 (K=32, ours)</th><td>16.0</td><td>37.0</td><td>56.8</td><td>38.9</td><td>22.8</td><td>39.7</td><td>50.0</td></tr><tr><th>QuadTree-B-b0 (K=32, ours)</th><td>16.5</td><td>38.4</td><td>58.7</td><td>41.1</td><td>22.5</td><td>41.7</td><td>51.6</td></tr><tr><th>ResNet18 (He et al., 2016)</th><td>38.6</td><td>31.8</td><td>49.6</td><td>33.6</td><td>16.3</td><td>34.3</td><td>43.2</td></tr><tr><th>PVTv1-Tiny (Wang et al., 2021c)</th><td>72.5</td><td>36.7</td><td>56.9</td><td>38.9</td><td>22.6</td><td>38.8</td><td>50.7</td></tr><tr><th>PVTv2-b1 (Wang et al., 2021b)</th><td>78.8</td><td>41.2</td><td>61.9</td><td>43.9</td><td>25.4</td><td>44.5</td><td>54.3</td></tr><tr><th>Quadtree-B-b1 (K=32, ours)</th><td>56.2</td><td>42.6</td><td>63.6</td><td>45.3</td><td>26.8</td><td>46.1</td><td>57.2</td></tr><tr><th>ResNet50 (He et al., 2016)</th><td>87.3</td><td>36.3</td><td>55.3</td><td>38.6</td><td>19.3</td><td>40.0</td><td>48.8</td></tr><tr><th>ResNet101 (He et al., 2016)</th><td>166.3</td><td>38.5</td><td>57.8</td><td>41.2</td><td>21.4</td><td>42.6</td><td>51.1</td></tr><tr><th>ResNeXt101-32x4d (Xie et al., 2017)</th><td>170.2</td><td>39.9</td><td>59.6</td><td>42.7</td><td>22.3</td><td>44.2</td><td>52.5</td></tr><tr><th>PVTv1-small (Wang et al., 2021c)</th><td>139.8</td><td>36.7</td><td>56.9</td><td>38.9</td><td>25.0</td><td>42.9</td><td>55.7</td></tr><tr><th>PVTv2-b2 (Wang et al., 2021c)</th><td>149.1</td><td>44.6</td><td>65.6</td><td>47.6</td><td>27.4</td><td>48.8</td><td>58.6</td></tr><tr><th>QuadTree-B-b2 (K=32, ours)</th><td>108.6</td><td>46.2</td><td>67.2</td><td>49.5</td><td>29.0</td><td>50.1</td><td>61.8</td></tr><tr><th>PVTv1-Medium (Wang et al., 2021c)</th><td>237.4</td><td>41.9</td><td>63.1</td><td>44.3</td><td>25.0</td><td>44.9</td><td>57.6</td></tr><tr><th>PVTv2-b3 (Wang et al., 2021b)</th><td>243.0</td><td>45.9</td><td>66.8</td><td>49.3</td><td>28.6</td><td>49.8</td><td>61.4</td></tr><tr><th>QuadTree-B-b3 (ours)</th><td>193.9</td><td>47.3</td><td>68.2</td><td>50.6</td><td>30.4</td><td>51.3</td><td>62.9</td></tr><tr><th>PVTv1-Large (Wang et al., 2021c)</th><td>346.6</td><td>42.6</td><td>63.7</td><td>45.4</td><td>25.8</td><td>46.0</td><td>58.4</td></tr><tr><th>PVTv2-b4 (Wang et al., 2021b)</th><td>353.3</td><td>46.1</td><td>66.9</td><td>49.2</td><td>28.4</td><td>50.0</td><td>62.2</td></tr><tr><th>QuadTree-B-b4 (ours)</th><td>283.9</td><td>47.9</td><td>69.1</td><td>51.3</td><td>29.4</td><td>52.2</td><td>63.9</td></tr></tbody></table>", "caption": "Table 4: Object detection results on COCO val2017 with RetinaNet. We use PVTv2 backbone and replace the reduction attention with quadtree attention. \u2018Flops\u2019 is the backbone flops for input image size of 800\\times 1,333.", "list_citation_info": ["Wang et al. (2021c) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021c.", "He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.", "Xie et al. (2017) Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492\u20131500, 2017.", "Wang et al. (2021b) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021b."]}, {"table": "<table><tbody><tr><td></td><td colspan=\"2\">ImageNet-1K</td><td colspan=\"4\">COCO (RetinaNet)</td></tr><tr><td></td><td>Flops (G)</td><td>Top-1 (%)</td><td>Mem. (MB)</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><td>PVTv2  (Wang et al., 2021b)</td><td>0.6</td><td>70.5</td><td>574</td><td>37.2</td><td>57.2</td><td>39.5</td></tr><tr><td>PVTv2+LePE  (Dong et al., 2021)</td><td>0.6</td><td>70.9</td><td>574</td><td>37.6</td><td>57.8</td><td>39.9</td></tr><tr><td>Swin  (Liu et al., 2021)</td><td>0.6</td><td>70.5</td><td>308</td><td>35.3</td><td>54.2</td><td>37.4</td></tr><tr><td>Swin+LePE</td><td>0.6</td><td>70.7</td><td>308</td><td>35.8</td><td>55.3</td><td>37.7</td></tr><tr><td>Focal Attention  (Yang et al., 2021)</td><td>0.7</td><td>71.6</td><td>732</td><td>37.5</td><td>57.6</td><td>39.5</td></tr><tr><td>Focal Attention+LePE</td><td>0.7</td><td>71.5</td><td>732</td><td>37.1</td><td>57.0</td><td>39.4</td></tr><tr><td>QuadTree-B</td><td>0.6</td><td>72.0</td><td>339</td><td>38.4</td><td>58.8</td><td>41.1</td></tr></tbody></table>", "caption": "Table 5: To fairly compare with Swin, PVT, Focal attention and our method, we replace the attention module in PVTv2-b0 with different types of attention and same position encoding method LePE and run image classification and object detection respectively.", "list_citation_info": ["Dong et al. (2021) Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "Yang et al. (2021) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.", "Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "Wang et al. (2021b) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021b."]}, {"table": "<table><thead><tr><th></th><th>AUC@5{}^{\\circ}</th><th>AUC@10{}^{\\circ}</th><th>AUC@20{}^{\\circ}</th></tr></thead><tbody><tr><th>DRC-Net (Li et al., 2020)</th><td>27.0</td><td>43.0</td><td>58.3</td></tr><tr><th>SuperPoint + SuperGlue (Sarlin et al., 2020)</th><td>42.2</td><td>61.2</td><td>76.0</td></tr><tr><th>LoFTR (Sun et al., 2021)</th><td>52.8</td><td>69.2</td><td>81.2</td></tr><tr><th>QuadTree-B (ours, K=16)</th><td>54.6</td><td>70.5</td><td>82.2</td></tr></tbody></table>", "caption": "Table 6: Feature matching results on megadepth. Our method obtains better performance than other methods.", "list_citation_info": ["Sun et al. (2021) Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8922\u20138931, 2021.", "Li et al. (2020) Xinghui Li, Kai Han, Shuda Li, and Victor Prisacariu. Dual-resolution correspondence networks. Advances in Neural Information Processing Systems, 33, 2020.", "Sarlin et al. (2020) Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4938\u20134947, 2020."]}, {"table": "<table><tbody><tr><th></th><td>AP{}^{b}</td><td>AP{}_{50}^{b}</td><td>AP{}_{75}^{b}</td><td>AP{}^{m}</td><td>AP{}^{m}_{50}</td><td>AP{}^{m}_{75}</td></tr><tr><th>PVTv2-b0 (Wang et al., 2021b)</th><td>38.2</td><td>60.5</td><td>40.7</td><td>36.2</td><td>57.8</td><td>38.6</td></tr><tr><th>QuadTree-B-b0 (K=32, ours)</th><td>38.8</td><td>60.7</td><td>42.1</td><td>36.5</td><td>58.0</td><td>39.1</td></tr><tr><th>ResNet18 (He et al., 2016)</th><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td></tr><tr><th>PVTv1-Tiny (Wang et al., 2021c)</th><td>36.7</td><td>59.2</td><td>39.3</td><td>35.1</td><td>56.7</td><td>37.3</td></tr><tr><th>PVTv2-b1 (Wang et al., 2021b)</th><td>41.8</td><td>64.3</td><td>45.9</td><td>38.8</td><td>61.2</td><td>41.6</td></tr><tr><th>Quadtree-B-b1 (K=32, ours)</th><td>43.5</td><td>65.6</td><td>47.6</td><td>40.1</td><td>62.6</td><td>43.3</td></tr><tr><th>ResNet50 (He et al., 2016)</th><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td></tr><tr><th>ResNet101 (He et al., 2016)</th><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td></tr><tr><th>ResNeXt101-32x4d (Xie et al., 2017)</th><td>41.9</td><td>62.5</td><td>45.9</td><td>37.5</td><td>59.4</td><td>40.2</td></tr><tr><th>PVTv1-small (Wang et al., 2021c)</th><td>40.4</td><td>62.9</td><td>43.8</td><td>37.8</td><td>60.1</td><td>40.3</td></tr><tr><th>PVTv2-b2 (Wang et al., 2021b)</th><td>45.3</td><td>67.1</td><td>49.6</td><td>41.2</td><td>64.2</td><td>44.4</td></tr><tr><th>QuadTree-B-b2 (K=32, ours)</th><td>46.7</td><td>68.5</td><td>51.2</td><td>42.4</td><td>65.7</td><td>45.7</td></tr><tr><th>PVTv1-Medium (Wang et al., 2021c)</th><td>42.0</td><td>64.4</td><td>45.6</td><td>39.0</td><td>61.6</td><td>42.1</td></tr><tr><th>PVTv2-b3 (Wang et al., 2021b)</th><td>45.9</td><td>66.8</td><td>49.3</td><td>28.6</td><td>49.8</td><td>61.4</td></tr><tr><th>QuadTree-B-b3</th><td>48.3</td><td>69.6</td><td>52.8</td><td>43.3</td><td>66.8</td><td>46.6</td></tr><tr><th>PVTv1-Large (Wang et al., 2021c)</th><td>42.9</td><td>65.0</td><td>46.6</td><td>39.5</td><td>61.9</td><td>42.5</td></tr><tr><th>PVTv2-b4 (Wang et al., 2021b)</th><td>47.5</td><td>68.7</td><td>52.0</td><td>42.7</td><td>66.1</td><td>46.1</td></tr><tr><th>QuadTree-B-b4</th><td>48.6</td><td>69.5</td><td>53.3</td><td>43.6</td><td>66.9</td><td>47.4</td></tr></tbody></table>", "caption": "Table 7: Object detection results on COCO val2017 with Mask-RCNN. We use PVTv2 backbone and replace the reduction attention with quadtree attention.", "list_citation_info": ["Wang et al. (2021c) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021c.", "He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.", "Xie et al. (2017) Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1492\u20131500, 2017.", "Wang et al. (2021b) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021b."]}, {"table": "<table><tbody><tr><th></th><td>#Params</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><th>QuadTree-B-b0</th><td>23.4</td><td>42.4</td><td>64.5</td><td>45.9</td><td>38.9</td><td>61.6</td><td>41.6</td></tr><tr><th>QuadTree-B-b1</th><td>33.3</td><td>46.4</td><td>68.6</td><td>50.7</td><td>41.9</td><td>65.6</td><td>44.7</td></tr><tr><th>Swin-T (Liu et al., 2021)</th><td>47.8</td><td>46.0</td><td>68.1</td><td>50.3</td><td>41.6</td><td>65.1</td><td>44.9</td></tr><tr><th>Focal-T (Yang et al., 2021)</th><td>48.8</td><td>47.2</td><td>69.4</td><td>51.9</td><td>42.7</td><td>66.5</td><td>45.9</td></tr><tr><th>QuadTree-B-b2</th><td>44.8</td><td>49.3</td><td>70.7</td><td>53.9</td><td>43.9</td><td>67.6</td><td>47.4</td></tr><tr><th>Swin-S (Liu et al., 2021)</th><td>69.1</td><td>48.5</td><td>70.2</td><td>53.5</td><td>43.3</td><td>67.3</td><td>46.6</td></tr><tr><th>Focal-S Yang et al. (2021)</th><td>71.2</td><td>48.8</td><td>70.5</td><td>53.6</td><td>43.8</td><td>67.7</td><td>47.2</td></tr><tr><th>QuadTree-B-b3</th><td>70.0</td><td>49.6</td><td>70.4</td><td>54.2</td><td>44.0</td><td>67.7</td><td>47.5</td></tr></tbody></table>", "caption": "Table 8: Object detection results on COCO val2017 with Mask-RCNN training with 36 epochs and multi-scale data argumentation strategy. We use PVTv2 backbone and replace the reduction attention with quadtree attention.", "list_citation_info": ["Yang et al. (2021) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.", "Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">ImageNet</th><th colspan=\"3\">COCO (RetinaNet)</th></tr><tr><th></th><th>Param. (M)</th><th>Flops (G)</th><th>Top1 (%)</th><th>AP</th><th>AP{}_{50}</th><th>AP{}_{75}</th></tr></thead><tbody><tr><td>Swin-T (Liu et al., 2021)</td><td>29</td><td>4.5</td><td>81.3</td><td>42.0</td><td>\\</td><td>\\</td></tr><tr><td>Focal-T (Yang et al., 2021)</td><td>29</td><td>4.9</td><td>82.2</td><td>43.7</td><td>\\</td><td>\\</td></tr><tr><td>Quadtree-B</td><td>30</td><td>4.6</td><td>82.2</td><td>44.6</td><td>65.8</td><td>47.7</td></tr></tbody></table>", "caption": "Table 9: Comparison under Swin-T settings in image classification and object detection.", "list_citation_info": ["Yang et al. (2021) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.", "Liu et al. (2021) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021."]}]}