{"title": "Polyp-pvt: Polyp Segmentation with Pyramid Vision Transformers", "abstract": "Most polyp segmentation methods use CNNs as their backbone, leading to two key issues when exchanging information between the encoder and decoder: 1) taking into account the differences in contribution between different-level features and 2) designing an effective mechanism for fusing these features. Unlike existing CNN-based methods, we adopt a transformer encoder, which learns more powerful and robust representations. In addition, considering the image acquisition influence and elusive properties of polyps, we introduce three standard modules, including a cascaded fusion module (CFM), a camouflage identification module (CIM), and a similarity aggregation module (SAM). Among these, the CFM is used to collect the semantic and location information of polyps from high-level features; the CIM is applied to capture polyp information disguised in low-level features, and the SAM extends the pixel features of the polyp area with high-level semantic position information to the entire polyp area, thereby effectively fusing cross-level features. The proposed model, named Polyp-PVT, effectively suppresses noises in the features and significantly improves their expressive capabilities. Extensive experiments on five widely adopted datasets show that the proposed model is more robust to various challenging situations (\\emph{e.g.}, appearance changes, small objects, rotation) than existing representative methods. The proposed model is available at https://github.com/DengPingFan/Polyp-PVT.", "authors": ["Bo Dong", " Wenhai Wang", " Deng-Ping Fan", " Jinpeng Li", " Huazhu Fu", " Ling Shao"], "pdf_url": "https://arxiv.org/abs/2108.06932", "list_table_and_caption": [{"table": "<table><tr><td>No.</td><td>Model</td><td>Publication</td><td>Year</td><td>Code</td><td>Type</td><td>Dataset</td><td>Core Components</td></tr><tr><td>1</td><td>CSCPD</td><td>IJPRAI</td><td>2014</td><td>N/A</td><td>IS</td><td>Own</td><td>Adaptive-scale candidate</td></tr><tr><td>2</td><td>APD</td><td>TMI</td><td>2014</td><td>N/A</td><td>IS</td><td>Own</td><td>Geometrical analysis, binary classifier</td></tr><tr><td>3</td><td>SBCP</td><td>SPMB</td><td>2017</td><td>N/A</td><td>IS</td><td>Own</td><td>Superpixel</td></tr><tr><td>4</td><td>FCN</td><td>EMBC</td><td>2018</td><td>N/A</td><td>IS</td><td>DB</td><td>FCN and patch selection</td></tr><tr><td>5</td><td>D-FCN</td><td>JMRR</td><td>2018</td><td>N/A</td><td>IS</td><td>CL, EL, AM, and DB</td><td>FCN and Shape-from-Shading (SfS)</td></tr><tr><td>6</td><td>UNet++</td><td>DLMIA</td><td>2018</td><td>PyTorch</td><td>IS</td><td>AM</td><td>Skip pathways and deep supervision</td></tr><tr><td>7</td><td>Psi-Net</td><td>EMBC</td><td>2019</td><td>PyTorch</td><td>IS</td><td>Endovis</td><td>Shape and boundary aware</td></tr><tr><td>8</td><td>Mask R-CNN</td><td>ISMICT</td><td>2019</td><td>N/A</td><td>IS</td><td>C6, EL, and DB</td><td>Deep feature extractors</td></tr><tr><td>9</td><td>UDC</td><td>ICMLA</td><td>2019</td><td>N/A</td><td>IS</td><td>C6 and EL</td><td>Dilation convolution</td></tr><tr><td>10</td><td>ThresholdNet</td><td>TMI</td><td>2020</td><td>PyTorch</td><td>IS</td><td>ES and WCE</td><td>Learn to thresholdConfidence-guided manifold mixup </td></tr><tr><td>11</td><td>MI2GAN</td><td>MICCAI</td><td>2020</td><td>N/A</td><td>IS</td><td>C6 and EL</td><td>GAN based model</td></tr><tr><td>12</td><td>ACSNet</td><td>MICCAI</td><td>2020</td><td>PyTorch</td><td>IS</td><td>ES and KS</td><td>Adaptive context selection</td></tr><tr><td>13</td><td>PraNet</td><td>MICCAI</td><td>2020</td><td>PyTorch</td><td>IS</td><td>PraNet</td><td>Parallel partial decoder attention</td></tr><tr><td>14</td><td>GAN</td><td>MediaEval</td><td>2020</td><td>N/A</td><td>IS</td><td>KS</td><td>Image-to-image translation</td></tr><tr><td>15</td><td>APS</td><td>MediaEval</td><td>2020</td><td>N/A</td><td>IS</td><td>KS</td><td>Variants of U-shaped structure</td></tr><tr><td>16</td><td>PFA</td><td>MediaEval</td><td>2020</td><td>PyTorch</td><td>IS</td><td>KS</td><td>Pyramid focus augmentation</td></tr><tr><td>17</td><td>MMT</td><td>MediaEval</td><td>2020</td><td>N/A</td><td>IS</td><td>KS</td><td>Competition introduction</td></tr><tr><td>18</td><td>U-Net-ResNet50</td><td>MediaEval</td><td>2020</td><td>N/A</td><td>IS</td><td>KS</td><td>Variants of U-shaped structure</td></tr><tr><td>19</td><td>Survey</td><td>CMIG</td><td>2021</td><td>N/A</td><td>CF</td><td>Own</td><td>Classification</td></tr><tr><td>20</td><td>Polyp-Net</td><td>TIM</td><td>2020</td><td>N/A</td><td>IS</td><td>DB and CV</td><td>Multimodel fusion network</td></tr><tr><td>21</td><td>Deep CNN</td><td>BSPC</td><td>2021</td><td>N/A</td><td>OD</td><td>EL</td><td>Convolutional neural network</td></tr><tr><td>22</td><td>EU-Net</td><td>CRV</td><td>2021</td><td>PyTorch</td><td>IS</td><td>PraNet</td><td>Semantic information enhancement</td></tr><tr><td>23</td><td>DSAS</td><td>MIDL</td><td>2021</td><td>Matlab</td><td>IS</td><td>KS</td><td>Stochastic activation selection</td></tr><tr><td>24</td><td>U-Net-MobileNetV2</td><td>arXiv</td><td>2021</td><td>N/A</td><td>IS</td><td>KS</td><td>Variants of U-shaped structure</td></tr><tr><td>25</td><td>DCRNet</td><td>arXiv</td><td>2021</td><td>PyTorch</td><td>IS</td><td> ES, KS, andPICCOLO </td><td>Within-imageand cross-image contextual relations </td></tr><tr><td>26</td><td>MSEG</td><td>arXiv</td><td>2021</td><td>PyTorch</td><td>IS</td><td>PraNet</td><td>Hardnet and partial decoder</td></tr><tr><td>27</td><td>FSSNet</td><td>arXiv</td><td>2021</td><td>N/A</td><td>IS</td><td>C6 and KS</td><td>Meta-learning</td></tr><tr><td>28</td><td>AG-CUResNeSt</td><td>RIVF</td><td>2021</td><td>N/A</td><td>IS</td><td>PraNet</td><td>ResNeSt, attention gates</td></tr><tr><td>29</td><td>MPAPS</td><td>JBHI</td><td>2021</td><td>PyTorch</td><td>IS</td><td>DB, KS, and EL</td><td>Mutual-prototype adaptation network</td></tr><tr><td>30</td><td>ResUNet++</td><td>JBHI</td><td>2021</td><td>PyTorch</td><td>IS, VS</td><td>PraNet and AM</td><td>ResUNet++, CRF and TTA</td></tr><tr><td>31</td><td>NanoNet</td><td>CBMS</td><td>2021</td><td>PyTorch</td><td>IS, VS</td><td>ED, KS, and KCS</td><td>Real-Time polyp segmentation</td></tr><tr><td>32</td><td>ColonSegNet</td><td>Access</td><td>2021</td><td>PyTorch</td><td>IS</td><td>KS</td><td>Residual block and SENet</td></tr><tr><td>33</td><td>Segtran</td><td>IJCAI</td><td>2021</td><td>PyTorch</td><td>IS</td><td>C6 and KS</td><td>Transformer</td></tr><tr><td>34</td><td>DDANet</td><td>ICPR</td><td>2021</td><td>PyTorch</td><td>IS</td><td>KS</td><td>Dual decoder attention network</td></tr><tr><td>35</td><td>UACANet</td><td>ACM MM</td><td>2021</td><td>PyTorch</td><td>IS</td><td>PraNet</td><td>Uncertainty augmentedContext attention network </td></tr><tr><td>36</td><td>DivergentNet</td><td>ISBI</td><td>2021</td><td>PyTorch</td><td>IS</td><td>EndoCV 2021</td><td>Combine multiple models</td></tr><tr><td>37</td><td>DWHieraSeg</td><td>MIA</td><td>2021</td><td>PyTorch</td><td>IS</td><td>ES</td><td>Dynamic-weighting</td></tr><tr><td>38</td><td>Transfuse</td><td>MICCAI</td><td>2021</td><td>N/A</td><td>IS</td><td>PraNet</td><td>Transformer and CNN</td></tr><tr><td>39</td><td>SANet</td><td>MICCAI</td><td>2021</td><td>PyTorch</td><td>IS</td><td>PraNet</td><td>Shallow attention network</td></tr><tr><td>40</td><td>PNS-Net</td><td>MICCAI</td><td>2021</td><td>PyTorch</td><td>VS</td><td>C6, KS, ES, and AM</td><td>Progressively normalizedself-attention network </td></tr></table>", "caption": "TABLE I: A survey on polyp segmentation. CL = CVC-CLINIC, EL = ETIS-Larib, C6 = CVC-612, AM = ASU-Mayo [41, 42], ES = EndoScene, DB = ColonDB, CV = CVC-VideoClinicDB, C = Colon, ED = Endotect 2020, KS = Kvasir-SEG, KCS = Kvasir Capsule-SEG, PraNet = same to datasets used in PraNet [5], IS = image segmentation, VS = video segmentation, CF = classfication, OD = object detection, Own = private data. CSCPD [1], APD [2], SBCP [3], FCN [21], D-FCN [22], UNet++ [23], Psi-Net [26], Mask R-CNN [27], UDC [25], ThresholdNet [6], MI2GAN [43], ACSNet [44], PraNet [5], GAN [33], APS [45], PFA [34], MMT [46], U-Net-ResNet50 [29], Survey [15], Polyp-Net [30], Deep CNN [31], EU-Net [47], DSAS [48], U-Net-MobileNetV2 [49], DCRNet [39], MSEG [36], FSSNet [50], AG-CUResNeSt [51], MPAPS [52], ResUNet++ [53], NanoNet [54], ColonSegNet [32], Segtran [55], DDANet [35], UACANet [56], DivergentNet [57], DWHieraSeg [58], Transfuse [38], SANet [7], PNS-Net [59]", "list_citation_info": ["[29] S. Alam, N. K. Tomar, A. Thakur, D. Jha, and A. Rauniyar, \u201cAutomatic polyp segmentation using u-net-resnet50,\u201d in MediaEvalW, 2020.", "[31] T. Rahim, S. A. Hassan, and S. Y. Shin, \u201cA deep convolutional neural network for the detection of polyps in colonoscopy images,\u201d BSPC, vol. 68, p. 102654, 2021.", "[2] A. V. Mamonov, I. N. Figueiredo, P. N. Figueiredo, and Y.-H. R. Tsai, \u201cAutomated polyp detection in colon capsule endoscopy,\u201d IEEE TMI, vol. 33, no. 7, pp. 1488\u20131502, 2014.", "[34] V. Thambawita, S. Hicks, P. Halvorsen, and M. A. Riegler, \u201cPyramid-focus-augmentation: Medical image segmentation with step-wise focus,\u201d in MediaEvalW, 2020.", "[35] N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A. Riegler, and P. Halvorsen, \u201cDdanet: Dual decoder attention network for automatic polyp segmentation,\u201d in ICPRW, 2021.", "[38] Y. Zhang, H. Liu, and Q. Hu, \u201cTransfuse: Fusing transformers and cnns for medical image segmentation,\u201d in MICCAI, 2021.", "[32] D. Jha, S. Ali, N. K. Tomar, H. D. Johansen, D. Johansen, J. Rittscher, M. A. Riegler, and P. Halvorsen, \u201cReal-time polyp detection, localization and segmentation in colonoscopy using deep learning,\u201d IEEE Access, vol. 9, pp. 40\u2009496\u201340\u2009510, 2021.", "[56] T. Kim, H. Lee, and D. Kim, \u201cUacanet: Uncertainty augmented context attention for polyp semgnetaion,\u201d in ACM MM, 2021.", "[6] X. Guo, C. Yang, Y. Liu, and Y. Yuan, \u201cLearn to threshold: Thresholdnet with confidence-guided manifold mixup for polyp segmentation,\u201d IEEE TMI, vol. 40, no. 4, pp. 1134\u20131146, 2020.", "[41] Z. Zhou, J. Shin, L. Zhang, S. Gurudu, M. Gotway, and J. Liang, \u201cFine-tuning convolutional neural networks for biomedical image analysis: actively and incrementally,\u201d in CVPR, 2017.", "[43] X. Xie, J. Chen, Y. Li, L. Shen, K. Ma, and Y. Zheng, \u201cMi22{}^{\\mbox{2}}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPTgan: Generative adversarial network for medical image domain adaptation using mutual information constraint,\u201d in MICCAI, 2020.", "[44] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y. Yu, \u201cAdaptive context selection for polyp segmentation,\u201d in MICCAI, 2020.", "[46] D. Jha, S. Hicks, K. Emanuelsen, H. D. Johansen, D. Johansen, T. de Lange, M. A. Riegler, and P. Halvorsen, \u201cMedico multimedia task at mediaeval 2020: Automatic polyp segmentation,\u201d in MediaEvalW, 2020.", "[30] D. Banik, K. Roy, D. Bhattacharjee, M. Nasipuri, and O. Krejcar, \u201cPolyp-net: A multimodel fusion network for polyp segmentation,\u201d IEEE TIM, vol. 70, pp. 1\u201312, 2020.", "[36] C.-H. Huang, H.-Y. Wu, and Y.-L. Lin, \u201cHardnet-mseg: A simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps,\u201d arXiv preprint arXiv:2101.07172, 2021.", "[45] N. K. Tomar, \u201cAutomatic polyp segmentation using fully convolutional neural network,\u201d in MediaEvalW, 2020.", "[57] V. Thambawita, S. A. Hicks, P. Halvorsen, and M. A. Riegler, \u201cDivergentnets: Medical image segmentation by network ensemble,\u201d in ISBI & EndoCV, 2021.", "[33] A. M. A. Ahmed, \u201cGenerative adversarial networks for automatic polyp segmentation,\u201d in MediaEvalW, 2020.", "[59] G.-P. Ji, Y.-C. Chou, D.-P. Fan, G. Chen, D. Jha, H. Fu, and L. Shao, \u201cPns-net: Progressively normalized self-attention network for video polyp segmentation,\u201d in MICCAI, 2021.", "[58] G. Xiaoqing, Y. Chen, and Y. Yixuan, \u201cDynamic-weighting hierarchical segmentation network for medical images,\u201d MIA, p. 102196, 2021.", "[21] M. Akbari, M. Mohrekesh, E. Nasr-Esfahani, S. R. Soroushmehr, N. Karimi, S. Samavi, and K. Najarian, \u201cPolyp segmentation in colonoscopy images using fully convolutional network,\u201d in IEEE EMBC, 2018.", "[55] S. Li, X. Sui, X. Luo, X. Xu, L. Yong, and R. S. M. Goh, \u201cMedical image segmentation using squeeze-and-expansion transformers,\u201d in IJCAI, 2021.", "[54] D. Jha, N. K. Tomar, S. Ali, M. A. Riegler, H. D. Johansen, D. Johansen, T. de Lange, and P. Halvorsen, \u201cNanonet: Real-time polyp segmentation in video capsule endoscopy and colonoscopy,\u201d in IEEE CBMS, 2021.", "[7] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, \u201cShallow attention network for polyp segmentation,\u201d in MICCAI, 2021.", "[5] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, \u201cPranet: Parallel reverse attention network for polyp segmentation,\u201d in MICCAI, 2020.", "[50] R. Khadga, D. Jha, S. Ali, S. Hicks, V. Thambawita, M. A. Riegler, and P. Halvorsen, \u201cFew-shot segmentation of medical images based on meta-learning with implicit gradients,\u201d arXiv preprint arXiv:2106.03223, 2021.", "[3] O. H. Maghsoudi, \u201cSuperpixel based segmentation and classification of polyps in wireless capsule endoscopy,\u201d in IEEE SPMB, 2017.", "[48] A. Lumini, L. Nanni, and G. Maguolo, \u201cDeep ensembles based on stochastic activation selection for polyp segmentation,\u201d in MIDL, 2021.", "[51] D. V. Sang, T. Q. Chung, P. N. Lan, D. V. Hang, D. Van Long, and N. T. Thuy, \u201cAg-curesnest: A novel method for colon polyp segmentation,\u201d in IEEE RIVF, 2021.", "[22] P. Brandao, O. Zisimopoulos, E. Mazomenos, G. Ciuti, J. Bernal, M. Visentini-Scarzanella, A. Menciassi, P. Dario, A. Koulaouzidis, A. Arezzo et al., \u201cTowards a computed-aided diagnosis system in colonoscopy: automatic polyp segmentation using convolution neural networks,\u201d JMRR, vol. 3, no. 02, p. 1840002, 2018.", "[53] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen, P. Halvorsen, and M. A. Riegler, \u201cA comprehensive study on colorectal polyp segmentation with resunet++, conditional random field and test-time augmentation,\u201d IEEE JBHI, vol. 25, no. 6, pp. 2029\u20132040, 2021.", "[23] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, \u201cUnet++: A nested u-net architecture for medical image segmentation,\u201d in DLMIA, 2018.", "[52] C. Yang, X. Guo, M. Zhu, B. Ibragimov, and Y. Yuan, \u201cMutual-prototype adaptation for cross-domain polyp segmentation,\u201d IEEE JBHI, 2021.", "[39] Z. Yin, K. Liang, Z. Ma, and J. Guo, \u201cDuplex contextual relation network for polyp segmentation,\u201d arXiv preprint arXiv:2103.06725, 2021.", "[1] M. Fiori, P. Mus\u00e9, and G. Sapiro, \u201cA complete system for candidate polyps detection in virtual colonoscopy,\u201d IJPRAI, vol. 28, no. 07, p. 1460014, 2014.", "[47] K. Patel, A. M. Bur, and G. Wang, \u201cEnhanced u-net: A feature enhancement network for polyp segmentation,\u201d in CRV, 2021.", "[27] H. A. Qadir, Y. Shin, J. Solhusvik, J. Bergsland, L. Aabakken, and I. Balasingham, \u201cPolyp detection and segmentation using mask r-cnn: Does a deeper feature extractor cnn always perform better?\u201d in ISMICT, 2019.", "[15] T. Rahim, M. A. Usman, and S. Y. Shin, \u201cA survey on contemporary computer-aided tumor, polyp, and ulcer detection methods in wireless capsule endoscopy imaging,\u201d CMIG, p. 101767, 2020.", "[49] M. V. Branch and A. S. Carvalho, \u201cPolyp segmentation in colonoscopy images using u-net-mobilenetv2,\u201d arXiv preprint arXiv:2103.15715, 2021.", "[25] X. Sun, P. Zhang, D. Wang, Y. Cao, and B. Liu, \u201cColorectal polyp segmentation by u-net with dilation convolution,\u201d in IEEE ICMLA, 2019.", "[26] B. Murugesan, K. Sarveswaran, S. M. Shankaranarayana, K. Ram, J. Joseph, and M. Sivaprakasam, \u201cPsi-net: Shape and boundary aware joint multi-task deep network for medical image segmentation,\u201d in IEEE EMBC, 2019."]}, {"table": "<table><tr><td></td><td colspan=\"7\">Kvasir-SEG [13]</td><td colspan=\"7\">ClinicDB [8]</td></tr><tr><td>Model</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td></tr><tr><td>MICCAI\u201915 U-Net</td><td>0.818</td><td>0.746</td><td>0.794</td><td>0.858</td><td>0.881</td><td>0.893</td><td>0.055</td><td>0.823</td><td>0.755</td><td>0.811</td><td>0.889</td><td>0.913</td><td>0.954</td><td>0.019</td></tr><tr><td>DLMIA\u201918 UNet++</td><td>0.821</td><td>0.743</td><td>0.808</td><td>0.862</td><td>0.886</td><td>0.909</td><td>0.048</td><td>0.794</td><td>0.729</td><td>0.785</td><td>0.873</td><td>0.891</td><td>0.931</td><td>0.022</td></tr><tr><td>MICCAI\u201919 SFA</td><td>0.723</td><td>0.611</td><td>0.670</td><td>0.782</td><td>0.834</td><td>0.849</td><td>0.075</td><td>0.700</td><td>0.607</td><td>0.647</td><td>0.793</td><td>0.840</td><td>0.885</td><td>0.042</td></tr><tr><td>arXiv\u201921 MSEG</td><td>0.897</td><td>0.839</td><td>0.885</td><td>0.912</td><td>0.942</td><td>0.948</td><td>0.028</td><td>0.909</td><td>0.864</td><td>0.907</td><td>0.938</td><td>0.961</td><td>0.969</td><td>0.007</td></tr><tr><td>arXiv\u201921 DCRNet</td><td>0.886</td><td>0.825</td><td>0.868</td><td>0.911</td><td>0.933</td><td>0.941</td><td>0.035</td><td>0.896</td><td>0.844</td><td>0.890</td><td>0.933</td><td>0.964</td><td>0.978</td><td>0.010</td></tr><tr><td>MICCAI\u201920 ACSNet</td><td>0.898</td><td>0.838</td><td>0.882</td><td>0.920</td><td>0.941</td><td>0.952</td><td>0.032</td><td>0.882</td><td>0.826</td><td>0.873</td><td>0.927</td><td>0.947</td><td>0.959</td><td>0.011</td></tr><tr><td>MICCAI\u201920 PraNet</td><td>0.898</td><td>0.840</td><td>0.885</td><td>0.915</td><td>0.944</td><td>0.948</td><td>0.030</td><td>0.899</td><td>0.849</td><td>0.896</td><td>0.936</td><td>0.963</td><td>0.979</td><td>0.009</td></tr><tr><td>CRV\u201921 EU-Net</td><td>0.908</td><td>0.854</td><td>0.893</td><td>0.917</td><td>0.951</td><td>0.954</td><td>0.028</td><td>0.902</td><td>0.846</td><td>0.891</td><td>0.936</td><td>0.959</td><td>0.965</td><td>0.011</td></tr><tr><td>MICCAI\u201921 SANet</td><td>0.904</td><td>0.847</td><td>0.892</td><td>0.915</td><td>0.949</td><td>0.953</td><td>0.028</td><td>0.916</td><td>0.859</td><td>0.909</td><td>0.939</td><td>0.971</td><td>0.976</td><td>0.012</td></tr><tr><td>Polyp-PVT (Ours)</td><td>0.917</td><td>0.864</td><td>0.911</td><td>0.925</td><td>0.956</td><td>0.962</td><td>0.023</td><td>0.937</td><td>0.889</td><td>0.936</td><td>0.949</td><td>0.985</td><td>0.989</td><td>0.006</td></tr></table>", "caption": "TABLE IV: Quantitative results of the test datasets, i.e.Kvasir-SEG and ClinicDB. The best results are in boldface.", "list_citation_info": ["[13] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange, D. Johansen, and H. D. Johansen, \u201cKvasir-seg: A segmented polyp dataset,\u201d in MMM, 2020.", "[8] J. Bernal, F. J. S\u00e1nchez, G. Fern\u00e1ndez-Esparrach, D. Gil, C. Rodr\u00edguez, and F. Vilari\u00f1o, \u201cWm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians,\u201d CMIG, vol. 43, pp. 99\u2013111, 2015."]}, {"table": "<table><tr><td></td><td colspan=\"7\">ColonDB [10]</td><td colspan=\"7\">ETIS [9]</td></tr><tr><td>Model</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td></tr><tr><td>MICCAI\u201915 U-Net</td><td>0.512</td><td>0.444</td><td>0.498</td><td>0.712</td><td>0.696</td><td>0.776</td><td>0.061</td><td>0.398</td><td>0.335</td><td>0.366</td><td>0.684</td><td>0.643</td><td>0.740</td><td>0.036</td></tr><tr><td>DLMIA\u201918 UNet++</td><td>0.483</td><td>0.410</td><td>0.467</td><td>0.691</td><td>0.680</td><td>0.760</td><td>0.064</td><td>0.401</td><td>0.344</td><td>0.390</td><td>0.683</td><td>0.629</td><td>0.776</td><td>0.035</td></tr><tr><td>MICCAI\u201919 SFA</td><td>0.469</td><td>0.347</td><td>0.379</td><td>0.634</td><td>0.675</td><td>0.764</td><td>0.094</td><td>0.297</td><td>0.217</td><td>0.231</td><td>0.557</td><td>0.531</td><td>0.632</td><td>0.109</td></tr><tr><td>MICCAI\u201920 ACSNet</td><td>0.716</td><td>0.649</td><td>0.697</td><td>0.829</td><td>0.839</td><td>0.851</td><td>0.039</td><td>0.578</td><td>0.509</td><td>0.530</td><td>0.754</td><td>0.737</td><td>0.764</td><td>0.059</td></tr><tr><td>arXiv\u201921 MSEG</td><td>0.735</td><td>0.666</td><td>0.724</td><td>0.834</td><td>0.859</td><td>0.875</td><td>0.038</td><td>0.700</td><td>0.630</td><td>0.671</td><td>0.828</td><td>0.854</td><td>0.890</td><td>0.015</td></tr><tr><td>arXiv\u201921 DCRNet</td><td>0.704</td><td>0.631</td><td>0.684</td><td>0.821</td><td>0.840</td><td>0.848</td><td>0.052</td><td>0.556</td><td>0.496</td><td>0.506</td><td>0.736</td><td>0.742</td><td>0.773</td><td>0.096</td></tr><tr><td>MICCAI\u201920 PraNet</td><td>0.712</td><td>0.640</td><td>0.699</td><td>0.820</td><td>0.847</td><td>0.872</td><td>0.043</td><td>0.628</td><td>0.567</td><td>0.600</td><td>0.794</td><td>0.808</td><td>0.841</td><td>0.031</td></tr><tr><td>CRV\u201921 EU-Net</td><td>0.756</td><td>0.681</td><td>0.730</td><td>0.831</td><td>0.863</td><td>0.872</td><td>0.045</td><td>0.687</td><td>0.609</td><td>0.636</td><td>0.793</td><td>0.807</td><td>0.841</td><td>0.067</td></tr><tr><td>MICCAI\u201921 SANet</td><td>0.753</td><td>0.670</td><td>0.726</td><td>0.837</td><td>0.869</td><td>0.878</td><td>0.043</td><td>0.750</td><td>0.654</td><td>0.685</td><td>0.849</td><td>0.881</td><td>0.897</td><td>0.015</td></tr><tr><td>Polyp-PVT (Ours)</td><td>0.808</td><td>0.727</td><td>0.795</td><td>0.865</td><td>0.913</td><td>0.919</td><td>0.031</td><td>0.787</td><td>0.706</td><td>0.750</td><td>0.871</td><td>0.906</td><td>0.910</td><td>0.013</td></tr></table>", "caption": "TABLE V: Quantitative results of the test datasets ColonDB and ETIS. The SFA result is generated using the published code.", "list_citation_info": ["[10] N. Tajbakhsh, S. R. Gurudu, and J. Liang, \u201cAutomated polyp detection in colonoscopy videos using shape and context information,\u201d IEEE TMI, vol. 35, no. 2, pp. 630\u2013644, 2015.", "[9] J. Silva, A. Histace, O. Romain, X. Dray, and B. Granado, \u201cToward embedded detection of polyps in wce images for early diagnosis of colorectal cancer,\u201d IJCARS, vol. 9, no. 2, pp. 283\u2013293, 2014."]}, {"table": "<table><tr><td></td><td colspan=\"7\">Endoscene [14]</td></tr><tr><td>Model</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td></tr><tr><td>U-Net</td><td>0.710</td><td>0.627</td><td>0.684</td><td>0.843</td><td>0.847</td><td>0.875</td><td>0.022</td></tr><tr><td>UNet++</td><td>0.707</td><td>0.624</td><td>0.687</td><td>0.839</td><td>0.834</td><td>0.898</td><td>0.018</td></tr><tr><td>SFA</td><td>0.467</td><td>0.329</td><td>0.341</td><td>0.640</td><td>0.644</td><td>0.817</td><td>0.065</td></tr><tr><td>MSEG</td><td>0.874</td><td>0.804</td><td>0.852</td><td>0.924</td><td>0.948</td><td>0.957</td><td>0.009</td></tr><tr><td>ACSNet</td><td>0.863</td><td>0.787</td><td>0.825</td><td>0.923</td><td>0.939</td><td>0.968</td><td>0.013</td></tr><tr><td>DCRNet</td><td>0.856</td><td>0.788</td><td>0.830</td><td>0.921</td><td>0.943</td><td>0.960</td><td>0.010</td></tr><tr><td>PraNet</td><td>0.871</td><td>0.797</td><td>0.843</td><td>0.925</td><td>0.950</td><td>0.972</td><td>0.010</td></tr><tr><td>EU-Net</td><td>0.837</td><td>0.765</td><td>0.805</td><td>0.904</td><td>0.919</td><td>0.933</td><td>0.015</td></tr><tr><td>SANet</td><td>0.888</td><td>0.815</td><td>0.859</td><td>0.928</td><td>0.962</td><td>0.972</td><td>0.008</td></tr><tr><td>Polyp-PVT</td><td>0.900</td><td>0.833</td><td>0.884</td><td>0.935</td><td>0.973</td><td>0.981</td><td>0.007</td></tr></table>", "caption": "TABLE VI: Quantitative results of the test dataset Endoscene. The SFA result is generated using the published code.", "list_citation_info": ["[14] D. V\u00e1zquez, J. Bernal, F. J. S\u00e1nchez, G. Fern\u00e1ndez-Esparrach, A. M. L\u00f3pez, A. Romero, M. Drozdzal, and A. Courville, \u201cA benchmark for endoluminal scene segmentation of colonoscopy images,\u201d JHE, vol. 2017, 2017."]}, {"table": "<table><tr><td></td><td colspan=\"7\">CVC-612-T [8]</td><td colspan=\"7\">CVC-612-V [8]</td></tr><tr><td>Model</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td></tr><tr><td>MICCAI\u201915 U-Net</td><td>0.711</td><td>0.618</td><td>0.694</td><td>0.810</td><td>0.836</td><td>0.853</td><td>0.058</td><td>0.709</td><td>0.597</td><td>0.680</td><td>0.826</td><td>0.855</td><td>0.872</td><td>0.023</td></tr><tr><td>TMI\u201919 UNet++</td><td>0.697</td><td>0.603</td><td>0.688</td><td>0.800</td><td>0.817</td><td>0.865</td><td>0.059</td><td>0.668</td><td>0.557</td><td>0.642</td><td>0.805</td><td>0.830</td><td>0.846</td><td>0.025</td></tr><tr><td>ISM\u201919 ResUNet++</td><td>0.616</td><td>0.512</td><td>0.604</td><td>0.727</td><td>0.758</td><td>0.760</td><td>0.084</td><td>0.750</td><td>0.646</td><td>0.717</td><td>0.829</td><td>0.877</td><td>0.879</td><td>0.023</td></tr><tr><td>MICCAI\u201920 ACSNet</td><td>0.780</td><td>0.697</td><td>0.772</td><td>0.838</td><td>0.864</td><td>0.866</td><td>0.053</td><td>0.801</td><td>0.710</td><td>0.765</td><td>0.847</td><td>0.887</td><td>0.890</td><td>0.054</td></tr><tr><td>MICCAI\u201920 PraNet</td><td>0.833</td><td>0.767</td><td>0.834</td><td>0.886</td><td>0.904</td><td>0.926</td><td>0.038</td><td>0.857</td><td>0.793</td><td>0.855</td><td>0.915</td><td>0.936</td><td>0.965</td><td>0.013</td></tr><tr><td>MICCAI\u201921 PNS-Net</td><td>0.837</td><td>0.765</td><td>0.838</td><td>0.903</td><td>0.903</td><td>0.923</td><td>0.038</td><td>0.851</td><td>0.769</td><td>0.836</td><td>0.923</td><td>0.944</td><td>0.962</td><td>0.012</td></tr><tr><td>Polyp-PVT (Ours)</td><td>0.846</td><td>0.776</td><td>0.850</td><td>0.895</td><td>0.908</td><td>0.926</td><td>0.037</td><td>0.882</td><td>0.810</td><td>0.874</td><td>0.924</td><td>0.963</td><td>0.967</td><td>0.012</td></tr></table>", "caption": "TABLE IX: The result of video polyp segmentation on the i.e.CVC-612-T and CVC-612-V, where the best results are in boldface.", "list_citation_info": ["[8] J. Bernal, F. J. S\u00e1nchez, G. Fern\u00e1ndez-Esparrach, D. Gil, C. Rodr\u00edguez, and F. Vilari\u00f1o, \u201cWm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians,\u201d CMIG, vol. 43, pp. 99\u2013111, 2015."]}, {"table": "<table><tr><td></td><td colspan=\"7\">CVC-300-TV [91]</td></tr><tr><td>Model</td><td>mDic</td><td>mIoU</td><td>{F}_{\\beta}^{w}</td><td>{S}_{\\alpha}</td><td>m{E}_{\\xi}</td><td>max{E}_{\\xi}</td><td>MAE</td></tr><tr><td>U-Net</td><td>0.631</td><td>0.516</td><td>0.567</td><td>0.793</td><td>0.826</td><td>0.849</td><td>0.027</td></tr><tr><td>UNet++</td><td>0.638</td><td>0.527</td><td>0.581</td><td>0.796</td><td>0.831</td><td>0.847</td><td>0.024</td></tr><tr><td>ResUNet++</td><td>0.533</td><td>0.410</td><td>0.469</td><td>0.703</td><td>0.718</td><td>0.720</td><td>0.052</td></tr><tr><td>ACSNet</td><td>0.732</td><td>0.627</td><td>0.703</td><td>0.837</td><td>0.871</td><td>0.875</td><td>0.016</td></tr><tr><td>PraNet</td><td>0.716</td><td>0.624</td><td>0.700</td><td>0.833</td><td>0.852</td><td>0.904</td><td>0.016</td></tr><tr><td>PNS-Net</td><td>0.813</td><td>0.710</td><td>0.778</td><td>0.909</td><td>0.921</td><td>0.942</td><td>0.013</td></tr><tr><td>Ours</td><td>0.880</td><td>0.802</td><td>0.869</td><td>0.915</td><td>0.961</td><td>0.965</td><td>0.011</td></tr></table>", "caption": "TABLE X: Video polyp segmentation results on the CVC-300-TV.", "list_citation_info": ["[91] J. Bernal, J. S\u00e1nchez, and F. Vilarino, \u201cTowards automatic polyp detection with a polyp appearance model,\u201d PR, vol. 45, no. 9, pp. 3166\u20133182, 2012."]}]}