{"title": "Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers", "abstract": "3D visual perception tasks, including 3D detection and map segmentation based on multi-camera images, are essential for autonomous driving systems. In this work, we present a new framework termed BEVFormer, which learns unified BEV representations with spatiotemporal transformers to support multiple autonomous driving perception tasks. In a nutshell, BEVFormer exploits both spatial and temporal information by interacting with spatial and temporal space through predefined grid-shaped BEV queries. To aggregate spatial information, we design spatial cross-attention that each BEV query extracts the spatial features from the regions of interest across camera views. For temporal information, we propose temporal self-attention to recurrently fuse the history BEV information. Our approach achieves the new state-of-the-art 56.9\\% in terms of NDS metric on the nuScenes \\texttt{test} set, which is 9.0 points higher than previous best arts and on par with the performance of LiDAR-based baselines. We further show that BEVFormer remarkably improves the accuracy of velocity estimation and recall of objects under low visibility conditions. The code is available at \\url{https://github.com/zhiqi-li/BEVFormer}.", "authors": ["Zhiqi Li", " Wenhai Wang", " Hongyang Li", " Enze Xie", " Chonghao Sima", " Tong Lu", " Qiao Yu", " Jifeng Dai"], "pdf_url": "https://arxiv.org/abs/2203.17270", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Modality</td><td>Backbone</td><td>NDS\\uparrow</td><td>mAP\\uparrow</td><td>mATE\\downarrow</td><td>mASE\\downarrow</td><td>mAOE\\downarrow</td><td>mAVE\\downarrow</td><td>mAAE\\downarrow</td></tr><tr><td>SSN [55]</td><td>L</td><td>-</td><td>0.569</td><td>0.463</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CenterPoint-Voxel [52]</td><td>L</td><td>-</td><td>0.655</td><td>0.580</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PointPainting [43]</td><td>L\\!\\&amp;\\!C</td><td>-</td><td>0.581</td><td>0.464</td><td>0.388</td><td>0.271</td><td>0.496</td><td>0.247</td><td>0.111</td></tr><tr><td>FCOS3D [45]</td><td>C</td><td>R101</td><td>0.428</td><td>0.358</td><td>0.690</td><td>0.249</td><td>0.452</td><td>1.434</td><td>0.124</td></tr><tr><td>PGD [44]</td><td>C</td><td>R101</td><td>0.448</td><td>0.386</td><td>0.626</td><td>0.245</td><td>0.451</td><td>1.509</td><td>0.127</td></tr><tr><td>BEVFormer-S</td><td>C</td><td>R101</td><td>0.462</td><td>0.409</td><td>0.650</td><td>0.261</td><td>0.439</td><td>0.925</td><td>0.147</td></tr><tr><td>BEVFormer</td><td>C</td><td>R101</td><td>0.535</td><td>0.445</td><td>0.631</td><td>0.257</td><td>0.405</td><td>0.435</td><td>0.143</td></tr><tr><td>DD3D [31]</td><td>C</td><td>V2-99{}^{*}</td><td>0.477</td><td>0.418</td><td>0.572</td><td>0.249</td><td>0.368</td><td>1.014</td><td>0.124</td></tr><tr><td>DETR3D [47]</td><td>C</td><td>V2-99{}^{*}</td><td>0.479</td><td>0.412</td><td>0.641</td><td>0.255</td><td>0.394</td><td>0.845</td><td>0.133</td></tr><tr><td>BEVFormer-S</td><td>C</td><td>V2-99{}^{*}</td><td>0.495</td><td>0.435</td><td>0.589</td><td>0.254</td><td>0.402</td><td>0.842</td><td>0.131</td></tr><tr><td>BEVFormer</td><td>C</td><td>V2-99{}^{*}</td><td>0.569</td><td>0.481</td><td>0.582</td><td>0.256</td><td>0.375</td><td>0.378</td><td>0.126</td></tr></tbody></table>", "caption": "Table 1: 3D detection results on nuScenes test set. * notes that VoVNet-99 (V2-99) [21] was pre-trained on the depth estimation task with extra data [31]. \u201cBEVFormer-S\u201d does not leverage temporal information in the BEV encoder. \u201cL\u201d and \u201cC\u201d indicate LiDAR and Camera, respectively.", "list_citation_info": ["[45] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 913\u2013922 (2021)", "[47] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning. pp. 180\u2013191. PMLR (2022)", "[44] Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting objects in perspective. In: Conference on Robot Learning. pp. 1475\u20131485. PMLR (2022)", "[21] Lee, Y., Hwang, J.w., Lee, S., Bae, Y., Park, J.: An energy and gpu-computation efficient backbone network for real-time object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 0\u20130 (2019)", "[52] Yin, T., Zhou, X., Krahenbuhl, P.: Center-based 3d object detection and tracking. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11784\u201311793 (2021)", "[31] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is pseudo-lidar needed for monocular 3d object detection? In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3142\u20133152 (2021)", "[55] Zhu, X., Ma, Y., Wang, T., Xu, Y., Shi, J., Lin, D.: Ssn: Shape signature networks for multi-class object detection from point clouds. In: European Conference on Computer Vision. pp. 581\u2013597. Springer (2020)", "[43] Vora, S., Lang, A.H., Helou, B., Beijbom, O.: Pointpainting: Sequential fusion for 3d object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4604\u20134612 (2020)"]}, {"table": "<table><thead><tr><th>Method</th><th>Modality</th><th>Backbone</th><th>NDS\\uparrow</th><th>mAP\\uparrow</th><th>mATE\\downarrow</th><th>mASE\\downarrow</th><th>mAOE\\downarrow</th><th>mAVE\\downarrow</th><th>mAAE\\downarrow</th></tr></thead><tbody><tr><td>FCOS3D [45]</td><td>C</td><td>R101</td><td>0.415</td><td>0.343</td><td>0.725</td><td>0.263</td><td>0.422</td><td>1.292</td><td>0.153</td></tr><tr><td>PGD [44]</td><td>C</td><td>R101</td><td>0.428</td><td>0.369</td><td>0.683</td><td>0.260</td><td>0.439</td><td>1.268</td><td>0.185</td></tr><tr><td>DETR3D [47]</td><td>C</td><td>R101</td><td>0.425</td><td>0.346</td><td>0.773</td><td>0.268</td><td>0.383</td><td>0.842</td><td>0.216</td></tr><tr><td>BEVFormer-S</td><td>C</td><td>R101</td><td>0.448</td><td>0.375</td><td>0.725</td><td>0.272</td><td>0.391</td><td>0.802</td><td>0.200</td></tr><tr><td>BEVFormer</td><td>C</td><td>R101</td><td>0.517</td><td>0.416</td><td>0.673</td><td>0.274</td><td>0.372</td><td>0.394</td><td>0.198</td></tr></tbody></table>", "caption": "Table 2: 3D detection results on nuScenes val set. \u201cC\u201d indicates Camera.", "list_citation_info": ["[45] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 913\u2013922 (2021)", "[47] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning. pp. 180\u2013191. PMLR (2022)", "[44] Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting objects in perspective. In: Conference on Robot Learning. pp. 1475\u20131485. PMLR (2022)"]}, {"table": "<table><tbody><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">Modality</td><td colspan=\"4\">Waymo Metrics</td><td colspan=\"5\">Nuscenes Metrics</td></tr><tr><td colspan=\"2\">IoU=0.5</td><td colspan=\"2\">IoU=0.7</td><td rowspan=\"2\">NDS{}^{\\text{\\textdagger}}\\uparrow</td><td rowspan=\"2\">AP\\uparrow</td><td rowspan=\"2\">ATE\\downarrow</td><td rowspan=\"2\">ASE\\downarrow</td><td rowspan=\"2\">AOE\\downarrow</td></tr><tr><td>L1/APH</td><td>L2/APH</td><td>L1/APH</td><td>L2/APH</td></tr><tr><td>PointPillars [20]</td><td>L</td><td>0.866</td><td>0.801</td><td>0.638</td><td>0.557</td><td>0.685</td><td>0.838</td><td>0.143</td><td>0.132</td><td>0.070</td></tr><tr><td>DETR3D [47]</td><td>C</td><td>0.220</td><td>0.216</td><td>0.055</td><td>0.051</td><td>0.394</td><td>0.388</td><td>0.741</td><td>0.156</td><td>0.108</td></tr><tr><td>BEVFormer</td><td>C</td><td>0.280</td><td>0.241</td><td>0.061</td><td>0.052</td><td>0.426</td><td>0.440</td><td>0.679</td><td>0.157</td><td>0.101</td></tr><tr><td>CaDNN{}^{*} [34]</td><td>C</td><td>0.175</td><td>0.165</td><td>0.050</td><td>0.045</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BEVFormer{}^{*}</td><td>C</td><td>0.308</td><td>0.277</td><td>0.077</td><td>0.069</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 3: 3D detection results on Waymo val set under Waymo evaluation metric and nuScenes evaluation metric. \u201cL1\u201d and \u201cL2\u201d refer \u201cLEVEL_1\u201d and \u201cLEVEL_2\u201d difficulties of Waymo [40]. *: Only use thefront camera and only consider object labels in the front camera\u2019s field of view (50.4\u00b0).\u2020: We compute the NDS score by setting ATE and AAE to be 1. \u201cL\u201d and \u201cC\u201d indicate LiDAR and Camera, respectively.", "list_citation_info": ["[34] Reading, C., Harakeh, A., Chae, J., Waslander, S.L.: Categorical depth distribution network for monocular 3d object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8555\u20138564 (2021)", "[47] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning. pp. 180\u2013191. PMLR (2022)", "[40] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., et al.: Scalability in perception for autonomous driving: Waymo open dataset. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2446\u20132454 (2020)", "[20] Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars: Fast encoders for object detection from point clouds. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12697\u201312705 (2019)"]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"2\">Task Head</td><td colspan=\"2\">3D Detection</td><td colspan=\"4\">BEV Segmentation (IoU)</td></tr><tr><td>Det</td><td>Seg</td><td>NDS\\uparrow</td><td>mAP\\uparrow</td><td>Car</td><td>Vehicles</td><td>Road</td><td>Lane</td></tr><tr><th>Lift-Splat{}^{\\text{\\textdagger}} [32]</th><td>\u2717</td><td>\u2713</td><td>-</td><td>-</td><td>32.1</td><td>32.1</td><td>72.9</td><td>20.0</td></tr><tr><th>FIERY{}^{\\text{\\textdagger}} [18]</th><td>\u2717</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>38.2</td><td>-</td><td>-</td></tr><tr><th>VPN{}^{*} [30]</th><td>\u2713</td><td>\u2717</td><td>0.333</td><td>0.253</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>VPN{}^{*}</th><td>\u2717</td><td>\u2713</td><td>-</td><td>-</td><td>31.0</td><td>31.8</td><td>76.9</td><td>19.4</td></tr><tr><th>VPN{}^{*}</th><td>\u2713</td><td>\u2713</td><td>0.334</td><td>0.257</td><td>36.6</td><td>37.3</td><td>76.0</td><td>18.0</td></tr><tr><th>Lift-Splat{}^{*}</th><td>\u2713</td><td>\u2717</td><td>0.397</td><td>0.348</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Lift-Splat{}^{*}</th><td>\u2717</td><td>\u2713</td><td>-</td><td>-</td><td>42.1</td><td>41.7</td><td>77.7</td><td>20.0</td></tr><tr><th>Lift-Splat{}^{*}</th><td>\u2713</td><td>\u2713</td><td>0.410</td><td>0.344</td><td>43.0</td><td>42.8</td><td>73.9</td><td>18.3</td></tr><tr><th>BEVFormer-S</th><td>\u2713</td><td>\u2717</td><td>0.448</td><td>0.375</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BEVFormer-S</th><td>\u2717</td><td>\u2713</td><td>-</td><td>-</td><td>43.1</td><td>43.2</td><td>80.7</td><td>21.3</td></tr><tr><th>BEVFormer-S</th><td>\u2713</td><td>\u2713</td><td>0.453</td><td>0.380</td><td>44.3</td><td>44.4</td><td>77.6</td><td>19.8</td></tr><tr><th>BEVFormer</th><td>\u2713</td><td>\u2717</td><td>0.517</td><td>0.416</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BEVFormer</th><td>\u2717</td><td>\u2713</td><td>-</td><td>-</td><td>44.8</td><td>44.8</td><td>80.1</td><td>25.7</td></tr><tr><th>BEVFormer</th><td>\u2713</td><td>\u2713</td><td>0.520</td><td>0.412</td><td>46.8</td><td>46.7</td><td>77.5</td><td>23.9</td></tr></tbody></table>", "caption": "Table 4: 3D detection and map segmentation results on nuScenes val set.Comparison of training segmentation and detection tasks jointly or not.*: We use VPN [30] and Lift-Splat [32] to replace our BEV encoder for comparison, and the task heads are the same. \u2020: Results from their paper.", "list_citation_info": ["[30] Pan, B., Sun, J., Leung, H.Y.T., Andonian, A., Zhou, B.: Cross-view semantic segmentation for sensing surroundings. IEEE Robotics and Automation Letters 5(3), 4867\u20134873 (2020)", "[32] Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In: European Conference on Computer Vision. pp. 194\u2013210. Springer (2020)", "[18] Hu, A., Murez, Z., Mohan, N., Dudas, S., Hawke, J., Badrinarayanan, V., Cipolla, R., Kendall, A.: Fiery: Future instance prediction in bird\u2019s-eye view from surround monocular cameras. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15273\u201315282 (2021)"]}, {"table": "<table><tbody><tr><td>Method</td><td>Attention</td><td>NDS\\uparrow</td><td>mAP\\uparrow</td><td>mATE\\downarrow</td><td>mAOE\\downarrow</td><td>#Param.</td><td>FLOPs</td><td>Memory</td></tr><tr><td>VPN{}^{*} [30]</td><td>-</td><td>0.334</td><td>0.252</td><td>0.926</td><td>0.598</td><td>111.2M</td><td>924.5G</td><td>\\sim\\!20G</td></tr><tr><td>List-Splat{}^{*} [32]</td><td>-</td><td>0.397</td><td>0.348</td><td>0.784</td><td>0.537</td><td>74.0M</td><td>1087.7G</td><td>\\sim\\!20G</td></tr><tr><td>BEVFormer-S{}^{\\text{\\textdagger}}</td><td>Global</td><td>0.404</td><td>0.325</td><td>0.837</td><td>0.442</td><td>62.1M</td><td>1245.1G</td><td>\\sim\\!36G</td></tr><tr><td>BEVFormer-S{}^{\\ddagger}</td><td>Points</td><td>0.423</td><td>0.351</td><td>0.753</td><td>0.442</td><td>68.1M</td><td>1264.3G</td><td>\\sim\\!20G</td></tr><tr><td>BEVFormer-S</td><td>Local</td><td>0.448</td><td>0.375</td><td>0.725</td><td>0.391</td><td>68.7M</td><td>1303.5G</td><td>\\sim\\!20G</td></tr></tbody></table>", "caption": "Table 5: The detection results of different methods with various BEV encoders on nuScenes val set. \u201cMemory\u201d is the consumed GPU memory during training.*: We use VPN [30] and Lift-Splat [32] to replace BEV encoder of our model for comparison. \u2020: We train BEVFormer-S using global attention in spatial cross-attention, and the model is trained with fp16 weights. In addition, we only adopt single-scale features from the backbone and set the spatial shape of BEV queries to be 100\\!\\times\\!100 to save memory.{\\ddagger}: We degrade the interaction targets of deformable attention from the local region to the reference points only by removing the predicted offsets and weights.", "list_citation_info": ["[30] Pan, B., Sun, J., Leung, H.Y.T., Andonian, A., Zhou, B.: Cross-view semantic segmentation for sensing surroundings. IEEE Robotics and Automation Letters 5(3), 4867\u20134873 (2020)", "[32] Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In: European Conference on Computer Vision. pp. 194\u2013210. Springer (2020)"]}]}