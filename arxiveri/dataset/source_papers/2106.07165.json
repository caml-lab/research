{"title": "Self-Training Guided Adversarial Domain Adaptation for Thermal Imagery", "abstract": "Deep models trained on large-scale RGB image datasets have shown tremendous success. It is important to apply such deep models to real-world problems. However, these models suffer from a performance bottleneck under illumination changes. Thermal IR cameras are more robust against such changes, and thus can be very useful for the real-world problems. In order to investigate efficacy of combining feature-rich visible spectrum and thermal image modalities, we propose an unsupervised domain adaptation method which does not require RGB-to-thermal image pairs. We employ large-scale RGB dataset MS-COCO as source domain and thermal dataset FLIR ADAS as target domain to demonstrate results of our method. Although adversarial domain adaptation methods aim to align the distributions of source and target domains, simply aligning the distributions cannot guarantee perfect generalization to the target domain. To this end, we propose a self-training guided adversarial domain adaptation method to promote generalization capabilities of adversarial domain adaptation methods. To perform self-training, pseudo labels are assigned to the samples on the target thermal domain to learn more generalized representations for the target domain. Extensive experimental analyses show that our proposed method achieves better results than the state-of-the-art adversarial domain adaptation methods. The code and models are publicly available.", "authors": ["Ibrahim Batuhan Akkaya", " Fazil Altinel", " Ugur Halici"], "pdf_url": "https://arxiv.org/abs/2106.07165", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th><p>Bicycle</p></th><th><p>Car</p></th><th><p>Person</p></th><th><p>Average</p></th></tr></thead><tbody><tr><th>Source only</th><td>69.89</td><td>83.89</td><td>86.52</td><td>80.10</td></tr><tr><th>Pixel-DA [2]</th><td>62.53</td><td>89.99</td><td>76.73</td><td>76.42</td></tr><tr><th>DTA [21]</th><td>75.45</td><td>97.65</td><td>92.45</td><td>88.52</td></tr><tr><th>MCD-DA [30]</th><td>81.71</td><td>94.90</td><td>91.83</td><td>89.48</td></tr><tr><th>DANN [8]</th><td>78.16</td><td>95.07</td><td>96.24</td><td>89.82</td></tr><tr><th>CDAN [25]</th><td>78.16</td><td>97.10</td><td>94.82</td><td>90.03</td></tr><tr><th>ADDA [32]</th><td>86.67</td><td>96.95</td><td>89.10</td><td>90.90</td></tr><tr><th>SGADA (ours)</th><td>87.13</td><td>94.44</td><td>92.03</td><td>91.20</td></tr><tr><th>Target only</th><td>87.59</td><td>98.78</td><td>96.35</td><td>94.24</td></tr></tbody></table>", "caption": "Table 1: Per-class classification performance comparison.", "list_citation_info": ["[2] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[8] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedings of the International Conference on Machine Learning (ICML), 2015.", "[32] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[25] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. In Advances in Neural Information Processing Systems (NeurIPS), 2018.", "[21] Seungmin Lee, Dongwan Kim, Namil Kim, and Seong-Gyun Jeong. Drop to adapt: Learning discriminative features for unsupervised domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.", "[30] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018."]}]}