{"title": "Exploring visual prompts for adapting large-scale models", "abstract": "We investigate the efficacy of visual prompting to adapt large-scale models in vision. Following the recent approach from prompt tuning and adversarial reprogramming, we learn a single image perturbation such that a frozen model prompted with this perturbation performs a new task. Through comprehensive experiments, we demonstrate that visual prompting is particularly effective for CLIP and robust to distribution shift, achieving performance competitive with standard linear probes. We further analyze properties of the downstream dataset, prompt design, and output transformation in regard to adaptation performance. The surprising effectiveness of visual prompting provides a new perspective on adapting pre-trained models in vision. Code is available at http://hjbahng.github.io/visual_prompting .", "authors": ["Hyojin Bahng", " Ali Jahanian", " Swami Sankaranarayanan", " Phillip Isola"], "pdf_url": "https://arxiv.org/abs/2203.17274", "list_table_and_caption": [{"table": "<br/><table><thead><tr><th>Model</th><th>Architecture</th><th>Modality</th><th>Pre-trained Dataset</th><th>Objective</th></tr></thead><tbody><tr><td>CLIP [37]</td><td>ViT-B/32</td><td>Vision-language</td><td>400M image-text pairs</td><td>Contrastive</td></tr><tr><td>Instagram [32]</td><td>ResNext101-32x8d</td><td>Vision</td><td>3.5B Instagram photos</td><td>Cross Entropy</td></tr><tr><td>BiT-M [24]</td><td>ResNet-50</td><td>Vision</td><td>14M ImageNet-21k</td><td>Cross Entropy</td></tr><tr><td>RN50 [16]</td><td>ResNet-50</td><td>Vision</td><td>1.2M ImageNet-1k</td><td>Cross Entropy</td></tr></tbody></table>", "caption": "Table 3: Overview of pre-trained models.", "list_citation_info": ["[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.", "[32] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pages 181\u2013196, 2018.", "[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.", "[24] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 491\u2013507. Springer, 2020."]}]}