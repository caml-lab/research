{"title": "Self-supervised pre-training for transformer-based person re-identification", "abstract": "Transformer-based supervised pre-training achieves great performance in person re-identification (ReID). However, due to the domain gap between ImageNet and ReID datasets, it usually needs a larger pre-training dataset (e.g. ImageNet-21K) to boost the performance because of the strong data fitting ability of the transformer. To address this challenge, this work targets to mitigate the gap between the pre-training and ReID datasets from the perspective of data and model structure, respectively. We first investigate self-supervised learning (SSL) methods with Vision Transformer (ViT) pretrained on unlabelled person images (the LUPerson dataset), and empirically find it significantly surpasses ImageNet supervised pre-training models on ReID tasks. To further reduce the domain gap and accelerate the pre-training, the Catastrophic Forgetting Score (CFS) is proposed to evaluate the gap between pre-training and fine-tuning data. Based on CFS, a subset is selected via sampling relevant data close to the down-stream ReID data and filtering irrelevant data from the pre-training dataset. For the model structure, a ReID-specific module named IBN-based convolution stem (ICS) is proposed to bridge the domain gap by learning more invariant features. Extensive experiments have been conducted to fine-tune the pre-training models under supervised learning, unsupervised domain adaptation (UDA), and unsupervised learning (USL) settings. We successfully downscale the LUPerson dataset to 50% with no performance degradation. Finally, we achieve state-of-the-art performance on Market-1501 and MSMT17. For example, our ViT-S/16 achieves 91.3%/89.9%/89.6% mAP accuracy on Market1501 for supervised/UDA/USL ReID. Codes and models will be released to https://github.com/michuanhaohao/TransReID-SSL.", "authors": ["Hao Luo", " Pichao Wang", " Yi Xu", " Feng Ding", " Yanxin Zhou", " Fan Wang", " Hao Li", " Rong Jin"], "pdf_url": "https://arxiv.org/abs/2111.12084", "list_table_and_caption": [{"table": "Pre-trainingMarketMSMT17mAPR1mAPR1Full (100%)90.395.464.283.4Random (50%)89.995.262.982.6Cluster (50%) [4]90.095.463.883.3CFS (50%)91.096.066.184.6<table><tbody><tr><th rowspan=\"2\">Pre-training</th><td colspan=\"2\">Market</td><td colspan=\"2\">MSMT17</td></tr><tr><td>mAP</td><td>R1</td><td>mAP</td><td>R1</td></tr><tr><th>MocoV2+R50 [14]</th><td>88.2</td><td>94.8</td><td>53.3</td><td>76.0</td></tr><tr><th>+CFS (50%)</th><td>89.4</td><td>95.5</td><td>56.8</td><td>78.8</td></tr><tr><th>DINO+ViT-S</th><td>90.3</td><td>95.4</td><td>64.2</td><td>83.4</td></tr><tr><th>+CFS (50%)</th><td>91.0</td><td>96.0</td><td>66.1</td><td>84.6</td></tr></tbody></table>", "caption": "Table 4:  Comparison of different data selection strategies. Random sampling (Random) and selecting data close to cluster centers (Cluster) are compared.", "list_citation_info": ["[14] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsupervised pre-training for person re-identification. In CVPR, pages 14750\u201314759, 2021.", "[4] Shuvam Chakraborty, Burak Uzkent, Kumar Ayush, Kumar Tanmay, Evan Sheehan, and Stefano Ermon. Efficient conditional pre-training for transfer learning. arXiv preprint arXiv:2011.10231, 2020."]}, {"table": "<table><tbody><tr><th></th><td></td><th colspan=\"2\">Market</th><th colspan=\"2\">MSMT17</th></tr><tr><th>Methods</th><td>Backbone</td><th>mAP</th><td>R1</td><th>mAP</th><td>R1</td></tr><tr><th>BOT [31]</th><td>R50-IBN</td><th>88.2</th><td>95.0</td><th>57.8</th><td>80.7</td></tr><tr><th>BOT{}^{*} [31]</th><td>R50-IBN</td><th>90.1</th><td>95.6</td><th>60.8</th><td>81.4</td></tr><tr><th>MGN [39]</th><td>R50\\uparrow384</td><th>87.5</th><td>95.1</td><th>63.7</th><td>85.1</td></tr><tr><th>SCSN [7]</th><td>R50\\uparrow384</td><th>88.5</th><td>95.7</td><th>58.5</th><td>83.8</td></tr><tr><th>ABDNet [5]</th><td>R50\\uparrow384</td><th>88.3</th><td>95.6</td><th>60.8</th><td>82.3</td></tr><tr><th>TransReID[19]</th><td>ViT-B\\uparrow384</td><th>89.5</th><td>95.2</td><th>69.4</th><td>89.2</td></tr><tr><th>MoCoV2{}^{*} [14]</th><td>MGN\\uparrow384</td><th>91.0</th><td>96.4</td><th>65.7</th><td>85.5</td></tr><tr><th>Ours{}^{*}</th><td>ViT{}_{I}-S</td><th>91.3</th><td>96.2</td><th>68.1</th><td>86.1</td></tr><tr><th>Ours{}^{*}</th><td>ViT{}_{I}-S\\uparrow384</td><th>91.7</th><td>96.3</td><th>70.5</th><td>87.8</td></tr><tr><th>Ours{}^{*}</th><td>ViT{}_{I}-B\\uparrow384</td><th>93.2</th><td>96.7</td><th>75.0</th><td>89.5</td></tr></tbody></table>", "caption": "Table 7: Comparison to state-of-the-art methods in supervised ReID. MGN is the improved version in fast-reid. {}^{*} means that backbones are pre-trained on LUPerson. \\uparrow384 represents that images are resized to 384\\times 128. From the perspective of computational complexity, ViT{}_{I}-S and ViT{}_{I}-B can be compared with R50/R50-IBN and MGN, respectively. R50-IBN stands for ResNet50-IBN-a.", "list_citation_info": ["[39] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi Zhou. Learning discriminative features with multiple granularities for person re-identification. In ACMMM, pages 274\u2013282, 2018.", "[7] Xuesong Chen, Canmiao Fu, Yong Zhao, Feng Zheng, Jingkuan Song, Rongrong Ji, and Yi Yang. Salience-guided cascaded suppression network for person re-identification. In CVPR, pages 3300\u20133310, 2020.", "[19] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang. Transreid: Transformer-based object re-identification. In ICCV, pages 15013\u201315022, October 2021.", "[14] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsupervised pre-training for person re-identification. In CVPR, pages 14750\u201314759, 2021.", "[5] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-net: Attentive but diverse person re-identification. In ICCV, 2019.", "[31] Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao, Shenqi Lai, and Jianyang Gu. A strong baseline and batch normalization neck for deep person re-identification. TMM, 22(10):2597\u20132609, 2019."]}, {"table": "<table><tbody><tr><th></th><td></td><th colspan=\"2\">Market</th><th colspan=\"2\">MSMT17</th></tr><tr><th>Methods</th><td>Backbone</td><th>mAP</th><td>R1</td><th>mAP</th><td>R1</td></tr><tr><th>MMCL [46]</th><td>R50</td><th>45.5</th><td>80.3</td><th>11.2</th><td>35.4</td></tr><tr><th>HCT [47]</th><td>R50</td><th>56.4</th><td>80.0</td><th>-</th><td>-</td></tr><tr><th>IICS [44]</th><td>R50</td><th>72.9</th><td>89.5</td><th>26.9</th><td>52.4</td></tr><tr><th>SPCL{}^{*}[17]</th><td>R50</td><th>76.2</th><td>90.2</td><th>-</th><td>-</td></tr><tr><th>C-Contrast [11]</th><td>R50</td><th>82.6</th><td>93.0</td><th>33.1</th><td>63.3</td></tr><tr><th>C-Contrast{}^{*} [11]</th><td>R50</td><th>84.0</th><td>93.4</td><th>31.4</th><td>58.8</td></tr><tr><th>C-Contrast{}^{*} [11]</th><td>R50-IBN</td><th>86.4</th><td>94.2</td><th>39.8</th><td>66.1</td></tr><tr><th>Ours{}^{*}</th><td>ViT-S</td><th>88.2</th><td>94.2</td><th>40.9</th><td>66.4</td></tr><tr><th>Ours{}^{*}</th><td>ViT{}_{I}-S</td><th>89.6</th><td>95.3</td><th>50.6</th><td>75.0</td></tr></tbody></table>", "caption": "Table 8: Comparison to state-of-the-art methods in USL ReID. {}^{*} means that backbones are pre-trained on LUPerson.", "list_citation_info": ["[47] Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo. Hierarchical clustering with hard-batch triplet loss for person re-identification. In CVPR, pages 13657\u201313665, 2020.", "[11] Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Siyu Zhu, and Ping Tan. Cluster contrast for unsupervised person re-identification. arXiv preprint arXiv:2103.11568, 2021.", "[44] Shiyu Xuan and Shiliang Zhang. Intra-inter camera similarity for unsupervised person re-identification. In CVPR, pages 11926\u201311935, 2021.", "[17] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hongsheng Li. Self-paced contrastive learning with hybrid memory for domain adaptive object re-id. In NeurIPS, 2020.", "[46] Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong, and Jian-Huang Lai. Unsupervised person re-identification by soft multilabel learning. In CVPR, pages 2148\u20132157, 2019."]}, {"table": "<table><tbody><tr><th></th><td></td><th colspan=\"2\">MS2MA</th><th colspan=\"2\">MA2MS</th></tr><tr><th>Methods</th><td>Backbone</td><th>mAP</th><td>R1</td><th>mAP</th><td>R1</td></tr><tr><th>DG-Net++ [52]</th><td>R50</td><th>64.6</th><td>83.1</td><th>22.1</th><td>48.4</td></tr><tr><th>MMT[16]</th><td>R50</td><th>75.6</th><td>89.3</td><th>24.0</th><td>50.1</td></tr><tr><th>SPCL[17]</th><td>R50</td><th>77.5</th><td>89.7</td><th>26.8</th><td>53.7</td></tr><tr><th>SPCL[17]</th><td>R50-IBN</td><th>79.9</th><td>92.0</td><th>31.0</th><td>58.1</td></tr><tr><th>C-Contrast [11]</th><td>R50</td><th>82.4</th><td>92.5</td><th>33.4</th><td>60.5</td></tr><tr><th>C-Contrast{}^{*} [11]</th><td>R50</td><th>85.1</th><td>94.4</td><th>28.3</th><td>53.8</td></tr><tr><th>C-Contrast{}^{*} [11]</th><td>R50-IBN</td><th>86.9</th><td>94.6</td><th>42.6</th><td>69.1</td></tr><tr><th>Ours{}^{*}</th><td>ViT-S</td><th>89.4</th><td>95.4</td><th>47.4</th><td>70.8</td></tr><tr><th>Ours{}^{*}</th><td>ViT{}_{I}-S</td><th>89.9</th><td>95.5</td><th>57.8</th><td>79.5</td></tr></tbody></table>", "caption": "Table 9: Comparison to state-of-the-art methods in UDA ReID. {}^{*} means that backbones are pre-trained on LUPerson.", "list_citation_info": ["[11] Zuozhuo Dai, Guangyuan Wang, Weihao Yuan, Siyu Zhu, and Ping Tan. Cluster contrast for unsupervised person re-identification. arXiv preprint arXiv:2103.11568, 2021.", "[16] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. In ICLR, 2020.", "[52] Yang Zou, Xiaodong Yang, Zhiding Yu, BVK Vijaya Kumar, and Jan Kautz. Joint disentangling and adaptation for cross-domain person re-identification. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 87\u2013104. Springer, 2020.", "[17] Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, and Hongsheng Li. Self-paced contrastive learning with hybrid memory for domain adaptive object re-id. In NeurIPS, 2020."]}]}