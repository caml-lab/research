{"title": "Uncertainty-aware joint salient object and camouflaged object detection", "abstract": "Visual salient object detection (SOD) aims at finding the salient object(s) that attract human attention, while camouflaged object detection (COD) on the contrary intends to discover the camouflaged object(s) that hidden in the surrounding. In this paper, we propose a paradigm of leveraging the contradictory information to enhance the detection ability of both salient object detection and camouflaged object detection. We start by exploiting the easy positive samples in the COD dataset to serve as hard positive samples in the SOD task to improve the robustness of the SOD model. Then, we introduce a similarity measure module to explicitly model the contradicting attributes of these two tasks. Furthermore, considering the uncertainty of labeling in both tasks' datasets, we propose an adversarial learning network to achieve both higher order similarity measure and network confidence estimation. Experimental results on benchmark datasets demonstrate that our solution leads to state-of-the-art (SOTA) performance for both tasks.", "authors": ["Aixuan Li", " Jing Zhang", " Yunqiu Lv", " Bowen Liu", " Tong Zhang", " Yuchao Dai"], "pdf_url": "https://arxiv.org/abs/2104.02628", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"4\">DUTS</th><th colspan=\"4\">ECSSD</th><th colspan=\"4\">DUT</th><th colspan=\"4\">HKU-IS</th><th colspan=\"4\">THUR</th><th colspan=\"4\">SOC</th></tr><tr><th>Method</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th></tr></thead><tbody><tr><th>NLDF [33]</th><td>.816</td><td>.757</td><td>.851</td><td>.065</td><td>.870</td><td>.871</td><td>.896</td><td>.066</td><td>.770</td><td>.683</td><td>.798</td><td>.080</td><td>.879</td><td>.871</td><td>.914</td><td>.048</td><td>.801</td><td>.711</td><td>.827</td><td>.081</td><td>.816</td><td>.319</td><td>.837</td><td>.106</td></tr><tr><th>PiCANet [30]</th><td>.842</td><td>.757</td><td>.853</td><td>.062</td><td>.898</td><td>.872</td><td>.909</td><td>.054</td><td>.817</td><td>.711</td><td>.823</td><td>.072</td><td>.895</td><td>.854</td><td>.910</td><td>.046</td><td>.818</td><td>.710</td><td>.821</td><td>.084</td><td>.801</td><td>.332</td><td>.810</td><td>.133</td></tr><tr><th>CPD [47]</th><td>.869</td><td>.821</td><td>.898</td><td>.043</td><td>.913</td><td>.909</td><td>.937</td><td>.040</td><td>.825</td><td>.742</td><td>.847</td><td>.056</td><td>.906</td><td>.892</td><td>.938</td><td>.034</td><td>.835</td><td>.750</td><td>.853</td><td>.068</td><td>.841</td><td>.356</td><td>.862</td><td>.093</td></tr><tr><th>SCRN [48]</th><td>.885</td><td>.833</td><td>.900</td><td>.040</td><td>.920</td><td>.910</td><td>.933</td><td>.041</td><td>.837</td><td>.749</td><td>.847</td><td>.056</td><td>.916</td><td>.894</td><td>.935</td><td>.034</td><td>.845</td><td>.758</td><td>.858</td><td>.066</td><td>.838</td><td>.363</td><td>.859</td><td>.099</td></tr><tr><th>PoolNet [29]</th><td>.887</td><td>.840</td><td>.910</td><td>.037</td><td>.919</td><td>.913</td><td>.938</td><td>.038</td><td>.831</td><td>.748</td><td>.848</td><td>.054</td><td>.919</td><td>.903</td><td>.945</td><td>.030</td><td>.834</td><td>.745</td><td>.850</td><td>.070</td><td>.829</td><td>.355</td><td>.846</td><td>.106</td></tr><tr><th>BASNet [38]</th><td>.876</td><td>.823</td><td>.896</td><td>.048</td><td>.910</td><td>.913</td><td>.938</td><td>.040</td><td>.836</td><td>.767</td><td>.865</td><td>.057</td><td>.909</td><td>.903</td><td>.943</td><td>.032</td><td>.823</td><td>.737</td><td>.841</td><td>.073</td><td>.841</td><td>.359</td><td>.864</td><td>.092</td></tr><tr><th>EGNet [57]</th><td>.878</td><td>.824</td><td>.898</td><td>.043</td><td>.914</td><td>.906</td><td>.933</td><td>.043</td><td>.840</td><td>.755</td><td>.855</td><td>.054</td><td>.917</td><td>.900</td><td>.943</td><td>.031</td><td>.839</td><td>.752</td><td>.854</td><td>.068</td><td>.858</td><td>.353</td><td>.873</td><td>.078</td></tr><tr><th>AFNet [12]</th><td>.867</td><td>.812</td><td>.893</td><td>.046</td><td>.907</td><td>.901</td><td>.929</td><td>.045</td><td>.826</td><td>.743</td><td>.846</td><td>.057</td><td>.905</td><td>.888</td><td>.934</td><td>.036</td><td>.825</td><td>.733</td><td>.840</td><td>.072</td><td>.700</td><td>.062</td><td>.684</td><td>.115</td></tr><tr><th>CSNet [15]</th><td>.884</td><td>.834</td><td>.907</td><td>.040</td><td>.920</td><td>.911</td><td>.940</td><td>.038</td><td>.836</td><td>.750</td><td>.852</td><td>.055</td><td>.918</td><td>.900</td><td>.944</td><td>.031</td><td>.841</td><td>.756</td><td>.856</td><td>.068</td><td>.834</td><td>.352</td><td>.850</td><td>.103</td></tr><tr><th>F3Net [46]</th><td>.888</td><td>.852</td><td>.920</td><td>.035</td><td>.919</td><td>.921</td><td>.943</td><td>.036</td><td>.839</td><td>.766</td><td>.864</td><td>.053</td><td>.917</td><td>.910</td><td>.952</td><td>.028</td><td>.838</td><td>.761</td><td>.858</td><td>.066</td><td>.828</td><td>.340</td><td>.846</td><td>.098</td></tr><tr><th>ITSD [59]</th><td>.886</td><td>.841</td><td>.917</td><td>.039</td><td>.920</td><td>.916</td><td>.943</td><td>.037</td><td>.842</td><td>.767</td><td>.867</td><td>.056</td><td>.921</td><td>.906</td><td>.950</td><td>.030</td><td>.836</td><td>.753</td><td>.852</td><td>.070</td><td>.773</td><td>.361</td><td>.792</td><td>.166</td></tr><tr><th>Ours</th><td>.899</td><td>.866</td><td>.937</td><td>.032</td><td>.933</td><td>.935</td><td>.960</td><td>.030</td><td>.850</td><td>.782</td><td>.884</td><td>.051</td><td>.931</td><td>.924</td><td>.867</td><td>.026</td><td>.849</td><td>.774</td><td>.872</td><td>.065</td><td>.845</td><td>.374</td><td>.856</td><td>.092</td></tr></tbody></table>", "caption": "Table 2: Performance comparison with benchmark saliency detection models.", "list_citation_info": ["[30] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3089\u20133098, 2018.", "[59] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for accurate and fast saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9141\u20139150, 2020.", "[12] Mengyang Feng, Huchuan Lu, and Errui Ding. Attentive feedback network for boundary-aware salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1623\u20131632, 2019.", "[46] Jun Wei, Shuhui Wang, and Qingming Huang. F33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTnet: Fusion, feedback and focus for salient object detection. In AAAI Conf. Art. Intell., pages 12321\u201312328, 2020.", "[48] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Proceedings of the IEEE International Conference on Computer Vision, October 2019.", "[29] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.", "[57] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet:edge guidance network for salient object detection. In Proceedings of the IEEE International Conference on Computer Vision, Oct 2019.", "[15] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. arXiv preprint arXiv:2003.05643, 2020.", "[38] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June 2019.", "[33] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep features for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, July 2017.", "[47] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June 2019."]}, {"table": "<table><thead><tr><th></th><th colspan=\"4\">CAMO</th><th colspan=\"4\">CHAMELEON</th><th colspan=\"4\">COD10K</th></tr><tr><th>Method</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>F_{\\beta}\\uparrow</th><th>E_{\\xi}\\uparrow</th><th>\\mathcal{M}\\downarrow</th></tr></thead><tbody><tr><th>NLDF[33]</th><td>0.665</td><td>0.564</td><td>0.664</td><td>0.123</td><td>0.798</td><td>0.714</td><td>0.809</td><td>0.063</td><td>0.701</td><td>0.539</td><td>0.709</td><td>0.059</td></tr><tr><th>PiCANet[30]</th><td>0.701</td><td>0.573</td><td>0.716</td><td>0.125</td><td>0.765</td><td>0.618</td><td>0.779</td><td>0.085</td><td>0.696</td><td>0.489</td><td>0.712</td><td>0.081</td></tr><tr><th>CPD [47]</th><td>0.716</td><td>0.618</td><td>0.723</td><td>0.113</td><td>0.857</td><td>0.771</td><td>0.874</td><td>0.048</td><td>0.750</td><td>0.595</td><td>0.776</td><td>0.053</td></tr><tr><th>SCRN [48]</th><td>0.779</td><td>0.705</td><td>0.796</td><td>0.090</td><td>0.876</td><td>0.787</td><td>0.889</td><td>0.042</td><td>0.789</td><td>0.651</td><td>0.817</td><td>0.047</td></tr><tr><th>PoolNet [29]</th><td>0.730</td><td>0.643</td><td>0.746</td><td>0.105</td><td>0.845</td><td>0.749</td><td>0.864</td><td>0.054</td><td>0.740</td><td>0.576</td><td>0.776</td><td>0.056</td></tr><tr><th>BASNet [38]</th><td>0.615</td><td>0.503</td><td>0.671</td><td>0.124</td><td>0.847</td><td>0.795</td><td>0.883</td><td>0.044</td><td>0.661</td><td>0.486</td><td>0.729</td><td>0.071</td></tr><tr><th>EGNet [57]</th><td>0.737</td><td>0.655</td><td>0.758</td><td>0.102</td><td>0.856</td><td>0.766</td><td>0.883</td><td>0.049</td><td>0.751</td><td>0.595</td><td>0.793</td><td>0.053</td></tr><tr><th>CSNet[15]</th><td>0.771</td><td>0.705</td><td>0.795</td><td>0.092</td><td>0.856</td><td>0.766</td><td>0.869</td><td>0.047</td><td>0.778</td><td>0.635</td><td>0.810</td><td>0.047</td></tr><tr><th>F3Net [46]</th><td>0.711</td><td>0.616</td><td>0.741</td><td>0.109</td><td>0.848</td><td>0.770</td><td>0.894</td><td>0.047</td><td>0.739</td><td>0.593</td><td>0.795</td><td>0.051</td></tr><tr><th>ITSD[59]</th><td>0.750</td><td>0.663</td><td>0.779</td><td>0.102</td><td>0.814</td><td>0.705</td><td>0.844</td><td>0.057</td><td>0.767</td><td>0.615</td><td>0.808</td><td>0.051</td></tr><tr><th>SINet [11]</th><td>0.745</td><td>0.702</td><td>0.804</td><td>0.092</td><td>0.872</td><td>0.827</td><td>0.936</td><td>0.034</td><td>0.776</td><td>0.679</td><td>0.864</td><td>0.043</td></tr><tr><th>Ours</th><td>0.803</td><td>0.759</td><td>0.853</td><td>0.076</td><td>0.894</td><td>0.848</td><td>0.943</td><td>0.030</td><td>0.817</td><td>0.726</td><td>0.892</td><td>0.035</td></tr></tbody></table>", "caption": "Table 3: Performance comparison with re-implemented camouflaged object detection models.", "list_citation_info": ["[30] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3089\u20133098, 2018.", "[59] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for accurate and fast saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9141\u20139150, 2020.", "[46] Jun Wei, Shuhui Wang, and Qingming Huang. F33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTnet: Fusion, feedback and focus for salient object detection. In AAAI Conf. Art. Intell., pages 12321\u201312328, 2020.", "[11] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2777\u20132787, 2020.", "[48] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Proceedings of the IEEE International Conference on Computer Vision, October 2019.", "[29] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.", "[57] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet:edge guidance network for salient object detection. In Proceedings of the IEEE International Conference on Computer Vision, Oct 2019.", "[15] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. arXiv preprint arXiv:2003.05643, 2020.", "[38] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June 2019.", "[33] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep features for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, July 2017.", "[47] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June 2019."]}]}