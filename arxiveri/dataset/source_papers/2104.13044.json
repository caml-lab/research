{"title": "Dual transformer for point cloud analysis", "abstract": "Following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise multi-head self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT module as a fundamental component, we construct the DTNet for performing point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our proposed transformer framework for the tasks of 3D point cloud classification and segmentation, achieving highly competitive performance in comparison with the state-of-the-art approaches.", "authors": ["Xian-Feng Han", " Yi-Fei Jin", " Hui-Xian Cheng", " Guo-Qiang Xiao"], "pdf_url": "https://arxiv.org/abs/2104.13044", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>Representation</th><th>Input Size</th><th>ModelNet40</th></tr></thead><tbody><tr><td>3DShapeNets [28]</td><td>Volumetric</td><td>30^{3}</td><td>77.3%</td></tr><tr><td>VoxNet [7]</td><td>Volumetric</td><td>32^{3}</td><td>83.0%</td></tr><tr><td>MVCNN [8]</td><td>Multi-view</td><td>12\\times 224^{2}</td><td>90.1%</td></tr><tr><td>DeepNet [30]</td><td>Points</td><td>5000\\times 3</td><td>90.0%</td></tr><tr><td>OctNet [15]</td><td>Volumetric</td><td>128^{3}</td><td>86.5%</td></tr><tr><td>Kd-Net [16]</td><td>Points</td><td>2^{15}\\times 3</td><td>88.5%</td></tr><tr><td>PointNet [9]</td><td>Points</td><td>1024\\times 3</td><td>89.2%</td></tr><tr><td>PointNet++ [10]</td><td>Points+normals</td><td>5000\\times 6</td><td>91.9%</td></tr><tr><td>ECC [31]</td><td>Points</td><td>1000 \\times3</td><td>83.2%</td></tr><tr><td>DGCNN [32]</td><td>Points</td><td>1024\\times 3</td><td>92.2%</td></tr><tr><td>PointCNN [33]</td><td>Points</td><td>1024\\times 3</td><td>92.5%</td></tr><tr><td>KC-Net [34]</td><td>Points</td><td>1024\\times 3</td><td>91.0%</td></tr><tr><td>FoldingNet [35]</td><td>Points</td><td>2048\\times 3</td><td>88.4%</td></tr><tr><td>Point2Sequence [36]</td><td>Points</td><td>1024\\times 3</td><td>92.6%</td></tr><tr><td>OctreeGCNN [37]</td><td>Points</td><td>1024\\times 3</td><td>92.0%</td></tr><tr><td>SFCNN [38]</td><td>Points+normals</td><td>1024\\times 6</td><td>92.3%</td></tr><tr><td>3D-GCN [39]</td><td>Points</td><td>1024\\times 3</td><td>92.1%</td></tr><tr><td>ELM [40]</td><td>Points</td><td>1024\\times 3</td><td>92.2%</td></tr><tr><td>FPConv [23]</td><td>Points+normals</td><td>-</td><td>92.5%</td></tr><tr><td>SPH3D-GCN [41]</td><td>Points</td><td>1000\\times 3</td><td>92.1%</td></tr><tr><td>DTNet</td><td>Points</td><td>1024\\times 3</td><td>92.9%</td></tr></tbody></table>", "caption": "TABLE I: 3D object classification results on ModelNet40. The best results are shown in bold.", "list_citation_info": ["[7] D. Maturana and S. Scherer, \u201cVoxnet: A 3d convolutional neural network for real-time object recognition,\u201d in 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2015, pp. 922\u2013928.", "[8] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, \u201cMulti-view convolutional neural networks for 3d shape recognition,\u201d in Proceedings of the IEEE international conference on computer vision, 2015, pp. 945\u2013953.", "[35] Y. Yang, C. Feng, Y. Shen, and D. Tian, \u201cFoldingnet: Point cloud auto-encoder via deep grid deformation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 206\u2013215.", "[16] R. Klokov and V. Lempitsky, \u201cEscape from cells: Deep kd-networks for the recognition of 3d point cloud models,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 863\u2013872.", "[32] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \u201cDynamic graph cnn for learning on point clouds,\u201d arXiv preprint arXiv:1801.07829, 2018.", "[30] S. Ravanbakhsh, J. Schneider, and B. Poczos, \u201cDeep learning with sets and point clouds,\u201d arXiv preprint arXiv:1611.04500, 2016.", "[36] X. Liu, Z. Han, Y.-S. Liu, and M. Zwicker, \u201cPoint2sequence: Learning the shape representation of 3d point clouds with an attention-based sequence to sequence network,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 8778\u20138785.", "[23] Y. Lin, Z. Yan, H. Huang, D. Du, L. Liu, S. Cui, and X. Han, \u201cFpconv: Learning local flattening for point convolution,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 4293\u20134302.", "[38] Y. Rao, J. Lu, and J. Zhou, \u201cSpherical fractal convolutional neural networks for point cloud recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 452\u2013460.", "[28] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, \u201c3d shapenets: A deep representation for volumetric shapes,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1912\u20131920.", "[33] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, \u201cPointcnn: Convolution on \u03c7\ud835\udf12\\chiitalic_\u03c7-transformed points,\u201d in Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018, pp. 828\u2013838.", "[39] Z.-H. Lin, S.-Y. Huang, and Y.-C. F. Wang, \u201cConvolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 1800\u20131809.", "[40] K. Fujiwara and T. Hashimoto, \u201cNeural implicit embedding for point cloud analysis,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009734\u201311\u2009743.", "[34] Y. Shen, C. Feng, Y. Yang, and D. Tian, \u201cMining point cloud local structures by kernel correlation and graph pooling,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4548\u20134557.", "[9] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652\u2013660.", "[31] M. Simonovsky and N. Komodakis, \u201cDynamic edge-conditioned filters in convolutional neural networks on graphs,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 3693\u20133702.", "[37] H. Lei, N. Akhtar, and A. Mian, \u201cOctree guided cnn with spherical kernels for 3d point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9631\u20139640.", "[15] G. Riegler, A. Osman Ulusoy, and A. Geiger, \u201cOctnet: Learning deep 3d representations at high resolutions,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 3577\u20133586.", "[41] H. Lei, N. Akhtar, and A. Mian, \u201cSpherical kernel for efficient graph convolution on 3d point clouds,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.", "[10] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d arXiv preprint arXiv:1706.02413, 2017."]}, {"table": "<table><thead><tr><th>Method</th><th>mIoU</th><th>aero</th><th>bag</th><th>cap</th><th>car</th><th>chair</th><th>ep</th><th>guitar</th><th>knife</th><th>lamp</th><th>laptop</th><th>motor</th><th>mug</th><th>pistol</th><th>rocket</th><th>skate</th><th>table</th></tr></thead><tbody><tr><th>ShapeNet [29]</th><th>81.4</th><td>81.0</td><td>78.4</td><td>77.7</td><td>75.7</td><td>87.6</td><td>61.9</td><td>92.0</td><td>85.4</td><td>82.5</td><td>95.7</td><td>70.6</td><td>91.9</td><td>85.9</td><td>53.1</td><td>69.8</td><td>75.3</td></tr><tr><th>PointNet [9]</th><th>83.7</th><td>83.4</td><td>78.7</td><td>82.5</td><td>74.9</td><td>89.6</td><td>73.0</td><td>91.5</td><td>85.9</td><td>80.8</td><td>95.3</td><td>65.2</td><td>93.0</td><td>81.2</td><td>57.9</td><td>72.8</td><td>80.6</td></tr><tr><th>PointNet++ [10]</th><th>85.1</th><td>82.4</td><td>79.0</td><td>87.7</td><td>77.3</td><td>90.8</td><td>71.8</td><td>91.0</td><td>85.9</td><td>83.7</td><td>95.3</td><td>71.6</td><td>94.1</td><td>81.3</td><td>58.7</td><td>76.4</td><td>82.6</td></tr><tr><th>KD-Net [16]</th><th>82.3</th><td>80.1</td><td>74.6</td><td>74.3</td><td>70.3</td><td>88.6</td><td>73.5</td><td>90.2</td><td>87.2</td><td>71.0</td><td>94.9</td><td>57.4</td><td>86.7</td><td>78.1</td><td>51.8</td><td>69.9</td><td>80.3</td></tr><tr><th>SO-Net [42]</th><th>84.9</th><td>82.8</td><td>77.8</td><td>88.0</td><td>77.3</td><td>90.6</td><td>73.5</td><td>90.7</td><td>83.9</td><td>82.8</td><td>94.8</td><td>69.1</td><td>94.2</td><td>80.9</td><td>53.1</td><td>72.9</td><td>83.0</td></tr><tr><th>RGCNN [43]</th><th>84.3</th><td>80.2</td><td>82.8</td><td>92.6</td><td>75.3</td><td>89.2</td><td>73.7</td><td>91.3</td><td>88.4</td><td>83.3</td><td>96.0</td><td>63.9</td><td>95.7</td><td>60.9</td><td>44.6</td><td>72.9</td><td>80.4</td></tr><tr><th>DGCNN [32]</th><th>85.2</th><td>84.0</td><td>83.4</td><td>86.7</td><td>77.8</td><td>90.6</td><td>74.7</td><td>91.2</td><td>87.5</td><td>82.8</td><td>95.7</td><td>66.3</td><td>94.9</td><td>81.1</td><td>63.5</td><td>74.5</td><td>82.6</td></tr><tr><th>SRN [44]</th><th>85.3</th><td>82.4</td><td>79.8</td><td>88.1</td><td>77.9</td><td>90.7</td><td>69.6</td><td>90.9</td><td>86.3</td><td>84.0</td><td>95.4</td><td>72.2</td><td>94.9</td><td>81.3</td><td>62.1</td><td>75.9</td><td>83.2</td></tr><tr><th>SFCNN [38]</th><th>85.4</th><td>83.0</td><td>83.4</td><td>87.0</td><td>80.2</td><td>90.1</td><td>75.9</td><td>91.1</td><td>86.2</td><td>84.2</td><td>96.7</td><td>69.5</td><td>94.8</td><td>82.5</td><td>59.9</td><td>75.1</td><td>82.9</td></tr><tr><th>3D-GCN [39]</th><th>85.1</th><td>83.1</td><td>84.0</td><td>86.6</td><td>77.5</td><td>90.3</td><td>74.1</td><td>90.9</td><td>86.4</td><td>83.8</td><td>95.6</td><td>66.8</td><td>94.8</td><td>81.3</td><td>59.6</td><td>75.7</td><td>82.6</td></tr><tr><th>ELM [40]</th><th>85.2</th><td>84.0</td><td>80.4</td><td>88.0</td><td>80.2</td><td>90.7</td><td>77.5</td><td>91.2</td><td>86.4</td><td>82.6</td><td>95.5</td><td>70.0</td><td>93.9</td><td>84.1</td><td>55.6</td><td>75.6</td><td>82.1</td></tr><tr><th>Weak Sup. [45]</th><th>85.0</th><td>83.1</td><td>82.6</td><td>80.8</td><td>77.7</td><td>90.4</td><td>77.3</td><td>90.9</td><td>87.6</td><td>82.9</td><td>95.8</td><td>64.7</td><td>93.9</td><td>79.8</td><td>61.9</td><td>74.9</td><td>82.9</td></tr><tr><th>DTNet</th><th>85.6</th><td>83.0</td><td>81.4</td><td>84.3</td><td>78.4</td><td>90.9</td><td>74.3</td><td>91.0</td><td>87.3</td><td>84.7</td><td>95.6</td><td>69.0</td><td>94.4</td><td>82.5</td><td>59.0</td><td>76.4</td><td>83.5</td></tr></tbody></table>", "caption": "TABLE III: Part segmentation results on ShapeNet part dataset. The mean IoU across all the shape instances and IoU for each category are reported. ", "list_citation_info": ["[45] X. Xu and G. H. Lee, \u201cWeakly supervised semantic point cloud segmentation: Towards 10x fewer labels,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 13\u2009706\u201313\u2009715.", "[29] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu, Q. Huang, A. Sheffer, L. Guibas et al., \u201cA scalable active framework for region annotation in 3d shape collections,\u201d ACM Transactions on Graphics (TOG), vol. 35, no. 6, p. 210, 2016.", "[39] Z.-H. Lin, S.-Y. Huang, and Y.-C. F. Wang, \u201cConvolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 1800\u20131809.", "[16] R. Klokov and V. Lempitsky, \u201cEscape from cells: Deep kd-networks for the recognition of 3d point cloud models,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 863\u2013872.", "[44] Y. Duan, Y. Zheng, J. Lu, J. Zhou, and Q. Tian, \u201cStructural relational reasoning of point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 949\u2013958.", "[32] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \u201cDynamic graph cnn for learning on point clouds,\u201d arXiv preprint arXiv:1801.07829, 2018.", "[42] J. Li, B. M. Chen, and G. H. Lee, \u201cSo-net: Self-organizing network for point cloud analysis,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 9397\u20139406.", "[40] K. Fujiwara and T. Hashimoto, \u201cNeural implicit embedding for point cloud analysis,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009734\u201311\u2009743.", "[10] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d arXiv preprint arXiv:1706.02413, 2017.", "[38] Y. Rao, J. Lu, and J. Zhou, \u201cSpherical fractal convolutional neural networks for point cloud recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 452\u2013460.", "[9] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652\u2013660.", "[43] G. Te, W. Hu, A. Zheng, and Z. Guo, \u201cRgcnn: Regularized graph cnn for point cloud segmentation,\u201d in Proceedings of the 26th ACM international conference on Multimedia, 2018, pp. 746\u2013754."]}]}