{"title": "Track without appearance: Learn box and tracklet embedding with local and global motion patterns for vehicle tracking", "abstract": "Vehicle tracking is an essential task in the multi-object tracking (MOT) field. A distinct characteristic in vehicle tracking is that the trajectories of vehicles are fairly smooth in both the world coordinate and the image coordinate. Hence, models that capture motion consistencies are of high necessity. However, tracking with the standalone motion-based trackers is quite challenging because targets could get lost easily due to limited information, detection error and occlusion. Leveraging appearance information to assist object re-identification could resolve this challenge to some extent. However, doing so requires extra computation while appearance information is sensitive to occlusion as well. In this paper, we try to explore the significance of motion patterns for vehicle tracking without appearance information. We propose a novel approach that tackles the association issue for long-term tracking with the exclusive fully-exploited motion information. We address the tracklet embedding issue with the proposed reconstruct-to-embed strategy based on deep graph convolutional neural networks (GCN). Comprehensive experiments on the KITTI-car tracking dataset and UA-Detrac dataset show that the proposed method, though without appearance information, could achieve competitive performance with the state-of-the-art (SOTA) trackers. The source code will be available at https://github.com/GaoangW/LGMTracker.", "authors": ["Gaoang Wang", " Renshu Gu", " Zuozhu Liu", " Weijie Hu", " Mingli Song", " Jenq-Neng Hwang"], "pdf_url": "https://arxiv.org/abs/2108.06029", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>HOTA (%) \\uparrow</th><th>AssA (%) \\uparrow</th><th>MOTA (%) \\uparrow</th><th>MT (%) \\uparrow</th><th>ML (%) \\downarrow</th><th>IDS \\downarrow</th><th>FRAG \\downarrow</th></tr></thead><tbody><tr><td>{}^{\\dagger\\star}JRMOT [47]</td><td>69.6</td><td>66.9</td><td>85.1</td><td>70.9</td><td>4.6</td><td>271</td><td>273</td></tr><tr><td>{}^{\\dagger\\star}AB3DMOT [60]</td><td>69.8</td><td>69.1</td><td>83.5</td><td>67.1</td><td>11.4</td><td>126</td><td>254</td></tr><tr><td>{}^{\\dagger\\star}MOTSFusion [33]</td><td>68.7</td><td>66.2</td><td>84.2</td><td>72.8</td><td>2.9</td><td>415</td><td>569</td></tr><tr><td>{}^{\\dagger\\star}mono3DT [26]</td><td>73.2</td><td>74.2</td><td>84.3</td><td>73.1</td><td>2.9</td><td>379</td><td>573</td></tr><tr><td>{}^{\\star}MASS [27]</td><td>68.3</td><td>64.5</td><td>84.6</td><td>74.0</td><td>2.9</td><td>353</td><td>516</td></tr><tr><td>{}^{\\star}TuSimple [11]</td><td>71.6</td><td>71.1</td><td>86.3</td><td>71.1</td><td>6.9</td><td>292</td><td>220</td></tr><tr><td>{}^{\\star}SMAT [19]</td><td>71.9</td><td>72.1</td><td>83.6</td><td>62.8</td><td>6.0</td><td>198</td><td>294</td></tr><tr><td>{}^{\\star}CenterTrack [68]</td><td>73.0</td><td>71.2</td><td>88.8</td><td>82.2</td><td>2.5</td><td>254</td><td>227</td></tr><tr><td>DCO-X [38]</td><td>46.5</td><td>38.7</td><td>66.2</td><td>38.3</td><td>14.5</td><td>955</td><td>708</td></tr><tr><td>SCEA [23]</td><td>56.1</td><td>52.2</td><td>74.9</td><td>53.7</td><td>12.3</td><td>324</td><td>317</td></tr><tr><td>MCMOT-CPD [31]</td><td>56.6</td><td>50.6</td><td>78.0</td><td>52.5</td><td>12.5</td><td>475</td><td>309</td></tr><tr><td>LGM (ours)</td><td>73.1</td><td>72.3</td><td>87.6</td><td>85.1</td><td>2.5</td><td>448</td><td>164</td></tr></tbody></table>", "caption": "Table 1: Result on KITTI-car tracking testing set. From top to bottom, we divide SOTA methods into three categories (3D trackers, 2D trackers, mere motion 2D trackers) according to different input information.{}^{\\dagger} represents 3D tracking methods and {}^{\\star} represents trackers using appearance information. The best result for each part is shown in red, blue and bold, respectively. ", "list_citation_info": ["[68] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. arXiv preprint arXiv:2004.01177, 2020.", "[27] Hasith Karunasekera, Han Wang, and Handuo Zhang. Multiple object tracking with attention to appearance, structure, motion and size. IEEE Access, 7:104423\u2013104434, 2019.", "[38] Anton Milan, Konrad Schindler, and Stefan Roth. Detection-and trajectory-level exclusion in multiple object tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3682\u20133689, 2013.", "[33] Jonathon Luiten, Tobias Fischer, and Bastian Leibe. Track to reconstruct and reconstruct to track. IEEE Robotics and Automation Letters, 5(2):1803\u20131810, 2020.", "[23] Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, and Kuk-Jin Yoon. Online multi-object tracking via structural constraint event aggregation. In Proceedings of the IEEE Conference on computer vision and pattern recognition, pages 1392\u20131400, 2016.", "[60] Xinshuo Weng, Jianren Wang, David Held, and Kris Kitani. 3d multi-object tracking: A baseline and new evaluation metrics. arXiv preprint arXiv:1907.03961, 2020.", "[47] Abhijeet Shenoi, Mihir Patel, JunYoung Gwak, Patrick Goebel, Amir Sadeghian, Hamid Rezatofighi, Roberto Martin-Martin, and Silvio Savarese. Jrmot: A real-time 3d multi-object tracker and a new large-scale dataset. arXiv preprint arXiv:2002.08397, 2020.", "[11] Wongun Choi. Near-online multi-target tracking with aggregated local flow descriptor. In Proceedings of the IEEE international conference on computer vision, pages 3029\u20133037, 2015.", "[31] Byungjae Lee, Enkhbayar Erdenee, Songguo Jin, Mi Young Nam, Young Giu Jung, and Phill Kyu Rhee. Multi-class multi-object tracking using changing point detection. In European Conference on Computer Vision, pages 68\u201383. Springer, 2016.", "[19] Nicolas Franco Gonzalez, Andres Ospina, and Philippe Calvez. Smat: Smart multiple affinity metrics for multiple object tracking. In International Conference on Image Analysis and Recognition, pages 48\u201362. Springer, 2020.", "[26] Hou-Ning Hu, Qi-Zhi Cai, Dequan Wang, Ji Lin, Min Sun, Philipp Krahenbuhl, Trevor Darrell, and Fisher Yu. Joint monocular 3d vehicle detection and tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5390\u20135399, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>PR-MOTA (%) \\uparrow</th><th>PR-MOTP (%) \\uparrow</th><th>PR-MT (%) \\uparrow</th><th>PR-ML (%) \\downarrow</th><th>PR-IDS \\downarrow</th><th>PR-FRAG \\downarrow</th></tr></thead><tbody><tr><th>IHTLS [16]</th><td>11.1</td><td>36.8</td><td>13.8</td><td>19.9</td><td>953.6</td><td>3556.9</td></tr><tr><th>H2T [59]</th><td>12.4</td><td>35.7</td><td>14.8</td><td>19.4</td><td>852.2</td><td>1117.2</td></tr><tr><th>CMOT [3]</th><td>12.6</td><td>36.1</td><td>16.1</td><td>18.6</td><td>285.3</td><td>1516.8</td></tr><tr><th>GOG [43]</th><td>14.2</td><td>37.0</td><td>13.9</td><td>19.9</td><td>3334.6</td><td>3172.4</td></tr><tr><th>IOUT [6]</th><td>16.1</td><td>37.0</td><td>14.8</td><td>19.7</td><td>2308.1</td><td>3250.4</td></tr><tr><th>V-IOUT [7]</th><td>17.7</td><td>36.4</td><td>17.4</td><td>18.8</td><td>363.8</td><td>1123.5</td></tr><tr><th>FAMNet [13]</th><td>19.8</td><td>36.7</td><td>17.1</td><td>18.2</td><td>617.4</td><td>970.2</td></tr><tr><th>LGM (ours)</th><td>22.5</td><td>35.2</td><td>15.5</td><td>10.1</td><td>1563.5</td><td>3186.8</td></tr></tbody></table>", "caption": "Table 2: Result on UA-Detrac testing set. The best performance is shown in bold type.", "list_citation_info": ["[6] Erik Bochinski, Volker Eiselein, and Thomas Sikora. High-speed tracking-by-detection without using image information. In International Workshop on Traffic and Street Surveillance for Safety and Security at IEEE AVSS 2017, Lecce, Italy, Aug. 2017.", "[3] Seung-Hwan Bae and Kuk-Jin Yoon. Robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1218\u20131225, 2014.", "[59] Longyin Wen, Wenbo Li, Junjie Yan, Zhen Lei, Dong Yi, and Stan Z Li. Multiple target tracking based on undirected hierarchical relation hypergraph. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1282\u20131289, 2014.", "[43] Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes. Globally-optimal greedy algorithms for tracking a variable number of objects. In CVPR 2011, pages 1201\u20131208. IEEE, 2011.", "[16] Caglayan Dicle, Octavia I Camps, and Mario Sznaier. The way they move: Tracking multiple targets with similar appearance. In Proceedings of the IEEE international conference on computer vision, pages 2304\u20132311, 2013.", "[13] Peng Chu and Haibin Ling. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking. In Proceedings of the IEEE International Conference on Computer Vision, pages 6172\u20136181, 2019.", "[7] Erik Bochinski, Tobias Senst, and Thomas Sikora. Extending iou based multi-object tracking by visual information. In IEEE International Conference on Advanced Video and Signals-based Surveillance, pages 441\u2013446, Auckland, New Zealand, Nov. 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>MOTA</th><th>IDF1</th><th>MOTP</th></tr></thead><tbody><tr><th>Tracktor++ [4]</th><td>56.3</td><td>55.1</td><td>78.8</td></tr><tr><th>TrctrD17 [63]</th><td>53.7</td><td>53.8</td><td>77.2</td></tr><tr><th>CenterTrack [68]</th><td>61.5</td><td>59.6</td><td>78.9</td></tr><tr><th>IOU Tracker</th><td>45.5</td><td>39.4</td><td>76.9</td></tr><tr><th>LGM (ours)</th><td>56.0</td><td>55.6</td><td>78.0</td></tr></tbody></table>", "caption": "Table 6: Pedestrian tracking result on MOT17 dataset, where top three methods use appearance information while the bottom two methods only employ motion features.", "list_citation_info": ["[68] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. arXiv preprint arXiv:2004.01177, 2020.", "[63] Yihong Xu, Aljosa Osep, Yutong Ban, Radu Horaud, Laura Leal-Taix\u00e9, and Xavier Alameda-Pineda. How to train your deep multi-object tracker. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6787\u20136796, 2020.", "[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In Proceedings of the IEEE international conference on computer vision, pages 941\u2013951, 2019."]}]}