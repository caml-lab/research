{"title": "Augmentation strategies for learning with noisy labels", "abstract": "Imperfect labels are ubiquitous in real-world datasets. Several recent successful methods for training deep neural networks (DNNs) robust to label noise have used two primary techniques: filtering samples based on loss during a warm-up phase to curate an initial set of cleanly labeled samples, and using the output of a network as a pseudo-label for subsequent loss calculations. In this paper, we evaluate different augmentation strategies for algorithms tackling the \"learning with noisy labels\" problem. We propose and examine multiple augmentation strategies and evaluate them using synthetic datasets based on CIFAR-10 and CIFAR-100, as well as on the real-world dataset Clothing1M. Due to several commonalities in these algorithms, we find that using one set of augmentations for loss modeling tasks and another set for learning is the most effective, improving results on the state-of-the-art and other previous methods. Furthermore, we find that applying augmentation during the warm-up period can negatively impact the loss convergence behavior of correctly versus incorrectly labeled samples. We introduce this augmentation strategy to the state-of-the-art technique and demonstrate that we can improve performance across all evaluated noise levels. In particular, we improve accuracy on the CIFAR-10 benchmark at 90% symmetric noise by more than 15% in absolute accuracy, and we also improve performance on the Clothing1M dataset.\n  (K. Nishi and Y. Ding contributed equally to this work)", "authors": ["Kento Nishi", " Yi Ding", " Alex Rich", " Tobias H\u00f6llerer"], "pdf_url": "https://arxiv.org/abs/2103.02130", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">CIFAR-10</th><th colspan=\"2\">CIFAR-100</th></tr><tr><th>Method/Noise</th><th></th><th>20%</th><th>90%</th><th>20%</th><th>90%</th></tr></thead><tbody><tr><th>Raw</th><th>Best</th><td>85.94</td><td>27.58</td><td>52.24</td><td>7.99</td></tr><tr><th></th><th>Last</th><td>83.23</td><td>23.92</td><td>39.18</td><td>2.98</td></tr><tr><th>Expansion-W</th><th>Best</th><td>90.86</td><td>31.22</td><td>57.11</td><td>7.30</td></tr><tr><th></th><th>Last</th><td>89.95</td><td>10.00</td><td>53.29</td><td>2.23</td></tr><tr><th>Expansion-S</th><th>Best</th><td>90.56</td><td>35.10</td><td>55.15</td><td>7.54</td></tr><tr><th></th><th>Last</th><td>89.51</td><td>34.23</td><td>54.37</td><td>3.24</td></tr><tr><th>Runtime-W [14]</th><th>Best</th><td>96.10</td><td>76.00</td><td>77.30</td><td>31.50</td></tr><tr><th></th><th>Last</th><td>95.70</td><td>75.40</td><td>76.90</td><td>31.00</td></tr><tr><th>Runtime-S</th><th>Best</th><td>96.54</td><td>70.47</td><td>79.89</td><td>40.52</td></tr><tr><th></th><th>Last</th><td>96.33</td><td>70.22</td><td>79.40</td><td>40.34</td></tr><tr><th>AugDesc-WW</th><th>Best</th><td>96.27</td><td>36.05</td><td>78.90</td><td>30.33</td></tr><tr><th></th><th>Last</th><td>96.08</td><td>23.50</td><td>78.44</td><td>29.88</td></tr><tr><th>AugDesc-SS</th><th>Best</th><td>96.47</td><td>81.77</td><td>79.79</td><td>38.85</td></tr><tr><th></th><th>Last</th><td>96.19</td><td>81.54</td><td>79.51</td><td>38.55</td></tr><tr><th>AugDesc-WS</th><th>Best</th><td>96.33</td><td>91.88</td><td>79.50</td><td>41.20</td></tr><tr><th></th><th>Last</th><td>96.17</td><td>91.76</td><td>79.22</td><td>40.90</td></tr></tbody></table>", "caption": "Table 1: Performance differences for each augmentation strategy. The best performance in each category is highlighted in bold. Removing all augmentation is highly detrimental to performance, while more augmentation seemingly improves performance. However, too much augmentation is also detrimental to performance (AugDesc-SS). Strategically adding augmentation by exploiting the loss properties (AugDesc-WS) yields the best results in general.", "list_citation_info": ["[14] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"5\">CIFAR-10</th><th colspan=\"4\">CIFAR-100</th></tr><tr><th>Model</th><th>Noise</th><th>20%</th><th>50%</th><th>80%</th><th>90%</th><th>40% Asym</th><th>20%</th><th>50%</th><th>80%</th><th>90%</th></tr></thead><tbody><tr><td rowspan=\"2\">DivideMix (baseline) [14]</td><td>Best</td><td>96.1</td><td>94.6</td><td>92.3</td><td>76.0</td><td>93.4</td><td>77.3</td><td>74.6</td><td>60.2</td><td>31.5</td></tr><tr><td>Last</td><td>95.7</td><td>94.4</td><td>92.9</td><td>75.4</td><td>92.1</td><td>76.9</td><td>74.2</td><td>59.6</td><td>31.0</td></tr><tr><td rowspan=\"2\">DM-AugDesc-WS-SAW</td><td>Best</td><td>96.3</td><td>95.6</td><td>93.7</td><td>35.3</td><td>94.4</td><td>79.6</td><td>77.6</td><td>61.8</td><td>17.3</td></tr><tr><td>Last</td><td>96.2</td><td>95.4</td><td>93.6</td><td>10.0</td><td>94.1</td><td>79.5</td><td>77.5</td><td>61.6</td><td>15.1</td></tr><tr><td rowspan=\"2\">DM-AugDesc-WS-WAW</td><td>Best</td><td>96.3</td><td>95.4</td><td>93.8</td><td>91.9</td><td>94.6</td><td>79.5</td><td>77.2</td><td>66.4</td><td>41.2</td></tr><tr><td>Last</td><td>96.2</td><td>95.1</td><td>93.6</td><td>91.8</td><td>94.3</td><td>79.2</td><td>77.0</td><td>66.1</td><td>40.9</td></tr></tbody></table>", "caption": "Table 2: Application of strong versus weak augmentation during the warm-up period of DivideMix, in comparison to the baseline model. WAW signifies weakly augmented warm-up, SAW represents strongly augmented warm-up. Weak warm-up appears to benefit datasets with higher noise while strong warm-up benefits datasets with lower noise.", "list_citation_info": ["[14] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"4\">CIFAR-10</th><th colspan=\"4\">CIFAR-100</th></tr><tr><th>Model</th><th>Noise</th><th>20%</th><th>50%</th><th>80%</th><th>90%</th><th>20%</th><th>50%</th><th>80%</th><th>90%</th></tr></thead><tbody><tr><th rowspan=\"2\">Cross-Entropy</th><th>Best</th><td>86.8</td><td>79.4</td><td>62.9</td><td>42.7</td><td>62.0</td><td>46.7</td><td>19.9</td><td>10.1</td></tr><tr><th>Last</th><td>82.7</td><td>57.9</td><td>26.1</td><td>16.8</td><td>61.8</td><td>37.3</td><td>8.8</td><td>3.5</td></tr><tr><th rowspan=\"2\">Reed et. al. [24]</th><th>Best</th><td>86.8</td><td>79.8</td><td>63.3</td><td>42.9</td><td>62.1</td><td>46.6</td><td>19.9</td><td>10.2</td></tr><tr><th>Last</th><td>82.9</td><td>58.4</td><td>26.8</td><td>17.0</td><td>62.0</td><td>37.9</td><td>8.9</td><td>3.8</td></tr><tr><th rowspan=\"2\">Yu et al. [34]</th><th>Best</th><td>89.5</td><td>85.7</td><td>67.4</td><td>47.9</td><td>65.6</td><td>51.8</td><td>27.9</td><td>13.7</td></tr><tr><th>Last</th><td>88.2</td><td>84.1</td><td>45.5</td><td>30.1</td><td>64.1</td><td>45.3</td><td>15.5</td><td>8.8</td></tr><tr><th rowspan=\"2\">Zhang et al. [35]</th><th>Best</th><td>95.6</td><td>87.1</td><td>71.6</td><td>52.2</td><td>67.8</td><td>57.3</td><td>30.8</td><td>14.6</td></tr><tr><th>Last</th><td>92.3</td><td>77.6</td><td>46.7</td><td>43.9</td><td>66.0</td><td>46.6</td><td>17.6</td><td>8.1</td></tr><tr><th rowspan=\"2\">Yi &amp; Wu [33]</th><th>Best</th><td>92.4</td><td>89.1</td><td>77.5</td><td>58.9</td><td>69.4</td><td>57.5</td><td>31.1</td><td>15.3</td></tr><tr><th>Last</th><td>92.0</td><td>88.7</td><td>76.5</td><td>58.2</td><td>68.1</td><td>56.4</td><td>20.7</td><td>8.8</td></tr><tr><th rowspan=\"2\">Li et al. [15]</th><th>Best</th><td>92.9</td><td>89.3</td><td>77.4</td><td>58.7</td><td>68.5</td><td>59.2</td><td>42.4</td><td>19.5</td></tr><tr><th>Last</th><td>92.0</td><td>88.8</td><td>76.1</td><td>58.3</td><td>67.7</td><td>58.0</td><td>40.1</td><td>14.3</td></tr><tr><th rowspan=\"2\">Arazo et al. [1]</th><th>Best</th><td>94.0</td><td>92.0</td><td>86.8</td><td>69.1</td><td>73.9</td><td>66.1</td><td>48.2</td><td>24.3</td></tr><tr><th>Last</th><td>93.8</td><td>91.9</td><td>86.6</td><td>68.7</td><td>73.4</td><td>65.4</td><td>47.6</td><td>20.5</td></tr><tr><th rowspan=\"2\">Li et al. [14]</th><th>Best</th><td>96.1</td><td>94.6</td><td>92.9</td><td>76.0</td><td>77.3</td><td>74.6</td><td>60.2</td><td>31.5</td></tr><tr><th>Last</th><td>95.7</td><td>94.4</td><td>92.3</td><td>75.4</td><td>76.9</td><td>74.2</td><td>59.6</td><td>31.0</td></tr><tr><th rowspan=\"2\">DM-AugDesc-WS-SAW</th><th>Best</th><td>96.3</td><td>95.6</td><td>93.7</td><td>35.3</td><td>79.6</td><td>77.6</td><td>61.8</td><td>17.3</td></tr><tr><th>Last</th><td>96.2</td><td>95.4</td><td>93.6</td><td>10.0</td><td>79.5</td><td>77.5</td><td>61.6</td><td>15.1</td></tr><tr><th rowspan=\"2\">DM-AugDesc-WS-WAW</th><th>Best</th><td>96.3</td><td>95.4</td><td>93.8</td><td>91.9</td><td>79.5</td><td>77.2</td><td>66.4</td><td>41.2</td></tr><tr><th>Last</th><td>96.2</td><td>95.1</td><td>93.6</td><td>91.8</td><td>79.2</td><td>77.0</td><td>66.1</td><td>40.9</td></tr></tbody></table>", "caption": "Table 3: Performance comparison when incorporating our best augmentation strategy into the current state-of-the-art. Our augmentation strategy improves performance at every noise level. Results for previous techniques were directly copied from their respective papers.", "list_citation_info": ["[14] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020.", "[24] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.", "[15] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy labeled data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5051\u20135059, 2019.", "[34] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? arXiv preprint arXiv:1901.04215, 2019.", "[33] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7017\u20137025, 2019.", "[1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.", "[35] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017."]}, {"table": "<table><thead><tr><th>Method</th><th>Test Accuracy</th></tr></thead><tbody><tr><th>Cross Entropy</th><td>69.21</td></tr><tr><th>M-correction [1]</th><td>71.00</td></tr><tr><th>Joint Optimization [29]</th><td>72.16</td></tr><tr><th>MetaCleaner [36]</th><td>72.50</td></tr><tr><th>MLNT [15]</th><td>73.47</td></tr><tr><th>PENCIL [33]</th><td>73.49</td></tr><tr><th>DivideMix [14]</th><td>74.76</td></tr><tr><th>ELR+ [17]</th><td>74.81</td></tr><tr><th>DM-AugDesc-WS-WAW (ours)</th><td>74.72</td></tr><tr><th>DM-AugDesc-WS-SAW (ours)</th><td>75.11</td></tr></tbody></table>", "caption": "Table 4: Comparison against state-of-the-art methods for accuracy on the Clothing1M dataset.", "list_citation_info": ["[14] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020.", "[15] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy labeled data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5051\u20135059, 2019.", "[36] Weihe Zhang, Yali Wang, and Yu Qiao. Metacleaner: Learning to hallucinate clean representations for noisy-labeled visual recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7373\u20137382, 2019.", "[17] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. arXiv preprint arXiv:2007.00151, 2020.", "[33] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7017\u20137025, 2019.", "[1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.", "[29] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5552\u20135560, 2018."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">CIFAR-10</th><th colspan=\"2\">CIFAR-100</th></tr><tr><th>Method/Noise</th><th></th><th>20%</th><th>90%</th><th>20%</th><th>90%</th></tr></thead><tbody><tr><th>Baseline [14]</th><th>Best</th><td>96.1</td><td>76.0</td><td>77.3</td><td>31.5</td></tr><tr><th></th><th>Last</th><td>95.7</td><td>75.4</td><td>76.9</td><td>31.0</td></tr><tr><th>AutoAugment</th><th>Best</th><td>96.3</td><td>91.9</td><td>79.5</td><td>41.2</td></tr><tr><th></th><th>Last</th><td>96.2</td><td>91.8</td><td>79.2</td><td>40.9</td></tr><tr><th>RandAugment</th><th>Best</th><td>96.1</td><td>89.6</td><td>78.1</td><td>36.8</td></tr><tr><th></th><th>Last</th><td>96.0</td><td>89.4</td><td>77.8</td><td>36.7</td></tr></tbody></table>", "caption": "Table 5: Comparison of different automated augmentation policy algorithms. We compare performance of each policy using the AugDesc-WS approach. Adjusting the augmentation policy has minimal effect but still handily outperforms the runtime augmentation used in the baseline. The improved performance is still large with a noise ratio of 90%.", "list_citation_info": ["[14] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. arXiv preprint arXiv:2002.07394, 2020."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">CIFAR-10</th><th colspan=\"2\">CIFAR-100</th></tr><tr><th></th><th></th><th>Base</th><th>Aug</th><th>Base</th><th>Aug</th></tr></thead><tbody><tr><th rowspan=\"2\">Cross Entropy</th><th>Best</th><td>86.8</td><td>89.9</td><td>60.2</td><td>61.2</td></tr><tr><th>Last</th><td>82.7</td><td>85.1</td><td>59.9</td><td>60.4</td></tr><tr><th rowspan=\"2\">Co-Teaching+ [34]</th><th>Best</th><td>59.3</td><td>60.6</td><td>26.2</td><td>25.6</td></tr><tr><th>Last</th><td>55.9</td><td>57.4</td><td>23.0</td><td>23.7</td></tr><tr><th rowspan=\"2\">M-DYR-H [1]</th><th>Best</th><td>94.0</td><td>93.9</td><td>68.2</td><td>73.0</td></tr><tr><th>Last</th><td>93.8</td><td>93.9</td><td>67.5</td><td>72.7</td></tr><tr><th rowspan=\"2\">DivideMix</th><th>Best</th><td>96.1</td><td>96.3</td><td>77.3</td><td>79.5</td></tr><tr><th>Last</th><td>95.7</td><td>96.2</td><td>76.9</td><td>79.2</td></tr></tbody></table>", "caption": "Table 6: Performance benefits when applying our augmentation strategy to previous techniques at 20% noise level. Baseline and augmented accuracy scores are reported.", "list_citation_info": ["[34] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? arXiv preprint arXiv:1901.04215, 2019.", "[1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O\u2019Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019."]}]}