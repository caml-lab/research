{"title": "Curvature-guided dynamic scale networks for multi-view stereo", "abstract": "Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most recent studies tried to improve the performance of matching cost volume in MVS by designing aggregated 3D cost volumes and their regularization. This paper focuses on learning a robust feature extraction network to enhance the performance of matching costs without heavy computation in the other steps. In particular, we present a dynamic scale feature extraction network, namely, CDSFNet. It is composed of multiple novel convolution layers, each of which can select a proper patch scale for each pixel guided by the normal curvature of the image surface. As a result, CDFSNet can estimate the optimal patch scales to learn discriminative features for accurate matching computation between reference and source images. By combining the robust extracted features with an appropriate cost formulation strategy, our resulting MVS architecture can estimate depth maps more precisely. Extensive experiments showed that the proposed method outperforms other state-of-the-art methods on complex outdoor scenes. It significantly improves the completeness of reconstructed models. As a result, the method can process higher resolution inputs within faster run-time and lower memory than other MVS methods. Our source code is available at url{https://github.com/TruongKhang/cds-mvsnet}.", "authors": ["Khang Truong Giang", " Soohwan Song", " Sungho Jo"], "pdf_url": "https://arxiv.org/abs/2112.05999", "list_table_and_caption": [{"table": "<table><tr><td> Method</td><td>Mean</td><td>Family</td><td>Francis</td><td>Horse</td><td>Lighthouse</td><td>M60</td><td>Panther</td><td>Playground</td><td>Train</td></tr><tr><td> ACMM{}^{\\star} (Xu &amp; Tao, 2019)</td><td><p>57.27</p></td><td><p>69.24</p></td><td><p>51.45</p></td><td><p>46.97</p></td><td><p>63.20</p></td><td><p>55.07</p></td><td><p>57.64</p></td><td><p>60.08</p></td><td><p>54.48</p></td></tr><tr><td><p>ACMP{}^{\\star} (Xu &amp; Tao, 2020b)</p></td><td><p>58.41</p></td><td><p>70.30</p></td><td><p>54.06</p></td><td><p>54.11</p></td><td><p>61.65</p></td><td><p>54.16</p></td><td><p>57.60</p></td><td><p>58.12</p></td><td><p>57.25</p></td></tr><tr><td><p>Fast-MVSNet{}^{\\circ} (Yu &amp; Gao, 2020)</p></td><td><p>47.39</p></td><td><p>65.18</p></td><td><p>39.59</p></td><td><p>34.98</p></td><td><p>47.81</p></td><td><p>49.16</p></td><td><p>46.20</p></td><td><p>53.27</p></td><td><p>42.91</p></td></tr><tr><td><p>Vis-MVSNet{}^{\\dagger} (Zhang et al., 2020)</p></td><td><p>60.03</p></td><td>77.40</td><td><p>60.23</p></td><td><p>47.07</p></td><td>63.44</td><td>62.21</td><td><p>57.28</p></td><td><p>60.54</p></td><td><p>52.07</p></td></tr><tr><td><p>AttMVS{}^{\\Diamond} (Luo et al., 2020)</p></td><td>60.05</td><td><p>73.90</p></td><td>62.58</td><td><p>44.08</p></td><td>64.88</td><td><p>56.08</p></td><td>59.39</td><td>63.42</td><td>56.06</td></tr><tr><td><p>CasMVSNet{}^{\\circ} (Gu et al., 2020)</p></td><td><p>56.84</p></td><td><p>76.37</p></td><td><p>58.45</p></td><td><p>46.26</p></td><td><p>55.81</p></td><td><p>56.11</p></td><td><p>54.06</p></td><td><p>58.18</p></td><td><p>49.51</p></td></tr><tr><td><p>UCSNet{}^{\\circ} (Cheng et al., 2020)</p></td><td><p>54.83</p></td><td><p>76.09</p></td><td><p>53.16</p></td><td><p>43.03</p></td><td><p>54.00</p></td><td><p>55.60</p></td><td><p>51.49</p></td><td><p>57.38</p></td><td><p>47.89</p></td></tr><tr><td><p>PVA-MVSNet{}^{\\circ} (Yi et al., 2020)</p></td><td><p>54.46</p></td><td><p>69.36</p></td><td><p>46.80</p></td><td><p>46.01</p></td><td><p>55.74</p></td><td><p>57.23</p></td><td><p>54.75</p></td><td><p>56.70</p></td><td><p>49.06</p></td></tr><tr><td><p>CVP-MVSNet{}^{\\circ} (Yang et al., 2020)</p></td><td><p>54.03</p></td><td><p>76.50</p></td><td><p>47.74</p></td><td><p>36.34</p></td><td><p>55.12</p></td><td><p>57.28</p></td><td><p>54.28</p></td><td><p>57.43</p></td><td><p>47.54</p></td></tr><tr><td><p>BP-MVSNet{}^{\\dagger} (Sormann et al., 2020)</p></td><td><p>57.60</p></td><td><p>77.31</p></td><td><p>60.90</p></td><td>47.89</td><td><p>58.26</p></td><td><p>56.00</p></td><td><p>51.54</p></td><td><p>58.47</p></td><td><p>50.41</p></td></tr><tr><td><p>Ours{}^{\\dagger} (fine-tuning on BlendedMVS)</p></td><td>60.82</td><td>78.17</td><td><p>61.74</p></td><td>53.12</td><td><p>60.25</p></td><td><p>61.91</p></td><td>58.45</td><td>62.35</td><td><p>50.58</p></td></tr><tr><td><p>Ours (DTU+BlendedMVS)</p></td><td>61.58</td><td>78.85</td><td>63.17</td><td>53.04</td><td><p>61.34</p></td><td>62.63</td><td>59.06</td><td>62.28</td><td><p>52.30</p></td></tr><tr><td colspan=\"10\">  \\star traditional method, \\circ only training on the training set of DTU, \\dagger training on DTU and then fine-tuning on BlendedMVS, \\Diamond training on a modified version of DTU</td></tr></table>", "caption": "Table 3: Performance comparisons (F-score) of various reconstruction algorithms on the intermediate sequences of the Tanks &amp; Temples benchmark. The higher value of F-score implies better reconstruction results. Our method achieves the best performance in terms of mean F-score.", "list_citation_info": ["Sormann et al. (2020) Christian Sormann, Patrick Kn\u00f6belreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, and Friedrich Fraundorfer. Bp-mvsnet: Belief-propagation-layers for multi-view-stereo. In 2020 International Conference on 3D Vision (3DV), pp. 394\u2013403. IEEE, 2020.", "Zhang et al. (2020) Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. British Machine Vision Conference (BMVC), 2020.", "Gu et al. (2020) Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2495\u20132504, 2020.", "Xu & Tao (2019) Qingshan Xu and Wenbing Tao. Multi-scale geometric consistency guided multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5483\u20135492, 2019.", "Yi et al. (2020) Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Pyramid multi-view stereo net with self-adaptive view aggregation. In European Conference on Computer Vision, pp. 766\u2013782. Springer, 2020.", "Xu & Tao (2020b) Qingshan Xu and Wenbing Tao. Planar prior assisted patchmatch multi-view stereo. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 12516\u201312523, 2020b.", "Yu & Gao (2020) Zehao Yu and Shenghua Gao. Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1949\u20131958, 2020.", "Yang et al. (2020) Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4877\u20134886, 2020.", "Luo et al. (2020) Keyang Luo, Tao Guan, Lili Ju, Yuesong Wang, Zhuo Chen, and Yawei Luo. Attention-aware multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1590\u20131599, 2020.", "Cheng et al. (2020) Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2524\u20132534, 2020."]}, {"table": "<table><tr><td rowspan=\"2\"> Methods</td><td colspan=\"2\">Design Options</td><td colspan=\"3\">Depth Map</td><td colspan=\"3\">Pointcloud</td><td colspan=\"2\">Time &amp; Memory</td></tr><tr><td>curv. type</td><td>vis. with curv.</td><td>Prec. 2mm</td><td>Prec. 4mm</td><td>MAE (mm)</td><td>Acc. (mm)</td><td>Comp. (mm)</td><td>Overall (mm)</td><td>Time (s)</td><td>Mem (Mb)</td></tr><tr><td> Model A</td><td><p>original</p></td><td></td><td><p>74.26</p></td><td><p>84.89</p></td><td><p>6.84</p></td><td><p>0.378</p></td><td><p>0.315</p></td><td><p>0.347</p></td><td><p>0.539</p></td><td><p>7993</p></td></tr><tr><td><p>Model B</p></td><td><p>original</p></td><td><p>\u2713</p></td><td><p>74.12</p></td><td><p>84.77</p></td><td><p>5.99</p></td><td><p>0.391</p></td><td><p>0.327</p></td><td><p>0.359</p></td><td><p>0.539</p></td><td><p>7993</p></td></tr><tr><td><p>Model C</p></td><td><p>learnable</p></td><td></td><td><p>76.14</p></td><td>86.08</td><td><p>6.30</p></td><td><p>0.373</p></td><td>0.302</td><td>0.338</td><td>0.421</td><td>4389</td></tr><tr><td><p>Model D</p></td><td><p>learnable</p></td><td><p>\u2713</p></td><td>76.23</td><td><p>85.92</p></td><td>5.89</td><td>0.372</td><td><p>0.305</p></td><td>0.339</td><td>0.421</td><td>4389</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 4: Comparison between our learnable curvature and the original curvature (Xu et al., 2020). All models are trained on a subset of DTU training set, using the same setups and hyperparameters. The evaluation of depth map and pointcloud is collected on DTU validation and test set, respectively.", "list_citation_info": ["Xu et al. (2020) Zhenyu Xu, Yiguang Liu, Xuelei Shi, Ying Wang, and Yunan Zheng. Marmvs: Matching ambiguity reduced multiple view stereo for efficient large scale scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5981\u20135990, 2020."]}]}