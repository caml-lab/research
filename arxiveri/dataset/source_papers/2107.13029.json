{"title": "A new split for evaluating true zero-shot action recognition", "abstract": "Zero-shot action recognition is the task of classifying action categories that are not available in the training set. In this setting, the standard evaluation protocol is to use existing action recognition datasets(e.g. UCF101) and randomly split the classes into seen and unseen. However, most recent work builds on representations pre-trained on the Kinetics dataset, where classes largely overlap with classes in the zero-shot evaluation datasets. As a result, classes which are supposed to be unseen, are present during supervised pre-training, invalidating the condition of the zero-shot setting. A similar concern was previously noted several years ago for image based zero-shot recognition but has not been considered by the zero-shot action recognition community. In this paper, we propose a new split for true zero-shot action recognition with no overlap between unseen test classes and training or pre-training classes. We benchmark several recent approaches on the proposed True Zero-Shot(TruZe) Split for UCF101 and HMDB51, with zero-shot and generalized zero-shot evaluation. In our extensive analysis, we find that our TruZesplits are significantly harder than comparable random splits as nothing is leaking from pre-training, i.e. unseen performance is consistently lower,up to 8.9% for zero-shot action recognition. In an additional evaluation we also find that similar issues exist in the splits used in few-shot action recognition, here we see differences of up to 17.1%. We publish oursplits1and hope that our benchmark analysis will change how the field is evaluating zero- and few-shot action recognition moving forward.", "authors": ["Shreyank N Gowda", " Laura Sevilla-Lara", " Kiyoon Kim", " Frank Keller", " Marcus Rohrbach"], "pdf_url": "https://arxiv.org/abs/2107.13029", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th colspan=\"3\">UCF101</th><th colspan=\"3\">HMDB51</th></tr></thead><tbody><tr><th></th><td>Random</td><td>TruZe</td><td>Diff</td><td>Random</td><td>TruZe</td><td>Diff</td></tr><tr><th>Latem [23]</th><th>21.4</th><th>15.5</th><th>5.9</th><th>17.8</th><th>9.4</th><th>8.4</th></tr><tr><th>SYNC [4]</th><td>22.1</td><td>15.3</td><td>6.8</td><td>18.1</td><td>11.6</td><td>6.5</td></tr><tr><th>BiDiLEL [21]</th><th>21.3</th><th>15.7</th><th>5.6</th><th>18.4</th><th>10.5</th><th>7.9</th></tr><tr><th>OD [10]</th><td>28.4</td><td>22.9</td><td>5.5</td><td>30.6</td><td>21.7</td><td>8.9</td></tr><tr><th>E2E [1]</th><th>46.6</th><th>45.5</th><th>1.1</th><th>33.2</th><th>31.5</th><th>1.7</th></tr><tr><th>CLASTER [7]</th><td>47.1</td><td>45.2</td><td>1.9</td><td>36.6</td><td>33.2</td><td>3.4</td></tr></tbody></table>", "caption": "Table 2: Results with different splits for Zero-Shot Learning (ZSL). Column \u2018Random\u2019 corresponds to the accuracy using splits in the traditional fashion (random selection of train and test classes, but with the same number of classes in train/test as in TruZe), \u2018TruZe\u2019 corresponds to the accuracy using our proposed split and \u2018Diff\u2019 corresponds to the difference in accuracy between using random splits and our proposed split. We run 10 independent runs for different random splits and report the average accuracy. We see positive differences in the \u2018Diff\u2019 column which we believe is due to the overlapping classes in Kinetics. ", "list_citation_info": ["[7] Gowda, S.N., Sevilla-Lara, L., Keller, F., Rohrbach, M.: Claster: Clustering with reinforcement learning for zero-shot action recognition. arXiv preprint arXiv:2101.07042 (2021)", "[1] Brattoli, B., Tighe, J., Zhdanov, F., Perona, P., Chalupka, K.: Rethinking zero-shot video classification: End-to-end training for realistic applications. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4613\u20134623 (2020)", "[21] Wang, Q., Chen, K.: Zero-shot visual recognition via bidirectional latent embedding. International Journal of Computer Vision 124(3), 356\u2013383 (2017)", "[10] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)", "[4] Changpinyo, S., Chao, W.L., Gong, B., Sha, F.: Synthesized classifiers for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5327\u20135336 (2016)", "[23] Xian, Y., Akata, Z., Sharma, G., Nguyen, Q., Hein, M., Schiele, B.: Latent embeddings for zero-shot classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 69\u201377 (2016)"]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">Acc{}_{U}</th><th colspan=\"3\">Acc{}_{S}</th><th colspan=\"3\">Harmonic mean</th><th></th></tr></thead><tbody><tr><td>Method</td><td>Rand</td><td>TruZe</td><td>Diff</td><td>Rand</td><td>TruZe</td><td>Diff</td><td>Rand</td><td>TruZe</td><td>Diff</td><td>Dataset</td></tr><tr><th>WGAN [25]</th><th>27.9</th><th>21.3</th><th>6.6</th><th>58.2</th><th>63.2</th><th>-5.0</th><th>37.7</th><th>31.8</th><th>5.9</th><th>HMDB51</th></tr><tr><td>WGAN [25]</td><td>28.2</td><td>23.9</td><td>4.3</td><td>74.9</td><td>75.6</td><td>-0.7</td><td>41.0</td><td>36.3</td><td>4.7</td><td>UCF101</td></tr><tr><th>OD [10]</th><th>34.1</th><th>24.7</th><th>9.4</th><th>58.5</th><th>62.8</th><th>-4.3</th><th>43.1</th><th>35.5</th><th>7.6</th><th>HMDB51</th></tr><tr><td>OD [10]</td><td>32.6</td><td>29.1</td><td>3.5</td><td>76.1</td><td>78.4</td><td>-2.3</td><td>45.6</td><td>42.4</td><td>3.2</td><td>UCF101</td></tr><tr><th>CLASTER [7]</th><th>41.8</th><th>38.4</th><th>3.4</th><th>52.3</th><th>53.1</th><th>-0.8</th><th>46.4</th><th>44.5</th><th>1.9</th><th>HMDB51</th></tr><tr><td>CLASTER [7]</td><td>37.5</td><td>35.6</td><td>1.9</td><td>68.8</td><td>70.6</td><td>-1.8</td><td>48.5</td><td>47.3</td><td>1.2</td><td>UCF101</td></tr></tbody></table>", "caption": "Table 3: Results with different splits for Generalized Zero-Shot Learning (GZSL). \u2018Rand\u2019 corresponds to the splits using random classes over 10 independent runs, \u2018TruZe\u2019 corresponds to the proposed split. Acc{}_{U} and Acc{}_{S} correspond to unseen class accuracy and seen class accuracy respectively. The semantic embedding used is sen2vec. \u2018diff\u2019 corresponds to the difference between \u2018Rand\u2019 and \u2018TruZe\u2019. We see consistent positive difference in performance on the unseen classes and negative difference in the performance of the seen classes while using the \u2018TruZe\u2019.", "list_citation_info": ["[25] Xian, Y., Lorenz, T., Schiele, B., Akata, Z.: Feature generating networks for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5542\u20135551 (2018)", "[7] Gowda, S.N., Sevilla-Lara, L., Keller, F., Rohrbach, M.: Claster: Clustering with reinforcement learning for zero-shot action recognition. arXiv preprint arXiv:2101.07042 (2021)", "[10] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)"]}, {"table": "<table><thead><tr><th>Method</th><th colspan=\"5\">SS</th><th colspan=\"5\">TruZe</th><th colspan=\"5\">Diff</th></tr><tr><th></th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><th>C3D-PN [16]</th><td>57.1</td><td>66.4</td><td>71.7</td><td>75.5</td><td>78.2</td><td>50.9</td><td>61.9</td><td>67.5</td><td>72.9</td><td>75.4</td><td>6.2</td><td>4.5</td><td>4.2</td><td>2.6</td><td>2.8</td></tr><tr><th>ARN [27]</th><td>66.3</td><td>73.1</td><td>77.9</td><td>80.4</td><td>83.1</td><td>61.2</td><td>70.7</td><td>75.2</td><td>78.8</td><td>80.2</td><td>5.1</td><td>2.4</td><td>2.7</td><td>1.6</td><td>2.9</td></tr><tr><th>TRX [14]</th><td>77.5</td><td>88.8</td><td>92.8</td><td>94.7</td><td>96.1</td><td>75.2</td><td>88.1</td><td>91.5</td><td>93.1</td><td>93.5</td><td>2.5</td><td>0.7</td><td>1.3</td><td>1.6</td><td>2.6</td></tr></tbody></table>", "caption": "Table 4: Few Shot Learning (FSL) with different splits on UCF101. Accuracies are reported for 5-way, 1, 2, 3, 4, 5-shot classification. \u2019SS\u2019 corresponds to the split used in [27, 14] and \u2019TruZe\u2019 corresponds to the proposed split. We can see that using our proposed split results in a drop in performance of up to 6.2 % for UCF101. This shows TruZe is much harder even in the FSL scenario.", "list_citation_info": ["[27] Zhang, H., Zhang, L., Qi, X., Li, H., Torr, P.H., Koniusz, P.: Few-shot action recognition with permutation-invariant attention. In: Proceedings of the European Conference on Computer Vision (ECCV). Springer (2020)", "[14] Perrett, T., Masullo, A., Burghardt, T., Mirmehdi, M., Damen, D.: Temporal-relational crosstransformers for few-shot action recognition. arXiv preprint arXiv:2101.06184 (2021)", "[16] Snell, J., Swersky, K., Zemel, R.S.: Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175 (2017)"]}, {"table": "<table><thead><tr><th>Method</th><th colspan=\"5\">SS</th><th colspan=\"5\">TruZe</th><th colspan=\"5\">Diff</th></tr><tr><th></th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><th>C3D-PN [16]</th><td>38.1</td><td>47.5</td><td>50.3</td><td>55.6</td><td>57.4</td><td>28.8</td><td>38.5</td><td>43.4</td><td>46.7</td><td>49.1</td><td>9.3</td><td>9.0</td><td>6.9</td><td>8.9</td><td>8.3</td></tr><tr><th>ARN [27]</th><td>45.5</td><td>50.1</td><td>54.2</td><td>58.7</td><td>60.6</td><td>31.9</td><td>42.3</td><td>46.5</td><td>49.8</td><td>53.2</td><td>12.6</td><td>7.8</td><td>7.7</td><td>8.9</td><td>7.4</td></tr><tr><th>TRX [14]</th><td>50.5</td><td>62.7</td><td>66.9</td><td>73.5</td><td>75.6</td><td>33.5</td><td>46.7</td><td>49.8</td><td>57.9</td><td>61.5</td><td>17.0</td><td>16.0</td><td>17.1</td><td>15.6</td><td>14.1</td></tr></tbody></table>", "caption": "Table 5: Few Shot Learning (FSL) with different splits on HMDB51. Accuracies are reported for 5-way, 1, 2, 3, 4, 5-shot classification. \u2019SS\u2019 corresponds to the split used in [27, 14] and \u2019TruZe\u2019 corresponds to the proposed split. We can see that using our proposed split results in a drop in performance of up to 17.1 % for HMDB51. This shows TruZe is much harder even in the FSL scenario.", "list_citation_info": ["[27] Zhang, H., Zhang, L., Qi, X., Li, H., Torr, P.H., Koniusz, P.: Few-shot action recognition with permutation-invariant attention. In: Proceedings of the European Conference on Computer Vision (ECCV). Springer (2020)", "[14] Perrett, T., Masullo, A., Burghardt, T., Mirmehdi, M., Damen, D.: Temporal-relational crosstransformers for few-shot action recognition. arXiv preprint arXiv:2101.06184 (2021)", "[16] Snell, J., Swersky, K., Zemel, R.S.: Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175 (2017)"]}, {"table": "<table><thead><tr><th>Method</th><th>Backbone</th><th>UCF101 Accuracy</th><th>HMDB51 Accuracy</th></tr></thead><tbody><tr><th>WGAN [25]</th><td>I3D</td><td>22.5</td><td>21.1</td></tr><tr><th>WGAN [25]</th><td>NL-I3D</td><td>22.7</td><td>21.3</td></tr><tr><th>WGAN [25]</th><td>SlowFast</td><td>23.1</td><td>21.5</td></tr><tr><th>OD [10]</th><td>I3D</td><td>22.9</td><td>21.7</td></tr><tr><th>OD [10]</th><td>NL-I3D</td><td>23.2</td><td>22.0</td></tr><tr><th>OD [10]</th><td>SlowFast</td><td>23.4</td><td>22.5</td></tr><tr><th>CLASTER [7]</th><td>I3D</td><td>45.2</td><td>33.2</td></tr><tr><th>CLASTER [7]</th><td>NL-I3D</td><td>45.3</td><td>33.6</td></tr><tr><th>CLASTER [7]</th><td>SlowFast</td><td>45.5</td><td>33.9</td></tr></tbody></table>", "caption": "Table 6: Results comparison using different backbones to extract visual features for the ZSL models. We evaluate OD, E2E and CLASTER using I3D, NL-I3D and SlowFast networks as backbones. All results are on the proposed split. We see that stronger backbones result in improved performance of the ZSL model.", "list_citation_info": ["[25] Xian, Y., Lorenz, T., Schiele, B., Akata, Z.: Feature generating networks for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5542\u20135551 (2018)", "[7] Gowda, S.N., Sevilla-Lara, L., Keller, F., Rohrbach, M.: Claster: Clustering with reinforcement learning for zero-shot action recognition. arXiv preprint arXiv:2101.07042 (2021)", "[10] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)"]}]}