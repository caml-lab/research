{"title": "Rest: An efficient transformer for visual recognition", "abstract": "This paper presents an efficient multi-scale vision Transformer, called ResT, that capably served as a general-purpose backbone for image recognition. Unlike existing Transformer methods, which employ standard Transformer blocks to tackle raw images with a fixed resolution, our ResT have several advantages: (1) A memory-efficient multi-head self-attention is built, which compresses the memory by a simple depth-wise convolution, and projects the interaction across the attention-heads dimension while keeping the diversity ability of multi-heads; (2) Position encoding is constructed as spatial attention, which is more flexible and can tackle with input images of arbitrary size without interpolation or fine-tune; (3) Instead of the straightforward tokenization at the beginning of each stage, we design the patch embedding as a stack of overlapping convolution operation with stride on the 2D-reshaped token map. We comprehensively validate ResT on image classification and downstream tasks. Experimental results show that the proposed ResT can outperform the recently state-of-the-art backbones by a large margin, demonstrating the potential of ResT as strong backbones. The code and models will be made publicly available at https://github.com/wofmanaf/ResT.", "authors": ["Qinglong Zhang", " Yubin Yang"], "pdf_url": "https://arxiv.org/abs/2105.13677", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td>#Params (M)</td><td>FLOPs (G)</td><td>Throughput</td><td>Top-1 (%)</td><td>Top-5 (%)</td></tr><tr><td colspan=\"6\">ConvNet</td></tr><tr><td>ResNet-18 DBLP:conf/cvpr/HeZRS16 </td><td>11.7</td><td>1.8</td><td>1852</td><td>69.7</td><td>89.1</td></tr><tr><td>ResNet-50 DBLP:conf/cvpr/HeZRS16 </td><td>25.6</td><td>4.1</td><td>871</td><td>79.0</td><td>94.4</td></tr><tr><td>ResNet-101 DBLP:conf/cvpr/HeZRS16 </td><td>44.7</td><td>7.9</td><td>635</td><td>80.3</td><td>95.2</td></tr><tr><td>RegNetY-4G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>20.6</td><td>4.0</td><td>1156</td><td>79.4</td><td>94.7</td></tr><tr><td>RegNetY-8G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>39.2</td><td>8.0</td><td>591</td><td>79.9</td><td>94.9</td></tr><tr><td>RegNetY-16G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>83.6</td><td>15.9</td><td>334</td><td>80.4</td><td>95.1</td></tr><tr><td colspan=\"6\">Transformer</td></tr><tr><td>DeiT-S DBLP:journals/corr/abs-2012-12877 </td><td>22.1</td><td>4.6</td><td>940</td><td>79.8</td><td>94.9</td></tr><tr><td>DeiT-B DBLP:journals/corr/abs-2012-12877 </td><td>86.6</td><td>17.6</td><td>292</td><td>81.8</td><td>95.6</td></tr><tr><td>PVT-T wang2021pyramid </td><td>13.2</td><td>1.9</td><td>1038</td><td>75.1</td><td>92.4</td></tr><tr><td>PVT-S wang2021pyramid </td><td>24.5</td><td>3.7</td><td>820</td><td>79.8</td><td>94.9</td></tr><tr><td>PVT-M wang2021pyramid </td><td>44.2</td><td>6.4</td><td>526</td><td>81.2</td><td>95.6</td></tr><tr><td>PVT-L wang2021pyramid </td><td>61.4</td><td>9.5</td><td>367</td><td>81.7</td><td>95.9</td></tr><tr><td>Swin-T DBLP:journals/corr/abs-2103-14030 </td><td>28.29</td><td>4.5</td><td>755</td><td>81.3</td><td>95.5</td></tr><tr><td>Swin-S DBLP:journals/corr/abs-2103-14030 </td><td>49.61</td><td>8.7</td><td>437</td><td>83.3</td><td>96.2</td></tr><tr><td>Swin-B DBLP:journals/corr/abs-2103-14030 </td><td>87.77</td><td>15.4</td><td>278</td><td>83.5</td><td>96.5</td></tr><tr><td>ResT-Lite (Ours)</td><td>10.49</td><td>1.4</td><td>1246</td><td>77.2 (\\uparrow 7.5)</td><td>93.7 (\\uparrow 4.6)</td></tr><tr><td>ResT-Small (Ours)</td><td>13.66</td><td>1.9</td><td>1043</td><td>79.6 (\\uparrow 9.9)</td><td>94.9 (\\uparrow 5.8)</td></tr><tr><td>ResT-Base (Ours)</td><td>30.28</td><td>4.3</td><td>673</td><td>81.6 (\\uparrow 2.6)</td><td>95.7 (\\uparrow 1.3)</td></tr><tr><td>ResT-Large (Ours)</td><td>51.63</td><td>7.9</td><td>429</td><td>83.6 (\\uparrow 3.3)</td><td>96.3 (\\uparrow 1.1)</td></tr></tbody></table>", "caption": "Table 2: Comparison with state-of-the-art backbones on ImageNet-1k benchmark. Throughput (images / s) is measured on a single V100 GPU, following  DBLP:journals/corr/abs-2012-12877 . All models are trained and evaluated on 224\\times224 resolution. The best records and the improvements over bench-marked ResNets are marked in bold and blue, respectively.", "list_citation_info": ["(18) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. CoRR, abs/2103.14030, 2021.", "(10) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016.", "(25) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. CoRR, abs/2012.12877, 2020.", "(21) Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10425\u201310433. IEEE, 2020.", "(28) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021."]}, {"table": "<table><tbody><tr><th>Backbones</th><td>AP50:95</td><td>AP50</td><td>AP75</td><td>APs</td><td>APm</td><td>APl</td><td>Param (M)</td></tr><tr><th>R18 DBLP:conf/cvpr/HeZRS16 </th><td>31.8</td><td>49.6</td><td>33.6</td><td>16.3</td><td>34.3</td><td>43.2</td><td>21.3</td></tr><tr><th>PVT-T wang2021pyramid </th><td>36.7</td><td>56.9</td><td>38.9</td><td>22.6</td><td>38.8</td><td>50.0</td><td>23.0</td></tr><tr><th>ResT-Small(Ours)</th><td>40.3</td><td>61.3</td><td>42.7</td><td>25.7</td><td>43.7</td><td>51.2</td><td>23.4</td></tr><tr><th>R50 DBLP:conf/cvpr/HeZRS16 </th><td>37.4</td><td>56.7</td><td>40.3</td><td>23.1</td><td>41.6</td><td>48.3</td><td>37.9</td></tr><tr><th>PVT-S wang2021pyramid </th><td>40.4</td><td>61.3</td><td>43.0</td><td>25.0</td><td>42.9</td><td>55.7</td><td>34.2</td></tr><tr><th>Swin-T DBLP:journals/corr/abs-2103-14030 </th><td>41.5</td><td>62.1</td><td>44.1</td><td>27.0</td><td>44.2</td><td>53.2</td><td>38.5</td></tr><tr><th>ResT-Base (Ours)</th><td>42.0</td><td>63.2</td><td>44.8</td><td>29.1</td><td>45.3</td><td>53.3</td><td>40.5</td></tr><tr><th>R101 DBLP:conf/cvpr/HeZRS16 </th><td>38.5</td><td>57.8</td><td>41.2</td><td>21.4</td><td>42.6</td><td>51.1</td><td>56.9</td></tr><tr><th>PVT-M wang2021pyramid </th><td>41.9</td><td>63.1</td><td>44.3</td><td>25.0</td><td>44.9</td><td>57.6</td><td>53.9</td></tr><tr><th>Swin-S DBLP:journals/corr/abs-2103-14030 </th><td>44.5</td><td>65.7</td><td>47.5</td><td>27.4</td><td>48.0</td><td>59.9</td><td>59.8</td></tr><tr><th>ResT-Large (Ours)</th><td>44.8</td><td>66.1</td><td>48.0</td><td>28.3</td><td>48.7</td><td>60.3</td><td>61.8</td></tr></tbody></table>", "caption": "Table 3: Object detection performance on the COCO val2017 split using the RetinaNet framework.", "list_citation_info": ["(18) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. CoRR, abs/2103.14030, 2021.", "(28) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.", "(10) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016."]}, {"table": "<table><thead><tr><th>Backbones</th><th>{\\rm AP^{box}}</th><th>{\\rm AP_{50}^{box}}</th><th>{\\rm AP_{75}^{box}}</th><th>{\\rm AP^{mask}}</th><th>{\\rm AP_{50}^{mask}}</th><th>{\\rm AP_{75}^{mask}}</th><th>Param (M)</th></tr></thead><tbody><tr><td>R18 DBLP:conf/cvpr/HeZRS16 </td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td><td>31.2</td></tr><tr><td>PVT-T wang2021pyramid </td><td>36.7</td><td>59.2</td><td>39.3</td><td>35.1</td><td>56.7</td><td>37.3</td><td>32.9</td></tr><tr><td>ResT-Small(Ours)</td><td>39.6</td><td>62.9</td><td>42.3</td><td>37.2</td><td>59.8</td><td>39.7</td><td>33.3</td></tr><tr><td>R50 DBLP:conf/cvpr/HeZRS16 </td><td>38.6</td><td>59.5</td><td>42.1</td><td>35.2</td><td>56.3</td><td>37.5</td><td>44.3</td></tr><tr><td>PVT-S wang2021pyramid </td><td>40.4</td><td>62.9</td><td>43.8</td><td>37.8</td><td>60.1</td><td>40.3</td><td>44.1</td></tr><tr><td>ResT-Base(Ours)</td><td>41.6</td><td>64.9</td><td>45.1</td><td>38.7</td><td>61.6</td><td>41.4</td><td>49.8</td></tr></tbody></table>", "caption": "Table 4: Object detection and instance segmentation performance on the COCO val2017 split using Mask RCNN framework.", "list_citation_info": ["(28) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.", "(10) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778. IEEE Computer Society, 2016."]}]}