{"title": "Memot: Multi-object tracking with memory", "abstract": "We propose an online tracking algorithm that performs the object detection and data association under a common framework, capable of linking objects after a long time span. This is realized by preserving a large spatio-temporal memory to store the identity embeddings of the tracked objects, and by adaptively referencing and aggregating useful information from the memory as needed. Our model, called MeMOT, consists of three main modules that are all Transformer-based: 1) Hypothesis Generation that produce object proposals in the current video frame; 2) Memory Encoding that extracts the core information from the memory for each tracked object; and 3) Memory Decoding that solves the object detection and data association tasks simultaneously for multi-object tracking. When evaluated on widely adopted MOT benchmark datasets, MeMOT observes very competitive performance.", "authors": ["Jiarui Cai", " Mingze Xu", " Wei Li", " Yuanjun Xiong", " Wei Xia", " Zhuowen Tu", " Stefano Soatto"], "pdf_url": "https://arxiv.org/abs/2203.16761", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>Training Data</td><td>Transformer</td><td>IDF1 \\uparrow</td><td>MOTA \\uparrow</td><td>HOTA \\uparrow</td><td>AssA \\uparrow</td><td>IDsw \\downarrow</td><td>MT(%) \\uparrow</td><td>ML(%) \\downarrow</td><td>FP \\downarrow</td><td>FN \\downarrow</td></tr><tr><td colspan=\"12\">MOT16 [35]</td></tr><tr><td>FairMOT [70]</td><td>13.1x</td><td></td><td>72.3</td><td>69.3</td><td>58.3</td><td>58.0</td><td>815</td><td>40.3</td><td>16.7</td><td>13501</td><td>41653</td></tr><tr><td>TubeTK [37]</td><td>44.5x</td><td></td><td>62.2</td><td>66.9</td><td>50.8</td><td>47.3</td><td>1236</td><td>39.0</td><td>16.1</td><td>11544</td><td>47502</td></tr><tr><td>CTracker [39]</td><td>1.0x</td><td></td><td>57.2</td><td>67.6</td><td>48.8</td><td>43.7</td><td>1897</td><td>32.9</td><td>23.1</td><td>8934</td><td>48350</td></tr><tr><td>JDE [54]</td><td>10.2x</td><td></td><td>55.8</td><td>64.4</td><td>-</td><td>-</td><td>1544</td><td>35.4</td><td>20.0</td><td>-</td><td>-</td></tr><tr><td>MOTR [69]</td><td>1.9x</td><td>\u2713</td><td>67.0</td><td>66.8</td><td>-</td><td>-</td><td>586</td><td>34.1</td><td>25.7</td><td>10364</td><td>49582</td></tr><tr><td>MeMOT (ours)</td><td>1.9x</td><td>\u2713</td><td>69.7</td><td>72.6</td><td>57.4</td><td>55.7</td><td>845</td><td>44.9</td><td>16.6</td><td>14595</td><td>34595</td></tr><tr><td colspan=\"12\">MOT17 [35]</td></tr><tr><td>CorrTracker [52]</td><td>13.1x</td><td></td><td>73.6</td><td>76.5</td><td>60.7</td><td>58.9</td><td>3396</td><td>47.6</td><td>12.7</td><td>29808</td><td>99510</td></tr><tr><td>FairMOT [70]</td><td>13.1x</td><td></td><td>72.3</td><td>73.7</td><td>59.3</td><td>58.0</td><td>3303</td><td>43.2</td><td>17.3</td><td>27507</td><td>117477</td></tr><tr><td>PermaTrack [49]</td><td>18.7x</td><td></td><td>68.9</td><td>73.8</td><td>55.5</td><td>53.1</td><td>3699</td><td>43.8</td><td>17.2</td><td>28998</td><td>115104</td></tr><tr><td>GSDT [53]</td><td>10.2x</td><td></td><td>66.5</td><td>73.2</td><td>55.2</td><td>51.0</td><td>3891</td><td>41.7</td><td>17.5</td><td>263397</td><td>120666</td></tr><tr><td>TraDeS [60]</td><td>3.8x</td><td></td><td>63.9</td><td>69.1</td><td>52.7</td><td>50.8</td><td>3555</td><td>36.4</td><td>21.5</td><td>20892</td><td>150060</td></tr><tr><td>TransTrack [48]</td><td>3.8x</td><td>\u2713</td><td>63.5</td><td>75.2</td><td>54.1</td><td>47.9</td><td>4614</td><td>55.3</td><td>10.2</td><td>50157</td><td>86442</td></tr><tr><td>TransCenter [67]</td><td>3.8x</td><td>\u2713</td><td>62.2</td><td>73.2</td><td>54.5</td><td>49.7</td><td>3663</td><td>40.8</td><td>18.5</td><td>23112</td><td>123738</td></tr><tr><td>TubeTK [37]</td><td>44.5x</td><td></td><td>58.6</td><td>63.0</td><td>48.0</td><td>45.1</td><td>4137</td><td>31.2</td><td>19.9</td><td>27060</td><td>177483</td></tr><tr><td>CTracker [39]</td><td>1.0x</td><td></td><td>57.4</td><td>66.6</td><td>49.0</td><td>37.8</td><td>5529</td><td>32.2</td><td>24.2</td><td>22284</td><td>160491</td></tr><tr><td>TrackFormer [34]</td><td>1.0x</td><td>\u2713</td><td>63.9</td><td>65.0</td><td>-</td><td>-</td><td>3258</td><td>-</td><td>-</td><td>70443</td><td>123552</td></tr><tr><td>MOTR [69]</td><td>1.9x</td><td>\u2713</td><td>67.0</td><td>67.4</td><td>-</td><td>-</td><td>1992</td><td>34.6</td><td>21.5</td><td>32355</td><td>149400</td></tr><tr><td>MeMOT (ours)</td><td>1.9x</td><td>\u2713</td><td>69.0</td><td>72.5</td><td>56.9</td><td>55.2</td><td>2724</td><td>43.8</td><td>18.0</td><td>37221</td><td>115248</td></tr><tr><td colspan=\"12\">MOT20 [10]</td></tr><tr><td>FairMOT [70]</td><td>8.2x</td><td></td><td>67.3</td><td>61.8</td><td>54.6</td><td>54.7</td><td>5243</td><td>68.8</td><td>7.6</td><td>103440</td><td>88901</td></tr><tr><td>TransTrack [48]</td><td>2.7x</td><td>\u2713</td><td>59.4</td><td>65.0</td><td>48.9</td><td>45.2</td><td>3608</td><td>50.1</td><td>13.4</td><td>27191</td><td>150197</td></tr><tr><td>TransCenter [67]</td><td>2.7x</td><td>\u2713</td><td>49.6</td><td>58.5</td><td>43.5</td><td>37.0</td><td>4695</td><td>48.6</td><td>14.9</td><td>64217</td><td>146019</td></tr><tr><td>MeMOT (ours)</td><td>1.0x</td><td>\u2713</td><td>66.1</td><td>63.7</td><td>54.1</td><td>55.0</td><td>1938</td><td>57.5</td><td>14.3</td><td>47882</td><td>137983</td></tr></table>", "caption": "Table 1: Evaluation results on MOT challenge datasets. Trackers with gray background use the in-network association solver (IAS), and others with white background use the post-model association solver (PAS).Best results of IAS are marked in bold.", "list_citation_info": ["[39] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In ECCV, 2020.", "[10] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix\u00e9. MOT20: A benchmark for multi object tracking in crowded scenes. arXiv:2003.09003, 2020.", "[35] Anton Milan, Laura Leal-Taix\u00e9, Ian Reid, Stefan Roth, and Konrad Schindler. MOT16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016.", "[52] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object tracking with correlation learning. In CVPR, 2021.", "[53] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In ICRA, 2021.", "[67] Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, and Xavier Alameda-Pineda. TransCenter: Transformers with dense queries for multiple-object tracking. arXiv:2103.15145, 2021.", "[37] Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, and Cewu Lu. Tubetk: Adopting tubes to track multi-object in a one-step training model. In CVPR, 2020.", "[54] Zhongdao Wang, Liang Zheng, Yixuan Liu, Yali Li, and Shengjin Wang. Towards real-time multi-object tracking. arXiv:1909.12605, 2019.", "[60] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In CVPR, 2021.", "[69] Fangao Zeng, Bin Dong, Tiancai Wang, Cheng Chen, Xiangyu Zhang, and Yichen Wei. MOTR: End-to-end multiple-object tracking with transformer. arXiv:2105.03247, 2021.", "[70] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. FairMOT: On the fairness of detection and re-identification in multiple object tracking. arXiv:2004.01888, 2020.", "[49] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In ICCV, 2021.", "[48] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. arXiv:2012.15460, 2020.", "[34] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking with transformers. arXiv:2101.02702, 2021."]}]}