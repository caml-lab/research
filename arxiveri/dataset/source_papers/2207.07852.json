{"title": "TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval", "abstract": "Text-Video retrieval is a task of great practical value and has received increasing attention, among which learning spatial-temporal video representation is one of the research hotspots. The video encoders in the state-of-the-art video retrieval models usually directly adopt the pre-trained vision backbones with the network structure fixed, they therefore can not be further improved to produce the fine-grained spatial-temporal video representation. In this paper, we propose Token Shift and Selection Network (TS2-Net), a novel token shift and selection transformer architecture, which dynamically adjusts the token sequence and selects informative tokens in both temporal and spatial dimensions from input video samples. The token shift module temporally shifts the whole token features back-and-forth across adjacent frames, to preserve the complete token representation and capture subtle movements. Then the token selection module selects tokens that contribute most to local spatial semantics. Based on thorough experiments, the proposed TS2-Net achieves state-of-the-art performance on major text-video retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC, ActivityNet, and DiDeMo.", "authors": ["Yuqi Liu", " Pengfei Xiong", " Luhui Xu", " Shengming Cao", " Qin Jin"], "pdf_url": "https://arxiv.org/abs/2207.07852", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"4\">Text \\Longrightarrow Video</th><th colspan=\"4\">Video \\Longrightarrow Text</th><th></th></tr><tr><th>Method</th><th>R@1</th><th>R@5</th><th>R@10</th><th>MnR</th><th>R@1</th><th>R@5</th><th>R@10</th><th>MnR</th><th>rsum</th></tr></thead><tbody><tr><td>Baseline</td><td>45.4</td><td>74.3</td><td>82.7</td><td>13.6</td><td>44.5</td><td>72.3</td><td>82.3</td><td>9.8</td><td>401.5</td></tr><tr><td>Channel Shift[52]</td><td>45.6</td><td>73.6</td><td>83.1</td><td>13.7</td><td>45.0</td><td>73.2</td><td>82.7</td><td>9.7</td><td>403.2</td></tr><tr><td>[VIS] Channel Shift[52]</td><td>45.1</td><td>73.8</td><td>83.5</td><td>13.9</td><td>44.7</td><td>73.3</td><td>82.2</td><td>9.8</td><td>402.6</td></tr><tr><td>[CLS] Channel Shift[52]</td><td>45.8</td><td>74.3</td><td>83.0</td><td>13.6</td><td>44.7</td><td>72.9</td><td>82.5</td><td>9.8</td><td>403.2</td></tr><tr><td>Token Shift</td><td>46.2</td><td>73.9</td><td>83.8</td><td>13.0</td><td>45.6</td><td>73.5</td><td>83.2</td><td>9.3</td><td>406.2</td></tr></tbody></table>", "caption": "Table 2: Performance comparison between other shift operation variants and our proposed token shift module on MSR-VTT-1k-A test split", "list_citation_info": ["[52] Zhang, H., Hao, Y., Ngo, C.W.: Token shift transformer for video classification. In: Proceedings of the 29th ACM International Conference on Multimedia. pp. 917\u2013925 (2021)"]}, {"table": "<table><tbody><tr><th></th><td colspan=\"5\">Text \\Longrightarrow Video</td><td colspan=\"5\">Video \\Longrightarrow Text</td></tr><tr><th>Method</th><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td></tr><tr><th>CE[32]</th><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td><td>28.2</td><td>20.6</td><td>50.3</td><td>64.0</td><td>5.3</td><td>25.1</td></tr><tr><th>TACo[49]</th><td>26.7</td><td>54.5</td><td>68.2</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MMT[21]</th><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td><td>24.0</td><td>27.0</td><td>57.5</td><td>69.7</td><td>3.7</td><td>21.3</td></tr><tr><th>SUPPORT-SET[37]</th><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td><td>-</td><td>26.6</td><td>55.1</td><td>67.5</td><td>3.0</td><td>-</td></tr><tr><th>TT-CE[14]</th><td>29.6</td><td>61.6</td><td>74.2</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>T2VLAD[45]</th><td>29.5</td><td>59.0</td><td>70.1</td><td>4.0</td><td>-</td><td>31.8</td><td>60.0</td><td>71.1</td><td>3.0</td><td>-</td></tr><tr><th>HIT-pretrained[31]</th><td>30.7</td><td>60.9</td><td>73.2</td><td>2.6</td><td>-</td><td>32.1</td><td>62.7</td><td>74.1</td><td>3.0</td><td>-</td></tr><tr><th>Frozen[4]</th><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MDMMT[17]</th><td>38.9</td><td>69.0</td><td>79.7</td><td>2.0</td><td>16.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP[38]</th><td>39.7</td><td>72.3</td><td>82.2</td><td>2.0</td><td>12.8</td><td>11.3</td><td>22.7</td><td>29.2</td><td>5.0</td><td>-</td></tr><tr><th>CLIP4Clip[35]</th><td>44.5</td><td>71.4</td><td>81.6</td><td>2.0</td><td>15.3</td><td>42.7</td><td>70.9</td><td>80.6</td><td>2.0</td><td>11.6</td></tr><tr><th>CAMoE[12]</th><td>44.6</td><td>72.6</td><td>81.8</td><td>2.0</td><td>13.3</td><td>45.1</td><td>72.4</td><td>83.1</td><td>2.0</td><td>10.0</td></tr><tr><th>CLIP2Video[19]</th><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td><td>14.6</td><td>43.5</td><td>72.3</td><td>82.1</td><td>2.0</td><td>10.2</td></tr><tr><th>TS2-Net</th><td>47.0</td><td>74.5</td><td>83.8</td><td>2.0</td><td>13.0</td><td>45.3</td><td>74.1</td><td>83.7</td><td>2.0</td><td>9.2</td></tr><tr><th>CLIP2TV[22]</th><td>48.3</td><td>74.6</td><td>82.8</td><td>2.0</td><td>14.9</td><td>46.5</td><td>75.4</td><td>84.9</td><td>2.0</td><td>10.2</td></tr><tr><th>TS2-Net(ViT16)</th><td>49.4</td><td>75.6</td><td>85.3</td><td>2.0</td><td>13.5</td><td>46.6</td><td>75.9</td><td>84.9</td><td>2.0</td><td>8.9</td></tr></tbody></table>", "caption": "Table 4: Retrieval results on MSR-VTT-1kA. Other SOTA methods are adopted as comparisons. Note that CLIP2TV uses patch size of 16\\times16, so we use TS2-Net(ViT16) for fair comparison. All results in this table do not use inverted softmax", "list_citation_info": ["[38] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning. pp. 8748\u20138763 (2021)", "[37] Patrick, M., Huang, P.Y., Asano, Y., Metze, F., Hauptmann, A., Henriques, J., Vedaldi, A.: Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824 (2020)", "[32] Liu, Y., Albanie, S., Nagrani, A., Zisserman, A.: Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487 (2019)", "[21] Gabeur, V., Sun, C., Alahari, K., Schmid, C.: Multi-modal transformer for video retrieval. In: European Conference on Computer Vision. pp. 214\u2013229 (2020)", "[14] Croitoru, I., Bogolin, S.V., Leordeanu, M., Jin, H., Zisserman, A., Albanie, S., Liu, Y.: Teachtext: Crossmodal generalized distillation for text-video retrieval. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11583\u201311593 (2021)", "[19] Fang, H., Xiong, P., Xu, L., Chen, Y.: Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097 (2021)", "[22] Gao, Z., Liu, J., Chen, S., Chang, D., Zhang, H., Yuan, J.: Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610 (2021)", "[45] Wang, X., Zhu, L., Yang, Y.: T2vlad: global-local sequence alignment for text-video retrieval. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5079\u20135088 (2021)", "[49] Yang, J., Bisk, Y., Gao, J.: Taco: Token-aware cascade contrastive learning for video-text alignment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11562\u201311572 (2021)", "[4] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1728\u20131738 (2021)", "[17] Dzabraev, M., Kalashnikov, M., Komkov, S., Petiushko, A.: Mdmmt: Multidomain multimodal transformer for video retrieval. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3354\u20133363 (2021)", "[35] Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860 (2021)", "[31] Liu, S., Fan, H., Qian, S., Chen, Y., Ding, W., Wang, Z.: Hit: Hierarchical transformer with momentum contrast for video-text retrieval. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11915\u201311925 (2021)", "[12] Cheng, X., Lin, H., Wu, X., Yang, F., Shen, D.: Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290 (2021)"]}, {"table": "<table><thead><tr><th>Dataset</th><th>Method</th><th>R@1</th><th>R@5</th><th>R@10</th><th>MdR</th><th>rsum</th></tr></thead><tbody><tr><th rowspan=\"2\">MSR-VTT[48]</th><th>CLIP2Video[19]</th><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td><td>199.9</td></tr><tr><th>TS2-Net(Ours)</th><td>47.0</td><td>74.5</td><td>83.8</td><td>2.0</td><td>205.3</td></tr><tr><th rowspan=\"2\">VATEX[46]</th><th>CLIP2Video[19]</th><td>57.3</td><td>90.0</td><td>95.5</td><td>1.0</td><td>242.8</td></tr><tr><th>TS2-Net(Ours)</th><td>59.1</td><td>90.0</td><td>95.2</td><td>1.0</td><td>244.3</td></tr><tr><th rowspan=\"2\">LSMDC[41]</th><th>CAMoE[12]</th><td>22.5</td><td>42.6</td><td>50.9</td><td>-</td><td>116.0</td></tr><tr><th>TS2-Net(Ours)</th><td>23.4</td><td>42.3</td><td>50.9</td><td>9.0</td><td>116.6</td></tr><tr><th rowspan=\"2\">DiDeMo[2]</th><th>CLIP4Clip[35]</th><td>42.5</td><td>70.2</td><td>80.6</td><td>2.0</td><td>193.3</td></tr><tr><th>TS2-Net(Ours)</th><td>41.8</td><td>71.6</td><td>82.0</td><td>2.0</td><td>195.4</td></tr><tr><th rowspan=\"2\">ActivityNet[18, 25]</th><th>CLIP4Clip[35]</th><td>40.5</td><td>73.4</td><td>-</td><td>2.0</td><td>-</td></tr><tr><th>TS2-Net(Ours)</th><td>41.0</td><td>73.6</td><td>84.5</td><td>2.0</td><td>199.1</td></tr></tbody></table>", "caption": "Table 7: Text-to-Video retrieval results on five benchmarks. We select the previous best performance on each dataset for comparison.", "list_citation_info": ["[48] Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for bridging video and language. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288\u20135296 (2016)", "[18] Fabian Caba Heilbron, Victor Escorcia, B.G., Niebles, J.C.: Activitynet: A large-scale video benchmark for human activity understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 961\u2013970 (2015)", "[41] Rohrbach, A., Torabi, A., Rohrbach, M., Tandon, N., Pal, C., Larochelle, H., Courville, A., Schiele, B.: Movie description. International Journal of Computer Vision pp. 94\u2013120 (2017)", "[2] Anne Hendricks, L., Wang, O., Shechtman, E., Sivic, J., Darrell, T., Russell, B.: Localizing moments in video with natural language. In: Proceedings of the IEEE international conference on computer vision. pp. 5803\u20135812 (2017)", "[19] Fang, H., Xiong, P., Xu, L., Chen, Y.: Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097 (2021)", "[35] Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860 (2021)", "[46] Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.F., Wang, W.Y.: Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4581\u20134591 (2019)", "[12] Cheng, X., Lin, H., Wu, X., Yang, F., Shen, D.: Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290 (2021)"]}]}