{"title": "JHU-CROWD++: Large-Scale Crowd Counting Dataset and A Benchmark Method", "abstract": "Due to its variety of applications in the real-world, the task of single image-based crowd counting has received a lot of interest in the recent years. Recently, several approaches have been proposed to address various problems encountered in crowd counting. These approaches are essentially based on convolutional neural networks that require large amounts of data to train the network parameters. Considering this, we introduce a new large scale unconstrained crowd counting dataset (JHU-CROWD++) that contains \"4,372\" images with \"1.51 million\" annotations. In comparison to existing datasets, the proposed dataset is collected under a variety of diverse scenarios and environmental conditions. Specifically, the dataset includes several images with weather-based degradations and illumination variations, making it a very challenging dataset. Additionally, the dataset consists of a rich set of annotations at both image-level and head-level. Several recent methods are evaluated and compared on this dataset. The dataset can be downloaded from http://www.crowd-counting.com .\n  Furthermore, we propose a novel crowd counting network that progressively generates crowd density maps via residual error estimation. The proposed method uses VGG16 as the backbone network and employs density map generated by the final layer as a coarse prediction to refine and generate finer density maps in a progressive fashion using residual learning. Additionally, the residual learning is guided by an uncertainty-based confidence weighting mechanism that permits the flow of only high-confidence residuals in the refinement path. The proposed Confidence Guided Deep Residual Counting Network (CG-DRCN) is evaluated on recent complex datasets, and it achieves significant improvements in errors.", "authors": ["Vishwanath A. Sindagi", " Rajeev Yasarla", " Vishal M. Patel"], "pdf_url": "https://arxiv.org/abs/2004.03597", "list_table_and_caption": [{"table": "<table><tbody><tr><th colspan=\"2\">Category</th><td colspan=\"2\">Low</td><td colspan=\"2\">Medium</td><td colspan=\"2\">High</td><td colspan=\"2\">Weather</td><td colspan=\"2\">Overall</td></tr><tr><th>Method</th><th>Model</th><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><th>MCNN [5] (CVPR 16)</th><th>Custom</th><td>90.6</td><td>202.9</td><td>125.3</td><td>259.5</td><td>494.9</td><td>856.0</td><td>241.1</td><td>532.2</td><td>160.6</td><td>377.7</td></tr><tr><th>CMTL [24] (AVSS 17)</th><th>Custom</th><td>50.2</td><td>129.2</td><td>88.1</td><td>170.7</td><td>583.1</td><td>986.5</td><td>165.0</td><td>312.9</td><td>138.1</td><td>379.5</td></tr><tr><th>CSR-Net [75](CVPR 18)</th><th>VGG16</th><td>22.2</td><td>40.0</td><td>49.0</td><td>99.5</td><td>302.5</td><td>669.5</td><td>83.0</td><td>168.7</td><td>72.2</td><td>249.9</td></tr><tr><th>SA-Net [26](ECCV 18)</th><th>VGG16</th><td>13.6</td><td>26.8</td><td>50.4</td><td>78.0</td><td>397.8</td><td>749.2</td><td>72.2</td><td>126.7</td><td>82.1</td><td>272.6</td></tr><tr><th>CACC [66] (CVPR 19)</th><th>VGG16</th><td>34.2</td><td>69.5</td><td>65.6</td><td>115.3</td><td>336.4</td><td>\\ul619.7</td><td>101.8</td><td>179.3</td><td>89.5</td><td>\\ul239.3</td></tr><tr><th>DSSI-Net [76] (ICCV 19)</th><th>VGG16</th><td>50.3</td><td>85.9</td><td>82.4</td><td>164.5</td><td>436.6</td><td>814.0</td><td>155.7</td><td>314.8</td><td>116.6</td><td>317.4</td></tr><tr><th>MBTTBF [63] (ICCV 19)</th><th>VGG16</th><td>23.3</td><td>48.5</td><td>53.2</td><td>119.9</td><td>\\ul294.5</td><td>674.5</td><td>88.2</td><td>200.8</td><td>73.8</td><td>256.8</td></tr><tr><th>LSC-CNN [69] (PAMI 20)</th><th>VGG16</th><td>\\ul6.8</td><td>\\ul10.1</td><td>\\ul39.2</td><td>\\ul64.1</td><td>504.7</td><td>860.0</td><td>77.6</td><td>187.2</td><td>87.3</td><td>309.0</td></tr><tr><th>CG-DRCN-CC-VGG16 (ours)</th><th>VGG16</th><td>17.1</td><td>44.7</td><td>40.8</td><td>71.2</td><td>317.4</td><td>719.8</td><td>\\ul63.5</td><td>\\ul116.6</td><td>\\ul67.9</td><td>262.1</td></tr><tr><th>SFCN [44] (CVPR 19)</th><th>ResNet-101</th><td>11.8</td><td>19.8</td><td>39.3</td><td>73.4</td><td>297.3</td><td>679.4</td><td>\\ul52.3</td><td>\\ul93.6</td><td>62.9</td><td>247.5</td></tr><tr><th>BCC [70](ICCV 19)</th><th>VGG19</th><td>\\ul6.9</td><td>\\ul10.3</td><td>39.7</td><td>85.2</td><td>279.8</td><td>\\ul620.4</td><td>58.9</td><td>124.7</td><td>59.3</td><td>\\ul229.2</td></tr><tr><th>CG-DRCN-CC-Res101 (ours)</th><th>ResNet-101</th><td>11.7</td><td>24.8</td><td>\\ul35.2</td><td>\\ul57.5</td><td>\\ul273.9</td><td>676.8</td><td>54.0</td><td>106.8</td><td>\\ul57.6</td><td>244.4</td></tr></tbody></table>", "caption": "TABLE X: Results on JHU-CROWD++ dataset (\u201cVal Set\u201d).  \\ulRED indicates best error and  BLUE indicates second-best error.", "list_citation_info": ["[63] \u2014\u2014, \u201cMulti-level bottom-top and top-bottom feature fusion for crowd counting,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1002\u20131012.", "[69] D. B. Sam, S. V. Peri, A. Kamath, R. V. Babu et al., \u201cLocate, size and count: Accurately resolving people in dense crowds via detection,\u201d arXiv preprint arXiv:1906.07538, 2019.", "[26] X. Cao, Z. Wang, Y. Zhao, and F. Su, \u201cScale aggregation network for accurate and efficient crowd counting,\u201d in European Conference on Computer Vision. Springer, 2018, pp. 757\u2013773.", "[66] W. Liu, M. Salzmann, and P. Fua, \u201cContext-aware crowd counting,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5099\u20135108.", "[5] Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma, \u201cSingle-image crowd counting via multi-column convolutional neural network,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 589\u2013597.", "[70] Z. Ma, X. Wei, X. Hong, and Y. Gong, \u201cBayesian loss for crowd count estimation with point supervision,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6142\u20136151.", "[24] V. A. Sindagi and V. M. Patel, \u201cCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting,\u201d in Advanced Video and Signal Based Surveillance (AVSS), 2017 IEEE International Conference on. IEEE, 2017.", "[75] Y. Li, X. Zhang, and D. Chen, \u201cCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 1091\u20131100.", "[44] Q. Wang, J. Gao, W. Lin, and Y. Yuan, \u201cLearning from synthetic data for crowd counting in the wild,\u201d arXiv preprint arXiv:1903.03303, 2019.", "[76] L. Liu, Z. Qiu, G. Li, S. Liu, W. Ouyang, and L. Lin, \u201cCrowd counting with deep structured scale integration network,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1774\u20131783."]}, {"table": "<table><tbody><tr><th colspan=\"2\">Category</th><td colspan=\"2\">Low</td><td colspan=\"2\">Medium</td><td colspan=\"2\">High</td><td colspan=\"2\">Weather</td><td colspan=\"2\">Overall</td></tr><tr><th>Method</th><th>Model</th><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><th>MCNN [5] (CVPR 16)</th><th>Custom</th><td>97.1</td><td>192.3</td><td>121.4</td><td>191.3</td><td>618.6</td><td>1,166.7</td><td>330.6</td><td>852.1</td><td>188.9</td><td>483.4</td></tr><tr><th>CMTL [24] (AVSS 17)</th><th>Custom</th><td>58.5</td><td>136.4</td><td>81.7</td><td>144.7</td><td>635.3</td><td>1,225.3</td><td>261.6</td><td>816.0</td><td>157.8</td><td>490.4</td></tr><tr><th>CSR-Net [75] (CVPR 18)</th><th>VGG16</th><td>27.1</td><td>64.9</td><td>43.9</td><td>71.2</td><td>356.2</td><td>784.4</td><td>141.4</td><td>640.1</td><td>85.9</td><td>309.2</td></tr><tr><th>SA-Net [26] (ECCV 18)</th><th>VGG16</th><td>17.3</td><td>37.9</td><td>46.8</td><td>69.1</td><td>397.9</td><td>817.7</td><td>154.2</td><td>685.7</td><td>91.1</td><td>320.4</td></tr><tr><th>CACC [66] (CVPR 19)</th><th>VGG16</th><td>37.6</td><td>78.8</td><td>56.4</td><td>86.2</td><td>384.2</td><td>789.0</td><td>155.4</td><td>\\ul617.0</td><td>100.1</td><td>314.0</td></tr><tr><th>DSSI-Net [76] (ICCV 19)</th><th>VGG16</th><td>53.6</td><td>112.8</td><td>70.3</td><td>108.6</td><td>525.5</td><td>1,047.4</td><td>229.1</td><td>760.3</td><td>133.5</td><td>416.5</td></tr><tr><th>MBTTBF [63] (ICCV 19)</th><th>VGG16</th><td>19.2</td><td>58.8</td><td>41.6</td><td>66.0</td><td>\\ul352.2</td><td>\\ul760.4</td><td>138.7</td><td>631.6</td><td>\\ul81.8</td><td>\\ul299.1</td></tr><tr><th>LSCCNN [69] (PAMI 20)</th><th>VGG16</th><td>\\ul10.6</td><td>\\ul31.8</td><td>\\ul34.9</td><td>\\ul55.6</td><td>601.9</td><td>1,172.2</td><td>178.0</td><td>744.3</td><td>112.7</td><td>454.4</td></tr><tr><th>CG-DRCN-CC-VGG16 (ours)</th><th>VGG16</th><td>19.5</td><td>58.7</td><td>38.4</td><td>62.7</td><td>367.3</td><td>837.5</td><td>\\ul138.6</td><td>654.0</td><td>82.3</td><td>328.0</td></tr><tr><th>SFCN [44] (CVPR 19)</th><th>ResNet-101</th><td>16.5</td><td>55.7</td><td>38.1</td><td>59.8</td><td>341.8</td><td>758.8</td><td>122.8</td><td>606.3</td><td>77.5</td><td>297.6</td></tr><tr><th>BCC [70] (ICCV 19)</th><th>VGG19</th><td>\\ul10.1</td><td>\\ul32.7</td><td>\\ul34.2</td><td>54.5</td><td>352.0</td><td>768.7</td><td>140.1</td><td>675.7</td><td>75.0</td><td>299.9</td></tr><tr><th>CG-DRCN-CC-Res101 (ours)</th><th>ResNet-101</th><td>14.0</td><td>42.8</td><td>35.0</td><td>\\ul53.7</td><td>\\ul314.7</td><td>\\ul712.3</td><td>\\ul120.0</td><td>\\ul580.8</td><td>\\ul71.0</td><td>\\ul278.6</td></tr></tbody></table>", "caption": "TABLE XI: Results on JHU-CROWD++ dataset (\u201cTest Set\u201d).  \\ulRED indicates best error and  BLUE indicates second-best error.", "list_citation_info": ["[63] \u2014\u2014, \u201cMulti-level bottom-top and top-bottom feature fusion for crowd counting,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1002\u20131012.", "[69] D. B. Sam, S. V. Peri, A. Kamath, R. V. Babu et al., \u201cLocate, size and count: Accurately resolving people in dense crowds via detection,\u201d arXiv preprint arXiv:1906.07538, 2019.", "[26] X. Cao, Z. Wang, Y. Zhao, and F. Su, \u201cScale aggregation network for accurate and efficient crowd counting,\u201d in European Conference on Computer Vision. Springer, 2018, pp. 757\u2013773.", "[66] W. Liu, M. Salzmann, and P. Fua, \u201cContext-aware crowd counting,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5099\u20135108.", "[5] Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma, \u201cSingle-image crowd counting via multi-column convolutional neural network,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 589\u2013597.", "[70] Z. Ma, X. Wei, X. Hong, and Y. Gong, \u201cBayesian loss for crowd count estimation with point supervision,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6142\u20136151.", "[24] V. A. Sindagi and V. M. Patel, \u201cCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting,\u201d in Advanced Video and Signal Based Surveillance (AVSS), 2017 IEEE International Conference on. IEEE, 2017.", "[75] Y. Li, X. Zhang, and D. Chen, \u201cCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 1091\u20131100.", "[44] Q. Wang, J. Gao, W. Lin, and Y. Yuan, \u201cLearning from synthetic data for crowd counting in the wild,\u201d arXiv preprint arXiv:1903.03303, 2019.", "[76] L. Liu, Z. Qiu, G. Li, S. Liu, W. Ouyang, and L. Lin, \u201cCrowd counting with deep structured scale integration network,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1774\u20131783."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"2\">Part-A</td><td colspan=\"2\">Part-B</td></tr><tr><th>Method</th><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><th>CP-CNN [6]</th><td>73.6</td><td>106.4</td><td>20.1</td><td>30.1</td></tr><tr><th>IG-CNN [30]</th><td>72.5</td><td>118.2</td><td>13.6</td><td>21.1</td></tr><tr><th>Liu et al.[58]</th><td>73.6</td><td>112.0</td><td>13.7</td><td>21.4</td></tr><tr><th>D-ConvNet [33]</th><td>73.5</td><td>112.3</td><td>18.7</td><td>26.0</td></tr><tr><th>CSRNet [75]</th><td>68.2</td><td>115.0</td><td>10.6</td><td>16.0</td></tr><tr><th>ic-CNN [25]</th><td>69.8</td><td>117.3</td><td>10.7</td><td>16.0</td></tr><tr><th>SA-Net [26]</th><td>67.0</td><td>104.5</td><td>8.4</td><td>13.6</td></tr><tr><th>ACSCP [32]</th><td>75.7</td><td>102.7</td><td>17.2</td><td>27.4</td></tr><tr><th>Jian et al.[64]</th><td>64.2</td><td>109.1</td><td>8.2</td><td>12.8</td></tr><tr><th>CA-Net [66]</th><td>61.3</td><td>100.0</td><td>7.8</td><td>12.2</td></tr><tr><th>BCC [70]</th><td>62.8</td><td>117.0</td><td>8.1</td><td>12.7</td></tr><tr><th>DSSI-Net [76]</th><td>60.6</td><td>96.0</td><td>\\ul6.8</td><td>\\ul10.3</td></tr><tr><th>MBTTBF [63]</th><td>60.2</td><td>94.1</td><td>8.0</td><td>15.5</td></tr><tr><th>LSC-CNN [69]</th><td>66.5</td><td>101.8</td><td>7.7</td><td>12.7</td></tr><tr><th>CG-DRCN-VGG16 (ours)</th><td>64.0</td><td>98.4</td><td>8.5</td><td>14.4</td></tr><tr><th>CG-DRCN-Res101 (ours)</th><td>\\ul60.2</td><td>\\ul94.0</td><td>7.5</td><td>12.1</td></tr></tbody></table>", "caption": "TABLE XII: Results on \u201cShanghaiTech\u201d dataset [5].", "list_citation_info": ["[33] Z. Shi, L. Zhang, Y. Liu, X. Cao, Y. Ye, M.-M. Cheng, and G. Zheng, \u201cCrowd counting with deep negative correlation learning,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "[63] \u2014\u2014, \u201cMulti-level bottom-top and top-bottom feature fusion for crowd counting,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1002\u20131012.", "[25] V. Ranjan, H. Le, and M. Hoai, \u201cIterative crowd counting,\u201d in European Conference on Computer Vision. Springer, 2018, pp. 278\u2013293.", "[69] D. B. Sam, S. V. Peri, A. Kamath, R. V. Babu et al., \u201cLocate, size and count: Accurately resolving people in dense crowds via detection,\u201d arXiv preprint arXiv:1906.07538, 2019.", "[26] X. Cao, Z. Wang, Y. Zhao, and F. Su, \u201cScale aggregation network for accurate and efficient crowd counting,\u201d in European Conference on Computer Vision. Springer, 2018, pp. 757\u2013773.", "[32] Z. Shen, Y. Xu, B. Ni, M. Wang, J. Hu, and X. Yang, \u201cCrowd counting via adversarial cross-scale consistency pursuit,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "[66] W. Liu, M. Salzmann, and P. Fua, \u201cContext-aware crowd counting,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5099\u20135108.", "[5] Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma, \u201cSingle-image crowd counting via multi-column convolutional neural network,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 589\u2013597.", "[30] D. Babu Sam, N. N. Sajjan, R. Venkatesh Babu, and M. Srinivasan, \u201cDivide and grow: Capturing huge diversity in crowd images with incrementally growing cnn,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 3618\u20133626.", "[64] X. Jiang, Z. Xiao, B. Zhang, X. Zhen, X. Cao, D. Doermann, and L. Shao, \u201cCrowd counting and density estimation by trellis encoder-decoder network,\u201d arXiv preprint arXiv:1903.00853, 2019.", "[70] Z. Ma, X. Wei, X. Hong, and Y. Gong, \u201cBayesian loss for crowd count estimation with point supervision,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6142\u20136151.", "[76] L. Liu, Z. Qiu, G. Li, S. Liu, W. Ouyang, and L. Lin, \u201cCrowd counting with deep structured scale integration network,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1774\u20131783.", "[58] X. Liu, J. van de Weijer, and A. D. Bagdanov, \u201cLeveraging unlabeled data for crowd counting by learning to rank,\u201d in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "[6] V. A. Sindagi and V. M. Patel, \u201cGenerating high-quality crowd density maps using contextual pyramid cnns,\u201d in The IEEE International Conference on Computer Vision (ICCV), Oct 2017.", "[75] Y. Li, X. Zhang, and D. Chen, \u201cCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 1091\u20131100."]}, {"table": "<table><tbody><tr><th>Method</th><td>MAE</td><td>MSE</td></tr><tr><th>Idrees et al.[3]</th><td>315.0</td><td>508.0</td></tr><tr><th>Zhang et al.[4]</th><td>277.0</td><td>426.0</td></tr><tr><th>CMTL et al.[24]</th><td>252.0</td><td>514.0</td></tr><tr><th>Switching-CNN [7]</th><td>228.0</td><td>445.0</td></tr><tr><th>Idrees et al.[21]</th><td>132.0</td><td>191.0</td></tr><tr><th>Jian et al.[64]</th><td>113.0</td><td>188.0</td></tr><tr><th>CA-Net [66]</th><td>107.0</td><td>183.0</td></tr><tr><th>DSSI-Net [76]</th><td>99.1</td><td>159.2</td></tr><tr><th>MBTTBF [63]</th><td>97.5</td><td>165.2</td></tr><tr><th>BCC [70]</th><td>\\ul88.7</td><td>\\ul154.8</td></tr><tr><th>LSC-CNN [69]</th><td>120.5</td><td>218.2</td></tr><tr><th>CG-DRCN-VGG16 (ours)</th><td>112.2</td><td>176.3</td></tr><tr><th>CG-DRCN-Res101 (ours)</th><td>95.5</td><td>164.3</td></tr></tbody></table>", "caption": "TABLE XIII: Results on \u201cUCF-QNRF \u201d dataset [21].", "list_citation_info": ["[63] \u2014\u2014, \u201cMulti-level bottom-top and top-bottom feature fusion for crowd counting,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1002\u20131012.", "[7] D. B. Sam, S. Surya, and R. V. Babu, \u201cSwitching convolutional neural network for crowd counting,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.", "[4] C. Zhang, H. Li, X. Wang, and X. Yang, \u201cCross-scene crowd counting via deep convolutional neural networks,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 833\u2013841.", "[21] H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed, N. Rajpoot, and M. Shah, \u201cComposition loss for counting, density map estimation and localization in dense crowds,\u201d in European Conference on Computer Vision. Springer, 2018, pp. 544\u2013559.", "[69] D. B. Sam, S. V. Peri, A. Kamath, R. V. Babu et al., \u201cLocate, size and count: Accurately resolving people in dense crowds via detection,\u201d arXiv preprint arXiv:1906.07538, 2019.", "[3] H. Idrees, I. Saleemi, C. Seibert, and M. Shah, \u201cMulti-source multi-scale counting in extremely dense crowd images,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2547\u20132554.", "[66] W. Liu, M. Salzmann, and P. Fua, \u201cContext-aware crowd counting,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 5099\u20135108.", "[70] Z. Ma, X. Wei, X. Hong, and Y. Gong, \u201cBayesian loss for crowd count estimation with point supervision,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6142\u20136151.", "[64] X. Jiang, Z. Xiao, B. Zhang, X. Zhen, X. Cao, D. Doermann, and L. Shao, \u201cCrowd counting and density estimation by trellis encoder-decoder network,\u201d arXiv preprint arXiv:1903.00853, 2019.", "[24] V. A. Sindagi and V. M. Patel, \u201cCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting,\u201d in Advanced Video and Signal Based Surveillance (AVSS), 2017 IEEE International Conference on. IEEE, 2017.", "[76] L. Liu, Z. Qiu, G. Li, S. Liu, W. Ouyang, and L. Lin, \u201cCrowd counting with deep structured scale integration network,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 1774\u20131783."]}]}