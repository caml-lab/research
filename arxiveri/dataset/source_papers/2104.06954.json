{"title": "Aligning latent and image spaces to connect the unconnectable", "abstract": "In this work, we develop a method to generate infinite high-resolution images with diverse and complex content. It is based on a perfectly equivariant generator with synchronous interpolations in the image and latent spaces. Latent codes, when sampled, are positioned on the coordinate grid, and each pixel is computed from an interpolation of the nearby style codes. We modify the AdaIN mechanism to work in such a setup and train the generator in an adversarial setting to produce images positioned between any two latent vectors. At test time, this allows for generating complex and diverse infinite images and connecting any two unrelated scenes into a single arbitrarily large panorama. Apart from that, we introduce LHQ: a new dataset of \\lhqsize high-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and LSUN Bridge and outperform the baselines by at least 4 times in terms of quality and diversity of the produced infinite images. The project page is located at https://universome.github.io/alis.", "authors": ["Ivan Skorokhodov", " Grigorii Sotnikov", " Mohamed Elhoseiny"], "pdf_url": "https://arxiv.org/abs/2104.06954", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Bridge 256^{2}</td><td colspan=\"2\">Tower 256^{2}</td><td colspan=\"2\">Landscapes 256^{2}</td><td rowspan=\"2\">     Speed</td><td rowspan=\"2\">#params</td></tr><tr><td>FID</td><td>\\infty-FID</td><td>FID</td><td>\\infty-FID</td><td>FID</td><td>\\infty-FID</td></tr><tr><td>Taming Transformers [13]</td><td>56.06</td><td>58.27</td><td>50.16</td><td>51.32</td><td>61.95</td><td>64.3</td><td>9981 ms/img</td><td>377M</td></tr><tr><td>LocoGAN [78]+SG2+Fourier</td><td>9.02</td><td>264.7</td><td>8.36</td><td>381.1</td><td>7.82</td><td>211.2</td><td>74.7 ms/img</td><td>53.7M</td></tr><tr><td>ALIS (ours)</td><td>10.24</td><td>10.79</td><td>8.83</td><td>8.99</td><td>10.48</td><td>10.64</td><td>53.9 ms/img</td><td>48.3M</td></tr><tr><td>   w/o coordinates</td><td>13.21</td><td>13.92</td><td>10.32</td><td>10.17</td><td>12.63</td><td>13.07</td><td>46.8 ms/img</td><td>47.1M</td></tr><tr><td>StyleGAN2 (config-e)</td><td>7.33</td><td>N/A</td><td>6.75</td><td>N/A</td><td>3.94</td><td>N/A</td><td>32.4 ms/img</td><td>47.1M</td></tr></table>", "caption": "Table 1: Scores for different models on different datasets in terms of FID and \\infty-FID. \u201cN/A\u201d denotes \u201cnot-applicable\u201d.", "list_citation_info": ["[78] \u0141ukasz Struski, Szymon Knop, Jacek Tabor, Wiktor Daniec, and Przemys\u0142aw Spurek. Locogan \u2013 locally convolutional gan, 2020.", "[13] Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for high-resolution image synthesis, 2020."]}, {"table": "<table><tr><td></td><td>SpatialGAN [27]</td><td>PS-GAN [5]</td><td>\\infty-GAN [39]</td><td>LocoGAN [78]</td><td>LocoGAN [78]+SG2 [78]+Fourier [60, 63]</td></tr><tr><td>Has local latents?</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td>Has global latents?</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td>Has periodic positional embeddings?</td><td></td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td>Not limited to a single image?</td><td>\u2713</td><td>\u2713</td><td></td><td>\u2713</td><td>\u2713</td></tr><tr><td>Was employed for non-texture datasets?</td><td></td><td></td><td>N/A</td><td>\u2713</td><td>\u2713</td></tr><tr><td>Is big?</td><td></td><td></td><td></td><td></td><td>\u2713</td></tr></table>", "caption": "Table 2: Comparison of existing infinite image generators. We used LocoGAN [78] as our benchmark since it shares all the features of previously developed infinite-texture image generators and was additionally tested on non-texture datasets. N/A denotes \u201cnot applicable\u201d: while \\infty-GAN was trained on non-texture images, its generations are in the \u201ctexture-like\u201d spirit.", "list_citation_info": ["[27] Nikolay Jetchev, Urs Bergmann, and Roland Vollgraf. Texture synthesis with spatial generative adversarial networks. arXiv preprint arXiv:1611.08207, 2016.", "[78] \u0141ukasz Struski, Szymon Knop, Jacek Tabor, Wiktor Daniec, and Przemys\u0142aw Spurek. Locogan \u2013 locally convolutional gan, 2020.", "[5] Urs Bergmann, Nikolay Jetchev, and Roland Vollgraf. Learning texture manifolds with the periodic spatial gan. arXiv preprint arXiv:1705.06566, 2017.", "[60] Vincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Proc. NeurIPS, 2020.", "[39] Chaochao Lu, Richard E. Turner, Yingzhen Li, and Nate Kushman. Interpreting spatially infinite generative models, 2020."]}]}