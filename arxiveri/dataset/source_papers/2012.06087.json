{"title": "Monocular Real-Time Full Body Capture with Inter-Part Correlations", "abstract": "We present the first method for real-time full body capture that estimates shape and motion of body and hands together with a dynamic 3D face model from a single color image. Our approach uses a new neural network architecture that exploits correlations between body and hands at high computational efficiency. Unlike previous works, our approach is jointly trained on multiple datasets focusing on hand, body or face separately, without requiring data where all the parts are annotated at the same time, which is much more difficult to create at sufficient variety. The possibility of such multi-dataset training enables superior generalization ability. In contrast to earlier monocular full body methods, our approach captures more expressive 3D face geometry and color by estimating the shape, expression, albedo and illumination parameters of a statistical face model. Our method achieves competitive accuracy on public benchmarks, while being significantly faster and providing more complete face reconstructions.", "authors": ["Yuxiao Zhou", " Marc Habermann", " Ikhsanul Habibie", " Ayush Tewari", " Christian Theobalt", " Feng Xu"], "pdf_url": "https://arxiv.org/abs/2012.06087", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">MPJPE (mm)</td></tr><tr><td>HM36M</td><td>MPII3D</td><td>MTC</td><td>HUMBI</td></tr><tr><td>Xiang et al. [68]</td><td>58.3</td><td>-</td><td>63.0</td><td>-</td></tr><tr><td>Kolotouros et al. [33]</td><td>41.1{}^{\\ddagger}</td><td>105.2</td><td>-</td><td>101.7{}^{\\ddagger\\mathsection}</td></tr><tr><td>Choutas et al. [9]</td><td>54.3{}^{\\ddagger}</td><td>-</td><td>-</td><td>67.2{}^{\\ddagger\\mathsection}</td></tr><tr><td>Kanazawa et al. [31]</td><td>56.8{}^{\\ddagger}</td><td>124.2</td><td>-</td><td>84.2{}^{\\ddagger\\mathsection}</td></tr><tr><td>DetNet</td><td>64.8</td><td>116.4</td><td>66.8</td><td>43.5</td></tr><tr><td>DetNet (PA)</td><td>50.3{}^{\\ddagger}</td><td>77.0{}^{\\ddagger}</td><td>61.5{}^{\\ddagger}</td><td>32.5{}^{\\ddagger}</td></tr></tbody></table>", "caption": "Table 2: Body MPJPE on public datasets.Our model has competitive results across all datasets while being much faster.{}^{\\mathsection} means the model is not trained on the train split.", "list_citation_info": ["[9] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Monocular expressive body regression through body-driven attention. arXiv preprint arXiv:2008.09062, 2020.", "[68] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10965\u201310974, 2019.", "[31] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7122\u20137131, 2018.", "[33] Nikos Kolotouros, Georgios Pavlakos, Michael Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2252\u20132261, 2019."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">MPJPE (mm)</td></tr><tr><td>MTC-Hand (left)</td><td>MTC-Hand (right)</td><td>FreiHand</td></tr><tr><td>Choutas et al. [9]</td><td>13.0{}^{\\ddagger\\mathsection}</td><td>12.2{}^{\\ddagger\\mathsection}</td><td>12.2{}^{\\ddagger}</td></tr><tr><td>Zhou et al. [78]</td><td>16.1{}^{\\ddagger\\mathsection}</td><td>15.6{}^{\\ddagger\\mathsection}</td><td>21.8{}^{\\ddagger\\mathsection}</td></tr><tr><td>DetNet</td><td>15.1</td><td>13.8</td><td>-</td></tr><tr><td>DetNet (PA)</td><td>8.50{}^{\\ddagger}</td><td>7.90{}^{\\ddagger}</td><td>24.2{}^{\\ddagger}</td></tr><tr><td>DetNet + IKNet (PA)</td><td>9.42{}^{\\ddagger}</td><td>9.10{}^{\\ddagger}</td><td>15.7{}^{\\ddagger}</td></tr></tbody></table>", "caption": "Table 4: Hand MPJPE on public datasets.Our model has the lowest error on MTC-Hand where the body information is available, and is comparable on FreiHand even the body is absent.{}^{\\mathsection} means the model is not trained on the train split.", "list_citation_info": ["[78] Yuxiao Zhou, Marc Habermann, Weipeng Xu, Ikhsanul Habibie, Christian Theobalt, and Feng Xu. Monocular real-time hand shape and motion capture using multi-modal data. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5346\u20135355, 2020.", "[9] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael J. Black. Monocular expressive body regression through body-driven attention. arXiv preprint arXiv:2008.09062, 2020."]}, {"table": "<table><thead><tr><th>Metric</th><th>Tewari et al. [61]</th><th>FaceNet</th><th>FaceNet-T</th></tr></thead><tbody><tr><td>Landmark Err.</td><td>4.70</td><td>3.43</td><td>3.37</td></tr><tr><td>Photometric Err.</td><td>0.0661</td><td>0.0447</td><td>0.0444</td></tr></tbody></table>", "caption": "Table 5: Landmark error in pixel and photometric error per channel on MTC-Face.FaceNet performs better than [61] on these challenging samples, and a tighter bounding box further improves accuracy.", "list_citation_info": ["[61] Ayush Tewari, Michael Zollhofer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard, Patrick Perez, and Christian Theobalt. Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction. In 2017 IEEE International Conference on Computer Vision Workshops (ICCVW), pages 1274\u20131283, 2017."]}]}