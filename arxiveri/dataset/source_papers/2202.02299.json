{"title": "Multi-task head pose estimation in-the-wild", "abstract": "We present a deep learning-based multi-task approach for head pose estimation in images. We contribute with a network architecture and training strategy that harness the strong dependencies among face pose, alignment and visibility, to produce a top performing model for all three tasks. Our architecture is an encoder-decoder CNN with residual blocks and lateral skip connections. We show that the combination of head pose estimation and landmark-based face alignment significantly improve the performance of the former task. Further, the location of the pose task at the bottleneck layer, at the end of the encoder, and that of tasks depending on spatial information, such as visibility and alignment, in the final decoder layer, also contribute to increase the final performance. In the experiments conducted the proposed model outperforms the state-of-the-art in the face pose and visibility tasks. By including a final landmark regression step it also produces face alignment results on par with the state-of-the-art.", "authors": ["Roberto Valle", " Jos\u00e9 Miguel Buenaposada", " Luis Baumela"], "pdf_url": "https://arxiv.org/abs/2202.02299", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"4\">AFLW</th><th colspan=\"4\">AFLW2000-3D</th><th colspan=\"4\">AFLW2000-3D-POSIT</th><th colspan=\"4\">Biwi</th></tr><tr><th>yaw</th><th>pitch</th><th>roll</th><th>mean</th><th>yaw</th><th>pitch</th><th>roll</th><th>mean</th><th>yaw</th><th>pitch</th><th>roll</th><th>mean</th><th>yaw</th><th>pitch</th><th>roll</th><th>mean</th></tr></thead><tbody><tr><th>FAb-Net [29]</th><td>10.70</td><td>7.13</td><td>5.14</td><td>7.65</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Kepler [4]</th><td>6.45</td><td>5.85</td><td>8.75</td><td>7.01</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8.08</td><td>17.2</td><td>16.1</td><td>13.8</td></tr><tr><th>Hyperface [5]</th><td>7.61</td><td>6.13</td><td>3.92</td><td>5.88</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HopeNet [22]</th><td>6.26</td><td>5.89</td><td>3.82</td><td>5.32</td><td>6.47</td><td>6.55</td><td>5.43</td><td>6.15</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.81</td><td>6.60</td><td>3.26</td><td>4.89</td></tr><tr><th>GLDL [24]</th><td>6.00</td><td>5.31</td><td>3.75</td><td>5.02</td><td>3.02</td><td>5.06</td><td>3.68</td><td>3.92</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.12</td><td>5.61</td><td>3.14</td><td>4.29</td></tr><tr><th>HF-ResNet[5]</th><td>6.24</td><td>5.33</td><td>3.29</td><td>4.95</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CCR [54]</th><td>5.22</td><td>5.85</td><td>2.51</td><td>4.52</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Amador et al. [21]</th><td>5.59</td><td>4.79</td><td>2.83</td><td>4.40</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>FSA-Caps-Fusion [35]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>4.50</td><td>6.08</td><td>4.64</td><td>5.07</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.27</td><td>4.96</td><td>2.76</td><td>4.00</td></tr><tr><th>QuatNet [23]</th><td>3.93</td><td>4.31</td><td>2.59</td><td>3.61</td><td>3.97</td><td>5.61</td><td>3.92</td><td>4.50</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.01</td><td>5.49</td><td>2.93</td><td>4.14</td></tr><tr><th>FDN [25]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>3.78</td><td>5.61</td><td>3.88</td><td>4.42</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.52</td><td>4.70</td><td>2.56</td><td>3.93</td></tr><tr><th>MNN</th><td>4.16</td><td>3.07</td><td>2.43</td><td>3.22</td><td>3.34</td><td>4.69</td><td>3.48</td><td>3.83</td><td>2.15</td><td>1.40</td><td>1.58</td><td>1.71</td><td>3.98</td><td>4.61</td><td>2.39</td><td>3.66</td></tr></tbody></table>", "caption": "TABLE II: Head pose MAEs for AFLW, AFLW2000-3D and Biwi. AFLW200-3D-POSIT is the outcome of re-annotating AFLW2000-3D with the corrected landmarks annotations from [50].", "list_citation_info": ["[4] A. Kumar, A. Alavi, and R. Chellappa, \u201cKEPLER: simultaneous estimation of keypoints and 3D pose of unconstrained faces in a unified framework by learning efficient H-CNN regressors,\u201d Image and Vision Computing, vol. 79, pp. 49\u201362, 2018.", "[22] N. Ruiz, E. Chong, and J. M. Rehg, \u201cFine-grained head pose estimation without keypoints,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2018, pp. 2074\u20132083.", "[29] O. Wiles, A. S. Koepke, and A. Zisserman, \u201cSelf-supervised learning of a facial attribute embedding from video,\u201d in Proc. British Machine Vision Conference, 2018, p. 302.", "[25] H. Zhang, M. Wang, Y. Liu, and Y. Yuan, \u201cFDN: Feature decoupling network for head pose estimation,\u201d in The Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020, pp. 12\u2009789\u201312\u2009796.", "[24] Z. Liu, Z. Chen, J. Bai, S. Li, and S. Lian, \u201cFacial pose estimation by deep learning from label distributions,\u201d in Proc. International Conference on Computer Vision Workshops, 2019.", "[21] E. Amador, R. Valle, J. M. Buenaposada, and L. Baumela, \u201cBenchmarking head pose estimation in-the-wild,\u201d in Proc. Iberoamerican Congress on Pattern Recognition, 2017, pp. 45\u201352.", "[5] R. Ranjan, V. M. Patel, and R. Chellappa, \u201cHyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 41, no. 1, pp. 121\u2013135, 2019.", "[23] H. Hsu, T. Wu, S. Wan, W. H. Wong, and C. Lee, \u201cQuatnet: Quaternion-based head pose estimation with multi-regression loss,\u201d IEEE Trans. on Multimedia, 2018.", "[35] T. Yang, Y. Chen, Y. Lin, and Y. Chuang, \u201cFSA-Net: Learning fine-grained structure aggregation for head pose estimation from a single image,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 1087\u20131096.", "[54] W. Zhang, H. Zhang, Q. Li, F. Liu, Z. Sun, X. Li, and X. Wanu, \u201cCross-cascading regression for simultaneous head pose estimation and facial landmark detection,\u201d in Biometric Recognition, 2018, pp. 148\u2013156.", "[50] A. Bulat and G. Tzimiropoulos, \u201cHow far are we from solving the 2D & 3D face alignment problem? (and a dataset of 230,000 3D facial landmarks),\u201d in Proc. International Conference on Computer Vision, 2017, pp. 1021\u20131030."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th>Full</th></tr><tr><th>occlusion</th></tr><tr><th>RCPR [42]</th><th>40</th></tr></thead><tbody><tr><th>Wu et al. [43]</th><td>44.43</td></tr><tr><th>Wu et al. [55]</th><td>49.11</td></tr><tr><th>ECT [56]</th><td>63.4</td></tr><tr><th>3DDE [45]</th><td>63.89</td></tr><tr><th>MNN</th><td>72.12</td></tr></tbody></table>", "caption": "TABLE III: Recall of landmarks visibility estimation methods at 80% precision using COFW.", "list_citation_info": ["[45] \u2014\u2014, \u201cFace alignment using a 3D deeply-initialized ensemble of regression trees,\u201d Computer Vision and Image Understanding, vol. 189, p. 102846, 2019.", "[42] X. P. Burgos-Artizzu, P. Perona, and P. Dollar, \u201cRobust face landmark estimation under occlusion,\u201d in Proc. International Conference on Computer Vision, 2013, pp. 1513\u20131520.", "[55] Y. Wu and Q. Ji, \u201cRobust facial landmark detection under significant head poses and occlusion,\u201d in Proc. International Conference on Computer Vision, 2015, pp. 234\u2013241.", "[43] Y. Wu, C. Gou, and Q. Ji, \u201cSimultaneous facial landmark detection, pose and deformation estimation under facial occlusion,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5719\u20135728.", "[56] H. Zhang, Q. Li, Z. Sun, and Y. Liu, \u201cCombining data-driven and model-driven methods for robust facial landmark detection,\u201d IEEE Trans. Information Forensics and Security, vol. 13, pp. 2409\u20132422, 2018."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td>Full</td></tr><tr><td>pupils</td></tr><tr><th>RCPR [42]</th><td>8.50</td></tr><tr><th>TCDCN [20]</th><td>8.05</td></tr><tr><th>Wu et al. [43]</th><td>6.40</td></tr><tr><th>Wu et al. [55]</th><td>5.93</td></tr><tr><th>ECT [56]</th><td>5.98</td></tr><tr><th>PCD-CNN [17]</th><td>5.77</td></tr><tr><th>SHN [39]</th><td>5.6</td></tr><tr><th>Wing [58]</th><td>5.44</td></tr><tr><th>ODN [59]</th><td>5.30</td></tr><tr><th>3DDE [45]</th><td>5.11</td></tr><tr><th>CHR2C [57]</th><td>5.09</td></tr><tr><th>MNN</th><td>5.65</td></tr><tr><th>MNN+OR</th><td>5.04</td></tr></tbody></table>", "caption": "TABLE IV: Face alignment NME using COFW.", "list_citation_info": ["[45] \u2014\u2014, \u201cFace alignment using a 3D deeply-initialized ensemble of regression trees,\u201d Computer Vision and Image Understanding, vol. 189, p. 102846, 2019.", "[59] M. Zhu, D. Shi, M. Zheng, and M. Sadiq, \u201cRobust facial landmark detection via occlusion-adaptive deep networks,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 3486\u20133496.", "[42] X. P. Burgos-Artizzu, P. Perona, and P. Dollar, \u201cRobust face landmark estimation under occlusion,\u201d in Proc. International Conference on Computer Vision, 2013, pp. 1513\u20131520.", "[17] A. Kumar and R. Chellappa, \u201cDisentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 430\u2013439.", "[20] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, \u201cLearning deep representation for face alignment with auxiliary attributes,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 38, no. 5, pp. 918\u2013930, 2016.", "[55] Y. Wu and Q. Ji, \u201cRobust facial landmark detection under significant head poses and occlusion,\u201d in Proc. International Conference on Computer Vision, 2015, pp. 234\u2013241.", "[43] Y. Wu, C. Gou, and Q. Ji, \u201cSimultaneous facial landmark detection, pose and deformation estimation under facial occlusion,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 5719\u20135728.", "[39] J. Yang, Q. Liu, and K. Zhang, \u201cStacked hourglass network for robust facial landmark localisation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017, pp. 2025\u20132033.", "[57] R. Valle, J. M. Buenaposada, and L. Baumela, \u201cCascade of encoder-decoder CNNs with learned coordinates regressor for robust facial landmarks detection,\u201d Pattern Recognition Letters, vol. 136, pp. 326\u2013332, 2020.", "[56] H. Zhang, Q. Li, Z. Sun, and Y. Liu, \u201cCombining data-driven and model-driven methods for robust facial landmark detection,\u201d IEEE Trans. Information Forensics and Security, vol. 13, pp. 2409\u20132422, 2018.", "[58] Z. Feng, J. Kittler, M. Awais, P. Huber, and X. Wu, \u201cWing loss for robust facial landmark localisation with convolutional neural networks,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 2235\u20132245."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td>[0{}^{\\circ}, 30{}^{\\circ}]</td><td>[30{}^{\\circ}, 60{}^{\\circ}]</td><td>[60{}^{\\circ}, 90{}^{\\circ}]</td><td>Full</td></tr><tr><td>height</td><td>height</td><td>height</td><td>height</td></tr><tr><th>CCR [54]</th><td>-</td><td>-</td><td>-</td><td>5.72</td></tr><tr><th>Hyperface [5]</th><td>3.93</td><td>4.14</td><td>4.71</td><td>4.26</td></tr><tr><th>Kepler [4]</th><td>-</td><td>-</td><td>-</td><td>2.98</td></tr><tr><th>AIO [19]</th><td>2.84</td><td>2.94</td><td>3.09</td><td>2.96</td></tr><tr><th>HF-ResNet [5]</th><td>2.71</td><td>2.88</td><td>3.19</td><td>2.93</td></tr><tr><th>Binary-CNN [60]</th><td>2.77</td><td>2.60</td><td>2.64</td><td>2.85</td></tr><tr><th>PCD-CNN [17]</th><td>2.33</td><td>2.60</td><td>2.64</td><td>2.49</td></tr><tr><th>3DDE [45]</th><td>2.10</td><td>2.00</td><td>2.04</td><td>2.06</td></tr><tr><th>CHR2C [57]</th><td>2.07</td><td>1.86</td><td>1.81</td><td>1.98</td></tr><tr><th>MNN</th><td>2.12</td><td>1.90</td><td>1.89</td><td>2.03</td></tr><tr><th>MNN+OR</th><td>2.05</td><td>1.86</td><td>1.85</td><td>1.97</td></tr></tbody></table>", "caption": "TABLE V: Face alignment NME using AFLW.", "list_citation_info": ["[45] \u2014\u2014, \u201cFace alignment using a 3D deeply-initialized ensemble of regression trees,\u201d Computer Vision and Image Understanding, vol. 189, p. 102846, 2019.", "[4] A. Kumar, A. Alavi, and R. Chellappa, \u201cKEPLER: simultaneous estimation of keypoints and 3D pose of unconstrained faces in a unified framework by learning efficient H-CNN regressors,\u201d Image and Vision Computing, vol. 79, pp. 49\u201362, 2018.", "[19] R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chellappa, \u201cAn all-in-one convolutional neural network for face analysis,\u201d in Proc. International Conference on Automatic Face and Gesture Recognition, 2017, pp. 17\u201324.", "[17] A. Kumar and R. Chellappa, \u201cDisentangling 3D pose in a dendritic CNN for unconstrained 2D face alignment,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 430\u2013439.", "[5] R. Ranjan, V. M. Patel, and R. Chellappa, \u201cHyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 41, no. 1, pp. 121\u2013135, 2019.", "[57] R. Valle, J. M. Buenaposada, and L. Baumela, \u201cCascade of encoder-decoder CNNs with learned coordinates regressor for robust facial landmarks detection,\u201d Pattern Recognition Letters, vol. 136, pp. 326\u2013332, 2020.", "[54] W. Zhang, H. Zhang, Q. Li, F. Liu, Z. Sun, X. Li, and X. Wanu, \u201cCross-cascading regression for simultaneous head pose estimation and facial landmark detection,\u201d in Biometric Recognition, 2018, pp. 148\u2013156.", "[60] A. Bulat and G. Tzimiropoulos, \u201cBinarized convolutional landmark localizers for human pose estimation and face alignment with limited resources,\u201d in Proc. International Conference on Computer Vision, 2017, pp. 3726\u20133734."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td>[0{}^{\\circ}, 30{}^{\\circ}]</td><td>[30{}^{\\circ}, 60{}^{\\circ}]</td><td>[60{}^{\\circ}, 90{}^{\\circ}]</td><td>Full</td></tr><tr><td>height</td><td>height</td><td>height</td><td>height</td></tr><tr><th>RCPR [42]</th><td>4.26</td><td>5.96</td><td>13.18</td><td>7.80</td></tr><tr><th>3DSTN [62]</th><td>3.15</td><td>4.33</td><td>5.98</td><td>4.49</td></tr><tr><th>3DDFA [48]</th><td>2.84</td><td>3.57</td><td>4.96</td><td>3.79</td></tr><tr><th>PRN [63]</th><td>2.75</td><td>3.51</td><td>4.61</td><td>3.62</td></tr><tr><th>Binary-CNN [60]</th><td>2.47</td><td>3.01</td><td>4.31</td><td>3.26</td></tr><tr><th>MHM [61]</th><td>2.36</td><td>2.80</td><td>4.08</td><td>3.08</td></tr><tr><th>MNN</th><td>2.71</td><td>2.53</td><td>3.48</td><td>2.77</td></tr><tr><th>MNN+OR</th><td>2.54</td><td>2.24</td><td>3.34</td><td>2.58</td></tr></tbody></table>", "caption": "TABLE VI: Face alignment NME using AFLW2000-3D.", "list_citation_info": ["[48] X. Zhu, X. Liu, Z. Lei, and S. Z. Li, \u201cFace alignment in full pose range: A 3D total solution,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 41, no. 1, pp. 78\u201392, 2017.", "[42] X. P. Burgos-Artizzu, P. Perona, and P. Dollar, \u201cRobust face landmark estimation under occlusion,\u201d in Proc. International Conference on Computer Vision, 2013, pp. 1513\u20131520.", "[61] J. Deng, Y. Zhou, S. Cheng, and S. Zafeiriou, \u201cCascade multi-view hourglass model for robust 3D face alignment,\u201d in Proc. International Conference on Automatic Face and Gesture Recognition, 2018, pp. 399\u2013403.", "[62] C. Bhagavatula, C. Zhu, K. Luu, and M. Savvides, \u201cFaster than real-time facial alignment: A 3D spatial transformer network approach in unconstrained poses,\u201d in Proc. International Conference on Computer Vision, 2017, pp. 4000\u20134009.", "[63] Y. Feng, F. Wu, X. Shao, Y. Wang, and X. Zhou, \u201cJoint 3D face reconstruction and dense alignment with position map regression network,\u201d in Proc. European Conference on Computer Vision, 2018, pp. 557\u2013574.", "[60] A. Bulat and G. Tzimiropoulos, \u201cBinarized convolutional landmark localizers for human pose estimation and face alignment with limited resources,\u201d in Proc. International Conference on Computer Vision, 2017, pp. 3726\u20133734."]}]}