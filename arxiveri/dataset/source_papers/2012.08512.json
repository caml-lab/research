{"title": "Flavr: Flow-agnostic video representations for fast frame interpolation", "abstract": "A majority of methods for video frame interpolation compute bidirectional optical flow between adjacent frames of a video, followed by a suitable warping algorithm to generate the output frames. However, approaches relying on optical flow often fail to model occlusions and complex non-linear motions directly from the video and introduce additional bottlenecks unsuitable for widespread deployment. We address these limitations with FLAVR, a flexible and efficient architecture that uses 3D space-time convolutions to enable end-to-end learning and inference for video frame interpolation. Our method efficiently learns to reason about non-linear motions, complex occlusions and temporal abstractions, resulting in improved performance on video interpolation, while requiring no additional inputs in the form of optical flow or depth maps. Due to its simplicity, FLAVR can deliver 3x faster inference speed compared to the current most accurate method on multi-frame interpolation without losing interpolation accuracy. In addition, we evaluate FLAVR on a wide range of challenging settings and consistently demonstrate superior qualitative and quantitative results compared with prior methods on various popular benchmarks including Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR for video frame interpolation can serve as a useful self-supervised pretext task for action recognition, optical flow estimation, and motion magnification.", "authors": ["Tarun Kalluri", " Deepak Pathak", " Manmohan Chandraker", " Du Tran"], "pdf_url": "https://arxiv.org/abs/2012.08512", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td rowspan=\"2\">Inputs</td><td colspan=\"2\">Vimeo-90K</td><td colspan=\"2\">UCF101</td><td colspan=\"2\">DAVIS</td></tr><tr><td>PSNR (\\uparrow)</td><td>SSIM(\\uparrow)</td><td>PSNR(\\uparrow)</td><td>SSIM(\\uparrow)</td><td>PSNR(\\uparrow)</td><td>SSIM(\\uparrow)</td></tr><tr><th>DAIN [2]</th><td>RGB+Depth+Flow</td><td>33.35</td><td>0.945</td><td>31.64</td><td>0.957</td><td>26.12</td><td>0.870</td></tr><tr><th>QVI [77]</th><td>RGB+Flow</td><td>35.15</td><td>0.971</td><td>32.89</td><td>0.970</td><td>27.17</td><td>0.874</td></tr><tr><th>DVF [33]</th><td>RGB</td><td>27.27</td><td>0.893</td><td>28.72</td><td>0.937</td><td>22.13</td><td>0.800</td></tr><tr><th>SepConv [45]</th><td>RGB</td><td>33.60</td><td>0.944</td><td>31.97</td><td>0.943</td><td>26.21</td><td>0.857</td></tr><tr><th>CAIN [9]</th><td>RGB</td><td>33.93</td><td>0.964</td><td>32.28</td><td>0.965</td><td>26.46</td><td>0.856</td></tr><tr><th>SuperSloMo [23]</th><td>RGB</td><td>32.90</td><td>0.957</td><td>32.33</td><td>0.960</td><td>25.65</td><td>0.857</td></tr><tr><th>BMBC [49]</th><td>RGB</td><td>34.76</td><td>0.965</td><td>32.61</td><td>0.955</td><td>26.42</td><td>0.868</td></tr><tr><th>AdaCoF [27]</th><td>RGB</td><td>35.40</td><td>0.971</td><td>32.71</td><td>0.969</td><td>26.49</td><td>0.866</td></tr><tr><th>FLAVR</th><td>RGB</td><td>36.25{}^{\\pm 0.06}</td><td>0.975</td><td>33.31{}^{\\pm 0.02}</td><td>0.971</td><td>27.43{}^{\\pm 0.02}</td><td>0.874</td></tr></tbody></table>", "caption": "Table 1: Comparison with state-of-the-art methods for 2x interpolation on Vimeo-90K, UCF101, and DAVIS datasets. The upper table includes the methods that use additional networks trained to predict optical flows and/or depth maps. The lower table represents the methods the use only RGB as input. The first and second best methods are marked in bold and underlined text. Our method consistently outperforms prior works which take only RGB as input, as well as works which additionally require optical flows and/or depth inputs.", "list_citation_info": ["[77] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video interpolation. In Advances in Neural Information Processing Systems, pages 1647\u20131656, 2019.", "[45] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE International Conference on Computer Vision, pages 261\u2013270, 2017.", "[49] Junheum Park, Keunsoo Ko, Chul Lee, and Chang-Su Kim. Bmbc: Bilateral motion estimation with bilateral cost volume for video interpolation. arXiv preprint arXiv:2007.12622, 2020.", "[33] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis using deep voxel flow. In Proceedings of the IEEE International Conference on Computer Vision, pages 4463\u20134471, 2017.", "[2] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3703\u20133712, 2019.", "[9] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and Kyoung Mu Lee. Channel attention is all you need for video frame interpolation. In AAAI, pages 10663\u201310671, 2020.", "[27] Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban, and Sangyoun Lee. Adacof: Adaptive collaboration of flows for video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5316\u20135325, 2020.", "[23] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9000\u20139008, 2018."]}, {"table": "MethodInputsAdobeGoProPSNRSSIMPSNRSSIMDAIN [2]RGB+Depth+Flow29.500.910290.91QVI [77]RGB+Flow33.680.9730.550.933DVF [33]RGB28.230.89621.940.776SuperSloMo [23]RGB30.660.39128.520.891FLAVRRGB32.200.95731.310.94", "caption": "Table 2: Comparison with state-of-the-art methods for 8x interpolation on Adobe and GoPro datasets. FLAVR outperforms all previous work that use only RGB as input.", "list_citation_info": ["[77] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video interpolation. In Advances in Neural Information Processing Systems, pages 1647\u20131656, 2019.", "[33] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis using deep voxel flow. In Proceedings of the IEEE International Conference on Computer Vision, pages 4463\u20134471, 2017.", "[2] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3703\u20133712, 2019.", "[23] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9000\u20139008, 2018."]}]}