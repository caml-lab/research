{"title": "Bifurcated Backbone Strategy for RGB-D Salient Object Detection", "abstract": "Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel cascaded refinement network. In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is simple, efficient, and backbone-independent. Extensive experiments show that BBS-Net significantly outperforms eighteen SOTA models on eight challenging datasets under five evaluation measures, demonstrating the superiority of our approach ($\\sim 4 \\%$ improvement in S-measure $vs.$ the top-ranked model: DMRA-iccv2019). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research.", "authors": ["Yingjie Zhai", " Deng-Ping Fan", " Jufeng Yang", " Ali Borji", " Ling Shao", " Junwei Han", " Liang Wang"], "pdf_url": "https://arxiv.org/abs/2007.02713", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">#</th><th rowspan=\"2\"><svg><g><path></path><g><g><foreignobject>Method</foreignobject></g></g><g><g><foreignobject>Dataset</foreignobject></g></g></g></svg></th><td colspan=\"3\">DUT [19]</td></tr><tr><td>S_{\\alpha}\\uparrow</td><td>max F_{\\beta} \\uparrow</td><td>max E_{\\xi} \\uparrow</td></tr><tr><th rowspan=\"5\">Handcrafted</th><th>MB [90]</th><td>.607</td><td>.577</td><td>.691</td></tr><tr><th>LHM [32]</th><td>.568</td><td>.659</td><td>.767</td></tr><tr><th>DESM [35]</th><td>.659</td><td>.668</td><td>.733</td></tr><tr><th>DCMC [91]</th><td>.499</td><td>.406</td><td>.712</td></tr><tr><th>CDCP [36]</th><td>.687</td><td>.633</td><td>.794</td></tr><tr><th rowspan=\"4\">Deep-based</th><th>DMRA [19]</th><td>.888</td><td>.883</td><td>.927</td></tr><tr><th>A2dele [92]</th><td>.886</td><td>.892</td><td>.929</td></tr><tr><th>SSF [93]</th><td>.916</td><td>.924</td><td>.951</td></tr><tr><th>BBS-Net (ours)</th><td>.920</td><td>.927</td><td>.955</td></tr></tbody></table>", "caption": "TABLE I:  Performance of different models on the DUT [19] dataset. Models are trained and tested on the DUT using the proposed training and test sets split from [19].", "list_citation_info": ["[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[36] C. Zhu, G. Li, W. Wang, and R. Wang, \u201cAn innovative salient object detection using center-dark channel prior,\u201d in CVPRW, 2017, pp. 1509\u20131515.", "[91] R. Cong, J. Lei, C. Zhang, Q. Huang, X. Cao, and C. Hou, \u201cSaliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion,\u201d IEEE SPL, vol. 23, no. 6, pp. 819\u2013823, 2016.", "[90] C. Zhu, G. Li, X. Guo, W. Wang, and R. Wang, \u201cA multilayer backpropagation saliency detection algorithm based on depth mining,\u201d in CAIP, 2017, pp. 14\u201323.", "[92] Y. Piao, Z. Rong, M. Zhang, W. Ren, and H. Lu, \u201cA2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection,\u201d in CVPR, 2020, pp. 9060\u20139069.", "[93] M. Zhang, W. Ren, Y. Piao, Z. Rong, and H. Lu, \u201cSelect, Supplement and Focus for RGB-D Saliency Detection,\u201d in CVPR, 2020, pp. 3472\u20133481.", "[19] Y. Piao, W. Ji, J. Li, M. Zhang, and H. Lu, \u201cDepth-induced multi-scale recurrent attention network for saliency detection,\u201d in ICCV, 2019, pp. 7254\u20137263.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Data</th><th rowspan=\"2\">Metric</th><td colspan=\"10\">Hand-crafted-features-Based Models</td><td colspan=\"8\">CNNs-Based Models</td><td colspan=\"2\">BBS-Net</td></tr><tr><td>LHM</td><td>CDB</td><td>DESM</td><td>GP</td><td>CDCP</td><td>ACSD</td><td>LBE</td><td>DCMC</td><td>MDSF</td><td>SE</td><td>DF</td><td>AFNet</td><td>CTMF</td><td>MMCI</td><td>PCF</td><td>TANet</td><td>CPFP</td><td>DMRA</td><td>Ours</td><td>Ours</td></tr><tr><th></th><th></th><td> [32]</td><td> [94]</td><td>[35]</td><td> [95]</td><td> [36]</td><td> [38]</td><td> [26]</td><td> [91]</td><td> [96]</td><td> [25]</td><td> [66]</td><td> [30]</td><td> [70]</td><td> [23]</td><td> [22]</td><td> [18]</td><td> [21]</td><td> [19]</td><td>\\divideontimes</td><td></td></tr><tr><th rowspan=\"4\">NJU2K</th><th>S_{\\alpha} \\uparrow</th><td>.514</td><td>.624</td><td>.665</td><td>.527</td><td>.669</td><td>.699</td><td>.695</td><td>.686</td><td>.748</td><td>.664</td><td>.763</td><td>.772</td><td>.849</td><td>.858</td><td>.877</td><td>.878</td><td>.879</td><td>.886</td><td>.916</td><td>.921</td></tr><tr><th>maxF_{\\beta} \\uparrow</th><td>.632</td><td>.648</td><td>.717</td><td>.647</td><td>.621</td><td>.711</td><td>.748</td><td>.715</td><td>.775</td><td>.748</td><td>.650</td><td>.775</td><td>.845</td><td>.852</td><td>.872</td><td>.874</td><td>.877</td><td>.886</td><td>.918</td><td>.920</td></tr><tr><th>maxE_{\\xi} \\uparrow</th><td>.724</td><td>.742</td><td>.791</td><td>.703</td><td>.741</td><td>.803</td><td>.803</td><td>.799</td><td>.838</td><td>.813</td><td>.696</td><td>.853</td><td>.913</td><td>.915</td><td>.924</td><td>.925</td><td>.926</td><td>.927</td><td>.948</td><td>.949</td></tr><tr><th>M \\downarrow</th><td>.205</td><td>.203</td><td>.283</td><td>.211</td><td>.180</td><td>.202</td><td>.153</td><td>.172</td><td>.157</td><td>.169</td><td>.141</td><td>.100</td><td>.085</td><td>.079</td><td>.059</td><td>.060</td><td>.053</td><td>.051</td><td>.038</td><td>.035</td></tr><tr><th rowspan=\"4\">NLPR</th><th>S_{\\alpha} \\uparrow</th><td>.630</td><td>.629</td><td>.572</td><td>.654</td><td>.727</td><td>.673</td><td>.762</td><td>.724</td><td>.805</td><td>.756</td><td>.802</td><td>.799</td><td>.860</td><td>.856</td><td>.874</td><td>.886</td><td>.888</td><td>.899</td><td>.925</td><td>.930</td></tr><tr><th>maxF_{\\beta} \\uparrow</th><td>.622</td><td>.618</td><td>.640</td><td>.611</td><td>.645</td><td>.607</td><td>.745</td><td>.648</td><td>.793</td><td>.713</td><td>.778</td><td>.771</td><td>.825</td><td>.815</td><td>.841</td><td>.863</td><td>.867</td><td>.879</td><td>.909</td><td>.918</td></tr><tr><th>maxE_{\\xi} \\uparrow</th><td>.766</td><td>.791</td><td>.805</td><td>.723</td><td>.820</td><td>.780</td><td>.855</td><td>.793</td><td>.885</td><td>.847</td><td>.880</td><td>.879</td><td>.929</td><td>.913</td><td>.925</td><td>.941</td><td>.932</td><td>.947</td><td>.959</td><td>.961</td></tr><tr><th>M \\downarrow</th><td>.108</td><td>.114</td><td>.312</td><td>.146</td><td>.112</td><td>.179</td><td>.081</td><td>.117</td><td>.095</td><td>.091</td><td>.085</td><td>.058</td><td>.056</td><td>.059</td><td>.044</td><td>.041</td><td>.036</td><td>.031</td><td>.026</td><td>.023</td></tr><tr><th rowspan=\"4\">STERE</th><th>S_{\\alpha} \\uparrow</th><td>.562</td><td>.615</td><td>.642</td><td>.588</td><td>.713</td><td>.692</td><td>.660</td><td>.731</td><td>.728</td><td>.708</td><td>.757</td><td>.825</td><td>.848</td><td>.873</td><td>.875</td><td>.871</td><td>.879</td><td>.835</td><td>.905</td><td>.908</td></tr><tr><th>maxF_{\\beta} \\uparrow</th><td>.683</td><td>.717</td><td>.700</td><td>.671</td><td>.664</td><td>.669</td><td>.633</td><td>.740</td><td>.719</td><td>.755</td><td>.757</td><td>.823</td><td>.831</td><td>.863</td><td>.860</td><td>.861</td><td>.874</td><td>.847</td><td>.898</td><td>.903</td></tr><tr><th>maxE_{\\xi} \\uparrow</th><td>.771</td><td>.823</td><td>.811</td><td>.743</td><td>.786</td><td>.806</td><td>.787</td><td>.819</td><td>.809</td><td>.846</td><td>.847</td><td>.887</td><td>.912</td><td>.927</td><td>.925</td><td>.923</td><td>.925</td><td>.911</td><td>.940</td><td>.942</td></tr><tr><th>M \\downarrow</th><td>.172</td><td>.166</td><td>.295</td><td>.182</td><td>.149</td><td>.200</td><td>.250</td><td>.148</td><td>.176</td><td>.143</td><td>.141</td><td>.075</td><td>.086</td><td>.068</td><td>.064</td><td>.060</td><td>.051</td><td>.066</td><td>.043</td><td>.041</td></tr><tr><th rowspan=\"4\">DES</th><th>S_{\\alpha} \\uparrow</th><td>.578</td><td>.645</td><td>.622</td><td>.636</td><td>.709</td><td>.728</td><td>.703</td><td>.707</td><td>.741</td><td>.741</td><td>.752</td><td>.770</td><td>.863</td><td>.848</td><td>.842</td><td>.858</td><td>.872</td><td>.900</td><td>.930</td><td>.933</td></tr><tr><th>maxF_{\\beta} \\uparrow</th><td>.511</td><td>.723</td><td>.765</td><td>.597</td><td>.631</td><td>.756</td><td>.788</td><td>.666</td><td>.746</td><td>.741</td><td>.766</td><td>.728</td><td>.844</td><td>.822</td><td>.804</td><td>.827</td><td>.846</td><td>.888</td><td>.921</td><td>.927</td></tr><tr><th>maxE_{\\xi} \\uparrow</th><td>.653</td><td>.830</td><td>.868</td><td>.670</td><td>.811</td><td>.850</td><td>.890</td><td>.773</td><td>.851</td><td>.856</td><td>.870</td><td>.881</td><td>.932</td><td>.928</td><td>.893</td><td>.910</td><td>.923</td><td>.943</td><td>.965</td><td>.966</td></tr><tr><th>M \\downarrow</th><td>.114</td><td>.100</td><td>.299</td><td>.168</td><td>.115</td><td>.169</td><td>.208</td><td>.111</td><td>.122</td><td>.090</td><td>.093</td><td>.068</td><td>.055</td><td>.065</td><td>.049</td><td>.046</td><td>.038</td><td>.030</td><td>.022</td><td>.021</td></tr><tr><th rowspan=\"4\">LFSD</th><th>S_{\\alpha} \\uparrow</th><td>.553</td><td>.515</td><td>.716</td><td>.635</td><td>.712</td><td>.727</td><td>.729</td><td>.753</td><td>.694</td><td>.692</td><td>.783</td><td>.738</td><td>.788</td><td>.787</td><td>.786</td><td>.801</td><td>.828</td><td>.839</td><td>.859</td><td>.864</td></tr><tr><th>maxF_{\\beta} \\uparrow</th><td>.708</td><td>.677</td><td>.762</td><td>.783</td><td>.702</td><td>.763</td><td>.722</td><td>.817</td><td>.779</td><td>.786</td><td>.813</td><td>.744</td><td>.787</td><td>.771</td><td>.775</td><td>.796</td><td>.826</td><td>.852</td><td>.855</td><td>.858</td></tr><tr><th>maxE_{\\xi} \\uparrow</th><td>.763</td><td>.871</td><td>.811</td><td>.824</td><td>.780</td><td>.829</td><td>.797</td><td>.856</td><td>.819</td><td>.832</td><td>.857</td><td>.815</td><td>.857</td><td>.839</td><td>.827</td><td>.847</td><td>.863</td><td>.893</td><td>.896</td><td>.901</td></tr><tr><th>M \\downarrow</th><td>.218</td><td>.225</td><td>.253</td><td>.190</td><td>.172</td><td>.195</td><td>.214</td><td>.155</td><td>.197</td><td>.174</td><td>.145</td><td>.133</td><td>.127</td><td>.132</td><td>.119</td><td>.111</td><td>.088</td><td>.083</td><td>.076</td><td>.072</td></tr><tr><th rowspan=\"4\">SSD</th><th>S_{\\alpha} \\uparrow</th><td>.566</td><td>.562</td><td>.602</td><td>.615</td><td>.603</td><td>.675</td><td>.621</td><td>.704</td><td>.673</td><td>.675</td><td>.747</td><td>.714</td><td>.776</td><td>.813</td><td>.841</td><td>.839</td><td>.807</td><td>.857</td><td>.858</td><td>.882</td></tr><tr><th>maxF_{\\beta} \\uparrow</th><td>.568</td><td>.592</td><td>.680</td><td>.740</td><td>.535</td><td>.682</td><td>.619</td><td>.711</td><td>.703</td><td>.710</td><td>.735</td><td>.687</td><td>.729</td><td>.781</td><td>.807</td><td>.810</td><td>.766</td><td>.844</td><td>.827</td><td>.859</td></tr><tr><th>maxE_{\\xi} \\uparrow</th><td>.717</td><td>.698</td><td>.769</td><td>.782</td><td>.700</td><td>.785</td><td>.736</td><td>.786</td><td>.779</td><td>.800</td><td>.828</td><td>.807</td><td>.865</td><td>.882</td><td>.894</td><td>.897</td><td>.852</td><td>.906</td><td>.894</td><td>.919</td></tr><tr><th>M \\downarrow</th><td>.195</td><td>.196</td><td>.308</td><td>.180</td><td>.214</td><td>.203</td><td>.278</td><td>.169</td><td>.192</td><td>.165</td><td>.142</td><td>.118</td><td>.099</td><td>.082</td><td>.062</td><td>.063</td><td>.082</td><td>.058</td><td>.058</td><td>.044</td></tr><tr><th rowspan=\"4\">SIP</th><th>S_{\\alpha} \\uparrow</th><td>.511</td><td>.557</td><td>.616</td><td>.588</td><td>.595</td><td>.732</td><td>.727</td><td>.683</td><td>.717</td><td>.628</td><td>.653</td><td>.720</td><td>.716</td><td>.833</td><td>.842</td><td>.835</td><td>.850</td><td>.806</td><td>.876</td><td>.879</td></tr><tr><th>maxF_{\\beta} \\uparrow</th><td>.574</td><td>.620</td><td>.669</td><td>.687</td><td>.505</td><td>.763</td><td>.751</td><td>.618</td><td>.698</td><td>.661</td><td>.657</td><td>.712</td><td>.694</td><td>.818</td><td>.838</td><td>.830</td><td>.851</td><td>.821</td><td>.880</td><td>.883</td></tr><tr><th>maxE_{\\xi} \\uparrow</th><td>.716</td><td>.737</td><td>.770</td><td>.768</td><td>.721</td><td>.838</td><td>.853</td><td>.743</td><td>.798</td><td>.771</td><td>.759</td><td>.819</td><td>.829</td><td>.897</td><td>.901</td><td>.895</td><td>.903</td><td>.875</td><td>.919</td><td>.922</td></tr><tr><th>M \\downarrow</th><td>.184</td><td>.192</td><td>.298</td><td>.173</td><td>.224</td><td>.172</td><td>.200</td><td>.186</td><td>.167</td><td>.164</td><td>.185</td><td>.118</td><td>.139</td><td>.086</td><td>.071</td><td>.075</td><td>.064</td><td>.085</td><td>.056</td><td>.055</td></tr></tbody></table>", "caption": "TABLE III:  Quantitative comparison of models using S-measure (S_{\\alpha}), max F-measure (maxF_{\\beta}), max E-measure (maxE_{\\xi}) and MAE (M) scores on seven public datasets.\\uparrow (\\downarrow) denotes that the higher (lower) the score, the better.\\divideontimes denotes the efficient version of BBS-Net.", "list_citation_info": ["[30] N. Wang and X. Gong, \u201cAdaptive fusion for RGB-D salient object detection,\u201d IEEE Access, vol. 7, pp. 55\u2009277\u201355\u2009284, 2019.", "[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[36] C. Zhu, G. Li, W. Wang, and R. Wang, \u201cAn innovative salient object detection using center-dark channel prior,\u201d in CVPRW, 2017, pp. 1509\u20131515.", "[70] J. Han, H. Chen, N. Liu, C. Yan, and X. Li, \u201cCNNs-Based RGB-D saliency detection via cross-view transfer and multiview fusion,\u201d IEEE TCYBERNETICS, vol. 48, no. 11, pp. 3171\u20133183, 2018.", "[95] J. Ren, X. Gong, L. Yu, W. Zhou, and M. Ying Yang, \u201cExploiting global priors for RGB-D saliency detection,\u201d in CVPRW, 2015, pp. 25\u201332.", "[91] R. Cong, J. Lei, C. Zhang, Q. Huang, X. Cao, and C. Hou, \u201cSaliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion,\u201d IEEE SPL, vol. 23, no. 6, pp. 819\u2013823, 2016.", "[23] H. Chen, Y. Li, and D. Su, \u201cMulti-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection,\u201d IEEE TCYBERNETICS, vol. 86, pp. 376\u2013385, 2019.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[96] H. Song, Z. Liu, H. Du, G. Sun, O. Le Meur, and T. Ren, \u201cDepth-aware salient object detection and segmentation via multiscale discriminative saliency fusion and bootstrap learning,\u201d IEEE TIP, vol. 26, no. 9, pp. 4204\u20134216, 2017.", "[26] D. Feng, N. Barnes, S. You, and C. McCarthy, \u201cLocal background enclosure for RGB-D salient object detection,\u201d in CVPR, 2016, pp. 2343\u20132350.", "[19] Y. Piao, W. Ji, J. Li, M. Zhang, and H. Lu, \u201cDepth-induced multi-scale recurrent attention network for saliency detection,\u201d in ICCV, 2019, pp. 7254\u20137263.", "[18] H. Chen and Y. Li, \u201cThree-stream attention-aware network for RGB-D salient object detection,\u201d IEEE TIP, vol. 28, no. 6, pp. 2825\u20132835, 2019.", "[94] F. Liang, L. Duan, W. Ma, Y. Qiao, Z. Cai, and L. Qing, \u201cStereoscopic saliency model using contrast and depth-guided-background prior,\u201d Neurocomputing, vol. 275, pp. 2227\u20132238, 2018.", "[25] J. Guo, T. Ren, and J. Bei, \u201cSalient object detection for RGB-D image via saliency evolution,\u201d in IEEE ICME, 2016, pp. 1\u20136.", "[21] J.-X. Zhao, Y. Cao, D.-P. Fan, M.-M. Cheng, X.-Y. Li, and L. Zhang, \u201cContrast prior and fluid pyramid integration for RGBD salient object detection,\u201d in CVPR, 2019, pp. 3927\u20133936.", "[22] H. Chen and Y. Li, \u201cProgressively complementarity-aware fusion network for RGB-D salient object detection,\u201d in CVPR, 2018, pp. 3051\u20133060.", "[66] L. Qu, S. He, J. Zhang, J. Tian, Y. Tang, and Q. Yang, \u201cRGBD salient object detection via deep fusion,\u201d IEEE TIP, vol. 26, no. 5, pp. 2274\u20132285, 2017.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Models</th><th colspan=\"2\">NJU2K [38]</th><th colspan=\"2\">NLPR [32]</th><th colspan=\"2\">STERE [39]</th><th colspan=\"2\">DES [35]</th><th colspan=\"2\">LFSD [88]</th><th colspan=\"2\">SSD [89]</th><th colspan=\"2\">SIP [37]</th></tr><tr><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th></tr></thead><tbody><tr><th>TANet (VGG-16) [18]</th><td>.878</td><td>.060</td><td>.886</td><td>.041</td><td>.871</td><td>.060</td><td>.858</td><td>.046</td><td>.801</td><td>.111</td><td>.839</td><td>.063</td><td>.835</td><td>.075</td></tr><tr><th>CPFP (VGG-16) [21]</th><td>.879</td><td>.053</td><td>.888</td><td>.036</td><td>.879</td><td>.051</td><td>.872</td><td>.038</td><td>.828</td><td>.088</td><td>.807</td><td>.082</td><td>.850</td><td>.064</td></tr><tr><th>Ours (VGG-16)</th><td>.916</td><td>.039</td><td>.923</td><td>.026</td><td>.896</td><td>.046</td><td>.908</td><td>.028</td><td>.845</td><td>.080</td><td>.858</td><td>.055</td><td>.874</td><td>.056</td></tr><tr><th>DMRA (VGG-19) [19]</th><td>.886</td><td>.051</td><td>.899</td><td>.031</td><td>.835</td><td>.066</td><td>.900</td><td>.030</td><td>.839</td><td>.083</td><td>.857</td><td>.058</td><td>.806</td><td>.085</td></tr><tr><th>Ours (VGG-19)</th><td>.918</td><td>.037</td><td>.925</td><td>.025</td><td>.901</td><td>.043</td><td>.915</td><td>.026</td><td>.852</td><td>.074</td><td>.855</td><td>.056</td><td>.878</td><td>.054</td></tr><tr><th>D3Net (ResNet-50) [37]</th><td>.900</td><td>.041</td><td>.912</td><td>.030</td><td>.899</td><td>.046</td><td>.898</td><td>.031</td><td>.825</td><td>.095</td><td>.857</td><td>.058</td><td>.860</td><td>.063</td></tr><tr><th>Ours (ResNet-50)</th><td>.921</td><td>.035</td><td>.930</td><td>.023</td><td>.908</td><td>.041</td><td>.933</td><td>.021</td><td>.864</td><td>.072</td><td>.882</td><td>.044</td><td>.879</td><td>.055</td></tr></tbody></table>", "caption": "TABLE IV:  Performance comparison using different backbone models. We experiment with multiple popular backbone models used in RGB-D SOD, including VGG-16 [100], VGG-19 [100] and ResNet-50 [69].", "list_citation_info": ["[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, \u201cSaliency detection on light field,\u201d in CVPR, 2014, pp. 2806\u20132813.", "[39] Y. Niu, Y. Geng, X. Li, and F. Liu, \u201cLeveraging stereopsis for saliency analysis,\u201d in CVPR, 2012, pp. 454\u2013461.", "[19] Y. Piao, W. Ji, J. Li, M. Zhang, and H. Lu, \u201cDepth-induced multi-scale recurrent attention network for saliency detection,\u201d in ICCV, 2019, pp. 7254\u20137263.", "[69] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in CVPR, 2016, pp. 770\u2013778.", "[100] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.", "[89] C. Zhu and G. Li, \u201cA three-pathway psychobiological framework of salient object detection using stereoscopic technology,\u201d in CVPRW, 2017, pp. 3008\u20133014.", "[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, \u201cRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,\u201d IEEE TNNLS, pp. 2075\u20132089, 2020.", "[18] H. Chen and Y. Li, \u201cThree-stream attention-aware network for RGB-D salient object detection,\u201d IEEE TIP, vol. 28, no. 6, pp. 2825\u20132835, 2019.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[21] J.-X. Zhao, Y. Cao, D.-P. Fan, M.-M. Cheng, X.-Y. Li, and L. Zhang, \u201cContrast prior and fluid pyramid integration for RGBD salient object detection,\u201d in CVPR, 2019, pp. 3927\u20133936.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><thead><tr><th rowspan=\"2\">#</th><th rowspan=\"2\">Settings</th><th colspan=\"2\">NJU2K [38]</th><th colspan=\"2\">NLPR [32]</th><th colspan=\"2\">STERE [39]</th><th colspan=\"2\">DES [35]</th><th colspan=\"2\">LFSD [88]</th><th colspan=\"2\">SSD [89]</th><th colspan=\"2\">SIP [37]</th></tr><tr><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th></tr></thead><tbody><tr><th>1</th><th>Low 3 levels</th><td>.881</td><td>.051</td><td>.882</td><td>.038</td><td>.832</td><td>.070</td><td>.853</td><td>.044</td><td>.779</td><td>.110</td><td>.805</td><td>.080</td><td>.760</td><td>.108</td></tr><tr><th>2</th><th>High 3 levels</th><td>.902</td><td>.042</td><td>.911</td><td>.029</td><td>.886</td><td>.048</td><td>.912</td><td>.026</td><td>.845</td><td>.080</td><td>.850</td><td>.058</td><td>.833</td><td>.073</td></tr><tr><th>3</th><th>All 5 levels</th><td>.905</td><td>.042</td><td>.915</td><td>.027</td><td>.891</td><td>.045</td><td>.901</td><td>.028</td><td>.845</td><td>.082</td><td>.848</td><td>.060</td><td>.839</td><td>.071</td></tr><tr><th>4</th><th>BBS-NoRF</th><td>.893</td><td>.050</td><td>.904</td><td>.035</td><td>.843</td><td>.072</td><td>.886</td><td>.039</td><td>.804</td><td>.105</td><td>.839</td><td>.069</td><td>.843</td><td>.076</td></tr><tr><th>5</th><th>BBS-RH</th><td>.913</td><td>.040</td><td>.922</td><td>.028</td><td>.881</td><td>.054</td><td>.919</td><td>.027</td><td>.833</td><td>.085</td><td>.872</td><td>.053</td><td>.866</td><td>.063</td></tr><tr><th>6</th><th>BBS-RL (ours)</th><td>.921</td><td>.035</td><td>.930</td><td>.023</td><td>.908</td><td>.041</td><td>.933</td><td>.021</td><td>.864</td><td>.072</td><td>.882</td><td>.044</td><td>.879</td><td>.055</td></tr></tbody></table>", "caption": "TABLE V: Comparison of feature aggregation strategies.1: Only aggregating the low-level features (Conv1\\sim3), 2: Only aggregating the high-level features (Conv3\\sim5), 3: Directly integrating all five-level features (Conv1\\sim5) by a single decoder, 4: Our model without the refinement flow, 5: High-level features (Conv3\\sim5) are first refined by the initial map aggregated by low-level features (Conv1\\sim3) and are then integrated to generate the final saliency map, and 6: Our cascaded refinement mechanism.", "list_citation_info": ["[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, \u201cSaliency detection on light field,\u201d in CVPR, 2014, pp. 2806\u20132813.", "[39] Y. Niu, Y. Geng, X. Li, and F. Liu, \u201cLeveraging stereopsis for saliency analysis,\u201d in CVPR, 2012, pp. 454\u2013461.", "[89] C. Zhu and G. Li, \u201cA three-pathway psychobiological framework of salient object detection using stereoscopic technology,\u201d in CVPRW, 2017, pp. 3008\u20133014.", "[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, \u201cRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,\u201d IEEE TNNLS, pp. 2075\u20132089, 2020.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><thead><tr><th rowspan=\"2\">#</th><th colspan=\"4\">Settings</th><th colspan=\"2\">NJU2K [38]</th><th colspan=\"2\">NLPR [32]</th><th colspan=\"2\">STERE [39]</th><th colspan=\"2\">DES [35]</th><th colspan=\"2\">LFSD [88]</th><th colspan=\"2\">SSD [89]</th><th colspan=\"2\">SIP [37]</th></tr><tr><th>BM</th><th>CA</th><th>SA</th><th>PTM</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th><th>S_{\\alpha}\\uparrow</th><th>M\\downarrow</th></tr></thead><tbody><tr><td>1</td><td>\u2713</td><td></td><td></td><td></td><td>.908</td><td>.045</td><td>.918</td><td>.029</td><td>.882</td><td>.055</td><td>.917</td><td>.027</td><td>.842</td><td>.083</td><td>.862</td><td>.057</td><td>.864</td><td>.066</td></tr><tr><td>2</td><td>\u2713</td><td>\u2713</td><td></td><td></td><td>.913</td><td>.042</td><td>.922</td><td>.027</td><td>.896</td><td>.048</td><td>.923</td><td>.025</td><td>.840</td><td>.086</td><td>.855</td><td>.057</td><td>.868</td><td>.063</td></tr><tr><td>3</td><td>\u2713</td><td></td><td>\u2713</td><td></td><td>.912</td><td>.045</td><td>.918</td><td>.029</td><td>.891</td><td>.054</td><td>.914</td><td>.029</td><td>.855</td><td>.083</td><td>.872</td><td>.054</td><td>.869</td><td>.063</td></tr><tr><td>4</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td></td><td>.919</td><td>.037</td><td>.928</td><td>.026</td><td>.900</td><td>.045</td><td>.924</td><td>.024</td><td>.861</td><td>.074</td><td>.873</td><td>.052</td><td>.869</td><td>.061</td></tr><tr><td>5</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>.921</td><td>.035</td><td>.930</td><td>.023</td><td>.908</td><td>.041</td><td>.933</td><td>.021</td><td>.864</td><td>.072</td><td>.882</td><td>.044</td><td>.879</td><td>.055</td></tr></tbody></table>", "caption": "TABLE VI:  Ablation analysis of our BBS-Net. \u2018BM\u2019 = base model. \u2018CA\u2019 = channel attentio. \u2018SA\u2019 = spatial attention. \u2018PTM\u2019 = progressively transposed module.", "list_citation_info": ["[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, \u201cSaliency detection on light field,\u201d in CVPR, 2014, pp. 2806\u20132813.", "[39] Y. Niu, Y. Geng, X. Li, and F. Liu, \u201cLeveraging stereopsis for saliency analysis,\u201d in CVPR, 2012, pp. 454\u2013461.", "[89] C. Zhu and G. Li, \u201cA three-pathway psychobiological framework of salient object detection using stereoscopic technology,\u201d in CVPRW, 2017, pp. 3008\u20133014.", "[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, \u201cRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,\u201d IEEE TNNLS, pp. 2075\u20132089, 2020.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Methods</th><th>NJU2K</th><th>NLPR</th><th>STERE</th><th>DES</th><th>SSD</th><th>LFSD</th><th>SIP</th></tr><tr><th> [38]</th><th> [32]</th><th> [39]</th><th> [35]</th><th> [89]</th><th> [88]</th><th> [37]</th></tr></thead><tbody><tr><th>Element-wise sum</th><td>.915</td><td>.925</td><td>.897</td><td>.925</td><td>.868</td><td>.856</td><td>.880</td></tr><tr><th>Cascaded decoder</th><td>.921</td><td>.930</td><td>.908</td><td>.933</td><td>.882</td><td>.864</td><td>.879</td></tr></tbody></table>", "caption": "TABLE VII:  Effectiveness analysis of the cascaded decoder in terms of the S-measure (S_{\\alpha}) on seven datasets.", "list_citation_info": ["[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, \u201cSaliency detection on light field,\u201d in CVPR, 2014, pp. 2806\u20132813.", "[39] Y. Niu, Y. Geng, X. Li, and F. Liu, \u201cLeveraging stereopsis for saliency analysis,\u201d in CVPR, 2012, pp. 454\u2013461.", "[89] C. Zhu and G. Li, \u201cA three-pathway psychobiological framework of salient object detection using stereoscopic technology,\u201d in CVPRW, 2017, pp. 3008\u20133014.", "[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, \u201cRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,\u201d IEEE TNNLS, pp. 2075\u20132089, 2020.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Settings</th><th>NJU2K</th><th>NLPR</th><th>STERE</th><th>DES</th><th>SSD</th><th>LFSD</th><th>SIP</th></tr><tr><th> [38]</th><th> [32]</th><th> [39]</th><th> [35]</th><th> [89]</th><th> [88]</th><th> [37]</th></tr></thead><tbody><tr><th>BBS-Net{}^{\\divideontimes} (w/o DAM)</th><td>.905</td><td>.922</td><td>.899</td><td>.928</td><td>.856</td><td>.841</td><td>.849</td></tr><tr><th>BBS-Net{}^{\\divideontimes} (w/ DAM)</th><td>.916</td><td>.925</td><td>.905</td><td>.930</td><td>.858</td><td>.859</td><td>.876</td></tr></tbody></table>", "caption": "TABLE IX: Effectiveness analysis of the depth adapter module in terms of the S-measure (S_{\\alpha}) on seven datasets. \\divideontimes represents the efficient version of BBS-Net, where the two backbones share parameters.", "list_citation_info": ["[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, \u201cSaliency detection on light field,\u201d in CVPR, 2014, pp. 2806\u20132813.", "[39] Y. Niu, Y. Geng, X. Li, and F. Liu, \u201cLeveraging stereopsis for saliency analysis,\u201d in CVPR, 2012, pp. 454\u2013461.", "[89] C. Zhu and G. Li, \u201cA three-pathway psychobiological framework of salient object detection using stereoscopic technology,\u201d in CVPRW, 2017, pp. 3008\u20133014.", "[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, \u201cRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,\u201d IEEE TNNLS, pp. 2075\u20132089, 2020.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td>NJU2K</td><td>NLPR</td><td>STERE</td><td>DES</td><td>LFSD</td><td>SSD</td><td>SIP</td></tr><tr><td> [38]</td><td> [32]</td><td> [39]</td><td> [35]</td><td> [88]</td><td> [89]</td><td> [37]</td></tr><tr><th>PiCANet [101]</th><td>.847</td><td>.834</td><td>.868</td><td>.854</td><td>.761</td><td>.832</td><td>-</td></tr><tr><th>PAGRN [50]</th><td>.829</td><td>.844</td><td>.851</td><td>.858</td><td>.779</td><td>.793</td><td>-</td></tr><tr><th>R3Net [82]</th><td>.837</td><td>.798</td><td>.855</td><td>.847</td><td>.797</td><td>.815</td><td>-</td></tr><tr><th>CPD [29]</th><td>.894</td><td>.915</td><td>.902</td><td>.897</td><td>.815</td><td>.839</td><td>.859</td></tr><tr><th>PoolNet [16]</th><td>.887</td><td>.900</td><td>.880</td><td>.873</td><td>.787</td><td>.773</td><td>.861</td></tr><tr><th>BBS-Net (w/o depth)</th><td>.914</td><td>.925</td><td>.915</td><td>.912</td><td>.836</td><td>.855</td><td>.875</td></tr><tr><th>BBS-Net (w/ depth)</th><td>.921</td><td>.930</td><td>.908</td><td>.933</td><td>.864</td><td>.882</td><td>.879</td></tr></tbody></table>", "caption": "TABLE X:  S-measure (S_{\\alpha}) comparison with SOTA RGB SOD methods. \u2018w/o depth\u2019 and \u2018w/ depth\u2019 represent training and testing the proposed method without/with the depth information (i.e., the inputs of the depth branch are or are not set to zeros).", "list_citation_info": ["[29] Z. Wu, L. Su, and Q. Huang, \u201cCascaded partial decoder for fast and accurate salient object detection,\u201d in CVPR, 2019, pp. 3907\u20133916.", "[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, \u201cSaliency detection on light field,\u201d in CVPR, 2014, pp. 2806\u20132813.", "[82] Z. Deng, X. Hu, L. Zhu, X. Xu, J. Qin, G. Han, and P.-A. Heng, \u201cR3Net: Recurrent residual refinement network for saliency detection,\u201d in IJCAI, 2018, pp. 684\u2013690.", "[39] Y. Niu, Y. Geng, X. Li, and F. Liu, \u201cLeveraging stereopsis for saliency analysis,\u201d in CVPR, 2012, pp. 454\u2013461.", "[50] X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang, \u201cProgressive attention guided recurrent network for salient object detection,\u201d in CVPR, 2018, pp. 714\u2013722.", "[101] N. Liu, J. Han, and M.-H. Yang, \u201cPiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection,\u201d in CVPR, 2018, pp. 3089\u20133098.", "[89] C. Zhu and G. Li, \u201cA three-pathway psychobiological framework of salient object detection using stereoscopic technology,\u201d in CVPRW, 2017, pp. 3008\u20133014.", "[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, \u201cRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,\u201d IEEE TNNLS, pp. 2075\u20132089, 2020.", "[16] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang, \u201cA simple pooling-based design for real-time salient object detection,\u201d in CVPR, 2019, pp. 3917\u20133926.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Strategy</th><th>NJU2K</th><th>NLPR</th><th>STERE</th><th>DES</th><th>LFSD</th><th>SSD</th><th>SIP</th><th>time</th></tr><tr><th> [38]</th><th> [32]</th><th> [39]</th><th> [35]</th><th> [88]</th><th> [89]</th><th> [37]</th><th>ms</th></tr></thead><tbody><tr><th>BBS-Net</th><td>.035</td><td>.023</td><td>.041</td><td>.021</td><td>.072</td><td>.044</td><td>.055</td><td>-</td></tr><tr><th>BBS-Net+ADP</th><td>.050</td><td>.024</td><td>.049</td><td>.018</td><td>.072</td><td>.053</td><td>.055</td><td>1.46</td></tr><tr><th>BBS-Net+Ostu</th><td>.030</td><td>.020</td><td>.036</td><td>.018</td><td>.066</td><td>.039</td><td>.051</td><td>0.99</td></tr><tr><th>BBS-Net+CRF</th><td>.030</td><td>.020</td><td>.035</td><td>.019</td><td>.065</td><td>.038</td><td>.051</td><td>450.8</td></tr></tbody></table>", "caption": "TABLE XI: Performance comparison (MAE) of different post-processing strategies on seven datasets. The last column is the time for the post-processing methods to optimize each image. See \\lx@sectionsign~{}V-B for details.", "list_citation_info": ["[35] Y. Cheng, H. Fu, X. Wei, J. Xiao, and X. Cao, \u201cDepth enhanced saliency detection method,\u201d in ICIMCS, 2014, pp. 23\u201327.", "[88] N. Li, J. Ye, Y. Ji, H. Ling, and J. Yu, \u201cSaliency detection on light field,\u201d in CVPR, 2014, pp. 2806\u20132813.", "[39] Y. Niu, Y. Geng, X. Li, and F. Liu, \u201cLeveraging stereopsis for saliency analysis,\u201d in CVPR, 2012, pp. 454\u2013461.", "[89] C. Zhu and G. Li, \u201cA three-pathway psychobiological framework of salient object detection using stereoscopic technology,\u201d in CVPRW, 2017, pp. 3008\u20133014.", "[37] D.-P. Fan, Z. Lin, Z. Zhang, M. Zhu, and M.-M. Cheng, \u201cRethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks,\u201d IEEE TNNLS, pp. 2075\u20132089, 2020.", "[38] R. Ju, L. Ge, W. Geng, T. Ren, and G. Wu, \u201cDepth saliency based on anisotropic center-surround difference,\u201d in ICIP, 2014, pp. 1115\u20131119.", "[32] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, \u201cRGBD salient object detection: a benchmark and algorithms,\u201d in ECCV, 2014, pp. 92\u2013109."]}]}