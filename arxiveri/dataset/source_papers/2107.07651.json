{"title": "Align before fuse: Vision and language representation learning with momentum distillation", "abstract": "Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at https://github.com/salesforce/ALBEF/.", "authors": ["Junnan Li", " Ramprasaath R. Selvaraju", " Akhilesh Deepak Gotmare", " Shafiq Joty", " Caiming Xiong", " Steven Hoi"], "pdf_url": "https://arxiv.org/abs/2107.07651", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td># Pre-train</td><td colspan=\"6\">Flickr30K (1K test set)</td></tr><tr><td>  Images</td><td colspan=\"3\">TR</td><td colspan=\"3\">IR</td></tr><tr><td></td><td></td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><td>UNITER [2]</td><td>4M</td><td>83.6</td><td>95.7</td><td>97.7</td><td>68.7</td><td>89.2</td><td>93.9</td></tr><tr><td>CLIP [6]</td><td>400M</td><td>88.0</td><td>98.7</td><td>99.4</td><td>68.7</td><td>90.6</td><td>95.2</td></tr><tr><td>ALIGN [7]</td><td>1.2B</td><td>88.6</td><td>98.7</td><td>99.7</td><td>75.7</td><td>93.8</td><td>96.8</td></tr><tr><td>ALBEF</td><td>4M</td><td>90.5</td><td>98.8</td><td>99.7</td><td>76.8</td><td>93.7</td><td>96.7</td></tr><tr><td>ALBEF</td><td>14M</td><td>94.1</td><td>99.5</td><td>99.7</td><td>82.8</td><td>96.3</td><td>98.1</td></tr></table>", "caption": "Table 3: Zero-shot image-text retrieval results on Flickr30K.", "list_citation_info": ["[7] Jia, C., Y. Yang, Y. Xia, et al. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.", "[2] Chen, Y., L. Li, L. Yu, et al. UNITER: universal image-text representation learning. In ECCV, vol. 12375, pages 104\u2013120. 2020.", "[6] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">VQA</td><td colspan=\"2\">NLVR{}^{2}</td><td colspan=\"2\">SNLI-VE</td></tr><tr><td>test-dev</td><td>test-std</td><td>dev</td><td>test-P</td><td>val</td><td>test</td></tr><tr><td>VisualBERT [13]</td><td>70.80</td><td>71.00</td><td>67.40</td><td>67.00</td><td>-</td><td>-</td></tr><tr><td>VL-BERT [10]</td><td>71.16</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LXMERT [1]</td><td>72.42</td><td>72.54</td><td>74.90</td><td>74.50</td><td>-</td><td>-</td></tr><tr><td>12-in-1 [12]</td><td>73.15</td><td>-</td><td>-</td><td>78.87</td><td>-</td><td>76.95</td></tr><tr><td>UNITER [2]</td><td>72.70</td><td>72.91</td><td>77.18</td><td>77.85</td><td>78.59</td><td>78.28</td></tr><tr><td>VL-BART/T5 [54]</td><td>-</td><td>71.3</td><td>-</td><td>73.6</td><td>-</td><td>-</td></tr><tr><td>ViLT [21]</td><td>70.94</td><td>-</td><td>75.24</td><td>76.21</td><td>-</td><td>-</td></tr><tr><td>OSCAR [3]</td><td>73.16</td><td>73.44</td><td>78.07</td><td>78.36</td><td>-</td><td>-</td></tr><tr><td>VILLA [8]</td><td>73.59</td><td>73.67</td><td>78.39</td><td>79.30</td><td>79.47</td><td>79.03</td></tr><tr><td>ALBEF (4M)</td><td>74.54</td><td>74.70</td><td>80.24</td><td>80.50</td><td>80.14</td><td>80.30</td></tr><tr><td>ALBEF (14M)</td><td>75.84</td><td>76.04</td><td>82.55</td><td>83.14</td><td>80.80</td><td>80.91</td></tr></table>", "caption": "Table 4: Comparison with state-of-the-art methods on downstream vision-language tasks.", "list_citation_info": ["[10] Su, W., X. Zhu, Y. Cao, et al. Vl-bert: Pre-training of generic visual-linguistic representations. In ICLR. 2020.", "[12] Lu, J., V. Goswami, M. Rohrbach, et al. 12-in-1: Multi-task vision and language representation learning. In CVPR, pages 10434\u201310443. 2020.", "[3] Li, X., X. Yin, C. Li, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, pages 121\u2013137. 2020.", "[2] Chen, Y., L. Li, L. Yu, et al. UNITER: universal image-text representation learning. In ECCV, vol. 12375, pages 104\u2013120. 2020.", "[8] Gan, Z., Y. Chen, L. Li, et al. Large-scale adversarial training for vision-and-language representation learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin, eds., NeurIPS. 2020.", "[1] Tan, H., M. Bansal. LXMERT: learning cross-modality encoder representations from transformers. In K. Inui, J. Jiang, V. Ng, X. Wan, eds., EMNLP, pages 5099\u20135110. 2019.", "[54] Cho, J., J. Lei, H. Tan, et al. Unifying vision-and-language tasks via text generation. arXiv preprint arXiv:2102.02779, 2021.", "[13] Li, L. H., M. Yatskar, D. Yin, et al. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, abs/1908.03557, 2019.", "[21] Kim, W., B. Son, I. Kim. Vilt: Vision-and-language transformer without convolution or region supervision. arXiv preprint arXiv:2102.03334, 2021."]}, {"table": "MethodValTestATestBARN [57]32.7834.3532.13CCL [58]34.2936.9133.56ALBEF{}_{\\mathrm{itc}}51.5860.0940.19ALBEF{}_{\\mathrm{itm}}58.4665.8946.25<img/>Figure 4: Grad-CAM visualization on the cross-attention maps in the 3rd layer of the multimodal encoder.", "caption": "Table 5: Weakly-supervised visual grounding on RefCOCO+ [56] dataset.", "list_citation_info": ["[57] Liu, X., L. Li, S. Wang, et al. Adaptive reconstruction network for weakly supervised referring expression grounding. In ICCV, pages 2611\u20132620. 2019.", "[56] Yu, L., P. Poirson, S. Yang, et al. Modeling context in referring expressions. In B. Leibe, J. Matas, N. Sebe, M. Welling, eds., ECCV, pages 69\u201385. 2016.", "[58] Zhang, Z., Z. Zhao, Z. Lin, et al. Counterfactual contrastive learning fo weakly-supervised vision-language grounding. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin, eds., NeurIPS. 2020."]}]}