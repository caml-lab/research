{"title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers", "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (ie, without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.", "authors": ["Sixiao Zheng", " Jiachen Lu", " Hengshuang Zhao", " Xiatian Zhu", " Zekun Luo", " Yabiao Wang", " Yanwei Fu", " Jianfeng Feng", " Tao Xiang", " Philip H. S. Torr", " Li Zhang"], "pdf_url": "https://arxiv.org/abs/2012.15840", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Pre</td><td>Backbone</td><td>#Params</td><td>40k</td><td>80k</td></tr><tr><td>FCN [39]</td><td>1K</td><td>R-101</td><td>68.59M</td><td>73.93</td><td>75.52</td></tr><tr><td>Semantic FPN [39]</td><td>1K</td><td>R-101</td><td>47.51M</td><td>-</td><td>75.80</td></tr><tr><td>Hybrid-Base</td><td>R</td><td>T-Base</td><td>112.59M</td><td>74.48</td><td>77.36</td></tr><tr><td>Hybrid-Base</td><td>21K</td><td>T-Base</td><td>112.59M</td><td>76.76</td><td>76.57</td></tr><tr><td>Hybrid-DeiT</td><td>21K</td><td>T-Base</td><td>112.59M</td><td>77.42</td><td>78.28</td></tr><tr><td>SETR-Na\u00efve</td><td>21K</td><td>T-Large</td><td>305.67M</td><td>77.37</td><td>77.90</td></tr><tr><td>SETR-MLA</td><td>21K</td><td>T-Large</td><td>310.57M</td><td>76.65</td><td>77.24</td></tr><tr><td>SETR-PUP</td><td>21K</td><td>T-Large</td><td>318.31M</td><td>78.39</td><td>79.34</td></tr><tr><td>SETR-PUP</td><td>R</td><td>T-Large</td><td>318.31M</td><td>42.27</td><td>-</td></tr><tr><td>SETR-Na\u00efve-Base</td><td>21K</td><td>T-Base</td><td>87.69M</td><td>75.54</td><td>76.25</td></tr><tr><td>SETR-MLA-Base</td><td>21K</td><td>T-Base</td><td>92.59M</td><td>75.60</td><td>76.87</td></tr><tr><td>SETR-PUP-Base</td><td>21K</td><td>T-Base</td><td>97.64M</td><td>76.71</td><td>78.02</td></tr><tr><td>SETR-Na\u00efve-DeiT</td><td>1K</td><td>T-Base</td><td>87.69M</td><td>77.85</td><td>78.66</td></tr><tr><td>SETR-MLA-DeiT</td><td>1K</td><td>T-Base</td><td>92.59M</td><td>78.04</td><td>78.98</td></tr><tr><td>SETR-PUP-DeiT</td><td>1K</td><td>T-Base</td><td>97.64M</td><td>78.79</td><td>79.45</td></tr></tbody></table>", "caption": "Table 2: Comparing SETR variants on different pre-training strategies and backbones.All experiments are trained on Cityscapes train fine set with batch size 8, and evaluated using the single scale test protocol on the Cityscapes validation set in mean IoU (%) rate.\u201cPre\u201d denotes the pre-training of transformer part.\u201cR\u201d means the transformer part is randomly initialized.", "list_citation_info": ["[39] OpenMMLab. mmsegmentation. https://github.com/open-mmlab/mmsegmentation, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Pre</th><th>Backbone</th><th>ADE20K</th><th>Cityscapes</th></tr></thead><tbody><tr><th>FCN  [39]</th><td>1K</td><td>R-101</td><td>39.91</td><td>73.93</td></tr><tr><th>FCN</th><td>21K</td><td>R-101</td><td>42.17</td><td>76.38</td></tr><tr><th>SETR-MLA</th><td>21K</td><td>T-Large</td><td>48.64</td><td>76.65</td></tr><tr><th>SETR-PUP</th><td>21K</td><td>T-Large</td><td>48.58</td><td>78.39</td></tr><tr><th>SETR-MLA-DeiT</th><td>1K</td><td>T-Large</td><td>46.15</td><td>78.98</td></tr><tr><th>SETR-PUP-DeiT</th><td>1K</td><td>T-Large</td><td>46.24</td><td>79.45</td></tr></tbody></table>", "caption": "Table 3: Comparison to FCN with different pre-trainingwith single-scale inference on the ADE20K val and Cityscapes val set.", "list_citation_info": ["[39] OpenMMLab. mmsegmentation. https://github.com/open-mmlab/mmsegmentation, 2020."]}, {"table": "<table><thead><tr><th><p>Method</p></th><th><p>Pre</p></th><th><p>Backbone</p></th><th><p>#Params</p></th><th><p>mIoU</p></th></tr></thead><tbody><tr><td>FCN (160k, SS) [39]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>68.59M</p></td><td><p>39.91</p></td></tr><tr><td>FCN (160k, MS) [39]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>68.59M</p></td><td><p>41.40</p></td></tr><tr><td>CCNet [25]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>-</p></td><td><p>45.22</p></td></tr><tr><td>Strip pooling [23]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>-</p></td><td><p>45.60</p></td></tr><tr><td>DANet [18]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>69.0M</p></td><td><p>45.30</p></td></tr><tr><td>OCRNet [54]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>71.0M</p></td><td><p>45.70</p></td></tr><tr><td>UperNet [49]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>86.0M</p></td><td><p>44.90</p></td></tr><tr><td>Deeplab V3+ [11]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>63.0M</p></td><td><p>46.40</p></td></tr><tr><td>SETR-Na\u00efve (160k, SS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>305.67M</p></td><td><p>48.06</p></td></tr><tr><td>SETR-Na\u00efve (160k, MS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>305.67M</p></td><td><p>48.80</p></td></tr><tr><td>SETR-PUP (160k, SS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>318.31M</p></td><td><p>48.58</p></td></tr><tr><td>SETR-PUP (160k, MS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>318.31M</p></td><td><p>50.09</p></td></tr><tr><td>SETR-MLA (160k, SS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>310.57M</p></td><td><p>48.64</p></td></tr><tr><td>SETR-MLA (160k, MS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>310.57M</p></td><td>50.28</td></tr><tr><td>SETR-PUP-DeiT (160k, SS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>97.64M</p></td><td><p>46.34</p></td></tr><tr><td>SETR-PUP-DeiT (160k, MS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>97.64M</p></td><td><p>47.30</p></td></tr><tr><td>SETR-MLA-DeiT (160k, SS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>92.59M</p></td><td><p>46.15</p></td></tr><tr><td>SETR-MLA-DeiT (160k, MS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>92.59M</p></td><td><p>47.71</p></td></tr></tbody></table>", "caption": "Table 4: State-of-the-art comparison on the ADE20K dataset.Performances of different model variantsare reported.SS: Single-scale inference. MS: Multi-scale inference.", "list_citation_info": ["[54] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In ECCV, 2020.", "[18] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In CVPR, 2019.", "[39] OpenMMLab. mmsegmentation. https://github.com/open-mmlab/mmsegmentation, 2020.", "[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.", "[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pooling: Rethinking spatial pooling for scene parsing. In CVPR, 2020.", "[49] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018.", "[25] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2019."]}, {"table": "<table><tbody><tr><td><p>Method</p></td><td><p>Pre</p></td><td><p>Backbone</p></td><td><p>mIoU</p></td></tr><tr><td>FCN (80k, SS) [39]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>44.47</p></td></tr><tr><td>FCN (80k, MS) [39]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>45.74</p></td></tr><tr><td>DANet [18]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>52.60</p></td></tr><tr><td>EMANet [31]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>53.10</p></td></tr><tr><td>SVCNet [16]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>53.20</p></td></tr><tr><td>Strip pooling [23]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>54.50</p></td></tr><tr><td>GFFNet [30]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>54.20</p></td></tr><tr><td>APCNet [19]</td><td><p>1K</p></td><td><p>ResNet-101</p></td><td><p>54.70</p></td></tr><tr><td>SETR-Na\u00efve (80k, SS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>52.89</p></td></tr><tr><td>SETR-Na\u00efve (80k, MS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>53.61</p></td></tr><tr><td>SETR-PUP (80k, SS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>54.40</p></td></tr><tr><td>SETR-PUP (80k, MS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>55.27</p></td></tr><tr><td>SETR-MLA (80k, SS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td><p>54.87</p></td></tr><tr><td>SETR-MLA (80k, MS)</td><td><p>21K</p></td><td><p>T-Large</p></td><td>55.83</td></tr><tr><td>SETR-PUP-DeiT (80k, SS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>52.71</p></td></tr><tr><td>SETR-PUP-DeiT (80k, MS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>53.71</p></td></tr><tr><td>SETR-MLA-DeiT (80k, SS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>52.91</p></td></tr><tr><td>SETR-MLA-DeiT (80k, MS)</td><td><p>1K</p></td><td><p>T-Base</p></td><td><p>53.74</p></td></tr></tbody></table>", "caption": "Table 5: State-of-the-art comparison on the Pascal Context dataset.Performances of different model variantsare reported.SS: Single-scale inference. MS: Multi-scale inference.", "list_citation_info": ["[18] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In CVPR, 2019.", "[39] OpenMMLab. mmsegmentation. https://github.com/open-mmlab/mmsegmentation, 2020.", "[16] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Semantic correlation promoted shape-variant context for segmentation. In CVPR, 2019.", "[31] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-maximization attention networks for semantic segmentation. In CVPR, 2019.", "[30] Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, and Kuiyuan Yang. Gff: Gated fully fusion for semantic segmentation. In AAAI, 2020.", "[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pooling: Rethinking spatial pooling for scene parsing. In CVPR, 2020.", "[19] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for semantic segmentation. In CVPR, 2019."]}, {"table": "<table><tbody><tr><td><p>Method</p></td><td><p>Backbone</p></td><td><p>mIoU</p></td></tr><tr><td>FCN (40k, SS) [39]</td><td><p>ResNet-101</p></td><td><p>73.93</p></td></tr><tr><td>FCN (40k, MS) [39]</td><td><p>ResNet-101</p></td><td><p>75.14</p></td></tr><tr><td>FCN (80k, SS) [39]</td><td><p>ResNet-101</p></td><td><p>75.52</p></td></tr><tr><td>FCN (80k, MS) [39]</td><td><p>ResNet-101</p></td><td><p>76.61</p></td></tr><tr><td>PSPNet [60]</td><td><p>ResNet-101</p></td><td><p>78.50</p></td></tr><tr><td>DeepLab-v3 [10] (MS)</td><td><p>ResNet-101</p></td><td><p>79.30</p></td></tr><tr><td>NonLocal [48]</td><td><p>ResNet-101</p></td><td><p>79.10</p></td></tr><tr><td>CCNet [25]</td><td><p>ResNet-101</p></td><td><p>80.20</p></td></tr><tr><td>GCNet [4]</td><td><p>ResNet-101</p></td><td><p>78.10</p></td></tr><tr><td>Axial-DeepLab-XL [47] (MS)</td><td><p>Axial-ResNet-XL</p></td><td><p>81.10</p></td></tr><tr><td>Axial-DeepLab-L [47] (MS)</td><td><p>Axial-ResNet-L</p></td><td><p>81.50</p></td></tr><tr><td>SETR-PUP (40k, SS)</td><td><p>T-Large</p></td><td><p>78.39</p></td></tr><tr><td>SETR-PUP (40k, MS)</td><td><p>T-Large</p></td><td><p>81.57</p></td></tr><tr><td>SETR-PUP (80k, SS)</td><td><p>T-Large</p></td><td><p>79.34</p></td></tr><tr><td>SETR-PUP (80k, MS)</td><td><p>T-Large</p></td><td>82.15</td></tr></tbody></table>", "caption": "Table 6: State-of-the-art comparison on the Cityscapes validation set.Performances of different training schedules (e.g., 40k and 80k) are reported.SS: Single-scale inference. MS: Multi-scale inference.", "list_citation_info": ["[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.", "[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020.", "[39] OpenMMLab. mmsegmentation. https://github.com/open-mmlab/mmsegmentation, 2020.", "[48] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018.", "[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. In ICCV workshops, 2019.", "[10] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint, 2017.", "[25] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2019."]}, {"table": "<table><tbody><tr><td><p>Method</p></td><td><p>Backbone</p></td><td><p>mIoU</p></td></tr><tr><td>PSPNet [60]</td><td><p>ResNet-101</p></td><td><p>78.40</p></td></tr><tr><td>DenseASPP [50]</td><td><p>DenseNet-161</p></td><td><p>80.60</p></td></tr><tr><td>BiSeNet [52]</td><td><p>ResNet-101</p></td><td><p>78.90</p></td></tr><tr><td>PSANet [61]</td><td><p>ResNet-101</p></td><td><p>80.10</p></td></tr><tr><td>DANet [18]</td><td><p>ResNet-101</p></td><td><p>81.50</p></td></tr><tr><td>OCNet [55]</td><td><p>ResNet-101</p></td><td><p>80.10</p></td></tr><tr><td>CCNet [25]</td><td><p>ResNet-101</p></td><td><p>81.90</p></td></tr><tr><td>Axial-DeepLab-L [47]</td><td><p>Axial-ResNet-L</p></td><td><p>79.50</p></td></tr><tr><td>Axial-DeepLab-XL [47]</td><td><p>Axial-ResNet-XL</p></td><td><p>79.90</p></td></tr><tr><td>SETR-PUP (100k)</td><td><p>T-Large</p></td><td><p>81.08</p></td></tr><tr><td>SETR-PUP{}^{\\ddagger}</td><td><p>T-Large</p></td><td><p>81.64</p></td></tr></tbody></table>", "caption": "Table 7: Comparison on the Cityscapes test set.{\\ddagger}: trained on fine and coarse annotated data.", "list_citation_info": ["[55] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene parsing. arXiv preprint, 2018.", "[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.", "[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020.", "[18] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In CVPR, 2019.", "[61] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In ECCV, 2018.", "[50] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. Denseaspp for semantic segmentation in street scenes. In CVPR, 2018.", "[25] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2019.", "[52] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In ECCV, 2018."]}]}