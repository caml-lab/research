{"title": "Crossformer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation", "abstract": "3D human pose estimation can be handled by encoding the geometric dependencies between the body parts and enforcing the kinematic constraints. Recently, Transformer has been adopted to encode the long-range dependencies between the joints in the spatial and temporal domains. While they had shown excellence in long-range dependencies, studies have noted the need for improving the locality of vision Transformers. In this direction, we propose a novel pose estimation Transformer featuring rich representations of body joints critical for capturing subtle changes across frames (i.e., inter-feature representation). Specifically, through two novel interaction modules; Cross-Joint Interaction and Cross-Frame Interaction, the model explicitly encodes the local and global dependencies between the body joints. The proposed architecture achieved state-of-the-art performance on two popular 3D human pose estimation datasets, Human3.6 and MPI-INF-3DHP. In particular, our proposed CrossFormer method boosts performance by 0.9% and 0.3%, compared to the closest counterpart, PoseFormer, using the detected 2D poses and ground-truth settings respectively.", "authors": ["Mohammed Hassanin", " Abdelwahed Khamiss", " Mohammed Bennamoun", " Farid Boussaid", " Ibrahim Radwan"], "pdf_url": "https://arxiv.org/abs/2203.13387", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Protocol 1</td><td></td><td>Dir.</td><td>Disc.</td><td>Eat.</td><td>Greet</td><td>Phone</td><td>Photo</td><td>Pose</td><td>Purch.</td><td>Sit</td><td>SitD.</td><td>Somke</td><td>Wait</td><td>WalkD.</td><td>Walk</td><td>WalkT.</td><td>Average</td></tr><tr><td>Debral et al. [9]</td><td>ECCV\u201918</td><td>44.8</td><td>50.4</td><td>44.7</td><td>49.0</td><td>52.9</td><td>61.4</td><td>43.5</td><td>45.5</td><td>63.1</td><td>87.3</td><td>51.7</td><td>48.5</td><td>52.2</td><td>37.6</td><td>41.9</td><td>52.1</td></tr><tr><td>Cai et al. [4] (f=7)</td><td>ICCV\u201919</td><td>44.6</td><td>47.4</td><td>45.6</td><td>48.8</td><td>50.8</td><td>59.0</td><td>47.2</td><td>43.9</td><td>57.9</td><td>61.9</td><td>49.7</td><td>46.6</td><td>51.3</td><td>37.1</td><td>39.4</td><td>48.8</td></tr><tr><td>Pavllo et al. [37] (f=243)*</td><td>CVPR\u201919</td><td>45.2</td><td>46.7</td><td>43.3</td><td>45.6</td><td>48.1</td><td>55.1</td><td>44.6</td><td>44.3</td><td>57.3</td><td>65.8</td><td>47.1</td><td>44.0</td><td>49.0</td><td>32.8</td><td>33.9</td><td>46.8</td></tr><tr><td>Lin et al. [26](f=50)</td><td>BMVC\u201919</td><td>42.5</td><td>44.8</td><td>42.6</td><td>44.2</td><td>48.5</td><td>57.1</td><td>52.6</td><td>41.4</td><td>56.5</td><td>64.5</td><td>47.4</td><td>43.0</td><td>48.1</td><td>33.0</td><td>35.1</td><td>46.6</td></tr><tr><td>Yeh et al. [46]</td><td>NIPS\u201919</td><td>44.8</td><td>46.1</td><td>43.3</td><td>46.4</td><td>49.0</td><td>55.2</td><td>44.6</td><td>44.0</td><td>58.3</td><td>62.7</td><td>47.1</td><td>43.9</td><td>48.6</td><td>32.7</td><td>33.3</td><td>46.7</td></tr><tr><td>Liu et al. [29] (f=243)*</td><td>CVPR\u201920</td><td>41.8</td><td>44.8</td><td>41.1</td><td>44.9</td><td>47.4</td><td>54.1</td><td>43.4</td><td>42.2</td><td>56.2</td><td>63.6</td><td>45.3</td><td>43.5</td><td>45.3</td><td>31.3</td><td>32.2</td><td>45.1</td></tr><tr><td>SRNet [49] *</td><td>ECCV\u201920</td><td>46.6</td><td>47.1</td><td>43.9</td><td>41.6</td><td>45.8</td><td>49.6</td><td>46.5</td><td>40.0</td><td>53.4</td><td>61.1</td><td>46.1</td><td>42.6</td><td>43.1</td><td>31.5</td><td>32.6</td><td>44.8</td></tr><tr><td>UGCN [42] (f = 96)</td><td>ECCV\u201920</td><td>41.3</td><td>43.9</td><td>44.0</td><td>42.2</td><td>48.0</td><td>57.1</td><td>42.2</td><td>43.2</td><td>57.3</td><td>61.3</td><td>47.0</td><td>43.5</td><td>47.0</td><td>32.6</td><td>31.8</td><td>45.6</td></tr><tr><td>METRO [27] (f = 1) \u2020</td><td>CVPR\u201921</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>54.0</td></tr><tr><td>PoseFormer (no PT) [51] (81)</td><td>ICCV\u201921</td><td>43.0</td><td>46.5</td><td>41.4</td><td>44.1</td><td>48.1</td><td>53.2</td><td>43.7</td><td>43.6</td><td>54.9</td><td>62.3</td><td>47.1</td><td>44.9</td><td>47.7</td><td>32.8</td><td>33.5</td><td>45.7</td></tr><tr><td>Chen et al. [6] (f = 81)*</td><td>TCSVT\u201921</td><td>42.1</td><td>43.8</td><td>41.0</td><td>43.8</td><td>46.1</td><td>53.5</td><td>42.4</td><td>43.1</td><td>53.9</td><td>60.5</td><td>45.7</td><td>42.1</td><td>46.2</td><td>32.2</td><td>33.8</td><td>44.6</td></tr><tr><td>PoseFormer (PT)[51] (81)</td><td>ICCV\u201921</td><td>41.5</td><td>44.8</td><td>39.8</td><td>42.5</td><td>46.5</td><td>51.6</td><td>42.1</td><td>42.0</td><td>53.3</td><td>60.7</td><td>45.5</td><td>43.3</td><td>46.1</td><td>31.8</td><td>32.2</td><td>44.3</td></tr><tr><td>CrossFormer (81)</td><td></td><td>40.7</td><td>44.1</td><td>40.8</td><td>41.5</td><td>45.8</td><td>52.8</td><td>41.2</td><td>40.8</td><td>55.3</td><td>61.9</td><td>44.9</td><td>41.8</td><td>44.6</td><td>29.2</td><td>31.1</td><td>43.7</td></tr><tr><td>Protocol 2</td><td></td><td>Dir.</td><td>Disc.</td><td>Eat.</td><td>Greet</td><td>Phone</td><td>Photo</td><td>Pose</td><td>Purch.</td><td>Sit</td><td>SitD.</td><td>Somke</td><td>Wait</td><td>WalkD.</td><td>Walk</td><td>WalkT.</td><td>Average</td></tr><tr><td>Pavlakos et al. [36]</td><td>CVPR\u201918</td><td>34.7</td><td>39.8</td><td>41.8</td><td>38.6</td><td>42.5</td><td>47.5</td><td>38.0</td><td>36.6</td><td>50.7</td><td>56.8</td><td>42.6</td><td>39.6</td><td>43.9</td><td>32.1</td><td>36.5</td><td>41.8</td></tr><tr><td>Hossain et al. </td><td>ECCV\u201918</td><td>35.7</td><td>39.3</td><td>44.6</td><td>43.0</td><td>47.2</td><td>54.0</td><td>38.3</td><td>37.5</td><td>51.6</td><td>61.3</td><td>46.5</td><td>41.4</td><td>47.3</td><td>34.2</td><td>39.4</td><td>44.1</td></tr><tr><td>Cai et al. [4] (f = 7)</td><td>ICCV\u201919</td><td>35.7</td><td>37.8</td><td>36.9</td><td>40.7</td><td>39.6</td><td>45.2</td><td>37.4</td><td>34.5</td><td>46.9</td><td>50.1</td><td>40.5</td><td>36.1</td><td>41.0</td><td>29.6</td><td>32.3</td><td>39.0</td></tr><tr><td>Lin et al. [26] (f = 50)</td><td>BMVC\u201919</td><td>32.5</td><td>35.3</td><td>34.3</td><td>36.2</td><td>37.8</td><td>43.0</td><td>33.0</td><td>32.2</td><td>45.7</td><td>51.8</td><td>38.4</td><td>32.8</td><td>37.5</td><td>25.8</td><td>28.9</td><td>36.8</td></tr><tr><td>Pavllo et al. [37] (f = 243)*</td><td>CVPR\u201919</td><td>34.1</td><td>36.1</td><td>34.4</td><td>37.2</td><td>36.4</td><td>42.2</td><td>34.4</td><td>33.6</td><td>45.0</td><td>52.5</td><td>37.4</td><td>33.8</td><td>37.8</td><td>25.6</td><td>27.3</td><td>36.5</td></tr><tr><td>Liu et al. [29] (f = 243)*</td><td>CVPR\u201920</td><td>32.3</td><td>35.2</td><td>33.3</td><td>35.8</td><td>35.9</td><td>41.5</td><td>33.2</td><td>32.7</td><td>44.6</td><td>50.9</td><td>37.0</td><td>32.4</td><td>37.0</td><td>25.2</td><td>27.2</td><td>35.6</td></tr><tr><td>UGCN [42] (f = 96)</td><td>ECCV\u201920</td><td>32.9</td><td>35.2</td><td>35.6</td><td>34.4</td><td>36.4</td><td>42.7</td><td>31.2</td><td>32.5</td><td>45.6</td><td>50.2</td><td>37.3</td><td>32.8</td><td>36.3</td><td>26.0</td><td>23.9</td><td>35.5</td></tr><tr><td>Chen et al. [6] (f = 81)*</td><td>TCSVT\u201921</td><td>33.1</td><td>35.3</td><td>33.4</td><td>35.9</td><td>36.1</td><td>41.7</td><td>32.8</td><td>33.3</td><td>42.6</td><td>49.4</td><td>37.0</td><td>32.7</td><td>36.5</td><td>25.5</td><td>27.9</td><td>35.6</td></tr><tr><td>PoseFormer (no PT) [51] (f=81)</td><td>ICCV\u201921</td><td>33.5</td><td>35.6</td><td>33.5</td><td>35.6</td><td>36.1</td><td>40.4</td><td>32.8</td><td>32.5</td><td>43.5</td><td>49.3</td><td>35.4</td><td>33.2</td><td>36.3</td><td>25.3</td><td>26.6</td><td>35.3</td></tr><tr><td>PoseFormer [51](f=81)</td><td>ICCV\u201921</td><td>32.5</td><td>34.8</td><td>32.6</td><td>34.6</td><td>35.3</td><td>39.5</td><td>32.1</td><td>32.0</td><td>42.8</td><td>48.5</td><td>34.8</td><td>32.4</td><td>35.3</td><td>24.5</td><td>26.0</td><td>34.6</td></tr><tr><td>CrossFormer (f=81)</td><td></td><td>31.4</td><td>34.6</td><td>32.6</td><td>33.7</td><td>34.3</td><td>39.7</td><td>31.6</td><td>31.0</td><td>44.3</td><td>49.3</td><td>35.9</td><td>31.3</td><td>34.4</td><td>23.4</td><td>25.5</td><td>34.3</td></tr></tbody></table>", "caption": "Table 1: Comparison between our proposed method and the state-of-the art approaches for 3D human pose estimation. Mean Per Joint Position Error(MPJPE) is used to measure the mean error between the estimated 3D pose and the ground truth 3D pose onHuman3.6M under Protocols 1&amp;2 where 2D pose detection is used as input. The top shows results of Protocol 1 (MPJPE), whereas the bottom part shows the resultsof Protocol 2 (P-MPJPE). f refers to the number of frames used in each method, \\ast denotes that the input 2D pose detection method used isthe cascaded pyramid network (CPN), and \\dagger refers to a transformer-based model. (Red: best; Blue: second best)", "list_citation_info": ["[46] Raymond Yeh, Yuan-Ting Hu, and Alexander Schwing. Chirality nets for human pose regression. Advances in Neural Information Processing Systems, 32:8163\u20138173, 2019.", "[36] Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis. Ordinal depth supervision for 3d human pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7307\u20137316, 2018.", "[37] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7753\u20137762, 2019.", "[49] Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang Xu, and Stephen Lin. Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach. In European Conference on Computer Vision, pages 507\u2013523. Springer, 2020.", "[9] Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer Afaque, Abhishek Sharma, and Arjun Jain. Learning 3d human pose from structure and motion. In Proceedings of the European Conference on Computer Vision (ECCV), pages 668\u2013683, 2018.", "[6] Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, and Jiebo Luo. Anatomy-aware 3d human pose estimation with bone-based pose decomposition. IEEE Transactions on Circuits and Systems for Video Technology, 2021.", "[4] Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann. Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2272\u20132281, 2019.", "[51] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. 3d human pose estimation with spatial and temporal transformers. arXiv preprint arXiv:2103.10455, 2021.", "[42] Jingbo Wang, Sijie Yan, Yuanjun Xiong, and Dahua Lin. Motion guided 3d pose estimation from videos. In European Conference on Computer Vision, pages 764\u2013780. Springer, 2020.", "[26] Jiahao Lin and Gim Hee Lee. Trajectory space factorization for deep video-based 3d human pose estimation. In BVMC, 2019.", "[27] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1954\u20131963, 2021.", "[29] Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung, and Vijayan Asari. Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5064\u20135073, 2020."]}, {"table": "<table><thead><tr><th>Protocol #1</th><th></th><th>Dir.</th><th>Disc</th><th>Eat</th><th>Greet</th><th>Phone</th><th>Photo</th><th>Pose</th><th>Purch.</th><th>Sit</th><th>SitD.</th><th>Smoke</th><th>Wait</th><th>WalkD.</th><th>Walk</th><th>WalkT.</th><th>Avg.</th></tr></thead><tbody><tr><td>Martinez et al. [30]</td><td>ICCV\u201917</td><td>37.7</td><td>44.4</td><td>40.3</td><td>42.1</td><td>48.2</td><td>54.9</td><td>44.4</td><td>42.1</td><td>54.6</td><td>58.0</td><td>45.1</td><td>46.4</td><td>47.6</td><td>36.4</td><td>40.4</td><td>45.5</td></tr><tr><td>Lee et al. [22]</td><td>ECCV\u201918</td><td>32.1</td><td>36.6</td><td>34.3</td><td>37.8</td><td>44.5</td><td>49.9</td><td>40.9</td><td>36.2</td><td>44.1</td><td>45.6</td><td>35.3</td><td>35.9</td><td>30.3</td><td>37.6</td><td>35.5</td><td>38.4</td></tr><tr><td>Pavllo et al. [37]</td><td>CVPR\u201919</td><td>35.2</td><td>40.2</td><td>32.7</td><td>35.7</td><td>38.2</td><td>45.5</td><td>40.6</td><td>36.1</td><td>48.8</td><td>47.3</td><td>37.8</td><td>39.7</td><td>38.7</td><td>27.8</td><td>29.5</td><td>37.8</td></tr><tr><td>Cai et al. [4]f = 243</td><td>ICCV\u201919</td><td>32.9</td><td>38.7</td><td>32.9</td><td>37.0</td><td>37.3</td><td>44.8</td><td>38.7</td><td>36.1</td><td>41.0</td><td>45.6</td><td>36.8</td><td>37.7</td><td>37.7</td><td>29.5</td><td>31.6</td><td>37.2</td></tr><tr><td>Xu et al. [45]</td><td>CVPR\u201921</td><td>35.8</td><td>38.1</td><td>31.0</td><td>35.3</td><td>35.8</td><td>43.2</td><td>37.3</td><td>31.7</td><td>38.4</td><td>45.5</td><td>35.4</td><td>36.7</td><td>36.8</td><td>27.9</td><td>30.7</td><td>35.8</td></tr><tr><td>Liu et al. [29](f=243)</td><td>CVPR\u201920</td><td>34.5</td><td>37.1</td><td>33.6</td><td>34.2</td><td>32.9</td><td>37.1</td><td>39.6</td><td>35.8</td><td>40.7</td><td>41.4</td><td>33.0</td><td>33.8</td><td>33.0</td><td>26.6</td><td>26.9</td><td>34.7</td></tr><tr><td>Chen et al. [6](f=243)</td><td>TCSVT\u201921</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>32.3</td></tr><tr><td>SRNet [49]</td><td>ECCV\u201920</td><td>34.8</td><td>32.1</td><td>28.5</td><td>30.7</td><td>31.4</td><td>36.9</td><td>35.6</td><td>30.5</td><td>38.9</td><td>40.5</td><td>32.5</td><td>31.0</td><td>29.9</td><td>22.5</td><td>24.5</td><td>32.0</td></tr><tr><td>PoseFormer [51] (f=81)</td><td>ICCV \u201921</td><td>30.0</td><td>33.6</td><td>29.9</td><td>31.0</td><td>30.2</td><td>33.3</td><td>34.8</td><td>31.4</td><td>37.8</td><td>38.6</td><td>31.7</td><td>31.5</td><td>29.0</td><td>23.3</td><td>23.1</td><td>31.3</td></tr><tr><td>CrossFormer (f=81)</td><td></td><td>26.0</td><td>30.0</td><td>26.8</td><td>26.2</td><td>28.0</td><td>31.0</td><td>30.4</td><td>29.6</td><td>35.4</td><td>37.1</td><td>28.4</td><td>27.3</td><td>26.7</td><td>20.5</td><td>19.9</td><td>28.3</td></tr></tbody></table>", "caption": "Table 2: Comparison between the estimated 3D pose of the proposed method and the ground truth 3D pose onHuman3.6M dataset using the Mean Per Joint Position Error under Protocol 1 (MPJPE). All methods use the ground truth 2D pose as input. (Red: best; Blue: second best)", "list_citation_info": ["[37] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7753\u20137762, 2019.", "[49] Ailing Zeng, Xiao Sun, Fuyang Huang, Minhao Liu, Qiang Xu, and Stephen Lin. Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach. In European Conference on Computer Vision, pages 507\u2013523. Springer, 2020.", "[45] Tianhan Xu and Wataru Takano. Graph stacked hourglass networks for 3d human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16105\u201316114, 2021.", "[6] Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, and Jiebo Luo. Anatomy-aware 3d human pose estimation with bone-based pose decomposition. IEEE Transactions on Circuits and Systems for Video Technology, 2021.", "[4] Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann. Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2272\u20132281, 2019.", "[51] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. 3d human pose estimation with spatial and temporal transformers. arXiv preprint arXiv:2103.10455, 2021.", "[22] Kyoungoh Lee, Inwoong Lee, and Sanghoon Lee. Propagating lstm: 3d pose estimation based on joint interdependency. In Proceedings of the European Conference on Computer Vision (ECCV), pages 119\u2013135, 2018.", "[30] Julieta Martinez, Rayat Hossain, Javier Romero, and James J Little. A simple yet effective baseline for 3d human pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2640\u20132649, 2017.", "[29] Ruixu Liu, Ju Shen, He Wang, Chen Chen, Sen-ching Cheung, and Vijayan Asari. Attention mechanism exploits temporal contexts: Real-time 3d human pose reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5064\u20135073, 2020."]}, {"table": "<table><thead><tr><th></th><th></th><th>PCK \u2191</th><th>AUC \u2191</th><th>MPJPE \u2193</th></tr></thead><tbody><tr><th>Mehta et al. [32]</th><th>3DV\u201917</th><td>75.7</td><td>39.3</td><td>117.6</td></tr><tr><th>Mehta et al. [33]</th><th>ACM ToG\u201917</th><td>76.6</td><td>40.4</td><td>124.7</td></tr><tr><th>Pavllo et al. [37] (f=81)</th><th>CVPR\u201919</th><td>86.0</td><td>51.9</td><td>84.0</td></tr><tr><th>Lin et al. [26] (f=25)</th><th>BMVC\u201919</th><td>83.6</td><td>51.4</td><td>79.8</td></tr><tr><th>Li et al. [23]</th><th>CVPR\u201920</th><td>81.2</td><td>46.1</td><td>99.7</td></tr><tr><th>Chen et al. [6]</th><th>TCSVT\u201921</th><td>87.9</td><td>54.0</td><td>78.8</td></tr><tr><th>PoseFormer [51] (f = 9)</th><th>ICCV\u201921</th><td>88.6</td><td>56.4</td><td>77.1</td></tr><tr><th>CrossFormer (f=9)</th><th></th><td>89.1</td><td>57.5</td><td>76.3</td></tr></tbody></table>", "caption": "Table 3: Comparison between the proposed method (CrossFormer) and previous SOTA methods on MPI-INF-3DHP. The metrics of the comparison are the Percentage of Correct Keypoints (PCK) and Area Under the Curve (AUC). The best scores are marked in bold", "list_citation_info": ["[37] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7753\u20137762, 2019.", "[33] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, and Christian Theobalt. Vnect: Real-time 3d human pose estimation with a single rgb camera. ACM Transactions on Graphics (TOG), 36(4):1\u201314, 2017.", "[6] Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, and Jiebo Luo. Anatomy-aware 3d human pose estimation with bone-based pose decomposition. IEEE Transactions on Circuits and Systems for Video Technology, 2021.", "[51] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. 3d human pose estimation with spatial and temporal transformers. arXiv preprint arXiv:2103.10455, 2021.", "[32] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 2017 international conference on 3D vision (3DV), pages 506\u2013516. IEEE, 2017.", "[23] Shichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung Tang, and Kwang-Ting Cheng. Cascaded deep monocular 3d human pose estimation with evolutionary training data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6173\u20136183, 2020.", "[26] Jiahao Lin and Gim Hee Lee. Trajectory space factorization for deep video-based 3d human pose estimation. In BVMC, 2019."]}, {"table": "<table><tbody><tr><th></th><td>f</td><td>Parameters (M)</td><td>FLOPs (M)</td><td>MPJPE</td><td>FPS</td></tr><tr><th>Hossain and Little [17]</th><td>-</td><td>16.95</td><td>33.88</td><td>58.3</td><td>-</td></tr><tr><th>Chen et al. [6]</th><td>27</td><td>31.88</td><td>61.7</td><td>45.3</td><td>410</td></tr><tr><th>Chen et al. [6]</th><td>81</td><td>45.53</td><td>88.9</td><td>44.6</td><td>315</td></tr><tr><th>Chen et al. [6]</th><td>243</td><td>59.18</td><td>116</td><td>44.1</td><td>264</td></tr><tr><th>PoseFormer [51]</th><td>9</td><td>9.58</td><td>150</td><td>49.9</td><td>320</td></tr><tr><th>PoseFormer [51]</th><td>27</td><td>9.59</td><td>452</td><td>47.0</td><td>297</td></tr><tr><th>PoseFormer [51]</th><td>81</td><td>9.60</td><td>1358</td><td>44.3</td><td>269</td></tr><tr><th>CrossFormer</th><td>9</td><td>9.93</td><td>163</td><td>48.5</td><td>284</td></tr><tr><th>CrossFormer</th><td>27</td><td>9.93</td><td>515</td><td>46.5</td><td>266</td></tr><tr><th>CrossFormer</th><td>81</td><td>9.93</td><td>1739</td><td>43.8</td><td>241</td></tr></tbody></table>", "caption": "Table 4: Comparison between the proposed method and a set of previous methods in terms of the comparison are computational complexity, number of the parameters, MPJPE, and Frames Per Second (FPS). The experiments are conducted on Human3.6M under Protocol 1 with the detected 2D poseas input.", "list_citation_info": ["[17] Mir Rayat Imtiaz Hossain and James J Little. Exploiting temporal information for 3d human pose estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 68\u201384, 2018.", "[6] Tianlang Chen, Chen Fang, Xiaohui Shen, Yiheng Zhu, Zhili Chen, and Jiebo Luo. Anatomy-aware 3d human pose estimation with bone-based pose decomposition. IEEE Transactions on Circuits and Systems for Video Technology, 2021.", "[51] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. 3d human pose estimation with spatial and temporal transformers. arXiv preprint arXiv:2103.10455, 2021."]}]}