{"title": "Plenoxels: Radiance Fields without Neural Networks", "abstract": "We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.", "authors": ["Alex Yu", " Sara Fridovich-Keil", " Matthew Tancik", " Qinhong Chen", " Benjamin Recht", " Angjoo Kanazawa"], "pdf_url": "https://arxiv.org/abs/2112.05131", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th>PSNR \\uparrow</th><th>SSIM \\uparrow</th><th>LPIPS \\downarrow</th><th>Train Time</th></tr></thead><tbody><tr><th>Ours</th><td>31.71</td><td>0.958</td><td>0.049</td><td>11 mins</td></tr><tr><th>NV [20]</th><td>26.05</td><td>0.893</td><td>0.160</td><td>&gt;1 day</td></tr><tr><th>JAXNeRF [26, 7]</th><td>31.85</td><td>0.954</td><td>0.072</td><td>1.45 days</td></tr><tr><th>Ours</th><td>26.29</td><td>0.839</td><td>0.210</td><td>24 mins</td></tr><tr><th>LLFF [25]</th><td>24.13</td><td>0.798</td><td>0.212</td><td>\u2014*</td></tr><tr><th>JAXNeRF [26, 7]</th><td>26.71</td><td>0.820</td><td>0.235</td><td>1.62 days</td></tr><tr><th>Ours</th><td>20.40</td><td>0.696</td><td>0.420</td><td>27 mins</td></tr><tr><th>NeRF++ [57]</th><td>20.49</td><td>0.648</td><td>0.478</td><td>\\sim4 days</td></tr></tbody></table>", "caption": "Table 2: Results. Top: average over the 8 synthetic scenes from NeRF; Middle: the 8 real, forward-facing scenes from NeRF; Bottom: the 4 real, 360^{\\circ} scenes from Tanks and Temples [15]. 4 of the synthetic scenes train in under 10 minutes. *LLFF requires pretraining a network to predict MPIs for each view, and then can render novel scenes without further training; this pretraining is amortized across all scenes so we do not include it in the table.", "list_citation_info": ["[15] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Trans. Graph., 36(4), July 2017.", "[57] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. NeRF++: Analyzing and improving neural radiance fields, 2020.", "[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405\u2013421. Springer, 2020.", "[25] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines, 2019.", "[20] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes. ACM Transactions on Graphics, 38(4):1\u201314, Jul 2019."]}, {"table": "<table><tbody><tr><th></th><td>PSNR \\uparrow</td><td>SSIM \\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><th>Ours: 100 images (low TV)</th><td>31.71</td><td>0.958</td><td>0.050</td></tr><tr><th>NeRF: 100 images [26]</th><td>31.01</td><td>0.947</td><td>0.081</td></tr><tr><th>Ours: 25 images (low TV)</th><td>26.88</td><td>0.911</td><td>0.099</td></tr><tr><th>Ours: 25 images (high TV)</th><td>28.25</td><td>0.932</td><td>0.078</td></tr><tr><th>NeRF: 25 images [26]</th><td>27.78</td><td>0.925</td><td>0.108</td></tr></tbody></table>", "caption": "Table 3: Ablation over the number of views. By increasing our TV regularization, we exceed NeRF fidelity even when the number of training views is only a quarter of the full dataset. Results are averaged over the 8 synthetic scenes from NeRF.", "list_citation_info": ["[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405\u2013421. Springer, 2020."]}, {"table": "<table><thead><tr><th>Rendering Formula</th><th>PSNR \\uparrow</th><th>SSIM \\uparrow</th><th>LPIPS \\downarrow</th></tr></thead><tbody><tr><th>Max [22], used in NeRF [26]</th><td>30.57</td><td>0.950</td><td>0.065</td></tr><tr><th>Neural Volumes [20]</th><td>27.54</td><td>0.906</td><td>0.201</td></tr></tbody></table>", "caption": "Table 5: Comparison of different rendering formulas. We compare the rendering formula from Max [22] (used in NeRF and our main method) to the one used in Neural Volumes [20], which uses absolute instead of relative transmittance. Results are averaged over the 8 synthetic scenes from NeRF.", "list_citation_info": ["[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405\u2013421. Springer, 2020.", "[20] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes. ACM Transactions on Graphics, 38(4):1\u201314, Jul 2019.", "[22] N. Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics, 1(2):99\u2013108, 1995."]}, {"table": "<table><thead><tr><th>LR Schedule</th><th>PSNR \\uparrow</th><th>SSIM \\uparrow</th><th>LPIPS \\downarrow</th></tr></thead><tbody><tr><th>Exp for SH, Delayed for \\sigma [2]</th><td>30.57</td><td>0.950</td><td>0.065</td></tr><tr><th>Exp for SH and \\sigma</th><td>30.58</td><td>0.950</td><td>0.066</td></tr><tr><th>Exp for SH, Constant for \\sigma</th><td>30.37</td><td>0.948</td><td>0.068</td></tr><tr><th>Constant for SH and \\sigma</th><td>30.13</td><td>0.945</td><td>0.075</td></tr></tbody></table>", "caption": "Table 6: Comparison of different learning rate schedules for \\sigma (voxel opacity) and spherical harmonics (SH), with fixed resolution 256^{3} and RMSProp [10]. Results are averaged over the 8 synthetic scenes from NeRF [26]. Our method is robust to variations in learning rate schedule.", "list_citation_info": ["[2] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields, 2021.", "[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405\u2013421. Springer, 2020.", "[10] Geoffrey Hinton. RMSProp."]}, {"table": "<table><thead><tr><th>Optimizer</th><th>PSNR \\uparrow</th><th>SSIM \\uparrow</th><th>LPIPS \\downarrow</th></tr></thead><tbody><tr><th>RMSProp [10] for SH and \\sigma</th><td>30.57</td><td>0.950</td><td>0.065</td></tr><tr><th>RMSProp for SH, SGD for \\sigma</th><td>30.20</td><td>0.946</td><td>0.072</td></tr><tr><th>SGD for SH, RMSProp for \\sigma</th><td>29.82</td><td>0.940</td><td>0.076</td></tr><tr><th>SGD for SH and \\sigma</th><td>29.35</td><td>0.932</td><td>0.087</td></tr></tbody></table>", "caption": "Table 7: Comparison of different optimizers for \\sigma and SH, with fixed resolution 256^{3}. Results are averaged over the 8 synthetic scenes from NeRF [26]. Our method is robust to variations in optimizer, although there is a benefit to RMSProp particularly for optimizing the spherical harmonic coefficients.", "list_citation_info": ["[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pages 405\u2013421. Springer, 2020.", "[10] Geoffrey Hinton. RMSProp."]}, {"table": "<table><tbody><tr><td colspan=\"6\">PSNR \\uparrow</td></tr><tr><td></td><td>M60</td><td>Playground</td><td>Train</td><td>Truck</td><td>Mean</td></tr><tr><td>Ours</td><td>17.93</td><td>23.03</td><td>17.97</td><td>22.67</td><td>20.40</td></tr><tr><td>NeRF++ [57]</td><td>18.49</td><td>22.93</td><td>17.77</td><td>22.77</td><td>20.49</td></tr><tr><td colspan=\"6\">SSIM \\uparrow</td></tr><tr><td></td><td>M60</td><td>Playground</td><td>Train</td><td>Truck</td><td>Mean</td></tr><tr><td>Ours</td><td>0.687</td><td>0.712</td><td>0.629</td><td>0.758</td><td>0.696</td></tr><tr><td>NeRF++</td><td>0.650</td><td>0.672</td><td>0.558</td><td>0.712</td><td>0.648</td></tr><tr><td colspan=\"6\">LPIPS \\downarrow</td></tr><tr><td></td><td>M60</td><td>Playground</td><td>Train</td><td>Truck</td><td>Mean</td></tr><tr><td>Ours</td><td>0.439</td><td>0.435</td><td>0.443</td><td>0.364</td><td>0.420</td></tr><tr><td>NeRF++</td><td>0.481</td><td>0.477</td><td>0.531</td><td>0.424</td><td>0.478</td></tr><tr><td colspan=\"6\">Optimization Time \\downarrow</td></tr><tr><td></td><td>M60</td><td>Playground</td><td>Train</td><td>Truck</td><td>Mean</td></tr><tr><td>Ours</td><td>25.5m</td><td>26.3m</td><td>29.5m</td><td>28.0m</td><td>27.3m</td></tr></tbody></table><br/>", "caption": "Table 9: Full results on 360^{\\circ} scenes.", "list_citation_info": ["[57] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. NeRF++: Analyzing and improving neural radiance fields, 2020."]}, {"table": "<table><thead><tr><th colspan=\"10\">PSNR \\uparrow</th></tr></thead><tbody><tr><th></th><td>Chair</td><td>Drums</td><td>Ficus</td><td>Hotdog</td><td>Lego</td><td>Materials</td><td>Mic</td><td>Ship</td><td>Mean</td></tr><tr><th>Ours</th><td>33.98</td><td>25.35</td><td>31.83</td><td>36.43</td><td>34.10</td><td>29.14</td><td>33.26</td><td>29.62</td><td>31.71</td></tr><tr><th>NV [20]</th><td>28.33</td><td>22.58</td><td>24.79</td><td>30.71</td><td>26.08</td><td>24.22</td><td>27.78</td><td>23.93</td><td>26.05</td></tr><tr><th>JAXNeRF [7, 26]</th><td>34.20</td><td>25.27</td><td>31.15</td><td>36.81</td><td>34.02</td><td>30.30</td><td>33.72</td><td>29.33</td><td>31.85</td></tr><tr><th colspan=\"10\">SSIM \\uparrow</th></tr><tr><th></th><td>Chair</td><td>Drums</td><td>Ficus</td><td>Hotdog</td><td>Lego</td><td>Materials</td><td>Mic</td><td>Ship</td><td>Mean</td></tr><tr><th>Ours</th><td>0.977</td><td>0.933</td><td>0.976</td><td>0.980</td><td>0.975</td><td>0.949</td><td>0.985</td><td>0.890</td><td>0.958</td></tr><tr><th>NV</th><td>0.916</td><td>0.873</td><td>0.910</td><td>0.944</td><td>0.880</td><td>0.888</td><td>0.946</td><td>0.784</td><td>0.893</td></tr><tr><th>JAXNeRF</th><td>0.975</td><td>0.929</td><td>0.970</td><td>0.978</td><td>0.970</td><td>0.955</td><td>0.983</td><td>0.868</td><td>0.954</td></tr><tr><th colspan=\"10\">LPIPS \\downarrow</th></tr><tr><th></th><td>Chair</td><td>Drums</td><td>Ficus</td><td>Hotdog</td><td>Lego</td><td>Materials</td><td>Mic</td><td>Ship</td><td>Mean</td></tr><tr><th>Ours</th><td>0.031</td><td>0.067</td><td>0.026</td><td>0.037</td><td>0.028</td><td>0.057</td><td>0.015</td><td>0.134</td><td>0.049</td></tr><tr><th>NV</th><td>0.109</td><td>0.214</td><td>0.162</td><td>0.109</td><td>0.175</td><td>0.130</td><td>0.107</td><td>0.276</td><td>0.160</td></tr><tr><th>JAXNeRF</th><td>0.036</td><td>0.085</td><td>0.037</td><td>0.074</td><td>0.068</td><td>0.057</td><td>0.023</td><td>0.192</td><td>0.072</td></tr><tr><th colspan=\"10\">Optimization Time \\downarrow</th></tr><tr><th></th><td>Chair</td><td>Drums</td><td>Ficus</td><td>Hotdog</td><td>Lego</td><td>Materials</td><td>Mic</td><td>Ship</td><td>Mean</td></tr><tr><th>Ours</th><td>9.6m</td><td>9.8m</td><td>8.8m</td><td>12.5m</td><td>10.8m</td><td>11.0m</td><td>8.2m</td><td>18.0m</td><td>11.1m</td></tr><tr><th>JAXNeRF</th><td>37.8h</td><td>37.8h</td><td>37.7h</td><td>38.0h</td><td>26.0h</td><td>38.1h</td><td>37.8h</td><td>26.0h</td><td>34.9h</td></tr></tbody></table><br/>", "caption": "Table 10: Full results on synthetic scenes.", "list_citation_info": ["[7] Boyang Deng, Jonathan T. Barron, and Pratul P. Srinivasan. JaxNeRF: an efficient JAX implementation of NeRF, 2020.", "[20] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes. ACM Transactions on Graphics, 38(4):1\u201314, Jul 2019."]}, {"table": "<table><thead><tr><th colspan=\"10\">PSNR \\uparrow</th></tr></thead><tbody><tr><th></th><td>Fern</td><td>Flower</td><td>Fortress</td><td>Horns</td><td>Leaves</td><td>Orchids</td><td>Room</td><td>T-Rex</td><td>Mean</td></tr><tr><th>Ours</th><td>25.46</td><td>27.83</td><td>31.09</td><td>27.58</td><td>21.41</td><td>20.24</td><td>30.22</td><td>26.48</td><td>26.29</td></tr><tr><th>LLFF [25]</th><td>28.42</td><td>22.85</td><td>19.52</td><td>29.40</td><td>18.52</td><td>25.46</td><td>24.15</td><td>24.70</td><td>24.13</td></tr><tr><th>JAXNeRF [7, 26]</th><td>25.20</td><td>27.80</td><td>31.57</td><td>27.70</td><td>21.10</td><td>20.37</td><td>32.81</td><td>27.12</td><td>26.71</td></tr><tr><th colspan=\"10\">SSIM \\uparrow</th></tr><tr><th></th><td>Fern</td><td>Flower</td><td>Fortress</td><td>Horns</td><td>Leaves</td><td>Orchids</td><td>Room</td><td>T-Rex</td><td>Mean</td></tr><tr><th>Ours</th><td>0.832</td><td>0.862</td><td>0.885</td><td>0.857</td><td>0.760</td><td>0.687</td><td>0.937</td><td>0.890</td><td>0.839</td></tr><tr><th>LLFF</th><td>0.932</td><td>0.753</td><td>0.697</td><td>0.872</td><td>0.588</td><td>0.844</td><td>0.857</td><td>0.840</td><td>0.798</td></tr><tr><th>JAXNeRF</th><td>0.798</td><td>0.840</td><td>0.890</td><td>0.840</td><td>0.703</td><td>0.649</td><td>0.952</td><td>0.890</td><td>0.820</td></tr><tr><th colspan=\"10\">LPIPS \\downarrow</th></tr><tr><th></th><td>Fern</td><td>Flower</td><td>Fortress</td><td>Horns</td><td>Leaves</td><td>Orchids</td><td>Room</td><td>T-Rex</td><td>Mean</td></tr><tr><th>Ours</th><td>0.224</td><td>0.179</td><td>0.180</td><td>0.231</td><td>0.198</td><td>0.242</td><td>0.192</td><td>0.238</td><td>0.210</td></tr><tr><th>LLFF</th><td>0.155</td><td>0.247</td><td>0.216</td><td>0.173</td><td>0.313</td><td>0.174</td><td>0.222</td><td>0.193</td><td>0.212</td></tr><tr><th>JAXNeRF</th><td>0.272</td><td>0.198</td><td>0.151</td><td>0.249</td><td>0.305</td><td>0.307</td><td>0.164</td><td>0.235</td><td>0.235</td></tr><tr><th colspan=\"10\">Optimization Time \\downarrow</th></tr><tr><th></th><td>Fern</td><td>Flower</td><td>Fortress</td><td>Horns</td><td>Leaves</td><td>Orchids</td><td>Room</td><td>T-Rex</td><td>Mean</td></tr><tr><th>Ours</th><td>23.7m</td><td>22.0m</td><td>31.2m</td><td>26.3m</td><td>13.3m</td><td>23.4m</td><td>28.8m</td><td>24.8m</td><td>24.2m</td></tr><tr><th>JAXNeRF</th><td>38.9h</td><td>38.8h</td><td>38.6h</td><td>38.7h</td><td>38.8h</td><td>38.7h</td><td>39.1h</td><td>38.6h</td><td>38.8h</td></tr></tbody></table><br/>", "caption": "Table 11: Full results on forward-facing scenes.", "list_citation_info": ["[7] Boyang Deng, Jonathan T. Barron, and Pratul P. Srinivasan. JaxNeRF: an efficient JAX implementation of NeRF, 2020.", "[25] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines, 2019."]}]}