{"title": "Masked autoencoders that listen", "abstract": "This paper studies a simple extension of image-based Masked Autoencoders (MAE) to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. The code and models will be at https://github.com/facebookresearch/AudioMAE.", "authors": ["Po-Yao Huang", " Hu Xu", " Juncheng Li", " Alexei Baevski", " Michael Auli", " Wojciech Galuba", " Florian Metze", " Christoph Feichtenhofer"], "pdf_url": "https://arxiv.org/abs/2207.06405", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td>Backbone</td><td>PT-Data</td><td>AS-20K</td><td>AS-2M</td><td>ESC-50</td><td>SPC-2</td><td>SPC-1</td><td>SID</td></tr><tr><td colspan=\"3\">No pre-training</td><td colspan=\"5\"></td><td></td></tr><tr><td>ERANN [58]</td><td>CNN</td><td>-</td><td>-</td><td>45.0</td><td>89.2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PANN [59]</td><td>CNN</td><td>-</td><td>27.8</td><td>43.1</td><td>83.3</td><td>61.8</td><td>-</td><td>-</td></tr><tr><td colspan=\"3\">In-domain self-supervised pre-training</td><td colspan=\"5\"></td><td></td></tr><tr><td>wav2vec 2.0 [33]</td><td>Transformer</td><td>LS</td><td>-</td><td>-</td><td>-</td><td>-</td><td>96.2<sup>*</sup></td><td>75.2<sup>*</sup></td></tr><tr><td>HuBERT [35]</td><td>Transformer</td><td>LS</td><td>-</td><td>-</td><td>-</td><td>-</td><td>96.3<sup>*</sup></td><td>81.4<sup>*</sup></td></tr><tr><td>Conformer [37]</td><td>Conformer</td><td>AS</td><td>-</td><td>41.1</td><td>88.0</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SS-AST [18]</td><td>ViT-B</td><td>AS+LS</td><td>31.0</td><td>-</td><td>88.8</td><td>98.0</td><td>96.0</td><td>64.3</td></tr><tr><td colspan=\"3\">Concurrent MAE-based works</td><td colspan=\"5\"></td><td></td></tr><tr><td>MaskSpec [43]</td><td>ViT-B</td><td>AS</td><td>32.3</td><td>47.1</td><td>89.6</td><td>97.7</td><td>-</td><td>-</td></tr><tr><td>MAE-AST [38]</td><td>ViT-B</td><td>AS+LS</td><td>30.6</td><td>-</td><td>90.0</td><td>97.9</td><td>95.8</td><td>63.3</td></tr><tr><td>Audio-MAE (global)</td><td>ViT-B</td><td>AS</td><td>36.6\\pm.11</td><td>46.8\\pm.06</td><td>93.6\\pm.11</td><td>98.3\\pm.06</td><td>97.6\\pm.06</td><td>94.1\\pm.06</td></tr><tr><td>Audio-MAE (local)</td><td>ViT-B</td><td>AS</td><td>37.1\\pm.06</td><td>47.3\\pm.06</td><td>94.1\\pm.10</td><td>98.3\\pm.06</td><td>96.9\\pm.00</td><td>94.8\\pm.11</td></tr><tr><td colspan=\"3\">Out-of-domain supervised pre-training</td><td colspan=\"5\"></td><td></td></tr><tr><td>PSLA [30]</td><td>EffNet [60]</td><td>IN</td><td>31.9</td><td>44.4</td><td>-</td><td>96.3</td><td>-</td><td>-</td></tr><tr><td>AST [10]</td><td>DeiT-B</td><td>IN</td><td>34.7</td><td>45.9</td><td>88.7</td><td>98.1</td><td>95.5</td><td>41.1</td></tr><tr><td>MBT [11]</td><td>ViT-B</td><td>IN-21K</td><td>31.3</td><td>44.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HTS-AT [29]</td><td>Swin-B</td><td>IN</td><td>-</td><td>47.1</td><td>97.0<sup>\\dagger</sup></td><td>98.0</td><td>-</td><td>-</td></tr><tr><td>PaSST [28]</td><td>DeiT-B</td><td>IN</td><td>-</td><td>47.1</td><td>96.8<sup>\\dagger</sup></td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Comparison with other state-of-the-art models on audio and speech classification tasks.Metrics are mAP for AS and accuracy (%) for ESC/SPC/SID.For pre-training (PT) dataset, AS:AudioSet, LS:LibriSpeech, and IN:ImageNet.<sup>\\dagger</sup>: Fine-tuning results with additional supervised training on AS-2M.We gray-out models pre-trained with external non-audio datasets (e.g., ImageNet).Best single models in AS-2M are compared (no ensembles). <sup>*</sup>: linear evaluation results from [53].", "list_citation_info": ["[60] M. Tan and Q. V. Le, \u201cEfficientnet: Rethinking model scaling for convolutional neural networks,\u201d in Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, ser. Proceedings of Machine Learning Research, vol. 97. PMLR, 2019, pp. 6105\u20136114.", "[35] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.", "[43] D. Chong, H. Wang, P. Zhou, and Q. Zeng, \u201cMasked spectrogram prediction for self-supervised audio pre-training,\u201d 2022.", "[10] Y. Gong, Y. Chung, and J. R. Glass, \u201cAST: audio spectrogram transformer,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 571\u2013575.", "[11] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun, \u201cAttention bottlenecks for multimodal fusion,\u201d in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021, pp. 14\u2009200\u201314\u2009213.", "[59] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE ACM Trans. Audio Speech Lang. Process., vol. 28, pp. 2880\u20132894, 2020.", "[18] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. R. Glass, \u201cSsast: Self-supervised audio spectrogram transformer,\u201d ArXiv, vol. abs/2110.09784, 2021.", "[28] K. Koutini, J. Schl\u00fcter, H. Eghbal-zadeh, and G. Widmer, \u201cEfficient training of audio transformers with patchout,\u201d CoRR, vol. abs/2110.05069, 2021.", "[30] Y. Gong, Y. Chung, and J. Glass, \u201cPsla: Improving audio tagging with pretraining, sampling, labeling, and aggregation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021.", "[37] S. Srivastava, Y. Wang, A. Tjandra, A. Kumar, C. Liu, K. Singh, and Y. Saraf, \u201cConformer-based self-supervised learning for non-speech audio tasks,\u201d arXiv preprint arXiv:2110.07313, 2021.", "[29] K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov, \u201cHts-at: A hierarchical token-semantic audio transformer for sound classification and detection,\u201d arXiv preprint arXiv:2202.00874, 2022.", "[33] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.", "[53] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Interspeech 2021, 2021, pp. 1194\u20131198.", "[38] A. Baade, P. Peng, and D. Harwath, \u201cMae-ast: Masked autoencoding audio spectrogram transformer,\u201d arXiv preprint arXiv:2203.16691, 2022.", "[58] S. Verbitskiy, V. Berikov, and V. Vyshegorodtsev, \u201cEranns: Efficient residual audio neural networks for audio pattern recognition,\u201d arXiv preprint arXiv:2106.01621, 2021."]}, {"table": "<table><tbody><tr><th></th><th>pre-training</th><td colspan=\"6\">fine-tuning</td></tr><tr><th>Configuration</th><th>AS-2M PT</th><td>AS-2M</td><td>AS-20K</td><td>ESC [13]</td><td>SPC-2 [52]</td><td>SPC-1</td><td>SID [54]</td></tr><tr><th>Optimizer</th><th colspan=\"7\">AdamW [63]</th></tr><tr><th>Optimizer momentum</th><th colspan=\"7\">\\beta_{1}=0.9, \\beta_{2}=0.95</th></tr><tr><th>Weight decay</th><th colspan=\"7\">0.0001</th></tr><tr><th>Base learning rate</th><th>0.0002</th><td>0.0002<sup>\\dagger</sup></td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td><td>0.001</td></tr><tr><th>Learning rate schedule</th><th colspan=\"7\">half-cycle cosine decay [64]</th></tr><tr><th>Minimum learning rate</th><th colspan=\"7\">0.000001</th></tr><tr><th>Gradient clipping</th><th colspan=\"7\">None</th></tr><tr><th>Warm-up epochs</th><th>3</th><td>20</td><td>4</td><td>4</td><td>4</td><td>1</td><td>4</td></tr><tr><th>Epochs</th><th>32</th><td>100</td><td>60</td><td>60</td><td>60</td><td>10</td><td>60</td></tr><tr><th>Batch size</th><th>512</th><td>512</td><td>32</td><td>64</td><td>256</td><td>256</td><td>64</td></tr><tr><th>GPUs</th><th>64</th><td>64</td><td>4</td><td>4</td><td>4</td><td>4</td><td>4</td></tr><tr><th>Weighted sampling</th><th>False</th><td>True</td><td>False</td><td>False</td><td>False</td><td>False<sup>*</sup></td><td>False</td></tr><tr><th>Weighted sampling size</th><th>-</th><td>200,000</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Augmentation</th><th>R</th><td>R</td><td>R</td><td>R</td><td>R+N</td><td>R+N</td><td>R+N</td></tr><tr><th>SpecAug [48] (time/frequency)</th><th>-</th><td>192/48</td><td>192/48</td><td>96/24</td><td>48/48</td><td>48/48</td><td>192/48</td></tr><tr><th>Drop path [65]</th><th>0.0</th><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><th>Dropout [66]</th><th>0.0</th><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><th>Mixup [57]</th><th>0.0</th><td>0.5</td><td>0.5</td><td>0.0</td><td>0.5</td><td>0.5</td><td>0.0</td></tr><tr><th>Multilabel</th><th>n/a</th><td>True</td><td>True</td><td>False</td><td>False</td><td>False</td><td>False</td></tr><tr><th>Loss Function</th><th>MSE</th><td>BCE</td><td>BCE</td><td>CE</td><td>BCE</td><td>BCE</td><td>CE</td></tr><tr><th>Dataset Mean for Normalization</th><th>-4.268</th><td>-4.268</td><td>-4.268</td><td>-6.627</td><td>-6.846</td><td>-6.702</td><td>-6.370</td></tr><tr><th>Dataset Std for Normalization</th><th>4.569</th><td>4.569</td><td>4.569</td><td>5.359</td><td>5.565</td><td>5.448</td><td>3.074</td></tr></tbody></table>", "caption": "Table 3: Pre-training (PT) and Fine-tuning (FT) hyperparameters. For augmentation, R: sampling random starting points with cyclic rolling in time; N: adding random noise (signal-to-noise ratio (SNR): 20dB) to spectrograms. For loss functions, BCE: binary cross entropy loss (for multi-label datasets or when using mixup [57]); CE: cross-entropy loss, MSE: mean square error loss.<sup>*</sup>: We repeat and balance each class to 50% of the size of the unknown class.<sup>\\dagger</sup>: For ViT-S, We use a learning rate of 0.0005 on AS-2M FT and 0.002 on AS-20K FT as we find larger learning rates work better for ViT-S encoder.", "list_citation_info": ["[54] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, \u201cVoxceleb: Large-scale speaker verification in the wild,\u201d Comput. Speech Lang., vol. 60, 2020.", "[64] \u2014\u2014, \u201cSGDR: stochastic gradient descent with warm restarts,\u201d in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.", "[66] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting,\u201d J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929\u20131958, 2014.", "[48] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmentation method for automatic speech recognition,\u201d ArXiv, vol. abs/1904.08779, 2019.", "[52] P. Warden, \u201cSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition,\u201d ArXiv e-prints, Apr. 2018.", "[65] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \u201cDeep networks with stochastic depth,\u201d in Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, ser. Lecture Notes in Computer Science, vol. 9908. Springer, 2016, pp. 646\u2013661.", "[57] H. Zhang, M. Ciss\u00e9, Y. N. Dauphin, and D. Lopez-Paz, \u201cmixup: Beyond empirical risk minimization,\u201d in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.", "[13] K. J. Piczak, \u201cESC: Dataset for Environmental Sound Classification,\u201d in Proceedings of the 23rd Annual ACM Conference on Multimedia. ACM Press, 2015, pp. 1015\u20131018.", "[63] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019."]}, {"table": "<table><thead><tr><th>Model</th><th>Backbone</th><th>Pre-training</th><th>ESC-50 FT</th></tr></thead><tbody><tr><td>ERANN [58]</td><td>CNN</td><td>AS-SL</td><td>96.1</td></tr><tr><td>PANN [59]</td><td>CNN</td><td>AS-SL</td><td>94.7</td></tr><tr><td>AST [10]</td><td>DeiT-B</td><td>IN-SL, AS-SL</td><td>95.6</td></tr><tr><td>HTS-AT [29]</td><td>Swin-B</td><td>IN-SL, AS-SL</td><td>97.0</td></tr><tr><td>PASST [28]</td><td>DeiT-B</td><td>IN-SL, AS-SL</td><td>96.8</td></tr><tr><td>Audio-MAE (global)</td><td>ViT-B</td><td>AS-SSL, AS-SL</td><td>96.9</td></tr><tr><td>Audio-MAE (local)</td><td>ViT-B</td><td>AS-SSL, AS-SL</td><td>97.4</td></tr></tbody></table>", "caption": "Table 4: Comparison with other state-of-the-art models on ESC-50 with an additional round of supervised pre-training on AudioSet (AS-SL). SSL: self-supervised learning. We gray-out the models with out-of-domain pre-training on ImageNet (IN).", "list_citation_info": ["[10] Y. Gong, Y. Chung, and J. R. Glass, \u201cAST: audio spectrogram transformer,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021. ISCA, 2021, pp. 571\u2013575.", "[28] K. Koutini, J. Schl\u00fcter, H. Eghbal-zadeh, and G. Widmer, \u201cEfficient training of audio transformers with patchout,\u201d CoRR, vol. abs/2110.05069, 2021.", "[59] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE ACM Trans. Audio Speech Lang. Process., vol. 28, pp. 2880\u20132894, 2020.", "[29] K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov, \u201cHts-at: A hierarchical token-semantic audio transformer for sound classification and detection,\u201d arXiv preprint arXiv:2202.00874, 2022.", "[58] S. Verbitskiy, V. Berikov, and V. Vyshegorodtsev, \u201cEranns: Efficient residual audio neural networks for audio pattern recognition,\u201d arXiv preprint arXiv:2106.01621, 2021."]}]}