{"title": "Hierarchical perceiver", "abstract": "We present an autoencoder-based semi-supervised approach to classify perceived human emotions from walking styles obtained from videos or motion-captured data and represented as sequences of 3D poses. Given the motion on each joint in the pose at each time step extracted from 3D pose sequences, we hierarchically pool these joint motions in a bottom-up manner in the encoder, following the kinematic chains in the human body. We also constrain the latent embeddings of the encoder to contain the space of psychologically-motivated affective features underlying the gaits. We train the decoder to reconstruct the motions per joint per time step in a top-down manner from the latent embeddings. For the annotated data, we also train a classifier to map the latent embeddings to emotion labels. Our semi-supervised approach achieves a mean average precision of 0.84 on the Emotion-Gait benchmark dataset, which contains both labeled and unlabeled gaits collected from multiple sources. We outperform current state-of-art algorithms for both emotion recognition and action recognition from 3D gaits by 7%--23% on the absolute. More importantly, we improve the average precision by 10%--50% on the absolute on classes that each makes up less than 25% of the labeled part of the Emotion-Gait benchmark dataset.", "authors": ["Uttaran Bhattacharya", " Christian Roncal", " Trisha Mittal", " Rohan Chandra", " Kyra Kapsaskis", " Kurt Gray", " Aniket Bera", " Dinesh Manocha"], "pdf_url": "https://arxiv.org/abs/1911.08708", "list_table_and_caption": [{"table": "<img/>Figure 1: 3D pose model. The names and numbering of the 21 joints in the pose follow the nomenclature in the ELMD dataset [23].<table><tbody><tr><td rowspan=\"11\"><p>Angles between</p></td><td><p>shoulders at lower back</p></td></tr><tr><td><p>hands at root</p></td></tr><tr><td><p>left shoulder and hand at elbow</p></td></tr><tr><td><p>right shoulder and hand at elbow</p></td></tr><tr><td><p>head and left shoulder at neck</p></td></tr><tr><td><p>head and right shoulder at neck</p></td></tr><tr><td><p>head and left knee at root</p></td></tr><tr><td><p>head and right knee at root</p></td></tr><tr><td><p>left toe and right toe at root</p></td></tr><tr><td><p>left hip and toe at knee</p></td></tr><tr><td><p>right hip and toe at knee</p></td></tr><tr><td rowspan=\"4\"><p>Distance ratios between</p></td><td><p>left hand index (LHI) to neck and LHI to root</p></td></tr><tr><td><p>right-hand index (RHI) to neck and RHI to root</p></td></tr><tr><td><p>LHI to RHI and neck to root</p></td></tr><tr><td><p>left toe to right toe and neck to root</p></td></tr><tr><td><p>Area(\\Delta)</p></td><td><p>\\Delta shoulders to lower back and \\Delta shoulders to root</p></td></tr><tr><td><p>ratios</p></td><td><p>\\Delta hands to lower back and \\Delta hands to root</p></td></tr><tr><td><p>between</p></td><td><p>\\Delta hand indices to neck and \\Delta toes to root</p></td></tr></tbody></table>", "caption": "Table 1: Affective Features. List of the 18 pose affective features that we use to describe the affective feature space for our network.", "list_citation_info": ["[23] Habibie, I., Holden, D., Schwarz, J., Yearsley, J., Komura, T.: A recurrent variational autoencoder for human motion synthesis. In: Proceedings of the British Machine Vision Conference (BMVC) (2017)"]}]}