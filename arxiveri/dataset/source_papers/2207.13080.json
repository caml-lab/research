{"title": "Detrs with hybrid matching", "abstract": "One-to-one set matching is a key design for DETR to establish its end-to-end capability, so that object detection does not require a hand-crafted NMS (non-maximum suppression) to remove duplicate detections. This end-to-end signature is important for the versatility of DETR, and it has been generalized to broader vision tasks. However, we note that there are few queries assigned as positive samples and the one-to-one set matching significantly reduces the training efficacy of positive samples. We propose a simple yet effective method based on a hybrid matching scheme that combines the original one-to-one matching branch with an auxiliary one-to-many matching branch during training. Our hybrid strategy has been shown to significantly improve accuracy. In inference, only the original one-to-one match branch is used, thus maintaining the end-to-end merit and the same inference efficiency of DETR. The method is named H-DETR, and it shows that a wide range of representative DETR methods can be consistently improved across a wide range of visual tasks, including DeformableDETR, PETRv2, PETR, and TransTrack, among others. The code is available at: https://github.com/HDETR", "authors": ["Ding Jia", " Yuhui Yuan", " Haodi He", " Xiaopei Wu", " Haojun Yu", " Weihong Lin", " Lei Sun", " Chao Zhang", " Han Hu"], "pdf_url": "https://arxiv.org/abs/2207.13080", "list_table_and_caption": [{"table": "<table><tr><td>Tasks</td><td colspan=\"2\">2D object detection</td><td>2D pose estimation</td><td colspan=\"2\">3D object detection</td><td>multi-object tracking</td></tr><tr><td>Methods</td><td colspan=\"2\">Deformable-DETR [81]</td><td>PETR [53]</td><td>PETRv2 [37]</td><td>3DETR-m [46]</td><td>TransTrack [55]</td></tr><tr><td>Datasets(metrics)</td><td>COCO(AP)</td><td>LVIS(AP)</td><td>COCO(AP)</td><td>nuScenes(NDS)</td><td>ScanNetV2(AP{}_{50})</td><td>MOT17(MOTA)</td></tr><tr><td>Baseline</td><td>47.0</td><td>43.7</td><td>73.3</td><td>50.68</td><td>45.45</td><td>67.1</td></tr><tr><td>Ours</td><td>48.7</td><td>47.5</td><td>74.9</td><td>52.38</td><td>47.01</td><td>68.7</td></tr><tr><td>\\bigtriangleup</td><td>+1.7</td><td>+3.8^{\\flat}</td><td>+1.6</td><td>+1.7</td><td>+1.56</td><td>+1.6</td></tr><tr><td colspan=\"7\">\\flat: Part of the gains is obtained with other tricks and we provide more details in the experiments and supplementary.</td></tr></table>", "caption": "Table 1: Illustrating the improvements of our hybrid matching scheme across four challenging vision tasks. All the improvements are obtained under the same training epochs and do not require any additional computation cost during evaluation.", "list_citation_info": ["[81] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[46] I. Misra, R. Girdhar, and A. Joulin. An end-to-end transformer model for 3d object detection. In ICCV, pages 2906\u20132917, 2021.", "[37] Y. Liu, J. Yan, F. Jia, S. Li, Q. Gao, T. Wang, X. Zhang, and J. Sun. Petrv2: A unified framework for 3d perception from multi-camera images. arXiv preprint arXiv:2206.01256, 2022.", "[55] P. Sun, J. Cao, Y. Jiang, R. Zhang, E. Xie, Z. Yuan, C. Wang, and P. Luo. Transtrack: Multiple object tracking with transformer. arXiv preprint arXiv:2012.15460, 2020.", "[53] D. Shi, X. Wei, L. Li, Y. Ren, and W. Tan. End-to-end multi-person pose estimation with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11069\u201311078, 2022."]}, {"table": "<table><tr><td>Model</td><td>Backbone</td><td>#epochs</td><td>AP</td></tr><tr><td>NAS-FPN + CopyPase [15]</td><td>EfficientNet-B7</td><td>48</td><td>41.6</td></tr><tr><td>Mask R-CNN + ViT-Det + MAE [25]</td><td>ViT-B</td><td>100</td><td>40.0</td></tr><tr><td>Mask R-CNN + ViT-Det + MAE [25]</td><td>ViT-L</td><td>100</td><td>46.1</td></tr><tr><td>Mask R-CNN + ViT-Det + MAE [25]</td><td>ViT-H</td><td>100</td><td>\\underline{49.1}</td></tr><tr><td>Deformable-DETR</td><td>R50</td><td>24</td><td>30.9</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>R50</td><td>24</td><td>33.5</td></tr><tr><td>Deformable-DETR</td><td>Swin-B</td><td>48</td><td>42.7</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-B</td><td>48</td><td>46.0</td></tr><tr><td>Deformable-DETR</td><td>Swin-L</td><td>48</td><td>43.7</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-L</td><td>48</td><td>47.5</td></tr></table>", "caption": "Table 3: Object detection results on LVIS v1.0 val.", "list_citation_info": ["[15] G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V. Le, and B. Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In CVPR, pages 2918\u20132928, 2021.", "[25] Y. Li, H. Mao, R. Girshick, and K. He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022."]}, {"table": "<table><tr><td>Model</td><td>Backbone</td><td>MS</td><td>TS</td><td>#query</td><td>#epochs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><td>Conditional-DETR [44]</td><td>R50</td><td>\u2717</td><td>\u2717</td><td>300</td><td>108</td><td>43.0</td><td>64.0</td><td>45.7</td><td>22.7</td><td>46.7</td><td>61.5</td></tr><tr><td>Conditional-DETR [44]</td><td>R101</td><td>\u2717</td><td>\u2717</td><td>300</td><td>108</td><td>44.5</td><td>65.6</td><td>47.5</td><td>23.6</td><td>48.4</td><td>63.6</td></tr><tr><td>SAM-DETR [73]</td><td>R50</td><td>\u2717</td><td>\u2717</td><td>300</td><td>50</td><td>39.8</td><td>61.8</td><td>41.6</td><td>20.5</td><td>43.4</td><td>59.6</td></tr><tr><td>SAM-DETR + SMCA [73]</td><td>R50</td><td>\u2717</td><td>\u2717</td><td>300</td><td>50</td><td>41.8</td><td>63.2</td><td>43.9</td><td>22.1</td><td>45.9</td><td>60.9</td></tr><tr><td>Anchor-DETR [62]</td><td>R50</td><td>\u2717</td><td>\u2717</td><td>300</td><td>50</td><td>42.1</td><td>63.1</td><td>44.9</td><td>22.3</td><td>46.2</td><td>60.0</td></tr><tr><td>Anchor-DETR [62]</td><td>R101</td><td>\u2717</td><td>\u2717</td><td>300</td><td>50</td><td>43.5</td><td>64.3</td><td>46.6</td><td>23.2</td><td>47.7</td><td>61.4</td></tr><tr><td>Dynamic-DETR [10]</td><td>R50</td><td>\u2717</td><td>\u2717</td><td>300</td><td>12</td><td>42.9</td><td>61.0</td><td>46.3</td><td>24.6</td><td>44.9</td><td>54.4</td></tr><tr><td>SMCA-DETR [13]</td><td>R50</td><td>\u2713</td><td>\u2717</td><td>300</td><td>108</td><td>45.6</td><td>65.5</td><td>49.1</td><td>25.9</td><td>49.3</td><td>62.6</td></tr><tr><td>SMCA-DETR [13]</td><td>R101</td><td>\u2713</td><td>\u2717</td><td>300</td><td>108</td><td>46.3</td><td>66.6</td><td>50.2</td><td>27.2</td><td>50.5</td><td>63.2</td></tr><tr><td>AdaMixer [14]</td><td>R50</td><td>\u2713</td><td>\u2717</td><td>300</td><td>36</td><td>47.0</td><td>66.0</td><td>51.1</td><td>30.1</td><td>50.2</td><td>61.8</td></tr><tr><td>AdaMixer [14]</td><td>R101</td><td>\u2713</td><td>\u2717</td><td>300</td><td>36</td><td>48.0</td><td>67.0</td><td>52.4</td><td>30.0</td><td>51.2</td><td>63.7</td></tr><tr><td>AdaMixer [14]</td><td>Swin-S</td><td>\u2713</td><td>\u2717</td><td>300</td><td>36</td><td>51.3</td><td>71.2</td><td>55.7</td><td>34.2</td><td>54.6</td><td>67.3</td></tr><tr><td>CF-DETR [4]</td><td>R50</td><td>\u2713</td><td>\u2717</td><td>300</td><td>36</td><td>47.8</td><td>66.5</td><td>52.4</td><td>31.2</td><td>50.6</td><td>62.8</td></tr><tr><td>CF-DETR [4]</td><td>R101</td><td>\u2713</td><td>\u2717</td><td>300</td><td>36</td><td>49.0</td><td>68.1</td><td>53.4</td><td>31.4</td><td>52.2</td><td>64.3</td></tr><tr><td>Deformable-DETR [81]</td><td>R50</td><td>\u2713</td><td>\u2717</td><td>300</td><td>50</td><td>46.2</td><td>65.0</td><td>50.0</td><td>28.3</td><td>49.2</td><td>61.5</td></tr><tr><td>Deformable-DETR [81]</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>50</td><td>46.9</td><td>65.6</td><td>51.0</td><td>29.6</td><td>50.1</td><td>61.6</td></tr><tr><td>Sparse-DETR [52]</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>50</td><td>46.3</td><td>66.0</td><td>50.1</td><td>29.0</td><td>49.5</td><td>60.8</td></tr><tr><td>Sparse-DETR [52]</td><td>Swin-T</td><td>\u2713</td><td>\u2713</td><td>300</td><td>50</td><td>49.3</td><td>69.5</td><td>53.3</td><td>32.0</td><td>52.7</td><td>64.9</td></tr><tr><td>Efficient-DETR [68]</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>45.1</td><td>63.1</td><td>49.1</td><td>28.3</td><td>48.4</td><td>59.0</td></tr><tr><td>Efficient-DETR [68]</td><td>R101</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>45.7</td><td>64.1</td><td>49.5</td><td>28.2</td><td>49.1</td><td>60.2</td></tr><tr><td>DAB-Deformable-DETR [34]</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>50</td><td>46.8</td><td>66.0</td><td>50.4</td><td>29.1</td><td>49.8</td><td>62.3</td></tr><tr><td>DN-Deformable-DETR [22]</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>12</td><td>43.4</td><td>61.9</td><td>47.2</td><td>24.8</td><td>46.8</td><td>59.4</td></tr><tr><td>DN-Deformable-DETR [22]</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>50</td><td>48.6</td><td>67.4</td><td>52.7</td><td>31.0</td><td>52.0</td><td>63.7</td></tr><tr><td>DN-Deformable-DETR [22]</td><td>R101</td><td>\u2713</td><td>\u2713</td><td>300</td><td>50</td><td>45.2</td><td>65.5</td><td>48.3</td><td>24.1</td><td>49.1</td><td>65.1</td></tr><tr><td>DINO-Deformable-DETR [74]{}^{\\dagger}</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>900</td><td>12</td><td>47.9</td><td>65.3</td><td>52.1</td><td>31.2</td><td>50.9</td><td>61.9</td></tr><tr><td>DINO-Deformable-DETR [74]{}^{\\dagger}</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>900</td><td>24</td><td>49.9</td><td>67.4</td><td>54.5</td><td>31.8</td><td>53.3</td><td>64.3</td></tr><tr><td>DINO-Deformable-DETR [74]{}^{\\dagger}</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>900</td><td>36</td><td>50.5</td><td>68.3</td><td>55.1</td><td>32.7</td><td>53.9</td><td>64.9</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>12</td><td>48.7</td><td>66.4</td><td>52.9</td><td>31.2</td><td>51.5</td><td>63.5</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>R101</td><td>\u2713</td><td>\u2713</td><td>300</td><td>12</td><td>49.4</td><td>67.2</td><td>53.7</td><td>31.9</td><td>53.1</td><td>64.2</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-T</td><td>\u2713</td><td>\u2713</td><td>300</td><td>12</td><td>50.6</td><td>68.9</td><td>55.1</td><td>33.4</td><td>53.7</td><td>65.9</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-S</td><td>\u2713</td><td>\u2713</td><td>300</td><td>12</td><td>52.5</td><td>71.2</td><td>57.5</td><td>35.2</td><td>56.1</td><td>68.0</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-S (IN-22K)</td><td>\u2713</td><td>\u2713</td><td>300</td><td>12</td><td>53.5</td><td>72.3</td><td>58.4</td><td>36.6</td><td>57.1</td><td>69.4</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>\u2713</td><td>300</td><td>12</td><td>55.9</td><td>75.2</td><td>61.0</td><td>39.1</td><td>59.9</td><td>72.2</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>\u2713</td><td>900</td><td>12</td><td>56.1</td><td>75.2</td><td>61.3</td><td>39.3</td><td>60.4</td><td>72.4</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>R50</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>50.0</td><td>68.3</td><td>54.4</td><td>32.9</td><td>52.7</td><td>65.3</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>R101</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>50.1</td><td>68.4</td><td>54.5</td><td>31.7</td><td>53.6</td><td>65.3</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-T</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>53.2</td><td>71.5</td><td>58.2</td><td>35.9</td><td>56.4</td><td>68.2</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-S</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>54.4</td><td>72.9</td><td>59.4</td><td>36.9</td><td>58.3</td><td>69.5</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-S (IN-22K)</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>55.3</td><td>74.4</td><td>60.5</td><td>38.3</td><td>58.9</td><td>72.0</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>\u2713</td><td>300</td><td>36</td><td>57.1</td><td>76.2</td><td>62.5</td><td>39.7</td><td>61.4</td><td>73.4</td></tr><tr><td>\\mathcal{H}-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>\u2713</td><td>900</td><td>36</td><td>{57.4}</td><td>{76.2}</td><td>{63.0}</td><td>{41.1}</td><td>{61.5}</td><td>\\underline{73.9}</td></tr><tr><td>\\mathcal{H}-Deformable-DETR{}^{\\dagger}</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>\u2713</td><td>900</td><td>36</td><td>\\underline{57.6}</td><td>\\underline{76.5}</td><td>\\underline{63.2}</td><td>\\underline{41.4}</td><td>\\underline{61.7}</td><td>\\underline{73.9}</td></tr><tr><td colspan=\"12\">MS: multi-scale features. TS: two-stage. {\\dagger}: keep 300 instead of 100 predictions for evaluation. #query: the number of queries used during evaluation.</td></tr></table>", "caption": "Table 4: Comparison results with previous DETR variants on COCO 2017 val.", "list_citation_info": ["[73] G. Zhang, Z. Luo, Y. Yu, K. Cui, and S. Lu. Accelerating DETR convergence via semantic-aligned matching. In CVPR, 2022.", "[22] F. Li, H. Zhang, S. Liu, J. Guo, L. M. Ni, and L. Zhang. Dn-detr: Accelerate detr training by introducing query denoising. arXiv preprint arXiv:2203.01305, 2022.", "[44] D. Meng, X. Chen, Z. Fan, G. Zeng, H. Li, Y. Yuan, L. Sun, and J. Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021.", "[13] P. Gao, M. Zheng, X. Wang, J. Dai, and H. Li. Fast convergence of detr with spatially modulated co-attention. In ICCV, pages 3621\u20133630, 2021.", "[14] Z. Gao, L. Wang, B. Han, and S. Guo. Adamixer: A fast-converging query-based object detector. In CVPR, 2022.", "[34] S. Liu, F. Li, H. Zhang, X. Yang, X. Qi, H. Su, J. Zhu, and L. Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.", "[62] Y. Wang, X. Zhang, T. Yang, and J. Sun. Anchor detr: Query design for transformer-based detector, 2021.", "[68] Z. Yao, J. Ai, B. Li, and C. Zhang. Efficient detr: improving end-to-end object detector with dense prior. arXiv preprint arXiv:2104.01318, 2021.", "[81] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[74] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni, and H.-Y. Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.", "[4] X. Cao, P. Yuan, B. Feng, K. Niu, and Y. Zhao. Cf-detr: Coarse-to-fine transformers for end-to-end object detection. In AAAI, 2022.", "[52] B. Roh, J. Shin, W. Shin, and S. Kim. Sparse detr: Efficient end-to-end object detection with learnable sparsity. arXiv preprint arXiv:2111.14330, 2021.", "[10] X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, and L. Zhang. Dynamic detr: End-to-end object detection with dynamic attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2988\u20132997, 2021."]}]}