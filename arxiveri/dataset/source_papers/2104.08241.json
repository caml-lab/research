{"title": "Spatial-temporal correlation and topology learning for person re-identification in videos", "abstract": "Video-based person re-identification aims to match pedestrians from video sequences across non-overlapping camera views. The key factor for video person re-identification is to effectively exploit both spatial and temporal clues from video sequences. In this work, we propose a novel Spatial-Temporal Correlation and Topology Learning framework (CTL) to pursue discriminative and robust representation by modeling cross-scale spatial-temporal correlation. Specifically, CTL utilizes a CNN backbone and a key-points estimator to extract semantic local features from human body at multiple granularities as graph nodes. It explores a context-reinforced topology to construct multi-scale graphs by considering both global contextual information and physical connections of human body. Moreover, a 3D graph convolution and a cross-scale graph convolution are designed, which facilitate direct cross-spacetime and cross-scale information propagation for capturing hierarchical spatial-temporal dependencies and structural information. By jointly performing the two convolutions, CTL effectively mines comprehensive clues that are complementary with appearance information to enhance representational capacity. Extensive experiments on two video benchmarks have demonstrated the effectiveness of the proposed method and the state-of-the-art performance.", "authors": ["Jiawei Liu", " Zheng-Jun Zha", " Wei Wu", " Kecheng Zheng", " Qibin Sun"], "pdf_url": "https://arxiv.org/abs/2104.08241", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Rank-1</td><td>Rank-5</td><td>Rank-20</td><td>mAP</td></tr><tr><td>CNN+XQDA [52]</td><td>68.3</td><td>82.6</td><td>89.4</td><td>49.3</td></tr><tr><td>TriNet [13]</td><td>79.8</td><td>91.36</td><td>-</td><td>67.7</td></tr><tr><td>STAL[3]</td><td>82.2</td><td>92.8</td><td>98.0</td><td>73.5</td></tr><tr><td>STAN [21]</td><td>82.3</td><td>-</td><td>-</td><td>65.8</td></tr><tr><td>COSAM[37]</td><td>84.9</td><td>95.5</td><td>97.9</td><td>79.9</td></tr><tr><td>VRSTC [15]</td><td>88.5</td><td>96.5</td><td>97.4</td><td>82.3</td></tr><tr><td>RGSAT [22]</td><td>89.4</td><td>96.9</td><td>98.3</td><td>84.0</td></tr><tr><td>AGRL [43]</td><td>89.8</td><td>96.1</td><td>97.6</td><td>81.1</td></tr><tr><td>TCLNet [14]</td><td>89.8</td><td>-</td><td>-</td><td>85.1</td></tr><tr><td>STGCN [46]</td><td>90.0</td><td>96.4</td><td>98.3</td><td>83.7</td></tr><tr><td>MGH [44]</td><td>90.0</td><td>96.7</td><td>98.5</td><td>85.8</td></tr><tr><td>AP3D[11]</td><td>90.1</td><td>-</td><td>-</td><td>85.1</td></tr><tr><td>AFA [4]</td><td>90.2</td><td>96.6</td><td>-</td><td>82.9</td></tr><tr><td>CTL</td><td>91.4</td><td>96.8</td><td>98.5</td><td>86.7</td></tr></tbody></table>", "caption": "Table 1: Performance comparison to the state-of-the-art methods on MARS dataset.", "list_citation_info": ["[21] Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang. Diversity regularized spatiotemporal attention for video-based person re-identification. In CVPR, pages 369\u2013378, 2018.", "[22] Xingze Li, Wengang Zhou, Yun Zhou, and Houqiang Li. Relation-guided spatial attention and temporal refinement for video-based person re-identification. In AAAI, pages 11434\u201311441, 2020.", "[15] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Vrstc: Occlusion-free video person re-identification. In CVPR, pages 7183\u20137192, 2019.", "[52] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, and Qi Tian. Mars: A video benchmark for large-scale person re-identification. In ECCV, pages 868\u2013884. Springer, 2016.", "[46] Jinrui Yang, Wei-Shi Zheng, Qize Yang, Ying-Cong Chen, and Qi Tian. Spatial-temporal graph convolutional network for video-based person re-identification. In CVPR, pages 3289\u20133299, 2020.", "[44] Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, and Ling Shao. Learning multi-granular hypergraphs for video-based person re-identification. In CVPR, pages 2899\u20132908, 2020.", "[4] Guangyi Chen, Yongming Rao, Jiwen Lu, and Jie Zhou. Temporal coherence or temporal motion: Which is more critical for video-based person re-identification? In ECCV, pages 660\u2013676, 2020.", "[11] Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, and Xilin Chen. Appearance-preserving 3d convolution for video-based person re-identification. In ECCV, 2020.", "[37] Arulkumar Subramaniam, Athira Nambiar, Anurag Mittal, and Anurag Mittal. Co-segmentation inspired attention networks for video-based person re-identification. In ICCV, pages 562\u2013572, 2019.", "[43] Yiming Wu, Omar El Farouk Bourahla, Xi Li, Fei Wu, Qi Tian, and Xue Zhou. Adaptive graph representation learning for video person re-identification. IEEE Transactions on Image Processing, 29:8821\u20138830, 2020.", "[14] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Temporal complementary learning for video person re-identification. In ECCV, pages 600\u2013616, 2020.", "[3] Guangyi Chen, Jiwen Lu, Ming Yang, and Jie Zhou. Spatial-temporal attention-aware learning for video-based person re-identification. IEEE Transactions on Image Processing, 28(9):4192\u20134205, 2019.", "[13] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737, 2017."]}, {"table": "<table><tbody><tr><th>Method</th><td>Rank-1</td><td>Rank-5</td><td>Rank-20</td></tr><tr><th>CNN+XQDA [52]</th><td>53.0</td><td>81.4</td><td>95.1</td></tr><tr><th>RCNet [29]</th><td>58</td><td>84.0</td><td>96.0</td></tr><tr><th>COSAM[37]</th><td>79.6</td><td>95.3</td><td>-</td></tr><tr><th>STAN [21]</th><td>80.2</td><td>-</td><td>-</td></tr><tr><th>STAL[3]</th><td>82.8</td><td>95.3</td><td>98.8</td></tr><tr><th>VRSTC [15]</th><td>83.4</td><td>95.5</td><td>99.5</td></tr><tr><th>AGRL [43]</th><td>83.7</td><td>95.4</td><td>99.5</td></tr><tr><th>MGH [44]</th><td>85.6</td><td>97.1</td><td>99.5</td></tr><tr><th>RGSAT [22]</th><td>86.0</td><td>98.0</td><td>99.4</td></tr><tr><th>TCLNet [14]</th><td>86.6</td><td>-</td><td>-</td></tr><tr><th>AP3D[11]</th><td>86.7</td><td>-</td><td>-</td></tr><tr><th>FGRA [6]</th><td>88.0</td><td>96.7</td><td>99.3</td></tr><tr><th>AFA [4]</th><td>88.5</td><td>96.8</td><td>99.7</td></tr><tr><th>CTL</th><td>89.7</td><td>97.0</td><td>100.0</td></tr></tbody></table>", "caption": "Table 2: Performance comparison to the state-of-the-art methods on iLIDS-VID dataset.", "list_citation_info": ["[21] Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang. Diversity regularized spatiotemporal attention for video-based person re-identification. In CVPR, pages 369\u2013378, 2018.", "[22] Xingze Li, Wengang Zhou, Yun Zhou, and Houqiang Li. Relation-guided spatial attention and temporal refinement for video-based person re-identification. In AAAI, pages 11434\u201311441, 2020.", "[15] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Vrstc: Occlusion-free video person re-identification. In CVPR, pages 7183\u20137192, 2019.", "[6] Zengqun Chen, Zhiheng Zhou, Junchu Huang, Pengyu Zhang, and Bo Li. Frame-guided region-aligned representation for video person re-identification. In AAAI, pages 10591\u201310598, 2020.", "[52] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, and Qi Tian. Mars: A video benchmark for large-scale person re-identification. In ECCV, pages 868\u2013884. Springer, 2016.", "[44] Yichao Yan, Jie Qin, Jiaxin Chen, Li Liu, Fan Zhu, Ying Tai, and Ling Shao. Learning multi-granular hypergraphs for video-based person re-identification. In CVPR, pages 2899\u20132908, 2020.", "[4] Guangyi Chen, Yongming Rao, Jiwen Lu, and Jie Zhou. Temporal coherence or temporal motion: Which is more critical for video-based person re-identification? In ECCV, pages 660\u2013676, 2020.", "[29] Niall McLaughlin, Jesus Martinez Del Rincon, and Paul Miller. Recurrent convolutional network for video-based person re-identification. In CVPR, pages 1325\u20131334, 2016.", "[37] Arulkumar Subramaniam, Athira Nambiar, Anurag Mittal, and Anurag Mittal. Co-segmentation inspired attention networks for video-based person re-identification. In ICCV, pages 562\u2013572, 2019.", "[43] Yiming Wu, Omar El Farouk Bourahla, Xi Li, Fei Wu, Qi Tian, and Xue Zhou. Adaptive graph representation learning for video person re-identification. IEEE Transactions on Image Processing, 29:8821\u20138830, 2020.", "[11] Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, and Xilin Chen. Appearance-preserving 3d convolution for video-based person re-identification. In ECCV, 2020.", "[14] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Temporal complementary learning for video person re-identification. In ECCV, pages 600\u2013616, 2020.", "[3] Guangyi Chen, Jiwen Lu, Ming Yang, and Jie Zhou. Spatial-temporal attention-aware learning for video-based person re-identification. IEEE Transactions on Image Processing, 28(9):4192\u20134205, 2019."]}]}