{"title": "Rethinking transformer-based set prediction for object detection", "abstract": "DETR is a recently proposed Transformer-based method which views object detection as a set prediction problem and achieves state-of-the-art performance but demands extra-long training time to converge. In this paper, we investigate the causes of the optimization difficulty in the training of DETR. Our examinations reveal several factors contributing to the slow convergence of DETR, primarily the issues with the Hungarian loss and the Transformer cross-attention mechanism. To overcome these issues we propose two solutions, namely, TSP-FCOS (Transformer-based Set Prediction with FCOS) and TSP-RCNN (Transformer-based Set Prediction with RCNN). Experimental results show that the proposed methods not only converge much faster than the original DETR, but also significantly outperform DETR and other baselines in terms of detection accuracy.", "authors": ["Zhiqing Sun", " Shengcao Cao", " Yiming Yang", " Kris Kitani"], "pdf_url": "https://arxiv.org/abs/2011.10881", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><th>Backbone</th><td>AP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>\\text{AP}_{\\text{S}}</td><td>\\text{AP}_{\\text{M}}</td><td>\\text{AP}_{\\text{L}}</td></tr><tr><th>RetinaNet [23]</th><th>ResNet-101</th><td>39.1</td><td>59.1</td><td>42.3</td><td>21.8</td><td>42.7</td><td>50.2</td></tr><tr><th>FSAF [45]</th><th>ResNet-101</th><td>40.9</td><td>61.5</td><td>44.0</td><td>24.0</td><td>44.2</td><td>51.3</td></tr><tr><th>FCOS [35]</th><th>ResNet-101</th><td>41.5</td><td>60.7</td><td>45.0</td><td>24.4</td><td>44.8</td><td>51.6</td></tr><tr><th>MAL [19]</th><th>ResNet-101</th><td>43.6</td><td>62.8</td><td>47.1</td><td>25.0</td><td>46.9</td><td>55.8</td></tr><tr><th>RepPoints [40]</th><th>ResNet-101-DCN</th><td>45.0</td><td>66.1</td><td>49.0</td><td>26.6</td><td>48.6</td><td>57.5</td></tr><tr><th>ATSS [43]</th><th>ResNet-101</th><td>43.6</td><td>62.1</td><td>47.4</td><td>26.1</td><td>47.0</td><td>53.6</td></tr><tr><th>ATSS [43]</th><th>ResNet-101-DCN</th><td>46.3</td><td>64.7</td><td>50.4</td><td>27.7</td><td>49.8</td><td>58.4</td></tr><tr><th>Fitness NMS [36]</th><th>ResNet-101</th><td>41.8</td><td>60.9</td><td>44.9</td><td>21.5</td><td>45.0</td><td>57.5</td></tr><tr><th>Libra RCNN [27]</th><th>ResNet-101</th><td>41.1</td><td>62.1</td><td>44.7</td><td>23.4</td><td>43.7</td><td>52.5</td></tr><tr><th>Cascade RCNN [2]</th><th>ResNet-101</th><td>42.8</td><td>62.1</td><td>46.3</td><td>23.7</td><td>45.5</td><td>55.2</td></tr><tr><th>TridentNet [21]</th><th>ResNet-101-DCN</th><td>46.8</td><td>67.6</td><td>51.5</td><td>28.0</td><td>51.2</td><td>60.5</td></tr><tr><th>TSD [33]</th><th>ResNet-101</th><td>43.2</td><td>64.0</td><td>46.9</td><td>24.0</td><td>46.3</td><td>55.8</td></tr><tr><th>Dynamic RCNN [42]</th><th>ResNet-101</th><td>44.7</td><td>63.6</td><td>49.1</td><td>26.0</td><td>47.4</td><td>57.2</td></tr><tr><th>Dynamic RCNN [42]</th><th>ResNet-101-DCN</th><td>46.9</td><td>65.9</td><td>51.3</td><td>28.1</td><td>49.6</td><td>60.0</td></tr><tr><th>TSP-RCNN</th><th>ResNet-101</th><td>46.6</td><td>66.2</td><td>51.3</td><td>28.4</td><td>49.0</td><td>58.5</td></tr><tr><th>TSP-RCNN</th><th>ResNet-101-DCN</th><td>47.4</td><td>66.7</td><td>51.9</td><td>29.0</td><td>49.7</td><td>59.1</td></tr></tbody></table>", "caption": "Table 4: Comparison with state-of-the-art models on COCO 2017 test set (single-model and single-scale results). Underlined and bold numbers represent the best model with ResNet-101 and ResNet-101-DCN backbone, respectively.", "list_citation_info": ["[35] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE international conference on computer vision, pages 9627\u20139636, 2019.", "[43] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9759\u20139768, 2020.", "[21] Yanghao Li, Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. Scale-aware trident networks for object detection. In Proceedings of the IEEE international conference on computer vision, pages 6054\u20136063, 2019.", "[36] Lachlan Tychsen-Smith and Lars Petersson. Improving object localization with fitness nms and bounded iou loss. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6877\u20136885, 2018.", "[23] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.", "[19] Wei Ke, Tianliang Zhang, Zeyi Huang, Qixiang Ye, Jianzhuang Liu, and Dong Huang. Multiple anchor learning for visual object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10206\u201310215, 2020.", "[45] Chenchen Zhu, Yihui He, and Marios Savvides. Feature selective anchor-free module for single-shot object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 840\u2013849, 2019.", "[42] Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. Dynamic r-cnn: Towards high quality object detection via dynamic training. arXiv preprint arXiv:2004.06002, 2020.", "[40] Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen Lin. Reppoints: Point set representation for object detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 9657\u20139666, 2019.", "[27] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards balanced learning for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 821\u2013830, 2019.", "[33] Guanglu Song, Yu Liu, and Xiaogang Wang. Revisiting the sibling head in object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11563\u201311572, 2020.", "[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154\u20136162, 2018."]}, {"table": "<table><tbody><tr><th>Model</th><th>Backbone</th><td>AP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>\\text{AP}_{\\text{S}}</td><td>\\text{AP}_{\\text{M}}</td><td>\\text{AP}_{\\text{L}}</td></tr><tr><th>Faster RCNN [31]</th><th>ResNet-101</th><td>36.2</td><td>59.1</td><td>39.0</td><td>18.2</td><td>39.0</td><td>48.2</td></tr><tr><th>Fitness NMS [36]</th><th>ResNet-101</th><td>41.8</td><td>60.9</td><td>44.9</td><td>21.5</td><td>45.0</td><td>57.5</td></tr><tr><th>Libra RCNN [27]</th><th>ResNet-101</th><td>41.1</td><td>62.1</td><td>44.7</td><td>23.4</td><td>43.7</td><td>52.5</td></tr><tr><th>Cascade RCNN [2]</th><th>ResNet-101</th><td>42.8</td><td>62.1</td><td>46.3</td><td>23.7</td><td>45.5</td><td>55.2</td></tr><tr><th>TridentNet [21]</th><th>ResNet-101-DCN</th><td>46.8</td><td>67.6</td><td>51.5</td><td>28.0</td><td>51.2</td><td>60.5</td></tr><tr><th>TSD [33]</th><th>ResNet-101</th><td>43.2</td><td>64.0</td><td>46.9</td><td>24.0</td><td>46.3</td><td>55.8</td></tr><tr><th>Dynamic RCNN [42]</th><th>ResNet-101</th><td>44.7</td><td>63.6</td><td>49.1</td><td>26.0</td><td>47.4</td><td>57.2</td></tr><tr><th>Dynamic RCNN [42]</th><th>ResNet-101-DCN</th><td>46.9</td><td>65.9</td><td>51.3</td><td>28.1</td><td>49.6</td><td>60.0</td></tr><tr><th>RetinaNet [23]</th><th>ResNet-101</th><td>39.1</td><td>59.1</td><td>42.3</td><td>21.8</td><td>42.7</td><td>50.2</td></tr><tr><th>FSAF [45]</th><th>ResNet-101</th><td>40.9</td><td>61.5</td><td>44.0</td><td>24.0</td><td>44.2</td><td>51.3</td></tr><tr><th>FCOS [35]</th><th>ResNet-101</th><td>41.5</td><td>60.7</td><td>45.0</td><td>24.4</td><td>44.8</td><td>51.6</td></tr><tr><th>MAL [19]</th><th>ResNet-101</th><td>43.6</td><td>62.8</td><td>47.1</td><td>25.0</td><td>46.9</td><td>55.8</td></tr><tr><th>RepPoints [40]</th><th>ResNet-101-DCN</th><td>45.0</td><td>66.1</td><td>49.0</td><td>26.6</td><td>48.6</td><td>57.5</td></tr><tr><th>ATSS [43]</th><th>ResNet-101</th><td>43.6</td><td>62.1</td><td>47.4</td><td>26.1</td><td>47.0</td><td>53.6</td></tr><tr><th>ATSS [43]</th><th>ResNet-101-DCN</th><td>46.3</td><td>64.7</td><td>50.4</td><td>27.7</td><td>49.8</td><td>58.4</td></tr><tr><th>TSP-FCOS</th><th>ResNet-101</th><td>46.1</td><td>65.8</td><td>50.3</td><td>27.3</td><td>49.0</td><td>58.2</td></tr><tr><th>TSP-FCOS</th><th>ResNet-101-DCN</th><td>46.8</td><td>66.4</td><td>51.0</td><td>27.6</td><td>49.5</td><td>59.0</td></tr></tbody></table>", "caption": "Table 5: Compare TSP-FCOS with state-of-the-art models on COCO 2017 test set (single-model and single-scale results). Underlined and bold numbers represent the best one-stage model with ResNet-101 and ResNet-101-DCN backbone, respectively.", "list_citation_info": ["[35] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE international conference on computer vision, pages 9627\u20139636, 2019.", "[43] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9759\u20139768, 2020.", "[21] Yanghao Li, Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. Scale-aware trident networks for object detection. In Proceedings of the IEEE international conference on computer vision, pages 6054\u20136063, 2019.", "[36] Lachlan Tychsen-Smith and Lars Petersson. Improving object localization with fitness nms and bounded iou loss. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6877\u20136885, 2018.", "[23] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.", "[19] Wei Ke, Tianliang Zhang, Zeyi Huang, Qixiang Ye, Jianzhuang Liu, and Dong Huang. Multiple anchor learning for visual object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10206\u201310215, 2020.", "[45] Chenchen Zhu, Yihui He, and Marios Savvides. Feature selective anchor-free module for single-shot object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 840\u2013849, 2019.", "[42] Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. Dynamic r-cnn: Towards high quality object detection via dynamic training. arXiv preprint arXiv:2004.06002, 2020.", "[40] Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen Lin. Reppoints: Point set representation for object detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 9657\u20139666, 2019.", "[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91\u201399, 2015.", "[27] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards balanced learning for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 821\u2013830, 2019.", "[33] Guanglu Song, Yu Liu, and Xiaogang Wang. Revisiting the sibling head in object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11563\u201311572, 2020.", "[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154\u20136162, 2018."]}]}