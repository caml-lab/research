{"title": "Zero-shot image-to-text generation for visual-semantic arithmetic", "abstract": "Recent text-to-image matching models apply contrastive learning to large corpora of uncurated pairs of images and sentences. While such models can provide a powerful score for matching and subsequent zero-shot tasks, they are not capable of generating caption given an image. In this work, we repurpose such models to generate a descriptive text given an image at inference time, without any further training or tuning steps. This is done by combining the visual-semantic model with a large language model, benefiting from the knowledge in both web-scale models. The resulting captions are much less restrictive than those obtained by supervised captioning methods. Moreover, as a zero-shot learning method, it is extremely flexible and we demonstrate its ability to perform image arithmetic in which the inputs can be either images or text, and the output is a sentence. This enables novel high-level vision capabilities such as comparing two images or solving visual analogy tests. Our code is available at: https://github.com/YoadTew/zero-shot-image-to-text.", "authors": ["Yoad Tewel", " Yoav Shalev", " Idan Schwartz", " Lior Wolf"], "pdf_url": "https://arxiv.org/abs/2111.14447", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><th colspan=\"5\">Supervised Metrics</th><th colspan=\"2\">Diversity Metrics</th><th>Unsupervised Metric</th></tr><tr><th>Method</th><th>B@4</th><th>M</th><th>C</th><th>S</th><th>\\text{CLIP-S}^{\\text{Ref}}</th><th>Vocab</th><th>%Novel</th><th>CLIP-S</th></tr><tr><td>ClipCap [51]</td><td>32.15</td><td>27.1</td><td>108.35</td><td>20.12</td><td>0.81</td><td>1650</td><td>66.4%</td><td>0.77</td></tr><tr><td>CLIP-VL [64]</td><td>40.2</td><td>29.7</td><td>134.2</td><td>23.8</td><td>0.82</td><td>2464</td><td>85.1%</td><td>0.77</td></tr><tr><td>VinVL [78]</td><td>41.0</td><td>31.1</td><td>140.9</td><td>25.2</td><td>0.83</td><td>1125</td><td>77.9%</td><td>0.78</td></tr><tr><td>Ours</td><td>2.6</td><td>11.5</td><td>14.6</td><td>5.5</td><td>0.79</td><td>8681</td><td>100%</td><td>0.87</td></tr></tbody></table>", "caption": "Table 1: For each method, we report supervised metrics (i.e., ones requiring human references): B@1 = BLEU-1, M = METEOR, C = CIDEr, S = SPICE. We also report diversity metrics, which measures the vocabulary size (Vocab), and the number of novel sentences w.r.t the training set (%Novel). Finally, we report semantic relatedness to the image (CLIP-S), and to the human references (\\text{CLIP-S}^{\\text{Ref}}) based on CLIP\u2019s embeddings.", "list_citation_info": ["[51] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021.", "[64] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383, 2021.", "[78] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">Building \\rightarrow Country</th><th colspan=\"3\">Country \\rightarrow Capital</th><th colspan=\"3\">CEO \\rightarrow Company</th><th colspan=\"3\">Food \\rightarrow Country</th><th colspan=\"3\">Leader \\rightarrow Country</th></tr><tr><th>Method</th><th>B@1</th><th>R@5</th><th>C-s</th><th>B@1</th><th>R@5</th><th>C-s</th><th>B@1</th><th>R@5</th><th>C-s</th><th>B@1</th><th>R@5</th><th>C-s</th><th>B@1</th><th>R@5</th><th>C-s</th></tr></thead><tbody><tr><th>ClipCap [51]</th><td>0.003</td><td>0.035</td><td>0.24</td><td>0.0</td><td>0.0</td><td>0.22</td><td>0.004</td><td>0.05</td><td>0.18</td><td>0.0</td><td>0.0</td><td>0.24</td><td>0.008</td><td>0.24</td><td>0.26</td></tr><tr><th>Ours</th><td>0.1</td><td>0.32</td><td>0.7</td><td>0.14</td><td>0.32</td><td>0.68</td><td>0.1</td><td>0.3</td><td>0.64</td><td>0.03</td><td>0.33</td><td>0.66</td><td>0.1</td><td>0.28</td><td>0.68</td></tr></tbody></table>", "caption": "Table 2: Comparison of our method and ClipCap baseline on our novel benchmark for visual relations. B@1 = BLEU-1, R@5 = Recall@5, C-s = CLIP-score.", "list_citation_info": ["[51] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021."]}]}