{"title": "Feature-metric loss for self-supervised learning of depth and egomotion", "abstract": "Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels. In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by $\u03b4_1$ for depth estimation, and significantly outperforms previous method for visual odometry.", "authors": ["Chang Shu", " Kun Yu", " Zhixiang Duan", " Kuiyuan Yang"], "pdf_url": "https://arxiv.org/abs/2007.10603", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Train</th><td colspan=\"4\">The lower the better</td><td colspan=\"3\">The higher the better</td></tr><tr><td>Abs Rel</td><td>Sq Rel</td><td>RMSE</td><td>RMSE log</td><td>\\delta_{1}</td><td>\\delta_{2}</td><td>\\delta_{3}</td></tr><tr><th>SfMLearner [56]</th><th>M</th><td>0.208</td><td>1.768</td><td>6.958</td><td>0.283</td><td>0.678</td><td>0.885</td><td>0.957</td></tr><tr><th>DNC [51]</th><th>M</th><td>0.182</td><td>1.481</td><td>6.501</td><td>0.267</td><td>0.725</td><td>0.906</td><td>0.963</td></tr><tr><th>Vid2Depth [27]</th><th>M</th><td>0.163</td><td>1.240</td><td>6.220</td><td>0.250</td><td>0.762</td><td>0.916</td><td>0.968</td></tr><tr><th>LEGO [50]</th><th>M</th><td>0.162</td><td>1.352</td><td>6.276</td><td>0.252</td><td>0.783</td><td>0.921</td><td>0.969</td></tr><tr><th>GeoNet [52]</th><th>M</th><td>0.155</td><td>1.296</td><td>5.857</td><td>0.233</td><td>0.793</td><td>0.931</td><td>0.973</td></tr><tr><th>DF-Net [57]</th><th>M</th><td>0.150</td><td>1.124</td><td>5.507</td><td>0.223</td><td>0.806</td><td>0.933</td><td>0.973</td></tr><tr><th>DDVO [46]</th><th>M</th><td>0.151</td><td>1.257</td><td>5.583</td><td>0.228</td><td>0.810</td><td>0.936</td><td>0.974</td></tr><tr><th>EPC++ [26]</th><th>M</th><td>0.141</td><td>1.029</td><td>5.350</td><td>0.216</td><td>0.816</td><td>0.941</td><td>0.976</td></tr><tr><th>Struct2Depth [4]</th><th>M</th><td>0.141</td><td>1.036</td><td>5.291</td><td>0.215</td><td>0.816</td><td>0.945</td><td>0.979</td></tr><tr><th>SIGNet [30]</th><th>M</th><td>0.133</td><td>0.905</td><td>5.181</td><td>0.208</td><td>0.825</td><td>0.947</td><td>0.981</td></tr><tr><th>CC [43]</th><th>M</th><td>0.140</td><td>1.070</td><td>5.326</td><td>0.217</td><td>0.826</td><td>0.941</td><td>0.975</td></tr><tr><th>LearnK [17]</th><th>M</th><td>0.128</td><td>0.959</td><td>5.230</td><td>0.212</td><td>0.845</td><td>0.947</td><td>0.976</td></tr><tr><th>DualNet [55]</th><th>M</th><td>0.121</td><td>0.837</td><td>4.945</td><td>0.197</td><td>0.853</td><td>0.955</td><td>0.982</td></tr><tr><th>SuperDepth [40]</th><th>M</th><td>0.116</td><td>1.055</td><td>-</td><td>0.209</td><td>0.853</td><td>0.948</td><td>0.977</td></tr><tr><th>Monodepth2 [15]</th><th>M</th><td>0.115</td><td>0.882</td><td>4.701</td><td>0.190</td><td>0.879</td><td>0.961</td><td>0.982</td></tr><tr><th>Ours</th><th>M</th><td>0.104</td><td>0.729</td><td>4.481</td><td>0.179</td><td>0.893</td><td>0.965</td><td>0.984</td></tr><tr><th>Struct2Depth [4]</th><th>M{{}^{*}}</th><td>0.109</td><td>0.825</td><td>4.750</td><td>0.187</td><td>0.874</td><td>0.958</td><td>0.983</td></tr><tr><th>GLNet [5]</th><th>M{}^{*}</th><td>0.099</td><td>0.796</td><td>4.743</td><td>0.186</td><td>0.884</td><td>0.955</td><td>0.979</td></tr><tr><th>Ours</th><th>M{}^{*}</th><td>0.088</td><td>0.712</td><td>4.137</td><td>0.169</td><td>0.915</td><td>0.965</td><td>0.982</td></tr><tr><th>Dorn [13]</th><th>Sup</th><td>0.099</td><td>0.593</td><td>3.714</td><td>0.161</td><td>0.897</td><td>0.966</td><td>0.986</td></tr><tr><th>BTS [23]</th><th>Sup</th><td>0.091</td><td>0.555</td><td>4.033</td><td>0.174</td><td>0.904</td><td>0.967</td><td>0.984</td></tr><tr><th>MonoDepth [16]</th><th>S</th><td>0.133</td><td>1.142</td><td>5.533</td><td>0.230</td><td>0.830</td><td>0.936</td><td>0.970</td></tr><tr><th>MonoDispNet [48]</th><th>S</th><td>0.126</td><td>0.832</td><td>4.172</td><td>0.217</td><td>0.840</td><td>0.941</td><td>0.973</td></tr><tr><th>MonoResMatch [44]</th><th>S</th><td>0.111</td><td>0.867</td><td>4.714</td><td>0.199</td><td>0.864</td><td>0.954</td><td>0.979</td></tr><tr><th>MonoDepth2 [15]</th><th>S</th><td>0.107</td><td>0.849</td><td>4.764</td><td>0.201</td><td>0.874</td><td>0.953</td><td>0.977</td></tr><tr><th>RefineDistill [41]</th><th>S</th><td>0.098</td><td>0.831</td><td>4.656</td><td>0.202</td><td>0.882</td><td>0.948</td><td>0.973</td></tr><tr><th>UnDeepVO [24]</th><th>MS</th><td>0.183</td><td>1.730</td><td>6.570</td><td>0.268</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DFR [53]</th><th>MS</th><td>0.135</td><td>1.132</td><td>5.585</td><td>0.229</td><td>0.820</td><td>0.933</td><td>0.971</td></tr><tr><th>EPC++ [26]</th><th>MS</th><td>0.128</td><td>0.935</td><td>5.011</td><td>0.209</td><td>0.831</td><td>0.945</td><td>0.979</td></tr><tr><th>MonoDepth2 [15]</th><th>MS</th><td>0.106</td><td>0.818</td><td>4.750</td><td>0.196</td><td>0.874</td><td>0.957</td><td>0.979</td></tr><tr><th>DepthHint [47]</th><th>MS{}^{\\dagger}</th><td>0.100</td><td>0.728</td><td>4.469</td><td>0.185</td><td>0.885</td><td>0.962</td><td>0.982</td></tr><tr><th>Ours</th><th>MS</th><td>0.099</td><td>0.697</td><td>4.427</td><td>0.184</td><td>0.889</td><td>0.963</td><td>0.982</td></tr><tr><th>Ours</th><th>MS{}^{*}</th><td>0.079</td><td>0.666</td><td>3.922</td><td>0.163</td><td>0.925</td><td>0.970</td><td>0.984</td></tr></tbody></table>", "caption": "Table 2: Comparison of performances are reported on the KITTI dataset.Best results are in bold, second best are underlined.M: trained on monocular videos.S: trained on stereo pairs.MS: trained on calibrated binocular videos.Sup: trained on labelled single images.*: using the online refinement technique [4], which advocated keeping the model training while performing inference.\\dagger: using post processing steps.", "list_citation_info": ["[40] Pillai, S., Ambrus, R., Gaidon, A.: Superdepth: Self-supervised, super-resolved monocular depth estimation. In: ICRA (2019)", "[57] Zou, Y., Luo, Z., Huang, J.B.: Df-net: Unsupervised joint learning of depth and flow using cross-task consistency. In: ECCV (2018)", "[24] Li, R., Wang, S., Long, Z., Gu, D.: Undeepvo: Monocular visual odometry through unsupervised deep learning. In: ICRA (2018)", "[17] Gordon, A., Li, H., Jonschkowski, R., Angelova, A.: Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. In: ICCV (2019)", "[46] Wang, C., Buenaposada, J.M., Zhu, R., Lucey, S.: Learning depth from monocular videos using direct methods. In: CVPR (2018)", "[16] Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth estimation with left-right consistency. In: CVPR (2017)", "[53] Zhan, H., Garg, R., Weerasekera, C.S., Li, K., Agarwal, H., Reid, I.: Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. In: CVPR (2018)", "[30] Meng, Y., Lu, Y., Raj, A., Sunarjo, S., Guo, R., Javidi, T., Bansal, G., Bharadia, D.: Signet: Semantic instance aided unsupervised 3d geometry perception. In: CVPR (2019)", "[26] Luo, C., Yang, Z., Wang, P., Wang, Y., Xu, W., Nevatia, R., Yuille, A.: Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding. arXiv:1810.06125 (2018)", "[43] Ranjan, A., Jampani, V., Kim, K., Sun, D., Wulff, J., Black, M.J.: Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. In: CVPR (2019)", "[47] Watson, J., Firman, M., Brostow, G.J., Turmukhambetov, D.: Self-supervised monocular depth hints. In: ICCV (2019)", "[5] Chen, Y., Schmid, C., Sminchisescu, C.: Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. In: ICCV (2019)", "[4] Casser, V., Pirk, S., Mahjourian, R., Angelova, A.: Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In: AAAI (2019)", "[56] Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and ego-motion from video. In: CVPR (2017)", "[15] Godard, C., Mac Aodha, O., Brostow, G.: Digging into self-supervised monocular depth estimation. In: ICCV (2019)", "[48] Wong, A., Hong, B.W., Soatto, S.: Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction. In: CVPR (2019)", "[50] Yang, Z., Wang, P., Wang, Y., Xu, W., Nevatia, R.: Lego: Learning edge with geometry all at once by watching videos. In: CVPR (2018)", "[51] Yang, Z., Wang, P., Xu, W., Zhao, L., Nevatia, R.: Unsupervised learning of geometry with edge-aware depth-normal consistency. In: AAAI (2018)", "[44] Tosi, F., Aleotti, F., Poggi, M., Mattoccia, S.: Learning monocular depth estimation infusing traditional stereo knowledge. In: CVPR (2019)", "[27] Mahjourian, R., Wicke, M., Angelova, A.: Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. In: CVPR (2018)", "[41] Pilzer, A., Lathuili\u00e8re, S., Sebe, N., Ricci, E.: Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation. In: CVPR (2019)", "[55] Zhou, J., Wang, Y., Qin, K., Zeng, W.: Unsupervised high-resolution depth learning from videos with dual networks. In: ICCV (2019)", "[52] Yin, Z., Shi, J.: GeoNet: Unsupervised learning of dense depth, optical flow and camera pose. In: CVPR (2018)", "[23] Lee, J.H., Han, M.K., Ko, D.W., Suh, I.H.: From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv:1907.10326 (2019)", "[13] Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D.: Deep ordinal regression network for monocular depth estimation. In: CVPR (2018)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">Seq. 09</th><th colspan=\"2\">Seq. 10</th></tr><tr><th>t_{err}</th><th>r_{err}</th><th>t_{err}</th><th>r_{err}</th></tr><tr><th>ORB-SLAM [33]</th><th>15.30</th><th>0.26</th><th>3.68</th><th>0.48</th></tr></thead><tbody><tr><th>SfMLearner [56]</th><td>17.84</td><td>6.78</td><td>37.91</td><td>17.78</td></tr><tr><th>DFR [53]</th><td>11.93</td><td>3.91</td><td>12.45</td><td>3.46</td></tr><tr><th>MonoDepth2 [15]</th><td>10.85</td><td>2.86</td><td>11.60</td><td>5.72</td></tr><tr><th>NeuralBundler [25]</th><td>8.10</td><td>2.81</td><td>12.90</td><td>3.17</td></tr><tr><th>SC-SfMlearner [2]</th><td>8.24</td><td>2.19</td><td>10.70</td><td>4.58</td></tr><tr><th>Ours</th><td>8.75</td><td>2.11</td><td>10.67</td><td>4.91</td></tr></tbody></table>", "caption": "Table 3: Comparison of performances are reported on the KITTI odometry dataset [14].Best results are in bold.", "list_citation_info": ["[53] Zhan, H., Garg, R., Weerasekera, C.S., Li, K., Agarwal, H., Reid, I.: Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. In: CVPR (2018)", "[33] Mur-Artal, R., Montiel, J.M.M., Tardos, J.D.: Orb-slam: a versatile and accurate monocular slam system. TR (2017)", "[14] Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti vision benchmark suite. In: CVPR (2012)", "[15] Godard, C., Mac Aodha, O., Brostow, G.: Digging into self-supervised monocular depth estimation. In: ICCV (2019)", "[25] Li, Y., Ushiku, Y., Harada, T.: Pose graph optimization for unsupervised monocular visual odometry. arXiv:1903.06315 (2019)", "[2] Bian, J.W., Li, Z., Wang, N., Zhan, H., Shen, C., Cheng, M.M., Reid, I.: Unsupervised scale-consistent depth and ego-motion learning from monocular video. NeurIPS (2019)", "[56] Zhou, T., Brown, M., Snavely, N., Lowe, D.G.: Unsupervised learning of depth and ego-motion from video. In: CVPR (2017)"]}]}