{"title": "Depth-Supervised NeRF: Fewer Views and Faster Training for Free", "abstract": "A commonly observed failure mode of Neural Radiance Field (NeRF) is fitting incorrect geometries when given an insufficient number of input views. One potential reason is that standard volumetric rendering does not enforce the constraint that most of a scene's geometry consist of empty space and opaque surfaces. We formalize the above assumption through DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning radiance fields that takes advantage of readily-available depth supervision. We leverage the fact that current NeRF pipelines require images with known camera poses that are typically estimated by running structure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that can be used as \"free\" depth supervision during training: we add a loss to encourage the distribution of a ray's terminating depth matches a given 3D keypoint, incorporating depth uncertainty. DS-NeRF can render better images given fewer training views while training 2-3x faster. Further, we show that our loss is compatible with other recently proposed NeRF methods, demonstrating that depth is a cheap and easily digestible supervisory signal. And finally, we find that DS-NeRF can support other types of depth supervision such as scanned depth sensors and RGB-D reconstruction outputs.", "authors": ["Kangle Deng", " Andrew Liu", " Jun-Yan Zhu", " Deva Ramanan"], "pdf_url": "https://arxiv.org/abs/2107.02791", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"3\">PSNR\\uparrow</td><td colspan=\"3\">SSIM\\uparrow</td><td colspan=\"3\">LPIPS\\downarrow</td></tr><tr><th>NeRF Real [14]</th><td>2-view</td><td>5-view</td><td>10-view</td><td>2-view</td><td>5-view</td><td>10-view</td><td>2-view</td><td>5-view</td><td>10-view</td></tr><tr><th>LLFF</th><td>14.3</td><td>17.6</td><td>22.3</td><td>0.48</td><td>0.49</td><td>0.53</td><td>0.55</td><td>0.51</td><td>0.53</td></tr><tr><th>NeRF</th><td>13.5</td><td>18.2</td><td>22.5</td><td>0.39</td><td>0.57</td><td>0.67</td><td>0.56</td><td>0.50</td><td>0.52</td></tr><tr><th>metaNeRF-DTU</th><td>13.1</td><td>13.8</td><td>14.3</td><td>0.43</td><td>0.45</td><td>0.46</td><td>0.89</td><td>0.88</td><td>0.87</td></tr><tr><th>pixelNeRF-DTU</th><td>9.6</td><td>9.5</td><td>9.7</td><td>0.39</td><td>0.40</td><td>0.40</td><td>0.82</td><td>0.87</td><td>0.81</td></tr><tr><th>\u2003finetuned</th><td>18.2</td><td>22.0</td><td>24.1</td><td>0.56</td><td>0.59</td><td>0.63</td><td>0.53</td><td>0.53</td><td>0.41</td></tr><tr><th>\u2003finetuned w/ DS</th><td>18.9</td><td>22.1</td><td>24.4</td><td>0.54</td><td>0.61</td><td>0.66</td><td>0.55</td><td>0.47</td><td>0.42</td></tr><tr><th>IBRNet</th><td>14.4</td><td>21.8</td><td>24.3</td><td>0.50</td><td>0.51</td><td>0.54</td><td>0.53</td><td>0.54</td><td>0.51</td></tr><tr><th>\u2003finetuned w/ DS</th><td>19.3</td><td>22.3</td><td>24.5</td><td>0.63</td><td>0.66</td><td>0.68</td><td>0.39</td><td>0.36</td><td>0.38</td></tr><tr><th>MVSNeRF</th><td>-</td><td>17.2</td><td>17.2</td><td>-</td><td>0.61</td><td>0.60</td><td>-</td><td>0.37</td><td>0.36</td></tr><tr><th>\u2003fintuned</th><td>-</td><td>21.8</td><td>22.9</td><td>-</td><td>0.70</td><td>0.74</td><td>-</td><td>0.27</td><td>0.23</td></tr><tr><th>\u2003fintuned w/ DS</th><td>-</td><td>22.0</td><td>22.9</td><td>-</td><td>0.70</td><td>0.75</td><td>-</td><td>0.27</td><td>0.25</td></tr><tr><th>DS-NeRF</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>\u2003MSE</th><td>19.5</td><td>22.2</td><td>24.7</td><td>0.65</td><td>0.69</td><td>0.71</td><td>0.43</td><td>0.40</td><td>0.37</td></tr><tr><th>\u2003KL divergence</th><td>20.2</td><td>22.6</td><td>24.9</td><td>0.67</td><td>0.69</td><td>0.72</td><td>0.39</td><td>0.35</td><td>0.34</td></tr></tbody></table>", "caption": "Table 1: View Synthesis on NeRF Real. We evaluate view synthesis quality for various methods when given 2, 5, 10 views from NeRF Real. We find that metaNeRF-DTU and pixelNeRF-DTU struggle to learn on NeRF Real due to its domain gap to DTU. PixelNeRF, IBRNet and MVSNeRF can benefit from incorporating the depth supervision loss to achieve their best performance. We find that our DS-NeRF outperforms these methods on a variety of metrics, but especially for the few view settings like 2 and 5 views.", "list_citation_info": ["[14] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">PSNR\\uparrow</th><th colspan=\"3\">SSIM\\uparrow</th><th colspan=\"3\">LPIPS\\downarrow</th></tr><tr><th>DTU [8]</th><th>3-view</th><th>6-view</th><th>9-view</th><th>3-view</th><th>6-view</th><th>9-view</th><th>3-view</th><th>6-view</th><th>9-view</th></tr></thead><tbody><tr><th>NeRF</th><td>9.9</td><td>18.6</td><td>22.1</td><td>0.37</td><td>0.72</td><td>0.82</td><td>0.62</td><td>0.35</td><td>0.26</td></tr><tr><th>metaNeRF-DTU</th><td>18.2</td><td>18.8</td><td>20.2</td><td>0.60</td><td>0.61</td><td>0.67</td><td>0.40</td><td>0.41</td><td>0.35</td></tr><tr><th>pixelNeRF-DTU</th><td>19.3</td><td>20.4</td><td>21.1</td><td>0.70</td><td>0.73</td><td>0.76</td><td>0.39</td><td>0.36</td><td>0.34</td></tr><tr><th>DS-NeRF</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>\u2003MSE</th><td>16.5</td><td>20.5</td><td>22.2</td><td>0.54</td><td>0.73</td><td>0.77</td><td>0.48</td><td>0.31</td><td>0.26</td></tr><tr><th>\u2003KL divergence</th><td>16.9</td><td>20.6</td><td>22.3</td><td>0.57</td><td>0.75</td><td>0.81</td><td>0.45</td><td>0.29</td><td>0.24</td></tr></tbody></table>", "caption": "Table 2: View Synthesis on DTU. We evaluate on 3, 6, and 9 views respectively for 15 test scenes from the DTU dataset. pixelNeRF-DTU and metaNeRF-DTU perform well given that the domain overlap between training and testing. This is especially true for the few view setting as the lack of information is supplemented by exploiting dataset priors. In spite of this, DS-NeRF is still competitive on view synthesis for 6 and 9 views.", "list_citation_info": ["[8] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aan\u00e6s. Large scale multi-view stereopsis evaluation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">PSNR\\uparrow</th><th colspan=\"3\">SSIM\\uparrow</th><th colspan=\"3\">LPIPS\\downarrow</th></tr><tr><th>Redwood-3dscan [6]</th><th>2-view</th><th>5-view</th><th>10-view</th><th>2-view</th><th>5-view</th><th>10-view</th><th>2-view</th><th>5-view</th><th>10-view</th></tr></thead><tbody><tr><th>NeRF</th><td>10.5</td><td>22.4</td><td>23.4</td><td>0.38</td><td>0.75</td><td>0.82</td><td>0.51</td><td>0.45</td><td>0.45</td></tr><tr><th>metaNeRF-DTU</th><td>14.3</td><td>14.6</td><td>15.1</td><td>0.37</td><td>0.39</td><td>0.40</td><td>0.76</td><td>0.76</td><td>0.75</td></tr><tr><th>pixelNeRF-DTU</th><td>12.7</td><td>12.9</td><td>12.8</td><td>0.43</td><td>0.47</td><td>0.50</td><td>0.76</td><td>0.75</td><td>0.70</td></tr><tr><th>MVSNeRF-DTU</th><td>-</td><td>17.1</td><td>17.1</td><td>-</td><td>0.54</td><td>0.53</td><td>-</td><td>0.63</td><td>0.63</td></tr><tr><th>\u2003finetuned</th><td>-</td><td>22.7</td><td>23.1</td><td>-</td><td>0.78</td><td>0.78</td><td>-</td><td>0.36</td><td>0.34</td></tr><tr><th>DS-NeRF</th><td>18.1</td><td>22.9</td><td>23.8</td><td>0.62</td><td>0.78</td><td>0.81</td><td>0.40</td><td>0.34</td><td>0.42</td></tr><tr><th>DS-NeRF w/ RGB-D</th><td>20.3</td><td>23.4</td><td>23.9</td><td>0.73</td><td>0.77</td><td>0.84</td><td>0.36</td><td>0.35</td><td>0.28</td></tr></tbody></table>", "caption": "Table 3: View Synthesis on Redwood. We evaluate view synthesis on 2, 5, and 10 input views on the Redwood dataset. DS-NeRF (with COLMAP [22] inputs) outperforms baselines on various metrics across varying numbers of views. Learning DS-NeRF with the RGB-D reconstruction output [34] further improves performance, highlighting the potential of applying our method alongside other sources of depth.", "list_citation_info": ["[22] Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "[6] Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun. A large dataset of object scans. arXiv:1602.02481, 2016.", "[34] Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017."]}]}