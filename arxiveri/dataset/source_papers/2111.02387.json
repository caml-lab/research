{"title": "An empirical study of training end-to-end vision-and-language transformers", "abstract": "Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-feature-based methods, their performance on downstream tasks often degrades significantly. In this paper, we present METER, a Multimodal End-to-end TransformER framework, through which we investigate how to design and pre-train a fully transformer-based VL model in an end-to-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments and provide insights on how to train a performant VL transformer. METER achieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-feature-based model by 1.04%, and outperforming the previous best fully transformer-based model by 1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy of 80.54%. Code and pre-trained models are released at https://github.com/zdou0830/METER.", "authors": ["Zi-Yi Dou", " Yichong Xu", " Zhe Gan", " Jianfeng Wang", " Shuohang Wang", " Lijuan Wang", " Chenguang Zhu", " Pengchuan Zhang", " Lu Yuan", " Nanyun Peng", " Zicheng Liu", " Michael Zeng"], "pdf_url": "https://arxiv.org/abs/2111.02387", "list_table_and_caption": [{"table": "<table><tr><td>Model</td><td>Vision Encoder</td><td>Text Encoder</td><td>Multimodal Fusion</td><td>Decoder</td><td>Pre-training Objectives</td></tr><tr><td>ViLBERT [38]</td><td rowspan=\"2\">OD+Xformer</td><td rowspan=\"2\">Xformer</td><td rowspan=\"2\">Co-attn.</td><td rowspan=\"7\">\u2717</td><td>MLM+ITM+MIM</td></tr><tr><td>LXMERT [51]</td><td>MLM+ITM+MIM+VQA</td></tr><tr><td>\\cdashline2-4VisualBERT [30]</td><td rowspan=\"6\">OD</td><td rowspan=\"6\">Emb.</td><td rowspan=\"6\">Merged-attn.</td><td>MLM+ITM</td></tr><tr><td>VL-BERT [49]</td><td>MLM+MIM</td></tr><tr><td>UNITER [6]</td><td>MLM+ITM+MIM+WRA</td></tr><tr><td>OSCAR [32]</td><td>MLM+ITM</td></tr><tr><td>VinVL [65]</td><td>MLM+ITM</td></tr><tr><td>\\cdashline5-5VL-T5 [7]</td><td>\u2713</td><td>MLM+ITM+VQA+Grounding+Captioning</td></tr><tr><td>PixelBERT [20]</td><td rowspan=\"5\">CNN</td><td rowspan=\"4\">Emb.</td><td rowspan=\"5\">Merged-attn.</td><td rowspan=\"3\">\u2717</td><td>MLM+ITM</td></tr><tr><td>SOHO [19]</td><td>MLM+ITM+MIM</td></tr><tr><td>CLIP-ViL [48]</td><td>MLM+ITM+VQA</td></tr><tr><td>\\cdashline5-5SimVLM [58]</td><td rowspan=\"2\">\u2713</td><td>PrefixLM</td></tr><tr><td>\\cdashline3-3MDETR [24]</td><td>Xformer</td><td>OD+Token Prediction+Contrastive Alignment</td></tr><tr><td>ViLT [25]</td><td>Patch Emb.</td><td rowspan=\"2\">Emb.</td><td rowspan=\"2\">Merged-attn.</td><td rowspan=\"4\">\u2717</td><td>MLM+ITM</td></tr><tr><td>\\cdashline2-2Visual Parsing [60]</td><td rowspan=\"3\">Xformer</td><td>MLM+ITM+MIM</td></tr><tr><td>\\cdashline3-4ALBEF [29]</td><td rowspan=\"2\">Xformer</td><td rowspan=\"2\">Co-attn.</td><td>MLM+ITM+ITC</td></tr><tr><td>Meter (Ours)</td><td>MLM+ITM</td></tr><tr><td>CLIP [41]</td><td>CNN/Xformer</td><td rowspan=\"2\">Xformer</td><td rowspan=\"2\">None</td><td rowspan=\"2\">\u2717</td><td rowspan=\"2\">ITC</td></tr><tr><td>ALIGN [22]</td><td>CNN</td></tr></table>", "caption": "Table 1: Glossary of representative VLP models. OD: objective detector. Xformer: transformer. Emb.: embedding. MLM/MIM: masked language/image modeling. ITM: image-text matching. WRA: word-region alginment. ITC: image-text contrastive learning.", "list_citation_info": ["[7] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In International Conference on Machine Learning (ICML), 2021.", "[58] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint, 2021.", "[65] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. VinVL: Revisiting visual representations in vision-language models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: Universal image-text representation learning. In European Conference on Computer Vision (ECCV), 2020.", "[30] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A simple and performant baseline for vision and language. arXiv preprint, 2019.", "[24] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In International Conference on Computer Vision (ICCV), 2021.", "[25] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning (ICML), 2021.", "[48] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint, 2021.", "[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.", "[60] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo. Probing inter-modality: Visual parsing with self-attention for vision-language pre-training. In Conference on Neural Information Processing Systems (NeurIPS), 2021.", "[38] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Conference on Neural Information Processing Systems (NeurIPS), 2019.", "[49] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training of generic visual-linguistic representations. In International Conference on Learning Representations (ICLR), 2019.", "[29] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In Conference on Neural Information Processing Systems (NeurIPS), 2021.", "[19] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out of the box: End-to-end pre-training for vision-language representation learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[22] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint, 2021.", "[32] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision (ECCV), 2020.", "[51] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.", "[20] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-BERT: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint, 2020."]}, {"table": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">VQAv2</td><td colspan=\"2\">NLVR{}^{2}</td><td colspan=\"2\">SNLI-VE</td><td colspan=\"6\">Flickr-ZS</td></tr><tr><td>test-dev</td><td>test-std</td><td>dev</td><td>test</td><td>dev</td><td>test</td><td>IR@1</td><td>IR@5</td><td>IR@10</td><td>TR@1</td><td>TR@5</td><td>TR@10</td></tr><tr><td colspan=\"13\">Pre-trained with &gt;10M images</td></tr><tr><td>ALBEF (14M) [29]</td><td>75.84</td><td>76.04</td><td>82.55</td><td>83.14</td><td>80.80</td><td>80.91</td><td>82.8</td><td>96.3</td><td>98.1</td><td>94.1</td><td>99.5</td><td>99.7</td></tr><tr><td>SimVLM{}_{\\text{BASE}} (1.8B) [58]</td><td>77.87</td><td>78.14</td><td>81.72</td><td>81.77</td><td>84.20</td><td>84.15</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SimVLM{}_{\\text{HUGE}} (1.8B) [58]</td><td>80.03</td><td>80.34</td><td>84.53</td><td>85.15</td><td>86.21</td><td>86.32</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td colspan=\"13\">Pre-trained with &lt;10M images</td></tr><tr><td>UNITER{}_{\\text{LARGE}}  [6]</td><td>73.82</td><td>74.02</td><td>79.12</td><td>79.98</td><td>79.39</td><td>79.38</td><td>68.74</td><td>89.20</td><td>93.86</td><td>83.60</td><td>95.70</td><td>97.70</td></tr><tr><td>VILLA{}_{\\text{LARGE}} [14]</td><td>74.69</td><td>74.87</td><td>79.76</td><td>81.47</td><td>80.18</td><td>80.02</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UNIMO{}_{\\text{LARGE}} [31]</td><td>75.06</td><td>75.27</td><td>-</td><td>-</td><td>81.11</td><td>80.63</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VinVL{}_{\\text{LARGE}} [65]</td><td>76.52</td><td>76.60</td><td>82.67</td><td>83.98</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PixelBERT [20]</td><td>74.45</td><td>74.55</td><td>76.5</td><td>77.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td><td></td></tr><tr><td>CLIP-ViL (ResNet50x4) [48]</td><td>76.48</td><td>76.70</td><td>-</td><td>-</td><td>80.61</td><td>80.20</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ViLT [65]</td><td>71.26</td><td>-</td><td>75.70</td><td>76.13</td><td>-</td><td>-</td><td>55.0</td><td>82.5</td><td>89.8</td><td>73.2</td><td>93.6</td><td>96.5</td></tr><tr><td>Visual Parsing [60]</td><td>74.00</td><td>74.17</td><td>77.61</td><td>78.05</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ALBEF (4M) [29]</td><td>74.54</td><td>74.70</td><td>80.24</td><td>80.50</td><td>80.14</td><td>80.30</td><td>76.8</td><td>93.7</td><td>96.7</td><td>90.5</td><td>98.8</td><td>99.7</td></tr><tr><td>Meter-Swin{}_{\\text{BASE}}</td><td>76.43</td><td>76.42</td><td>82.23</td><td>82.47</td><td>80.61</td><td>80.45</td><td>71.68</td><td>91.80</td><td>95.30</td><td>85.30</td><td>97.70</td><td>99.20</td></tr><tr><td>Meter-CLIP-ViT{}_{\\text{BASE}}</td><td>77.68</td><td>77.64</td><td>82.33</td><td>83.05</td><td>80.86</td><td>81.19</td><td>79.60</td><td>94.96</td><td>97.28</td><td>90.90</td><td>98.30</td><td>99.50</td></tr></table>", "caption": "Table 8: Comparisons with models pre-trained with &lt;10M images on visual question answering, visual reasoning, visual entailment, and zero-shot image retrieval (IR) and text retrieval (TR) tasks. The best scores are in bold, and the second best scores are underlined. ", "list_citation_info": ["[58] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint, 2021.", "[65] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. VinVL: Revisiting visual representations in vision-language models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: Universal image-text representation learning. In European Conference on Computer Vision (ECCV), 2020.", "[31] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021.", "[48] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can clip benefit vision-and-language tasks? arXiv preprint, 2021.", "[60] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo. Probing inter-modality: Visual parsing with self-attention for vision-language pre-training. In Conference on Neural Information Processing Systems (NeurIPS), 2021.", "[29] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In Conference on Neural Information Processing Systems (NeurIPS), 2021.", "[14] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. In Conference on Neural Information Processing Systems (NeurIPS), 2020.", "[20] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-BERT: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint, 2020."]}, {"table": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"6\">Flickr</td><td colspan=\"6\">COCO</td></tr><tr><td>IR@1</td><td>IR@5</td><td>IR@10</td><td>TR@1</td><td>TR@5</td><td>TR@10</td><td>IR@1</td><td>IR@5</td><td>IR@10</td><td>TR@1</td><td>TR@5</td><td>TR@10</td></tr><tr><td colspan=\"13\">Pre-trained with &gt;10M images</td></tr><tr><td>ALBEF (14M) [29]</td><td>85.6</td><td>97.5</td><td>98.9</td><td>95.9</td><td>99.8</td><td>100.0</td><td>60.7</td><td>84.3</td><td>90.5</td><td>77.6</td><td>94.3</td><td>97.2</td></tr><tr><td colspan=\"13\">Pre-trained with &lt;10M images</td></tr><tr><td>UNITER{}_{\\text{LARGE}} [6]</td><td>75.56</td><td>94.08</td><td>96.76</td><td>87.30</td><td>98.00</td><td>99.20</td><td>52.93</td><td>79.93</td><td>87.95</td><td>65.68</td><td>88.56</td><td>93.76</td></tr><tr><td>VILLA{}_{\\text{LARGE}} [14]</td><td>76.26</td><td>94.24</td><td>96.84</td><td>87.90</td><td>97.50</td><td>98.80</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UNIMO{}_{\\text{LARGE}} [31]</td><td>78.04</td><td>94.24</td><td>97.12</td><td>89.40</td><td>98.90</td><td>99.80</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VinVL{}_{\\text{LARGE}} [65]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>58.8</td><td>83.5</td><td>90.3</td><td>75.4</td><td>92.9</td><td>96.2</td></tr><tr><td>PixelBERT [20]</td><td>71.5</td><td>92.1</td><td>95.8</td><td>87.0</td><td>98.9</td><td>99.5</td><td>50.1</td><td>77.6</td><td>86.2</td><td>63.6</td><td>87.5</td><td>93.6</td></tr><tr><td>ViLT [65]</td><td>64.4</td><td>88.7</td><td>93.8</td><td>83.5</td><td>96.7</td><td>98.6</td><td>42.7</td><td>72.9</td><td>83.1</td><td>61.5</td><td>86.3</td><td>92.7</td></tr><tr><td>Visual Parsing [60]</td><td>73.5</td><td>93.1</td><td>96.4</td><td>87.0</td><td>98.4</td><td>99.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ALBEF (4M) [29]</td><td>82.8</td><td>96.7</td><td>98.4</td><td>94.3</td><td>99.4</td><td>99.8</td><td>56.8</td><td>81.5</td><td>89.2</td><td>73.1</td><td>91.4</td><td>96.0</td></tr><tr><td>Meter-Swin{}_{\\text{BASE}}</td><td>79.02</td><td>95.58</td><td>98.04</td><td>92.40</td><td>99.00</td><td>99.50</td><td>54.85</td><td>81.41</td><td>89.31</td><td>72.96</td><td>92.02</td><td>96.26</td></tr><tr><td>Meter-CLIP-ViT{}_{\\text{BASE}}</td><td>82.22</td><td>96.34</td><td>98.36</td><td>94.30</td><td>99.60</td><td>99.90</td><td>57.08</td><td>82.66</td><td>90.07</td><td>76.16</td><td>93.16</td><td>96.82</td></tr></table>", "caption": "Table 9: Comparisons with models pre-trained with &lt;10M images on Flickr30k and COCO image retrieval (IR) and text retrieval (TR) tasks in the finetuning setting. The best scores are in bold, and the second best scores are underlined. ", "list_citation_info": ["[65] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. VinVL: Revisiting visual representations in vision-language models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: Universal image-text representation learning. In European Conference on Computer Vision (ECCV), 2020.", "[31] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021.", "[60] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo. Probing inter-modality: Visual parsing with self-attention for vision-language pre-training. In Conference on Neural Information Processing Systems (NeurIPS), 2021.", "[29] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In Conference on Neural Information Processing Systems (NeurIPS), 2021.", "[14] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. In Conference on Neural Information Processing Systems (NeurIPS), 2020.", "[20] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-BERT: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint, 2020."]}, {"table": "<table><tr><td>Model</td><td>Time ( [25])</td><td>Time (ours)</td><td>VQAv2</td></tr><tr><td>ViLBERT</td><td>920</td><td>-</td><td>70.55</td></tr><tr><td>VisualBERT</td><td>925</td><td>-</td><td>70.80</td></tr><tr><td>LXMERT</td><td>900</td><td>-</td><td>72.42</td></tr><tr><td>UNITER-Base</td><td>900</td><td>-</td><td>72.70</td></tr><tr><td>OSCAR-Base</td><td>900</td><td>-</td><td>73.16</td></tr><tr><td>VinVL-Base</td><td>650</td><td>-</td><td>75.95</td></tr><tr><td>PixelBERT-X152</td><td>160</td><td>-</td><td>74.45</td></tr><tr><td>CLIP-ViL (ResNet50x4)</td><td>-</td><td>57</td><td>76.70</td></tr><tr><td>ViLT</td><td>15</td><td>26</td><td>71.26</td></tr><tr><td>ALBEF (14M)</td><td>-</td><td>52</td><td>76.04</td></tr><tr><td>\\cdashline1-4Meter-Swin{}_{\\text{BASE}}</td><td>-</td><td>59</td><td>76.42</td></tr><tr><td>Meter-CLIP-ViT{}_{\\text{BASE}}</td><td>-</td><td>53</td><td>77.64</td></tr></table>", "caption": "Table 12: Inference time (ms) of different models. We report the inference time measured by [25] and in our setting. We also list the model performance on the VQAv2 test-std set.", "list_citation_info": ["[25] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning (ICML), 2021."]}]}