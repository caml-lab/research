{"title": "Localizing visual sounds the hard way", "abstract": "The objective of this work is to localize sound sources that are visible in a video without using manual annotations. Our key technical contribution is to show that, by training the network to explicitly discriminate challenging image fragments, even for images that do contain the object emitting the sound, we can significantly boost the localization performance. We do so elegantly by introducing a mechanism to mine hard samples and add them to a contrastive learning formulation automatically. We show that our algorithm achieves state-of-the-art performance on the popular Flickr SoundNet dataset. Furthermore, we introduce the VGG-Sound Source (VGG-SS) benchmark, a new set of annotations for the recently-introduced VGG-Sound dataset, where the sound sources visible in each video clip are explicitly marked with bounding box annotations. This dataset is 20 times larger than analogous existing ones, contains 5K videos spanning over 200 categories, and, differently from Flickr SoundNet, is video-based. On VGG-SS, we also show that our algorithm achieves state-of-the-art performance against several baselines.", "authors": ["Honglie Chen", " Weidi Xie", " Triantafyllos Afouras", " Arsha Nagrani", " Andrea Vedaldi", " Andrew Zisserman"], "pdf_url": "https://arxiv.org/abs/2104.02691", "list_table_and_caption": [{"table": "<table><thead><tr><th>Benchmark Datasets</th><th># Data</th><th># Classes</th><th>Video</th><th>BBox</th></tr></thead><tbody><tr><th>Flickr SoundNet [31]</th><th>250</th><td>\\sim50\\ddagger</td><td>\\times</td><td>\\checkmark</td></tr><tr><th>AVE [35]\\dagger</th><th>402</th><td>28</td><td>\\checkmark</td><td>\\times</td></tr><tr><th>LLP [37]\\dagger</th><th>1,200</th><td>25</td><td>\\checkmark</td><td>\\times</td></tr><tr><th>VGG-SS</th><th>5,158</th><td>220</td><td>\\checkmark</td><td>\\checkmark</td></tr></tbody></table>", "caption": "Table 1: Comparison with the existing sound-source localisation benchmrks. Note that VGG-SS has more images and classes. \\daggerThese datasets contain only temporal localisation of sounds, not spatial localisation. \\ddagger We determined this via manual inspection. ", "list_citation_info": ["[37] Dingzeyu Li Yapeng Tian and Chenliang Xu. Unified multisensory perception: Weakly-supervised audio-visual video parsing. In Proc. ECCV, 2020.", "[31] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In Proc. CVPR, 2018.", "[35] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization in unconstrained videos. In Proc. ECCV, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Training set</th><th>CIoU</th><th>AUC</th></tr></thead><tbody><tr><th>Attention10k [31]</th><th>Flickr10k</th><td>0.436</td><td>0.449</td></tr><tr><th>CoarsetoFine [27]</th><th>Flickr10k</th><td>0.522</td><td>0.496</td></tr><tr><th>AVObject [1]</th><th>Flickr10k</th><td>0.546</td><td>0.504</td></tr><tr><th>Ours</th><th>Flickr10k</th><td>0.582</td><td>0.525</td></tr><tr><th>Ours</th><th>VGG-Sound10k</th><td>0.618</td><td>0.536</td></tr><tr><th>\\hdashline</th><th></th><td></td><td></td></tr><tr><th>Attention10k [31]</th><th>Flickr144k</th><td>0.660</td><td>0.558</td></tr><tr><th>DMC [18]</th><th>Flickr144k</th><td>0.671</td><td>0.568</td></tr><tr><th>Ours</th><th>Flickr144k</th><td>0.699</td><td>0.573</td></tr><tr><th>Ours</th><th>VGG-Sound144k</th><td>0.719</td><td>0.582</td></tr><tr><th>Ours</th><th>VGG-Sound Full</th><td>0.735</td><td>0.590</td></tr></tbody></table>", "caption": "Table 3: Quantitative results on Flickr SoundNet testset. We outperform all recent works using different training sets and number of training data.", "list_citation_info": ["[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In Proc. ECCV, 2020.", "[27] Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, and Weiyao Lin. Multiple sound sources localization from coarse to fine. In Proc. ECCV, 2020.", "[31] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In Proc. CVPR, 2018.", "[18] Di Hu, Feiping Nie, and Xuelong Li. Deep multimodal clustering for unsupervised audiovisual learning. In Proc. CVPR, June 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>CIoU</th><th>AUC</th></tr></thead><tbody><tr><th>Attention10k [31]</th><td>0.185</td><td>0.302</td></tr><tr><th>AVobject [1]</th><td>0.297</td><td>0.357</td></tr><tr><th>Ours</th><td>0.344</td><td>0.382</td></tr></tbody></table>", "caption": "Table 5: Quantitative results on the VGG-SS testset. All models are trained on VGG-Sound 144k and tested on VGG-SS.", "list_citation_info": ["[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In Proc. ECCV, 2020.", "[31] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In Proc. CVPR, 2018."]}]}