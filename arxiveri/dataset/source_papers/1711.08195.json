{"title": "On the automatic generation of medical imaging reports", "abstract": "Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time- consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the re- ports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the pre- diction of tags and the generation of para- graphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.", "authors": ["Baoyu Jing", " Pengtao Xie", " Eric Xing"], "pdf_url": "https://arxiv.org/abs/1711.08195", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>Methods</th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>METEOR</th><th>ROUGE</th><th>CIDER</th></tr></thead><tbody><tr><th rowspan=\"8\">IU X-Ray</th><th>CNN-RNN Vinyals et al. (2015)</th><td>0.316</td><td>0.211</td><td>0.140</td><td>0.095</td><td>0.159</td><td>0.267</td><td>0.111</td></tr><tr><th>LRCN Donahue et al. (2015)</th><td>0.369</td><td>0.229</td><td>0.149</td><td>0.099</td><td>0.155</td><td>0.278</td><td>0.190</td></tr><tr><th>Soft ATT Xu et al. (2015)</th><td>0.399</td><td>0.251</td><td>0.168</td><td>0.118</td><td>0.167</td><td>0.323</td><td>0.302</td></tr><tr><th>ATT-RK You et al. (2016)</th><td>0.369</td><td>0.226</td><td>0.151</td><td>0.108</td><td>0.171</td><td>0.323</td><td>0.155</td></tr><tr><th>Ours-no-Attention</th><td>0.505</td><td>0.383</td><td>0.290</td><td>0.224</td><td>0.200</td><td>0.420</td><td>0.259</td></tr><tr><th>Ours-Semantic-only</th><td>0.504</td><td>0.371</td><td>0.291</td><td>0.230</td><td>0.207</td><td>0.418</td><td>0.286</td></tr><tr><th>Ours-Visual-only</th><td>0.507</td><td>0.373</td><td>0.297</td><td>0.238</td><td>0.211</td><td>0.426</td><td>0.300</td></tr><tr><th>Ours-CoAttention</th><td>0.517</td><td>0.386</td><td>0.306</td><td>0.247</td><td>0.217</td><td>0.447</td><td>0.327</td></tr><tr><th rowspan=\"7\">PEIR Gross</th><th>CNN-RNN Vinyals et al. (2015)</th><td>0.247</td><td>0.178</td><td>0.134</td><td>0.092</td><td>0.129</td><td>0.247</td><td>0.205</td></tr><tr><th>LRCN Donahue et al. (2015)</th><td>0.261</td><td>0.184</td><td>0.136</td><td>0.088</td><td>0.135</td><td>0.254</td><td>0.203</td></tr><tr><th>Soft ATT Xu et al. (2015)</th><td>0.283</td><td>0.212</td><td>0.163</td><td>0.113</td><td>0.147</td><td>0.271</td><td>0.276</td></tr><tr><th>ATT-RK You et al. (2016)</th><td>0.274</td><td>0.201</td><td>0.154</td><td>0.104</td><td>0.141</td><td>0.264</td><td>0.279</td></tr><tr><th>Ours-No-Attention</th><td>0.248</td><td>0.180</td><td>0.133</td><td>0.093</td><td>0.131</td><td>0.242</td><td>0.206</td></tr><tr><th>Ours-Semantic-only</th><td>0.263</td><td>0.191</td><td>0.145</td><td>0.098</td><td>0.138</td><td>0.261</td><td>0.274</td></tr><tr><th>Ours-Visual-only</th><td>0.284</td><td>0.209</td><td>0.156</td><td>0.105</td><td>0.149</td><td>0.274</td><td>0.280</td></tr><tr><th></th><th>Ours-CoAttention</th><td>0.300</td><td>0.218</td><td>0.165</td><td>0.113</td><td>0.149</td><td>0.279</td><td>0.329</td></tr></tbody></table>", "caption": "Table 1: Main results for paragraph generation on the IU X-Ray dataset (upper part), and single sentence generation on the PEIR Gross dataset (lower part). BLUE-n denotes the BLEU score that uses up to n-grams.", "list_citation_info": ["Vinyals et al. (2015) Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156\u20133164, 2015.", "You et al. (2016) Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4651\u20134659, 2016.", "Xu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pages 2048\u20132057, 2015.", "Donahue et al. (2015) Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2625\u20132634, 2015."]}]}