{"title": "Highlightme: Detecting highlights from human-centric videos", "abstract": "We present a domain- and user-preference-agnostic approach to detect highlightable excerpts from human-centric videos. Our method works on the graph-based representation of multiple observable human-centric modalities in the videos, such as poses and faces. We use an autoencoder network equipped with spatial-temporal graph convolutions to detect human activities and interactions based on these modalities. We train our network to map the activity- and interaction-based latent structural representations of the different modalities to per-frame highlight scores based on the representativeness of the frames. We use these scores to compute which frames to highlight and stitch contiguous frames to produce the excerpts. We train our network on the large-scale AVA-Kinetics action dataset and evaluate it on four benchmark video highlight datasets: DSH, TVSum, PHD2, and SumMe. We observe a 4-12% improvement in the mean average precision of matching the human-annotated highlights over state-of-the-art methods in these datasets, without requiring any user-provided preferences or dataset-specific fine-tuning.", "authors": ["Uttaran Bhattacharya", " Gang Wu", " Stefano Petrangeli", " Viswanathan Swaminathan", " Dinesh Manocha"], "pdf_url": "https://arxiv.org/abs/2110.01774", "list_table_and_caption": [{"table": "<table><thead><tr><th>Domain</th><th><p>RRAE [57]</p></th><th><p>Video2 GIF [17]</p></th><th><p>LSVM [47]</p></th><th><p>Less is More [53]</p></th><th><p>Ours</p></th></tr></thead><tbody><tr><th>dog show</th><td><p>0.49</p></td><td><p>0.31</p></td><td>0.60</td><td><p>0.58</p></td><td>0.63</td></tr><tr><th>gymnastics</th><td><p>0.35</p></td><td><p>0.34</p></td><td><p>0.41</p></td><td>0.44</td><td>0.73</td></tr><tr><th>parkour</th><td><p>0.50</p></td><td><p>0.54</p></td><td><p>0.61</p></td><td>0.67</td><td>0.72</td></tr><tr><th>skating</th><td><p>0.25</p></td><td><p>0.55</p></td><td>0.62</td><td><p>0.58</p></td><td>0.64</td></tr><tr><th>skiing</th><td><p>0.22</p></td><td><p>0.33</p></td><td><p>0.36</p></td><td>0.49</td><td>0.52</td></tr><tr><th>surfing</th><td><p>0.49</p></td><td><p>0.54</p></td><td><p>0.61</p></td><td>0.65</td><td>0.62</td></tr><tr><th>Mean</th><td><p>0.38</p></td><td><p>0.46</p></td><td><p>0.54</p></td><td>0.57</td><td>0.64</td></tr></tbody></table>", "caption": "Table 1: Mean average precision on the DSH dataset [47]. Bold: best, underline: second-best. Our method performs second-best in the surfing domain, where not enough poses and faces were detected, and best in all the other domains.", "list_citation_info": ["[57] Huan Yang, Baoyuan Wang, Stephen Lin, David Wipf, Minyi Guo, and Baining Guo. Unsupervised extraction of video highlights via robust recurrent auto-encoders. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.", "[53] Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, and Kristen Grauman. Less is more: Learning highlight detection from video duration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1258\u20131267, 2019.", "[47] Min Sun, Ali Farhadi, and Steve Seitz. Ranking domain-specific highlights by analyzing edited videos. In European conference on computer vision, pages 787\u2013802. Springer, 2014.", "[17] Michael Gygli, Yale Song, and Liangliang Cao. Video2gif: Automatic generation of animated gifs from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016."]}, {"table": "<table><tbody><tr><td><p>Random</p></td><td><p>FCSN [44]</p></td><td><p>Video2 GIF [17]</p></td><td><p>Ad-FCSN [42]</p></td><td><p>Ours</p></td></tr><tr><td><p>0.12</p></td><td><p>0.15</p></td><td><p>0.15</p></td><td>0.16</td><td>0.20</td></tr></tbody></table>", "caption": "Table 2: Mean average precision on PHD{}^{2} [11]. Bold: best, underline: second-best.", "list_citation_info": ["[42] Mrigank Rochan, Mahesh Kumar Krishna Reddy, Linwei Ye, and Yang Wang. Adaptive video highlight detection by learning from user history. In Proceedings of the European Conference on Computer Vision (ECCV), August 2020.", "[11] Ana Garcia del Molino and Michael Gygli. Phd-gifs: Personalized highlight detection for automatic gif creation. In Proceedings of the 26th ACM International Conference on Multimedia, MM \u201918, page 600\u2013608, New York, NY, USA, 2018. Association for Computing Machinery.", "[17] Michael Gygli, Yale Song, and Liangliang Cao. Video2gif: Automatic generation of animated gifs from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.", "[44] Mrigank Rochan, Linwei Ye, and Yang Wang. Video summarization using fully convolutional sequence networks. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018."]}, {"table": "<table><thead><tr><th>Domain</th><th><p>MBF [7]</p></th><th><p>KVS [41]</p></th><th><p>CVS [40]</p></th><th><p>Adv-LSTM [36]</p></th><th><p>Less is More [53]</p></th><th><p>Ours</p></th></tr></thead><tbody><tr><th>BK</th><td><p>0.31</p></td><td><p>0.34</p></td><td><p>0.33</p></td><td><p>0.42</p></td><td>0.66</td><td>0.57</td></tr><tr><th>BT</th><td><p>0.37</p></td><td><p>0.42</p></td><td><p>0.40</p></td><td><p>0.48</p></td><td>0.69</td><td>0.93</td></tr><tr><th>DS</th><td><p>0.36</p></td><td><p>0.39</p></td><td><p>0.38</p></td><td><p>0.47</p></td><td>0.63</td><td>0.60</td></tr><tr><th>FM</th><td><p>0.37</p></td><td><p>0.40</p></td><td><p>0.37</p></td><td>0.46</td><td><p>0.43</p></td><td>0.88</td></tr><tr><th>GA</th><td><p>0.33</p></td><td><p>0.40</p></td><td><p>0.38</p></td><td><p>0.48</p></td><td>0.61</td><td>0.50</td></tr><tr><th>MS</th><td><p>0.41</p></td><td><p>0.42</p></td><td><p>0.40</p></td><td><p>0.49</p></td><td>0.54</td><td>0.50</td></tr><tr><th>PR</th><td><p>0.33</p></td><td><p>0.40</p></td><td><p>0.38</p></td><td><p>0.47</p></td><td>0.53</td><td>0.84</td></tr><tr><th>PK</th><td><p>0.32</p></td><td><p>0.38</p></td><td><p>0.35</p></td><td><p>0.46</p></td><td>0.60</td><td>0.76</td></tr><tr><th>VT</th><td><p>0.30</p></td><td><p>0.35</p></td><td><p>0.33</p></td><td><p>0.42</p></td><td>0.56</td><td>0.65</td></tr><tr><th>VU</th><td><p>0.36</p></td><td><p>0.44</p></td><td><p>0.41</p></td><td><p>0.47</p></td><td>0.50</td><td>0.77</td></tr><tr><th>Mean</th><td><p>0.35</p></td><td><p>0.40</p></td><td><p>0.37</p></td><td><p>0.46</p></td><td>0.58</td><td>0.70</td></tr></tbody></table>", "caption": "Table 3: Mean average precision on the TVSum dataset [46]. Full domain names are in Section 5.1. Bold: best, underline: second-best. Our method performs second-best in the domains that are not fully human-centric (BK, DS, GA, MS), and best in all the other domains.", "list_citation_info": ["[53] Bo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, and Kristen Grauman. Less is more: Learning highlight detection from video duration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1258\u20131267, 2019.", "[7] Wen-Sheng Chu, Yale Song, and Alejandro Jaimes. Video co-summarization: Video summarization by visual co-occurrence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "[41] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid. Category-specific video summarization. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision \u2013 ECCV 2014, pages 540\u2013555, Cham, 2014. Springer International Publishing.", "[46] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web videos using titles. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "[40] Rameswar Panda and Amit K. Roy-Chowdhury. Collaborative summarization of topic-related videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.", "[36] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video summarization with adversarial lstm networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 202\u2013211, 2017."]}, {"table": "<table><tbody><tr><td><p>Int [15]</p></td><td><p>Sub [16]</p></td><td><p>DPP-LSTM [63]</p></td><td><p>GAN-S [36]</p></td><td><p>DRL-S [67]</p></td><td><p>S{}^{2}N [52]</p></td><td><p>Ad-FCSN [42]</p></td><td><p>Ours</p></td></tr><tr><td><p>0.39</p></td><td><p>0.40</p></td><td><p>0.39</p></td><td><p>0.42</p></td><td><p>0.42</p></td><td><p>0.43</p></td><td>0.44</td><td>0.48</td></tr></tbody></table>", "caption": "Table 4: F-scores on the SumMe dataset [15]. Bold: best, underline: second-best.", "list_citation_info": ["[42] Mrigank Rochan, Mahesh Kumar Krishna Reddy, Linwei Ye, and Yang Wang. Adaptive video highlight detection by learning from user history. In Proceedings of the European Conference on Computer Vision (ECCV), August 2020.", "[67] Kaiyang Zhou, Yu Qiao, and Tao Xiang. Deep reinforcement learning for unsupervised video summarization with diversity-representativeness reward. pages 7582\u20137589, 2018.", "[16] Michael Gygli, Helmut Grabner, and Luc Van Gool. Video summarization by learning submodular mixtures of objectives. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "[15] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating summaries from user videos. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision \u2013 ECCV 2014, pages 505\u2013520, Cham, 2014. Springer International Publishing.", "[36] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video summarization with adversarial lstm networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 202\u2013211, 2017.", "[52] Zijun Wei, Boyu Wang, Minh Hoai Nguyen, Jianming Zhang, Zhe Lin, Xiaohui Shen, Radomir Mech, and Dimitris Samaras. Sequence-to-segment networks for segment detection. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31, pages 3507\u20133516. Curran Associates, Inc., 2018.", "[63] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. Video summarization with long short-term memory. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision \u2013 ECCV 2016, pages 766\u2013782, Cham, 2016. Springer International Publishing."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Dataset</th><th colspan=\"6\">Using Modality</th></tr><tr><th colspan=\"2\">Face only</th><th colspan=\"2\">Pose only</th><th colspan=\"2\">Both</th></tr><tr><th></th><th>mAP</th><th>F</th><th>mAP</th><th>F</th><th>mAP</th><th>F</th></tr></thead><tbody><tr><th>DSH [47]</th><td>0.51</td><td>0.45</td><td>0.57</td><td>0.48</td><td>0.64</td><td>0.56</td></tr><tr><th>TVSum [46]</th><td>0.57</td><td>0.46</td><td>0.64</td><td>0.56</td><td>0.70</td><td>0.59</td></tr><tr><th>PHD{}^{2} [11]</th><td>0.16</td><td>0.20</td><td>0.15</td><td>0.18</td><td>0.20</td><td>0.22</td></tr><tr><th>SumMe [15]</th><td>0.48</td><td>0.39</td><td>0.45</td><td>0.41</td><td>0.52</td><td>0.48</td></tr></tbody></table>", "caption": "Table 5: Comparison of mean mAP and mean F-score for different ablated versions of our method on benchmark datasets. Bold: best, underline: second-best.", "list_citation_info": ["[46] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web videos using titles. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "[15] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating summaries from user videos. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision \u2013 ECCV 2014, pages 505\u2013520, Cham, 2014. Springer International Publishing.", "[11] Ana Garcia del Molino and Michael Gygli. Phd-gifs: Personalized highlight detection for automatic gif creation. In Proceedings of the 26th ACM International Conference on Multimedia, MM \u201918, page 600\u2013608, New York, NY, USA, 2018. Association for Computing Machinery.", "[47] Min Sun, Ali Farhadi, and Steve Seitz. Ranking domain-specific highlights by analyzing edited videos. In European conference on computer vision, pages 787\u2013802. Springer, 2014."]}]}