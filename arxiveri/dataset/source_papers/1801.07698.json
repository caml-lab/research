{"title": "Arcface: Additive Angular Margin Loss for Deep Face Recognition", "abstract": "Recently, a popular line of research in face recognition is adopting margins in the well-established softmax loss function to maximize class separability. In this paper, we first introduce an Additive Angular Margin Loss (ArcFace), which not only has a clear geometric interpretation but also significantly enhances the discriminative power. Since ArcFace is susceptible to the massive label noise, we further propose sub-center ArcFace, in which each class contains $K$ sub-centers and training samples only need to be close to any of the $K$ positive sub-centers. Sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Based on this self-propelled isolation, we boost the performance through automatically purifying raw web faces under massive real-world noise. Besides discriminative feature embedding, we also explore the inverse problem, mapping feature vectors to face images. Without training any additional generator or discriminator, the pre-trained ArcFace model can generate identity-preserved face images for both subjects inside and outside the training data only by using the network gradient and Batch Normalization (BN) priors. Extensive experiments demonstrate that ArcFace can enhance the discriminative feature embedding as well as strengthen the generative face synthesis.", "authors": ["Jiankang Deng", " Jia Guo", " Jing Yang", " Niannan Xue", " Irene Kotsia", " Stefanos Zafeiriou"], "pdf_url": "https://arxiv.org/abs/1801.07698", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Datasets</td><td>#Identity</td><td>#Image/Video</td></tr><tr><td>CASIA [56]</td><td>10K</td><td>0.5M</td></tr><tr><td>VGG2 [9]</td><td>9.1K</td><td>3.3M</td></tr><tr><td>MS1MV0 [37]</td><td>100K</td><td>10M</td></tr><tr><td>MS1MV3 [88]</td><td>93K</td><td>5.1M</td></tr><tr><td>Celeb500K [38]</td><td>500K</td><td>50M</td></tr><tr><td>IBUG-500K</td><td>493K</td><td>11.96M</td></tr><tr><td>LFW [89]</td><td>5,749</td><td>13,233</td></tr><tr><td>YTF [90]</td><td>1,595</td><td>3,425</td></tr><tr><td>CFP-FP [74]</td><td>500</td><td>7,000</td></tr><tr><td>CPLFW [75]</td><td>5,749</td><td>11,652</td></tr><tr><td>AgeDB [76]</td><td>568</td><td>16,488</td></tr><tr><td>CALFW [77]</td><td>5,749</td><td>12,174</td></tr><tr><td>MegaFace [78]</td><td>530</td><td>1M (D)</td></tr><tr><td>IJB-B [79]</td><td>1,845</td><td>76.8K</td></tr><tr><td>IJB-C [80]</td><td>3,531</td><td>148.8K</td></tr><tr><td>LFR2019-Image [88]</td><td>5.7K</td><td>1.58M(D)</td></tr><tr><td>LFR2019-Video [88]</td><td>10K</td><td>200K</td></tr></tbody></table>", "caption": "TABLE I: Face datasets for training and testing. \u201c(D)\u201d refers to the distractors. IBUG-500K is the training data automatically refined by the proposed sub-center ArcFace. LFR2019-Image and LFR2019-Video are the proposed large-scale image and video test sets.", "list_citation_info": ["[37] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, \u201cMs-celeb-1m: A dataset and benchmark for large-scale face recognition,\u201d in ECCV, 2016.", "[38] J. Cao, Y. Li, and Z. Zhang, \u201cCeleb-500k: A large training dataset for face recognition,\u201d in ICIP, 2018.", "[78] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard, \u201cThe megaface benchmark: 1 million faces for recognition at scale,\u201d in CVPR, 2016.", "[74] S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, and D. W. Jacobs, \u201cFrontal to profile face verification in the wild,\u201d in WACV, 2016.", "[56] D. Yi, Z. Lei, S. Liao, and S. Z. Li, \u201cLearning face representation from scratch,\u201d arXiv:1411.7923, 2014.", "[75] T. Zheng and W. Deng, \u201cCross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments,\u201d Tech. Rep., 2018.", "[9] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVggface2: A dataset for recognising faces across pose and age,\u201d in FG, 2018.", "[80] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto, A. K. Jain, W. T. Niggel, J. Anderson, and J. Cheney, \u201cIarpa janus benchmark\u2013c: Face dataset and protocol,\u201d in ICB, 2018.", "[90] L. Wolf, T. Hassner, and I. Maoz, \u201cFace recognition in unconstrained videos with matched background similarity,\u201d in CVPR, 2011.", "[89] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, \u201cLabeled faces in the wild: A database for studying face recognition in unconstrained environments,\u201d Tech. Rep., 2007.", "[79] C. Whitelam, E. Taborsky, A. Blanton, B. Maze, J. C. Adams, T. Miller, N. D. Kalka, A. K. Jain, J. A. Duncan, and K. Allen, \u201cIarpa janus benchmark-b face dataset.\u201d in CVPR Workshop, 2017.", "[77] T. Zheng, W. Deng, and J. Hu, \u201cCross-age lfw: A database for studying cross-age face recognition in unconstrained environments,\u201d arXiv:1708.08197, 2017.", "[88] J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu, and S. Shi, \u201cLightweight face recognition challenge,\u201d in ICCV Workshop, 2019.", "[76] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou, \u201cAgedb: The first manually collected in-the-wild age database,\u201d in CVPR Workshop, 2017."]}, {"table": "<table><tbody><tr><td>Loss Functions</td><td>LFW</td><td>CFP-FP</td><td>AgeDB</td></tr><tr><td>ArcFace (0.4)</td><td>99.53</td><td>95.41</td><td>94.98</td></tr><tr><td>ArcFace (0.45)</td><td>99.46</td><td>95.47</td><td>94.93</td></tr><tr><td>ArcFace (0.5)</td><td>99.53</td><td>95.56</td><td>95.15</td></tr><tr><td>ArcFace (0.55)</td><td>99.41</td><td>95.32</td><td>95.05</td></tr><tr><td>SphereFace [13]</td><td>99.42</td><td>-</td><td>-</td></tr><tr><td>SphereFace (1.35)</td><td>99.11</td><td>94.38</td><td>91.70</td></tr><tr><td>CosFace [14]</td><td>99.33</td><td>-</td><td>-</td></tr><tr><td>CosFace (0.35)</td><td>99.51</td><td>95.44</td><td>94.56</td></tr><tr><td>CM1 (1, 0.3, 0.2)</td><td>99.48</td><td>95.12</td><td>94.38</td></tr><tr><td>CM2 (0.9, 0.4, 0.15)</td><td>99.50</td><td>95.24</td><td>94.86</td></tr><tr><td>Softmax</td><td>99.08</td><td>94.39</td><td>92.33</td></tr><tr><td>Norm-Softmax (s=64)</td><td>98.56</td><td>89.79</td><td>88.72</td></tr><tr><td>Norm-Softmax (s=20)</td><td>99.20</td><td>94.61</td><td>92.65</td></tr><tr><td>Norm-Softmax+Intra</td><td>99.30</td><td>94.85</td><td>93.58</td></tr><tr><td>Norm-Softmax+Inter</td><td>99.22</td><td>94.73</td><td>92.94</td></tr><tr><td>Norm-Softmax+Intra+Inter</td><td>99.31</td><td>94.88</td><td>93.76</td></tr><tr><td>Triplet (0.35)</td><td>98.98</td><td>91.90</td><td>89.98</td></tr><tr><td>ArcFace+Intra</td><td>99.45</td><td>95.37</td><td>94.73</td></tr><tr><td>ArcFace+Inter</td><td>99.43</td><td>95.25</td><td>94.55</td></tr><tr><td>ArcFace+Intra+Inter</td><td>99.43</td><td>95.42</td><td>95.10</td></tr><tr><td>ArcFace+Triplet</td><td>99.50</td><td>95.51</td><td>94.40</td></tr></tbody></table>", "caption": "TABLE II: Verification results (\\%) of different loss functions ([CASIA, ResNet50, Loss*]).", "list_citation_info": ["[13] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u201cSphereface: Deep hypersphere embedding for face recognition,\u201d in CVPR, 2017.", "[14] H. Wang, Y. Wang, Z. Zhou, X. Ji, Z. Li, D. Gong, J. Zhou, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in CVPR, 2018."]}, {"table": "<table><tbody><tr><td>Settings</td><td>IJB-B</td><td>IJB-C</td></tr><tr><td>(1) MS1MV0,K=1</td><td>87.87</td><td>90.27</td></tr><tr><td>(2) MS1MV0,K=3</td><td>91.70</td><td>93.72</td></tr><tr><td>(3) MS1MV0,K=3, softmax pooling [62]</td><td>91.53</td><td>93.55</td></tr><tr><td>(4) MS1MV0,K=5</td><td>91.47</td><td>93.62</td></tr><tr><td>(5) MS1MV0,K=10</td><td>63.84</td><td>67.94</td></tr><tr><td>(6) MS1MV0, K=3\\downarrow 1, drop &gt;70^{\\circ}</td><td>94.44</td><td>95.91</td></tr><tr><td>(7) MS1MV0, K=3\\downarrow 1, drop &gt;75^{\\circ}</td><td>94.56</td><td>95.92</td></tr><tr><td>(8) MS1MV0, K=3\\downarrow 1, drop &gt;80^{\\circ}</td><td>94.04</td><td>95.74</td></tr><tr><td>(9) MS1MV0, K=3\\downarrow 1, drop &gt;85^{\\circ}</td><td>93.33</td><td>95.10</td></tr><tr><td>(10) MS1MV0, K=3, regularizer [62]</td><td>91.53</td><td>93.64</td></tr><tr><td>(11) MS1MV0,Co-mining [21]</td><td>91.80</td><td>93.82</td></tr><tr><td>(12) MS1MV0,NT [19]</td><td>91.57</td><td>93.65</td></tr><tr><td>(13) MS1MV0,NR [20]</td><td>91.58</td><td>93.60</td></tr><tr><td>(14) MS1MV3, K=1</td><td>95.13</td><td>96.50</td></tr><tr><td>(15) MS1MV3, K=3</td><td>94.84</td><td>96.35</td></tr><tr><td>(16) MS1MV3, K=3\\downarrow 1</td><td>94.87</td><td>96.43</td></tr><tr><td>(17) Celeb500K, K=1</td><td>90.96</td><td>92.15</td></tr><tr><td>(18) Celeb500K, K=3</td><td>93.76</td><td>94.90</td></tr><tr><td>(19) Celeb500K, K=3\\downarrow 1</td><td>95.65</td><td>96.91</td></tr></tbody></table>", "caption": "TABLE III: Ablation experiments of different settings of the proposed sub-center ArcFace on MS1MV0, MS1MV3 and Celeb500K. The 1:1 verification accuracy (TPR@FPR=1e-4) is reported on the IJB-B and IJB-C datasets. ([MS1MV0 / MS1MV3 / Celeb500K, ResNet50, Sub-center ArcFace])", "list_citation_info": ["[21] X. Wang, S. Wang, J. Wang, H. Shi, and T. Mei, \u201cCo-mining: Deep face recognition with noisy labels,\u201d in ICCV, 2019.", "[20] Y. Zhong, W. Deng, M. Wang, J. Hu, J. Peng, X. Tao, and Y. Huang, \u201cUnequal-training for deep face recognition with long-tailed noisy data,\u201d in CVPR, 2019.", "[62] Q. Qian, L. Shang, B. Sun, J. Hu, H. Li, and R. Jin, \u201cSofttriple loss: Deep metric learning without triplet sampling,\u201d in ICCV, 2019.", "[19] W. Hu, Y. Huang, F. Zhang, and R. Li, \u201cNoise-tolerant paradigm for training face recognition cnns,\u201d in CVPR, 2019."]}, {"table": "<table><tbody><tr><td>Method</td><td>#Image</td><td>LFW</td><td>YTF</td></tr><tr><td>DeepID [1]</td><td>0.2M</td><td>99.47</td><td>93.20</td></tr><tr><td>Deep Face [2]</td><td>4.4M</td><td>97.35</td><td>91.4</td></tr><tr><td>VGG Face [4]</td><td>2.6M</td><td>98.95</td><td>97.30</td></tr><tr><td>FaceNet [3]</td><td>200M</td><td>99.63</td><td>95.10</td></tr><tr><td>Baidu [95]</td><td>1.3M</td><td>99.13</td><td>-</td></tr><tr><td>Center Loss [72]</td><td>0.7M</td><td>99.28</td><td>94.9</td></tr><tr><td>Range Loss [73]</td><td>5M</td><td>99.52</td><td>93.70</td></tr><tr><td>Marginal Loss [17]</td><td>3.8M</td><td>99.48</td><td>95.98</td></tr><tr><td>SphereFace [13]</td><td>0.5M</td><td>99.42</td><td>95.0</td></tr><tr><td>SphereFace+ [84]</td><td>0.5M</td><td>99.47</td><td>-</td></tr><tr><td>CosFace [14]</td><td>5M</td><td>99.73</td><td>97.6</td></tr><tr><td>RegularFace [51]</td><td>3.1M</td><td>99.61</td><td>96.7</td></tr><tr><td>UniformFace [52]</td><td>6.1M</td><td>99.8</td><td>97.7</td></tr><tr><td>DAL [96]</td><td>0.5M</td><td>99.47</td><td>-</td></tr><tr><td>FTL [97]</td><td>5M</td><td>99.55</td><td>-</td></tr><tr><td>Fair Loss [98]</td><td>0.5M</td><td>99.57</td><td>96.2</td></tr><tr><td>Unequal-training [20]</td><td>0.55M</td><td>99.53</td><td>96.04</td></tr><tr><td>Noise-Tolerant [19]</td><td>1M noisy</td><td>99.72</td><td>97.36</td></tr><tr><td>AdaptiveFace [50]</td><td>5M</td><td>99.62</td><td>-</td></tr><tr><td>AFRN [99]</td><td>3.1M</td><td>99.85</td><td>97.1</td></tr><tr><td>PFE [100]</td><td>4.4M</td><td>99.82</td><td>97.36</td></tr><tr><td>DUL [101]</td><td>3.6M</td><td>99.78</td><td>96.78</td></tr><tr><td>RDCFace [102]</td><td>1.7M</td><td>99.80</td><td>97.10</td></tr><tr><td>HPDA [103]</td><td>5M</td><td>99.80</td><td>-</td></tr><tr><td>URFace [104]</td><td>5M</td><td>99.78</td><td>-</td></tr><tr><td>CircleLoss [105]</td><td>3.6M</td><td>99.73</td><td>96.38</td></tr><tr><td>GroupFace [55]</td><td>5.8M</td><td>99.85</td><td>97.8</td></tr><tr><td>BioMetricNet [106]</td><td>3.8M</td><td>99.80</td><td>98.06</td></tr><tr><td>BroadFace [107]</td><td>5.8M</td><td>99.85</td><td>98.0</td></tr><tr><td>IBUG500K,R100,BroadFace</td><td>11.96M</td><td>99.83</td><td>98.03</td></tr><tr><td>MS1MV3, R100, ArcFace</td><td>5.1M</td><td>99.83</td><td>98.02</td></tr><tr><td>IBUG500K, R100, ArcFace</td><td>11.96M</td><td>99.83</td><td>98.01</td></tr></tbody></table>", "caption": "TABLE IV: Verification performance (\\%) of different methods on LFW and YTF. ([Dataset*, ResNet100, ArcFace])", "list_citation_info": ["[2] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, \u201cDeepface: Closing the gap to human-level performance in face verification,\u201d in CVPR, 2014.", "[3] F. Schroff, D. Kalenichenko, and J. Philbin, \u201cFacenet: A unified embedding for face recognition and clustering,\u201d in CVPR, 2015.", "[99] B.-N. Kang, Y. Kim, B. Jun, and D. Kim, \u201cAttentional feature-pair relation networks for accurate face recognition,\u201d in ICCV, 2019.", "[97] X. Yin, X. Yu, K. Sohn, X. Liu, and M. Chandraker, \u201cFeature transfer learning for face recognition with under-represented data,\u201d in CVPR, 2019.", "[72] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, \u201cA discriminative feature learning approach for deep face recognition,\u201d in ECCV, 2016.", "[106] A. Ali, M. Testa, T. Bianchi, and E. Magli, \u201cBiometricnet: deep unconstrained face verification through learning of metrics regularized onto gaussian distributions,\u201d in ECCV, 2020.", "[107] Y. Kim, W. Park, and J. Shin, \u201cBroadface: Looking at tens of thousands of people at once for face recognition,\u201d in ECCV, 2020.", "[101] J. Chang, Z. Lan, C. Cheng, and Y. Wei, \u201cData uncertainty learning in face recognition,\u201d in CVPR, 2020.", "[95] J. Liu, Y. Deng, T. Bai, Z. Wei, and C. Huang, \u201cTargeting ultimate accuracy: Face recognition via deep embedding,\u201d arXiv:1506.07310, 2015.", "[73] X. Zhang, Z. Fang, Y. Wen, Z. Li, and Y. Qiao, \u201cRange loss for deep face recognition with long-tail,\u201d in ICCV, 2017.", "[98] B. Liu, W. Deng, Y. Zhong, M. Wang, J. Hu, X. Tao, and Y. Huang, \u201cFair loss: margin-aware reinforcement learning for deep face recognition,\u201d in ICCV, 2019.", "[102] H. Zhao, X. Ying, Y. Shi, X. Tong, J. Wen, and H. Zha, \u201cRdcface: Radial distortion correction for face recognition,\u201d in CVPR, 2020.", "[105] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y. Wei, \u201cCircle loss: A unified perspective of pair similarity optimization,\u201d in CVPR, 2020.", "[100] Y. Shi and A. K. Jain, \u201cProbabilistic face embeddings,\u201d in ICCV, 2019.", "[103] Q. Wang, T. Wu, H. Zheng, and G. Guo, \u201cHierarchical pyramid diverse attention networks for face recognition,\u201d in CVPR, 2020.", "[1] Y. Sun, Y. Chen, X. Wang, and X. Tang, \u201cDeep learning face representation by joint identification-verification,\u201d in NeurIPS, 2014.", "[96] H. Wang, D. Gong, Z. Li, and W. Liu, \u201cDecorrelated adversarial learning for age-invariant face recognition,\u201d in CVPR, 2019.", "[14] H. Wang, Y. Wang, Z. Zhou, X. Ji, Z. Li, D. Gong, J. Zhou, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in CVPR, 2018.", "[55] Y. Kim, W. Park, M.-C. Roh, and J. Shin, \u201cGroupface: Learning latent groups and constructing group-based representations for face recognition,\u201d in CVPR, 2020.", "[84] W. Liu, R. Lin, Z. Liu, L. Liu, Z. Yu, B. Dai, and L. Song, \u201cLearning towards minimum hyperspherical energy,\u201d in NeurIPS, 2018.", "[19] W. Hu, Y. Huang, F. Zhang, and R. Li, \u201cNoise-tolerant paradigm for training face recognition cnns,\u201d in CVPR, 2019.", "[51] K. Zhao, J. Xu, and M.-M. Cheng, \u201cRegularface: Deep face recognition via exclusive regularization,\u201d in CVPR, 2019.", "[104] Y. Shi, X. Yu, K. Sohn, M. Chandraker, and A. K. Jain, \u201cTowards universal representation learning for deep face recognition,\u201d in CVPR, 2020.", "[20] Y. Zhong, W. Deng, M. Wang, J. Hu, J. Peng, X. Tao, and Y. Huang, \u201cUnequal-training for deep face recognition with long-tailed noisy data,\u201d in CVPR, 2019.", "[4] O. M. Parkhi, A. Vedaldi, and A. Zisserman, \u201cDeep face recognition.\u201d in BMVC, 2015.", "[13] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u201cSphereface: Deep hypersphere embedding for face recognition,\u201d in CVPR, 2017.", "[17] J. Deng, Y. Zhou, and S. Zafeiriou, \u201cMarginal loss for deep face recognition,\u201d in CVPR Workshop, 2017.", "[50] H. Liu, X. Zhu, Z. Lei, and S. Z. Li, \u201cAdaptiveface: Adaptive margin and sampling for face recognition,\u201d in CVPR, 2019.", "[52] Y. Duan, J. Lu, and J. Zhou, \u201cUniformface: Learning deep equidistributed representation for face recognition,\u201d in CVPR, 2019."]}, {"table": "<table><tbody><tr><th>Method</th><td>CFP-FP</td><td>CPLFW</td><td>AgeDB</td><td>CALFW</td></tr><tr><th>Center Loss [72]</th><td>-</td><td>77.48</td><td>-</td><td>85.48</td></tr><tr><th>SphereFace [13]</th><td>-</td><td>81.40</td><td>-</td><td>90.30</td></tr><tr><th>VGGFace2 [9]</th><td>-</td><td>84.00</td><td>-</td><td>90.57</td></tr><tr><th>MV-Softmax [53]</th><td>98.28</td><td>92.83</td><td>97.95</td><td>96.10</td></tr><tr><th>Search-Softmax [108]</th><td>95.64</td><td>89.50</td><td>97.75</td><td>95.40</td></tr><tr><th>FaceGraph [109]</th><td>96.90</td><td>92.27</td><td>97.92</td><td>95.67</td></tr><tr><th>CurricularFace [54]</th><td>98.36</td><td>93.13</td><td>98.37</td><td>96.05</td></tr><tr><th>MS1MV3, R100, ArcFace</th><td>98.79</td><td>93.21</td><td>98.23</td><td>96.02</td></tr><tr><th>IBUG500K, R100, ArcFace</th><td>98.87</td><td>93.43</td><td>98.38</td><td>96.10</td></tr></tbody></table>", "caption": "TABLE V: Verification performance (\\%) of different methods on CFP-FP, CPLFW, AgeDB and CALFW. ([Dataset*, ResNet100, ArcFace])", "list_citation_info": ["[13] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u201cSphereface: Deep hypersphere embedding for face recognition,\u201d in CVPR, 2017.", "[108] X. Wang, S. Wang, C. Chi, S. Zhang, and T. Mei, \u201cLoss function search for face recognition,\u201d in ICML, 2020.", "[53] X. Wang, S. Zhang, S. Wang, T. Fu, H. Shi, and T. Mei, \u201cMis-classified vector guided softmax loss for face recognition,\u201d in AAAI, 2020.", "[9] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVggface2: A dataset for recognising faces across pose and age,\u201d in FG, 2018.", "[109] Y. Zhang, W. Deng, M. Wang, J. Hu, X. Li, D. Zhao, and D. Wen, \u201cGlobal-local gcn: Large-scale label noise cleansing for face recognition,\u201d in CVPR, 2020.", "[54] Y. Huang, Y. Wang, Y. Tai, X. Liu, P. Shen, S. Li, J. Li, and F. Huang, \u201cCurricularface: adaptive curriculum learning loss for deep face recognition,\u201d in CVPR, 2020.", "[72] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, \u201cA discriminative feature learning approach for deep face recognition,\u201d in ECCV, 2016."]}, {"table": "<table><tbody><tr><td>Methods</td><td>Id (\\%)</td><td>Ver (\\%)</td></tr><tr><td>Softmax [13]</td><td>54.85</td><td>65.92</td></tr><tr><td>Contrastive Loss[13, 1]</td><td>65.21</td><td>78.86</td></tr><tr><td>Triplet [13, 3]</td><td>64.79</td><td>78.32</td></tr><tr><td>Center Loss[72]</td><td>65.49</td><td>80.14</td></tr><tr><td>SphereFace [13]</td><td>72.729</td><td>85.561</td></tr><tr><td>CosFace [14]</td><td>77.11</td><td>89.88</td></tr><tr><td>AM-Softmax [15]</td><td>72.47</td><td>84.44</td></tr><tr><td>SphereFace+ [84]</td><td>73.03</td><td>-</td></tr><tr><td>RegularFace [51]</td><td>70.23</td><td>84.07</td></tr><tr><td>CASIA, R50, ArcFace</td><td>77.42</td><td>91.69</td></tr><tr><td>CASIA, R50, ArcFace, R</td><td>91.12</td><td>93.56</td></tr><tr><td>FaceNet [3]</td><td>70.49</td><td>86.47</td></tr><tr><td>CosFace [14]</td><td>82.72</td><td>96.65</td></tr><tr><td>UniformFace [52]</td><td>79.98</td><td>95.36</td></tr><tr><td>RegularFace [51]</td><td>75.61</td><td>91.13</td></tr><tr><td>AdaptiveFace, R [50]</td><td>95.02</td><td>95.61</td></tr><tr><td>MV-Softmax, R [53]</td><td>98.00</td><td>98.31</td></tr><tr><td>P2SGrad,R [48]</td><td>97.25</td><td>-</td></tr><tr><td>Adocos, R [49]</td><td>97.41</td><td>-</td></tr><tr><td>PFE [100]</td><td>78.95</td><td>92.51</td></tr><tr><td>Fair Loss [98]</td><td>77.45</td><td>92.87</td></tr><tr><td>Search-Softmax, R [108]</td><td>96.97</td><td>97.84</td></tr><tr><td>Domain Balancing, R [110]</td><td>96.35</td><td>96.56</td></tr><tr><td>URFace [104]</td><td>78.60</td><td>95.04</td></tr><tr><td>DUL, R [101]</td><td>98.60</td><td>-</td></tr><tr><td>CircleLoss, R [105]</td><td>98.50</td><td>98.73</td></tr><tr><td>CurricularFace, R [54]</td><td>98.25</td><td>98.44</td></tr><tr><td>GroupFace, R [55]</td><td>98.74</td><td>98.79</td></tr><tr><td>MC-FaceGraph, R [109]</td><td>99.02</td><td>98.94</td></tr><tr><td>SST, R [111]</td><td>96.27</td><td>96.96</td></tr><tr><td>BroadFace, R [107]</td><td>98.70</td><td>98.95</td></tr><tr><td>MS1MV3, R100, ArcFace</td><td>80.71</td><td>97.46</td></tr><tr><td>MS1MV3, R100, ArcFace, R</td><td>98.51</td><td>98.74</td></tr><tr><td>IBUG-500K, R100, ArcFace</td><td>81.43</td><td>97.63</td></tr><tr><td>IBUG-500K, R100, ArcFace,R</td><td>98.98</td><td>99.08</td></tr></tbody></table>", "caption": "TABLE VI: Face identification and verification evaluation of different methods on MegaFace Challenge1 using FaceScrub as the probe set. \u201cId\u201d refers to the rank-1 face identification accuracy with 1M distractors, and \u201cVer\u201d refers to the face verification TPR at 10^{-6} FPR. \u201cR\u201d refers to data refinement on both probe set and 1M distractors of MegaFace. ArcFace obtains state-of-the-art performance under both small and large protocols.", "list_citation_info": ["[108] X. Wang, S. Wang, C. Chi, S. Zhang, and T. Mei, \u201cLoss function search for face recognition,\u201d in ICML, 2020.", "[3] F. Schroff, D. Kalenichenko, and J. Philbin, \u201cFacenet: A unified embedding for face recognition and clustering,\u201d in CVPR, 2015.", "[110] D. Cao, X. Zhu, X. Huang, J. Guo, and Z. Lei, \u201cDomain balancing: Face recognition on long-tailed domains,\u201d in CVPR, 2020.", "[72] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, \u201cA discriminative feature learning approach for deep face recognition,\u201d in ECCV, 2016.", "[107] Y. Kim, W. Park, and J. Shin, \u201cBroadface: Looking at tens of thousands of people at once for face recognition,\u201d in ECCV, 2020.", "[15] F. Wang, W. Liu, H. Liu, and J. Cheng, \u201cAdditive margin softmax for face verification,\u201d SPL, 2018.", "[53] X. Wang, S. Zhang, S. Wang, T. Fu, H. Shi, and T. Mei, \u201cMis-classified vector guided softmax loss for face recognition,\u201d in AAAI, 2020.", "[101] J. Chang, Z. Lan, C. Cheng, and Y. Wei, \u201cData uncertainty learning in face recognition,\u201d in CVPR, 2020.", "[98] B. Liu, W. Deng, Y. Zhong, M. Wang, J. Hu, X. Tao, and Y. Huang, \u201cFair loss: margin-aware reinforcement learning for deep face recognition,\u201d in ICCV, 2019.", "[105] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y. Wei, \u201cCircle loss: A unified perspective of pair similarity optimization,\u201d in CVPR, 2020.", "[100] Y. Shi and A. K. Jain, \u201cProbabilistic face embeddings,\u201d in ICCV, 2019.", "[48] X. Zhang, R. Zhao, J. Yan, M. Gao, Y. Qiao, X. Wang, and H. Li, \u201cP2sgrad: Refined gradients for optimizing deep face models,\u201d in CVPR, 2019.", "[14] H. Wang, Y. Wang, Z. Zhou, X. Ji, Z. Li, D. Gong, J. Zhou, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in CVPR, 2018.", "[55] Y. Kim, W. Park, M.-C. Roh, and J. Shin, \u201cGroupface: Learning latent groups and constructing group-based representations for face recognition,\u201d in CVPR, 2020.", "[84] W. Liu, R. Lin, Z. Liu, L. Liu, Z. Yu, B. Dai, and L. Song, \u201cLearning towards minimum hyperspherical energy,\u201d in NeurIPS, 2018.", "[54] Y. Huang, Y. Wang, Y. Tai, X. Liu, P. Shen, S. Li, J. Li, and F. Huang, \u201cCurricularface: adaptive curriculum learning loss for deep face recognition,\u201d in CVPR, 2020.", "[104] Y. Shi, X. Yu, K. Sohn, M. Chandraker, and A. K. Jain, \u201cTowards universal representation learning for deep face recognition,\u201d in CVPR, 2020.", "[51] K. Zhao, J. Xu, and M.-M. Cheng, \u201cRegularface: Deep face recognition via exclusive regularization,\u201d in CVPR, 2019.", "[13] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u201cSphereface: Deep hypersphere embedding for face recognition,\u201d in CVPR, 2017.", "[50] H. Liu, X. Zhu, Z. Lei, and S. Z. Li, \u201cAdaptiveface: Adaptive margin and sampling for face recognition,\u201d in CVPR, 2019.", "[111] H. Du, H. Shi, Y. Liu, J. Wang, Z. Lei, D. Zeng, and T. Mei, \u201cSemi-siamese training for shallow face learning,\u201d in ECCV, 2020.", "[109] Y. Zhang, W. Deng, M. Wang, J. Hu, X. Li, D. Zhao, and D. Wen, \u201cGlobal-local gcn: Large-scale label noise cleansing for face recognition,\u201d in CVPR, 2020.", "[49] X. Zhang, R. Zhao, Y. Qiao, X. Wang, and H. Li, \u201cAdacos: Adaptively scaling cosine logits for effectively learning deep face representations,\u201d in CVPR, 2019.", "[52] Y. Duan, J. Lu, and J. Zhou, \u201cUniformface: Learning deep equidistributed representation for face recognition,\u201d in CVPR, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>IJB-B (\\%)</th><th>IJB-C (\\%)</th></tr></thead><tbody><tr><td>ResNet50 [9]</td><td>78.4</td><td>82.5</td></tr><tr><td>SENet50 [9]</td><td>80.0</td><td>84.0</td></tr><tr><td>MN-vc [113]</td><td>83.1</td><td>86.2</td></tr><tr><td>DCN [94]</td><td>84.9</td><td>88.5</td></tr><tr><td>Crystal Loss [114]</td><td>-</td><td>92.29</td></tr><tr><td>AIM [115]</td><td>-</td><td>89.5</td></tr><tr><td>P2SGrad [48]</td><td>-</td><td>92.25</td></tr><tr><td>Adocos [49]</td><td>-</td><td>92.4</td></tr><tr><td>PFE [100]</td><td>-</td><td>93.3</td></tr><tr><td>MV-Softmax [53]</td><td>93.6</td><td>95.2</td></tr><tr><td>AFRN [99]</td><td>88.5</td><td>93.1</td></tr><tr><td>PFE [100]</td><td>-</td><td>93.25</td></tr><tr><td>DUL [101]</td><td>-</td><td>94.61</td></tr><tr><td>URFace [104]</td><td>-</td><td>96.6</td></tr><tr><td>CircleLoss [105]</td><td>-</td><td>93.95</td></tr><tr><td>CurricularFace [54]</td><td>94.86</td><td>96.15</td></tr><tr><td>GroupFace [55]</td><td>94.93</td><td>96.26</td></tr><tr><td>BroadFace [107]</td><td>94.61</td><td>96.03</td></tr><tr><td>VGG2, R50, ArcFace</td><td>89.8</td><td>92.79</td></tr><tr><td>MS1MV3, R100, ArcFace</td><td>95.42</td><td>96.83</td></tr><tr><td>IBUG-500K, R100, ArcFace</td><td>96.02</td><td>97.27</td></tr></tbody></table>", "caption": "TABLE VII: 1:1 verification (TPR@FPR=1e-4) on IJB-B and IJB-C.", "list_citation_info": ["[107] Y. Kim, W. Park, and J. Shin, \u201cBroadface: Looking at tens of thousands of people at once for face recognition,\u201d in ECCV, 2020.", "[114] R. Ranjan, A. Bansal, H. Xu, S. Sankaranarayanan, J.-C. Chen, C. D. Castillo, and R. Chellappa, \u201cCrystal loss and quality pooling for unconstrained face verification and recognition,\u201d arXiv:1804.01159, 2018.", "[101] J. Chang, Z. Lan, C. Cheng, and Y. Wei, \u201cData uncertainty learning in face recognition,\u201d in CVPR, 2020.", "[53] X. Wang, S. Zhang, S. Wang, T. Fu, H. Shi, and T. Mei, \u201cMis-classified vector guided softmax loss for face recognition,\u201d in AAAI, 2020.", "[94] W. Xie, S. Li, and A. Zisserman, \u201cComparator networks,\u201d in ECCV, 2018.", "[113] W. Xie and A. Zisserman, \u201cMulticolumn networks for face recognition,\u201d in BMVC, 2018.", "[9] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVggface2: A dataset for recognising faces across pose and age,\u201d in FG, 2018.", "[104] Y. Shi, X. Yu, K. Sohn, M. Chandraker, and A. K. Jain, \u201cTowards universal representation learning for deep face recognition,\u201d in CVPR, 2020.", "[115] J. Zhao, Y. Cheng, Y. Cheng, Y. Yang, F. Zhao, J. Li, H. Liu, S. Yan, and J. Feng, \u201cLook across elapse: Disentangled representation learning and photorealistic cross-age face synthesis for age-invariant face recognition,\u201d in AAAI, 2019.", "[55] Y. Kim, W. Park, M.-C. Roh, and J. Shin, \u201cGroupface: Learning latent groups and constructing group-based representations for face recognition,\u201d in CVPR, 2020.", "[105] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y. Wei, \u201cCircle loss: A unified perspective of pair similarity optimization,\u201d in CVPR, 2020.", "[100] Y. Shi and A. K. Jain, \u201cProbabilistic face embeddings,\u201d in ICCV, 2019.", "[49] X. Zhang, R. Zhao, Y. Qiao, X. Wang, and H. Li, \u201cAdacos: Adaptively scaling cosine logits for effectively learning deep face representations,\u201d in CVPR, 2019.", "[99] B.-N. Kang, Y. Kim, B. Jun, and D. Kim, \u201cAttentional feature-pair relation networks for accurate face recognition,\u201d in ICCV, 2019.", "[48] X. Zhang, R. Zhao, J. Yan, M. Gao, Y. Qiao, X. Wang, and H. Li, \u201cP2sgrad: Refined gradients for optimizing deep face models,\u201d in CVPR, 2019.", "[54] Y. Huang, Y. Wang, Y. Tai, X. Liu, P. Shen, S. Li, J. Li, and F. Huang, \u201cCurricularface: adaptive curriculum learning loss for deep face recognition,\u201d in CVPR, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Training Datasets</th><td colspan=\"2\">IJB-B</td><td colspan=\"2\">IJB-C</td></tr><tr><td>Ver.(\\%)</td><td>Id.(\\%)</td><td>Ver.(\\%)</td><td>Id.(\\%)</td></tr><tr><th>CASIA [56]</th><td>62.42</td><td>86.70</td><td>69.61</td><td>88.05</td></tr><tr><th>IMDB-Face [18]</th><td>64.87</td><td>93.41</td><td>66.85</td><td>94.52</td></tr><tr><th>VGG2 [9]</th><td>41.64</td><td>93.20</td><td>59.33</td><td>94.44</td></tr><tr><th>MS1MV1 [17]</th><td>80.27</td><td>92.19</td><td>88.16</td><td>93.54</td></tr><tr><th>MS1MV2 [16]</th><td>89.33</td><td>94.50</td><td>93.15</td><td>95.72</td></tr><tr><th>MC-FaceGraph [109]</th><td>92.82</td><td>95.76</td><td>95.62</td><td>96.93</td></tr><tr><th>MS1MV3</th><td>91.27</td><td>95.04</td><td>95.56</td><td>96.94</td></tr><tr><th>IBUG-500K</th><td>93.48</td><td>95.94</td><td>96.07</td><td>97.21</td></tr></tbody></table>", "caption": "TABLE VIII: 1:1 verification (TPR@FPR=1e-5) and 1:N identification (Rank-1) on IJB-B and IJB-C. ([Dataset*, ResNet100, ArcFace])", "list_citation_info": ["[18] F. Wang, L. Chen, C. Li, S. Huang, Y. Chen, C. Qian, and C. C. Loy, \u201cThe devil of face recognition is in the noise,\u201d in ECCV, 2018.", "[17] J. Deng, Y. Zhou, and S. Zafeiriou, \u201cMarginal loss for deep face recognition,\u201d in CVPR Workshop, 2017.", "[56] D. Yi, Z. Lei, S. Liao, and S. Z. Li, \u201cLearning face representation from scratch,\u201d arXiv:1411.7923, 2014.", "[9] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVggface2: A dataset for recognising faces across pose and age,\u201d in FG, 2018.", "[16] J. Deng, J. Guo, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in CVPR, 2019.", "[109] Y. Zhang, W. Deng, M. Wang, J. Hu, X. Li, D. Zhao, and D. Wen, \u201cGlobal-local gcn: Large-scale label noise cleansing for face recognition,\u201d in CVPR, 2020."]}, {"table": "<table><tbody><tr><th>Methods</th><td>Image</td><td>Video</td></tr><tr><th>YMJ{}^{1} [116]</th><td>88.78</td><td>-</td></tr><tr><th>count{}^{2} [117]</th><td>88.42</td><td>-</td></tr><tr><th>NothingLC{}^{3}</th><td>88.14</td><td>-</td></tr><tr><th>NothingLC{}^{1}</th><td>-</td><td>63.23</td></tr><tr><th>Rhapsody{}^{2}</th><td>-</td><td>61.87</td></tr><tr><th>xfr {}^{3}</th><td>-</td><td>61.05</td></tr><tr><th>Our Method</th><td>88.65</td><td>63.60</td></tr><tr><th>MS1MV3, EfficientNet-B0, ArcFace</th><td>86.44</td><td>61.47</td></tr><tr><th>MS1MV3, R100, ArcFace</th><td>92.75</td><td>64.89</td></tr></tbody></table>", "caption": "TABLE IX: Verification results (\\%) on the LFR2019-Image (TPR@FPR=1e-8) and LFR2019-Video (TPR@FPR=1e-4) datasets. ([Dataset*, Network*, ArcFace])", "list_citation_info": ["[117] X. Li, F. Wang, Q. Hu, and C. Leng, \u201cAirface: lightweight and efficient model for face recognition,\u201d in ICCV Workshops, 2019.", "[116] Q. Zhang, J. Li, M. Yao, L. Song, H. Zhou, Z. Li, W. Meng, X. Zhang, and G. Wang, \u201cVargnet: Variable group convolutional neural network for efficient embedded computing,\u201d arXiv:1907.05653, 2019."]}]}