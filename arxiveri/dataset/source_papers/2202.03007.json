{"title": "Learning sound localization better from semantically similar samples", "abstract": "The objective of this work is to localize the sound sources in visual scenes. Existing audio-visual works employ contrastive learning by assigning corresponding audio-visual pairs from the same source as positives while randomly mismatched pairs as negatives. However, these negative pairs may contain semantically matched audio-visual information. Thus, these semantically correlated pairs, \"hard positives\", are mistakenly grouped as negatives. Our key contribution is showing that hard positives can give similar response maps to the corresponding pairs. Our approach incorporates these hard positives by adding their response maps into a contrastive learning objective directly. We demonstrate the effectiveness of our approach on VGG-SS and SoundNet-Flickr test sets, showing favorable performance to the state-of-the-art methods.", "authors": ["Arda Senocak", " Hyeonggon Ryu", " Junsik Kim", " In So Kweon"], "pdf_url": "https://arxiv.org/abs/2202.03007", "list_table_and_caption": [{"table": "<table><tr><td></td><td colspan=\"2\">VGG-SS</td><td colspan=\"2\">Flickr</td></tr><tr><td>Method</td><td>cIoU</td><td>AUC</td><td>cIoU</td><td>AUC</td></tr><tr><td>Attention [18]</td><td>0.185</td><td>0.302</td><td>0.660</td><td>0.558</td></tr><tr><td>AVEL [31]</td><td>0.291</td><td>0.348</td><td>-</td><td>-</td></tr><tr><td>AVObject [5]</td><td>0.297</td><td>0.357</td><td>-</td><td>-</td></tr><tr><td>Vanilla-LVS</td><td>0.278</td><td>0.350</td><td>0.692</td><td>0.563</td></tr><tr><td>LVS [21]\\dagger</td><td>0.303</td><td>0.364</td><td>0.724</td><td>0.578</td></tr><tr><td>Random HP</td><td>0.207</td><td>0.314</td><td>0.572</td><td>0.528</td></tr><tr><td>Ours</td><td>0.346</td><td>0.380</td><td>0.768</td><td>0.592</td></tr></table>", "caption": "Table 1: Quantitative results on the VGG-SS and SoundNet-Flickr test sets. All models are trained with 144K samples from VGG-Sound and tested on VGG-SS and SoundNet-Flickr. \\dagger is the result of the model released on the official project page and the authors report  3\\% drop in cIoU performance comparing to their paper.", "list_citation_info": ["[18] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon, \u201cLearning to localize sound source in visual scenes,\u201d in CVPR, 2018.", "[5] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman, \u201cSelf-supervised learning of audio-visual objects from video,\u201d in ECCV, 2020.", "[21] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman, \u201cLocalizing visual sounds the hard way,\u201d in CVPR, 2021.", "[31] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu, \u201cAudio-visual event localization in unconstrained videos,\u201d in ECCV, 2018."]}, {"table": "<table><tr><td>Method</td><td>cIoU</td><td>AUC</td></tr><tr><td>Attention[18]</td><td>0.660</td><td>0.558</td></tr><tr><td>Vanilla-LVS</td><td>0.704</td><td>0.581</td></tr><tr><td>LVS [21]\\dagger</td><td>0.672</td><td>0.562</td></tr><tr><td>Ours</td><td>0.752</td><td>0.597</td></tr></table>", "caption": "Table 2: Quantitative results on the SoundNet-Flickr test set. All models are trained and tested on the SoundNet-Flickr dataset. \\dagger is the result of the model from the official project page.", "list_citation_info": ["[18] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon, \u201cLearning to localize sound source in visual scenes,\u201d in CVPR, 2018.", "[21] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman, \u201cLocalizing visual sounds the hard way,\u201d in CVPR, 2021."]}]}