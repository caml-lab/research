{"title": "Ptt: Point-track-transformer module for 3d single object tracking in point clouds", "abstract": "3D single object tracking is a key issue for robotics. In this paper, we propose a transformer module called Point-Track-Transformer (PTT) for point cloud-based 3D single object tracking. PTT module contains three blocks for feature embedding, position encoding, and self-attention feature computation. Feature embedding aims to place features closer in the embedding space if they have similar semantic information. Position encoding is used to encode coordinates of point clouds into high dimension distinguishable features. Self-attention generates refined attention features by computing attention weights. Besides, we embed the PTT module into the open-source state-of-the-art method P2B to construct PTT-Net. Experiments on the KITTI dataset reveal that our PTT-Net surpasses the state-of-the-art by a noticeable margin (~10%). Additionally, PTT-Net could achieve real-time performance (~40FPS) on NVIDIA 1080Ti GPU. Our code is open-sourced for the robotics community at https://github.com/shanjiayao/PTT.", "authors": ["Jiayao Shan", " Sifan Zhou", " Zheng Fang", " Yubo Cui"], "pdf_url": "https://arxiv.org/abs/2108.06455", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Module</td><td>Modality</td><td>3D Success</td><td>3D Precision</td><td>FPS</td></tr><tr><td>AVOD-Tracking[27]</td><td>R+L</td><td>63.1</td><td>69.7</td><td>-</td></tr><tr><td>F-Siamese[21]</td><td>R+L</td><td>37.1</td><td>50.6</td><td>-</td></tr><tr><td>SC3D[10]</td><td>L</td><td>41.3</td><td>57.9</td><td>1.8</td></tr><tr><td>P2B[11]</td><td>L</td><td>56.2</td><td>72.8</td><td>45.5</td></tr><tr><td>3D-SiamRPN[12]</td><td>L</td><td>58.2{}^{2}</td><td>76.2</td><td>20.8</td></tr><tr><td>PTT-Net(Ours)</td><td>L</td><td>67.8{}^{1}</td><td>81.8</td><td>40.0</td></tr></tbody></table><p>{}^{1} {}^{2}Red and blue mean the performance score is ranked first and second respectively.</p>", "caption": "TABLE I: Performance comparison on the KITTI dataset for the car category. R and L denote RGB ans LiDAR respectively.", "list_citation_info": ["[11] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020.", "[12] Z. Fang, S. Zhou, Y. Cui, and S. Scherer, \u201c3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud,\u201d IEEE Sensors Journal, vol. 21, no. 4, pp. 4995\u20135011, 2021.", "[21] H. Zou, J. Cui, X. Kong, C. Zhang, Y. Liu, F. Wen, and W. Li, \u201cF-siamese tracker: A frustum-based double siamese network for 3d single object tracking,\u201d 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8133\u20138139, 2020.", "[27] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. Waslander, \u201cJoint 3d proposal generation and object detection from view aggregation,\u201d in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1\u20138, 2018.", "[10] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1359\u20131368, 2019."]}, {"table": "<table><thead><tr><th></th><th>Method</th><th>The FirstGT</th><th>PreviousResult</th><th>First &amp;Previous</th><th>AllPrevious</th></tr></thead><tbody><tr><th rowspan=\"4\">Success</th><td>SC3D[10]</td><td>31.6</td><td>25.7</td><td>34.9</td><td>41.3</td></tr><tr><td>P2B[11]</td><td>46.7</td><td>53.1</td><td>56.2</td><td>51.4</td></tr><tr><td>3DSiamRPN[12]</td><td>57.2</td><td>-</td><td>58.2</td><td>-</td></tr><tr><td>PTT-Net(Ours)</td><td>62.9</td><td>64.9</td><td>67.8</td><td>59.8</td></tr><tr><th rowspan=\"4\">Precision</th><td>SC3D[10]</td><td>44.4</td><td>35.1</td><td>49.8</td><td>57.9</td></tr><tr><td>P2B[11]</td><td>59.7</td><td>68.9</td><td>72.8</td><td>66.8</td></tr><tr><td>3DSiamRPN[12]</td><td>75.0</td><td>-</td><td>76.2</td><td>-</td></tr><tr><td>PTT-Net(Ours)</td><td>76.5</td><td>77.5</td><td>81.8</td><td>74.5</td></tr></tbody></table>", "caption": "TABLE III: Different ways for template generation. \"GT\" denotes \"ground truth\". \"First &amp; Previous\" denotes \"The first ground truth and Previous result\". ", "list_citation_info": ["[11] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020.", "[10] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1359\u20131368, 2019.", "[12] Z. Fang, S. Zhou, Y. Cui, and S. Scherer, \u201c3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud,\u201d IEEE Sensors Journal, vol. 21, no. 4, pp. 4995\u20135011, 2021."]}, {"table": "<table><thead><tr><th> Ablation</th><th>3D Success</th><th>3D Precision</th></tr></thead><tbody><tr><td>baseline[11]</td><td>56.2</td><td>72.8</td></tr><tr><td>Only PTT in Vote</td><td>62.1</td><td>76.9</td></tr><tr><td>Only PTT in Prop</td><td>65.7</td><td>78.9</td></tr><tr><td>PTT in all(PTT-Net)</td><td>67.8</td><td>81.8</td></tr></tbody></table>", "caption": "TABLE IV: Different embedded positions of PTT module.", "list_citation_info": ["[11] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020."]}]}