{"title": "Elaborative rehearsal for zero-shot action recognition", "abstract": "The growing number of action classes has posed a new challenge for video understanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction. The ZSAR task aims to recognize target (unseen) actions without training examples by leveraging semantic representations to bridge seen and unseen actions. However, due to the complexity and diversity of actions, it remains challenging to semantically represent action classes and transfer knowledge from seen data. In this work, we propose an ER-enhanced ZSAR model inspired by an effective human memory technique Elaborative Rehearsal (ER), which involves elaborating a new concept and relating it to known concepts. Specifically, we expand each action class as an Elaborative Description (ED) sentence, which is more discriminative than a class name and less costly than manual-defined attributes. Besides directly aligning class semantics with videos, we incorporate objects from the video as Elaborative Concepts (EC) to improve video semantics and generalization from seen actions to unseen actions. Our ER-enhanced ZSAR model achieves state-of-the-art results on three existing benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics dataset to overcome limitations of current benchmarks and demonstrate the first case where ZSAR performance is comparable to few-shot learning baselines on this more realistic setting. We will release our codes and collected EDs at https://github.com/DeLightCMU/ElaborativeRehearsal.", "authors": ["Shizhe Chen", " Dong Huang"], "pdf_url": "https://arxiv.org/abs/2108.02833", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>Video</th><th>Class</th><th>Olympics</th><th>HMDB51</th><th>UCF101</th></tr></thead><tbody><tr><th>DAP [26]</th><td>FV</td><td>A</td><td>45.4 \\pm 12.8</td><td>N/A</td><td>15.9 \\pm 1.2</td></tr><tr><th>IAP [26]</th><td>FV</td><td>A</td><td>42.3 \\pm 12.5</td><td>N/A</td><td>16.7 \\pm 1.1</td></tr><tr><th>HAA [29]</th><td>FV</td><td>A</td><td>46.1 \\pm 12.4</td><td>N/A</td><td>14.9 \\pm 0.8</td></tr><tr><th>SVE [49]</th><td>BoW</td><td>W{}_{N}</td><td>N/A</td><td>13.0 \\pm 2.7</td><td>10.9 \\pm 1.5</td></tr><tr><th>ESZSL [37]</th><td>FV</td><td>W{}_{N}</td><td>39.6 \\pm 9.6</td><td>18.5 \\pm 2.0</td><td>15.0 \\pm 1.3</td></tr><tr><th>SJE [2]</th><td>FV</td><td>W{}_{N}</td><td>28.6 \\pm 4.9</td><td>13.3 \\pm 2.4</td><td>9.9 \\pm 1.4</td></tr><tr><th>SJE [2]</th><td>FV</td><td>A</td><td>47.5 \\pm 14.8</td><td>N/A</td><td>12.0 \\pm 1.2</td></tr><tr><th>MTE [51]</th><td>FV</td><td>W{}_{N}</td><td>44.3 \\pm 8.1</td><td>19.7 \\pm 1.6</td><td>15.8 \\pm 1.3</td></tr><tr><th>ZSECOC [35]</th><td>FV</td><td>W{}_{N}</td><td>59.8 \\pm 5.6</td><td>22.6 \\pm 1.2</td><td>15.1 \\pm 1.7</td></tr><tr><th>UR [54]</th><td>FV</td><td>W{}_{N}</td><td>N/A</td><td>24.4 \\pm 1.6</td><td>17.5 \\pm 1.6</td></tr><tr><th>O2A [21]</th><td>Obj{}^{\\dagger}</td><td>W{}_{N}</td><td>N/A</td><td>15.6</td><td>30.3</td></tr><tr><th>ASR [45]</th><td>C3D{}^{*}</td><td>W{}_{T}</td><td>N/A</td><td>21.8 \\pm 0.9</td><td>24.4 \\pm 1.0</td></tr><tr><th>TS-GCN [15]</th><td>Obj{}^{\\dagger}</td><td>W{}_{N}</td><td>56.5 \\pm 6.6</td><td>23.2 \\pm 3.0</td><td>34.2 \\pm 3.1</td></tr><tr><th>E2E [4]</th><td>r(2+1)d{}^{*}</td><td>W{}_{N}</td><td>N/A</td><td>32.7</td><td>48</td></tr><tr><th>Ours</th><td>(S+Obj){}^{\\dagger}</td><td>ED</td><td>60.2 \\pm 8.9</td><td>35.3 \\pm 4.6</td><td>51.8 \\pm 2.9</td></tr></tbody></table>", "caption": "Table 2: ZSAR performances on the three existing benchmarks. Video: fisher vector (FV), bag of words (BoW), object (Obj), image spatial feature (S), {}^{*}(trained on video datasets), {}^{\\dagger}(trained on ImageNet dataset); Class: attribute (A), word embedding of class names (W{}_{N}), word embedding of class texts (W{}_{T}), elaborative description (ED). The average top-1 accuracy (%) \\pm standard deviation are reported.", "list_citation_info": ["[2] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for fine-grained image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2927\u20132936, 2015.", "[49] Xun Xu, Timothy Hospedales, and Shaogang Gong. Semantic embedding space for zero-shot action recognition. In 2015 IEEE International Conference on Image Processing (ICIP), pages 63\u201367. IEEE, 2015.", "[51] Xun Xu, Timothy M Hospedales, and Shaogang Gong. Multi-task zero-shot action recognition with prioritised data augmentation. In European Conference on Computer Vision, pages 343\u2013359. Springer, 2016.", "[35] Jie Qin, Li Liu, Ling Shao, Fumin Shen, Bingbing Ni, Jiaxin Chen, and Yunhong Wang. Zero-shot action recognition with error-correcting output codes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2833\u20132842, 2017.", "[21] Mihir Jain, Jan C van Gemert, Thomas Mensink, and Cees GM Snoek. Objects2action: Classifying and localizing actions without any video example. In Proceedings of the IEEE international conference on computer vision, pages 4588\u20134596, 2015.", "[54] Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, and Ling Shao. Towards universal representation for unseen action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9436\u20139445, 2018.", "[45] Qian Wang and Ke Chen. Alternative semantic representations for zero-shot human action recognition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 87\u2013102. Springer, 2017.", "[4] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4613\u20134623, 2020.", "[15] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8303\u20138311, 2019.", "[26] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 951\u2013958. IEEE, 2009.", "[29] Jingen Liu, Benjamin Kuipers, and Silvio Savarese. Recognizing human actions by attributes. In CVPR 2011, pages 3337\u20133344. IEEE, 2011.", "[37] Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In International Conference on Machine Learning, pages 2152\u20132161, 2015."]}, {"table": "<table><tbody><tr><th>Method</th><td>Video</td><td>Class</td><td>top-1</td><td>top-5</td></tr><tr><th>DEVISE [12]</th><td rowspan=\"6\">ST</td><td rowspan=\"6\">W{}_{N}</td><td>23.8 \\pm 0.3</td><td>51.0 \\pm 0.6</td></tr><tr><th>ALE [1]</th><td>23.4 \\pm 0.8</td><td>50.3 \\pm 1.4</td></tr><tr><th>SJE [2]</th><td>22.3 \\pm 0.6</td><td>48.2 \\pm 0.4</td></tr><tr><th>DEM [53]</th><td>23.6 \\pm 0.7</td><td>49.5 \\pm 0.4</td></tr><tr><th>ESZSL [37]</th><td>22.9 \\pm 1.2</td><td>48.3 \\pm 0.8</td></tr><tr><th>GCN [17]</th><td>22.3 \\pm 0.6</td><td>49.7 \\pm 0.6</td></tr><tr><th rowspan=\"2\">Ours</th><td>ST</td><td rowspan=\"2\">ED</td><td>37.1 \\pm 1.7</td><td>69.3 \\pm 0.8</td></tr><tr><td>ST+Obj</td><td>42.1 \\pm 1.4</td><td>73.1 \\pm 0.3</td></tr></tbody></table>", "caption": "Table 3: ZSAR performance on the proposed Kinetics benchmark. Notations are the same as Table 2; ST: spatio-temporal feature.", "list_citation_info": ["[2] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for fine-grained image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2927\u20132936, 2015.", "[53] Li Zhang, Tao Xiang, and Shaogang Gong. Learning a deep embedding model for zero-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2021\u20132030, 2017.", "[17] Pallabi Ghosh, Nirat Saini, Larry S Davis, and Abhinav Shrivastava. All about knowledge graphs for actions. arXiv preprint arXiv:2008.12432, 2020.", "[1] Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for image classification. IEEE transactions on pattern analysis and machine intelligence, 38(7):1425\u20131438, 2015.", "[12] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc\u2019Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121\u20132129, 2013.", "[37] Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In International Conference on Machine Learning, pages 2152\u20132161, 2015."]}]}