{"title": "Semantic grouping network for video captioning", "abstract": "This paper considers a video caption generating network referred to as Semantic Grouping Network (SGN) that attempts (1) to group video frames with discriminating word phrases of partially decoded caption and then (2) to decode those semantically aligned groups in predicting the next word. As consecutive frames are not likely to provide unique information, prior methods have focused on discarding or merging repetitive information based only on the input video. The SGN learns an algorithm to capture the most discriminating word phrases of the partially decoded caption and a mapping that associates each phrase to the relevant video frames - establishing this mapping allows semantically related frames to be clustered, which reduces redundancy. In contrast to the prior methods, the continuous feedback from decoded words enables the SGN to dynamically update the video representation that adapts to the partially decoded caption. Furthermore, a contrastive attention loss is proposed to facilitate accurate alignment between a word phrase and video frames without manual annotations. The SGN achieves state-of-the-art performances by outperforming runner-up methods by a margin of 2.1%p and 2.4%p in a CIDEr-D score on MSVD and MSR-VTT datasets, respectively. Extensive experiments demonstrate the effectiveness and interpretability of the SGN.", "authors": ["Hobin Ryu", " Sunghun Kang", " Haeyong Kang", " Chang D. Yoo"], "pdf_url": "https://arxiv.org/abs/2102.00831", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th></th><td colspan=\"4\">MSVD</td><td colspan=\"4\">MSR-VTT</td></tr><tr><th>Model</th><th>Detector</th><td> B@4</td><td>  C</td><td>  M</td><td>  R</td><td> B@4</td><td>  C</td><td>  M</td><td>  R</td></tr><tr><th>TA (G) (Yao et al. 2015)</th><th>\u2718</th><td>41.9</td><td>51.7</td><td>29.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HRNE (G) (Pan et al. 2016)</th><th>\u2718</th><td>43.8</td><td>-</td><td>33.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>\\textup{MGSA}^{\\dagger} (G+O) (Chen and Jiang 2019)</th><th>\u2718</th><td>49.5</td><td>74.2</td><td>32.2</td><td>-</td><td>39.9</td><td>45.0</td><td>26.3</td><td>-</td></tr><tr><th>MAM (V) (Li et al. 2017)</th><th>\u2718</th><td>41.3</td><td>53.9</td><td>32.2</td><td>68.8</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>h-RNN (V) (Yu et al. 2016)</th><th>\u2718</th><td>44.3</td><td>62.1</td><td>31.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>M3 (V) (Wang et al. 2018)</th><th>\u2718</th><td>49.6</td><td>-</td><td>30.1</td><td>-</td><td>35.0</td><td>-</td><td>24.6</td><td>-</td></tr><tr><th>BAE (R50+C) (Baraldi et al. 2017)</th><th>\u2718</th><td>42.5</td><td>63.5</td><td>32.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>hLSTMat (R152) (Song et al. 2017)</th><th>\u2718</th><td>53.0</td><td>73.8</td><td>33.6</td><td>-</td><td>38.3</td><td>-</td><td>26.3</td><td>-</td></tr><tr><th>PickNet (R152) (Chen et al. 2018)</th><th>\u2718</th><td>52.3</td><td>76.5</td><td>33.3</td><td>69.6</td><td>39.4</td><td>42.3</td><td>27.3</td><td>59.7</td></tr><tr><th>\\textup{MARN}^{\\dagger} (R101+RN) (Pei et al. 2019)</th><th>\u2718</th><td>48.6</td><td>92.2</td><td>35.1</td><td>71.9</td><td>40.4</td><td>47.1</td><td>28.1</td><td>60.7</td></tr><tr><th>OA-BTG (R200) (Zhang and Peng 2019a)</th><th>\u2714</th><td>56.9</td><td>90.6</td><td>36.2</td><td>-</td><td>41.4</td><td>46.9</td><td>28.2</td><td>-</td></tr><tr><th>STG-KD (R101+RN) (Pan et al. 2020)</th><th>\u2714</th><td>52.2</td><td>93.0</td><td>36.9</td><td>73.9</td><td>40.5</td><td>47.1</td><td>28.3</td><td>60.9</td></tr><tr><th>\\textup{SAAT}^{\\dagger} (IRV2+C3D) (Zheng et al. 2020)</th><th>\u2714</th><td>46.5</td><td>81.0</td><td>33.5</td><td>69.4</td><td>40.5</td><td>49.1</td><td>28.2</td><td>60.9</td></tr><tr><th>ORG-TRL (IRV2+C3D) (Zhang et al. 2020)</th><th>\u2714</th><td>54.3</td><td>95.2</td><td>36.4</td><td>73.9</td><td>43.6</td><td>50.9</td><td>28.8</td><td>62.1</td></tr><tr><th>SGN (G)</th><th>\u2718</th><td>46.3</td><td>73.2</td><td>32.1</td><td>67.3</td><td>37.3</td><td>41.2</td><td>26.8</td><td>58.2</td></tr><tr><th>SGN (V)</th><th>\u2718</th><td>47.7</td><td>74.9</td><td>33.1</td><td>69.0</td><td>37.8</td><td>41.9</td><td>27.0</td><td>58.3</td></tr><tr><th>SGN (R152)</th><th>\u2718</th><td>48.2</td><td>84.6</td><td>34.2</td><td>69.8</td><td>39.6</td><td>45.2</td><td>27.6</td><td>59.6</td></tr><tr><th>SGN (R101+RN)</th><th>\u2718</th><td>52.8</td><td>94.3</td><td>35.5</td><td>72.9</td><td>40.8</td><td>49.5</td><td>28.3</td><td>60.8</td></tr></tbody></table>", "caption": "Table 1: Quantitative results on MSVD and MSR-VTT datasets. G, V, R, C, RN, and O denote GoogLeNet, VGGNet-19, ResNet, C3D, 3D-ResNext-101, and Optical Flow, respectively. B@4, C, M, and R denote BLEU@4, CIDEr-D, METEOR, and ROUGE_L, respectively. Methods with a dagger (\\dagger) utilize video categories as auxiliary data on the MSR-VTT dataset.", "list_citation_info": ["Li et al. (2017) Li, X.; Zhao, B.; Lu, X.; et al. 2017. MAM-RNN: Multi-level Attention Model Based RNN for Video Captioning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence.", "Song et al. (2017) Song, J.; Gao, L.; Guo, Z.; Liu, W.; Zhang, D.; and Shen, H.-T. 2017. Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence.", "Chen and Jiang (2019) Chen, S.; and Jiang, Y.-G. 2019. Motion Guided Spatial Attention for Video Captioning. In Proceedings of AAAI Conference on Artificial Intelligence.", "Yu et al. (2016) Yu, H.; Wang, J.; Huang, Z.; Yang, Y.; and Xu, W. 2016. Video paragraph captioning using hierarchical recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "Zhang et al. (2020) Zhang, Z.; Shi, Y.; Yuan, C.; Li, B.; Wang, P.; Hu, W.; and Zha, Z.-J. 2020. Object Relational Graph with Teacher-Recommended Learning for Video Captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.", "Zhang and Peng (2019a) Zhang, J.; and Peng, Y. 2019a. Object-aware Aggregation with Bidirectional Temporal Graph for Video Captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "Pei et al. (2019) Pei, W.; Zhang, J.; Wang, X.; Ke, L.; Shen, X.; and Tai, Y.-W. 2019. Memory-Attended Recurrent Network for Video Captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "Yao et al. (2015) Yao, L.; Torabi, A.; Cho, K.; Ballas, N.; Pal, C.; Larochelle, H.; and Courville, A. 2015. Describing videos by exploiting temporal structure. In Proceedings of the IEEE International Conference on Computer Vision.", "Pan et al. (2020) Pan, B.; Cai, H.; Huang, D.-A.; Lee, K.-H.; Gaidon, A.; Adeli, E.; and Niebles, J. C. 2020. Spatio-Temporal Graph for Video Captioning With Knowledge Distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.", "Chen et al. (2018) Chen, Y.; Wang, S.; Zhang, W.; and Huang, Q. 2018. Less is more: Picking informative frames for video captioning. In Proceedings of the European Conference on Computer Vision.", "Pan et al. (2016) Pan, P.; Xu, Z.; Yang, Y.; Wu, F.; and Zhuang, Y. 2016. Hierarchical recurrent neural encoder for video representation with application to captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "Wang et al. (2018) Wang, J.; Wang, W.; Huang, Y.; Wang, L.; and Tan, T. 2018. M3: Multimodal memory modelling for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition."]}, {"table": "<table><thead><tr><th>Model</th><th>Time complexity</th><th>MSVD</th><th>MSR-VTT</th></tr></thead><tbody><tr><th>TA</th><th>O(N)</th><td>865</td><td>268</td></tr><tr><th>SGN</th><th>O(Nt)</th><td>657</td><td>203</td></tr></tbody></table>", "caption": "Table 4: Inference speed of SGN and TA (Yao et al. 2015) in terms of the number of decoded videos per second.", "list_citation_info": ["Yao et al. (2015) Yao, L.; Torabi, A.; Cho, K.; Ballas, N.; Pal, C.; Larochelle, H.; and Courville, A. 2015. Describing videos by exploiting temporal structure. In Proceedings of the IEEE International Conference on Computer Vision."]}]}