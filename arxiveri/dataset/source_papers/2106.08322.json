{"title": "Dynamic head: Unifying object detection heads with attentions", "abstract": "The complex nature of combining localization and classification in object detection has resulted in the flourished development of methods. Previous works tried to improve the performance in various object detection heads but failed to present a unified view. In this paper, we present a novel dynamic head framework to unify object detection heads with attentions. By coherently combining multiple self-attention mechanisms between feature levels for scale-awareness, among spatial locations for spatial-awareness, and within output channels for task-awareness, the proposed approach significantly improves the representation ability of object detection heads without any computational overhead. Further experiments demonstrate that the effectiveness and efficiency of the proposed dynamic head on the COCO benchmark. With a standard ResNeXt-101-DCN backbone, we largely improve the performance over popular object detectors and achieve a new state-of-the-art at 54.0 AP. Furthermore, with latest transformer backbone and extra data, we can push current best COCO result to a new record at 60.6 AP. The code will be released at https://github.com/microsoft/DynamicHead.", "authors": ["Xiyang Dai", " Yinpeng Chen", " Bin Xiao", " Dongdong Chen", " Mengchen Liu", " Lu Yuan", " Lei Zhang"], "pdf_url": "https://arxiv.org/abs/2106.08322", "list_table_and_caption": [{"table": "<table><tbody><tr><th>  Method</th><td>  AP</td><td>  AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><th colspan=\"4\">anchor-based two-stage:</th></tr><tr><th>Faster R-CNN [23]</th><td>36.4</td><td>57.9</td><td>39.4</td></tr><tr><th>\u2003+ DyHead</th><td>38.9</td><td>57.6</td><td>42.0</td></tr><tr><th colspan=\"4\">anchor-based one-stage:</th></tr><tr><th>RetinaNet [16]</th><td>35.7</td><td>54.3</td><td>37.9</td></tr><tr><th>\u2003+ DyHead</th><td>38.4</td><td>57.5</td><td>41.3</td></tr><tr><th colspan=\"4\">anchor-free box-based:</th></tr><tr><th>ATSS [35]</th><td>39.4</td><td>57.5</td><td>42.9</td></tr><tr><th>\u2003+ DyHead</th><td>42.6</td><td>60.1</td><td>46.4</td></tr><tr><th colspan=\"4\">anchor-free center-based:</th></tr><tr><th>FCOS [28]</th><td>38.8</td><td>57.3</td><td>41.9</td></tr><tr><th>\u2003+ DyHead</th><td>40.0</td><td>58.2</td><td>43.4</td></tr><tr><th colspan=\"4\">anchor-free keypoint-based:</th></tr><tr><th>RepPoints [33]</th><td>38.2</td><td>59.7</td><td>40.7</td></tr><tr><th>\u2003+ DyHead</th><td>39.6</td><td>59.8</td><td>42.8</td></tr></tbody></table>", "caption": "Table 3: Ablation study on the generalization of our dynamic head when applying to popular object detection methods.", "list_citation_info": ["[23] Shaoqing Ren, Kaiming He, Ross B. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:1137\u20131149, 2015.", "[28] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9626\u20139635, 2019.", "[33] Ze Yang, S. Liu, H. Hu, Liwei Wang, and Stephen Lin. Reppoints: Point set representation for object detection. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9656\u20139665, 2019.", "[35] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Z. Lei, and S. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9756\u20139765, 2020.", "[16] Tsung-Yi Lin, Priyal Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:318\u2013327, 2020."]}, {"table": "<table><tbody><tr><th>  Method</th><th>  Backbone</th><th>  Iteration</th><td>  AP</td><td>  AP{}_{50}</td><td>  AP{}_{75}</td><td>  AP{}_{S}</td><td>AP{}_{M}</td><td>  AP{}_{L}</td></tr><tr><th colspan=\"9\">two-stage detector:</th></tr><tr><th>Mask R-CNN[12]</th><th>ResNet-101</th><th>2x</th><td>38.2</td><td>60.3</td><td>41.7</td><td>20.1</td><td>41.1</td><td>50.2</td></tr><tr><th>Cascade-RCNN[1]</th><th>ResNet-50</th><th>3x</th><td>40.6</td><td>59.9</td><td>44.0</td><td>22.6</td><td>42.7</td><td>52.1</td></tr><tr><th>Cascade-RCNN[1]</th><th>ResNet-101</th><th>3x</th><td>42.8</td><td>62.1</td><td>46.3</td><td>23.7</td><td>45.5</td><td>55.2</td></tr><tr><th colspan=\"9\">one-stage detector:</th></tr><tr><th>FCOS[28]</th><th>ResNet-101</th><th>2x</th><td>41.5</td><td>60.7</td><td>45.0</td><td>24.4</td><td>44.8</td><td>51.6</td></tr><tr><th>FCOS[28]</th><th>ResNeXt-64x4d-101</th><th>2x</th><td>43.2</td><td>62.8</td><td>46.6</td><td>26.5</td><td>46.2</td><td>53.3</td></tr><tr><th>ATSS[35]</th><th>ResNet-101</th><th>2x</th><td>43.6</td><td>62.1</td><td>47.4</td><td>26.1</td><td>47.0</td><td>53.6</td></tr><tr><th>ATSS[35]</th><th>ResNeXt-64x4d-101</th><th>2x</th><td>45.6</td><td>64.6</td><td>49.7</td><td>28.5</td><td>48.9</td><td>55.6</td></tr><tr><th>BorderDet[21]</th><th>ResNet-101</th><th>1x</th><td>43.2</td><td>62.1</td><td>46.7</td><td>24.4</td><td>46.3</td><td>54.9</td></tr><tr><th>BorderDet[21]</th><th>ResNet-101</th><th>2x</th><td>45.4</td><td>64.1</td><td>48.8</td><td>26.7</td><td>48.3</td><td>56.5</td></tr><tr><th>BorderDet[21]</th><th>ResNeXt-64x4d-101</th><th>2x</th><td>46.5</td><td>65.7</td><td>50.5</td><td>29.1</td><td>49.4</td><td>57.5</td></tr><tr><th>DyHead</th><th>ResNet-50</th><th>1x</th><td>43.0</td><td>60.7</td><td>46.8</td><td>24.7</td><td>46.4</td><td>53.9</td></tr><tr><th>DyHead</th><th>ResNet-101</th><th>2x</th><td>46.5</td><td>64.5</td><td>50.7</td><td>28.3</td><td>50.3</td><td>57.5</td></tr><tr><th>DyHead</th><th>ResNeXt-64x4d-101</th><th>2x</th><td>47.7</td><td>65.7</td><td>51.9</td><td>31.5</td><td>51.7</td><td>60.7</td></tr></tbody></table>", "caption": "Table 4: Comparison with results using different backbones on the MS COCO test-dev set", "list_citation_info": ["[12] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask r-cnn. 2017 IEEE International Conference on Computer Vision (ICCV), pages 2980\u20132988, 2017.", "[1] Zhaowei Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality object detection. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6154\u20136162, 2018.", "[28] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9626\u20139635, 2019.", "[35] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Z. Lei, and S. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9756\u20139765, 2020.", "[21] Han Qiu, Yuchen Ma, Zeming Li, Songtao Liu, and J. Sun. Borderdet: Border feature for dense object detection. In ECCV, 2020."]}, {"table": "<table><tbody><tr><th>  Method</th><th>  Backbone</th><th>  Iteration</th><td>  AP</td><td>  AP{}_{50}</td><td>  AP{}_{75}</td><td>  AP{}_{S}</td><td>AP{}_{M}</td><td>  AP{}_{L}</td></tr><tr><th colspan=\"9\">multi-scale training:</th></tr><tr><th>ATSS[35]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>47.7</td><td>66.5</td><td>51.9</td><td>29.7</td><td>50.8</td><td>59.4</td></tr><tr><th>SEPC[31]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>50.1</td><td>69.8</td><td>54.3</td><td>31.3</td><td>53.3</td><td>63.7</td></tr><tr><th>BorderDet[21]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>48.0</td><td>67.1</td><td>52.1</td><td>29.4</td><td>50.7</td><td>60.5</td></tr><tr><th>RepPoints v2[4]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>49.4</td><td>68.9</td><td>53.4</td><td>30.3</td><td>52.1</td><td>62.3</td></tr><tr><th>RelationNet++[5]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>50.3</td><td>69.0</td><td>55.0</td><td>32.8</td><td>55.0</td><td>65.8</td></tr><tr><th>DETR[2]</th><th>ResNet-101</th><th>\\sim25x</th><td>44.9</td><td>64.7</td><td>47.7</td><td>23.7</td><td>49.5</td><td>62.3</td></tr><tr><th>Deformable DETR[38]</th><th>ResNeXt-64x4d-101-DCN</th><th>\\sim4x</th><td>50.1</td><td>69.7</td><td>54.6</td><td>30.6</td><td>52.8</td><td>64.7</td></tr><tr><th>EfficientDet[27]</th><th>Efficient-B7</th><th>\\sim50x</th><td>52.2</td><td>71.4</td><td>56.3</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>SpineNet[8]</th><th>SpineNet-190</th><th>\\sim40x</th><td>52.1</td><td>71.8</td><td>56.5</td><td>35.4</td><td>55.0</td><td>63.6</td></tr><tr><th>DyHead</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>52.3</td><td>70.7</td><td>57.2</td><td>35.1</td><td>56.2</td><td>63.4</td></tr><tr><th colspan=\"9\">multi-scale training and multi-scale testing:</th></tr><tr><th>ATSS[35]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>50.7</td><td>68.9</td><td>56.3</td><td>33.2</td><td>52.9</td><td>62.4</td></tr><tr><th>BorderDet[21]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>50.3</td><td>68.9</td><td>55.2</td><td>32.8</td><td>52.8</td><td>62.3</td></tr><tr><th>RepPoints v2[4]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>52.1</td><td>70.1</td><td>57.5</td><td>34.5</td><td>54.6</td><td>63.6</td></tr><tr><th>Deformable DETR[38]</th><th>ResNeXt-64x4d-101-DCN</th><th>\\sim4x</th><td>52.3</td><td>71.9</td><td>58.1</td><td>34.4</td><td>54.4</td><td>65.6</td></tr><tr><th>RelationNet++[5]</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>52.7</td><td>70.4</td><td>58.3</td><td>35.8</td><td>55.3</td><td>64.7</td></tr><tr><th>DyHead</th><th>ResNeXt-64x4d-101-DCN</th><th>2x</th><td>54.0</td><td>72.1</td><td>59.3</td><td>37.1</td><td>57.2</td><td>66.3</td></tr></tbody></table>", "caption": "Table 5: Comparison with the state-of-the-art results on the MS COCO test-dev set", "list_citation_info": ["[5] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Bridging visual representations for object detection via transformer decoder. ArXiv, abs/2010.15831, 2020.", "[4] Y. Chen, Zheng Zhang, Yue Cao, L. Wang, Stephen Lin, and H. Hu. Reppoints v2: Verification meets regression for object detection. ArXiv, abs/2007.08508, 2020.", "[31] Xinjiang Wang, S. Zhang, Zhuoran Yu, Litong Feng, and Wayne Zhang. Scale-equalizing pyramid convolution for object detection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13356\u201313365, 2020.", "[27] Mingxing Tan, R. Pang, and Quoc V. Le. Efficientdet: Scalable and efficient object detection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10778\u201310787, 2020.", "[2] Nicolas Carion, F. Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. ArXiv, abs/2005.12872, 2020.", "[8] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V. Le, and Xiaodan Song. Spinenet: Learning scale-permuted backbone for recognition and localization, 2020.", "[38] X. Zhu, Weijie Su, Lewei Lu, Bin Li, X. Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. ArXiv, abs/2010.04159, 2020.", "[35] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Z. Lei, and S. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9756\u20139765, 2020.", "[21] Han Qiu, Yuchen Ma, Zeming Li, Songtao Liu, and J. Sun. Borderdet: Border feature for dense object detection. In ECCV, 2020."]}, {"table": "<table><thead><tr><th>  Method</th><th>  Backbone</th><th>Iteration</th><th>  AP</th><th>  AP{}_{50}</th><th>  AP{}_{75}</th><th>  AP{}_{S}</th><th>  AP{}_{M}</th><th>  AP{}_{L}</th></tr></thead><tbody><tr><th>Mask R-CNN[12]</th><th>Swin-T</th><th>3x</th><td>46.0</td><td>68.1</td><td>50.3</td><td>31.2</td><td>49.2</td><td>60.1</td></tr><tr><th>Cascade Mask R-CNN[1]</th><th>Swin-T</th><th>3x</th><td>50.4</td><td>69.2</td><td>54.7</td><td>33.8</td><td>54.1</td><td>65.2</td></tr><tr><th>RepPoints v2[4]</th><th>Swin-T</th><th>3x</th><td>50.0</td><td>68.5</td><td>54.2</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>SparseRCNN[26]</th><th>Swin-T</th><th>3x</th><td>47.9</td><td>67.3</td><td>52.3</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>ATSS[35]</th><th>Swin-T</th><th>3x</th><td>47.2</td><td>66.5</td><td>51.3</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>DyHead</th><th>Swin-T</th><th>2x</th><td>49.7</td><td>68.0</td><td>54.3</td><td>33.3</td><td>54.2</td><td>64.2</td></tr></tbody></table>", "caption": "Table 6: Comparison with results using transformer backbone on the MS COCO validation set.", "list_citation_info": ["[4] Y. Chen, Zheng Zhang, Yue Cao, L. Wang, Stephen Lin, and H. Hu. Reppoints v2: Verification meets regression for object detection. ArXiv, abs/2007.08508, 2020.", "[1] Zhaowei Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality object detection. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6154\u20136162, 2018.", "[35] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Z. Lei, and S. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9756\u20139765, 2020.", "[12] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask r-cnn. 2017 IEEE International Conference on Computer Vision (ICCV), pages 2980\u20132988, 2017.", "[26] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. SparseR-CNN: End-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450, 2020."]}, {"table": "<table><thead><tr><th>  Method</th><th>  Backbone</th><th>Iteration</th><th>AP{}_{val}</th><th>AP</th><th>AP{}_{50}</th><th>AP{}_{75}</th><th>AP{}_{S}</th><th>AP{}_{M}</th><th>AP{}_{L}</th></tr></thead><tbody><tr><td>CenterNet2\\dagger [36]</td><td>Res2Net-101-DCN</td><td>8x</td><td>56.1</td><td>56.4</td><td>74.0</td><td>61.6</td><td>38.7</td><td>59.7</td><td>68.6</td></tr><tr><td>CopyPaste\\dagger [10]</td><td>Efficient-B7</td><td>8x</td><td>57.0</td><td>57.3</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>HTC++[19]</td><td>Swin-L</td><td>6x</td><td>58.0</td><td>58.7</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>DyHead</td><td>Swin-L</td><td>2x</td><td>58.4</td><td>58.7</td><td>77.1</td><td>64.5</td><td>41.7</td><td>62.0</td><td>72.8</td></tr><tr><td>DyHead\\dagger</td><td>Swin-L</td><td>2x</td><td>60.3</td><td>60.6</td><td>78.5</td><td>66.6</td><td>43.9</td><td>64.0</td><td>74.2</td></tr></tbody></table>", "caption": "Table 7: Comparison with latest methods on the MS COCO test-dev set. \\dagger demonstrates method with extra data.", "list_citation_info": ["[10] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation, 2020.", "[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.", "[36] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Probabilistic two-stage detection. In arXiv preprint arXiv:2103.07461, 2021."]}]}