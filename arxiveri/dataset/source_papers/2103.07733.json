{"title": "Redet: A rotation-equivariant detector for aerial object detection", "abstract": "Recently, object detection in aerial images has gained much attention in computer vision. Different from objects in natural images, aerial objects are often distributed with arbitrary orientation. Therefore, the detector requires more parameters to encode the orientation information, which are often highly redundant and inefficient. Moreover, as ordinary CNNs do not explicitly model the orientation variation, large amounts of rotation augmented data is needed to train an accurate object detector. In this paper, we propose a Rotation-equivariant Detector (ReDet) to address these issues, which explicitly encodes rotation equivariance and rotation invariance. More precisely, we incorporate rotation-equivariant networks into the detector to extract rotation-equivariant features, which can accurately predict the orientation and lead to a huge reduction of model size. Based on the rotation-equivariant features, we also present Rotation-invariant RoI Align (RiRoI Align), which adaptively extracts rotation-invariant features from equivariant features according to the orientation of RoI. Extensive experiments on several challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and HRSC2016, show that our method can achieve state-of-the-art performance on the task of aerial object detection. Compared with previous best results, our ReDet gains 1.2, 3.5 and 2.6 mAP on DOTA-v1.0, DOTA-v1.5 and HRSC2016 respectively while reducing the number of parameters by 60\\% (313 Mb vs. 121 Mb). The code is available at: \\url{https://github.com/csuhan/ReDet}.", "authors": ["Jiaming Han", " Jian Ding", " Nan Xue", " Gui-Song Xia"], "pdf_url": "https://arxiv.org/abs/2103.07733", "list_table_and_caption": [{"table": "<table><tbody><tr><th>method</th><th>backbone</th><td>PL</td><td>BD</td><td>BR</td><td>GTF</td><td>SV</td><td>LV</td><td>SH</td><td>TC</td><td>BC</td><td>ST</td><td>SBF</td><td>RA</td><td>HA</td><td>SP</td><td>HC</td><td>mAP</td></tr><tr><th>single-scale:</th><th></th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>FR-O [35]</th><th>R101</th><td>79.42</td><td>77.13</td><td>17.70</td><td>64.05</td><td>35.30</td><td>38.02</td><td>37.16</td><td>89.41</td><td>69.64</td><td>59.28</td><td>50.30</td><td>52.91</td><td>47.89</td><td>47.40</td><td>46.30</td><td>54.13</td></tr><tr><th>ICN [1]</th><th>R101-FPN</th><td>81.36</td><td>74.30</td><td>47.70</td><td>70.32</td><td>64.89</td><td>67.82</td><td>69.98</td><td>90.76</td><td>79.06</td><td>78.20</td><td>53.64</td><td>62.90</td><td>67.02</td><td>64.17</td><td>50.23</td><td>68.16</td></tr><tr><th>CADNet [42]</th><th>R101-FPN</th><td>87.80</td><td>82.40</td><td>49.40</td><td>73.50</td><td>71.10</td><td>63.50</td><td>76.60</td><td>90.90</td><td>79.20</td><td>73.30</td><td>48.40</td><td>60.90</td><td>62.00</td><td>67.00</td><td>62.20</td><td>69.90</td></tr><tr><th>DRN [24]</th><th>H-104</th><td>88.91</td><td>80.22</td><td>43.52</td><td>63.35</td><td>73.48</td><td>70.69</td><td>84.94</td><td>90.14</td><td>83.85</td><td>84.11</td><td>50.12</td><td>58.41</td><td>67.62</td><td>68.60</td><td>52.50</td><td>70.70</td></tr><tr><th>CenterMap [30]</th><th>R50-FPN</th><td>88.88</td><td>81.24</td><td>53.15</td><td>60.65</td><td>78.62</td><td>66.55</td><td>78.10</td><td>88.83</td><td>77.80</td><td>83.61</td><td>49.36</td><td>66.19</td><td>72.10</td><td>72.36</td><td>58.70</td><td>71.74</td></tr><tr><th>SCRDet [40]</th><th>R101-FPN</th><td>89.98</td><td>80.65</td><td>52.09</td><td>68.36</td><td>68.36</td><td>60.32</td><td>72.41</td><td>90.85</td><td>87.94</td><td>86.86</td><td>65.02</td><td>66.68</td><td>66.25</td><td>68.24</td><td>65.21</td><td>72.61</td></tr><tr><th>R{}^{3}Det [37]</th><th>R152-FPN</th><td>89.49</td><td>81.17</td><td>50.53</td><td>66.10</td><td>70.92</td><td>78.66</td><td>78.21</td><td>90.81</td><td>85.26</td><td>84.23</td><td>61.81</td><td>63.77</td><td>68.16</td><td>69.83</td><td>67.17</td><td>73.74</td></tr><tr><th>S{}^{2}A-Net [10]</th><th>R50-FPN</th><td>89.11</td><td>82.84</td><td>48.37</td><td>71.11</td><td>78.11</td><td>78.39</td><td>87.25</td><td>90.83</td><td>84.90</td><td>85.64</td><td>60.36</td><td>62.60</td><td>65.26</td><td>69.13</td><td>57.94</td><td>74.12</td></tr><tr><th>ReDet (Ours)</th><th>ReR50-ReFPN</th><td>88.79</td><td>82.64</td><td>53.97</td><td>74.00</td><td>78.13</td><td>84.06</td><td>88.04</td><td>90.89</td><td>87.78</td><td>85.75</td><td>61.76</td><td>60.39</td><td>75.96</td><td>68.07</td><td>63.59</td><td>76.25</td></tr><tr><th>multi-scale:</th><th></th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>RoI Trans.{}^{*} [7]</th><th>R101-FPN</th><td>88.64</td><td>78.52</td><td>43.44</td><td>75.92</td><td>68.81</td><td>73.68</td><td>83.59</td><td>90.74</td><td>77.27</td><td>81.46</td><td>58.39</td><td>53.54</td><td>62.83</td><td>58.93</td><td>47.67</td><td>69.56</td></tr><tr><th>O{}^{2}-DNet{}^{*} [31]</th><th>H104</th><td>89.30</td><td>83.30</td><td>50.10</td><td>72.10</td><td>71.10</td><td>75.60</td><td>78.70</td><td>90.90</td><td>79.90</td><td>82.90</td><td>60.20</td><td>60.00</td><td>64.60</td><td>68.90</td><td>65.70</td><td>72.80</td></tr><tr><th>DRN{}^{*} [24]</th><th>H104</th><td>89.71</td><td>82.34</td><td>47.22</td><td>64.10</td><td>76.22</td><td>74.43</td><td>85.84</td><td>90.57</td><td>86.18</td><td>84.89</td><td>57.65</td><td>61.93</td><td>69.30</td><td>69.63</td><td>58.48</td><td>73.23</td></tr><tr><th>Gliding Vertex{}^{*} [36]</th><th>R101-FPN</th><td>89.64</td><td>85.00</td><td>52.26</td><td>77.34</td><td>73.01</td><td>73.14</td><td>86.82</td><td>90.74</td><td>79.02</td><td>86.81</td><td>59.55</td><td>70.91</td><td>72.94</td><td>70.86</td><td>57.32</td><td>75.02</td></tr><tr><th>BBAVectors{}^{*} [41]</th><th>R101</th><td>88.63</td><td>84.06</td><td>52.13</td><td>69.56</td><td>78.26</td><td>80.40</td><td>88.06</td><td>90.87</td><td>87.23</td><td>86.39</td><td>56.11</td><td>65.62</td><td>67.10</td><td>72.08</td><td>63.96</td><td>75.36</td></tr><tr><th>CenterMap{}^{*} [30]</th><th>R101-FPN</th><td>89.83</td><td>84.41</td><td>54.60</td><td>70.25</td><td>77.66</td><td>78.32</td><td>87.19</td><td>90.66</td><td>84.89</td><td>85.27</td><td>56.46</td><td>69.23</td><td>74.13</td><td>71.56</td><td>66.06</td><td>76.03</td></tr><tr><th>CSL{}^{*} [38]</th><th>R152-FPN</th><td>90.25</td><td>85.53</td><td>54.64</td><td>75.31</td><td>70.44</td><td>73.51</td><td>77.62</td><td>90.84</td><td>86.15</td><td>86.69</td><td>69.60</td><td>68.04</td><td>73.83</td><td>71.10</td><td>68.93</td><td>76.17</td></tr><tr><th>SCRDet++{}^{*} [39]</th><th>R152-FPN</th><td>88.68</td><td>85.22</td><td>54.70</td><td>73.71</td><td>71.92</td><td>84.14</td><td>79.39</td><td>90.82</td><td>87.04</td><td>86.02</td><td>67.90</td><td>60.86</td><td>74.52</td><td>70.76</td><td>72.66</td><td>76.56</td></tr><tr><th>S{}^{2}A-Net{}^{*} [10]</th><th>R50-FPN</th><td>88.89</td><td>83.60</td><td>57.74</td><td>81.95</td><td>79.94</td><td>83.19</td><td>89.11</td><td>90.78</td><td>84.87</td><td>87.81</td><td>70.30</td><td>68.25</td><td>78.30</td><td>77.01</td><td>69.58</td><td>79.42</td></tr><tr><th>ReDet{}^{*} (Ours)</th><th>ReR50-ReFPN</th><td>88.81</td><td>82.48</td><td>60.83</td><td>80.82</td><td>78.34</td><td>86.06</td><td>88.31</td><td>90.87</td><td>88.77</td><td>87.03</td><td>68.65</td><td>66.90</td><td>79.26</td><td>79.71</td><td>74.67</td><td>80.10</td></tr></tbody></table>", "caption": "Table 6: Comparisons with state-of-the-art methods on DOTA-v1.0 OBB Task. H-104 means Hourglass 104. {}^{*} indicates multi-scale training and testing. The results with red and blue colors indicate the best and second-best results of each column, respectively.", "list_citation_info": ["[41] Jingru Yi, Pengxiang Wu, Bo Liu, Qiaoying Huang, Hui Qu, and Dimitris Metaxas. Oriented object detection in aerial images with box boundary-aware vectors. arXiv preprint arXiv:2008.07043, 2020.", "[24] Xingjia Pan, Yuqiang Ren, Kekai Sheng, Weiming Dong, Haolei Yuan, Xiaowei Guo, Chongyang Ma, and Changsheng Xu. Dynamic refinement network for oriented and densely packed object detection. In CVPR, June 2020.", "[38] Xue Yang and Junchi Yan. Arbitrary-oriented object detection with circular smooth label. In ECCV, 2020.", "[42] Gongjie Zhang, Shijian Lu, and Wei Zhang. Cad-net: A context-aware detection network for objects in remote sensing imagery. IEEE Transactions on Geoscience and Remote Sensing, PP:1\u201310, 2019.", "[36] Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Guisong Xia, and Xiang Bai. Gliding vertex on the horizontal bounding box for multi-oriented object detection. IEEE Trans. on PAMI, 2020.", "[10] J. Han, J. Ding, J. Li, and G. S. Xia. Align deep features for oriented object detection. IEEE Transactions on Geoscience and Remote Sensing, pages 1\u201311, 2021.", "[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. DOTA: A large-scale dataset for object detection in aerial images. In CVPR, pages 3974\u20133983, 2018.", "[31] Haoran Wei, Yue Zhang, Zhonghan Chang, Hao Li, Hongqi Wang, and Xian Sun. Oriented objects as pairs of middle lines. ISPRS Journal of Photogrammetry and Remote Sensing, 169:268\u2013279, 2020.", "[1] Seyed Majid Azimi, Eleonora Vig, Reza Bahmanyar, Marco K\u00f6rner, and Peter Reinartz. Towards multi-class object detection in unconstrained remote sensing imagery. In ACCV, pages 150\u2013165, 2018.", "[39] Xue Yang, Junchi Yan, Xiaokang Yang, Jin Tang, Wenlong Liao, and Tao He. Scrdet++: Detecting small, cluttered and rotated objects via instance-level feature denoising and rotation loss smoothing. arXiv preprint arXiv:2004.13316, 2020.", "[37] Xue Yang, Qingqing Liu, Junchi Yan, Ang Li, Zhiqiang Zhang, and Gang Yu. R3det: Refined single-stage detector with feature refinement for rotating object. arXiv preprint arXiv:1908.05612, 2019.", "[7] Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu. Learning roi transformer for oriented object detection in aerial images. In CVPR, pages 2849\u20132858, 2019.", "[40] Xue Yang, Jirui Yang, Junchi Yan, Yue Zhang, Tengfei Zhang, Zhi Guo, Xian Sun, and Kun Fu. Scrdet: Towards more robust detection for small, cluttered and rotated objects. In ICCV, pages 8231\u20138240, 2019.", "[30] Jinwang Wang, Wen Yang, Heng-Chao Li, Haijian Zhang, and Gui-Song Xia. Learning center probability map for detecting objects in aerial images. IEEE Transactions on Geoscience and Remote Sensing, 2020."]}, {"table": "<table><thead><tr><th>method</th><th>PL</th><th>BD</th><th>BR</th><th>GTF</th><th>SV</th><th>LV</th><th>SH</th><th>TC</th><th>BC</th><th>ST</th><th>SBF</th><th>RA</th><th>HA</th><th>SP</th><th>HC</th><th>CC</th><th>mAP</th></tr></thead><tbody><tr><th>OBB results:</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>RetinaNet-O [18]</th><td>71.43</td><td>77.64</td><td>42.12</td><td>64.65</td><td>44.53</td><td>56.79</td><td>73.31</td><td>90.84</td><td>76.02</td><td>59.96</td><td>46.95</td><td>69.24</td><td>59.65</td><td>64.52</td><td>48.06</td><td>0.83</td><td>59.16</td></tr><tr><th>FR-O [27]</th><td>71.89</td><td>74.47</td><td>44.45</td><td>59.87</td><td>51.28</td><td>68.98</td><td>79.37</td><td>90.78</td><td>77.38</td><td>67.50</td><td>47.75</td><td>69.72</td><td>61.22</td><td>65.28</td><td>60.47</td><td>1.54</td><td>62.00</td></tr><tr><th>Mask R-CNN [11]</th><td>76.84</td><td>73.51</td><td>49.90</td><td>57.80</td><td>51.31</td><td>71.34</td><td>79.75</td><td>90.46</td><td>74.21</td><td>66.07</td><td>46.21</td><td>70.61</td><td>63.07</td><td>64.46</td><td>57.81</td><td>9.42</td><td>62.67</td></tr><tr><th>HTC [2]</th><td>77.80</td><td>73.67</td><td>51.40</td><td>63.99</td><td>51.54</td><td>73.31</td><td>80.31</td><td>90.48</td><td>75.12</td><td>67.34</td><td>48.51</td><td>70.63</td><td>64.84</td><td>64.48</td><td>55.87</td><td>5.15</td><td>63.40</td></tr><tr><th>OWSR{}^{*} [15]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>74.90</td></tr><tr><th>ReDet (Ours)</th><td>79.20</td><td>82.81</td><td>51.92</td><td>71.41</td><td>52.38</td><td>75.73</td><td>80.92</td><td>90.83</td><td>75.81</td><td>68.64</td><td>49.29</td><td>72.03</td><td>73.36</td><td>70.55</td><td>63.33</td><td>11.53</td><td>66.86</td></tr><tr><th>ReDet{}^{*} (Ours)</th><td>88.51</td><td>86.45</td><td>61.23</td><td>81.20</td><td>67.60</td><td>83.65</td><td>90.00</td><td>90.86</td><td>84.30</td><td>75.33</td><td>71.49</td><td>72.06</td><td>78.32</td><td>74.73</td><td>76.10</td><td>46.98</td><td>76.80</td></tr><tr><th>HBB results:</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>RetinaNet-O [18]</th><td>71.66</td><td>77.22</td><td>48.71</td><td>65.16</td><td>49.48</td><td>69.64</td><td>79.21</td><td>90.84</td><td>77.21</td><td>61.03</td><td>47.30</td><td>68.69</td><td>67.22</td><td>74.48</td><td>46.16</td><td>5.78</td><td>62.49</td></tr><tr><th>FR-O [27]</th><td>71.91</td><td>71.60</td><td>50.58</td><td>61.95</td><td>51.99</td><td>71.05</td><td>80.16</td><td>90.78</td><td>77.16</td><td>67.66</td><td>47.93</td><td>69.35</td><td>69.51</td><td>74.40</td><td>60.33</td><td>5.17</td><td>63.85</td></tr><tr><th>HTC [2]</th><td>78.41</td><td>74.41</td><td>53.41</td><td>63.17</td><td>52.45</td><td>63.56</td><td>79.89</td><td>90.34</td><td>75.17</td><td>67.64</td><td>48.44</td><td>69.94</td><td>72.13</td><td>74.02</td><td>56.42</td><td>12.14</td><td>64.47</td></tr><tr><th>Mask R-CNN [11]</th><td>78.36</td><td>77.41</td><td>53.36</td><td>56.94</td><td>52.17</td><td>63.60</td><td>79.74</td><td>90.31</td><td>74.28</td><td>66.41</td><td>45.49</td><td>71.32</td><td>70.77</td><td>73.87</td><td>61.49</td><td>17.11</td><td>64.54</td></tr><tr><th>OWSR{}^{*} [15]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>77.90</td></tr><tr><th>ReDet (Ours)</th><td>79.51</td><td>82.63</td><td>53.81</td><td>69.82</td><td>52.76</td><td>75.64</td><td>87.82</td><td>90.83</td><td>75.81</td><td>68.78</td><td>49.11</td><td>71.65</td><td>75.57</td><td>75.17</td><td>58.29</td><td>15.36</td><td>67.66</td></tr><tr><th>ReDet{}^{*} (Ours)</th><td>88.68</td><td>86.57</td><td>61.93</td><td>81.20</td><td>73.71</td><td>83.59</td><td>90.06</td><td>90.86</td><td>84.30</td><td>75.56</td><td>71.55</td><td>71.86</td><td>83.93</td><td>80.38</td><td>75.62</td><td>49.55</td><td>78.08</td></tr></tbody></table>", "caption": "Table 7: Performance comparisons on DOTA-v1.5 test set. Note the results of Faster R-CNN OBB (FR-O) [27], RetinaNet OBB (RetinaNet-O) [18], Mask R-CNN [11] and Hybrid Task Cascade (HTC) [2] are our re-implemented version for DOTA. OWSR [15] is a method from DOAI 2019, and we report its single model performance for fair comparisons. The HBB results of our method are converted from OBB results by calculating the axis-aligned bounding boxes. {}^{\\mathbf{*}} means multi-scale training and testing.", "list_citation_info": ["[2] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In CVPR, pages 4974\u20134983, 2019.", "[18] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, pages 2980\u20132988, 2017.", "[11] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In ICCV, pages 2980\u20132988, 2017.", "[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. IEEE Trans. on PAMI, pages 1137\u20131149, 2017.", "[15] Chengzheng Li, Chunyan Xu, Zhen Cui, Dan Wang, Zequn Jie, Tong Zhang, and Jian Yang. Learning object-wise semantic representation for detection in remote sensing imagery. In CVPR workshops, pages 20\u201327, 2019."]}, {"table": "<table><thead><tr><th>method</th><th>RC2 [19]</th><th>RRPN [22]</th><th>R{}^{2}PN [43]</th><th>RRD [16]</th><th>RoI Trans. [7]</th><th>Gliding Vertex [36]</th></tr><tr><th>mAP</th><th>75.7</th><th>79.08</th><th>79.6</th><th>84.3</th><th>86.2</th><th>88.2</th></tr></thead><tbody><tr><th>method</th><td>R{}^{3}Det [37]</td><td>DRN [24]</td><td>CenterMap [30]</td><td>CSL [38]</td><td>S{}^{2}A-Net [10]</td><td>ReDet (Ours)</td></tr><tr><th>mAP</th><td>89.26</td><td>92.7{}^{*}</td><td>92.8{}^{*}</td><td>89.62</td><td>90.17 / 95.01{}^{*}</td><td>90.46 / 97.63{}^{*}</td></tr></tbody></table>", "caption": "Table 8: Comparisons of state-of-the-art methods on HRSC2016. {}^{*} indicates that the result is evaluated under VOC2012 metrics, while other methods are all evaluated under VOC2007 metrics. We report both results for fair comparisons.", "list_citation_info": ["[24] Xingjia Pan, Yuqiang Ren, Kekai Sheng, Weiming Dong, Haolei Yuan, Xiaowei Guo, Chongyang Ma, and Changsheng Xu. Dynamic refinement network for oriented and densely packed object detection. In CVPR, June 2020.", "[43] Zenghui Zhang, Weiwei Guo, Shengnan Zhu, and Wenxian Yu. Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks. IEEE Geoscience and Remote Sensing Letters, (99):1\u20135, 2018.", "[38] Xue Yang and Junchi Yan. Arbitrary-oriented object detection with circular smooth label. In ECCV, 2020.", "[19] Lei Liu, Zongxu Pan, and Bin Lei. Learning a rotation invariant detector with rotatable bounding box. arXiv preprint arXiv:1711.09405, 2017.", "[22] Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang, Yingbin Zheng, and Xiangyang Xue. Arbitrary-oriented scene text detection via rotation proposals. IEEE Trans. on Multimedia, 2018.", "[36] Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Guisong Xia, and Xiang Bai. Gliding vertex on the horizontal bounding box for multi-oriented object detection. IEEE Trans. on PAMI, 2020.", "[16] Minghui Liao, Zhen Zhu, Baoguang Shi, Guisong Xia, and Xiang Bai. Rotation-sensitive regression for oriented scene text detection. In CVPR, pages 5909\u20135918, 2018.", "[10] J. Han, J. Ding, J. Li, and G. S. Xia. Align deep features for oriented object detection. IEEE Transactions on Geoscience and Remote Sensing, pages 1\u201311, 2021.", "[37] Xue Yang, Qingqing Liu, Junchi Yan, Ang Li, Zhiqiang Zhang, and Gang Yu. R3det: Refined single-stage detector with feature refinement for rotating object. arXiv preprint arXiv:1908.05612, 2019.", "[7] Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu. Learning roi transformer for oriented object detection in aerial images. In CVPR, pages 2849\u20132858, 2019.", "[30] Jinwang Wang, Wen Yang, Heng-Chao Li, Haijian Zhang, and Gui-Song Xia. Learning center probability map for detecting objects in aerial images. IEEE Transactions on Geoscience and Remote Sensing, 2020."]}]}