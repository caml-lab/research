{"title": "Joint multi-object detection and tracking with camera-lidar fusion for autonomous driving", "abstract": "Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results of object detection, affinity computation and data association in real time. This paper presents an efficient multi-modal MOT framework with online joint detection and tracking schemes and robust data association for autonomous driving applications. The novelty of this work includes: (1) development of an end-to-end deep neural network for joint object detection and correlation using 2D and 3D measurements; (2) development of a robust affinity computation module to compute occlusion-aware appearance and motion affinities in 3D space; (3) development of a comprehensive data association module for joint optimization among detection confidences, affinities and start-end probabilities. The experiment results on the KITTI tracking benchmark demonstrate the superior performance of the proposed method in terms of both tracking accuracy and processing speed.", "authors": ["Kemiao Huang", " Qi Hao"], "pdf_url": "https://arxiv.org/abs/2108.04602", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Type</td><td rowspan=\"2\">Method</td><td colspan=\"2\">Object Detection and Correlation</td><td colspan=\"3\">Affinity Metric</td><td rowspan=\"2\">Data Association</td></tr><tr><td>Detection</td><td>Correlation</td><td>Appearance Modality</td><td>Motion</td><td>Geometry</td></tr><tr><td rowspan=\"4\">Tracking by Detection</td><td>ODESA [12]</td><td>2D</td><td>Re-ID</td><td><p>Camera</p></td><td>KF</td><td>2D Distance</td><td><p>HA</p></td></tr><tr><td>SMAT [14]</td><td>2D</td><td>Re-ID + Optical Flow</td><td><p>Camera</p></td><td>\\times</td><td>2D IoU</td><td><p>HA</p></td></tr><tr><td>JRMOT [7]</td><td>3D</td><td>Re-ID</td><td><p>Camera + LiDAR (Batch Fusion)</p></td><td>KF</td><td>3D IoU</td><td><p>JPDA</p></td></tr><tr><td>mmMOT [5]</td><td>3D</td><td>Re-ID + Start-End</td><td><p>Camera + LiDAR (Batch Fusion)</p></td><td>\\times</td><td>\\times</td><td><p>MIP</p></td></tr><tr><td rowspan=\"5\">Joint Detection and Tracking</td><td>CenterTrack [15]</td><td>2D / 3D</td><td>Paired Detection</td><td><p>Camera</p></td><td>Offset</td><td>2D Distance</td><td><p>Greedy</p></td></tr><tr><td>ChainedTrack [11]</td><td>2D</td><td>Parallel Re-ID</td><td><p>Camera</p></td><td>\\times</td><td>2D IoU</td><td><p>HA</p></td></tr><tr><td>JDE [8]</td><td>2D</td><td>Parallel Re-ID</td><td><p>Camera</p></td><td>KF</td><td>2D Distance</td><td><p>HA</p></td></tr><tr><td>RetinaTrack [9]</td><td>2D</td><td>Parallel Re-ID</td><td><p>Camera</p></td><td>KF</td><td>2D IoU</td><td><p>HA</p></td></tr><tr><td>JMODT (ours)</td><td>3D</td><td>Parallel Re-ID + Start-End</td><td><p>Camera + LiDAR (Point-Wise Fusion)</p></td><td>KF</td><td>3D DIoU</td><td><p>Improved MIP</p></td></tr></tbody></table><p>\u201cKF\u201d means Kalman filter, \u201cOffset\u201d means the image-based deep offset prediction, \u201cIoU\u201d means intersection-over-union, \u201cDIoU\u201d means distance-IoU affinity, \u201cHA\u201d means the Hungarian algorithm, \u201cJPDA\u201d means joint probabilistic data association, \u201cMIP\u201d means mixed-integer programming.</p>", "caption": "TABLE I: A methodological comparison between state-of-the-art MOT methods and the proposed method (JMODT).", "list_citation_info": ["[7] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatofighi, R. Martin-Martin, and S. Savarese, \u201cJrmot: A real-time 3d multi-object tracker and a new large-scale dataset,\u201d arXiv preprint arXiv:2002.08397, 2020.", "[5] W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, and C. C. Loy, \u201cRobust multi-modality multi-object tracking,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 2365\u20132374.", "[11] J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang, and Y. Fu, \u201cChained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 145\u2013161.", "[8] Z. Wang, L. Zheng, Y. Liu, and S. Wang, \u201cTowards real-time multi-object tracking,\u201d arXiv preprint arXiv:1909.12605, vol. 2, no. 3, p. 4, 2019.", "[14] N. F. Gonzalez, A. Ospina, and P. Calvez, \u201cSmat: Smart multiple affinity metrics for multiple object tracking,\u201d in International Conference on Image Analysis and Recognition. Springer, 2020, pp. 48\u201362.", "[9] Z. Lu, V. Rathod, R. Votel, and J. Huang, \u201cRetinatrack: Online single stage joint detection and tracking,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 14\u2009668\u201314\u2009678.", "[15] X. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl, \u201cTracking objects as points,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 474\u2013490.", "[12] D. Mykheievskyi, D. Borysenko, and V. Porokhonskyy, \u201cLearning local feature descriptors for multiple object tracking,\u201d in Proceedings of the Asian Conference on Computer Vision, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>JDT</th><th>AT</th><th>\\textcolorredMOTA\\uparrow</th><th>MOTP\\uparrow</th><th>FP\\downarrow</th><th>FN\\downarrow</th><th>MT\\uparrow</th><th>ML\\downarrow</th><th>IDSW\\downarrow</th><th>FRAG\\downarrow</th><th>\\textcolorredRuntime\\downarrow</th></tr></thead><tbody><tr><td>JRMOT [7]</td><td>\\times</td><td>\\checkmark</td><td>85.70%</td><td>85.48%</td><td>772</td><td>4049</td><td>71.85%</td><td>4.00%</td><td>98</td><td>372</td><td>0.07s</td></tr><tr><td>mmMOT [5]</td><td>\\times</td><td>\\checkmark</td><td>84.77%</td><td>85.21%</td><td>711</td><td>4243</td><td>73.23%</td><td>2.77%</td><td>284</td><td>753</td><td>0.02s</td></tr><tr><td>JMODT (ours)</td><td>\\checkmark</td><td>\\times</td><td>86.27%</td><td>85.41%</td><td>1244</td><td>3433</td><td>77.38%</td><td>2.92%</td><td>45</td><td>586</td><td>0.01s</td></tr></tbody></table><p>\u201cJDT\u201d means joint detection and tracking. \u201cAT\u201d means the use of additional data sources for training. \u201cMOTA\u201d means MOT accuracy. \u201cMOTP\u201d means MOT precision. \u201cMT\u201d means mostly tracked. \u201cML\u201d means mostly lost. The data come from http://www.cvlibs.net/datasets/kitti/old_eval_tracking.php.</p>", "caption": "TABLE IV: A comparison of tracking performance of camera-LiDAR based MOT methods on the test set of the KITTI car tracking benchmark.", "list_citation_info": ["[7] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatofighi, R. Martin-Martin, and S. Savarese, \u201cJrmot: A real-time 3d multi-object tracker and a new large-scale dataset,\u201d arXiv preprint arXiv:2002.08397, 2020.", "[5] W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, and C. C. Loy, \u201cRobust multi-modality multi-object tracking,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 2365\u20132374."]}]}