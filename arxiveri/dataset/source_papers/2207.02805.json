{"title": "Dpodv2: Dense correspondence-based 6 dof pose estimation", "abstract": "We propose a three-stage 6 DoF object detection method called DPODv2 (Dense Pose Object Detector) that relies on dense correspondences. We combine a 2D object detector with a dense correspondence estimation network and a multi-view pose refinement method to estimate a full 6 DoF pose. Unlike other deep learning methods that are typically restricted to monocular RGB images, we propose a unified deep learning network allowing different imaging modalities to be used (RGB or Depth). Moreover, we propose a novel pose refinement method, that is based on differentiable rendering. The main concept is to compare predicted and rendered correspondences in multiple views to obtain a pose which is consistent with predicted correspondences in all views. Our proposed method is evaluated rigorously on different data modalities and types of training data in a controlled setup. The main conclusions is that RGB excels in correspondence estimation, while depth contributes to the pose accuracy if good 3D-3D correspondences are available. Naturally, their combination achieves the overall best performance. We perform an extensive evaluation and an ablation study to analyze and validate the results on several challenging datasets. DPODv2 achieves excellent results on all of them while still remaining fast and scalable independent of the used data modality and the type of training data", "authors": ["Ivan Shugurov", " Sergey Zakharov", " Slobodan Ilic"], "pdf_url": "https://arxiv.org/abs/2207.02805", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Train Data</th><td colspan=\"8\">Synthetic</td></tr><tr><th>Method</th><td>SSD6D [12]</td><td>AAE [50]</td><td>DPOD [35]</td><td>Ours</td><td>SSD6D [12]</td><td>DPOD [35]</td><td>Ours</td><td>Ours</td></tr><tr><th>Refinement</th><td colspan=\"4\">-</td><td>DL [43]</td><td>DL [35]</td><td>2 calib. views</td><td>4 calib. views</td></tr><tr><th>Ape</th><td>2.60</td><td>3.96</td><td>37.22</td><td>62.14</td><td>-</td><td>55.23</td><td>98.54</td><td>100.00</td></tr><tr><th>Bvise</th><td>15.10</td><td>20.92</td><td>66.76</td><td>88.39</td><td>-</td><td>72.69</td><td>100.00</td><td>100.00</td></tr><tr><th>Cam</th><td>6.10</td><td>30.47</td><td>24.22</td><td>92.51</td><td>-</td><td>34.76</td><td>99.67</td><td>100.00</td></tr><tr><th>Can</th><td>27.30</td><td>35.87</td><td>52.57</td><td>96.66</td><td>-</td><td>83.59</td><td>100.00</td><td>100.00</td></tr><tr><th>Cat</th><td>9.30</td><td>17.90</td><td>32.36</td><td>86.17</td><td>-</td><td>65.1</td><td>99.66</td><td>100.00</td></tr><tr><th>Driller</th><td>12.00</td><td>23.99</td><td>66.60</td><td>90.15</td><td>-</td><td>73.32</td><td>99.83</td><td>100.00</td></tr><tr><th>Duck</th><td>1.30</td><td>4.86</td><td>26.12</td><td>54.86</td><td>-</td><td>50.04</td><td>99.68</td><td>100.00</td></tr><tr><th>Eggbox</th><td>2.80</td><td>81.01</td><td>73.35</td><td>98.64</td><td>-</td><td>89.05</td><td>97.70</td><td>99.04</td></tr><tr><th>Glue</th><td>3.40</td><td>45.49</td><td>74.96</td><td>95.41</td><td>-</td><td>84.37</td><td>97.70</td><td>98.03</td></tr><tr><th>Holep.</th><td>3.10</td><td>17.60</td><td>24.50</td><td>27.08</td><td>-</td><td>35.35</td><td>94.01</td><td>99.03</td></tr><tr><th>Iron</th><td>14.60</td><td>32.03</td><td>85.02</td><td>98.26</td><td>-</td><td>98.78</td><td>100.00</td><td>100.00</td></tr><tr><th>Lamp</th><td>11.40</td><td>60.47</td><td>57.26</td><td>91.04</td><td>-</td><td>74.27</td><td>99.51</td><td>100.00</td></tr><tr><th>Phone</th><td>9.70</td><td>33.79</td><td>29.08</td><td>74.34</td><td>-</td><td>46.98</td><td>100.00</td><td>100.00</td></tr><tr><th>Mean</th><td>9.10</td><td>28.65</td><td>50.00</td><td>81.20</td><td>34.1</td><td>66.42</td><td>98.95</td><td>99.70</td></tr><tr><th>Time (ms)</th><td>-</td><td>24</td><td>-</td><td>32</td><td>-</td><td>-</td><td>202.00</td><td>132.00</td></tr></tbody></table>", "caption": "TABLE II: Pose estimation performance on Linemod on RGB images of methods trained on synthetic data: The table reports the percentages of correctly estimated poses w.r.t. the ADD score. Our approach sets the new state of the art both among the methods trained on real data and the methods trained on synthetic data. Our refiner outperforms other RGB refiners. Run times are provided as they are reported in the original papers using non-identical hardware.", "list_citation_info": ["[35] S. Zakharov, I. Shugurov, and S. Ilic, \u201cDpod: 6d pose object detector and refiner,\u201d in ICCV, 2019.", "[12] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab, \u201cSsd-6d: Making rgb-based 3d detection and 6d pose estimation great again,\u201d in ICCV, 2017.", "[50] M. Sundermeyer, Z.-C. Marton, M. Durner, and R. Triebel, \u201cAugmented autoencoders: Implicit 3d orientation learning for 6d object detection,\u201d IJCV, 2020.", "[43] F. Manhardt, W. Kehl, N. Navab, and F. Tombari, \u201cDeep model-based 6d pose refinement in rgb,\u201d in ECCV, 2018."]}, {"table": "<table><tbody><tr><td>Train Data</td><td colspan=\"11\">Real</td></tr><tr><td>Method</td><td>Pix2Pose [36]</td><td>DPOD [35]</td><td>PVNet [34]</td><td>CDPN [37]</td><td>HybridPose [42]</td><td>Ours</td><td>BB8 [30]</td><td>PoseCNN [76]</td><td>DPOD [35]</td><td>Ours</td><td>Ours</td></tr><tr><td>Refinement</td><td colspan=\"6\">-</td><td>DL [30]</td><td>DeepIM [15]</td><td>DL [35]</td><td>2 calib. views</td><td>4 calib. views</td></tr><tr><td>Ape</td><td>58.10</td><td>53.28</td><td>43.62</td><td>64.38</td><td>63.1</td><td>80.09</td><td>40.40</td><td>76.95</td><td>87.73</td><td>98.66</td><td>100.00</td></tr><tr><td>Bvise</td><td>91.00</td><td>95.34</td><td>99.90</td><td>97.77</td><td>99.9</td><td>99.71</td><td>91.80</td><td>97.48</td><td>98.45</td><td>100.00</td><td>100.00</td></tr><tr><td>Cam</td><td>60.90</td><td>90.36</td><td>86.86</td><td>91.67</td><td>90.4</td><td>99.21</td><td>55.70</td><td>93.53</td><td>96.07</td><td>100.00</td><td>100.00</td></tr><tr><td>Can</td><td>84.40</td><td>94.10</td><td>95.47</td><td>95.87</td><td>98.5</td><td>99.60</td><td>64.10</td><td>96.46</td><td>99.71</td><td>100.00</td><td>100.00</td></tr><tr><td>Cat</td><td>65.00</td><td>60.38</td><td>79.34</td><td>83.83</td><td>89.4</td><td>95.11</td><td>62.60</td><td>82.14</td><td>94.71</td><td>100.00</td><td>100.00</td></tr><tr><td>Driller</td><td>76.30</td><td>97.72</td><td>96.43</td><td>96.23</td><td>98.5</td><td>98.91</td><td>74.40</td><td>94.95</td><td>98.80</td><td>100.00</td><td>100.00</td></tr><tr><td>Duck</td><td>43.80</td><td>66.01</td><td>52.58</td><td>66.76</td><td>65.0</td><td>79.54</td><td>44.30</td><td>77.65</td><td>86.29</td><td>98.12</td><td>100.00</td></tr><tr><td>Eggbox</td><td>96.80</td><td>99.72</td><td>99.15</td><td>99.72</td><td>100.0</td><td>99.63</td><td>57.80</td><td>97.09</td><td>99.91</td><td>99.68</td><td>100.00</td></tr><tr><td>Glue</td><td>79.40</td><td>93.83</td><td>95.66</td><td>99.61</td><td>98.8</td><td>99.81</td><td>41.20</td><td>99.42</td><td>96.82</td><td>97.46</td><td>98.84</td></tr><tr><td>Holep.</td><td>74.80</td><td>65.83</td><td>81.92</td><td>85.82</td><td>89.7</td><td>72.30</td><td>67.20</td><td>52.81</td><td>86.87</td><td>99.81</td><td>100.00</td></tr><tr><td>Iron</td><td>83.40</td><td>99.80</td><td>98.88</td><td>97.85</td><td>100.0</td><td>99.49</td><td>84.70</td><td>98.26</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><td>Lamp</td><td>82.00</td><td>88.11</td><td>99.33</td><td>97.89</td><td>99.5</td><td>96.35</td><td>76.50</td><td>97.5</td><td>96.84</td><td>100.00</td><td>100.00</td></tr><tr><td>Phone</td><td>45.00</td><td>74.24</td><td>92.41</td><td>90.75</td><td>94.9</td><td>96.88</td><td>54.00</td><td>87.72</td><td>94.69</td><td>100.00</td><td>100.00</td></tr><tr><td>Mean</td><td>72.40</td><td>82.98</td><td>86.27</td><td>89.86</td><td>91.3</td><td>93.59</td><td>62.70</td><td>88.61</td><td>95.15</td><td>99.52</td><td>99.91</td></tr><tr><td>Time (ms)</td><td>100-167</td><td>36</td><td>40</td><td>30</td><td>1000</td><td>31</td><td>330</td><td>-</td><td>-</td><td>201.00</td><td>131.00</td></tr></tbody></table>", "caption": "TABLE III: Pose estimation performance on Linemod on RGB images of methods trained on real data: The table reports the percentages of correctly estimated poses w.r.t. the ADD score. Our approach sets the new state of the art both among the methods trained on real data and the methods trained on synthetic data. Our refiner outperforms other RGB refiners. Run times are provided as they are reported in the original papers using non-identical hardware.", "list_citation_info": ["[15] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox, \u201cDeepim: Deep iterative matching for 6d pose estimation,\u201d in ECCV, 2018.", "[36] K. Park, T. Patten, and M. Vincze, \u201cPix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation,\u201d in ICCV, 2019.", "[37] Z. Li, G. Wang, and X. Ji, \u201cCdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation,\u201d in ICCV, 2019.", "[42] C. Song, J. Song, and Q. Huang, \u201cHybridpose: 6d object pose estimation under hybrid representations,\u201d in CVPR, 2020.", "[30] M. Rad and V. Lepetit, \u201cBb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth,\u201d in ICCV, 2017.", "[76] Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox, \u201cPosecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes,\u201d arXiv preprint arXiv:1711.00199, 2017.", "[34] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.", "[35] S. Zakharov, I. Shugurov, and S. Ilic, \u201cDpod: 6d pose object detector and refiner,\u201d in ICCV, 2019."]}, {"table": "<table><tbody><tr><th>Train Data</th><td colspan=\"9\">Synthetic</td></tr><tr><th>Modality</th><td>RGBD</td><td>Depth</td><td>Depth</td><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGB + D-Kabsch</td><td>RGB + D-Kabsch</td></tr><tr><th>Method</th><td>AAE [50]</td><td>PPF [2]</td><td>PPF++ [81]</td><td>Ours</td><td>Ours</td><td>SSD6D [12]</td><td>Brachmann et. al. [5]</td><td>Ours</td><td>Ours</td></tr><tr><th>Refinement</th><td>ICP</td><td>ICP</td><td>ICP</td><td>-</td><td>ICP</td><td>ICP</td><td>iterative</td><td>ICP</td><td>-</td></tr><tr><th>Ape</th><td>20.55</td><td>86.50</td><td>98.50</td><td>87.54</td><td>91.91</td><td>-</td><td>-</td><td>98.38</td><td>98.79</td></tr><tr><th>Bvise</th><td>64.25</td><td>70.70</td><td>99.80</td><td>99.92</td><td>99.92</td><td>-</td><td>-</td><td>99.92</td><td>99.92</td></tr><tr><th>Cam</th><td>63.20</td><td>78.60</td><td>99.30</td><td>96.42</td><td>97.84</td><td>-</td><td>-</td><td>99.75</td><td>99.75</td></tr><tr><th>Can</th><td>76.09</td><td>80.20</td><td>98.70</td><td>98.16</td><td>98.66</td><td>-</td><td>-</td><td>99.75</td><td>99.58</td></tr><tr><th>Cat</th><td>72.01</td><td>85.40</td><td>99.90</td><td>99.41</td><td>99.83</td><td>-</td><td>-</td><td>100.00</td><td>100.00</td></tr><tr><th>Driller</th><td>41.58</td><td>87.30</td><td>93.40</td><td>97.55</td><td>97.64</td><td>-</td><td>-</td><td>98.82</td><td>98.82</td></tr><tr><th>Duck</th><td>32.38</td><td>46.00</td><td>98.20</td><td>90.82</td><td>94.26</td><td>-</td><td>-</td><td>98.41</td><td>98.96</td></tr><tr><th>Eggbox</th><td>98.64</td><td>97.00</td><td>98.80</td><td>98.48</td><td>98.80</td><td>-</td><td>-</td><td>99.52</td><td>99.52</td></tr><tr><th>Glue</th><td>96.39</td><td>57.20</td><td>75.40</td><td>99.26</td><td>99.34</td><td>-</td><td>-</td><td>99.84</td><td>99.75</td></tr><tr><th>Holep.</th><td>49.88</td><td>77.40</td><td>98.10</td><td>93.45</td><td>96.12</td><td>-</td><td>-</td><td>97.81</td><td>97.17</td></tr><tr><th>Iron</th><td>63.11</td><td>84.90</td><td>98.30</td><td>48.35</td><td>49.21</td><td>-</td><td>-</td><td>100.00</td><td>99.91</td></tr><tr><th>Lamp</th><td>91.69</td><td>93.30</td><td>96.00</td><td>50.12</td><td>50.20</td><td>-</td><td>-</td><td>99.02</td><td>99.59</td></tr><tr><th>Phone</th><td>70.96</td><td>80.70</td><td>98.60</td><td>96.70</td><td>97.02</td><td>-</td><td>-</td><td>98.15</td><td>98.07</td></tr><tr><th>Mean</th><td>64.67</td><td>78.86</td><td>96.38</td><td>88.94</td><td>90.06</td><td>90.90</td><td>98.3</td><td>99.18</td><td>99.22</td></tr><tr><th>Time (ms)</th><td>224</td><td>-</td><td>-</td><td>49</td><td>58</td><td>100</td><td>545</td><td>55</td><td>49</td></tr></tbody></table>", "caption": "TABLE IV: Pose estimation performance on Linemod on depth and RGBD images: The table reports the percentages of correctly estimated poses w.r.t. the ADD score. The proposed detector shows state of the art results both if only real or only synthetic train data is used. Run times are provided as they are reported in the original papers using non-identical hardware. ", "list_citation_info": ["[5] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton, and C. Rother, \u201cLearning 6d object pose estimation using 3d object coordinates,\u201d in ECCV, 2014.", "[12] W. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab, \u201cSsd-6d: Making rgb-based 3d detection and 6d pose estimation great again,\u201d in ICCV, 2017.", "[50] M. Sundermeyer, Z.-C. Marton, M. Durner, and R. Triebel, \u201cAugmented autoencoders: Implicit 3d orientation learning for 6d object detection,\u201d IJCV, 2020.", "[81] S. Hinterstoisser, V. Lepetit, N. Rajkumar, and K. Konolige, \u201cGoing further with point pair features,\u201d in ECCV, 2016.", "[2] B. Drost, M. Ulrich, N. Navab, and S. Ilic, \u201cModel globally, match locally: Efficient and robust 3d object recognition,\u201d in 2010 IEEE computer society conference on computer vision and pattern recognition, 2010."]}, {"table": "<table><tbody><tr><th>Train Data</th><td colspan=\"10\">Real</td></tr><tr><th>Modality</th><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGBD</td><td>RGB + D-Kabsch</td><td>RGB + D-Kabsch</td></tr><tr><th>Method</th><td>PointFusion [82]</td><td>DF [13]</td><td>DF [13]</td><td>Brachmann et. al. [5]</td><td>Ours</td><td>Brachmann et. al. [83]</td><td>Ours</td><td>PVN3D [41]</td><td>Ours</td><td>Ours</td></tr><tr><th>Refinement</th><td>-</td><td>-</td><td>DL [13]</td><td>-</td><td>-</td><td>iterative</td><td>ICP</td><td>-</td><td>ICP</td><td>-</td></tr><tr><th>Ape</th><td>70.40</td><td>79.50</td><td>92.30</td><td>-</td><td>97.34</td><td>-</td><td>97.62</td><td>97.30</td><td>97.72</td><td>98.56</td></tr><tr><th>Bvise</th><td>80.70</td><td>84.20</td><td>93.20</td><td>-</td><td>99.90</td><td>-</td><td>100.00</td><td>99.70</td><td>100.00</td><td>99.90</td></tr><tr><th>Cam</th><td>60.80</td><td>76.50</td><td>94.40</td><td>-</td><td>99.41</td><td>-</td><td>99.70</td><td>99.60</td><td>99.90</td><td>99.90</td></tr><tr><th>Can</th><td>61.10</td><td>86.60</td><td>93.10</td><td>-</td><td>99.41</td><td>-</td><td>99.51</td><td>99.50</td><td>100.00</td><td>99.80</td></tr><tr><th>Cat</th><td>79.10</td><td>88.80</td><td>96.50</td><td>-</td><td>99.90</td><td>-</td><td>99.90</td><td>99.80</td><td>100.00</td><td>100.00</td></tr><tr><th>Driller</th><td>47.30</td><td>77.70</td><td>87.00</td><td>-</td><td>97.52</td><td>-</td><td>98.02</td><td>99.30</td><td>99.90</td><td>99.90</td></tr><tr><th>Duck</th><td>63.00</td><td>76.30</td><td>92.30</td><td>-</td><td>94.74</td><td>-</td><td>97.09</td><td>98.20</td><td>98.31</td><td>99.25</td></tr><tr><th>Eggbox</th><td>99.90</td><td>99.90</td><td>99.80</td><td>-</td><td>99.63</td><td>-</td><td>99.72</td><td>99.80</td><td>99.72</td><td>99.72</td></tr><tr><th>Glue</th><td>99.30</td><td>99.40</td><td>100.00</td><td>-</td><td>99.71</td><td>-</td><td>99.71</td><td>100.00</td><td>100.00</td><td>100.00</td></tr><tr><th>Holep.</th><td>71.80</td><td>79.00</td><td>92.10</td><td>-</td><td>99.53</td><td>-</td><td>99.71</td><td>99.90</td><td>99.71</td><td>99.61</td></tr><tr><th>Iron</th><td>83.20</td><td>92.10</td><td>97.00</td><td>-</td><td>98.88</td><td>-</td><td>99.18</td><td>99.70</td><td>100.00</td><td>99.90</td></tr><tr><th>Lamp</th><td>62.30</td><td>92.30</td><td>95.30</td><td>-</td><td>99.14</td><td>-</td><td>99.04</td><td>99.80</td><td>100.00</td><td>99.90</td></tr><tr><th>Phone</th><td>78.80</td><td>88.00</td><td>92.80</td><td>-</td><td>99.43</td><td>-</td><td>99.43</td><td>99.50</td><td>100.00</td><td>100.00</td></tr><tr><th>Mean</th><td>73.70</td><td>86.20</td><td>94.30</td><td>98.10</td><td>98.81</td><td>99.00</td><td>99.13</td><td>99.40</td><td>99.64</td><td>99.73</td></tr><tr><th>Time (ms)</th><td>-</td><td>-</td><td>-</td><td>545</td><td>49</td><td>-</td><td>57</td><td>-</td><td>55</td><td>49</td></tr></tbody></table>", "caption": "TABLE V: Pose estimation performance on Linemod on depth and RGBD images: The table reports the percentages of correctly estimated poses w.r.t. the ADD score. The proposed detector shows state of the art results both if only real or only synthetic train data is used. Run times are provided as they are reported in the original papers using non-identical hardware. ", "list_citation_info": ["[13] C. Wang, D. Xu, Y. Zhu, R. Mart\u00edn-Mart\u00edn, C. Lu, L. Fei-Fei, and S. Savarese, \u201cDensefusion: 6d object pose estimation by iterative dense fusion,\u201d in CVPR, 2019.", "[82] D. Xu, D. Anguelov, and A. Jain, \u201cPointfusion: Deep sensor fusion for 3d bounding box estimation,\u201d in CVPR, 2018.", "[5] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton, and C. Rother, \u201cLearning 6d object pose estimation using 3d object coordinates,\u201d in ECCV, 2014.", "[41] Y. He, W. Sun, H. Huang, J. Liu, H. Fan, and J. Sun, \u201cPvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation,\u201d in CVPR, 2020.", "[83] E. Brachmann, F. Michel, A. Krull, M. Ying Yang, S. Gumhold et al., \u201cUncertainty-driven 6d pose estimation of objects and scenes from a single rgb image,\u201d in CVPR, 2016."]}, {"table": "<table><tbody><tr><th>Train data</th><th>Data modality</th><th>Method</th><th>Refinement</th><td>VSD</td><td>MSSD</td><td>MSPD</td><td>AR</td><td>Time (s)</td></tr><tr><th rowspan=\"16\">PBR</th><th rowspan=\"8\">RGBD</th><th>CosyPose [1]</th><th>ICP</th><td>0.567</td><td>0.748</td><td>0.826</td><td>0.714</td><td>8.289</td></tr><tr><th>Ours + Kabsch</th><th>-</th><td>0.565</td><td>0.748</td><td>0.788</td><td>0.700</td><td>0.334</td></tr><tr><th>Ours + Kabsch</th><th>ICP</th><td>0.557</td><td>0.749</td><td>0.787</td><td>0.698</td><td>0.387</td></tr><tr><th>PVNet [34]</th><th>ICP</th><td>0.502</td><td>0.683</td><td>0.73</td><td>0.638</td><td>-</td></tr><tr><th>CDPN [37]</th><th>ICP</th><td>0.469</td><td>0.689</td><td>0.731</td><td>0.630</td><td>0.506</td></tr><tr><th>Pix2Pose [36]</th><th>ICP</th><td>0.473</td><td>0.631</td><td>0.659</td><td>0.588</td><td>5.191</td></tr><tr><th>Ours</th><th>ICP</th><td>0.472</td><td>0.621</td><td>0.654</td><td>0.582</td><td>0.398</td></tr><tr><th>Ours</th><th>-</th><td>0.422</td><td>0.58</td><td>0.621</td><td>0.541</td><td>0.325</td></tr><tr><th rowspan=\"8\">RGB</th><th>Ours</th><th>4 calibrated views</th><td>0.572</td><td>0.735</td><td>0.777</td><td>0.695</td><td>2.479</td></tr><tr><th>Ours</th><th>2 calibrated views</th><td>0.520</td><td>0.690</td><td>0.771</td><td>0.660</td><td>1.353</td></tr><tr><th>CosyPose [1]</th><th>-</th><td>0.480</td><td>0.606</td><td>0.812</td><td>0.633</td><td>0.550</td></tr><tr><th>Ours</th><th>-</th><td>0.432</td><td>0.560</td><td>0.761</td><td>0.584</td><td>0.274</td></tr><tr><th>PVNet [34]</th><th>-</th><td>0.428</td><td>0.543</td><td>0.754</td><td>0.575</td><td>-</td></tr><tr><th>CDPN [37]</th><th>-</th><td>0.393</td><td>0.537</td><td>0.779</td><td>0.569</td><td>0.279</td></tr><tr><th>EPOS [39]</th><th>-</th><td>0.389</td><td>0.501</td><td>0.750</td><td>0.547</td><td>0.468</td></tr><tr><th>Pix2Pose [36]</th><th>-</th><td>0.233</td><td>0.307</td><td>0.550</td><td>0.363</td><td>1.310</td></tr><tr><th rowspan=\"2\">synt</th><th rowspan=\"2\">RGB</th><th>EPOS [39]</th><th>-</th><td>0.29</td><td>0.38</td><td>0.659</td><td>0.443</td><td>0.487</td></tr><tr><th>DPOD [35]</th><th>-</th><td>0.101</td><td>0.126</td><td>0.278</td><td>0.169</td><td>0.172</td></tr><tr><th rowspan=\"3\">mix</th><th>RGBD</th><th>AAE [49]</th><th>ICP</th><td>0.208</td><td>0.218</td><td>0.285</td><td>0.237</td><td>1.197</td></tr><tr><th rowspan=\"2\">RGB</th><th>Multi-Path AAE</th><th>-</th><td>0.15</td><td>0.153</td><td>0.346</td><td>0.217</td><td>0.200</td></tr><tr><th>AAE [49]</th><th>-</th><td>0.09</td><td>0.095</td><td>0.254</td><td>0.146</td><td>0.201</td></tr><tr><th rowspan=\"2\">-</th><th rowspan=\"2\">depth</th><th>Drost, PPF [2]</th><th>ICP</th><td>0.437</td><td>0.563</td><td>0.581</td><td>0.527</td><td>15.947{}^{*}</td></tr><tr><th>Drost, PPF [2]</th><th>ICP, 3D edges</th><td>0.409</td><td>0.525</td><td>0.542</td><td>0.492</td><td>3.389{}^{*}</td></tr><tr><th rowspan=\"7\">real</th><th rowspan=\"4\">RGBD</th><th>Ours + Kabsch</th><th>ICP</th><td>0.557</td><td>0.739</td><td>0.773</td><td>0.69</td><td>0.393</td></tr><tr><th>Ours + Kabsch</th><th>-</th><td>0.557</td><td>0.721</td><td>0.759</td><td>0.679</td><td>0.329</td></tr><tr><th>Ours</th><th>ICP</th><td>0.478</td><td>0.62</td><td>0.66</td><td>0.586</td><td>0.453</td></tr><tr><th>Ours</th><th>-</th><td>0.403</td><td>0.553</td><td>0.595</td><td>0.517</td><td>0.343</td></tr><tr><th rowspan=\"3\">RGB</th><th>Ours</th><th>4 calibrated views</th><td>0.598</td><td>0.751</td><td>0.793</td><td>0.714</td><td>1.326</td></tr><tr><th>Ours</th><th>2 calibrated views</th><td>0.528</td><td>0.692</td><td>0.764</td><td>0.661</td><td>2.201</td></tr><tr><th>Ours</th><th>-</th><td>0.427</td><td>0.55</td><td>0.728</td><td>0.568</td><td>0.278</td></tr></tbody></table>", "caption": "TABLE VI: Pose estimation performance comparison on the Occlusion dataset: Results are reported in terms of theAverage Recall score. The results prove the effectiveness of the proposed approach on all used data modalities. Run times are provided as they are reported in the BOP challenge [14] using non-identical hardware. PPF-based methods, labeled with {}^{*} in the Time column, use only CPU. ", "list_citation_info": ["[36] K. Park, T. Patten, and M. Vincze, \u201cPix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation,\u201d in ICCV, 2019.", "[49] M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and R. Triebel, \u201cImplicit 3d orientation learning for 6d object detection from rgb images,\u201d in ECCV, 2018.", "[14] T. Hodan and A. Melenovsky. (2019) Bop: Benchmark for 6d object pose estimation: https://bop.felk.cvut.cz/home/.", "[39] T. Hodan, D. Barath, and J. Matas, \u201cEpos: Estimating 6d pose of objects with symmetries,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.", "[2] B. Drost, M. Ulrich, N. Navab, and S. Ilic, \u201cModel globally, match locally: Efficient and robust 3d object recognition,\u201d in 2010 IEEE computer society conference on computer vision and pattern recognition, 2010.", "[35] S. Zakharov, I. Shugurov, and S. Ilic, \u201cDpod: 6d pose object detector and refiner,\u201d in ICCV, 2019.", "[34] S. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \u201cPvnet: Pixel-wise voting network for 6dof pose estimation,\u201d in CVPR, 2019.", "[1] Y. Labb\u00e9, J. Carpentier, M. Aubry, and J. Sivic, \u201cCosypose: Consistent multi-view multi-object 6d pose estimation,\u201d in ECCV, 2020.", "[37] Z. Li, G. Wang, and X. Ji, \u201cCdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation,\u201d in ICCV, 2019."]}, {"table": "<table><tbody><tr><th>Train data</th><th>Data modality</th><th>Method</th><th>Refinement</th><td>VSD</td><td>MSSD</td><td>MSPD</td><td>AR</td><td>Time (s)</td></tr><tr><th rowspan=\"15\">PBR</th><th rowspan=\"7\">RGBD</th><th>Ours + Kabsch</th><th>ICP</th><td>0.784</td><td>0.872</td><td>0.879</td><td>0.845</td><td>0.329</td></tr><tr><th>Ours + Kabsch</th><th>-</th><td>0.796</td><td>0.856</td><td>0.867</td><td>0.84</td><td>0.238</td></tr><tr><th>CosyPose [1]</th><th>ICP</th><td>0.679</td><td>0.719</td><td>0.737</td><td>0.712</td><td>5.326</td></tr><tr><th>CDPNv2 [37]</th><th>ICP</th><td>0.629</td><td>0.757</td><td>0.749</td><td>0.712</td><td>0.713</td></tr><tr><th>Pix2Pose [36]</th><th>ICP</th><td>0.64</td><td>0.721</td><td>0.724</td><td>0.695</td><td>3.248</td></tr><tr><th>Ours</th><th>ICP</th><td>0.462</td><td>0.468</td><td>0.473</td><td>0.468</td><td>0.47</td></tr><tr><th>Ours</th><th>-</th><td>0.344</td><td>0.398</td><td>0.403</td><td>0.382</td><td>0.245</td></tr><tr><th rowspan=\"8\">RGB</th><th>Ours</th><th>4 calibrated views</th><td>0.79</td><td>0.857</td><td>0.86</td><td>0.835</td><td>0.584</td></tr><tr><th>Ours</th><th>2 calibrated views</th><td>0.754</td><td>0.84</td><td>0.858</td><td>0.818</td><td>0.926</td></tr><tr><th>CosyPose [1]</th><th>8 uncalibrated views</th><td>0.691</td><td>0.744</td><td>0.804</td><td>0.746</td><td>0.427</td></tr><tr><th>Ours</th><th>-</th><td>0.642</td><td>0.704</td><td>0.828</td><td>0.725</td><td>0.163</td></tr><tr><th>CDPNv2 [37]</th><th>-</th><td>0.614</td><td>0.708</td><td>0.845</td><td>0.722</td><td>0.273</td></tr><tr><th>CosyPose [1]</th><th>4 uncalibrated views</th><td>0.646</td><td>0.685</td><td>0.756</td><td>0.696</td><td>0.445</td></tr><tr><th>CosyPose [1]</th><th>-</th><td>0.613</td><td>0.634</td><td>0.721</td><td>0.656</td><td>0.417</td></tr><tr><th>EPOS [39]</th><th>-</th><td>0.484</td><td>0.527</td><td>0.729</td><td>0.58</td><td>0.657</td></tr><tr><th rowspan=\"3\">synt</th><th>RGBD</th><th>Sundermeyer-IJCV19 [49]</th><th>ICP</th><td>0.479</td><td>0.506</td><td>0.533</td><td>0.506</td><td>1.352</td></tr><tr><th rowspan=\"2\">RGB</th><th>Sundermeyer-IJCV19 [50]</th><th>-</th><td>0.273</td><td>0.306</td><td>0.461</td><td>0.346</td><td>0.19</td></tr><tr><th>DPOD [35]</th><th>-</th><td>0.218</td><td>0.262</td><td>0.379</td><td>0.286</td><td>0.18</td></tr><tr><th rowspan=\"4\">-</th><th rowspan=\"3\">Depth</th><th>Vidal-Sensors18 [84]</th><th>ICP</th><td>0.707</td><td>0.704</td><td>0.708</td><td>0.706</td><td>2.608{}^{*}</td></tr><tr><th>Drost-CVPR10-3D-Edges [2]</th><th>ICP, 3D edges</th><td>0.618</td><td>0.624</td><td>0.626</td><td>0.623</td><td>127.372{}^{*}</td></tr><tr><th>Drost-CVPR10-3D-Only [2]</th><th>ICP</th><td>0.593</td><td>0.627</td><td>0.627</td><td>0.615</td><td>16.136{}^{*}</td></tr><tr><th>RGBD</th><th>Drost-CVPR10-Edges [2]</th><th>ICP,3D edges, images</th><td>0.677</td><td>0.665</td><td>0.67</td><td>0.671</td><td>144.029{}^{*}</td></tr></tbody></table>", "caption": "TABLE VII: Pose estimation performance comparison on the Homebrewed dataset: Results are reported in terms of the Average Recall score [14]. The results prove the effectiveness of the proposed approach on all used data modalities. Run times are provided as they are reported in the BOP challenge [14] using non-identical hardware. PPF-based methods, labeled with {}^{*} in the Time column, use only CPU. ", "list_citation_info": ["[36] K. Park, T. Patten, and M. Vincze, \u201cPix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation,\u201d in ICCV, 2019.", "[84] J. Vidal, C.-Y. Lin, X. Llad\u00f3, and R. Mart\u00ed, \u201cA method for 6d pose estimation of free-form rigid objects using point pair features on range data,\u201d Sensors, 2018.", "[49] M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and R. Triebel, \u201cImplicit 3d orientation learning for 6d object detection from rgb images,\u201d in ECCV, 2018.", "[14] T. Hodan and A. Melenovsky. (2019) Bop: Benchmark for 6d object pose estimation: https://bop.felk.cvut.cz/home/.", "[39] T. Hodan, D. Barath, and J. Matas, \u201cEpos: Estimating 6d pose of objects with symmetries,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.", "[50] M. Sundermeyer, Z.-C. Marton, M. Durner, and R. Triebel, \u201cAugmented autoencoders: Implicit 3d orientation learning for 6d object detection,\u201d IJCV, 2020.", "[35] S. Zakharov, I. Shugurov, and S. Ilic, \u201cDpod: 6d pose object detector and refiner,\u201d in ICCV, 2019.", "[2] B. Drost, M. Ulrich, N. Navab, and S. Ilic, \u201cModel globally, match locally: Efficient and robust 3d object recognition,\u201d in 2010 IEEE computer society conference on computer vision and pattern recognition, 2010.", "[1] Y. Labb\u00e9, J. Carpentier, M. Aubry, and J. Sivic, \u201cCosypose: Consistent multi-view multi-object 6d pose estimation,\u201d in ECCV, 2020.", "[37] Z. Li, G. Wang, and X. Ji, \u201cCdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation,\u201d in ICCV, 2019."]}, {"table": "<table><tbody><tr><th>Train data</th><th>Data modality</th><th>Method</th><th>Refinement</th><td>VSD</td><td>MSSD</td><td>MSPD</td><td>AR</td><td>Time (s)</td></tr><tr><th rowspan=\"11\">PBR</th><th rowspan=\"5\">RGBD</th><th>Ours + Kabsch</th><th>-</th><td>0.646</td><td>0.716</td><td>0.736</td><td>0.699</td><td>0.320</td></tr><tr><th>Ours + Kabsch</th><th>ICP</th><td>0.485</td><td>0.632</td><td>0.652</td><td>0.590</td><td>0.523</td></tr><tr><th>CDPNv2 [37]</th><th>ICP</th><td>0.368</td><td>0.449</td><td>0.488</td><td>0.435</td><td>2.486</td></tr><tr><th>Ours</th><th>ICP</th><td>0.277</td><td>0.320</td><td>0.339</td><td>0.312</td><td>0.688</td></tr><tr><th>Ours</th><th>-</th><td>0.251</td><td>0.303</td><td>0.312</td><td>0.289</td><td>0.330</td></tr><tr><th rowspan=\"6\">RGB</th><th>Ours</th><th>2 calibrated views</th><td>0.634</td><td>0.707</td><td>0.728</td><td>0.690</td><td>1.293</td></tr><tr><th>Ours</th><th>4 calibrated views</th><td>0.645</td><td>0.710</td><td>0.712</td><td>0.689</td><td>0.886</td></tr><tr><th>CosyPose [1]</th><th>-</th><td>0.571</td><td>0.589</td><td>0.761</td><td>0.64</td><td>0.493</td></tr><tr><th>Ours</th><th>-</th><td>0.561</td><td>0.602</td><td>0.744</td><td>0.636</td><td>0.328</td></tr><tr><th>EPOS [39]</th><th>-</th><td>0.38</td><td>0.403</td><td>0.619</td><td>0.467</td><td>1.992</td></tr><tr><th>CDPNv2 [37]</th><th>-</th><td>0.303</td><td>0.338</td><td>0.579</td><td>0.407</td><td>1.849</td></tr><tr><th rowspan=\"2\">synt</th><th rowspan=\"2\">RGB</th><th>EPOS [39]</th><th>-</th><td>0.369</td><td>0.423</td><td>0.635</td><td>0.476</td><td>1.177</td></tr><tr><th>DPOD [35]</th><th>-</th><td>0.048</td><td>0.055</td><td>0.139</td><td>0.081</td><td>0.206</td></tr><tr><th rowspan=\"4\">-</th><th rowspan=\"3\">Depth</th><th>Vidal-Sensors18 [84]</th><th>ICP</th><td>0.464</td><td>0.575</td><td>0.574</td><td>0.538</td><td>7.063{}^{*}</td></tr><tr><th>Drost-CVPR10-3D-Only [2]</th><th>ICP</th><td>0.375</td><td>0.478</td><td>0.480</td><td>0.444</td><td>9.204{}^{*}</td></tr><tr><th>Drost-CVPR10-3D-Edges [2]</th><th>ICP, 3D edges</th><td>0.370</td><td>0.422</td><td>0.420</td><td>0.404</td><td>62.507{}^{*}</td></tr><tr><th rowspan=\"4\">RGBD</th><th>Drost-CVPR10-Edges [2]</th><th>ICP, 3D edges, images</th><td>0.469</td><td>0.512</td><td>0.518</td><td>0.500</td><td>70.914{}^{*}</td></tr><tr><th rowspan=\"8\">real</th><th>Ours + Kabsch</th><th>-</th><td>0.537</td><td>0.557</td><td>0.586</td><td>0.56</td><td>0.315</td></tr><tr><th>Pix2Pose [36]</th><th>ICP</th><td>0.438</td><td>0.548</td><td>0.549</td><td>0.512</td><td>4.180</td></tr><tr><th>Ours + Kabsch</th><th>ICP</th><td>0.416</td><td>0.516</td><td>0.537</td><td>0.490</td><td>0.578</td></tr><tr><th rowspan=\"5\">RGB</th><th>Ours</th><th>4 calibrated views</th><td>0.467</td><td>0.541</td><td>0.563</td><td>0.524</td><td>0.727</td></tr><tr><th>Ours</th><th>2 calibrated views</th><td>0.461</td><td>0.530</td><td>0.578</td><td>0.523</td><td>0.973</td></tr><tr><th>Ours</th><th>-</th><td>0.469</td><td>0.491</td><td>0.595</td><td>0.518</td><td>0.367</td></tr><tr><th>Pix2Pose [36]</th><th>-</th><td>0.261</td><td>0.296</td><td>0.476</td><td>0.344</td><td>1.084</td></tr><tr><th>Pix2Pose-Original-ICCV19 [36]</th><th>-</th><td>0.214</td><td>0.238</td><td>0.432</td><td>0.295</td><td>1.522</td></tr><tr><th rowspan=\"14\">mix</th><th rowspan=\"9\">RGB</th><th>CosyPose [1]</th><th>8 uncalibrated views</th><td>0.773</td><td>0.836</td><td>0.907</td><td>0.839</td><td>0.969</td></tr><tr><th>CosyPose [1]</th><th>4 uncalibrated views</th><td>0.742</td><td>0.795</td><td>0.864</td><td>0.801</td><td>0.792</td></tr><tr><th>CosyPose [1]</th><th>-</th><td>0.669</td><td>0.695</td><td>0.821</td><td>0.728</td><td>0.451</td></tr><tr><th>Ours</th><th>4 calibrated views</th><td>0.679</td><td>0.742</td><td>0.740</td><td>0.72</td><td>0.909</td></tr><tr><th>Ours</th><th>2 calibrated views</th><td>0.665</td><td>0.739</td><td>0.753</td><td>0.719</td><td>1.306</td></tr><tr><th>Ours</th><th>-</th><td>0.579</td><td>0.621</td><td>0.764</td><td>0.655</td><td>0.306</td></tr><tr><th>CDPN [37]</th><th>-</th><td>0.377</td><td>0.418</td><td>0.674</td><td>0.49</td><td>0.708</td></tr><tr><th>CDPNv2 [37]</th><th>-</th><td>0.386</td><td>0.426</td><td>0.62</td><td>0.478</td><td>1.852</td></tr><tr><th>Sundermeyer-IJCV19 [50]</th><th>-</th><td>0.196</td><td>0.211</td><td>0.504</td><td>0.304</td><td>0.194</td></tr><tr><th rowspan=\"5\">RGBD</th><th>Ours + Kabsch</th><th>-</th><td>0.665</td><td>0.738</td><td>0.756</td><td>0.720</td><td>0.7367</td></tr><tr><th>CosyPose [1]</th><th>ICP</th><td>0.587</td><td>0.749</td><td>0.767</td><td>0.701</td><td>0.274</td></tr><tr><th>Ours + Kabsch</th><th>ICP</th><td>0.503</td><td>0.656</td><td>0.672</td><td>0.610</td><td>0.455</td></tr><tr><th>Sundermeyer-IJCV19 [50]</th><th>ICP</th><td>0.459</td><td>0.489</td><td>0.514</td><td>0.487</td><td>0.531</td></tr><tr><th>CDPNv2 [37]</th><th>ICP</th><td>0.385</td><td>0.489</td><td>0.516</td><td>0.464</td><td>2.645</td></tr></tbody></table>", "caption": "TABLE VIII: Pose estimation performance comparison on the BOP images of the TLESS dataset: Results are reported in terms of the Average Recall score [14]. The results prove the effectiveness of the proposed approach on all used data modalities. Run times are provided as they are reported in the BOP challenge [14] using non-identical hardware. PPF-based methods, labeled with {}^{*} in the Time column, use only CPU. ", "list_citation_info": ["[36] K. Park, T. Patten, and M. Vincze, \u201cPix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation,\u201d in ICCV, 2019.", "[84] J. Vidal, C.-Y. Lin, X. Llad\u00f3, and R. Mart\u00ed, \u201cA method for 6d pose estimation of free-form rigid objects using point pair features on range data,\u201d Sensors, 2018.", "[14] T. Hodan and A. Melenovsky. (2019) Bop: Benchmark for 6d object pose estimation: https://bop.felk.cvut.cz/home/.", "[37] Z. Li, G. Wang, and X. Ji, \u201cCdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation,\u201d in ICCV, 2019.", "[39] T. Hodan, D. Barath, and J. Matas, \u201cEpos: Estimating 6d pose of objects with symmetries,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.", "[50] M. Sundermeyer, Z.-C. Marton, M. Durner, and R. Triebel, \u201cAugmented autoencoders: Implicit 3d orientation learning for 6d object detection,\u201d IJCV, 2020.", "[2] B. Drost, M. Ulrich, N. Navab, and S. Ilic, \u201cModel globally, match locally: Efficient and robust 3d object recognition,\u201d in 2010 IEEE computer society conference on computer vision and pattern recognition, 2010.", "[1] Y. Labb\u00e9, J. Carpentier, M. Aubry, and J. Sivic, \u201cCosypose: Consistent multi-view multi-object 6d pose estimation,\u201d in ECCV, 2020.", "[35] S. Zakharov, I. Shugurov, and S. Ilic, \u201cDpod: 6d pose object detector and refiner,\u201d in ICCV, 2019."]}, {"table": "<table><tbody><tr><th>Dataset</th><td>Data modality</td><td>Data type</td><td>Bboxes</td><td>Refinement</td><td>AR</td><td>Corr. Err.</td></tr><tr><th rowspan=\"28\">LMO</th><td rowspan=\"12\">RGB</td><td rowspan=\"6\">real</td><td rowspan=\"3\">GT</td><td>-</td><td>0.601</td><td rowspan=\"3\">13.830</td></tr><tr><td>2 views</td><td>0.690</td></tr><tr><td>4 views</td><td>0.751</td></tr><tr><td rowspan=\"3\">YOLO</td><td>-</td><td>0.568</td><td rowspan=\"3\">16.150</td></tr><tr><td>2 views</td><td>0.661</td></tr><tr><td>4 views</td><td>0.714</td></tr><tr><td rowspan=\"6\">synt</td><td rowspan=\"3\">GT</td><td>-</td><td>0.649</td><td rowspan=\"3\">12.275</td></tr><tr><td>2 views</td><td>0.741</td></tr><tr><td>4 views</td><td>0.773</td></tr><tr><td rowspan=\"3\">YOLO</td><td>-</td><td>0.584</td><td rowspan=\"3\">12.987</td></tr><tr><td>2 views</td><td>0.660</td></tr><tr><td>4 views</td><td>0.695</td></tr><tr><td rowspan=\"8\">RGBD</td><td rowspan=\"4\">real</td><td rowspan=\"2\">GT</td><td>-</td><td>0.534</td><td rowspan=\"2\">26.482</td></tr><tr><td>ICP</td><td>0.609</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.517</td><td rowspan=\"2\">26.611</td></tr><tr><td>ICP</td><td>0.586</td></tr><tr><td rowspan=\"4\">synt</td><td rowspan=\"2\">GT</td><td>-</td><td>0.606</td><td rowspan=\"2\">21.267</td></tr><tr><td>ICP</td><td>0.657</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.582</td><td rowspan=\"2\">22.356</td></tr><tr><td>ICP</td><td>0.541</td></tr><tr><td rowspan=\"8\">RGB + D-Kabsch</td><td rowspan=\"4\">real</td><td rowspan=\"2\">GT</td><td>-</td><td>0.738</td><td rowspan=\"2\">13.830</td></tr><tr><td>ICP</td><td>0.757</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.679</td><td rowspan=\"2\">16.150</td></tr><tr><td>ICP</td><td>0.690</td></tr><tr><td rowspan=\"4\">synt</td><td rowspan=\"2\">GT</td><td>-</td><td>0.796</td><td rowspan=\"2\">12.987</td></tr><tr><td>ICP</td><td>0.793</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.582</td><td rowspan=\"2\">22.356</td></tr><tr><td>ICP</td><td>0.698</td></tr></tbody></table>", "caption": "TABLE X: Pose estimation performance on the Occlusion dataset on different data modalities and type of train data: The table reports in terms of the AR score [14]. An AR score for each data modality and train data type is provided separately on ground truth 2D detections and on YOLO detections. Symmetry-aware median L_{2} correspondence error demonstrates the quality of predicted correspondences.", "list_citation_info": ["[14] T. Hodan and A. Melenovsky. (2019) Bop: Benchmark for 6d object pose estimation: https://bop.felk.cvut.cz/home/."]}, {"table": "<table><tbody><tr><th>Dataset</th><td>Data modality</td><td>Data type</td><td>Bboxes</td><td>Refinement</td><td>AR</td><td>Corr. Err.</td></tr><tr><th rowspan=\"14\">HBD</th><td rowspan=\"6\">RGB</td><td rowspan=\"6\">synt</td><td rowspan=\"3\">GT</td><td>-</td><td>0.723</td><td rowspan=\"3\">9.52</td></tr><tr><td>2 views</td><td>0.808</td></tr><tr><td>4 views</td><td>0.807</td></tr><tr><td rowspan=\"3\">YOLO</td><td>-</td><td>0.668</td><td rowspan=\"3\">13.78</td></tr><tr><td>2 views</td><td>0.766</td></tr><tr><td>4 views</td><td>0.783</td></tr><tr><td rowspan=\"4\">RGBD</td><td rowspan=\"4\">synt</td><td rowspan=\"2\">GT</td><td>-</td><td>0.375</td><td rowspan=\"2\">52.69</td></tr><tr><td>ICP</td><td>0.453</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.362</td><td rowspan=\"2\">54.10</td></tr><tr><td>ICP</td><td>0.438</td></tr><tr><td rowspan=\"4\">RGB + D-Kabsch</td><td rowspan=\"4\">synt</td><td rowspan=\"2\">GT</td><td>-</td><td>0.843</td><td rowspan=\"2\">9.52</td></tr><tr><td>ICP</td><td>0.853</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.793</td><td rowspan=\"2\">13.78</td></tr><tr><td>ICP</td><td>0.801</td></tr></tbody></table>", "caption": "TABLE XI: Pose estimation performance on the Homebrewed on different data modalities and type of train data: The table reports the percentages of correctly estimated poses w.r.t. the Average Recall score [14]. An AR score for each data modality and train data type is provided separately on ground truth 2D detections and on YOLO detections. Symmetry-aware median L_{2} correspondence error demonstrates the quality of predicted correspondences.", "list_citation_info": ["[14] T. Hodan and A. Melenovsky. (2019) Bop: Benchmark for 6d object pose estimation: https://bop.felk.cvut.cz/home/."]}, {"table": "<table><tbody><tr><th>Dataset</th><td>Data modality</td><td>Data type</td><td>Bboxes</td><td>Refinement</td><td>AR</td><td>Corr. Err.</td></tr><tr><th rowspan=\"29\">TLESS</th><td rowspan=\"15\">RGB</td><td rowspan=\"6\">real</td><td rowspan=\"3\">GT</td><td>-</td><td>0.535</td><td rowspan=\"3\">26.946</td></tr><tr><td>2 views</td><td>0.542</td></tr><tr><td>4 views</td><td>0.548</td></tr><tr><td rowspan=\"3\">YOLO</td><td>-</td><td>0.518</td><td rowspan=\"3\">27.516</td></tr><tr><td>2 views</td><td>0.523</td></tr><tr><td>4 views</td><td>0.524</td></tr><tr><td rowspan=\"6\">synt</td><td rowspan=\"3\">GT</td><td>-</td><td>0.729</td><td rowspan=\"3\">10.400</td></tr><tr><td>2 views</td><td>0.798</td></tr><tr><td>4 views</td><td>0.795</td></tr><tr><td rowspan=\"3\">YOLO</td><td>-</td><td>0.636</td><td rowspan=\"3\">20.616</td></tr><tr><td>2 views</td><td>0.689</td></tr><tr><td>4 views</td><td>0.690</td></tr><tr><td rowspan=\"3\">mix</td><td rowspan=\"3\">YOLO</td><td>-</td><td>0.655</td><td rowspan=\"3\">18.373</td></tr><tr><td>2 views</td><td>0.719</td></tr><tr><td>4 views</td><td>0.720</td></tr><tr><td rowspan=\"4\">RGBD</td><td rowspan=\"4\">synt</td><td rowspan=\"2\">GT</td><td>-</td><td>0.322</td><td rowspan=\"2\">34.396</td></tr><tr><td>ICP</td><td>0.354</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.289</td><td rowspan=\"2\">41.087</td></tr><tr><td>ICP</td><td>0.312</td></tr><tr><td rowspan=\"10\">RGB + D-Kabsch</td><td rowspan=\"4\">real</td><td rowspan=\"2\">GT</td><td>-</td><td>0.583</td><td rowspan=\"2\">26.946</td></tr><tr><td>ICP</td><td>0.513</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.560</td><td rowspan=\"2\">27.516</td></tr><tr><td>ICP</td><td>0.490</td></tr><tr><td rowspan=\"4\">synt</td><td rowspan=\"2\">GT</td><td>-</td><td>0.812</td><td rowspan=\"2\">10.400</td></tr><tr><td>ICP</td><td>0.678</td></tr><tr><td rowspan=\"2\">YOLO</td><td>-</td><td>0.699</td><td rowspan=\"2\">20.616</td></tr><tr><td>ICP</td><td>0.590</td></tr><tr><td rowspan=\"2\">mix</td><td rowspan=\"2\">YOLO</td><td>-</td><td>0.720</td><td rowspan=\"2\">18.373</td></tr><tr><td>ICP</td><td>0.610</td></tr></tbody></table>", "caption": "TABLE XII: Pose estimation performance on the TLESS on different data modalities and type of train data: The table reports the percentages of correctly estimated poses w.r.t. the Average Recall score [14]. An AR score for each data modality and train data type is provided separately on ground truth 2D detections and on YOLO detections. Symmetry-aware median L_{2} correspondence error demonstrates the quality of predicted correspondences.", "list_citation_info": ["[14] T. Hodan and A. Melenovsky. (2019) Bop: Benchmark for 6d object pose estimation: https://bop.felk.cvut.cz/home/."]}, {"table": "<table><tbody><tr><td>Dataset</td><td>Data type</td><td>N views</td><td>Views sampling</td><td>ADD</td></tr><tr><td rowspan=\"14\">Linemod</td><td rowspan=\"7\">real</td><td>-</td><td>-</td><td>93.59</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>86.43</td></tr><tr><td>Random</td><td>99.52</td></tr><tr><td>Furthest</td><td>99.71</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>96.53</td></tr><tr><td>Random</td><td>99.91</td></tr><tr><td>Furthest</td><td>99.97</td></tr><tr><td rowspan=\"7\">synt</td><td>-</td><td>-</td><td>81.20</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>87.45</td></tr><tr><td>Random</td><td>98.95</td></tr><tr><td>Furthest</td><td>99.02</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>94.65</td></tr><tr><td>Random</td><td>99.70</td></tr><tr><td>Furthest</td><td>99.80</td></tr></tbody></table>", "caption": "TABLE XIII: Different view sampling strategies for the multi-view refiner on the Linemod [18] dataset.", "list_citation_info": ["[18] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige, and N. Navab, \u201cModel based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes,\u201d in ACCV, 2012."]}, {"table": "<table><tbody><tr><td>Dataset</td><td>Data type</td><td>N views</td><td>Views sampling</td><td>AR</td></tr><tr><td rowspan=\"14\">LMO</td><td rowspan=\"7\">real</td><td>-</td><td>-</td><td>0.568</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>0.627</td></tr><tr><td>Random</td><td>0.661</td></tr><tr><td>Furthest</td><td>0.648</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>0.693</td></tr><tr><td>Random</td><td>0.714</td></tr><tr><td>Furthest</td><td>0.706</td></tr><tr><td rowspan=\"7\">synt</td><td>-</td><td>-</td><td>0.584</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>0.627</td></tr><tr><td>Random</td><td>0.660</td></tr><tr><td>Furthest</td><td>0.664</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>0.693</td></tr><tr><td>Random</td><td>0.695</td></tr><tr><td>Furthest</td><td>0.687</td></tr></tbody></table>", "caption": "TABLE XIV: Different view sampling strategies for the multi-view refiner on the Occlusion [83] dataset. ", "list_citation_info": ["[83] E. Brachmann, F. Michel, A. Krull, M. Ying Yang, S. Gumhold et al., \u201cUncertainty-driven 6d pose estimation of objects and scenes from a single rgb image,\u201d in CVPR, 2016."]}, {"table": "<table><tbody><tr><td>Dataset</td><td>Data type</td><td>N views</td><td>Views sampling</td><td>AR</td></tr><tr><td rowspan=\"7\">HBD</td><td rowspan=\"7\">synt</td><td>-</td><td>-</td><td>0.668</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>0.693</td></tr><tr><td>Random</td><td>0.808</td></tr><tr><td>Furthest</td><td>0.762</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>0.724</td></tr><tr><td>Random</td><td>0.783</td></tr><tr><td>Furthest</td><td>0.775</td></tr></tbody></table>", "caption": "TABLE XV: Different view sampling strategies for the multi-view refiner on the Homebrewed [7] dataset. ", "list_citation_info": ["[7] R. Kaskman, S. Zakharov, I. Shugurov, and S. Ilic, \u201cHomebreweddb: Rgb-d dataset for 6d pose estimation of 3d objects,\u201d in ICCV Workshops, 2019."]}, {"table": "<table><tbody><tr><td>Dataset</td><td>Data type</td><td>N views</td><td>Views sampling</td><td>AR</td></tr><tr><td rowspan=\"21\">TLESS</td><td rowspan=\"7\">real</td><td>-</td><td>-</td><td>0.518</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>0.524</td></tr><tr><td>Random</td><td>0.523</td></tr><tr><td>Furthest</td><td>0.534</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>0.544</td></tr><tr><td>Random</td><td>0.524</td></tr><tr><td>Furthest</td><td>0.540</td></tr><tr><td rowspan=\"7\">synt</td><td>-</td><td>-</td><td>0.636</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>0.693</td></tr><tr><td>Random</td><td>0.690</td></tr><tr><td>Furthest</td><td>0.694</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>0.712</td></tr><tr><td>Random</td><td>0.689</td></tr><tr><td>Furthest</td><td>0.707</td></tr><tr><td rowspan=\"7\">mix</td><td>-</td><td>-</td><td>0.655</td></tr><tr><td rowspan=\"3\">2 views</td><td>Closest</td><td>0.715</td></tr><tr><td>Random</td><td>0.719</td></tr><tr><td>Furthest</td><td>0.725</td></tr><tr><td rowspan=\"3\">4 views</td><td>Closest</td><td>0.738</td></tr><tr><td>Random</td><td>0.720</td></tr><tr><td>Furthest</td><td>0.742</td></tr></tbody></table>", "caption": "TABLE XVI: Different view sampling strategies for the multi-view refiner on the TLESS [52] dataset. ", "list_citation_info": ["[52] T. Hoda\u0148, P. Haluza, \u0160. Obdr\u017e\u00e1lek, J. Matas, M. Lourakis, and X. Zabulis, \u201cT-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects,\u201d WACV, 2017."]}]}