{"title": "Lung swapping autoencoder: Learning a disentangled structure-texture representation of chest radiographs", "abstract": "Well-labeled datasets of chest radiographs (CXRs) are difficult to acquire due to the high cost of annotation. Thus, it is desirable to learn a robust and transferable representation in an unsupervised manner to benefit tasks that lack labeled data. Unlike natural images, medical images have their own domain prior; e.g., we observe that many pulmonary diseases, such as the COVID-19, manifest as changes in the lung tissue texture rather than the anatomical structure. Therefore, we hypothesize that studying only the texture without the influence of structure variations would be advantageous for downstream prognostic and predictive modeling tasks. In this paper, we propose a generative framework, the Lung Swapping Autoencoder (LSAE), that learns factorized representations of a CXR to disentangle the texture factor from the structure factor. Specifically, by adversarial training, the LSAE is optimized to generate a hybrid image that preserves the lung shape in one image but inherits the lung texture of another. To demonstrate the effectiveness of the disentangled texture representation, we evaluate the texture encoder $Enc^t$ in LSAE on ChestX-ray14 (N=112,120), and our own multi-institutional COVID-19 outcome prediction dataset, COVOC (N=340 (Subset-1) + 53 (Subset-2)). On both datasets, we reach or surpass the state-of-the-art by finetuning $Enc^t$ in LSAE that is 77% smaller than a baseline Inception v3. Additionally, in semi-and-self supervised settings with a similar model budget, $Enc^t$ in LSAE is also competitive with the state-of-the-art MoCo. By \"re-mixing\" the texture and shape factors, we generate meaningful hybrid images that can augment the training set. This data augmentation method can further improve COVOC prediction performance. The improvement is consistent even when we directly evaluate the Subset-1 trained model on Subset-2 without any fine-tuning.", "authors": ["Lei Zhou", " Joseph Bae", " Huidong Liu", " Gagandeep Singh", " Jeremy Green", " Amit Gupta", " Dimitris Samaras", " Prateek Prasanna"], "pdf_url": "https://arxiv.org/abs/2201.07344", "list_table_and_caption": [{"table": "<table><tr><td> Pre-train </td><td>Method</td><td>Params</td><td>mAUC \\uparrow</td></tr><tr><td rowspan=\"4\">Supervised</td><td>CXR14-R50 [Wang et al. (2017)]</td><td>23M</td><td>0.745</td></tr><tr><td>ChestNet [Wang &amp; Xia (2018)]</td><td>60M</td><td>0.781</td></tr><tr><td>CheXNet{}^{\\ast} [Rajpurkar et al. (2017)]</td><td>7M</td><td>0.789</td></tr><tr><td>Inception v3</td><td>22M</td><td>0.796</td></tr><tr><td rowspan=\"2\">Unsupervised</td><td>MoCo-R18 [He et al. (2020)]</td><td>11M</td><td>0.786</td></tr><tr><td>Enc^{t} in LSAE</td><td>5M</td><td>0.790</td></tr></table>", "caption": "Table 5: Performance comparison on ChestX-ray14. The texture encoder in LSAE achieves competitive results with a smaller model size. {}^{\\ast}Note that the data split in [Rajpurkar et al. (2017)] is not released. We reimplemented CheXNet on the official split. Note that CheXNet follows the structure of DenseNet-121. The inception v3 model is also self-implemented.", "list_citation_info": ["Wang & Xia (2018) Wang, H., & Xia, Y. (2018). Chestnet: A deep neural network for classification of thoracic diseases on chest radiography. arXiv preprint arXiv:1807.03058, .", "He et al. (2020) He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9729\u20139738).", "Rajpurkar et al. (2017) Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K. et al. (2017). Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, .", "Wang et al. (2017) Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., & Summers, R. M. (2017). Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 2097\u20132106)."]}, {"table": "<table><tr><td>Lung Diseases</td><td>ChestNet [Wang &amp; Xia (2018)]</td><td>Enc^{t} in LSAE</td></tr><tr><td>Atelectasis</td><td>0.7433</td><td>0.7545</td></tr><tr><td>Cardiomegaly</td><td>0.8748</td><td>0.8668</td></tr><tr><td>Effusion</td><td>0.8114</td><td>0.8214</td></tr><tr><td>Infiltration</td><td>0.6772</td><td>0.6927</td></tr><tr><td>Mass</td><td>0.7833</td><td>0.7948</td></tr><tr><td>Nodule</td><td>0.6975</td><td>0.7380</td></tr><tr><td>Pneumonia</td><td>0.6959</td><td>0.7088</td></tr><tr><td>Pneumothorax</td><td>0.8098</td><td>0.8442</td></tr><tr><td>Consolidation</td><td>0.7256</td><td>0.7297</td></tr><tr><td>Edema</td><td>0.8327</td><td>0.8401</td></tr><tr><td>Emphysema</td><td>0.8222</td><td>0.8547</td></tr><tr><td>Fibrosis</td><td>0.8041</td><td>0.8078</td></tr><tr><td>Pleural Thickening</td><td>0.7513</td><td>0.7489</td></tr><tr><td>Hernia</td><td>0.8996</td><td>0.8667</td></tr><tr><td>mAUC</td><td>0.781</td><td>0.7906</td></tr></table>", "caption": "Table 6: Class-wise classification performance on ChestX-ray14. We report class-wise AUC metric of our model, i.e., the texture encoder in a pre-trained LSAE. The mean AUC corresponds to the results reported in Table. 5. We also list class-wise AUC of ChestNet for a reference.", "list_citation_info": ["Wang & Xia (2018) Wang, H., & Xia, Y. (2018). Chestnet: A deep neural network for classification of thoracic diseases on chest radiography. arXiv preprint arXiv:1807.03058, ."]}]}