{"title": "Attention aware cost volume pyramid based multi-view stereo network for 3d reconstruction", "abstract": "We present an efficient multi-view stereo (MVS) network for 3D reconstruction from multiview images. While previous learning based reconstruction approaches performed quite well, most of them estimate depth maps at a fixed resolution using plane sweep volumes with a fixed depth hypothesis at each plane, which requires densely sampled planes for desired accuracy and therefore is difficult to achieve high resolution depth maps. In this paper we introduce a coarseto-fine depth inference strategy to achieve high resolution depth. This strategy estimates the depth map at coarsest level, while the depth maps at finer levels are considered as the upsampled depth map from previous level with pixel-wise depth residual. Thus, we narrow the depth searching range with priori information from previous level and construct new cost volumes from the pixel-wise depth residual to perform depth map refinement. Then the final depth map could be achieved iteratively since all the parameters are shared between different levels. At each level, the self-attention layer is introduced to the feature extraction block for capturing the long range dependencies for depth inference task, and the cost volume is generated using similarity measurement instead of the variance based methods used in previous work. Experiments were conducted on both the DTU benchmark dataset and recently released BlendedMVS dataset. The results demonstrated that our model could outperform most state-of-the-arts (SOTA) methods. The codebase of this project is at https://github.com/ArthasMil/AACVP-MVSNet.", "authors": ["Anzhu Yu", " Wenyue Guo", " Bing Liu", " Xin Chen", " Xin Wang", " Xuefeng Cao", " Bingchuan Jiang"], "pdf_url": "https://arxiv.org/abs/2011.12722", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Methods</th><td>Acc.(mm)</td><td>Comp. (mm)</td><td>OA (mm)</td></tr><tr><th>Camp (Campbell et al., 2008)</th><td>0.835</td><td>0.554</td><td>0.695</td></tr><tr><th>Gipuma (Galliani et al., 2015b)</th><td>0.283</td><td>0.873</td><td>0.578</td></tr><tr><th>Furu (Furukawa and Ponce, 2009)</th><td>0.613</td><td>0.941</td><td>0.777</td></tr><tr><th>SurfaceNet (Ji et al., 2017)</th><td>0.450</td><td>1.040</td><td>0.745</td></tr><tr><th>\\text{MVSNet}^{*} (Yao et al., 2018)</th><td>0.396</td><td>0.527</td><td>0.462</td></tr><tr><th>\\text{R-MVSNet}^{*} (Yao et al., 2019)</th><td>0.383</td><td>0.452</td><td>0.418</td></tr><tr><th>PruMVSNet (Xiang et al., 2020)</th><td>0.495</td><td>0.433</td><td>0.464</td></tr><tr><th>PointMVSNet (Chen et al., 2019a)</th><td>0.361</td><td>0.421</td><td>0.391</td></tr><tr><th>CasMVSNet (Gu et al., 2020)</th><td>0.325</td><td>0.385</td><td>0.355</td></tr><tr><th>CVP-MVSNet (Yang et al., 2020)</th><td>0.296</td><td>0.406</td><td>0.351</td></tr><tr><th>Ours(G=8)</th><td>0.363</td><td>0.332</td><td>0.347</td></tr><tr><th>Ours(G=2)</th><td>0.360</td><td>0.341</td><td>0.351</td></tr><tr><th>Ours(G=4)</th><td>0.357</td><td>0.326</td><td>0.341</td></tr></tbody></table><p>*Official MVSNet and R-MVSNet implementation used the Altizure internal library for post-processing and could achievehigher accuracy compared to the Fusibile toolbox.</p>", "caption": "Table 1:  Quantitative results of reconstruction quality on theDTU evaluation dataset (lower is better).", "list_citation_info": ["Furukawa and Ponce (2009) Furukawa, Y., Ponce, J., 2009. Accurate, dense, and robust multiview stereopsis. IEEE transactions on pattern analysis and machine intelligence 32, 1362\u20131376.", "Galliani et al. (2015b) Galliani, S., Lasinger, K., Schindler, K., 2015b. Massively parallel multiview stereopsis by surface normal diffusion, in: IEEE International Conference on Computer Vision.", "Gu et al. (2020) Gu, X., Fan, Z., Zhu, S., Dai, Z., Tan, F., Tan, P., 2020. Cascade cost volume for high-resolution multi-view stereo and stereo matching, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2495\u20132504.", "Yao et al. (2019) Yao, Y., Luo, Z., Li, S., Shen, T., Fang, T., Quan, L., 2019. Recurrent mvsnet for high-resolution multi-view stereo depth inference, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5525\u20135534.", "Campbell et al. (2008) Campbell, N.D., Vogiatzis, G., Hern\u00e1ndez, C., Cipolla, R., 2008. Using multiple hypotheses to improve depth-maps for multi-view stereo, in: European Conference on Computer Vision, Springer. pp. 766\u2013779.", "Yang et al. (2020) Yang, J., Mao, W., Alvarez, J.M., Liu, M., 2020. Cost volume pyramid based depth inference for multi-view stereo, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4877\u20134886.", "Ji et al. (2017) Ji, M., Gall, J., Zheng, H., Liu, Y., Fang, L., 2017. Surfacenet: An end-to-end 3d neural network for multiview stereopsis, in: Proceedings of the IEEE International Conference on Computer Vision, pp. 2307\u20132315.", "Yao et al. (2018) Yao, Y., Luo, Z., Li, S., Fang, T., Quan, L., 2018. Mvsnet: Depth inference for unstructured multi-view stereo, in: Proceedings of the European Conference on Computer Vision (ECCV), pp. 767\u2013783.", "Chen et al. (2019a) Chen, R., Han, S., Xu, J., Su, H., 2019a. Point-based multi-view stereo network, in: Proceedings of the IEEE International Conference on Computer Vision, pp. 1538\u20131547.", "Xiang et al. (2020) Xiang, X., Wang, Z., Lao, S., Zhang, B., 2020. Pruning multi-view stereo net for efficient 3d reconstruction. ISPRS Journal of Photogrammetry and Remote Sensing 168, 17\u201327."]}]}