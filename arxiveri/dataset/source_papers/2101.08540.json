{"title": "Activity Graph Transformer for Temporal Action Localization", "abstract": "We introduce Activity Graph Transformer, an end-to-end learnable model for temporal action localization, that receives a video as input and directly predicts a set of action instances that appear in the video. Detecting and localizing action instances in untrimmed videos requires reasoning over multiple action instances in a video. The dominant paradigms in the literature process videos temporally to either propose action regions or directly produce frame-level detections. However, sequential processing of videos is problematic when the action instances have non-sequential dependencies and/or non-linear temporal ordering, such as overlapping action instances or re-occurrence of action instances over the course of the video. In this work, we capture this non-linear temporal structure by reasoning over the videos as non-sequential entities in the form of graphs. We evaluate our model on challenging datasets: THUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed model outperforms the state-of-the-art by a considerable margin.", "authors": ["Megha Nawhal", " Greg Mori"], "pdf_url": "https://arxiv.org/abs/2101.08540", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"5\">mAP@tIoU \\uparrow</th></tr><tr><th>0.1</th><th>0.2</th><th>0.3</th><th>0.4</th><th>0.5</th></tr></thead><tbody><tr><th>Oneata et al. [35]</th><td>36.6</td><td>33.6</td><td>27.0</td><td>20.8</td><td>14.4</td></tr><tr><th>Wang et al. [56]</th><td>18.2</td><td>17.0</td><td>14.0</td><td>11.7</td><td>08.3</td></tr><tr><th>Caba et al. [4]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>13.5</td></tr><tr><th>Richard et al. [41]</th><td>39.7</td><td>35.7</td><td>30.0</td><td>23.2</td><td>15.2</td></tr><tr><th>Shou et al. [44]</th><td>47.7</td><td>43.5</td><td>36.3</td><td>28.7</td><td>19.0</td></tr><tr><th>Yeung et al. [63]</th><td>48.9</td><td>44.0</td><td>36.0</td><td>26.4</td><td>17.1</td></tr><tr><th>Yuan et al. [64]</th><td>51.4</td><td>42.6</td><td>33.6</td><td>26.1</td><td>18.8</td></tr><tr><th>Buch et al. [3]</th><td>-</td><td>-</td><td>37.8</td><td>-</td><td>23.0</td></tr><tr><th>Shou et al. [42]</th><td>-</td><td>-</td><td>40.1</td><td>29.4</td><td>23.3</td></tr><tr><th>Yuan et al. [65]</th><td>51.0</td><td>45.2</td><td>36.5</td><td>27.8</td><td>17.8</td></tr><tr><th>Buch et al. [2]</th><td>-</td><td>-</td><td>45.7</td><td>-</td><td>29.2</td></tr><tr><th>Gao et al. [14]</th><td>60.1</td><td>56.7</td><td>50.1</td><td>41.3</td><td>31.0</td></tr><tr><th>Dai et al. [8]</th><td>-</td><td>-</td><td>-</td><td>33.3</td><td>25.6</td></tr><tr><th>Xu et al. [61]</th><td>54.5</td><td>51.5</td><td>44.8</td><td>35.6</td><td>28.9</td></tr><tr><th>Zhao et al. [69]</th><td>66.0</td><td>59.4</td><td>51.9</td><td>41.0</td><td>29.8</td></tr><tr><th>Lin et al. [30]</th><td>-</td><td>-</td><td>53.5</td><td>45.0</td><td>36.9</td></tr><tr><th>Chao et al. [7]</th><td>59.8</td><td>57.1</td><td>53.2</td><td>48.5</td><td>42.8</td></tr><tr><th>Zeng et al. [66]</th><td>69.5</td><td>67.8</td><td>63.6</td><td>57.8</td><td>49.1</td></tr><tr><th>Xu et al. [62]</th><td>66.1</td><td>64.2</td><td>54.5</td><td>47.6</td><td>40.2</td></tr><tr><th>AGT (Ours)</th><td>72.1</td><td>69.8</td><td>65.0</td><td>58.1</td><td>50.2</td></tr></tbody></table>", "caption": "Table 1: Comparison with state-of-the-art (THUMOS14). We report the mean average precision at different intersection over union thresholds (mAP@tIoU) for tIoU\\in\\{0.1,0.2,0.3,0.4,0.5\\}. \\uparrow indicates higher is better.", "list_citation_info": ["[56] Limin Wang, Yu Qiao, and Xiaoou Tang. Action recognition and detection by combining motion and appearance features. THUMOS14 Action Recognition Challenge, 2014.", "[7] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia Deng, and Rahul Sukthankar. Rethinking the faster r-cnn architecture for temporal action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.", "[44] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2016.", "[3] Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard Ghanem, and Juan Carlos Niebles. Sst: Single-stream temporal action proposals. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2017.", "[2] Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, and Juan Carlos Niebles. End-to-end, single-stream temporal action detection in untrimmed videos. In Proceedings of the British Machine Vision Conference (BMVC), 2017.", "[14] Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and Ram Nevatia. Turn tap: Temporal unit regression network for temporal action proposals. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.", "[4] Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Fast temporal activity proposals for efficient detection of human actions in untrimmed videos. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2016.", "[42] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2017.", "[69] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin. Temporal action detection with structured segment networks. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2017.", "[41] Alexander Richard and Juergen Gall. Temporal action detection using a statistical language model. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "[61] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region convolutional 3d network for temporal activity detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[66] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, and Chuang Gan. Graph convolutional networks for temporal action localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.", "[35] Dan Oneata, Jakob Verbeek, and Cordelia Schmid. Action and event recognition with fisher vectors on a compact feature set. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013.", "[63] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei. End-to-end learning of action detection from frame glimpses in videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2678\u20132687, 2016.", "[62] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and Bernard Ghanem. G-tad: Sub-graph localization for temporal action detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.", "[65] Zehuan Yuan, Jonathan C Stroud, Tong Lu, and Jia Deng. Temporal action localization by structured maximal sums. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[64] Jun Yuan, Bingbing Ni, Xiaokang Yang, and Ashraf A Kassim. Temporal action localization with pyramid of score distribution features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "[8] Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S Davis, and Yan Qiu Chen. Temporal context network for activity localization in videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.", "[30] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. Bsn: Boundary sensitive network for temporal action proposal generation. In Proceedings of the European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>mAP \\uparrow</th></tr></thead><tbody><tr><td>Predictive-corrective (Dave et al. [11])</td><td>08.9</td></tr><tr><td>Two-stream (Siggurdson et al. [45])</td><td>08.9</td></tr><tr><td>Two-stream + LSTM (Siggurdson et al. [45])</td><td>09.6</td></tr><tr><td>R-C3D (Xu et al. [61])</td><td>12.7</td></tr><tr><td>SSN (Zhao et al. [69])</td><td>16.4</td></tr><tr><td>I3D baseline  [38]</td><td>17.2</td></tr><tr><td>Super-events (Piergiovanni et al. [39])</td><td>19.4</td></tr><tr><td>TGM (Piergiovanni et al. [39])</td><td>22.3</td></tr><tr><td>Mavroudi et al. [33]</td><td>23.7</td></tr><tr><td>3D ResNet-50 + super-events (Piergiovanni et al. [40])</td><td>25.2</td></tr><tr><td>AGT (Ours)</td><td>28.6</td></tr></tbody></table>", "caption": "Table 2: Comparison with state-of-the-art (Charades). We report mean average precision (mAP) computed using Charades_v1_localize setting in  [45]. \\uparrow: higher is better.", "list_citation_info": ["[11] Achal Dave, Olga Russakovsky, and Deva Ramanan. Predictive-corrective networks for action detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[40] AJ Piergiovanni and Michael S Ryoo. Avid dataset: Anonymized videos from diverse countries. In Advances in Neural Information Processing Systems (NeurIPS), 2020.", "[38] AJ Piergiovanni and Michael Ryoo. Temporal gaussian mixture layer for videos. In Proceedings of the International Conference on Machine Learning (ICML), 2019.", "[69] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin. Temporal action detection with structured segment networks. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2017.", "[61] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region convolutional 3d network for temporal activity detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[39] AJ Piergiovanni and Michael S Ryoo. Learning latent super-events to detect multiple activities in videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.", "[45] Gunnar A Sigurdsson, G\u00fcl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.", "[33] Effrosyni Mavroudi, Benjam\u00edn B\u00e9jar Haro, and Ren\u00e9 Vidal. Representation learning on visual-symbolic graphs for video understanding. In Proceedings of the European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Task</th><th colspan=\"5\">mAP@tIoU \\uparrow</th></tr><tr><th>0.1</th><th>0.2</th><th>0.3</th><th>0.4</th><th>0.5</th></tr></thead><tbody><tr><th></th><th>Verb</th><th>10.51</th><td>09.24</td><td>07.67</td><td>06.40</td><td>05.12</td></tr><tr><th>Damen</th><th>Noun</th><th>10.71</th><td>08.73</td><td>06.75</td><td>05.05</td><td>03.35</td></tr><tr><th>et al. [10]</th><th>Action</th><th>06.78</th><td>06.03</td><td>04.94</td><td>04.04</td><td>03.35</td></tr><tr><th></th><th>Verb</th><th>12.01</th><td>10.25</td><td>08.15</td><td>07.12</td><td>06.14</td></tr><tr><th>AGT</th><th>Noun</th><th>11.63</th><td>09.33</td><td>07.05</td><td>06.57</td><td>03.89</td></tr><tr><th>(Ours)</th><th>Action</th><th>07.78</th><td>06.92</td><td>05.53</td><td>04.22</td><td>03.86</td></tr></tbody></table>", "caption": "Table 3: Comparison with state-of-the-art (EPIC-Kitchens100). We report mean average precision at different intersection over union thresholds (mAP@tIoU) for tIoU\\in\\{0.1,0.2,0.3,0.4,0.5\\}. We use the validation split in the original dataset for testing. \\uparrow indicates higher is better.", "list_citation_info": ["[10] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, , Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision. CoRR, abs/2006.13256, 2020."]}]}