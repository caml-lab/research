{"title": "Align and prompt: Video-and-language pre-training with entity prompts", "abstract": "Video-and-language pre-training has shown promising improvements on various downstream tasks. Most previous methods capture cross-modal interactions with a transformer-based multimodal encoder, not fully addressing the misalignment between unimodal video and text features. Besides, learning fine-grained visual-language alignment usually requires off-the-shelf object detectors to provide object information, which is bottlenecked by the detector's limited vocabulary and expensive computation cost.\n  We propose Align and Prompt: an efficient and effective video-and-language pre-training framework with better cross-modal alignment. First, we introduce a video-text contrastive (VTC) loss to align unimodal video-text features at the instance level, which eases the modeling of cross-modal interactions. Then, we propose a new visually-grounded pre-training task, prompting entity modeling (PEM), which aims to learn fine-grained region-entity alignment. To achieve this, we first introduce an entity prompter module, which is trained with VTC to produce the similarity between a video crop and text prompts instantiated with entity names. The PEM task then asks the model to predict the entity pseudo-labels (i.e~normalized similarity scores) for randomly-selected video crops. The resulting pre-trained model achieves state-of-the-art performance on both text-video retrieval and videoQA, outperforming prior work by a substantial margin. Our code and pre-trained models are available at https://github.com/salesforce/ALPRO.", "authors": ["Dongxu Li", " Junnan Li", " Hongdong Li", " Juan Carlos Niebles", " Steven C. H. Hoi"], "pdf_url": "https://arxiv.org/abs/2112.09583", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>PT datasets</td><td>R1\\uparrow</td><td>R5\\uparrow</td><td>R10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td colspan=\"6\">Finetuning</td></tr><tr><td>JSFusion [55]</td><td>-</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13</td></tr><tr><td>HT100M [39]</td><td>HT (100M)</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9</td></tr><tr><td>ActBERT [57]</td><td>HT (100M)</td><td>16.3</td><td>42.8</td><td>56.9</td><td>10</td></tr><tr><td>NoiseEst. [1]</td><td>HT (100M)</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8</td></tr><tr><td>HERO [12]</td><td>HT (100M)</td><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><td>ClipBERT [26]</td><td> COCO +VG (5.6M) </td><td>22.0</td><td>46.8</td><td>59.9</td><td>6</td></tr><tr><td>AVLNet [25]</td><td>HT (100M)</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4</td></tr><tr><td>VideoClip [52]</td><td>HT (100M)</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><td>SupportSet [43]</td><td>HT (100M)</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3</td></tr><tr><td>FiT [3]</td><td> Web2M +CC3M (5.5M) </td><td>31.0</td><td>59.5</td><td>70.5</td><td>3</td></tr><tr><td>AlPro</td><td> Web2M +CC3M (5.5M) </td><td>33.9</td><td>60.7</td><td>73.2</td><td>3</td></tr><tr><td colspan=\"6\">Zero-shot</td></tr><tr><td>HT100M [39]</td><td>HT (100M)</td><td>7.5</td><td>21.2</td><td>29.6</td><td>38</td></tr><tr><td>ActBERT [57]</td><td>HT (100M)</td><td>8.6</td><td>23.4</td><td>33.1</td><td>36</td></tr><tr><td>SupportSet [43]</td><td>HT (100M)</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31</td></tr><tr><td>MIL-NCE [37]</td><td>HT (100M)</td><td>9.9</td><td>24.0</td><td>32.4</td><td>29.5</td></tr><tr><td>VideoCLIP [52]</td><td>HT (100M)</td><td>10.4</td><td>22.2</td><td>30.0</td><td>-</td></tr><tr><td>FiT [3]</td><td> Web2M +CC3M (5.5M) </td><td>18.7</td><td>39.5</td><td>51.6</td><td>10</td></tr><tr><td>AlPro</td><td> Web2M +CC3M (5.5M) </td><td>24.1</td><td>44.7</td><td>55.4</td><td>8</td></tr></table>", "caption": "Table 2: Comparisons with existing text-to-video retrieval methods with finetuning and zero-shot setups on MSRVTT. We follow the common partition with 7k training videos. Methods using 9k training videos are greyed out. Both partition protocols share the same 1k testing videos. R@k denotes recall (%) with k retrieval efforts; MdR denotes median ranking for retrieved videos. The pre-training datasets are HowTo100M (HT) [39], MS-COCO (COCO) [31], Visual Genome (VG) [22], WebVid2M (Web2M) [3] and Conceptual Captions (CC3M) [46].", "list_citation_info": ["[52] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, 2021.", "[37] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879\u20139889, 2020.", "[46] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.", "[43] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations, 2021.", "[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.", "[1] Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6644\u20136652, 2021.", "[12] Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1999\u20132007, 2019.", "[57] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8746\u20138755, 2020.", "[22] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017.", "[55] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision, pages 471\u2013487, 2018.", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.", "[3] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.", "[25] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9972\u20139981, 2020.", "[39] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630\u20132640, 2019."]}, {"table": "<table><tr><td>Method</td><td>PT datasets</td><td>R1\\uparrow</td><td>R5\\uparrow</td><td>R10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td colspan=\"6\">Finetuning</td></tr><tr><td>S2VT [50]</td><td>-</td><td>11.9</td><td>33.6</td><td>-</td><td>13</td></tr><tr><td>FSE [56]</td><td>-</td><td>13.9</td><td>36.0</td><td>-</td><td>11</td></tr><tr><td>CE [32]</td><td>-</td><td>16.1</td><td>41.1</td><td>-</td><td>8</td></tr><tr><td>MoEE [38]</td><td>-</td><td>16.1</td><td>41.2</td><td>55.2</td><td>8</td></tr><tr><td>ClipBERT [26]</td><td> COCO +VG (5.6M) </td><td>20.4</td><td>48.0</td><td>60.8</td><td>6</td></tr><tr><td>TT-CE [8]</td><td>-</td><td>21.6</td><td>48.6</td><td>62.9</td><td>6</td></tr><tr><td>FiT [3]</td><td> Web2M +CC3M (5.5M) </td><td>31.0</td><td>59.8</td><td>72.4</td><td>3</td></tr><tr><td>AlPro</td><td> Web2M +CC3M (5.5M) </td><td>35.9</td><td>67.5</td><td>78.8</td><td>3</td></tr><tr><td colspan=\"6\">Zero-shot</td></tr><tr><td>VideoCLIP [52]</td><td>HT (100M)</td><td>16.6</td><td>46.9</td><td>-</td><td>-</td></tr><tr><td>FiT [3]</td><td> Web2M +CC3M (5.5M) </td><td>21.1</td><td>46.0</td><td>56.2</td><td>7</td></tr><tr><td>AlPro</td><td> Web2M +CC3M (5.5M) </td><td>23.8</td><td>47.3</td><td>57.9</td><td>6</td></tr></table>", "caption": "Table 3: Comparisons with existing text-to-video retrieval methods with finetuning and zero-shot setups on DiDeMo.R@k denotes recall (%) with k retrieval efforts; MdR denotes median ranking for retrieved videos.", "list_citation_info": ["[52] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, 2021.", "[38] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[56] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In Proceedings of the European Conference on Computer Vision, pages 374\u2013390, 2018.", "[50] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond J Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. In HLT-NAACL, 2015.", "[3] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.", "[32] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In British Machine Vision Conference, 2019.", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.", "[8] Ioana Croitoru, Simion-Vlad Bogolin, Marius Leordeanu, Hailin Jin, Andrew Zisserman, Samuel Albanie, and Yang Liu. Teachtext: Crossmodal generalized distillation for text-video retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11583\u201311593, 2021."]}, {"table": "<table><tr><td>Method</td><td>PT datasets</td><td>MSRVTT</td><td>MSVD</td></tr><tr><td>E-SA [51]</td><td>-</td><td>29.3</td><td>27.6</td></tr><tr><td>ST-TP [17]</td><td>-</td><td>30.9</td><td>31.3</td></tr><tr><td>AMU [51]</td><td>-</td><td>32.5</td><td>32.0</td></tr><tr><td>Co-mem [15]</td><td>-</td><td>32.0</td><td>31.7</td></tr><tr><td>HME [12]</td><td>-</td><td>33.0</td><td>33.7</td></tr><tr><td>LAGCN [16]</td><td>-</td><td>-</td><td>34.3</td></tr><tr><td>HGA [20]</td><td>-</td><td>35.5</td><td>34.7</td></tr><tr><td>QUEST [19]</td><td>-</td><td>34.6</td><td>36.1</td></tr><tr><td>HCRN [25]</td><td>-</td><td>35.6</td><td>36.1</td></tr><tr><td>ClipBERT [26]</td><td> COCO +VG (5.6M) </td><td>37.4</td><td>-</td></tr><tr><td>SSML [1]</td><td>HT (100M)</td><td>35.1</td><td>35.1</td></tr><tr><td>CoMVT [45]</td><td>HT (100M)</td><td>39.5</td><td>42.6</td></tr><tr><td>VQA-T [54]</td><td>HTVQA (69M)</td><td>41.5</td><td>46.3</td></tr><tr><td>AlPro</td><td> Web2M +CC3M (5.5M) </td><td>42.1</td><td>45.9</td></tr></table>", "caption": "Table 4: Comparisons with existing methods on MSRVTT-QA and MSVD-QA in top-1 accuracy (%). VQA-T [54] uses 69M QA domain-specific data to pre-train their model while AlPro uses an order of magnitude less video-text pairs from the web.", "list_citation_info": ["[51] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the ACM international conference on Multimedia, pages 1645\u20131653, 2017.", "[20] Pin Jiang and Yahong Han. Reasoning with heterogeneous graph alignment for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11109\u201311116, 2020.", "[45] Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid. Look before you speak: Visually contextualized utterances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16877\u201316887, 2021.", "[12] Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1999\u20132007, 2019.", "[1] Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6644\u20136652, 2021.", "[19] Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and Yue Gao. Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11101\u201311108, 2020.", "[25] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9972\u20139981, 2020.", "[16] Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. Location-aware graph convolutional networks for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11021\u201311028, 2020.", "[54] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697, 2021.", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.", "[17] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766, 2017.", "[15] Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. Motion-appearance co-memory networks for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6576\u20136585, 2018."]}]}