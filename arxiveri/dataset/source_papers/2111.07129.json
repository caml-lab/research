{"title": "Visual understanding of complex table structures from document images", "abstract": "Table structure recognition is necessary for a comprehensive understanding of documents. Tables in unstructured business documents are tough to parse due to the high diversity of layouts, varying alignments of contents, and the presence of empty cells. The problem is particularly difficult because of challenges in identifying individual cells using visual or linguistic contexts or both. Accurate detection of table cells (including empty cells) simplifies structure extraction and hence, it becomes the prime focus of our work. We propose a novel object-detection-based deep model that captures the inherent alignments of cells within tables and is fine-tuned for fast optimization. Despite accurate detection of cells, recognizing structures for dense tables may still be challenging because of difficulties in capturing long-range row/column dependencies in presence of multi-row/column spanning cells. Therefore, we also aim to improve structure recognition by deducing a novel rectilinear graph-based formulation. From a semantics perspective, we highlight the significance of empty cells in a table. To take these cells into account, we suggest an enhancement to a popular evaluation criterion. Finally, we introduce a modestly sized evaluation dataset with an annotation style inspired by human cognition to encourage new approaches to the problem. Our framework improves the previous state-of-the-art performance by a 2.7% average F1-score on benchmark datasets.", "authors": ["Sachin Raja", " Ajoy Mondal", " C V Jawahar"], "pdf_url": "https://arxiv.org/abs/2111.07129", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th>Document</th><td>Alignment</td><td>#Train</td><td>#Test</td></tr><tr><th>Dataset</th><th>Domain</th><td>Constraint</td><td>Image</td><td>Image</td></tr><tr><th>icdar-2013</th><th>Business</th><td>\\times</td><td>-</td><td>156</td></tr><tr><th>unlv</th><th>Business</th><td>\u2713</td><td>-</td><td>558</td></tr><tr><th>ctdar</th><th>Business</th><td>\\times</td><td>600</td><td>150</td></tr><tr><th>scitsr</th><th>Scientific</th><td>\\times</td><td>12k</td><td>3k</td></tr><tr><th>table2latex</th><th>Scientific</th><td>\\times</td><td>447k</td><td>9k</td></tr><tr><th>tablebank</th><th>Scientific</th><td>\\times</td><td>145k</td><td>1k</td></tr><tr><th>pubTabnet</th><th>Scientific</th><td>\\times</td><td>420k</td><td>40k</td></tr><tr><th>fintabnet</th><th>Business</th><td>\\times</td><td>91k</td><td>10k</td></tr><tr><th>tucd (our)</th><th>Business</th><td>\u2713</td><td>-</td><td>4.5k</td></tr></tbody></table>", "caption": "Table 1: Presents statistics of datasets for table structure recognition. Only tablebank [17] is dedicated for logical table structure recognition. All other datasets are used for physical table structure recognition. ", "list_citation_info": ["[17] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. TableBank: Table benchmark for image-based table detection and recognition. In ICDAR, 2019."]}, {"table": "<table><tbody><tr><th></th><th></th><th></th><td colspan=\"15\">Average Over Test Set</td></tr><tr><th></th><th>Training</th><th></th><td colspan=\"3\">ICDAR-2013</td><td colspan=\"3\">SciTSR</td><td colspan=\"3\">SciTSR Comp</td><td colspan=\"3\">ICDAR-19</td><td colspan=\"3\">TUCD</td></tr><tr><th>Method</th><th>Dataset</th><th>EC</th><td>P\\uparrow</td><td>R\\uparrow</td><td>F1\\uparrow</td><td>P\\uparrow</td><td>R\\uparrow</td><td>F1\\uparrow</td><td>P\\uparrow</td><td>R\\uparrow</td><td>F1\\uparrow</td><td>P\\uparrow</td><td>R\\uparrow</td><td>F1\\uparrow</td><td>P\\uparrow</td><td>R\\uparrow</td><td>F1\\uparrow</td></tr><tr><th>deepdesrt [26]</th><th>icdar-13</th><th></th><td>0.96</td><td>0.87</td><td>0.91</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>splerge(H) [30]</th><th>Private</th><th></th><td>0.96</td><td>0.95</td><td>0.95</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>split [30]</th><th>Private</th><th>sec</th><td>0.87</td><td>0.87</td><td>0.87</td><td>0.92</td><td>0.97</td><td>0.97</td><td>0.91</td><td>0.88</td><td>0.90</td><td>0.70</td><td>0.67</td><td>0.69</td><td>0.87</td><td>0.86</td><td>0.86</td></tr><tr><th>tabstruct-net [24]</th><th>scitsr</th><th></th><td>0.92</td><td>0.90</td><td>0.91</td><td>0.93</td><td>0.91</td><td>0.92</td><td>0.91</td><td>0.88</td><td>0.90</td><td>0.60</td><td>0.57</td><td>0.58</td><td>0.90</td><td>0.89</td><td>0.90</td></tr><tr><th>gte-cell [41]</th><th>fintabnet</th><th></th><td>0.96</td><td>0.97</td><td>0.96</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>sem [40]</th><th>scitsr</th><th></th><td>-</td><td>-</td><td>-</td><td>0.98</td><td>0.97</td><td>0.97</td><td>0.97</td><td>0.95</td><td>0.96</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>lgpma [23]</th><th>scitsr</th><th></th><td>0.93</td><td>0.98</td><td>0.95</td><td>0.98</td><td>0.99</td><td>0.99</td><td>0.97</td><td>0.99</td><td>0.98</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>deepdesrt [26]</th><th>fintabnet</th><th></th><td>0.82</td><td>0.80</td><td>0.81</td><td>0.87</td><td>0.85</td><td>0.86</td><td>0.85</td><td>0.83</td><td>0.84</td><td>0.55</td><td>0.51</td><td>0.53</td><td>0.73</td><td>0.70</td><td>0.72</td></tr><tr><th>dgcnn{}^{\\dagger} [22, 24]</th><th>fintabnet</th><th></th><td>0.94</td><td>0.93</td><td>0.94</td><td>0.91</td><td>0.89</td><td>0.90</td><td>0.89</td><td>0.88</td><td>0.89</td><td>0.73</td><td>0.70</td><td>0.71</td><td>0.89</td><td>0.87</td><td>0.88</td></tr><tr><th>dgcnn{}^{\\ddagger} [22, 24]</th><th>fintabnet</th><th></th><td>0.96</td><td>0.95</td><td>0.96</td><td>0.91</td><td>0.90</td><td>0.91</td><td>0.90</td><td>0.89</td><td>0.89</td><td>0.76</td><td>0.73</td><td>0.74</td><td>0.92</td><td>0.91</td><td>0.91</td></tr><tr><th>tabstruct-net [24]</th><th>fintabnet</th><th>sec</th><td>0.95</td><td>0.94</td><td>0.95</td><td>0.90</td><td>0.89</td><td>0.90</td><td>0.88</td><td>0.87</td><td>0.87</td><td>0.76</td><td>0.73</td><td>0.75</td><td>0.91</td><td>0.90</td><td>0.90</td></tr><tr><th>Ours{}^{\\dagger}</th><th>fintabnet</th><th></th><td>0.95</td><td>0.95</td><td>0.95</td><td>0.92</td><td>0.91</td><td>0.92</td><td>0.92</td><td>0.90</td><td>0.91</td><td>0.72</td><td>0.70</td><td>0.71</td><td>0.91</td><td>0.90</td><td>0.91</td></tr><tr><th>Ours{}^{\\ddagger}</th><th>fintabnet</th><th></th><td>0.98</td><td>0.97</td><td>0.97</td><td>0.94</td><td>0.92</td><td>0.93</td><td>0.93</td><td>0.89</td><td>0.91</td><td>0.77</td><td>0.76</td><td>0.77</td><td>0.94</td><td>0.93</td><td>0.93</td></tr><tr><th>deepdesrt [26]</th><th>fintabnet</th><th></th><td>0.74</td><td>0.71</td><td>0.73</td><td>0.82</td><td>0.80</td><td>0.81</td><td>0.80</td><td>0.79</td><td>0.79</td><td>0.53</td><td>0.48</td><td>0.50</td><td>0.70</td><td>0.68</td><td>0.69</td></tr><tr><th>split [30]</th><th>Private</th><th></th><td>0.83</td><td>0.81</td><td>0.82</td><td>0.89</td><td>0.87</td><td>0.88</td><td>0.87</td><td>0.87</td><td>0.87</td><td>0.68</td><td>0.66</td><td>0.67</td><td>0.82</td><td>0.81</td><td>0.81</td></tr><tr><th>dgcnn{}^{\\dagger} [22, 24]</th><th>fintabnet</th><th></th><td>0.87</td><td>0.85</td><td>0.86</td><td>0.89</td><td>0.87</td><td>0.88</td><td>0.87</td><td>0.85</td><td>0.86</td><td>0.69</td><td>0.67</td><td>0.68</td><td>0.86</td><td>0.85</td><td>0.85</td></tr><tr><th>dgcnn{}^{\\ddagger} [22, 24]</th><th>fintabnet</th><th></th><td>0.90</td><td>0.89</td><td>0.89</td><td>0.88</td><td>0.85</td><td>0.86</td><td>0.86</td><td>0.84</td><td>0.85</td><td>0.71</td><td>0.69</td><td>0.70</td><td>0.89</td><td>0.88</td><td>0.89</td></tr><tr><th>tabstruct-net [24]</th><th>scitsr</th><th>nec</th><td>0.89</td><td>0.87</td><td>0.88</td><td>0.90</td><td>0.87</td><td>0.88</td><td>0.88</td><td>0.86</td><td>0.87</td><td>0.54</td><td>0.49</td><td>0.51</td><td>0.84</td><td>0.83</td><td>0.83</td></tr><tr><th>tabstruct-net [24]</th><th>fintabnet</th><th></th><td>0.90</td><td>0.87</td><td>0.89</td><td>0.88</td><td>0.85</td><td>0.86</td><td>0.86</td><td>0.84</td><td>0.85</td><td>0.70</td><td>0.69</td><td>0.70</td><td>0.88</td><td>0.86</td><td>0.87</td></tr><tr><th>Ours{}^{\\dagger}</th><th>fintabnet</th><th></th><td>0.91</td><td>0.90</td><td>0.90</td><td>0.90</td><td>0.86</td><td>0.88</td><td>0.88</td><td>0.84</td><td>0.86</td><td>0.70</td><td>0.67</td><td>0.68</td><td>0.90</td><td>0.88</td><td>0.89</td></tr><tr><th>Ours{}^{\\ddagger}</th><th>fintabnet</th><th></th><td>0.93</td><td>0.92</td><td>0.92</td><td>0.91</td><td>0.88</td><td>0.89</td><td>0.89</td><td>0.87</td><td>0.88</td><td>0.73</td><td>0.72</td><td>0.72</td><td>0.92</td><td>0.91</td><td>0.92</td></tr></tbody></table>", "caption": "Table 2: Compares various methods for table structure recognition on icdar-2013, sci-tsr, sci-tsr comp, icdar-19 and tucd datasets. Scores in italics are directly reported from corresponding papers. For others, we use open source implementations and pre-trained models released by authors. For deepdesrt [26], we use our implementation. ec: indicates evaluation criteria, sec: indicates standard evaluation criteria, and nec: indicates new evaluation criteria. P: indicates precision, R: indicates recall, and F1: indicates F1 score. tod-net{}^{\\dagger}: indicates tod-net for direct cell detection and tod-net{}^{\\ddagger}: indicates cell detection using intersection of tod-net results row and column predictions, dgcnn{}^{\\dagger} indicates tod-net{}^{\\dagger}+dgcnn+pp, dgcnn{}^{\\ddagger} indicates tod-net{}^{\\ddagger}+dgcnn+pp ts-net indicates tabstruct-net, Ours{}^{\\dagger} indicates tod-net{}^{\\dagger}+tsr+pp, Ours{}^{\\ddagger} indicates tod-net{}^{\\ddagger}+tsr+pp and (H) indicates dataset specific heuristics. For comparison on icdar-2013 using sec, icdar-2013 text-based evaluation was used. All other results are based on a fixed IoU threshold of 0.6. For the nec, we additionally consider empty cells for evaluation. ", "list_citation_info": ["[24] Sachin Raja, Ajoy Mondal, and C. V. Jawahar. Table structure recognition using top-down and bottom-up cues. In ECCV, 2020.", "[30] Christopher Tensmeyer, Vlad Morariu, Brian Price, Scott Cohen, and Tony Martinezp. Deep splitting and merging for table structure decomposition. In ICDAR, 2019.", "[40] Zhenrong Zhang, Jianshu Zhang, and Jun Du. Split, embed and merge: An accurate table structure recognizer. arXiv preprint arXiv:2107.05214, 2021.", "[41] Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global table extractor (GTE): A framework for joint table identification and cell structure recognition using visual context. In WACV, 2021.", "[26] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. DeepDeSRT: Deep learning for detection and structure recognition of tables in document images. In ICDAR, 2017.", "[22] Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait. Rethinking table parsing using graph neural networks. In ICDAR, 2019.", "[23] Liang Qiao, Zaisheng Li, Zhanzhan Cheng, Peng Zhang, Shiliang Pu, Yi Niu, Wenqi Ren, Wenming Tan, and Fei Wu. Lgpma: Complicated table structure recognition with local and global pyramid mask alignment. arXiv preprint arXiv:2105.06224, 2021."]}, {"table": "<table><thead><tr><th></th><th></th><th>FinTabNet</th><th>ICDAR-13</th><th>Sci-TSR</th><th>TUCD</th></tr><tr><th>Method</th><th>IoU</th><th>TSR-F1\\uparrow</th><th>TSR-F1\\uparrow</th><th>TSR-F1\\uparrow</th><th>TSR-F1\\uparrow</th></tr></thead><tbody><tr><th>ts-net</th><td></td><td>0.898</td><td>0.904</td><td>0.876</td><td>0.900</td></tr><tr><th>Ours{}^{\\dagger}</th><td>0.5</td><td>0.906</td><td>0.903</td><td>0.880</td><td>0.889</td></tr><tr><th>Ours{}^{\\ddagger}</th><td></td><td>0.944</td><td>0.904</td><td>0.894</td><td>0.918</td></tr><tr><th>ts-net</th><td></td><td>0.848</td><td>0.886</td><td>0.864</td><td>0.871</td></tr><tr><th>Ours{}^{\\dagger}</th><td>0.6</td><td>0.892</td><td>0.903</td><td>0.878</td><td>0.889</td></tr><tr><th>Ours{}^{\\ddagger}</th><td></td><td>0.920</td><td>0.904</td><td>0.894</td><td>0.918</td></tr><tr><th>ts-net</th><td></td><td>0.704</td><td>0.720</td><td>0.682</td><td>0.722</td></tr><tr><th>Ours{}^{\\dagger}</th><td>0.7</td><td>0.802</td><td>0.820</td><td>0.746</td><td>0.797</td></tr><tr><th>Ours{}^{\\ddagger}</th><td></td><td>0.868</td><td>0.852</td><td>0.823</td><td>0.839</td></tr><tr><th>ts-net</th><td></td><td>0.496</td><td>0.597</td><td>0.565</td><td>0.582</td></tr><tr><th>Ours{}^{\\dagger}</th><td>0.8</td><td>0.561</td><td>0.675</td><td>0.637</td><td>0.659</td></tr><tr><th>Ours{}^{\\ddagger}</th><td></td><td>0.680</td><td>0.748</td><td>0.714</td><td>0.735</td></tr><tr><th>ts-net</th><td></td><td>0.120</td><td>0.292</td><td>0.255</td><td>0.289</td></tr><tr><th>Ours{}^{\\dagger}</th><td>0.9</td><td>0.325</td><td>0.307</td><td>0.296</td><td>0.301</td></tr><tr><th>Ours{}^{\\ddagger}</th><td></td><td>0.404</td><td>0.454</td><td>0.368</td><td>0.408</td></tr></tbody></table>", "caption": "Table 3: Shows the comparison between the performances of the proposed network and tabstruct-net (ts-net) [24] on cell detection and table structure recognition of dataset over various IoU thresholds.tsr: indicates table structure recognition. We use fintabnet [41] dataset for training.", "list_citation_info": ["[24] Sachin Raja, Ajoy Mondal, and C. V. Jawahar. Table structure recognition using top-down and bottom-up cues. In ECCV, 2020.", "[41] Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global table extractor (GTE): A framework for joint table identification and cell structure recognition using visual context. In WACV, 2021."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"3\">Cell Detection</td></tr><tr><th>Method</th><td>P\\uparrow</td><td>R\\uparrow</td><td>F1\\uparrow</td></tr><tr><th>Mask r-cnn+al</th><td>0.880</td><td>0.862</td><td>0.871</td></tr><tr><th>Mask r-cnn+al+cl</th><td>0.891</td><td>0.868</td><td>0.879</td></tr><tr><th>Mask r-cnn+al+cl+ol</th><td>0.907</td><td>0.873</td><td>0.890</td></tr><tr><th>Mask r-cnn +al+</th><td></td><td></td><td></td></tr><tr><th>cl+ol+roi_Att.</th><td>0.922</td><td>0.900</td><td>0.911</td></tr><tr><th>Mask r-cnn+al+</th><td></td><td></td><td></td></tr><tr><th>cl+ol+roi_Att.+losswt</th><td>0.926</td><td>0.904</td><td>0.915</td></tr></tbody></table>", "caption": "Table 4: Shows the ablation study for cell detection on various structural constraints on baseline (Mask r-cnn+al) [24]. We use new evaluation criteria with IoU threshold = 0.6. tod: indicates table object detection, al: indicates alignment loss, cl: indicates continuity loss, ol: indicates overlapping loss, roi_Att.: indicates roi attention, and losswt: indicates loss weights. We use fintabnet [41] dataset for training and evaluation. ", "list_citation_info": ["[24] Sachin Raja, Ajoy Mondal, and C. V. Jawahar. Table structure recognition using top-down and bottom-up cues. In ECCV, 2020.", "[41] Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global table extractor (GTE): A framework for joint table identification and cell structure recognition using visual context. In WACV, 2021."]}]}