{"title": "VGGSound: A large-scale audio-visual dataset", "abstract": "Our goal is to collect a large-scale audio-visual dataset with low label noise from videos in the wild using computer vision techniques. The resulting dataset can be used for training and evaluating audio recognition models. We make three contributions. First, we propose a scalable pipeline based on computer vision techniques to create an audio dataset from open-source media. Our pipeline involves obtaining videos from YouTube; using image classification algorithms to localize audio-visual correspondence; and filtering out ambient noise using audio verification. Second, we use this pipeline to curate the VGGSound dataset consisting of more than 210k videos for 310 audio classes. Third, we investigate various Convolutional Neural Network~(CNN) architectures and aggregation approaches to establish audio recognition baselines for our new dataset. Compared to existing audio datasets, VGGSound ensures audio-visual correspondence and is collected under unconstrained conditions. Code and the dataset are available at http://www.robots.ox.ac.uk/~vgg/data/vggsound/", "authors": ["Honglie Chen", " Weidi Xie", " Andrea Vedaldi", " Andrew Zisserman"], "pdf_url": "https://arxiv.org/abs/2004.14368", "list_table_and_caption": [{"table": "<table><thead><tr><th>Datasets</th><th># Clips</th><th>Length</th><th># Class</th><th>Video</th><th>AV-C</th></tr></thead><tbody><tr><td>UrbanSound [6]</td><td>8k</td><td>8.75h</td><td>10</td><td>\\times</td><td>\\times</td></tr><tr><td>MIVIA [7]</td><td>6k</td><td>29h</td><td>3</td><td>\\times</td><td>\\times</td></tr><tr><td>DCASE2017 [20]</td><td>57k</td><td>89h</td><td>17</td><td>\\times</td><td>\\times</td></tr><tr><td>FSD [8]</td><td>24k</td><td>119h</td><td>398</td><td>\\times</td><td>\\times</td></tr><tr><td>AudioSet [10]</td><td>2.1m</td><td>5833h</td><td>527</td><td>\\checkmark</td><td>\\times</td></tr><tr><td>AVE [21]</td><td>4k</td><td>11.5h</td><td>28</td><td>\\checkmark</td><td>\\checkmark</td></tr><tr><td>VGG-Sound (Ours)</td><td>200k</td><td>550h</td><td>309</td><td>\\checkmark</td><td>\\checkmark</td></tr></tbody></table>", "caption": "Table 2: Statistics for recent audio datasets.\u201c# Clips\u201d, the number of clips in the dataset;\u201cLength\u201d, the total duration of the dataset;\u201c# Classes\u201d, number of classes in the dataset;\u201cVideo\u201d, whether videos are available;\u201cAV-C\u201d, whether audios and videos correspond,in the sense that the sound source is always visually evident within the video clip.", "list_citation_info": ["[10] J Gemmeke, D Ellis, D Freedman, A Jansen, W Lawrence, C Moore, M Plakal, and M Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in Proc. ICASSP, 2017.", "[21] Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu, \u201cAudio-visual event localization in unconstrained videos,\u201d in Proc. ECCV, 2018.", "[8] F. Eduardo, P. Jordi, F. Xavier, F. Frederic, B. Dmitry, F. Andr\u00e9s, O. Sergio, P. Alastair, and S. Xavier, \u201cFreesound datasets: a platform for the creation of open audio datasets,\u201d in ISMIR 2017, 2017.", "[20] A. Mesaros, A. Diment, B. Elizalde, T. Heittola, E. Vincent, B. Raj, and T. Virtanen, \u201cSound event detection in the DCASE 2017 challenge,\u201d IEEE/ACM TASLP, 2019.", "[6] J. Salamon, C. Jacoby, and J. P. Bello, \u201cA dataset and taxonomy for urban sound research,\u201d in Proc. ACMM, 2014.", "[7] P. Foggia, N. Petkov, A. Saggese, N. Strisciuglio, and M. Vento, \u201cReliable detection of audio events in highly noisy environments,\u201d Pattern Recognition Letters, 2015."]}]}