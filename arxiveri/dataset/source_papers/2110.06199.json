{"title": "Abo: Dataset and benchmarks for real-world 3d object understanding", "abstract": "We introduce Amazon Berkeley Objects (ABO), a new large-scale dataset designed to help bridge the gap between real and virtual 3D worlds. ABO contains product catalog images, metadata, and artist-created 3D models with complex geometries and physically-based materials that correspond to real, household objects. We derive challenging benchmarks that exploit the unique properties of ABO and measure the current limits of the state-of-the-art on three open problems for real-world 3D object understanding: single-view 3D reconstruction, material estimation, and cross-domain multi-view object retrieval.", "authors": ["Jasmine Collins", " Shubham Goel", " Kenan Deng", " Achleshwar Luthra", " Leon Xu", " Erhan Gundogdu", " Xi Zhang", " Tomas F. Yago Vicente", " Thomas Dideriksen", " Himanshu Arora", " Matthieu Guillaumin", " Jitendra Malik"], "pdf_url": "https://arxiv.org/abs/2110.06199", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th># Models</th><th># Classes</th><th>Real images</th><th>Full 3D</th><th>PBR</th></tr></thead><tbody><tr><td>ShapeNet [10]</td><td>51.3K</td><td>55</td><td>\u2717</td><td>\u2713</td><td>\u2717</td></tr><tr><td>3D-Future [19]</td><td>16.6K</td><td>8</td><td>\u2717</td><td>\u2713</td><td>\u2717</td></tr><tr><td>Google Scans [56]</td><td>1K</td><td>-</td><td>\u2717</td><td>\u2713</td><td>\u2717</td></tr><tr><td>CO3D [55]</td><td>18.6K</td><td>50</td><td>\u2713</td><td>\u2717</td><td>\u2717</td></tr><tr><td>IKEA [43]</td><td>219</td><td>11</td><td>\u2713</td><td>\u2713</td><td>\u2717</td></tr><tr><td>Pix3D [59]</td><td>395</td><td>9</td><td>\u2713</td><td>\u2713</td><td>\u2717</td></tr><tr><td>PhotoShape [53]</td><td>5.8K</td><td>1</td><td>\u2717</td><td>\u2713</td><td>\u2713</td></tr><tr><td>ABO (Ours)</td><td>8K</td><td>63</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody></table>", "caption": "Table 1: A comparison of the 3D models in ABO and other commonly used object-centric 3D datasets. ABO contains nearly 8K 3D models with physically-based rendering (PBR) materials and corresponding real-world catalog images.", "list_citation_info": ["[10] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.", "[55] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. arXiv preprint arXiv:2109.00512, 2021.", "[19] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. arXiv preprint arXiv:2009.09633, 2020.", "[43] Joseph J Lim, Hamed Pirsiavash, and Antonio Torralba. Parsing ikea objects: Fine pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2992\u20132999, 2013.", "[59] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, and William T Freeman. Pix3d: Dataset and methods for single-image 3d shape modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2974\u20132983, 2018.", "[56] Google Research. Google scanned objects, August.", "[53] Keunhong Park, Konstantinos Rematas, Ali Farhadi, and Steven M Seitz. Photoshape: Photorealistic materials for large-scale shape collections. arXiv preprint arXiv:1809.09761, 2018."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Benchmark</th><th rowspan=\"2\">Domain</th><th rowspan=\"2\">Classes</th><th colspan=\"3\">Instances</th><th colspan=\"4\">Images</th><th rowspan=\"2\">Structure</th><th rowspan=\"2\">Recall@1</th><td></td></tr><tr><th>train</th><th>val</th><th>test</th><th>train</th><th>val</th><th>test-target</th><th>test-query</th><td></td></tr><tr><td>CUB-200-2011</td><td>Birds</td><td>200</td><td>-</td><td>-</td><td>-</td><td>5994</td><td>0</td><td>-</td><td>5794</td><td>15 parts</td><td>79.2%</td><td>[30]</td></tr><tr><td>Cars-196</td><td>Cars</td><td>196</td><td>-</td><td>-</td><td>-</td><td>8144</td><td>0</td><td>-</td><td>8041</td><td>-</td><td>94.8%</td><td>[30]</td></tr><tr><td>In-Shop</td><td>Clothes</td><td>25</td><td>3997</td><td>0</td><td>3985</td><td>25882</td><td>0</td><td>12612</td><td>14218</td><td>Landmarks, poses, masks</td><td>92.6%</td><td>[33]</td></tr><tr><td>SOP</td><td>Ebay</td><td>12</td><td>11318</td><td>0</td><td>11316</td><td>59551</td><td>0</td><td>-</td><td>60502</td><td>-</td><td>84.2%</td><td>[30]</td></tr><tr><td>ABO (MVR)</td><td>Amazon</td><td>562</td><td>49066</td><td>854</td><td>836</td><td>298840</td><td>26235</td><td>4313</td><td>23328</td><td>Subset with 3D models</td><td>30.0%</td><td></td></tr></tbody></table>", "caption": "Table 2: Common image retrieval benchmarks for deep metric learning and their statistics. Our proposed multi-view retrieval (MVR) benchmark based on ABO is significantly larger, more diverse and challenging than existing benchmarks, and exploits 3D models.", "list_citation_info": ["[33] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[30] HeeJae Jun, ByungSoo Ko, Youngjoon Kim, Insik Kim, and Jongtack Kim. Combination of multiple global descriptors for image retrieval. arXiv preprint arXiv:1903.10663, 2019."]}, {"table": "<table><thead><tr><th></th><th colspan=\"6\">Chamfer Distance (\\downarrow)</th><th colspan=\"6\">Absolute Normal Consistency (\\uparrow)</th></tr><tr><th></th><th>bench</th><th>chair</th><th>couch</th><th>cabinet</th><th>lamp</th><th>table</th><th>bench</th><th>chair</th><th>couch</th><th>cabinet</th><th>lamp</th><th>table</th></tr></thead><tbody><tr><th>3D R2N2 [13]</th><td>2.46/0.85</td><td>1.46/0.77</td><td>1.15/0.59</td><td>1.88/0.25</td><td>3.79/2.02</td><td>2.83/0.66</td><td>0.51/0.55</td><td>0.59/0.61</td><td>0.57/0.62</td><td>0.53/0.67</td><td>0.51/0.54</td><td>0.51/0.65</td></tr><tr><th>Occ Nets [48]</th><td>1.72/0.51</td><td>0.72/0.39</td><td>0.86/0.30</td><td>0.80/0.23</td><td>2.53/1.66</td><td>1.79/0.41</td><td>0.66/0.68</td><td>0.67/0.76</td><td>0.70/0.77</td><td>0.71/0.77</td><td>0.65/0.69</td><td>0.67/0.78</td></tr><tr><th>GenRe [71]</th><td>1.54/2.86</td><td>0.89/0.79</td><td>1.08/2.18</td><td>1.40/2.03</td><td>3.72/2.47</td><td>2.26/2.37</td><td>0.63/0.56</td><td>0.69/0.67</td><td>0.66/0.60</td><td>0.62/0.59</td><td>0.59/0.57</td><td>0.61/0.59</td></tr><tr><th>Mesh R-CNN [22]</th><td>1.05/0.09</td><td>0.78/0.13</td><td>0.45/0.10</td><td>0.80/0.11</td><td>1.97/0.24</td><td>1.15/0.12</td><td>0.62/0.65</td><td>0.62/0.70</td><td>0.62/0.72</td><td>0.65/0.74</td><td>0.57/0.66</td><td>0.62/0.74</td></tr></tbody></table>", "caption": "Table 3: Single-view 3D reconstruction generalization from ShapeNet to ABO. Chamfer distance and absolute normal consistency of predictions made on ABO objects from common ShapeNet classes. We report the same metrics for ShapeNet objects (denoted in  gray), following the same evaluation protocol. All methods, with the exception of GenRe, are trained on all of the ShapeNet categories listed.", "list_citation_info": ["[22] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 9785\u20139795, 2019.", "[48] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4460\u20134470, 2019.", "[13] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision, pages 628\u2013644. Springer, 2016.", "[71] Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh Tenenbaum, Bill Freeman, and Jiajun Wu. Learning to reconstruct shapes from unseen classes. In Advances in Neural Information Processing Systems, pages 2257\u20132268, 2018."]}, {"table": "<table><thead><tr><th></th><th>Chamfer (\\downarrow)</th><th>Abs. Normal Consistency (\\uparrow)</th></tr></thead><tbody><tr><th>3D R2N2 [13]</th><td>1.97</td><td>0.55</td></tr><tr><th>OccNets [48]</th><td>1.19</td><td>0.70</td></tr><tr><th>GenRe [71]</th><td>1.61</td><td>0.66</td></tr><tr><th>Mesh R-CNN [21]</th><td>0.82</td><td>0.62</td></tr></tbody></table>", "caption": "Table 7: 3D reconstruction on ABO test split. Chamfer distance and absolute normal consistency averaged across all categories", "list_citation_info": ["[21] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In ICCV, 2019.", "[48] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4460\u20134470, 2019.", "[13] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision, pages 628\u2013644. Springer, 2016.", "[71] Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh Tenenbaum, Bill Freeman, and Jiajun Wu. Learning to reconstruct shapes from unseen classes. In Advances in Neural Information Processing Systems, pages 2257\u20132268, 2018."]}]}