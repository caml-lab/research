{"title": "Skip-convolutions for efficient video processing", "abstract": "We propose Skip-Convolutions to leverage the large amount of redundancies in video streams and save computations. Each video is represented as a series of changes across frames and network activations, denoted as residuals. We reformulate standard convolution to be efficiently computed on residual frames: each layer is coupled with a binary gate deciding whether a residual is important to the model prediction,~\\eg foreground regions, or it can be safely skipped, e.g. background regions. These gates can either be implemented as an efficient network trained jointly with convolution kernels, or can simply skip the residuals based on their magnitude. Gating functions can also incorporate block-wise sparsity structures, as required for efficient implementation on hardware platforms. By replacing all convolutions with Skip-Convolutions in two state-of-the-art architectures, namely EfficientDet and HRNet, we reduce their computational cost consistently by a factor of 3~4x for two different tasks, without any accuracy drop. Extensive comparisons with existing model compression, as well as image and video efficiency methods demonstrate that Skip-Convolutions set a new state-of-the-art by effectively exploiting the temporal redundancies in videos.", "authors": ["Amirhossein Habibian", " Davide Abati", " Taco S. Cohen", " Babak Ehteshami Bejnordi"], "pdf_url": "https://arxiv.org/abs/2104.11487", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td>GMAC</td><td>Head</td><td>Sho.</td><td>Elb.</td><td>Wri.</td><td>Hip</td><td>Knee</td><td>Ank.</td><td>Avg</td></tr><tr><th>Park et al. [37]</th><td>-</td><td>79.0</td><td>60.3</td><td>28.7</td><td>16.0</td><td>74.8</td><td>59.2</td><td>49.3</td><td>52.5</td></tr><tr><th>Nie et al. [57]</th><td>-</td><td>80.3</td><td>63.5</td><td>32.5</td><td>21.6</td><td>76.3</td><td>62.7</td><td>53.1</td><td>55.7</td></tr><tr><th>Iqbal et al. [17]</th><td>-</td><td>90.3</td><td>76.9</td><td>59.3</td><td>55.0</td><td>85.9</td><td>76.4</td><td>73.0</td><td>73.8</td></tr><tr><th>Song et al. [45]</th><td>-</td><td>97.1</td><td>95.7</td><td>87.5</td><td>81.6</td><td>98.0</td><td>92.7</td><td>89.8</td><td>92.1</td></tr><tr><th>Luo et al. [31]</th><td>70.98</td><td>98.2</td><td>96.5</td><td>89.6</td><td>86.0</td><td>98.7</td><td>95.6</td><td>90.9</td><td>93.6</td></tr><tr><th>DKD et al. [34]</th><td>8.65</td><td>98.3</td><td>96.6</td><td>90.4</td><td>87.1</td><td>99.1</td><td>96.0</td><td>92.9</td><td>94.0</td></tr><tr><th>HRNet-w32 [52]</th><td>10.19</td><td>98.5</td><td>97.3</td><td>91.8</td><td>87.6</td><td>98.4</td><td>95.4</td><td>90.7</td><td>94.5</td></tr><tr><th>\u2003+S-SVD [18]</th><td>5.04</td><td>97.9</td><td>96.9</td><td>90.6</td><td>87.3</td><td>98.7</td><td>95.3</td><td>91.1</td><td>94.3</td></tr><tr><th>\u2003+W-SVD [68]</th><td>5.08</td><td>97.9</td><td>96.3</td><td>87.2</td><td>82.8</td><td>98.1</td><td>93.2</td><td>88.8</td><td>92.4</td></tr><tr><th>\u2003+L0 [30]</th><td>4.57</td><td>97.1</td><td>95.5</td><td>86.5</td><td>81.7</td><td>98.5</td><td>92.9</td><td>88.6</td><td>92.1</td></tr><tr><th>\u2003+Skip-Conv</th><td>5.30</td><td>98.7</td><td>97.7</td><td>92.0</td><td>88.1</td><td>99.3</td><td>96.6</td><td>91.0</td><td>95.1</td></tr></tbody></table>", "caption": "Table 1: Comparison with the state-of-the-art on JHMDB. Skip-Conv outperforms in PCK the best image and video models, whilst requiring fewer MAC per frame.", "list_citation_info": ["[57] B. Xiaohan Nie, C. Xiong, and S.-C. Zhu. Joint action recognition and pose estimation from video. In CVPR, 2015.", "[45] J. Song, L. Wang, L. Van Gool, and O. Hilliges. Thin-slicing network: A deep structured model for pose estimation in videos. In CVPR, 2017.", "[52] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang, et al. Deep high-resolution representation learning for visual recognition. TPAMI, 2020.", "[18] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. BMVC, 2014.", "[37] D. Park and D. Ramanan. N-best maximal decoders for part models. In ICCV, 2011.", "[68] X. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep convolutional networks for classification and detection. TPAMI, 2015.", "[34] X. Nie, Y. Li, L. Luo, N. Zhang, and J. Feng. Dynamic kernel distillation for efficient pose estimation in videos. In ICCV, 2019.", "[17] U. Iqbal, M. Garbade, and J. Gall. Pose for action-action for pose. In FG, 2017.", "[31] Y. Luo, J. Ren, Z. Wang, W. Sun, J. Pan, J. Liu, J. Pang, and L. Lin. Lstm pose machines. In CVPR, 2018.", "[30] C. Louizos, M. Welling, and D. P. Kingma. Learning sparse neural networks through l_0 regularization. In ICLR, 2018."]}]}