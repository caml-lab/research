{"title": "Habitat-web: Learning embodied object-search strategies from human demonstrations at scale", "abstract": "We present a large-scale study of imitating human demonstrations on tasks that require a virtual robot to search for objects in new environments -- (1) ObjectGoal Navigation (e.g. 'find & go to a chair') and (2) Pick&Place (e.g. 'find mug, pick mug, find counter, place mug on counter'). First, we develop a virtual teleoperation data-collection infrastructure -- connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale. We collect 80k demonstrations for ObjectNav and 12k demonstrations for Pick&Place, which is an order of magnitude larger than existing human demonstration datasets in simulation or on real robots.\n  Second, we attempt to answer the question -- how does large-scale imitation learning (IL) (which hasn't been hitherto possible) compare to reinforcement learning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no bells or whistles) using 70k human demonstrations outperforms RL using 240k agent-gathered trajectories. The IL-trained agent demonstrates efficient object-search behavior -- it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view -- none of these are exhibited as prominently by the RL agent, and to induce these behaviors via RL would require tedious reward engineering. Finally, accuracy vs. training data size plots show promising scaling behavior, suggesting that simply collecting more demonstrations is likely to advance the state of art further. On Pick&Place, the comparison is starker -- IL agents achieve ${\\sim}$18% success on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0%. Overall, our work provides compelling evidence for investing in large-scale imitation learning.\n  Project page: https://ram81.github.io/projects/habitat-web.", "authors": ["Ram Ramrakhya", " Eric Undersander", " Dhruv Batra", " Abhishek Das"], "pdf_url": "https://arxiv.org/abs/2204.03514", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>Success (\\mathbf{\\uparrow})</th><th>SPL (\\mathbf{\\uparrow})</th></tr></thead><tbody><tr><th>18)\u2009IL wo/ Vision</th><td>0.0\\%</td><td>0.0\\%</td></tr><tr><th>19)\u2009IL wo/ Semantic Input</th><td>22.7\\%</td><td>6.1\\%</td></tr><tr><th>20)\u2009IL w/ RGBD + Semantic Input</th><td>31.6\\%</td><td>8.5\\%</td></tr></tbody></table>", "caption": "Table 1: ObjectNav ablation results on the MP3Dval split [2, 31].", "list_citation_info": ["[2] P. Anderson, A. X. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva, and A. R. Zamir, \u201cOn evaluation of embodied navigation agents,\u201d arXiv preprint arXiv:1807.06757, 2018."]}, {"table": "<table><thead><tr><th></th><th>Team / Method</th><th>Success (\\mathbf{\\uparrow})</th><th>SPL (\\mathbf{\\uparrow})</th></tr></thead><tbody><tr><th>21)</th><th>DD-PPO baseline [13, 15]</th><td>6.2\\%</td><td>2.1\\%</td></tr><tr><th>22)</th><th>Active Exploration (Pre-explore)</th><td>8.9\\%</td><td>4.1\\%</td></tr><tr><th>23)</th><th>SRCB-robot-sudoer</th><td>14.4\\%</td><td>7.5\\%</td></tr><tr><th>24)</th><th>SemExp [51]</th><td>17.9\\%</td><td>7.1\\%</td></tr><tr><th>25)</th><th>Red Rabbit (6-Act Base) [15]</th><td>24.5\\%</td><td>6.4\\%</td></tr><tr><th>26)</th><th>Red Rabbit (6-Act Tether) [15]</th><td>21.1\\%</td><td>8.1\\%</td></tr><tr><th>27)</th><th>ExploreTillSeen + THDA [16]</th><td>21.1\\%</td><td>8.8\\%</td></tr><tr><th>28)</th><th>IL w/ 70k Human Demos</th><td>\\mathbf{27.8\\%}</td><td>\\mathbf{9.9\\%}</td></tr></tbody></table>", "caption": "Table 2: Results on Habitat ObjectNav Challengetest-std [52].", "list_citation_info": ["[16] O. Maksymets, V. Cartillier, A. Gokaslan, E. Wijmans, W. Galuba, S. Lee, and D. Batra, \u201cTHDA: Treasure Hunt Data Augmentation for Semantic Navigation,\u201d in ICCV, 2021.", "[51] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdinov, \u2018\u2018Object goal navigation using goal-oriented semantic exploration,\u2019\u2019 in NeurIPS, 2020.", "[13] E. Wijmans, A. Kadian, A. Morcos, S. Lee, I. Essa, D. Parikh, M. Savva, and D. Batra, \u201cDD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames,\u201d in ICLR, 2020.", "[15] J. Ye, D. Batra, A. Das, and E. Wijmans, \u201cAuxiliary Tasks and Exploration Enable ObjectNav,\u201d in ICCV, 2021.", "[52] H. Team, \u2018\u2018Habitat challenge, 2020.\u2019\u2019 https://aihabitat.org/challenge/2020, 2020."]}, {"table": "<table><thead><tr><th></th><th>Method</th><th>OC (%)</th><th>SC (%)</th><th>GRTS (%)</th><th>Peeks (%)</th><th>PT (%)</th><th>Beeline (%)</th><th>ES (%)</th></tr></thead><tbody><tr><th>38)</th><th>IL w/ shortest paths</th><td>4.2\\pm 1.1</td><td>31.2\\pm 3.2</td><td>20.5\\pm 4.3</td><td>3.0\\pm 1.9</td><td>0.0\\pm 0.0</td><td>0.0\\pm 5.2</td><td>10.1\\pm 3.5</td></tr><tr><th>39)</th><th>IL w/ human demos</th><td>21.4\\pm 1.8</td><td>72.1\\pm 3.5</td><td>22.4\\pm 4.1</td><td>19.6\\pm 4.4</td><td>4.3\\pm 2.3</td><td>10.3\\pm 5.1</td><td>55.3\\pm 5.6</td></tr><tr><th>40)</th><th>RL [15]</th><td>14.6\\pm 1.6</td><td>66.6\\pm 5.1</td><td>27.7\\pm 8.5</td><td>9.7\\pm 5.5</td><td>0.0\\pm 0.0</td><td>0.1\\pm 2.2</td><td>49.0\\pm 7.0</td></tr><tr><th>41)</th><th>Humans</th><td>15.4\\pm 1.6</td><td>70.3\\pm 3.4</td><td>-</td><td>13.8\\pm 3.9</td><td>5.1\\pm 2.4</td><td>23.6\\pm 4.8</td><td>52.1\\pm 5.6</td></tr></tbody></table>", "caption": "Table 4: Quantifying semantic exploration behaviors for IL agents trainedon shortest paths (row 1) and human demonstrations (row 2), the Red RabbitRL agent [15] (row 3), and humans (row 4).", "list_citation_info": ["[15] J. Ye, D. Batra, A. Das, and E. Wijmans, \u201cAuxiliary Tasks and Exploration Enable ObjectNav,\u201d in ICCV, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Success (\\mathbf{\\uparrow})</th><th>SPL (\\mathbf{\\uparrow})</th></tr></thead><tbody><tr><th>42)\u2009Random</th><td>0.4\\%</td><td>0.4\\%</td></tr><tr><th>43)\u2009RGBD+RL [19]</th><td>8.2\\%</td><td>2.7\\%</td></tr><tr><th>44)\u2009RGBD+Semantics+RL [54]</th><td>15.9\\%</td><td>4.9\\%</td></tr><tr><th>45)\u2009Classical Map + FBE</th><td>40.3\\%</td><td>12.4\\%</td></tr><tr><th>46)\u2009Active Neural SLAM [55]</th><td>44.6\\%</td><td>14.5\\%</td></tr><tr><th>47)\u2009SemExp [51]</th><td>54.4\\%</td><td>19.9\\%</td></tr><tr><th>48)\u2009PONI [53]</th><td>73.6\\%</td><td>41.0\\%</td></tr><tr><th>49)\u2009IL w/ 70k Human Demos (Zero-Shot)</th><td>49.5\\%</td><td>16.4\\%</td></tr></tbody></table>", "caption": "Table 5: ObjectNav results on the Gibson val split.", "list_citation_info": ["[51] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdinov, \u2018\u2018Object goal navigation using goal-oriented semantic exploration,\u2019\u2019 in NeurIPS, 2020.", "[54] A. Mousavian, A.toshev, M. Fiser, J. Kosecka, A. Wahid, and J. Davidson, \u2018\u2018Visual representations for semantic target driven navigation,\u2019\u2019 in ICRA, 2019.", "[55] D. S. Chaplot, S. Gupta, D. Gandhi, A. Gupta, and R. Salakhutdinov, \u2018\u2018Learning to explore using active neural mapping,\u2019\u2019 in ICLR, 2020.", "[19] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V. Koltun, J. Malik, et al., \u201cHabitat: A platform for embodied AI research,\u201d in ICCV, 2019.", "[53] S. K. Ramakrishnan, D. S. Chaplot, Z. Al-Halah, J. Malik, and K. Grauman, \u2018\u2018PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning,\u2019\u2019 in CVPR, 2022."]}]}