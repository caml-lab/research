{"title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", "abstract": "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.", "authors": ["Elad Ben Zaken", " Shauli Ravfogel", " Yoav Goldberg"], "pdf_url": "https://arxiv.org/abs/2106.10199", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th></th><td>%Param</td><td>QNLI</td><td>SST-2</td><td>MNLI<sub>m</sub></td><td>MNLI<sub>mm</sub></td><td>CoLA</td><td>MRPC</td><td>STS-B</td><td>RTE</td><td>QQP</td><td>Avg.</td></tr><tr><th></th><th>Train size</th><td></td><td>105k</td><td>67k</td><td>393k</td><td>393k</td><td>8.5k</td><td>3.7k</td><td>7k</td><td>2.5k</td><td>364k</td><td></td></tr><tr><th>(V)</th><th>Full-FT\\dagger</th><td>100%</td><td>93.5</td><td>94.1</td><td>86.5</td><td>87.1</td><td>62.8</td><td>91.9</td><td>89.8</td><td>71.8</td><td>87.6</td><td>84.8</td></tr><tr><th>(V)</th><th>Full-FT</th><td>100%</td><td>91.7\\pm0.1</td><td>93.4\\pm0.2</td><td>85.5\\pm0.4</td><td>85.7\\pm0.4</td><td>62.2\\pm1.2</td><td>90.7\\pm0.3</td><td>90.0\\pm0.4</td><td>71.9\\pm1.3</td><td>87.5\\pm0.4</td><td>84.1</td></tr><tr><th>(V)</th><th>Diff-Prune\\dagger</th><td>0.5%</td><td>93.4</td><td>94.2</td><td>86.4</td><td>86.9</td><td>63.5</td><td>91.3</td><td>89.5</td><td>71.5</td><td>86.6</td><td>84.6</td></tr><tr><th>(V)</th><th>BitFit</th><td>0.08%</td><td>91.4\\pm2.4</td><td>93.2\\pm0.4</td><td>84.4\\pm0.2</td><td>84.8\\pm0.1</td><td>63.6\\pm0.7</td><td>91.7\\pm0.5</td><td>90.3\\pm0.1</td><td>73.2\\pm3.7</td><td>85.4\\pm0.1</td><td>84.2</td></tr><tr><th>(T)</th><th>Full-FT\\ddagger</th><td>100%</td><td>91.1</td><td>94.9</td><td>86.7</td><td>85.9</td><td>60.5</td><td>89.3</td><td>87.6</td><td>70.1</td><td>72.1</td><td>81.8</td></tr><tr><th>(T)</th><th>Full-FT\\dagger</th><td>100%</td><td>93.4</td><td>94.1</td><td>86.7</td><td>86.0</td><td>59.6</td><td>88.9</td><td>86.6</td><td>71.2</td><td>71.7</td><td>81.5</td></tr><tr><th>(T)</th><th>Adapters\\ddagger</th><td>3.6%</td><td>90.7</td><td>94.0</td><td>84.9</td><td>85.1</td><td>59.5</td><td>89.5</td><td>86.9</td><td>71.5</td><td>71.8</td><td>81.1</td></tr><tr><th>(T)</th><th>Diff-Prune\\dagger</th><td>0.5%</td><td>93.3</td><td>94.1</td><td>86.4</td><td>86.0</td><td>61.1</td><td>89.7</td><td>86.0</td><td>70.6</td><td>71.1</td><td>81.5</td></tr><tr><th>(T)</th><th>BitFit</th><td>0.08%</td><td>92.0</td><td>94.2</td><td>84.5</td><td>84.8</td><td>59.7</td><td>88.9</td><td>85.5</td><td>72.0</td><td>70.5</td><td>80.9</td></tr></tbody></table>", "caption": "Table 1:  BERT<sub>LARGE</sub> model performance on the GLUE benchmark validation set (V) and test set (T). Lines with \\dagger and \\ddagger indicate results taken from Guo et al. (2020) and Houlsby et al. (2019) (respectively).", "list_citation_info": ["Guo et al. (2020) Demi Guo, Alexander M. Rush, and Yoon Kim. 2020. Parameter-efficient transfer learning with diff pruning.", "Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. CoRR, abs/1902.00751."]}]}