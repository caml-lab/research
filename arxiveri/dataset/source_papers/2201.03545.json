{"title": "A convnet for the 2020s", "abstract": "The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.", "authors": ["Zhuang Liu", " Hanzi Mao", " Chao-Yuan Wu", " Christoph Feichtenhofer", " Trevor Darrell", " Saining Xie"], "pdf_url": "https://arxiv.org/abs/2201.03545", "list_table_and_caption": [{"table": "<table><tr><td>backbone</td><td>FLOPs</td><td>FPS</td><td>\\text{AP}^{\\text{box}}</td><td>\\text{AP}^{\\text{box}}_{50}</td><td>\\text{AP}^{\\text{box}}_{75}</td><td>\\text{AP}^{\\text{mask}}</td><td>\\text{AP}^{\\text{mask}}_{\\text{50}}</td><td>\\text{AP}^{\\text{mask}}_{75}</td></tr><tr><td colspan=\"9\">Mask-RCNN 3\\times schedule</td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-T</td><td>267G</td><td>23.1</td><td>46.0</td><td>68.1</td><td>50.3</td><td>41.6</td><td>65.1</td><td>44.9</td></tr><tr><td>\\bullet\u2009ConvNeXt-T</td><td>262G</td><td>25.6</td><td>46.2</td><td>67.9</td><td>50.8</td><td>41.7</td><td>65.0</td><td>44.9</td></tr><tr><td colspan=\"9\">Cascade Mask-RCNN 3\\times schedule</td></tr><tr><td>\\bullet\u2009ResNet-50</td><td>739G</td><td>16.2</td><td>46.3</td><td>64.3</td><td>50.5</td><td>40.1</td><td>61.7</td><td>43.4</td></tr><tr><td>\\bullet\u2009X101-32</td><td>819G</td><td>13.8</td><td>48.1</td><td>66.5</td><td>52.4</td><td>41.6</td><td>63.9</td><td>45.2</td></tr><tr><td>\\bullet\u2009X101-64</td><td>972G</td><td>12.6</td><td>48.3</td><td>66.4</td><td>52.3</td><td>41.7</td><td>64.0</td><td>45.1</td></tr><tr><td> \\mathbf{\\circ}\u2009Swin-T</td><td>745G</td><td>12.2</td><td>50.4</td><td>69.2</td><td>54.7</td><td>43.7</td><td>66.6</td><td>47.3</td></tr><tr><td>\\bullet\u2009ConvNeXt-T</td><td>741G</td><td>13.5</td><td>50.4</td><td>69.1</td><td>54.8</td><td>43.7</td><td>66.5</td><td>47.3</td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-S</td><td>838G</td><td>11.4</td><td>51.9</td><td>70.7</td><td>56.3</td><td>45.0</td><td>68.2</td><td>48.8</td></tr><tr><td>\\bullet\u2009ConvNeXt-S</td><td>827G</td><td>12.0</td><td>51.9</td><td>70.8</td><td>56.5</td><td>45.0</td><td>68.4</td><td>49.1</td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-B</td><td>982G</td><td>10.7</td><td>51.9</td><td>70.5</td><td>56.4</td><td>45.0</td><td>68.1</td><td>48.9</td></tr><tr><td>\\bullet\u2009ConvNeXt-B</td><td>964G</td><td>11.4</td><td>52.7</td><td>71.3</td><td>57.2</td><td>45.6</td><td>68.9</td><td>49.5</td></tr><tr><td> \\mathbf{\\circ}\u2009Swin-B{}^{\\ddagger}</td><td>982G</td><td>10.7</td><td>53.0</td><td>71.8</td><td>57.5</td><td>45.8</td><td>69.4</td><td>49.7</td></tr><tr><td>\\bullet\u2009ConvNeXt-B{}^{\\ddagger}</td><td>964G</td><td>11.5</td><td>54.0</td><td>73.1</td><td>58.8</td><td>46.9</td><td>70.6</td><td>51.3</td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-L{}^{\\ddagger}</td><td>1382G</td><td>9.2</td><td>53.9</td><td>72.4</td><td>58.8</td><td>46.7</td><td>70.1</td><td>50.8</td></tr><tr><td>\\bullet\u2009ConvNeXt-L{}^{\\ddagger}</td><td>1354G</td><td>10.0</td><td>54.8</td><td>73.8</td><td>59.8</td><td>47.6</td><td>71.3</td><td>51.7</td></tr><tr><td>\\bullet\u2009ConvNeXt-XL{}^{\\ddagger}</td><td>1898G</td><td>8.6</td><td>55.2</td><td>74.2</td><td>59.9</td><td>47.7</td><td>71.6</td><td>52.2</td></tr></table>", "caption": "Table 3: COCO object detection and segmentation results using Mask-RCNN and Cascade Mask-RCNN. {}^{\\ddagger} indicates that the model is pre-trained on ImageNet-22K. ImageNet-1K pre-trained Swin results are from their Github repository [3]. AP numbers of the ResNet-50 and X101 models are from [45]. We measure FPS on an A100 GPU. FLOPs are calculated with image size (1280, 800).", "list_citation_info": ["[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. 2021.", "[3] GitHub repository: Swin transformer for object detection. https://github.com/SwinTransformer/Swin-Transformer-Object-Detection, 2021."]}, {"table": "<table><tr><td>backbone</td><td>input crop.</td><td>mIoU</td><td>#param.</td><td>FLOPs</td></tr><tr><td colspan=\"2\">  ImageNet-1K pre-trained</td><td></td><td></td><td></td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-T</td><td>512{}^{2}</td><td>45.8</td><td>60M</td><td>945G</td></tr><tr><td>\\bullet\u2009ConvNeXt-T</td><td>512{}^{2}</td><td>46.7</td><td>60M</td><td>939G</td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-S</td><td>512{}^{2}</td><td>49.5</td><td>81M</td><td>1038G</td></tr><tr><td>\\bullet\u2009ConvNeXt-S</td><td>512{}^{2}</td><td>49.6</td><td>82M</td><td>1027G</td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-B</td><td>512{}^{2}</td><td>49.7</td><td>121M</td><td>1188G</td></tr><tr><td>\\bullet\u2009ConvNeXt-B</td><td>512{}^{2}</td><td>49.9</td><td>122M</td><td>1170G</td></tr><tr><td colspan=\"4\">ImageNet-22K pre-trained</td><td></td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-B{}^{\\ddagger}</td><td>640{}^{2}</td><td>51.7</td><td>121M</td><td>1841G</td></tr><tr><td>\\bullet\u2009ConvNeXt-B{}^{\\ddagger}</td><td>640{}^{2}</td><td>53.1</td><td>122M</td><td>1828G</td></tr><tr><td>\\mathbf{\\circ}\u2009Swin-L{}^{\\ddagger}</td><td>640{}^{2}</td><td>53.5</td><td>234M</td><td>2468G</td></tr><tr><td>\\bullet\u2009ConvNeXt-L{}^{\\ddagger}</td><td>640{}^{2}</td><td>53.7</td><td>235M</td><td>2458G</td></tr><tr><td>\\bullet\u2009ConvNeXt-XL{}^{\\ddagger}</td><td>640{}^{2}</td><td>54.0</td><td>391M</td><td>3335G</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 4: ADE20K validation results using UperNet [85]. {}^{\\ddagger} indicates IN-22K pre-training. Swins\u2019 results are from its GitHub repository [2]. Following Swin, we report mIoU results with multi-scale testing. FLOPs are based on input sizes of (2048, 512) and (2560, 640) for IN-1K and IN-22K pre-trained models, respectively.", "list_citation_info": ["[85] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018.", "[2] GitHub repository: Swin transformer. {https://github.com/microsoft/Swin-Transformer}, 2021."]}, {"table": "<table><tr><td></td><td>ConvNeXt-T/S/B/L</td><td>ConvNeXt-T/S/B/L/XL</td></tr><tr><td rowspan=\"2\">(pre-)training config</td><td>ImageNet-1K</td><td>ImageNet-22K</td></tr><tr><td>224{}^{2}</td><td>224{}^{2}</td></tr><tr><td>weight init</td><td>trunc. normal (0.2)</td><td>trunc. normal (0.2)</td></tr><tr><td>optimizer</td><td>AdamW</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>4e-3</td><td>4e-3</td></tr><tr><td>weight decay</td><td>0.05</td><td>0.05</td></tr><tr><td>optimizer momentum</td><td>\\beta_{1},\\beta_{2}{=}0.9,0.999</td><td>\\beta_{1},\\beta_{2}{=}0.9,0.999</td></tr><tr><td>batch size</td><td>4096</td><td>4096</td></tr><tr><td>training epochs</td><td>300</td><td>90</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td><td>cosine decay</td></tr><tr><td>warmup epochs</td><td>20</td><td>5</td></tr><tr><td>warmup schedule</td><td>linear</td><td>linear</td></tr><tr><td>layer-wise lr decay [12, 6]</td><td>None</td><td>None</td></tr><tr><td>randaugment [14]</td><td>(9, 0.5)</td><td>(9, 0.5)</td></tr><tr><td>mixup [90]</td><td>0.8</td><td>0.8</td></tr><tr><td>cutmix [89]</td><td>1.0</td><td>1.0</td></tr><tr><td>random erasing [91]</td><td>0.25</td><td>0.25</td></tr><tr><td>label smoothing [69]</td><td>0.1</td><td>0.1</td></tr><tr><td>stochastic depth [37]</td><td>0.1/0.4/0.5/0.5</td><td>0.0/0.0/0.1/0.1/0.2</td></tr><tr><td>layer scale [74]</td><td>1e-6</td><td>1e-6</td></tr><tr><td>head init scale [74]</td><td>None</td><td>None</td></tr><tr><td>gradient clip</td><td>None</td><td>None</td></tr><tr><td>exp. mov. avg. (EMA) [51]</td><td>0.9999</td><td>None</td></tr></table>", "caption": "Table 5: ImageNet-1K/22K (pre-)training settings. Multiple stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model (e.g., ConvNeXt-T/S/B/L) respectively.", "list_citation_info": ["[14] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In CVPR Workshops, 2020.", "[69] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.", "[37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016.", "[91] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, 2020.", "[51] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 1992.", "[74] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. ICCV, 2021.", "[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020.", "[89] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.", "[90] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018."]}, {"table": "<table><tr><td>Model</td><td>Data/Size</td><td>FLOPs / Params</td><td>Clean</td><td>C (\\downarrow)</td><td>\\bar{\\text{C}} (\\downarrow)</td><td>A</td><td>R</td><td>SK</td></tr><tr><td>ResNet-50</td><td>1K/224{}^{2}</td><td>4.1 / 25.6</td><td>76.1</td><td>76.7</td><td>57.7</td><td>0.0</td><td>36.1</td><td>24.1</td></tr><tr><td>Swin-T [45]</td><td>1K/224{}^{2}</td><td>4.5 / 28.3</td><td>81.2</td><td>62.0</td><td>-</td><td>21.6</td><td>41.3</td><td>29.1</td></tr><tr><td>RVT-S* [47]</td><td>1K/224{}^{2}</td><td>4.7 / 23.3</td><td>81.9</td><td>49.4</td><td>37.5</td><td>25.7</td><td>47.7</td><td>34.7</td></tr><tr><td>ConvNeXt-T</td><td>1K/224{}^{2}</td><td>4.5 / 28.6</td><td>82.1</td><td>53.2</td><td>40.0</td><td>24.2</td><td>47.2</td><td>33.8</td></tr><tr><td>Swin-B [45]</td><td>1K/224{}^{2}</td><td>15.4 / 87.8</td><td>83.4</td><td>54.4</td><td>-</td><td>35.8</td><td>46.6</td><td>32.4</td></tr><tr><td>RVT-B* [47]</td><td>1K/224{}^{2}</td><td>17.7 / 91.8</td><td>82.6</td><td>46.8</td><td>30.8</td><td>28.5</td><td>48.7</td><td>36.0</td></tr><tr><td>ConvNeXt-B</td><td>1K/224{}^{2}</td><td>15.4 / 88.6</td><td>83.8</td><td>46.8</td><td>34.4</td><td>36.7</td><td>51.3</td><td>38.2</td></tr><tr><td>ConvNeXt-B</td><td>22K/384{}^{2}</td><td>45.1 / 88.6</td><td>86.8</td><td>43.1</td><td>30.7</td><td>62.3</td><td>64.9</td><td>51.6</td></tr><tr><td>ConvNeXt-L</td><td>22K/384{}^{2}</td><td>101.0 / 197.8</td><td>87.5</td><td>40.2</td><td>29.9</td><td>65.5</td><td>66.7</td><td>52.8</td></tr><tr><td>ConvNeXt-XL</td><td>22K/384{}^{2}</td><td>179.0 / 350.2</td><td>87.8</td><td>38.8</td><td>27.1</td><td>69.3</td><td>68.2</td><td>55.0</td></tr></table>", "caption": "Table 8: Robustness evaluation of ConvNeXt. We do not make use of any specialized modules or additional fine-tuning procedures. ", "list_citation_info": ["[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. 2021.", "[47] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision transformer. arXiv preprint arXiv:2105.07926, 2021."]}, {"table": "<table><tr><td>model</td><td>IN-1K acc.</td><td>GFLOPs</td></tr><tr><td>ResNet-50 (PyTorch[1])</td><td>76.13</td><td>4.09</td></tr><tr><td>ResNet-50 (enhanced recipe)</td><td>78.82 \\pm 0.07</td><td>4.09</td></tr><tr><td>stage ratio</td><td>79.36 \\pm 0.07</td><td>4.53</td></tr><tr><td>\u201cpatchify\u201d stem</td><td>79.51 \\pm 0.18</td><td>4.42</td></tr><tr><td>depthwise conv</td><td>78.28 \\pm 0.08</td><td>2.35</td></tr><tr><td>increase width</td><td>80.50 \\pm 0.02</td><td>5.27</td></tr><tr><td>inverting dimensions</td><td>80.64 \\pm 0.03</td><td>4.64</td></tr><tr><td>move up depthwise conv</td><td>79.92 \\pm 0.08</td><td>4.07</td></tr><tr><td>kernel size \\rightarrow 5</td><td>80.35 \\pm 0.08</td><td>4.10</td></tr><tr><td>kernel size \\rightarrow 7</td><td>80.57 \\pm 0.14</td><td>4.15</td></tr><tr><td>kernel size \\rightarrow 9</td><td>80.57 \\pm 0.06</td><td>4.21</td></tr><tr><td>kernel size \\rightarrow 11</td><td>80.47 \\pm 0.11</td><td>4.29</td></tr><tr><td>ReLU \\rightarrow GELU</td><td>80.62 \\pm 0.14</td><td>4.15</td></tr><tr><td>fewer activations</td><td>81.27 \\pm 0.06</td><td>4.15</td></tr><tr><td>fewer norms</td><td>81.41 \\pm 0.09</td><td>4.15</td></tr><tr><td>BN \\rightarrow LN</td><td>81.47 \\pm 0.09</td><td>4.46</td></tr><tr><td>separate d.s. conv (ConvNeXt-T)</td><td>81.97 \\pm 0.06</td><td>4.49</td></tr><tr><td>Swin-T [45]</td><td>81.30</td><td>4.50</td></tr></table>", "caption": "Table 10: Detailed results for modernizing a ResNet-50. Mean and standard deviation are obtained by training the network with three different random seeds.", "list_citation_info": ["[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. 2021.", "[1] PyTorch Vision Models. {https://pytorch.org/vision/stable/models.html}. Accessed: 2021-10-01."]}, {"table": "<table><tr><td>model</td><td>IN-1K acc.</td><td>GFLOPs</td></tr><tr><td>ResNet-200 [29]</td><td>78.20</td><td>15.01</td></tr><tr><td>ResNet-200 (enhanced recipe)</td><td>81.14</td><td>15.01</td></tr><tr><td>stage ratio and increase width</td><td>81.33</td><td>14.52</td></tr><tr><td>\u201cpatchify\u201d stem</td><td>81.59</td><td>14.38</td></tr><tr><td>depthwise conv</td><td>80.54</td><td>7.23</td></tr><tr><td>increase width</td><td>81.85</td><td>16.76</td></tr><tr><td>inverting dimensions</td><td>82.64</td><td>15.68</td></tr><tr><td>move up depthwise conv</td><td>82.04</td><td>14.63</td></tr><tr><td>kernel size \\rightarrow 5</td><td>82.32</td><td>14.70</td></tr><tr><td>kernel size \\rightarrow 7</td><td>82.30</td><td>14.81</td></tr><tr><td>kernel size \\rightarrow 9</td><td>82.27</td><td>14.95</td></tr><tr><td>kernel size \\rightarrow 11</td><td>82.18</td><td>15.13</td></tr><tr><td>ReLU \\rightarrow GELU</td><td>82.19</td><td>14.81</td></tr><tr><td>fewer activations</td><td>82.71</td><td>14.81</td></tr><tr><td>fewer norms</td><td>83.17</td><td>14.81</td></tr><tr><td>BN \\rightarrow LN</td><td>83.35</td><td>14.81</td></tr><tr><td>separate d.s. conv (ConvNeXt-B)</td><td>83.60</td><td>15.35</td></tr><tr><td>Swin-B[45]</td><td>83.50</td><td>15.43</td></tr></table>", "caption": "Table 11: Detailed results for modernizing a ResNet-200.", "list_citation_info": ["[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV, 2016.", "[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. 2021."]}]}