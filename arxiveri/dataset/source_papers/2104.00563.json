{"title": "Latent variable sequential set transformers for joint multi-agent motion prediction", "abstract": "Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories. We refer to these architectures as \"AutoBots\". The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the entire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the model's socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the sequential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a single desktop GPU (1080 Ti) in under 48h.", "authors": ["Roger Girgis", " Florian Golemo", " Felipe Codevilla", " Martin Weiss", " Jim Aldon D'Souza", " Samira Ebrahimi Kahou", " Felix Heide", " Christopher Pal"], "pdf_url": "https://arxiv.org/abs/2104.00563", "list_table_and_caption": [{"table": "<p>ModelMin ADE(5)Min ADE(10)Miss RateTop-5 (2m)Miss RateTop-10 (2m)Min FDE(1)Off RoadRateNoah_prediction1.591.370.690.629.230.08CXX1.631.290.690.608.860.08LISA(MHA_JAM)1.811.240.590.468.570.07Trajectron++1.881.510.700.579.520.25CoverNet2.621.920.760.6411.360.13Physics Oracle3.703.700.880.889.090.12WIMP1.841.110.550.438.490.04GOHOME1.421.150.570.476.990.04AutoBot-Ego (c=10)1.431.050.660.458.660.03AutoBot-Ego (ensemble)1.371.030.620.448.190.02</p>", "caption": "Table 1: Quantitative Results on the nuScenes dataset. Other methods: LISA (Messaoud et al., 2020); Trajectron++ (Salzmann et al., 2020); CoverNet (Phan-Minh et al., 2020); Physics Oracle (Caesar et al., 2020); WIMP (Khandelwal et al., 2020); GOHOME (Gilles et al., 2021)", "list_citation_info": ["Messaoud et al. [2020] Kaouther Messaoud, Nachiket Deo, Mohan M Trivedi, and Fawzi Nashashibi. Multi-head attention with joint agent-map representation for trajectory prediction in autonomous driving. arXiv, pages arXiv\u20132005, 2020.", "Phan-Minh et al. [2020] Tung Phan-Minh, E. Grigore, F. Boulton, Oscar Beijbom, and Eric M. Wolff. Covernet: Multimodal behavior prediction using trajectory sets. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14062\u201314071, 2020.", "Salzmann et al. [2020] Tim Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone. Trajectron++: Multi-agent generative trajectory forecasting with heterogeneous data for control. ArXiv, abs/2001.03093, 2020.", "Gilles et al. [2021] Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde. Gohome: Graph-oriented heatmap output forfuture motion estimation. arXiv preprint arXiv:2109.01827, 2021.", "Khandelwal et al. [2020] Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, and Deva Ramanan. What-if motion prediction for autonomous driving, 2020.", "Caesar et al. [2020] H. Caesar, Varun Bankiti, A. Lang, Sourabh Vora, Venice Erin Liong, Q. Xu, A. Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11618\u201311628, 2020."]}, {"table": "<p>ModelMin ADE (\\downarrow)Min FDE (\\downarrow)Miss Rate (\\downarrow)DAC (\\uparrow)AutoBot-Ego (Valid Set)0.731.100.12-AutoBot-Ego (Test Set)0.89 (top-5)1.41 (top-5)0.16 (top-6)0.9886 (top-3)Jean (2020 winner)0.971.420.130.9868WIMP0.901.420.170.9815TNT0.941.540.130.9889LaneGCN0.871.360.160.9800TPCN0.851.350.160.9884mmTransformer0.841.340.150.9842GOHOME0.941.450.1050.9811Scene Transformer0.801.230.130.9899</p>", "caption": "Table 2: Quantitative Results on the Argoverse test set. Other methods that had an associated paper:Jean (Mercat et al., 2020); WIMP (Khandelwal et al., 2020); TNT (Zhao et al., 2020a); LaneGCN (Liang et al., 2020); TPCN (Ye et al., 2021); mmTransformer (Liu et al., 2021); GOHOME (Gilles et al., 2021); Scene Transformer (Ngiam et al., 2021).", "list_citation_info": ["Ngiam et al. [2021] Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zhengdong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, et al. Scene transformer: A unified multi-task model for behavior prediction and planning. arXiv preprint arXiv:2106.08417, 2021.", "Mercat et al. [2020] Jean Mercat, Thomas Gilles, Nicole Zoghby, Guillaume Sandou, Dominique Beauvois, and Guillermo Gil. Multi-head attention for joint multi-modal vehicle motion forecasting. In IEEE International Conference on Robotics and Automation, 2020.", "Ye et al. [2021] Maosheng Ye, Tongyi Cao, and Qifeng Chen. Tpcn: Temporal point cloud networks for motion forecasting. ArXiv, abs/2103.03067, 2021.", "Gilles et al. [2021] Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde. Gohome: Graph-oriented heatmap output forfuture motion estimation. arXiv preprint arXiv:2109.01827, 2021.", "Khandelwal et al. [2020] Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, and Deva Ramanan. What-if motion prediction for autonomous driving, 2020.", "Liang et al. [2020] Ming Liang, Binh Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning lane graph representations for motion forecasting. ArXiv, abs/2007.13732, 2020.", "Liu et al. [2021] Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, and Bolei Zhou. Multimodal motion prediction with stacked transformers. ArXiv, abs/2103.11624, 2021.", "Zhao et al. [2020a] Hang Zhao, Jiyang Gao, T. Lan, Chen Sun, Benjamin Sapp, Balakrishnan Varadarajan, Y. Shen, Yuning Chai, C. Schmid, Congcong Li, and Dragomir Anguelov. Tnt: Target-driven trajectory prediction. ArXiv, abs/2008.08294, 2020a."]}]}