{"title": "Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels", "abstract": "Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings. The data and code are released at the following link: http://www.lujiang.info/cnlw.html", "authors": ["Lu Jiang", " Di Huang", " Mason Liu", " Weilong Yang"], "pdf_url": "https://arxiv.org/abs/1911.09781", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Data</th><th rowspan=\"2\">Method</th><td colspan=\"4\">Noise level (%)</td></tr><tr><td>20</td><td>40</td><td>60</td><td>80</td></tr><tr><th rowspan=\"5\">CIFAR100</th><th>Arazo et al. (2019)</th><td>73.7</td><td>70.1</td><td>59.5</td><td>39.5</td></tr><tr><th>Zhang &amp; Sabuncu (2018)</th><td>67.6</td><td>62.6</td><td>54.0</td><td>29.6</td></tr><tr><th>MentorNet (2018)</th><td>73.5</td><td>68.5</td><td>61.2</td><td>35.5</td></tr><tr><th>Mixup (2018)</th><td>73.9</td><td>66.8</td><td>58.8</td><td>40.1</td></tr><tr><th>Huang et al. (2019)</th><td>74.1</td><td>69.2</td><td>39.4</td><td>-</td></tr><tr><th></th><th>Ours (MentorMix)</th><td>78.6</td><td>71.3</td><td>64.6</td><td>41.2</td></tr><tr><th rowspan=\"7\">CIFAR10</th><th>Arazo et al. (2019)</th><td>94.0</td><td>92.8</td><td>90.3</td><td>74.1</td></tr><tr><th>Zhang &amp; Sabuncu (2018)</th><td>89.7</td><td>87.6</td><td>82.7</td><td>67.9</td></tr><tr><th>Lee et al. (2019)</th><td>87.1</td><td>81.8</td><td>75.4</td><td>-</td></tr><tr><th>Chen et al. (2019)</th><td>89.7</td><td>-</td><td>-</td><td>52.3</td></tr><tr><th>Huang et al. (2019)</th><td>92.6</td><td>90.3</td><td>43.4</td><td>-</td></tr><tr><th>MentorNet (2018)</th><td>92.0</td><td>91.2</td><td>74.2</td><td>60.0</td></tr><tr><th>Mixup (2018)</th><td>94.0</td><td>91.5</td><td>86.8</td><td>76.9</td></tr><tr><th></th><th>Ours (MentorMix)\\dagger</th><td>95.6</td><td>94.2</td><td>91.3</td><td>81.0</td></tr></tbody></table>", "caption": "Table 3: Comparison with the state-of-the-art in terms of the validation accuracy on CIFAR-100 (top) and CIFAR-10 (bottom).", "list_citation_info": ["Jiang et al. (2018) Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. International Conference on Machine Learning (ICML), 2018.", "Lee et al. (2019) Lee, K., Yun, S., Lee, K., Lee, H., Li, B., and Shin, J. Robust inference via generative classifiers for handling noisy labels. International Conference on Machine Learning (ICML), 2019.", "Chen et al. (2019) Chen, P., Liao, B., Chen, G., and Zhang, S. Understanding and utilizing deep neural networks trained with noisy labels. International Conference on Machine Learning (ICML), 2019.", "Huang et al. (2019) Huang, J., Qu, L., Jia, R., and Zhao, B. O2u-net: A simple noisy label detection approach for deep neural networks. In International Conference on Computer Vision (ICCV), 2019.", "Zhang et al. (2018) Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018.", "Arazo et al. (2019) Arazo, E., Ortego, D., Albert, P., O\u2019Connor, N. E., and McGuinness, K. Unsupervised label noise modeling and loss correction. In International Conference on Machine Learning (ICML), 2019.", "Zhang & Sabuncu (2018) Zhang, Z. and Sabuncu, M. Generalized cross entropy loss for training deep neural networks with noisy labels. In Conference on Neural Information Processing Systems (NeurIPS), 2018."]}, {"table": "<table><tbody><tr><th>Data</th><th>Method</th><td>ILSVRC12</td><td>WebVision</td></tr><tr><th>Full</th><th>Lee et al. (2018)\\dagger</th><td>61.0(82.0)</td><td>69.1(86.7)</td></tr><tr><th>Full</th><th>Vanilla</th><td>61.7(82.4)</td><td>70.9(88.0)</td></tr><tr><th>Full</th><th>MentorNet (2018)\\dagger</th><td>64.2(84.8)</td><td>72.6(88.9)</td></tr><tr><th>Full</th><th>Guo et al. (2018)\\dagger</th><td>64.8(84.9)</td><td>72.1(89.2)</td></tr><tr><th>Full</th><th>Saxena et al. (2019)</th><td>\u2014</td><td>67.5(\u2014\u2013)</td></tr><tr><th>Full</th><th>Ours (MentorMix)</th><td>67.5(87.2)</td><td>74.3(90.5)</td></tr><tr><th>Mini</th><th>MentorNet (2018)</th><td>63.8(85.8)</td><td>\u2014</td></tr><tr><th>Mini</th><th>Chen et al. (2019)</th><td>61.6(85.0)</td><td>65.2(85.3)</td></tr><tr><th>Mini</th><th>Ours (MentorMix)</th><td>72.9(91.1)</td><td>76.0(90.2)</td></tr></tbody></table>", "caption": "Table 4: Comparison with the state-of-the-art on the clean validation set of ILSVRC12 and WebVision. The number outside (inside) the parentheses denotes the top-1 (top-5) classification accuracy(%). \\dagger marks the method trained using extra clean labels.", "list_citation_info": ["Jiang et al. (2018) Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. International Conference on Machine Learning (ICML), 2018.", "Chen et al. (2019) Chen, P., Liao, B., Chen, G., and Zhang, S. Understanding and utilizing deep neural networks trained with noisy labels. International Conference on Machine Learning (ICML), 2019.", "Guo et al. (2018) Guo, S., Huang, W., Zhang, H., Zhuang, C., Dong, D., Scott, M. R., and Huang, D. Curriculumnet: Weakly supervised learning from large-scale web images. In European Conference on Computer Vision (ECCV), 2018.", "Saxena et al. (2019) Saxena, S., Tuzel, O., and DeCoste, D. Data parameters: A new family of parameters for learning a differentiable curriculum. In Conference on Neural Information Processing Systems (NeurIPS), 2019.", "Lee et al. (2018) Lee, K.-H., He, X., Zhang, L., and Yang, L. Cleannet: Transfer learning for scalable image classifier training with label noise. Conference on Computer Vision and Pattern Recognition (CVPR), 2018."]}]}