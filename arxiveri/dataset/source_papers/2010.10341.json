{"title": "Learning to learn variational semantic memory", "abstract": "In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the benefit of variational semantic memory in boosting few-shot recognition.", "authors": ["Xiantong Zhen", " Yingjun Du", " Huan Xiong", " Qiang Qiu", " Cees G. M. Snoek", " Ling Shao"], "pdf_url": "https://arxiv.org/abs/2010.10341", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><th colspan=\"2\">miniImageNet, 5-way</th><th colspan=\"2\">tieredImageNet, 5-way</th><th colspan=\"2\">cifar-fs, 5-way</th></tr><tr><td></td><th>1-shot</th><th>5-shot</th><th>1-shot</th><th>5-shot</th><th>1-shot</th><th>5-shot</th></tr><tr><td>MANN [62]</td><td>41.38 \\pm 1.70</td><td>61.73 \\pm 0.80</td><td>44.27 \\pm 1.69</td><td>67.15 \\pm 0.70</td><td>54.31 \\pm 1.91</td><td>67.98 \\pm 0.80</td></tr><tr><td>KM [79]</td><td>53.84 \\pm 1.70</td><td>67.35 \\pm 0.80</td><td>55.73 \\pm 1.65</td><td>73.36 \\pm 0.70</td><td>62.58 \\pm 1.80</td><td>77.11 \\pm 0.80</td></tr><tr><td>Variational semantic memory</td><td>54.73 \\pm 1.60</td><td>68.01 \\pm 0.90</td><td>56.88 \\pm 1.71</td><td>74.65 \\pm 0.81</td><td>63.42 \\pm 1.90</td><td>77.93 \\pm 0.80</td></tr></tbody></table>", "caption": "Table 4: Comparison with other memory models.", "list_citation_info": ["[79] Y. Wu, G. Wayne, A. Graves, and T. Lillicrap. The kanerva machine: A generative distributed memory. In ICLR, 2018.", "[62] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with memory-augmented neural networks. In ICML, 2016."]}, {"table": "<table><tbody><tr><td></td><th colspan=\"2\">miniImageNet, 5-way</th><th colspan=\"2\">tieredImageNet, 5-way</th><th colspan=\"2\">cifar-fs, 5-way</th></tr><tr><td></td><th>1-shot</th><th>5-shot</th><th>1-shot</th><th>5-shot</th><th>1-shot</th><th>5-shot</th></tr><tr><td>Matching Net [75]</td><td>43.56 \\pm 0.84</td><td>55.31 \\pm 0.73</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MAML [17]</td><td>48.70 \\pm 1.84</td><td>63.11 \\pm 0.92</td><td>51.67 \\pm 1.81</td><td>70.30 \\pm 1.75</td><td>58.90 \\pm 1.91</td><td>71.52 \\pm 1.10</td></tr><tr><td>Relation Net [68]</td><td>50.44 \\pm 0.82</td><td>65.32 \\pm 0.70</td><td>54.48 \\pm 0.93</td><td>65.32 \\pm 0.70</td><td>55.00 \\pm 1.01</td><td>69.30 \\pm 0.80</td></tr><tr><td>SNAIL (32C) by [4]</td><td>45.10 \\pm 0.85</td><td>55.20 \\pm 0.80</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GNN [21]</td><td>50.31 \\pm 0.83</td><td>66.42 \\pm 0.90</td><td>-</td><td>-</td><td>61.90 \\pm 1.03</td><td>75.30 \\pm 0.91</td></tr><tr><td>PLATIPUS [18]</td><td>50.10 \\pm 1.90</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VERSA [22]</td><td>53.31 \\pm 1.80</td><td>67.30 \\pm 0.91</td><td>-</td><td>-</td><td>62.51 \\pm 1.70</td><td>75.11 \\pm 0.91</td></tr><tr><td>R2-D2 (64C) [4]</td><td>49.50 \\pm 0.20</td><td>65.40 \\pm 0.20</td><td>-</td><td>-</td><td>62.30 \\pm 0.20</td><td>77.40 \\pm 0.20</td></tr><tr><td>R2-D2 [12]</td><td>51.70 \\pm 1.80</td><td>63.31 \\pm 0.91</td><td>-</td><td>-</td><td>60.20 \\pm 1.80</td><td>70.91 \\pm 0.91</td></tr><tr><td>CAVIA [87]</td><td>51.80 \\pm 0.70</td><td>65.61 \\pm 0.60</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>iMAML [51]</td><td>49.30 \\pm 1.90</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VSM (This paper)</td><td>54.73 \\pm 1.60</td><td>68.01 \\pm 0.90</td><td>56.88 \\pm 1.71</td><td>74.65 \\pm 0.81</td><td>63.42 \\pm 1.90</td><td>77.93 \\pm 0.80</td></tr></tbody></table>", "caption": "Table 5: Comparison (\\%) on miniImageNet, tieredImageNet and cifar-fs using a shallow feature extractor. ", "list_citation_info": ["[75] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot learning. In NeurIPS, 2016.", "[17] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.", "[87] L. Zintgraf, K. Shiarli, V. Kurin, K. Hofmann, and S. Whiteson. Fast context adaptation via meta-learning. In ICML, 2019.", "[68] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation network for few-shot learning. In CVPR, 2018.", "[21] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In ICLR, 2018.", "[22] J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. E. Turner. Meta-learning probabilistic inference for prediction. In ICLR, 2019.", "[18] C. Finn, K. Xu, and S. Levine. Probabilistic model-agnostic meta-learning. In NeurIPS, 2018.", "[51] A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine. Meta-learning with implicit gradients. In NeurIPS, 2019.", "[12] A. Devos, S. Chatel, and M. Grossglauser. Reproducing meta-learning with differentiable closed-form solvers. In ICLR Workshop, 2019.", "[4] L. Bertinetto, J. F. Henriques, P. H. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In ICLR, 2019."]}, {"table": "<table><tbody><tr><td></td><th colspan=\"2\">miniImageNet, 5-way</th><th colspan=\"2\">tieredImageNet, 5-way</th></tr><tr><td></td><th>1-shot</th><th>5-shot</th><th>1-shot</th><th>5-shot</th></tr><tr><td>SNAIL [43]</td><td>55.71 \\pm 0.99</td><td>68.88 \\pm 0.92</td><td>-</td><td>-</td></tr><tr><td>AdaResNet [46]</td><td>56.88 \\pm 0.62</td><td>71.94 \\pm 0.57</td><td>-</td><td>-</td></tr><tr><td>TADAM [47]</td><td>58.50 \\pm 0.30</td><td>76.70 \\pm 0.30</td><td>-</td><td>-</td></tr><tr><td>Shot-Free [56]</td><td>59.04 \\pm n/a</td><td>77.64 \\pm n/a</td><td>63.52 \\pm n/a</td><td>82.59 \\pm n/a</td></tr><tr><td>TEWAM [50]</td><td>60.07 \\pm n/a</td><td>75.90 \\pm n/a</td><td>-</td><td>-</td></tr><tr><td>MTL [67]</td><td>61.20 \\pm 1.80</td><td>75.50 \\pm 0.80</td><td>-</td><td>-</td></tr><tr><td>Variational FSL [85]</td><td>61.23 \\pm 0.26</td><td>77.69 \\pm 0.17</td><td>-</td><td>-</td></tr><tr><td>MetaOptNet [40]</td><td>62.64 \\pm 0.61</td><td>78.63 \\pm 0.46</td><td>65.99 \\pm 0.72</td><td>81.56 \\pm 0.53</td></tr><tr><td>Diversity w/ Cooperation [15]</td><td>59.48 \\pm 0.65</td><td>75.62 \\pm 0.48</td><td>-</td><td>-</td></tr><tr><td>Meta-Baseline [11]</td><td>63.17 \\pm 0.23</td><td>79.26 \\pm 0.17</td><td>-</td><td>-</td></tr><tr><td>Tian et al. [70]</td><td>64.82 \\pm 0.60</td><td>82.14 \\pm 0.43</td><td>71.52 \\pm 0.69</td><td>86.03 \\pm 0.49</td></tr><tr><td>VSM (This paper)</td><td>65.72 \\pm 0.57</td><td>82.73 \\pm 0.51</td><td>72.01 \\pm 0.71</td><td>86.77 \\pm 0.44</td></tr></tbody></table>", "caption": "Table 6: Comparison (\\%) on miniImageNet and tieredImageNet using a deep feature extractor.", "list_citation_info": ["[50] S. Qiao, C. Liu, W. Shen, and A. L. Yuille. Few-shot image recognition by predicting parameters from activations. In CVPR, 2018.", "[56] A. Ravichandran, R. Bhotika, and S. Soatto. Few-shot learning with embedded class models and shot-free meta training. In ICCV, 2019.", "[47] B. Oreshkin, P. R. L\u00f3pez, and A. Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. In NeurIPS, 2018.", "[67] Q. Sun, Y. Liu, T.-S. Chua, and B. Schiele. Meta-transfer learning for few-shot learning. In CVPR, 2019.", "[70] Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola. Rethinking few-shot image classification: a good embedding is all you need? arXiv preprint arXiv:2003.11539, 2020.", "[85] J. Zhang, C. Zhao, B. Ni, M. Xu, and X. Yang. Variational few-shot learning. In ICCV, 2019.", "[40] K. Lee, S. Maji, A. Ravichandran, and S. Soatto. Meta-learning with differentiable convex optimization. In CVPR, 2019.", "[43] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In ICLR, 2018.", "[15] N. Dvornik, C. Schmid, and J. Mairal. Diversity with cooperation: Ensemble methods for few-shot classification. In ICCV, 2019.", "[11] Y. Chen, X. Wang, Z. Liu, H. Xu, and T. Darrell. A new meta-baseline for few-shot learning. arXiv preprint arXiv:2003.04390, 2020.", "[46] T. Munkhdalai, X. Yuan, S. Mehri, and A. Trischler. Rapid adaptation with conditionally shifted neurons. In ICML, 2018."]}, {"table": "<table><tbody><tr><td></td><td colspan=\"2\">Omniglot, 5-way</td><td colspan=\"2\">Omniglot, 20-way</td></tr><tr><td></td><td>1-shot</td><td>5-shot</td><td>1-shot</td><td>5-shot</td></tr><tr><td>Siamese Net [37]</td><td>96.7</td><td>98.4</td><td>88.0</td><td>96.5</td></tr><tr><td>Matching net [75]</td><td>98.1</td><td>98.9</td><td>93.8</td><td>98.5</td></tr><tr><td>MAML [17]</td><td>98.7 \\pm 0.4</td><td>99.9 \\pm 0.1</td><td>95.8 \\pm 0.3</td><td>98.9 \\pm 0.2</td></tr><tr><td>SNAIL [43]</td><td>99.1 \\pm 0.2</td><td>99.8 \\pm 0.1</td><td>97.6 \\pm 0.3</td><td>99.4 \\pm 0.2</td></tr><tr><td>GNN [21]</td><td>99.2</td><td>99.7</td><td>97.4</td><td>99.0</td></tr><tr><td>VERSA [22]</td><td>99.7 \\pm 0.2</td><td>99.8 \\pm 0.1</td><td>97.7 \\pm 0.3</td><td>98.8 \\pm 0.2</td></tr><tr><td>R2-D2 [4]</td><td>98.6</td><td>99.7</td><td>94.7</td><td>98.9</td></tr><tr><td>IMP [1]</td><td>98.4 \\pm 0.3</td><td>99.5 \\pm 0.1</td><td>95.0 \\pm 0.1</td><td>98.6 \\pm 0.1</td></tr><tr><td>ProtoNet [66]</td><td>98.5 \\pm 0.2</td><td>99.5 \\pm 0.1</td><td>95.3 \\pm 0.2</td><td>98.7 \\pm 0.1</td></tr><tr><td>This paper</td><td>99.8 \\pm 0.1</td><td>99.9 \\pm 0.1</td><td>98.3 \\pm 0.3</td><td>99.4 \\pm 0.2</td></tr></tbody></table>", "caption": "Table 7: Comparison (\\%) on Omniglot using a shallow feature extractor. ", "list_citation_info": ["[37] G. Koch. Siamese neural networks for one-shot image recognition. In ICML Workshop, 2015.", "[75] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra. Matching networks for one shot learning. In NeurIPS, 2016.", "[17] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.", "[21] V. Garcia and J. Bruna. Few-shot learning with graph neural networks. In ICLR, 2018.", "[66] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017.", "[43] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. In ICLR, 2018.", "[22] J. Gordon, J. Bronskill, M. Bauer, S. Nowozin, and R. E. Turner. Meta-learning probabilistic inference for prediction. In ICLR, 2019.", "[1] K. R. Allen, E. Shelhamer, H. Shin, and J. B. Tenenbaum. Infinite mixture prototypes for few-shot learning. In ICML, 2019.", "[4] L. Bertinetto, J. F. Henriques, P. H. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In ICLR, 2019."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">miniImageNet</th><th colspan=\"2\">tieredImageNet</th></tr><tr><th></th><th>1-shot</th><th>5-shot</th><th>1-shot</th><th>5-shot</th></tr></thead><tbody><tr><th>TAML [32]</th><td>19.73 \\pm 0.65</td><td>29.81 \\pm 0.35</td><td>n/a</td><td>n/a</td></tr><tr><th>Baseline++ [10]</th><td>n/a</td><td>38.03 \\pm 0.24</td><td>n/a</td><td>n/a</td></tr><tr><th>This paper</th><td>22.07 \\pm 0.53</td><td>39.98 \\pm 0.27</td><td>24.76 \\pm 0.51</td><td>41.84 \\pm 0.31</td></tr></tbody></table>", "caption": "Table 8: Performance comparison under 20-way settings on the miniImageNet and tieredImageNet datasets.", "list_citation_info": ["[32] M. A. Jamal and G.-J. Qi. Task agnostic meta-learning for few-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11719\u201311727, 2019.", "[10] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang. A closer look at few-shot classification. arXiv preprint arXiv:1904.04232, 2019."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">miniImageNet, 5-way</th><th colspan=\"2\">tieredImageNet, 5-way</th></tr><tr><th></th><th>1-shot</th><th>5-shot</th><th>1-shot</th><th>5-shot</th></tr></thead><tbody><tr><th>Activation to Parameter [50]</th><td>59.60 \\pm 0.41</td><td>73.74 \\pm 0.19</td><td>-</td><td>-</td></tr><tr><th>Fine-Tuning [13]</th><td>57.73 \\pm 0.62</td><td>78.17 \\pm 0.49</td><td>66.58 \\pm 0.70</td><td>85.55 \\pm 0.48</td></tr><tr><th>LEO [61]</th><td>61.76 \\pm 0.08</td><td>77.59 \\pm 0.12</td><td>66.33 \\pm 0.05</td><td>81.44 \\pm 0.09</td></tr><tr><th>This paper</th><td>63.45 \\pm 0.39</td><td>78.99 \\pm 0.29</td><td>68.54 \\pm 0.61</td><td>86.25 \\pm 0.39</td></tr></tbody></table>", "caption": "Table 9: Comparison (\\%) on miniImageNet and tieredImageNet using WRN-28-10 feature extractor.", "list_citation_info": ["[61] A. A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero, and R. Hadsell. Meta-learning with latent embedding optimization. In ICLR, 2019.", "[50] S. Qiao, C. Liu, W. Shen, and A. L. Yuille. Few-shot image recognition by predicting parameters from activations. In CVPR, 2018.", "[13] G. S. Dhillon, P. Chaudhari, A. Ravichandran, and S. Soatto. A baseline for few-shot image classification. In ICLR, 2020."]}]}