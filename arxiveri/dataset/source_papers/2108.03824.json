{"title": "Aa-rmvsnet: Adaptive Aggregation Recurrent Multi-View Stereo Network", "abstract": "In this paper, we present a novel recurrent multi-view stereo network based on long short-term memory (LSTM) with adaptive aggregation, namely AA-RMVSNet. We firstly introduce an intra-view aggregation module to adaptively extract image features by using context-aware convolution and multi-scale aggregation, which efficiently improves the performance on challenging regions, such as thin objects and large low-textured surfaces. To overcome the difficulty of varying occlusion in complex scenes, we propose an inter-view cost volume aggregation module for adaptive pixel-wise view aggregation, which is able to preserve better-matched pairs among all views. The two proposed adaptive aggregation modules are lightweight, effective and complementary regarding improving the accuracy and completeness of 3D reconstruction. Instead of conventional 3D CNNs, we utilize a hybrid network with recurrent structure for cost volume regularization, which allows high-resolution reconstruction and finer hypothetical plane sweep. The proposed network is trained end-to-end and achieves excellent performance on various datasets. It ranks $1^{st}$ among all submissions on Tanks and Temples benchmark and achieves competitive results on DTU dataset, which exhibits strong generalizability and robustness. Implementation of our method is available at https://github.com/QT-Zhu/AA-RMVSNet.", "authors": ["Zizhuang Wei", " Qingtian Zhu", " Chen Min", " Yisong Chen", " Guoping Wang"], "pdf_url": "https://arxiv.org/abs/2108.03824", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>Acc.(mm)</th><th>Comp.(mm)</th><th>Overall(mm)</th></tr></thead><tbody><tr><th>Furu [9]</th><td>0.613</td><td>0.941</td><td>0.777</td></tr><tr><th>Gipuma [10]</th><td>0.283</td><td>0.873</td><td>0.578</td></tr><tr><th>COLMAP [26]</th><td>0.400</td><td>0.664</td><td>0.532</td></tr><tr><th>MVSNet [33]</th><td>0.396</td><td>0.527</td><td>0.462</td></tr><tr><th>R-MVSNet [34]</th><td>0.385</td><td>0.459</td><td>0.422</td></tr><tr><th>P-MVSNet [20]</th><td>0.406</td><td>0.434</td><td>0.420</td></tr><tr><th>PointMVSNet [6]</th><td>0.361</td><td>0.421</td><td>0.391</td></tr><tr><th>D^{2}HC-RMVSNet [31]</th><td>0.395</td><td>0.378</td><td>0.386</td></tr><tr><th>PointMVSNet [6]</th><td>0.342</td><td>0.411</td><td>0.376</td></tr><tr><th>Vis-MVSNet [37]</th><td>0.369</td><td>0.361</td><td>0.365</td></tr><tr><th>CasMVSNet [11]</th><td>0.325</td><td>0.385</td><td>0.355</td></tr><tr><th>CVP-MVSNet [32]</th><td>0.296</td><td>0.406</td><td>0.351</td></tr><tr><th>AA-RMVSNet</th><td>0.376</td><td>0.339</td><td>0.357</td></tr></tbody></table>", "caption": "Table 1: Quantitative results on DTU evaluation set [3] (lower is better). Our method AA-RMVSNet exhibits a competitive overall score compared with other state-of-the-art methods. Specially, our method outperforms all methods mentioned in terms of completeness.", "list_citation_info": ["[11] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.", "[34] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.", "[10] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In Proceedings of the IEEE International Conference on Computer Vision, pages 873\u2013881, 2015.", "[9] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE transactions on pattern analysis and machine intelligence, 32(8):1362\u20131376, 2009.", "[31] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. In European Conference on Computer Vision, pages 674\u2013689. Springer, 2020.", "[33] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European Conference on Computer Vision (ECCV), pages 767\u2013783, 2018.", "[32] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4877\u20134886, 2020.", "[26] Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision, pages 501\u2013518. Springer, 2016.", "[37] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. British Machine Vision Conference (BMVC), 2020.", "[6] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1538\u20131547, 2019.", "[3] Henrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, 120(2):153\u2013168, 2016.", "[20] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10452\u201310461, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>Rank</th><th>Mean</th><th>Family</th><th>Francis</th><th>Horse</th><th>L.H.</th><th>M60</th><th>Panther</th><th>P.G.</th><th>Train</th></tr></thead><tbody><tr><th>CIDER [30]</th><td>95.00</td><td>46.76</td><td>56.79</td><td>32.39</td><td>29.89</td><td>54.67</td><td>53.46</td><td>53.51</td><td>50.48</td><td>42.85</td></tr><tr><th>Point-MVSNet [6]</th><td>93.88</td><td>48.27</td><td>61.79</td><td>41.15</td><td>34.20</td><td>50.79</td><td>51.97</td><td>50.85</td><td>52.38</td><td>43.06</td></tr><tr><th>Dense R-MVSNet [34]</th><td>83.50</td><td>50.55</td><td>73.01</td><td>54.46</td><td>43.42</td><td>43.88</td><td>46.80</td><td>46.69</td><td>50.87</td><td>45.25</td></tr><tr><th>PVA-MVSNet [36]</th><td>56.62</td><td>54.46</td><td>69.36</td><td>46.80</td><td>46.01</td><td>55.74</td><td>57.23</td><td>54.75</td><td>56.70</td><td>49.06</td></tr><tr><th>CVP-MVSNet [32]</th><td>55.12</td><td>54.03</td><td>76.50</td><td>47.74</td><td>36.34</td><td>55.12</td><td>57.28</td><td>54.28</td><td>57.43</td><td>47.54</td></tr><tr><th>P-MVSNet [20]</th><td>43.12</td><td>55.62</td><td>70.04</td><td>44.64</td><td>40.22</td><td>65.20</td><td>55.08</td><td>55.17</td><td>60.37</td><td>54.29</td></tr><tr><th>CasMVSNet [11]</th><td>40.38</td><td>56.84</td><td>76.37</td><td>58.45</td><td>46.26</td><td>55.81</td><td>56.11</td><td>54.06</td><td>58.18</td><td>49.51</td></tr><tr><th>ACMM [29]</th><td>34.25</td><td>57.27</td><td>69.24</td><td>51.45</td><td>46.97</td><td>63.20</td><td>55.07</td><td>57.64</td><td>60.08</td><td>54.48</td></tr><tr><th>DeepC-MVS [17]</th><td>24.62</td><td>59.79</td><td>71.91</td><td>54.08</td><td>42.29</td><td>66.54</td><td>55.77</td><td>67.47</td><td>60.47</td><td>59.83</td></tr><tr><th>Altizure-HKUST-2019 [1]</th><td>24.00</td><td>59.03</td><td>77.19</td><td>61.52</td><td>42.09</td><td>63.50</td><td>59.36</td><td>58.20</td><td>57.05</td><td>53.30</td></tr><tr><th>AttMVS [21]</th><td>19.00</td><td>60.05</td><td>73.90</td><td>62.58</td><td>44.08</td><td>64.88</td><td>56.08</td><td>59.39</td><td>63.42</td><td>56.06</td></tr><tr><th>D^{2}HC-RMVSNet [31]</th><td>18.38</td><td>59.20</td><td>74.69</td><td>56.04</td><td>49.42</td><td>60.08</td><td>59.81</td><td>59.61</td><td>60.04</td><td>53.92</td></tr><tr><th>Vis-MVSNet [37]</th><td>15.38</td><td>60.03</td><td>77.40</td><td>60.23</td><td>47.07</td><td>63.44</td><td>62.21</td><td>57.28</td><td>60.54</td><td>52.07</td></tr><tr><th>AA-RMVSNet</th><td>6.38</td><td>61.51</td><td>77.77</td><td>59.53</td><td>51.53</td><td>64.02</td><td>64.05</td><td>59.47</td><td>60.85</td><td>54.90</td></tr></tbody></table>", "caption": "Table 2: Benchmarking results on the Tanks and Temples [16]. The evaluation metric is mean F-score (higher is better). AA-RMVSNet outperforms all existing MVS methods with a significant margin and ranks 1^{st} on Tanks and Temples leaderboard (Mar. 15, 2021). The Rank is a metric representing the average rank of all 8 scenes and is the basis for final ranking.", "list_citation_info": ["[11] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.", "[34] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.", "[1] Altizure. https://github.com/altizure.", "[37] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. British Machine Vision Conference (BMVC), 2020.", "[30] Qingshan Xu and Wenbing Tao. Learning inverse depth regression for multi-view stereo with correlation cost volume. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12508\u201312515, 2020.", "[17] Andreas Kuhn, Christian Sormann, Mattia Rossi, Oliver Erdler, and Friedrich Fraundorfer. Deepc-mvs: Deep confidence prediction for multi-view stereo reconstruction. In 2020 International Conference on 3D Vision (3DV), pages 404\u2013413. IEEE, 2020.", "[31] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. In European Conference on Computer Vision, pages 674\u2013689. Springer, 2020.", "[29] Qingshan Xu and Wenbing Tao. Multi-scale geometric consistency guided multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5483\u20135492, 2019.", "[16] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):1\u201313, 2017.", "[36] Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Pyramid multi-view stereo net with self-adaptive view aggregation. In European Conference on Computer Vision, pages 766\u2013782. Springer, 2020.", "[32] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4877\u20134886, 2020.", "[21] Keyang Luo, Tao Guan, Lili Ju, Yuesong Wang, Zhuo Chen, and Yawei Luo. Attention-aware multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1590\u20131599, 2020.", "[6] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1538\u20131547, 2019.", "[20] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10452\u201310461, 2019."]}, {"table": "<table><thead><tr><th>Model</th><th>Acc.</th><th>Comp.</th><th>O.A.(mm)</th><th>Mem.(GB)</th></tr></thead><tbody><tr><th>Baseline</th><td>0.408</td><td>0.374</td><td>0.391</td><td>2.41</td></tr><tr><th>+intra-view AA</th><td>0.396</td><td>0.346</td><td>0.371</td><td>4.15</td></tr><tr><th>+inter-view AA</th><td>0.377</td><td>0.363</td><td>0.370</td><td>2.52</td></tr><tr><th>Full</th><td>0.376</td><td>0.339</td><td>0.357</td><td>4.25</td></tr><tr><th>MVSNet [33]</th><td>0.396</td><td>0.527</td><td>0.462</td><td>15.4</td></tr><tr><th>R-MVSNet [34]</th><td>0.385</td><td>0.459</td><td>0.422</td><td>6.7</td></tr></tbody></table>", "caption": "Table 3: Quantitative and memory performance with different components on DTU evaluation dataset [3]. ", "list_citation_info": ["[33] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European Conference on Computer Vision (ECCV), pages 767\u2013783, 2018.", "[34] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.", "[3] Henrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, 120(2):153\u2013168, 2016."]}]}