{"title": "Swin-UNet: UNet-like pure transformer for medical image segmentation", "abstract": "In the past few years, convolutional neural networks (CNNs) have achieved milestones in medical image analysis. Especially, the deep neural networks based on U-shaped architecture and skip-connections have been widely applied in a variety of medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global and long-range semantic information interaction well due to the locality of the convolution operation. In this paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical image segmentation. The tokenized image patches are fed into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Specifically, we use hierarchical Swin Transformer with shifted windows as the encoder to extract context features. And a symmetric Swin Transformer-based decoder with patch expanding layer is designed to perform the up-sampling operation to restore the spatial resolution of the feature maps. Under the direct down-sampling and up-sampling of the inputs and outputs by 4x, experiments on multi-organ and cardiac segmentation tasks demonstrate that the pure Transformer-based U-shaped Encoder-Decoder network outperforms those methods with full-convolution or the combination of transformer and convolution. The codes and trained models will be publicly available at https://github.com/HuCaoFighting/Swin-Unet.", "authors": ["Hu Cao", " Yueyue Wang", " Joy Chen", " Dongsheng Jiang", " Xiaopeng Zhang", " Qi Tian", " Manning Wang"], "pdf_url": "https://arxiv.org/abs/2105.05537", "list_table_and_caption": [{"table": "<table><tr><td>Methods</td><td>DSC\\uparrow</td><td>HD\\downarrow</td><td>Aorta</td><td>Gallbladder</td><td>Kidney(L)</td><td>Kidney(R)</td><td>Liver</td><td>Pancreas</td><td>Spleen</td><td>Stomach</td></tr><tr><td>V-Net [35]</td><td>68.81</td><td>-</td><td>75.34</td><td>51.87</td><td>77.10</td><td>80.75</td><td>87.84</td><td>40.05</td><td>80.56</td><td>56.98</td></tr><tr><td>DARR [36]</td><td>69.77</td><td>-</td><td>74.74</td><td>53.77</td><td>72.31</td><td>73.24</td><td>94.08</td><td>54.18</td><td>89.90</td><td>45.96</td></tr><tr><td>R50 U-Net [2]</td><td>74.68</td><td>36.87</td><td>87.74</td><td>63.66</td><td>80.60</td><td>78.19</td><td>93.74</td><td>56.90</td><td>85.87</td><td>74.16</td></tr><tr><td>U-Net [3]</td><td>76.85</td><td>39.70</td><td>89.07</td><td>69.72</td><td>77.77</td><td>68.60</td><td>93.43</td><td>53.98</td><td>86.67</td><td>75.58</td></tr><tr><td>R50 Att-UNet [2]</td><td>75.57</td><td>36.97</td><td>55.92</td><td>63.91</td><td>79.20</td><td>72.71</td><td>93.56</td><td>49.37</td><td>87.19</td><td>74.95</td></tr><tr><td>Att-UNet [37]</td><td>77.77</td><td>36.02</td><td>89.55</td><td>68.88</td><td>77.98</td><td>71.11</td><td>93.57</td><td>58.04</td><td>87.30</td><td>75.75</td></tr><tr><td>R50 ViT [2]</td><td>71.29</td><td>32.87</td><td>73.73</td><td>55.13</td><td>75.80</td><td>72.20</td><td>91.51</td><td>45.99</td><td>81.99</td><td>73.95</td></tr><tr><td>TransUnet [2]</td><td>77.48</td><td>31.69</td><td>87.23</td><td>63.13</td><td>81.87</td><td>77.02</td><td>94.08</td><td>55.86</td><td>85.08</td><td>75.62</td></tr><tr><td>SwinUnet</td><td>79.13</td><td>21.55</td><td>85.47</td><td>66.53</td><td>83.28</td><td>79.61</td><td>94.29</td><td>56.58</td><td>90.66</td><td>76.60</td></tr></table>", "caption": "Table 1: Segmentation accuracy of different methods on the Synapse multi-organ CT dataset.", "list_citation_info": ["[37] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, B. Glocker, and D. Rueckert, \u201cAttention u-net: Learning where to look for the pancreas,\u201d IMIDL Conference, 2018.", "[36] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, \u201cDomain adaptive relational reasoning for 3d multi-organ segmentation,\u201d Germany, 2020, pp. 656\u2013666.", "[35] F. Milletari, N. Navab, and S.-A. Ahmadi, \u201cV-net: Fully convolutional neural networks for volumetric medical image segmentation,\u201d in 2016 Fourth International Conference on 3D Vision (3DV), 2016, pp. 565\u2013571.", "[2] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou, \u201cTransunet: Transformers make strong encoders for medical image segmentation,\u201d CoRR, vol. abs/2102.04306, 2021.", "[3] O. Ronneberger, P.Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in Medical Image Computing and Computer-Assisted Intervention (MICCAI), ser. LNCS, vol. 9351. Springer, 2015, pp. 234\u2013241."]}]}