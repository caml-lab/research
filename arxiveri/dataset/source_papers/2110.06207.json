{"title": "Open-set recognition: A good closed-set classifier is all you need", "abstract": "The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of a maximum logit score OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, in contrast to other forms of distribution shift also considered in related sub-fields, such as out-of-distribution detection. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Project Page: https://www.robots.ox.ac.uk/~vgg/research/osr/", "authors": ["Sagar Vaze", " Kai Han", " Andrea Vedaldi", " Andrew Zisserman"], "pdf_url": "https://arxiv.org/abs/2110.06207", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>MNIST</td><td>SVHN</td><td>CIFAR10</td><td>CIFAR + 10</td><td>CIFAR + 50</td><td>TinyImageNet</td></tr><tr><td>Baseline (MSP) (Neal et al., 2018)</td><td>97.8</td><td>88.6</td><td>67.7</td><td>81.6</td><td>80.5</td><td>57.7</td></tr><tr><td>OSRCI (Neal et al., 2018)</td><td>98.8</td><td>91.0</td><td>69.9</td><td>83.8</td><td>82.7</td><td>58.6</td></tr><tr><td>OpenHybrid (Zhang et al., 2020)</td><td>99.5</td><td>94.7</td><td>95.0</td><td>96.2</td><td>95.5</td><td>79.3</td></tr><tr><td>ARPL + CS (Chen et al., 2021)</td><td>99.7</td><td>96.7</td><td>91.0</td><td>97.1</td><td>95.1</td><td>78.2</td></tr><tr><td>OSRCI+</td><td>98.5 (-0.3)</td><td>89.9 (-1.1)</td><td>87.2 (+17.3)</td><td>91.1 (+7.3)</td><td>90.3 (+7.6)</td><td>62.6 (+4.0)</td></tr><tr><td>(ARPL + CS)+</td><td>99.2 (-0.5)</td><td>96.8 (+0.1)</td><td>93.9 (+2.9)</td><td>98.1 (+1.0)</td><td>96.7 (+1.6)</td><td>82.5 (+4.3)</td></tr><tr><td>Baseline (MSP+)</td><td>98.6 (+0.8)</td><td>96.0 (+7.4)</td><td>90.1 (+22.4)</td><td>95.6 (+14.0)</td><td>94.0 (+13.5)</td><td>82.7 (+25.0)</td></tr><tr><td>Baseline (MLS)</td><td>99.3 (+1.5)</td><td>97.1 (+8.5)</td><td>93.6 (+25.9)</td><td>97.9 (+16.3)</td><td>96.5 (+16.0)</td><td>83.0 (+25.3)</td></tr></table>", "caption": "Table 1: Comparisons of our improved baselines (MSP+, MLS) against state-of-the-art methods on the standard OSR benchmark datasets. All results indicate the area under the Receiver-Operator curve (AUROC) averaged over five \u2018known/unknown\u2019 class splits. \u2018+\u2019 indicates prior methods augmented with improved closed-set optimization strategies, including: MSP+ (Neal et al., 2018), OSRCI+ (Neal et al., 2018) and (ARPL + CS)+ (Chen et al., 2021).", "list_citation_info": ["Zhang et al. (2020) Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. In ECCV, 2020.", "Neal et al. (2018) Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with counterfactual images. In ECCV, 2018.", "Chen et al. (2021) Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. IEEE TPAMI, 2021."]}, {"table": "<table><tr><td>Method</td><td>Backbone</td><td>MNIST</td><td>SVHN</td><td>CIFAR10</td><td>CIFAR + 10</td><td>CIFAR + 50</td><td>TinyImageNet</td></tr><tr><td>MSP (Neal et al., 2018)</td><td>VGG32</td><td>97.8</td><td>88.6</td><td>67.7</td><td>81.6</td><td>80.5</td><td>57.7</td></tr><tr><td>OpenMax (Bendale &amp; Boult, 2016)</td><td>VGG32</td><td>98.1</td><td>89.4</td><td>69.5</td><td>81.7</td><td>79.6</td><td>57.6</td></tr><tr><td>G-OpenMax (Ge et al., 2017)</td><td>VGG32</td><td>98.4</td><td>89.6</td><td>67.5</td><td>82.7</td><td>81.9</td><td>58.0</td></tr><tr><td>OSRCI (Neal et al., 2018)</td><td>VGG32</td><td>98.8</td><td>91.0</td><td>69.9</td><td>83.8</td><td>82.7</td><td>58.6</td></tr><tr><td>CROSR (Yoshihashi et al., 2019)</td><td>DHRNet</td><td>99.1</td><td>89.9</td><td>-</td><td>-</td><td>-</td><td>58.9</td></tr><tr><td>C2AE (Oza &amp; Patel, 2019)</td><td>VGG32</td><td>98.9</td><td>92.2</td><td>89.5</td><td>95.5</td><td>93.7</td><td>74.8</td></tr><tr><td>GFROSR (Perera et al., 2020)</td><td>VGG32 / WRN-28-10</td><td>-</td><td>93.5 / 95.5</td><td>80.7 / 83.1</td><td>92.8 / 91.5</td><td>92.6 / 91.3</td><td>60.8 / 64.7</td></tr><tr><td>CGDL (Sun et al., 2021)</td><td>CPGM-AAE</td><td>99.5</td><td>96.8</td><td>95.3</td><td>96.5</td><td>96.1</td><td>77.0</td></tr><tr><td>OpenHybrid (Zhang et al., 2020)</td><td>VGG32</td><td>99.5</td><td>94.7</td><td>95.0</td><td>96.2</td><td>95.5</td><td>79.3</td></tr><tr><td>RPL (Chen et al., 2020a)</td><td>VGG32 / WRN-40-4</td><td>99.3 / 99.6</td><td>95.1 / 96.8</td><td>86.1 / 90.1</td><td>85.6 / 97.6</td><td>85.0 / 96.8</td><td>70.2 / 80.9</td></tr><tr><td>PROSER (Zhou et al., 2021)</td><td>WRN-28-10</td><td>-</td><td>94.3</td><td>89.1</td><td>96.0</td><td>85.3</td><td>69.3</td></tr><tr><td>ARPL (Chen et al., 2021)</td><td>VGG32</td><td>99.6</td><td>96.3</td><td>90.1</td><td>96.5</td><td>94.3</td><td>76.2</td></tr><tr><td>ARPL + CS (Chen et al., 2021)</td><td>VGG32</td><td>99.7</td><td>96.7</td><td>91.0</td><td>97.1</td><td>95.1</td><td>78.2</td></tr><tr><td>OSRCI+</td><td>VGG32</td><td>98.5 (-0.3)</td><td>89.9 (-1.1)</td><td>87.2 (+17.3)</td><td>91.1 (+7.3)</td><td>90.3 (+7.6)</td><td>62.6 (+4.0)</td></tr><tr><td>(ARPL + CS)+</td><td>VGG32</td><td>99.2 (-0.5)</td><td>96.8 (+0.1)</td><td>93.9 (+2.9)</td><td>98.1 (+1.0)</td><td>96.7 (+1.6)</td><td>82.5 (+4.3)</td></tr><tr><td>Baseline (MSP+)</td><td>VGG32</td><td>98.6 (+0.8)</td><td>96.0 (+7.4)</td><td>90.1 (+22.4)</td><td>95.6 (+14.0)</td><td>94.0 (+13.5)</td><td>82.7 (+25.0)</td></tr><tr><td>Baseline (MLS)</td><td>VGG32</td><td>99.3 (+1.5)</td><td>97.1 (+8.5)</td><td>93.6 (+25.9)</td><td>97.9 (+16.3)</td><td>96.5 (+16.0)</td><td>83.0 (+25.3)</td></tr></table>", "caption": "Table 6: Comparing our improved baseline with other deep learning based OSR methods on the standard benchmark datasets. All results indicate the area under the Receiver-Operator curve (AUROC) as a percentage. We also show the backbone architecture used for each method, showing results with multiple backbones when reported.", "list_citation_info": ["Bendale & Boult (2016) Abhijit Bendale and Terrance E. Boult. Towards open set deep networks. In CVPR, 2016.", "Perera et al. (2020) Pramuditha Perera, Vlad I. Morariu, Rajiv Jain, Varun Manjunatha, Curtis Wigington, Vicente Ordonez, and Vishal M. Patel. Generative-discriminative feature representations for open-set recognition. In CVPR, 2020.", "Chen et al. (2020a) Guangyao Chen, Limeng Qiao, Yemin Shi, Peixi Peng, Jia Li, Tiejun Huang, Shiliang Pu, and Yonghong Tian. Learning open set network with discriminative reciprocal points. In ECCV, 2020a.", "Sun et al. (2021) Xin Sun, Chi Zhang, Guosheng Lin, and Keck-Voon Ling. Open set recognition with conditional probabilistic generative models. arXiv preprint arXiv:2008.05129, 2021.", "Yoshihashi et al. (2019) Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, and Takeshi Naemura. Classification-reconstruction learning for open-set recognition. In CVPR, 2019.", "Chen et al. (2021) Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. IEEE TPAMI, 2021.", "Oza & Patel (2019) Poojan Oza and Vishal M. Patel. C2ae: Class conditioned auto-encoder for open-set recognition. In CVPR, 2019.", "Ge et al. (2017) Zongyuan Ge, Sergey Demyanov, and Rahil Garnavi. Generative openmax for multi-class open set classification. In BMVC, 2017.", "Zhang et al. (2020) Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. In ECCV, 2020.", "Neal et al. (2018) Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with counterfactual images. In ECCV, 2018.", "Zhou et al. (2021) Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. Learning placeholders for open-set recognition. In CVPR, 2021."]}, {"table": "<table><tr><td>Method</td><td>CIFAR10</td><td>CIFAR100</td></tr><tr><td>MSP  (Hendrycks &amp; Gimpel, 2017)</td><td>90.9</td><td>75.5</td></tr><tr><td>ODIN  (Liang et al., 2018)</td><td>91.1</td><td>77.4</td></tr><tr><td>Energy Score  (Liu et al., 2020)</td><td>91.9</td><td>79.6</td></tr><tr><td>Mahanabolis  (Lee et al., 2018b)</td><td>93.3</td><td>84.1</td></tr><tr><td>VOS  (Du et al., 2022)</td><td>94.1</td><td>-</td></tr><tr><td>MLS (Ours)</td><td>95.1</td><td>80.8</td></tr></table>", "caption": "Table 8: Results of our strong baseline on the full OoD benchmark suite. We take strong WideResNet-40 models from (Lim et al., 2019) and run our MLS baseline on top. Models are trained on CIFAR10 and CIFAR100 as \u2018in-distribution\u2019 and we report AUROC averaged across six OoD datasets. All compared figures are taken from  (Du et al., 2022) and  Liu et al. (2020).", "list_citation_info": ["Liu et al. (2020) Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. NeurIPS, 2020.", "Liang et al. (2018) Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In ICLR, 2018.", "Lee et al. (2018b) Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. NeurIPS, 2018b.", "Hendrycks & Gimpel (2017) Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In ICLR, 2017.", "Du et al. (2022) Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don\u2019t know by virtual outlier synthesis. ICLR, 2022.", "Lim et al. (2019) Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In NeurIPS, 2019."]}]}