{"title": "Openforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild", "abstract": "The proliferation of deepfake media is raising concerns among the public and relevant authorities. It has become essential to develop countermeasures against forged faces in social media. This paper presents a comprehensive study on two new countermeasure tasks: multi-face forgery detection and segmentation in-the-wild. Localizing forged faces among multiple human faces in unrestricted natural scenes is far more challenging than the traditional deepfake recognition task. To promote these new tasks, we have created the first large-scale dataset posing a high level of challenges that is designed with face-wise rich annotations explicitly for face forgery detection and segmentation, namely OpenForensics. With its rich annotations, our OpenForensics dataset has great potentials for research in both deepfake prevention and general human face detection. We have also developed a suite of benchmarks for these tasks by conducting an extensive evaluation of state-of-the-art instance detection and segmentation methods on our newly constructed dataset in various scenarios. The dataset, benchmark results, codes, and supplementary materials will be publicly available on our project page: https://sites.google.com/view/ltnghia/research/openforensics", "authors": ["Trung-Nghia Le", " Huy H. Nguyen", " Junichi Yamagishi", " Isao Echizen"], "pdf_url": "https://arxiv.org/abs/2107.14480", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Year</th><td colspan=\"8\">Multi-Face Forgery Detection</td><td colspan=\"8\">Multi-Face Forgery Segmentation</td></tr><tr><td>AP\\uparrow</td><td>AP{}_{S}\\uparrow</td><td>AP{}_{M}\\uparrow</td><td>AP{}_{L}\\uparrow</td><td>oLRP\\downarrow</td><td>oLRP{}_{Loc}\\downarrow</td><td>oLRP{}_{FP}\\downarrow</td><td>oLRP{}_{FN}\\downarrow</td><td>AP\\uparrow</td><td>AP{}_{S}\\uparrow</td><td>AP{}_{M}\\uparrow</td><td>AP{}_{L}\\uparrow</td><td>oLRP\\downarrow</td><td>oLRP{}_{Loc}\\downarrow</td><td>oLRP{}_{FP}\\downarrow</td><td>oLRP{}_{FN}\\downarrow</td></tr><tr><th>MaskRCNN [22]</th><th>ICCV 2017</th><td>79.2</td><td>29.9</td><td>80.2</td><td>79.5</td><td>24.3</td><td>9.5</td><td>2.7</td><td>4.0</td><td>83.6</td><td>16.1</td><td>82.1</td><td>85.8</td><td>21.2</td><td>7.6</td><td>3.0</td><td>4.2</td></tr><tr><th>MSRCNN [25]</th><th>CVPR 2019</th><td>79.0</td><td>29.5</td><td>80.1</td><td>79.5</td><td>24.3</td><td>9.6</td><td>2.7</td><td>3.8</td><td>85.1</td><td>16.8</td><td>84.2</td><td>86.8</td><td>21.1</td><td>7.7</td><td>2.6</td><td>4.4</td></tr><tr><th>RetinaMask [17]</th><th>arXiv 2019</th><td>80.0</td><td>30.9</td><td>80.2</td><td>80.7</td><td>24.2</td><td>9.0</td><td>3.0</td><td>4.6</td><td>82.8</td><td>16.4</td><td>80.6</td><td>85.1</td><td>22.6</td><td>8.1</td><td>2.9</td><td>4.9</td></tr><tr><th>YOLACT [4]</th><th>ICCV 2019</th><td>68.1</td><td>12.5</td><td>67.1</td><td>69.3</td><td>37.2</td><td>13.4</td><td>6.3</td><td>8.7</td><td>72.5</td><td>3.1</td><td>67.0</td><td>75.7</td><td>34.0</td><td>11.4</td><td>6.4</td><td>8.7</td></tr><tr><th>YOLACT++ [5]</th><th>TPAMI 2020</th><td>72.9</td><td>20.9</td><td>73.4</td><td>73.6</td><td>31.5</td><td>12.1</td><td>4.0</td><td>5.8</td><td>77.3</td><td>6.5</td><td>73.9</td><td>80.0</td><td>28.2</td><td>10.0</td><td>3.9</td><td>6.5</td></tr><tr><th>CenterMask [41]</th><th>CVPR 2020</th><td>85.5</td><td>32.0</td><td>85.2</td><td>86.2</td><td>21.1</td><td>6.8</td><td>3.3</td><td>5.9</td><td>87.2</td><td>16.5</td><td>85.0</td><td>89.4</td><td>21.4</td><td>6.1</td><td>3.2</td><td>7.8</td></tr><tr><th>BlendMask [7]</th><th>CVPR 2020</th><td>87.0</td><td>32.7</td><td>86.3</td><td>88.0</td><td>19.5</td><td>6.2</td><td>2.4</td><td>6.2</td><td>89.2</td><td>19.8</td><td>87.3</td><td>91.0</td><td>18.3</td><td>5.4</td><td>2.5</td><td>6.3</td></tr><tr><th>PolarMask [76]</th><th>CVPR 2020</th><td>85.0</td><td>27.4</td><td>85.4</td><td>85.7</td><td>20.7</td><td>6.6</td><td>2.5</td><td>6.6</td><td>85.0</td><td>15.3</td><td>83.3</td><td>87.0</td><td>21.3</td><td>6.9</td><td>2.5</td><td>6.6</td></tr><tr><th>MEInst [80]</th><th>CVPR 2020</th><td>82.8</td><td>26.0</td><td>82.7</td><td>83.4</td><td>23.8</td><td>7.6</td><td>4.1</td><td>6.8</td><td>82.2</td><td>13.9</td><td>81.5</td><td>83.3</td><td>25.0</td><td>8.1</td><td>4.0</td><td>7.2</td></tr><tr><th>CondInst [69]</th><th>ECCV 2020</th><td>84.0</td><td>29.4</td><td>83.6</td><td>84.8</td><td>20.8</td><td>7.4</td><td>2.3</td><td>5.2</td><td>87.7</td><td>18.1</td><td>85.1</td><td>89.8</td><td>18.3</td><td>5.9</td><td>2.4</td><td>5.3</td></tr><tr><th>SOLO [72]</th><th>ECCV 2020</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>86.6</td><td>15.4</td><td>85.6</td><td>88.4</td><td>20.0</td><td>6.6</td><td>2.1</td><td>6.0</td></tr><tr><th>SOLO2 [73]</th><th>NeurIPS 2020</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>85.1</td><td>13.7</td><td>83.7</td><td>87.1</td><td>21.5</td><td>7.1</td><td>3.1</td><td>5.8</td></tr></tbody></table>", "caption": "Table 4: Benchmark results for multi-face forgery detection and segmentation on test-dev set. Higher AP is better while lower oLRP error is better. Best and second-best results are shown in blue and red, respectively.", "list_citation_info": ["[72] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. SOLO: Segmenting objects by locations. In European Conference on Computer Vision, 2020.", "[5] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact++: Better real-time instance segmentation. Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[7] Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, and Youliang Yan. Blendmask: Top-down meets bottom-up for instance segmentation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[25] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, and Xinggang Wang. Mask Scoring R-CNN. In Conference on Computer Vision and Pattern Recognition, 2019.", "[76] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask: Single shot instance segmentation with polar representation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[17] Cheng-Yang Fu, Mykhailo Shvets, and Alexander C. Berg. RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free. In arXiv preprint arXiv:1901.03353, 2019.", "[22] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In International Conference on Computer Vision, pages 2980\u20132988, 2017.", "[41] Youngwan Lee and Jongyoul Park. Centermask: Real-time anchor-free instance segmentation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[80] Rufeng Zhang, Zhi Tian, Chunhua Shen, Mingyu You, and Youliang Yan. Mask encoding for single shot instance segmentation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[69] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, 2020.", "[4] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In International Conference on Computer Vision, 2019.", "[73] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic, faster and stronger. In Conference on Neural Information Processing Systems, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Year</th><td colspan=\"8\">Multi-Face Forgery Detection</td><td colspan=\"8\">Multi-Face Forgery Segmentation</td></tr><tr><td>AP\\uparrow</td><td>AP{}_{S}\\uparrow</td><td>AP{}_{M}\\uparrow</td><td>AP{}_{L}\\uparrow</td><td>oLRP\\downarrow</td><td>oLRP{}_{Loc}\\downarrow</td><td>oLRP{}_{FP}\\downarrow</td><td>oLRP{}_{FN}\\downarrow</td><td>AP\\uparrow</td><td>AP{}_{S}\\uparrow</td><td>AP{}_{M}\\uparrow</td><td>AP{}_{L}\\uparrow</td><td>oLRP\\downarrow</td><td>oLRP{}_{Loc}\\downarrow</td><td>oLRP{}_{FP}\\downarrow</td><td>oLRP{}_{FN}\\downarrow</td></tr><tr><th>MaskRCNN [22]</th><th>ICCV 2017</th><td>42.1</td><td>11.8</td><td>46.2</td><td>40.5</td><td>65.4</td><td>13.6</td><td>29.3</td><td>40.0</td><td>43.7</td><td>4.7</td><td>44.3</td><td>44.0</td><td>64.4</td><td>11.8</td><td>29.4</td><td>41.2</td></tr><tr><th>MSRCNN [25]</th><th>CVPR 2019</th><td>42.2</td><td>11.8</td><td>45.9</td><td>40.8</td><td>65.3</td><td>13.7</td><td>29.6</td><td>39.9</td><td>43.3</td><td>5.2</td><td>44.6</td><td>43.5</td><td>64.1</td><td>11.8</td><td>30.4</td><td>39.6</td></tr><tr><th>RetinaMask [17]</th><th>arXiv 2019</th><td>48.5</td><td>12.8</td><td>51.0</td><td>48.1</td><td>63.3</td><td>12.6</td><td>33.2</td><td>34.6</td><td>48.0</td><td>4.7</td><td>46.5</td><td>49.7</td><td>63.3</td><td>11.8</td><td>30.9</td><td>38.0</td></tr><tr><th>YOLACT [4]</th><th>ICCV 2019</th><td>49.4</td><td>5.6</td><td>49.6</td><td>50.3</td><td>60.1</td><td>15.3</td><td>23.2</td><td>29.9</td><td>51.8</td><td>1.4</td><td>47.2</td><td>54.6</td><td>58.4</td><td>13.5</td><td>23.4</td><td>30.1</td></tr><tr><th>YOLACT++ [5]</th><th>TPAMI 2020</th><td>53.7</td><td>11.1</td><td>54.0</td><td>54.8</td><td>57.1</td><td>14.1</td><td>19.7</td><td>29.3</td><td>54.7</td><td>2.4</td><td>50.7</td><td>57.9</td><td>55.4</td><td>12.2</td><td>20.0</td><td>30.0</td></tr><tr><th>CenterMask [41]</th><th>CVPR 2020</th><td>0.03</td><td>0.4</td><td>0.0</td><td>0.0</td><td>99.5</td><td>29.7</td><td>97.7</td><td>97.9</td><td>0.02</td><td>0.0</td><td>0.0</td><td>0.0</td><td>99.6</td><td>28.3</td><td>97.9</td><td>98.4</td></tr><tr><th>BlendMask [7]</th><th>CVPR 2020</th><td>53.9</td><td>13.5</td><td>56.6</td><td>53.5</td><td>60.2</td><td>10.6</td><td>26.5</td><td>37.4</td><td>54.0</td><td>7.1</td><td>54.5</td><td>54.5</td><td>59.9</td><td>9.8</td><td>26.4</td><td>38.4</td></tr><tr><th>PolarMask [76]</th><th>CVPR 2020</th><td>51.7</td><td>12.3</td><td>53.2</td><td>51.5</td><td>60.4</td><td>10.7</td><td>24.6</td><td>39.5</td><td>52.7</td><td>5.3</td><td>54.1</td><td>37.6</td><td>60.2</td><td>10.4</td><td>24.7</td><td>39.5</td></tr><tr><th>MEInst [80]</th><th>CVPR 2020</th><td>46.1</td><td>8.6</td><td>49.9</td><td>44.9</td><td>65.9</td><td>12.4</td><td>34.6</td><td>39.7</td><td>46.0</td><td>3.8</td><td>49.0</td><td>45.2</td><td>66.2</td><td>12.6</td><td>34.8</td><td>39.8</td></tr><tr><th>CondInst [69]</th><th>ECCV 2020</th><td>52.7</td><td>12.6</td><td>55.3</td><td>51.8</td><td>60.7</td><td>11.5</td><td>28.3</td><td>35.3</td><td>54.1</td><td>6.5</td><td>55.2</td><td>53.8</td><td>59.6</td><td>10.0</td><td>26.7</td><td>37.3</td></tr><tr><th>SOLO [72]</th><th>ECCV 2020</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>55.9</td><td>3.9</td><td>53.3</td><td>57.3</td><td>57.6</td><td>11.3</td><td>24.6</td><td>33.0</td></tr><tr><th>SOLO2 [73]</th><th>NeurIPS 2020</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>53.2</td><td>3.6</td><td>52.1</td><td>54.0</td><td>59.6</td><td>11.0</td><td>24.5</td><td>37.2</td></tr></tbody></table>", "caption": "Table 5: Benchmark results for multi-face forgery detection and segmentation on test-challenge set. Higher AP is better while lower oLRP error is better. Best and second-best results are shown in blue and red, respectively.", "list_citation_info": ["[72] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. SOLO: Segmenting objects by locations. In European Conference on Computer Vision, 2020.", "[5] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact++: Better real-time instance segmentation. Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[7] Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang, and Youliang Yan. Blendmask: Top-down meets bottom-up for instance segmentation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[25] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang Huang, and Xinggang Wang. Mask Scoring R-CNN. In Conference on Computer Vision and Pattern Recognition, 2019.", "[76] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask: Single shot instance segmentation with polar representation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[17] Cheng-Yang Fu, Mykhailo Shvets, and Alexander C. Berg. RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free. In arXiv preprint arXiv:1901.03353, 2019.", "[22] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In International Conference on Computer Vision, pages 2980\u20132988, 2017.", "[41] Youngwan Lee and Jongyoul Park. Centermask: Real-time anchor-free instance segmentation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[80] Rufeng Zhang, Zhi Tian, Chunhua Shen, Mingyu You, and Youliang Yan. Mask encoding for single shot instance segmentation. In Conference on Computer Vision and Pattern Recognition, 2020.", "[69] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European Conference on Computer Vision, 2020.", "[4] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In International Conference on Computer Vision, 2019.", "[73] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic, faster and stronger. In Conference on Neural Information Processing Systems, 2020."]}]}