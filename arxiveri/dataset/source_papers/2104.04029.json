{"title": "Tripod: Human trajectory and pose dynamics forecasting in the wild", "abstract": "Joint forecasting of human trajectory and pose dynamics is a fundamental building block of various applications ranging from robotics and autonomous driving to surveillance systems. Predicting body dynamics requires capturing subtle information embedded in the humans' interactions with each other and with the objects present in the scene. In this paper, we propose a novel TRajectory and POse Dynamics (nicknamed TRiPOD) method based on graph attentional networks to model the human-human and human-object interactions both in the input space and the output space (decoded future output). The model is supplemented by a message passing interface over the graphs to fuse these different levels of interactions efficiently. Furthermore, to incorporate a real-world challenge, we propound to learn an indicator representing whether an estimated body joint is visible/invisible at each frame, e.g. due to occlusion or being outside the sensor field of view. Finally, we introduce a new benchmark for this joint task based on two challenging datasets (PoseTrack and 3DPW) and propose evaluation metrics to measure the effectiveness of predictions in the global space, even when there are invisible cases of joints. Our evaluation shows that TRiPOD outperforms all prior work and state-of-the-art specifically designed for each of the trajectory and pose forecasting tasks.", "authors": ["Vida Adeli", " Mahsa Ehsanpour", " Ian Reid", " Juan Carlos Niebles", " Silvio Savarese", " Ehsan Adeli", " Hamid Rezatofighi"], "pdf_url": "https://arxiv.org/abs/2104.04029", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"5\">3DPW</td><td colspan=\"10\">PoseTrack</td></tr><tr><th></th><td colspan=\"5\">VIM (Invisibility ignored)</td><td colspan=\"5\">VIM (Invisibility ignored)</td><td colspan=\"5\">VAM (Invisibility considered \\beta=200)</td></tr><tr><th rowspan=\"2\"></th><td colspan=\"5\">prediction time in milliseconds</td><td colspan=\"5\">prediction time in milliseconds</td><td colspan=\"5\">prediction time in milliseconds</td></tr><tr><td>100</td><td>240</td><td>500</td><td>640</td><td>900</td><td>80</td><td>160</td><td>320</td><td>400</td><td>560</td><td>80</td><td>160</td><td>320</td><td>400</td><td>560</td></tr><tr><th>\\hlineB2Center pose      Trajectory</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>PF-RNN [41] + S-LSTM [3]</th><td>73.2</td><td>126.79</td><td>180.03</td><td>201.75</td><td>277.53</td><td>87.05</td><td>103.35</td><td>129.19</td><td>138.66</td><td>160.96</td><td>101.37</td><td>114.09</td><td>133.88</td><td>145.39</td><td>160.49</td></tr><tr><th>PF-RNN [41] + S-GAN [22]</th><td>68.40</td><td>119.72</td><td>172.73</td><td>195.88</td><td>263.05</td><td>84.74</td><td>98.94</td><td>121.35</td><td>129.55</td><td>150.16</td><td>99.96</td><td>111.51</td><td>129.3</td><td>140.08</td><td>154.09</td></tr><tr><th>PF-RNN [41] + ST-GAT [27]</th><td>67.12</td><td>116.53</td><td>164.61</td><td>189.82</td><td>250.88</td><td>80.93</td><td>95.72</td><td>119.03</td><td>127.66</td><td>149.44</td><td>96.16</td><td>109.06</td><td>127.5</td><td>137.75</td><td>152.49</td></tr><tr><th>Mo-Att [40] + S-LSTM [3]</th><td>65.24</td><td>109.67</td><td>168.94</td><td>200.16</td><td>268.14</td><td>84.45</td><td>101.63</td><td>121.16</td><td>135.48</td><td>157.48</td><td>100.02</td><td>113.89</td><td>130.41</td><td>144.27</td><td>158.24</td></tr><tr><th>Mo-Att [40] + S-GAN [22]</th><td>63.41</td><td>106.25</td><td>161.89</td><td>193.98</td><td>258.51</td><td>81.33</td><td>97.45</td><td>118.74</td><td>125.78</td><td>147.12</td><td>99.21</td><td>109.56</td><td>129.08</td><td>139.25</td><td>152.47</td></tr><tr><th>Mo-Att [40] + ST-GAT [27]</th><td>62.41</td><td>94.59</td><td>153.24</td><td>188.02</td><td>249.91</td><td>78.14</td><td>93.75</td><td>115.61</td><td>119.31</td><td>140.83</td><td>97.16</td><td>107.42</td><td>125.36</td><td>136.04</td><td>149.78</td></tr><tr><th>Joint Trajectory &amp; Pose</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>SC-MPF [1]</th><td>45.44</td><td>73.73</td><td>129.23</td><td>159.47</td><td>208.31</td><td>21.41</td><td>39.92</td><td>66.32</td><td>77.73</td><td>93.41</td><td>82.74</td><td>95.67</td><td>106.5</td><td>113.19</td><td>133.78</td></tr><tr><th>TRiPOD</th><td>31.04</td><td>50.8</td><td>84.74</td><td>104.05</td><td>150.41</td><td>15.36</td><td>26.32</td><td>46.45</td><td>57.94</td><td>71.78</td><td>50.62</td><td>60.77</td><td>79.69</td><td>80.07</td><td>96.98</td></tr></tbody></table>", "caption": "Table 1: Error rate in 3DPW (in cm) and PoseTrack (in pixel). In each column the best obtained result is highlighted with boldface typesetting.", "list_citation_info": ["[41] Julieta Martinez, Michael J. Black, and Javier Romero. On human motion prediction using recurrent neural networks. In CVPR, 2017.", "[40] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. History repeats itself: Human motion prediction via motion attention. In ECCV, 2020.", "[22] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In CVPR, pages 2255\u20132264, 2018.", "[27] Yingfan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, and Zhaoqi Wang. Stgat: Modeling spatial-temporal interactions for human trajectory prediction. In ICCV, pages 6272\u20136281, 2019.", "[1] Vida Adeli, Ehsan Adeli, Ian Reid, Juan Carlos Niebles, and Hamid Rezatofighi. Socially and contextually aware human motion and pose forecasting. IEEE Robotics and Automation Letters, 5(4):6033\u20136040, 2020.", "[3] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In CVPR, pages 961\u2013971, 2016."]}, {"table": "<table><thead><tr><th rowspan=\"2\">\\hlineB2</th><th rowspan=\"2\">\\mathcal{C}</th><th rowspan=\"2\">\\mathcal{P}</th><th rowspan=\"2\">\\mathcal{H}</th><th rowspan=\"2\">\\mathcal{O}</th><th rowspan=\"2\">\\mathcal{M}</th><th rowspan=\"2\">\\mathcal{FH}</th><th colspan=\"5\">prediction time in milliseconds</th></tr><tr><th>100</th><th>240</th><th>500</th><th>640</th><th>900</th></tr><tr><th>\\hlineB1.5S-MPF  [1]</th><th>\u2717</th><th>T</th><th>M</th><th>\u2717</th><th>\u2717</th><th>\u2717</th><th>52.89</th><th>89.27</th><th>146.2</th><th>176.98</th><th>249.18</th></tr><tr><th>SC-MPF  [1]</th><th>\u2713</th><th>T</th><th>M</th><th>\u2717</th><th>\u2717</th><th>\u2717</th><th>45.44</th><th>73.73</th><th>129.23</th><th>159.47</th><th>208.31</th></tr></thead><tbody><tr><th>Baseline 1</th><td>\u2713</td><td>T</td><td>G</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>39.74</td><td>64.44</td><td>106.13</td><td>128.36</td><td>181.32</td></tr><tr><th>Baseline 2</th><td>\u2713</td><td>G</td><td>G</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>33.99</td><td>54.57</td><td>93.75</td><td>114.75</td><td>167.32</td></tr><tr><th>Baseline 3</th><td>\u2713</td><td>G</td><td>G</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>32.64</td><td>52.73</td><td>91.25</td><td>111.9</td><td>166.68</td></tr><tr><th>Baseline 4</th><td>\u2713</td><td>G</td><td>G</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>32.85</td><td>52.64</td><td>88.77</td><td>108.38</td><td>161.72</td></tr><tr><th>TRiPOD</th><td>\u2713</td><td>G</td><td>G</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>31.56</td><td>51.97</td><td>86.53</td><td>107.52</td><td>153.12</td></tr><tr><th>TRiPOD(CL)</th><td>\u2713</td><td>G</td><td>G</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>31.04</td><td>50.8</td><td>84.74</td><td>104.05</td><td>150.41</td></tr><tr><th>\\hlineB2</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 2: Ablation study on 3DPW based on VIM (ignored invisibility). Each notation is defined as: \\mathbfcal{C}: Scene context, \\mathbfcal{P}: Input pose representation, tensors (T) or attention graph (G). \\mathbfcal{H}: Social module (max operation (M) or attention graph (G)). \\mathbfcal{O}: Human-object graph. \\mathbfcal{M}: Message passing. \\mathbfcal{FH}: Human interactions in future. CL: Curriculum Learning. ", "list_citation_info": ["[1] Vida Adeli, Ehsan Adeli, Ian Reid, Juan Carlos Niebles, and Hamid Rezatofighi. Socially and contextually aware human motion and pose forecasting. IEEE Robotics and Automation Letters, 5(4):6033\u20136040, 2020."]}]}