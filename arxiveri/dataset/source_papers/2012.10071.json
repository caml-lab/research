{"title": "TDN: Temporal Difference Networks for Efficient Action Recognition", "abstract": "Temporal modeling still remains challenging for action recognition in videos. To mitigate this issue, this paper presents a new video architecture, termed as Temporal Difference Network (TDN), with a focus on capturing multi-scale temporal information for efficient action recognition. The core of our TDN is to devise an efficient temporal module (TDM) by explicitly leveraging a temporal difference operator, and systematically assess its effect on short-term and long-term motion modeling. To fully capture temporal information over the entire video, our TDN is established with a two-level difference modeling paradigm. Specifically, for local motion modeling, temporal difference over consecutive frames is used to supply 2D CNNs with finer motion pattern, while for global motion modeling, temporal difference across segments is incorporated to capture long-range structure for motion feature excitation. TDN provides a simple and principled temporal modeling framework and could be instantiated with the existing CNNs at a small extra computational cost. Our TDN presents a new state of the art on the Something-Something V1 & V2 datasets and is on par with the best performance on the Kinetics-400 dataset. In addition, we conduct in-depth ablation studies and plot the visualization results of our TDN, hopefully providing insightful analysis on temporal difference modeling. We release the code at https://github.com/MCG-NJU/TDN.", "authors": ["Limin Wang", " Zhan Tong", " Bin Ji", " Gangshan Wu"], "pdf_url": "https://arxiv.org/abs/2012.10071", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">Frames</td><td rowspan=\"2\">GFLOPs</td><td colspan=\"2\">Sth-Sth V1</td></tr><tr><td>Top1</td><td>Top5</td></tr><tr><th>TSN-RGB [38]</th><td><p>BNInception</p></td><td><p>8</p></td><td><p>16</p></td><td><p>19.5</p></td><td><p>-</p></td></tr><tr><th>TRN-Multiscale [44]</th><td><p>BNInception</p></td><td><p>8</p></td><td><p>33</p></td><td><p>34.4</p></td><td><p>-</p></td></tr><tr><th>S3D-G [41]</th><td><p>Inception</p></td><td><p>64</p></td><td><p>71.38</p></td><td><p>48.2</p></td><td><p>78.7</p></td></tr><tr><th>TSM [19]</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>98</p></td><td><p>49.7</p></td><td><p>78.5</p></td></tr><tr><th>TEINet [20]</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>99</p></td><td><p>52.5</p></td><td><p>-</p></td></tr><tr><th>TANet [21]</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>99</p></td><td><p>50.6</p></td><td><p>79.3</p></td></tr><tr><th>TEA [18]</th><td><p>ResNet50</p></td><td><p>16</p></td><td><p>70</p></td><td><p>51.9</p></td><td><p>80.3</p></td></tr><tr><th>TAM [4]</th><td><p>bLResNet50</p></td><td><p>16\\times2</p></td><td><p>47.7</p></td><td><p>48.4</p></td><td><p>78.8</p></td></tr><tr><th>ECO{}_{En}Lite [46]</th><td><p>BNIncep+R18</p></td><td><p>92</p></td><td><p>267</p></td><td><p>46.4</p></td><td><p>-</p></td></tr><tr><th>I3D [1]</th><td><p>ResNet50</p></td><td><p>32\\times2</p></td><td><p>306</p></td><td><p>41.6</p></td><td><p>72.2</p></td></tr><tr><th>NL I3D+GCN [40]</th><td><p>R50+GCN</p></td><td><p>32\\times2</p></td><td><p>606</p></td><td><p>46.1</p></td><td><p>76.8</p></td></tr><tr><th>GST [22]</th><td><p>ResNet50</p></td><td><p>16</p></td><td><p>59</p></td><td><p>48.6</p></td><td><p>77.9</p></td></tr><tr><th>STM [13]</th><td><p>ResNet50</p></td><td><p>16\\times30</p></td><td><p>67\\times30</p></td><td><p>50.7</p></td><td><p>80.4</p></td></tr><tr><th>V4D [42]</th><td><p>ResNet50</p></td><td><p>8\\times4</p></td><td><p>167.6</p></td><td><p>50.4</p></td><td><p>-</p></td></tr><tr><th>SmallBigNet [17]</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>157</p></td><td><p>50.4</p></td><td><p>80.5</p></td></tr><tr><th>CorrNet [35]</th><td><p>ResNet50</p></td><td><p>32\\times10</p></td><td><p>115\\times10</p></td><td><p>49.3</p></td><td><p>-</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet50</p></td><td><p>8</p></td><td><p>36</p></td><td><p>52.3</p></td><td><p>80.6</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet50</p></td><td><p>16</p></td><td><p>72</p></td><td><p>53.9</p></td><td><p>82.1</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>108</p></td><td>55.1</td><td>82.9</td></tr><tr><th>CorrNet [35]</th><td><p>ResNet101</p></td><td><p>32\\times30</p></td><td><p>224\\times30</p></td><td><p>51.7</p></td><td><p>-</p></td></tr><tr><th>CorrNet [35] <sup>1</sup><sup>1</sup>1Pre-trained on Sports1M.</th><td><p>ResNet101</p></td><td><p>32\\times30</p></td><td><p>224\\times30</p></td><td><p>53.3</p></td><td><p>-</p></td></tr><tr><th>GSM [30]</th><td><p>Inception V3</p></td><td><p>fusion</p></td><td><p>268</p></td><td><p>55.2</p></td><td><p>-</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet101</p></td><td><p>8</p></td><td><p>66</p></td><td><p>54.1</p></td><td><p>81.9</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet101</p></td><td><p>16</p></td><td><p>132</p></td><td><p>55.3</p></td><td><p>83.3</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet101</p></td><td><p>8+16</p></td><td><p>198</p></td><td>56.8</td><td>84.1</td></tr><tr><th rowspan=\"2\">Method</th><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">Frames</td><td rowspan=\"2\">GFLOPs</td><td colspan=\"2\">Sth-Sth V2</td></tr><tr><td>Top1</td><td>Top5</td></tr><tr><th>TRN-Multiscale [44]</th><td><p>BNInception</p></td><td><p>8</p></td><td><p>33</p></td><td><p>48.8</p></td><td><p>77.6</p></td></tr><tr><th>TAM [4]</th><td><p>bLResNet50</p></td><td><p>16\\times2</p></td><td><p>47.7</p></td><td><p>61.7</p></td><td><p>88.1</p></td></tr><tr><th>TSM [19]</th><td><p>ResNet50</p></td><td><p>16\\times6</p></td><td><p>65\\times6</p></td><td><p>63.4</p></td><td><p>88.5</p></td></tr><tr><th>GST [22]</th><td><p>ResNet50</p></td><td><p>16</p></td><td><p>59</p></td><td><p>62.6</p></td><td><p>87.9</p></td></tr><tr><th>STM [13]</th><td><p>ResNet50</p></td><td><p>16\\times30</p></td><td><p>67\\times30</p></td><td><p>64.2</p></td><td><p>89.8</p></td></tr><tr><th>SmallBigNet [17]</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>157</p></td><td><p>63.3</p></td><td><p>88.8</p></td></tr><tr><th>TEINet [19]</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>98</p></td><td><p>65.5</p></td><td><p>89.8</p></td></tr><tr><th>TANet [21]</th><td><p>ResNet50</p></td><td><p>24\\times6</p></td><td><p>99 \\times6</p></td><td><p>66.0</p></td><td><p>90.1</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet50</p></td><td><p>8</p></td><td><p>36</p></td><td><p>64.0</p></td><td><p>88.8</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet50</p></td><td><p>16</p></td><td><p>72</p></td><td><p>65.3</p></td><td><p>89.5</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet50</p></td><td><p>8+16</p></td><td><p>108</p></td><td>67.0</td><td>90.3</td></tr><tr><th>TDN (Ours)</th><td><p>ResNet101</p></td><td><p>8</p></td><td><p>66</p></td><td><p>65.8</p></td><td><p>90.2</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet101</p></td><td><p>16</p></td><td><p>132</p></td><td><p>66.9</p></td><td><p>90.9</p></td></tr><tr><th>TDN (Ours)</th><td><p>ResNet101</p></td><td><p>8+16</p></td><td><p>198</p></td><td>68.2</td><td>91.6</td></tr></tbody></table>", "caption": "Table 2: Comparison with the state-of-the-art methods on Something-Something V1 and V2. We instantiate our TDN with the backbones of ResNet50 and ResNet101 for evaluation. We compare with other methods with similar backbones under the 1-clip and center crop setting. \u201c-\u201d indicates the numbers are not available for us. {}^{1} Pre-trained on Sports1M. ", "list_citation_info": ["[19] Ji Lin, Chuang Gan, and Song Han. TSM: temporal shift module for efficient video understanding. In ICCV, pages 7082\u20137092, 2019.", "[20] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. Teinet: Towards an efficient architecture for video recognition. In AAAI, 2020.", "[21] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. TAM: temporal adaptive module for video recognition. CoRR, abs/2005.06803, 2020.", "[13] Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. STM: spatiotemporal and motion encoding for action recognition. In ICCV, pages 2000\u20132009, 2019.", "[42] Shiwen Zhang, Sheng Guo, Weilin Huang, Matthew R Scott, and Limin Wang. V4d: 4d convolutional neural networks for video-level representation learning. In ICLR, 2020.", "[40] Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In ECCV, pages 413\u2013431, 2018.", "[41] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In ECCV, volume 11219, pages 318\u2013335. Springer, 2018.", "[38] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, pages 20\u201336, 2016.", "[18] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages 909\u2013918, 2020.", "[44] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos. In ECCV, volume 11205, pages 831\u2013846. Springer, 2018.", "[4] Quanfu Fan, Chun-Fu (Richard) Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation. In NIPS, pages 2261\u20132270, 2019.", "[35] Heng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli. Video modeling with correlation networks. In CVPR, pages 352\u2013361, 2020.", "[1] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724\u20134733, 2017.", "[30] Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz. Gate-shift networks for video action recognition. In CVPR, pages 1099\u20131108, 2020.", "[22] Chenxu Luo and Alan L Yuille. Grouped spatial-temporal aggregation for efficient action recognition. In ICCV, pages 5512\u20135521, 2019.", "[17] Xianhang Li, Yali Wang, Zhipeng Zhou, and Yu Qiao. Smallbignet: Integrating core and contextual views for video classification. In CVPR, pages 1092\u20131101, 2020.", "[46] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. ECO: efficient convolutional network for online video understanding. In ECCV, pages 713\u2013730, 2018."]}, {"table": "<table><tbody><tr><th>Method</th><td>Backbone</td><td>Frames</td><td>GFLOPs</td><td>Top1</td><td>Top5</td></tr><tr><th>TSN [38]</th><td>InceptionV3</td><td>25\\times1\\times10</td><td>3.2\\times250</td><td>72.5</td><td>90.2</td></tr><tr><th>S3D-G [41]</th><td>InceptionV1</td><td>64\\times10\\times3</td><td>71.4\\times30</td><td>74.7</td><td>93.4</td></tr><tr><th>R(2+1)D [33]</th><td>ResNet34</td><td>32\\times10\\times1</td><td>152\\times10</td><td>74.3</td><td>91.4</td></tr><tr><th>TSM [19]</th><td>ResNet50</td><td>16\\times10\\times3</td><td>65\\times30</td><td>74.7</td><td>91.4</td></tr><tr><th>TEINet [20]</th><td>ResNet50</td><td>16\\times10\\times3</td><td>66\\times30</td><td>76.2</td><td>92.5</td></tr><tr><th>TEA [18]</th><td>ResNet50</td><td>16\\times10\\times3</td><td>70\\times30</td><td>76.1</td><td>92.5</td></tr><tr><th>TAM [4]</th><td>bLResNet50</td><td>48\\times3\\times3</td><td>93.4\\times9</td><td>73.5</td><td>91.2</td></tr><tr><th>TANet [21]</th><td>ResNet50</td><td>16\\times4\\times3</td><td>86\\times12</td><td>76.9</td><td>92.9</td></tr><tr><th>ARTNet [36]</th><td>ResNet18</td><td>16 \\times25\\times10</td><td>23.5\\times250</td><td>70.7</td><td>89.3</td></tr><tr><th>I3D [1]</th><td>InceptionV1</td><td>64\\timesN/A\\timesN/A</td><td>108\\timesN/A</td><td>72.1</td><td>90.3</td></tr><tr><th>NL I3D [39]</th><td>ResNet50</td><td>128\\times10\\times3</td><td>282\\times30</td><td>76.5</td><td>92.6</td></tr><tr><th>SlowOnly [6]</th><td>ResNet50</td><td>8\\times10\\times3</td><td>41.9\\times30</td><td>74.8</td><td>91.6</td></tr><tr><th>SlowFast [6]</th><td>ResNet50</td><td>(4+32)\\times10\\times3</td><td>36.1\\times30</td><td>75.6</td><td>92.1</td></tr><tr><th>SlowFast [6]</th><td>ResNet50</td><td>(8+32)\\times10\\times3</td><td>65.7\\times30</td><td>77.0</td><td>92.6</td></tr><tr><th>SmallBigNet [17]</th><td>ResNet50</td><td>8\\times10\\times3</td><td>57\\times30</td><td>76.3</td><td>92.5</td></tr><tr><th>CorrNet [35]</th><td>ResNet50</td><td>32\\times10\\times1</td><td>115\\times10</td><td>77.2</td><td>-</td></tr><tr><th>TDN (Ours)</th><td>ResNet50</td><td>8\\times10\\times3</td><td>36\\times30</td><td>76.6</td><td>92.8</td></tr><tr><th>TDN (Ours)</th><td>ResNet50</td><td>16\\times10\\times3</td><td>72\\times30</td><td>77.5</td><td>93.2</td></tr><tr><th>TDN (Ours)</th><td>ResNet50</td><td>(8+16)\\times10\\times3</td><td>108\\times30</td><td>78.4</td><td>93.6</td></tr><tr><th>NL I3D [39]</th><td>ResNet101</td><td>128\\times10\\times3</td><td>359\\times30</td><td>77.7</td><td>93.3</td></tr><tr><th>ip-CSN [32]</th><td>ResNet101</td><td>32\\times10\\times3</td><td>83.0\\times30</td><td>76.7</td><td>92.3</td></tr><tr><th>SlowFast [6]</th><td>ResNet101</td><td>(8+32)\\times10\\times3</td><td>106\\times30</td><td>77.9</td><td>93.2</td></tr><tr><th>SlowFast [6]</th><td>ResNet101</td><td>(16+64)\\times10\\times3</td><td>213\\times30</td><td>78.9</td><td>93.5</td></tr><tr><th>SmallBigNet [17]</th><td>ResNet101</td><td>32\\times4\\times3</td><td>418\\times12</td><td>77.4</td><td>93.3</td></tr><tr><th>CorrNet [35]</th><td>ResNet101</td><td>32\\times10\\times3</td><td>224\\times30</td><td>79.2</td><td>-</td></tr><tr><th>TDN (Ours)</th><td>ResNet101</td><td>8\\times10\\times3</td><td>66\\times30</td><td>77.5</td><td>93.6</td></tr><tr><th>TDN (Ours)</th><td>ResNet101</td><td>16\\times10\\times3</td><td>132\\times30</td><td>78.5</td><td>93.9</td></tr><tr><th>TDN (Ours)</th><td>ResNet101</td><td>(8+16)\\times10\\times3</td><td>198\\times30</td><td>79.4</td><td>94.4</td></tr><tr><th>SlowFast [6]</th><td>R101+NL</td><td>(16+64)\\times10\\times3</td><td>234\\times30</td><td>79.8</td><td>93.9</td></tr><tr><th>X3D [5]</th><td>X3D-XL</td><td>16\\times10\\times3</td><td>48.4\\times30</td><td>79.1</td><td>93.9</td></tr></tbody></table>", "caption": "Table 3: Comparison with the state-of-the-art methods on the validation set of Kinetics-400. We instantiate our TDN with the backbones of ResNet50 and ResNet101. For fair comparison, we compare with the other methods by using the similar backbones without pre-training on extra videos. \u201c-\u201d indicates the numbers are not available for us.", "list_citation_info": ["[19] Ji Lin, Chuang Gan, and Song Han. TSM: temporal shift module for efficient video understanding. In ICCV, pages 7082\u20137092, 2019.", "[20] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. Teinet: Towards an efficient architecture for video recognition. In AAAI, 2020.", "[21] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. TAM: temporal adaptive module for video recognition. CoRR, abs/2005.06803, 2020.", "[41] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In ECCV, volume 11219, pages 318\u2013335. Springer, 2018.", "[38] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, pages 20\u201336, 2016.", "[33] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, pages 6450\u20136459, 2018.", "[18] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages 909\u2013918, 2020.", "[4] Quanfu Fan, Chun-Fu (Richard) Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation. In NIPS, pages 2261\u20132270, 2019.", "[35] Heng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli. Video modeling with correlation networks. In CVPR, pages 352\u2013361, 2020.", "[1] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724\u20134733, 2017.", "[32] Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torresani. Video classification with channel-separated convolutional networks. In ICCV, pages 5551\u20135560, 2019.", "[6] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In ICCV, pages 6201\u20136210, 2019.", "[36] Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In CVPR, pages 1430\u20131439, 2018.", "[17] Xianhang Li, Yali Wang, Zhipeng Zhou, and Yu Qiao. Smallbignet: Integrating core and contextual views for video classification. In CVPR, pages 1092\u20131101, 2020.", "[39] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, pages 7794\u20137803, 2018.", "[5] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In CVPR, pages 203\u2013213, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Pretrain</th><th>Backbone</th><th>UCF101</th><th>HMDB51</th></tr></thead><tbody><tr><th>TSN [38]</th><td>ImageNet</td><td>Inception V2</td><td>86.4%</td><td>53.7%</td></tr><tr><th>P3D [25]</th><td>ImageNet</td><td>ResNet50</td><td>88.6%</td><td>-</td></tr><tr><th>C3D [31]</th><td>Sports-1M</td><td>ResNet18</td><td>85.8%</td><td>54.9%</td></tr><tr><th>I3D [1]</th><td>ImageNet+Kinetics</td><td>Inception V2</td><td>95.6%</td><td>74.8%</td></tr><tr><th>ARTNet [36]</th><td>Kinetics</td><td>ResNet18</td><td>94.3%</td><td>70.9%</td></tr><tr><th>S3D [41]</th><td>ImageNet+Kinetics</td><td>Inception V2</td><td>96.8%</td><td>75.9%</td></tr><tr><th>R(2+1)D [33]</th><td>Kinetics</td><td>ResNet34</td><td>96.8%</td><td>74.5%</td></tr><tr><th>TSM [19]</th><td>Kinetics</td><td>ResNet50</td><td>96.0%</td><td>73.2%</td></tr><tr><th>STM [13]</th><td>ImageNet + Kinetics</td><td>ResNet50</td><td>96.2%</td><td>72.2%</td></tr><tr><th>TEA [18]</th><td>ImageNet + Kinetics</td><td>ResNet50</td><td>96.9%</td><td>73.3%</td></tr><tr><th>TDN(Ours)</th><td>ImageNet + Kinetics</td><td>ResNet50</td><td>97.4%</td><td>76.3%</td></tr></tbody></table>", "caption": "Table 4: Comparison with the state-of-the-art methods on UCF101 and HMDB51. ", "list_citation_info": ["[19] Ji Lin, Chuang Gan, and Song Han. TSM: temporal shift module for efficient video understanding. In ICCV, pages 7082\u20137092, 2019.", "[13] Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. STM: spatiotemporal and motion encoding for action recognition. In ICCV, pages 2000\u20132009, 2019.", "[38] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, pages 20\u201336, 2016.", "[41] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In ECCV, volume 11219, pages 318\u2013335. Springer, 2018.", "[33] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, pages 6450\u20136459, 2018.", "[31] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In ICCV, pages 4489\u20134497, 2015.", "[25] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. ICCV, pages 5534\u20135542, 2017.", "[18] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages 909\u2013918, 2020.", "[1] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724\u20134733, 2017.", "[36] Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In CVPR, pages 1430\u20131439, 2018."]}, {"table": "<table><tbody><tr><th>Method</th><th>Frames\\timesClips\\timesCrops</th><td>Time (ms/video)</td><td>Top1 (%)</td></tr><tr><th>TSN [38]</th><th>8\\times 1\\times 1</th><td>7.9</td><td>19.7</td></tr><tr><th>TSM [19]</th><th>16\\times 1\\times 1</th><td>16.7</td><td>47.2</td></tr><tr><th>STM [13]</th><th>8\\times 1\\times 1</th><td>11.1</td><td>47.5</td></tr><tr><th>I3D [1]</th><th>32\\times 3\\times 2</th><td>2095</td><td>41.6</td></tr><tr><th>S-TDM</th><th>8\\times 1\\times 1</th><td>12.3</td><td>49.5</td></tr><tr><th>L-TDM</th><th>8\\times 1\\times 1</th><td>15.8</td><td>48.9</td></tr><tr><th>TDN</th><th>8\\times 1\\times 1</th><td>22.1</td><td>52.3</td></tr></tbody></table>", "caption": "Table 5: Running time analysis on a Tesla V100.", "list_citation_info": ["[19] Ji Lin, Chuang Gan, and Song Han. TSM: temporal shift module for efficient video understanding. In ICCV, pages 7082\u20137092, 2019.", "[1] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724\u20134733, 2017.", "[13] Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. STM: spatiotemporal and motion encoding for action recognition. In ICCV, pages 2000\u20132009, 2019.", "[38] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, pages 20\u201336, 2016."]}]}