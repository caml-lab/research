{"title": "Detreg: Unsupervised pretraining with region priors for object detection", "abstract": "Recent self-supervised pretraining methods for object detection largely focus on pretraining the backbone of the object detector, neglecting key parts of detection architecture. Instead, we introduce DETReg, a new self-supervised method that pretrains the entire object detection network, including the object localization and embedding components. During pretraining, DETReg predicts object localizations to match the localizations from an unsupervised region proposal generator and simultaneously aligns the corresponding feature embeddings with embeddings from a self-supervised image encoder. We implement DETReg using the DETR family of detectors and show that it improves over competitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship benchmarks. In low-data regimes, including semi-supervised and few-shot learning settings, DETReg establishes many state-of-the-art results, e.g., on COCO we see a +6.0 AP improvement for 10-shot detection and over 2 AP improvements when training with only 1\\% of the labels. For code and pretrained models, visit the project page at https://amirbar.net/detreg", "authors": ["Amir Bar", " Xin Wang", " Vadim Kantorov", " Colorado J Reed", " Roei Herzig", " Gal Chechik", " Anna Rohrbach", " Trevor Darrell", " Amir Globerson"], "pdf_url": "https://arxiv.org/abs/2106.04550", "list_table_and_caption": [{"table": "<table><thead><tr><th>Pretraining</th><th>Detector</th><th>Epochs</th><th>AP</th><th>\\textrm{AP}_{50}</th><th>\\textrm{AP}_{75}</th></tr></thead><tbody><tr><th>Supervised</th><th rowspan=\"4\">DETR</th><th rowspan=\"4\">150</th><td>39.5</td><td>60.3</td><td>41.4</td></tr><tr><th>SwAV [6]</th><td>39.7</td><td>60.3</td><td>41.7</td></tr><tr><th>UP-DETR</th><td>40.5</td><td>60.8</td><td>42.6</td></tr><tr><th>DETReg</th><td>41.9{}^{+1.4}</td><td>61.9{}^{+1.1}</td><td>44.1{}^{+1.5}</td></tr><tr><th>Supervised</th><th rowspan=\"4\">DETR</th><th rowspan=\"4\">300</th><td>40.8</td><td>61.2</td><td>42.9</td></tr><tr><th>SwAV [6]</th><td>42.1</td><td>63.1</td><td>44.5</td></tr><tr><th>UP-DETR</th><td>42.8</td><td>63.0</td><td>45.3</td></tr><tr><th>DETReg</th><td>43.7{}^{+0.9}</td><td>63.7{}^{+0.7}</td><td>46.6{}^{+1.3}</td></tr><tr><th>Supervised</th><th rowspan=\"4\">DDETR</th><th rowspan=\"4\">50</th><td>44.5</td><td>63.6</td><td>48.7</td></tr><tr><th>SwAV [6]</th><td>45.2</td><td>64.0</td><td>49.5</td></tr><tr><th>UP-DETR</th><td>44.7</td><td>63.7</td><td>48.6</td></tr><tr><th>DETReg</th><td>45.5{}^{+0.8}</td><td>64.1{}^{+0.4}</td><td>49.9{}^{+1.3}</td></tr></tbody></table><br/>", "caption": "Table 1: Object detection results when trained on MS COCO train2017 and evaluated on val2017. Both DETReg and UP-DETR are pretrained on IN1K under comparable settings, while supervised and SwAV only pretrain the backbone of the object detector. We explore both the DETR and Deformable DETR (DDETR) architectures; for compatibility with prior work, we finetuned the DETR for 150/300 epochs and DDETR for 50 epochs.", "list_citation_info": ["[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">PASCAL VOC</th><th colspan=\"3\">Airbus Ship</th></tr><tr><th>AP</th><th>AP{}_{50}</th><th>AP{}_{75}</th><th>AP</th><th>AP{}_{50}</th><th>AP{}_{75}</th></tr></thead><tbody><tr><th>Supervised</th><td>59.5</td><td>82.6</td><td>65.6</td><td>79.8</td><td>95.8</td><td>89.4</td></tr><tr><th>SwAV [6]</th><td>61.0</td><td>83.0</td><td>68.1</td><td>78.3</td><td>95.7</td><td>88.7</td></tr><tr><th>DETReg</th><td>63.5</td><td>83.3</td><td>70.3</td><td>81.0</td><td>95.9</td><td>89.7</td></tr></tbody></table>", "caption": "Table 2: Object detection finetuned on PASCAL VOC and Airbus Ship data. The model is finetuned on PASCAL VOC trainval07+2012 and evaluated on test07 (left), and Airbus Ship Detection finetuned on the train split and evaluated on the 3k test images (right). All models are based on Deformable DETR [71]. Bold values indicate an improvement \\geq 0.3 AP.", "list_citation_info": ["[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020.", "[71] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Model</th><td rowspan=\"2\">Detector</td><td colspan=\"2\">Novel AP</td><td colspan=\"2\">Novel AP{}_{75}</td></tr><tr><td>10</td><td>30</td><td>10</td><td>30</td></tr><tr><th>YOLO-ft-full [47, 35]</th><td rowspan=\"2\">YOLOv2</td><td>3.1</td><td>1.7</td><td>7.7</td><td>6.4</td></tr><tr><th>FSRW [35]</th><td>5.6</td><td>9.1</td><td>4.6</td><td>7.6</td></tr><tr><th>FRCN-ft-full [66]</th><td rowspan=\"7\">FRCN</td><td>6.5</td><td>11.1</td><td>5.9</td><td>10.3</td></tr><tr><th>FRCN-ft-full [56]</th><td>9.2</td><td>12.5</td><td>9.2</td><td>12.0</td></tr><tr><th>MetaDet [58]</th><td>7.1</td><td>11.3</td><td>6.1</td><td>8.1</td></tr><tr><th>Meta R-CNN [66]</th><td>8.7</td><td>12.4</td><td>6.6</td><td>10.8</td></tr><tr><th>TFA [56]</th><td>10.0</td><td>13.7</td><td>9.3</td><td>13.4</td></tr><tr><th>DeFRCN [71]</th><td>18.5</td><td>22.6</td><td>-</td><td>-</td></tr><tr><th>DAnA [12]</th><td>18.6</td><td>21.6</td><td>17.2</td><td>20.3</td></tr><tr><th>DDETR-ft-full [69]</th><td rowspan=\"2\">DDETR*</td><td>11.7</td><td>16.3</td><td>12.1</td><td>16.7</td></tr><tr><th>Meta-DETR [69]</th><td>19.0</td><td>22.2</td><td>19.7</td><td>22.8</td></tr><tr><th>DDETR-ft-full</th><td rowspan=\"2\">DDETR</td><td>23.3</td><td>28.4</td><td>25.4</td><td>31.7</td></tr><tr><th>DETReg-ft-full</th><td>25.0</td><td>30.0</td><td>27.6</td><td>33.7</td></tr></tbody><tfoot><tr><th colspan=\"6\"> DDETR{}^{*} is the customized single scale deformable DETR model used in  [69].</th></tr></tfoot></table>", "caption": "Table 3: Few-shot detection evaluation on COCO. We trained the model on the 60 base classes and then evaluate the model performance on the 20 novel categories, following the data split used in [56]. We show that DETReg outperforms previous few-shot object detectors by a large margin through simple fine-tuning on the few-shot datasets.", "list_citation_info": ["[69] Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, and Shijian Lu. Meta-detr: Few-shot object detection via unified image-level meta-learning. arXiv preprint arXiv:2103.11731, 2021.", "[12] Tung-I Chen, Yueh-Cheng Liu, Hung-Ting Su, Yu-Cheng Chang, Yu-Hsiang Lin, Jia-Fong Yeh, and Winston H Hsu. Should i look at the head or the tail? dual-awareness attention for few-shot object detection. arXiv preprint arXiv:2102.12152, 2021.", "[66] Xiaopeng Yan, Ziliang Chen, Anni Xu, Xiaoxi Wang, Xiaodan Liang, and Liang Lin. Meta r-cnn: Towards general solver for instance-level low-shot learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 9577\u20139586, 2019.", "[47] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7263\u20137271, 2017.", "[35] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In ICCV, 2019.", "[56] Xin Wang, Thomas E Huang, Trevor Darrell, Joseph E Gonzalez, and Fisher Yu. Frustratingly simple few-shot object detection. arXiv preprint arXiv:2003.06957, 2020.", "[71] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[58] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Meta-learning to detect rare objects. In Proceedings of the IEEE International Conference on Computer Vision, pages 9925\u20139934, 2019."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th rowspan=\"2\">Detector</th><th colspan=\"2\">Novel AP</th><th colspan=\"2\">Novel AP{}_{75}</th></tr><tr><th>10</th><th>30</th><th>10</th><th>30</th></tr><tr><th>TFA [56] (w/base)</th><th>FRCN</th><th>10.0</th><th>13.7</th><th>9.3</th><th>13.4</th></tr></thead><tbody><tr><th>DDETR-ft-full</th><td rowspan=\"4\">DDETR</td><td>0.03</td><td>0.01</td><td>0.04</td><td>0.02</td></tr><tr><th>DDETR-ft-decoder</th><td>3.3</td><td>10.2</td><td>2.7</td><td>10.7</td></tr><tr><th>DETReg-ft-decoder</th><td>10.2</td><td>17.9</td><td>11.1</td><td>19.2</td></tr><tr><th>DETReg-ft-full</th><td>10.6</td><td>18.0</td><td>11.6</td><td>19.6</td></tr></tbody></table>", "caption": "Table 4: Few-shot object detection without training on the COCO base classes. To test DETReg\u2019s performance on extreme few-shot scenarios, we conduct evaluation where DETReg is finetuned only on the K-shot COCO subsets. DETReg outperforms previous methods such as TFA [56] that use base class data. ", "list_citation_info": ["[56] Xin Wang, Thomas E Huang, Trevor Darrell, Joseph E Gonzalez, and Fisher Yu. Frustratingly simple few-shot object detection. arXiv preprint arXiv:2003.06957, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>AP</th><th>AP{}_{\\text{50}}</th><th>AP{}_{\\text{75}}</th><th>R@1</th><th>R@10</th><th>R@100</th></tr></thead><tbody><tr><th>UP-DETR [16]</th><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.4</td></tr><tr><th>Rand. Prop.</th><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.8</td></tr><tr><th>Selective Search [54]</th><td>0.2</td><td>0.5</td><td>0.1</td><td>0.2</td><td>1.5</td><td>10.9</td></tr><tr><th>ImpSamp (ours)</th><td>0.7</td><td>2.0</td><td>0.1</td><td>0.3</td><td>1.8</td><td>9.0</td></tr><tr><th>Random-K (ours)</th><td>0.7</td><td>2.4</td><td>0.2</td><td>0.5</td><td>2.9</td><td>11.7</td></tr><tr><th>Top-K (ours)</th><td>1.0</td><td>3.1</td><td>0.6</td><td>0.6</td><td>3.6</td><td>12.7</td></tr></tbody></table>", "caption": "Table 7: Class agnostic object proposal evaluation on MS COCO val2017. The models are trained on IN100 and for each method, we consider the top 100 proposals. We show DETReg identifies objects more effectively than the previous methods.", "list_citation_info": ["[16] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. UP-DETR: Unsupervised pre-training for object detection with transformers. CVPR, 2021.", "[54] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. IJCV, 104(2):154\u2013171, 2013."]}, {"table": "<table><thead><tr><th>Method</th><th>Detector</th><th>AP</th><th>AP{}_{50}</th><th>AP{}_{75}</th></tr></thead><tbody><tr><th>Supervised</th><th rowspan=\"13\">FRCN</th><td>56.1</td><td>82.6</td><td>62.7</td></tr><tr><th>InsDis [61]</th><td>55.2</td><td>80.9</td><td>61.2</td></tr><tr><th>Jigsaw [25]</th><td>48.9</td><td>75.1</td><td>52.9</td></tr><tr><th>NPID++ [43]</th><td>52.3</td><td>79.1</td><td>56.9</td></tr><tr><th>SimCLR [10]</th><td>51.5</td><td>79.4</td><td>55.6</td></tr><tr><th>PIRL [43]</th><td>54.0</td><td>80.7</td><td>59.7</td></tr><tr><th>BoWNet [22]</th><td>55.8</td><td>81.3</td><td>61.1</td></tr><tr><th>MoCo [28]</th><td>55.9</td><td>81.5</td><td>62.6</td></tr><tr><th>MoCo-v2 [13]</th><td>57.0</td><td>82.4</td><td>63.6</td></tr><tr><th>SwAV [6]</th><td>56.1</td><td>82.6</td><td>62.7</td></tr><tr><th>DenseCL [57]</th><td>58.7</td><td>82.8</td><td>65.2</td></tr><tr><th>DetCo [64]</th><td>58.2</td><td>82.7</td><td>65.0</td></tr><tr><th>ReSim [62]</th><td>59.2</td><td>82.9</td><td>65.9</td></tr><tr><th>Supervised</th><th rowspan=\"2\">DETR</th><td>54.1</td><td>78.0</td><td>58.3</td></tr><tr><th>UP-DETR [16]</th><td>57.2</td><td>80.1</td><td>62.0</td></tr><tr><th>Supervised</th><th rowspan=\"3\">DDETR</th><td>59.5</td><td>82.6</td><td>65.6</td></tr><tr><th>SwAV [6]</th><td>61.0</td><td>83.0</td><td>68.1</td></tr><tr><th>DETReg</th><td>63.5</td><td>83.3</td><td>70.3</td></tr></tbody></table>", "caption": "Table 8: Object detection finetuned on PASCAL VOC. The model is finetuned on PASCAL VOC trainval07+2012 and evaluated on test07. Models are based on Faster-RCNN [49] (FRCN), DETR [5], and Deformable DETR [71] (DDETR). Bold values indicate an improvement \\geq 0.3 AP.", "list_citation_info": ["[16] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. UP-DETR: Unsupervised pre-training for object detection with transformers. CVPR, 2021.", "[61] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3733\u20133742, 2018.", "[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020.", "[28] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020.", "[22] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, and Matthieu Cord. Learning representations by predicting bags of visual words. In CVPR, 2020.", "[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.", "[13] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.", "[49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.", "[25] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-supervised visual representation learning. In ICCV, 2019.", "[43] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In CVPR, 2020.", "[71] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[57] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. arXiv preprint arXiv:2011.09157, 2020.", "[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. ECCV, 2020.", "[64] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Zhenguo Li, and Ping Luo. Detco: Unsupervised contrastive learning for object detection. arXiv preprint arXiv:2102.04803, 2021.", "[62] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, and Trevor Darrell. Region similarity representation learning. arXiv preprint arXiv:2103.12902, 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Approach</th><th rowspan=\"2\">Detector</th><th colspan=\"4\">COCO</th></tr><tr><td>1%</td><td>2%</td><td>5%</td><td>10%</td></tr><tr><th>CSD [34]</th><th rowspan=\"4\">Auxiliary</th><th rowspan=\"4\">FRCN</th><th>10.5 \\pm 0.1</th><th>13.9 \\pm 0.1</th><th>18.6 \\pm 0.1</th><th>22.5 \\pm 0.1</th></tr><tr><th>STAC [53]</th><td>14.0 \\pm 0.6</td><td>18.3 \\pm 0.3</td><td>24.4 \\pm 0.1</td><td>28.6 \\pm 0.2</td></tr><tr><th>U-T [42]</th><th>20.8 \\pm 0.1</th><th>24.3 \\pm 0.1</th><th>28.3 \\pm 0.1</th><th>31.5 \\pm 0.1</th></tr><tr><th>S-T [65]</th><td>20.5 \\pm 0.4</td><td>-</td><td>30.7 \\pm 0.1</td><td>34.0 \\pm 0.1</td></tr><tr><th>Supervised</th><th rowspan=\"4\">Pretraining</th><th rowspan=\"4\">DDETR</th><th>11.31 \\pm 0.3</th><th>15.22 \\pm 0.32</th><th>21.33 \\pm 0.2</th><th>26.34 \\pm 0.1</th></tr><tr><th>SwAV</th><td>11.79 \\pm 0.3</td><td>16.02 \\pm 0.4</td><td>22.81 \\pm 0.3</td><td>27.79 \\pm 0.2</td></tr><tr><th>ReSim</th><th>11.07 \\pm 0.4</th><th>15.26 \\pm 0.26</th><th>21.48 \\pm 0.1</th><th>26.56 \\pm 0.3</th></tr><tr><th>DETReg</th><td>14.58 \\pm 0.3</td><td>18.69 \\pm 0.2</td><td>24.80 \\pm 0.2</td><td>29.12 \\pm 0.2</td></tr></tbody></table>", "caption": "Table 9: Object detection using k% of the labeled data on COCO. The models are trained on train2017 using k% and then evaluated on val2017. Methods like [42] utilize auxiliary losses during the training stage using unlabeled data, whereas DETReg utilizes unlabeled data during the pretraining stage only.", "list_citation_info": ["[34] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object detection. In nips, 2019.", "[53] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020.", "[42] Yen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and Peter Vajda. Unbiased teacher for semi-supervised object detection. arXiv preprint arXiv:2102.09480, 2021.", "[65] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher. arXiv preprint arXiv:2106.09018, 2021."]}]}