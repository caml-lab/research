{"title": "Group-aware label transfer for domain adaptive person re-identification", "abstract": "Unsupervised Domain Adaptive (UDA) person re-identification (ReID) aims at adapting the model trained on a labeled source-domain dataset to a target-domain dataset without any further annotations. Most successful UDA-ReID approaches combine clustering-based pseudo-label prediction with representation learning and perform the two steps in an alternating fashion. However, offline interaction between these two steps may allow noisy pseudo labels to substantially hinder the capability of the model. In this paper, we propose a Group-aware Label Transfer (GLT) algorithm, which enables the online interaction and mutual promotion of pseudo-label prediction and representation learning. Specifically, a label transfer algorithm simultaneously uses pseudo labels to train the data while refining the pseudo labels as an online clustering algorithm. It treats the online label refinery problem as an optimal transport problem, which explores the minimum cost for assigning M samples to N pseudo labels. More importantly, we introduce a group-aware strategy to assign implicit attribute group IDs to samples. The combination of the online label refining algorithm and the group-aware strategy can better correct the noisy pseudo label in an online fashion and narrow down the search space of the target identity. The effectiveness of the proposed GLT is demonstrated by the experimental results (Rank-1 accuracy) for Market1501$\\to$DukeMTMC (82.0\\%) and DukeMTMC$\\to$Market1501 (92.2\\%), remarkably closing the gap between unsupervised and supervised performance on person re-identification.", "authors": ["Kecheng Zheng", " Wu Liu", " Lingxiao He", " Tao Mei", " Jiebo Luo", " Zheng-Jun Zha"], "pdf_url": "https://arxiv.org/abs/2103.12366", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td colspan=\"4\">Duke\\toMarket</td></tr><tr><td><p>mAP</p></td><td><p>top-1</p></td><td><p>top-5</p></td><td><p>top-10</p></td></tr><tr><th><p>Supervised learning</p></th><td><p>85.7</p></td><td><p>94.1</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th><p>Source Pretrain</p></th><td><p>38.5</p></td><td><p>65.7</p></td><td><p>79.2</p></td><td><p>84.0</p></td></tr><tr><th><p>Baseline</p></th><td><p>59.5</p></td><td><p>80.3</p></td><td><p>90.4</p></td><td><p>93.0</p></td></tr><tr><th><p>Baseline + G</p></th><td><p>63.0</p></td><td><p>84.7</p></td><td><p>91.4</p></td><td><p>93.7</p></td></tr><tr><th><p>Self-labeling [1]</p></th><td><p>40.9</p></td><td><p>66.6</p></td><td><p>80.6</p></td><td><p>86.3</p></td></tr><tr><th><p>Baseline + TMB</p></th><td><p>62.4</p></td><td><p>82.1</p></td><td><p>91.8</p></td><td><p>94.9</p></td></tr><tr><th><p>Baseline + LT</p></th><td><p>66.4</p></td><td><p>88.1</p></td><td><p>94.4</p></td><td><p>96.2</p></td></tr><tr><th><p>Baseline + TMB + LT</p></th><td><p>68.2</p></td><td><p>89.0</p></td><td><p>94.5</p></td><td><p>96.1</p></td></tr><tr><th><p>Baseline + G + TMB</p></th><td><p>65.2</p></td><td><p>84.3</p></td><td><p>94.8</p></td><td><p>96.7</p></td></tr><tr><th><p>Baseline + G + GLT</p></th><td><p>78.7</p></td><td><p>91.4</p></td><td><p>96.7</p></td><td><p>97.4</p></td></tr><tr><th><p>Baseline + G + TMB + GLT</p></th><td>79.5</td><td>92.2</td><td>96.5</td><td>97.8</td></tr></tbody></table>", "caption": "Table 2: Ablation studies for our proposed framework on individual components. TMB: target instance memory bank. G: baseline with the multi-group pseudo labels. LT: label transfer. GLT: group-aware label transfer.", "list_citation_info": ["[1] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020."]}, {"table": "<table><thead><tr><th>Loss Functions</th><th colspan=\"2\">Duke\\toMarket</th><th colspan=\"2\">Market\\toDuke</th></tr><tr><th></th><th><p>mAP</p></th><th><p>top-1</p></th><th><p>mAP</p></th><th><p>top-1</p></th></tr></thead><tbody><tr><th><p>GLT w/o TRI loss</p></th><td><p>77.3</p></td><td><p>90.9</p></td><td><p>67.1</p></td><td><p>81.2</p></td></tr><tr><th><p>GLT w/o WCL loss</p></th><td><p>77.9</p></td><td><p>90.8</p></td><td><p>66.4</p></td><td><p>80.7</p></td></tr><tr><th><p>GLT</p></th><td>79.5</td><td>92.2</td><td>69.2</td><td>82.0</td></tr></tbody></table>", "caption": "Table 3: Evaluation of the effectiveness of triplet loss and weighted contrastive loss, \u201cw/o WCL\u201d denotes to using the weighted contrastive loss [12].", "list_citation_info": ["[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020."]}]}