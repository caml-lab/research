{"title": "Excavating the potential capacity of self-supervised monocular depth estimation", "abstract": "Self-supervised methods play an increasingly important role in monocular depth estimation due to their great potential and low annotation cost. To close the gap with supervised methods, recent works take advantage of extra constraints, e.g., semantic segmentation. However, these methods will inevitably increase the burden on the model. In this paper, we show theoretical and empirical evidence that the potential capacity of self-supervised monocular depth estimation can be excavated without increasing this cost. In particular, we propose (1) a novel data augmentation approach called data grafting, which forces the model to explore more cues to infer depth besides the vertical image position, (2) an exploratory self-distillation loss, which is supervised by the self-distillation label generated by our new post-processing method - selective post-processing, and (3) the full-scale network, designed to endow the encoder with the specialization of depth estimation task and enhance the representational power of the model. Extensive experiments show that our contributions can bring significant performance improvement to the baseline with even less computational overhead, and our model, named EPCDepth, surpasses the previous state-of-the-art methods even those supervised by additional constraints.", "authors": ["Rui Peng", " Ronggang Wang", " Yawen Lai", " Luyang Tang", " Yangang Cai"], "pdf_url": "https://arxiv.org/abs/2109.12484", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>PP</td><td>Data</td><td>H\\times W</td><td>Abs Rel</td><td>Sq Rel</td><td>RMSE</td><td>RMSE log</td><td>\\delta&lt;1.25</td><td>\\delta&lt;1.25^{2}</td><td>\\delta&lt;1.25^{3}</td></tr><tr><td>Eigen et al. [7]</td><td></td><td>D</td><td>184\\times 612</td><td>0.203</td><td>1.548</td><td>6.307</td><td>0.282</td><td>0.702</td><td>0.890</td><td>0.890</td></tr><tr><td>Kuznietsov et al. [28]</td><td></td><td>DS</td><td>187\\times 621</td><td>0.113</td><td>0.741</td><td>4.621</td><td>0.189</td><td>0.862</td><td>0.960</td><td>0.986</td></tr><tr><td>Yang et al. [53]</td><td>\u2713</td><td>\\mathrm{D}^{\\dagger}S</td><td>256\\times 512</td><td>0.097</td><td>0.734</td><td>4.442</td><td>0.187</td><td>0.888</td><td>0.958</td><td>0.980</td></tr><tr><td>Luo et al. [31]</td><td></td><td>\\mathrm{D}^{\\ast}DS</td><td>192\\times 640 crop</td><td>0.094</td><td>0.626</td><td>4.252</td><td>0.177</td><td>0.891</td><td>0.965</td><td>0.984</td></tr><tr><td>Fu et al. [8]</td><td></td><td>D</td><td>385\\times 513 crop</td><td>0.099</td><td>0.593</td><td>3.714</td><td>0.161</td><td>0.897</td><td>0.966</td><td>0.986</td></tr><tr><td>Lee et al. [30]</td><td></td><td>D</td><td>352\\times 1216</td><td>0.091</td><td>0.555</td><td>4.033</td><td>0.174</td><td>0.904</td><td>0.967</td><td>0.984</td></tr><tr><td>Zhan et al. [62]</td><td></td><td>MS</td><td>160\\times 608</td><td>0.135</td><td>1.132</td><td>5.585</td><td>0.229</td><td>0.820</td><td>0.933</td><td>0.971</td></tr><tr><td>Godard et al. [13]</td><td>\u2713</td><td>MS</td><td>320\\times 1024</td><td>0.104</td><td>0.775</td><td>4.562</td><td>0.191</td><td>0.878</td><td>0.959</td><td>0.981</td></tr><tr><td>Watson et al. [49]</td><td>\u2713</td><td>MS</td><td>320\\times 1024</td><td>0.098</td><td>0.702</td><td>4.398</td><td>0.183</td><td>0.887</td><td>0.963</td><td>0.983</td></tr><tr><td>Shu et al. [42]</td><td></td><td>MS</td><td>320\\times 1024</td><td>0.099</td><td>0.697</td><td>4.427</td><td>0.184</td><td>0.889</td><td>0.963</td><td>0.982</td></tr><tr><td>Lyu et al. [32]</td><td></td><td>MS</td><td>320\\times 1024</td><td>0.101</td><td>0.716</td><td>4.395</td><td>0.179</td><td>0.899</td><td>0.966</td><td>0.983</td></tr><tr><td>Garg et al. [10]</td><td></td><td>S</td><td>188\\times 620</td><td>0.169</td><td>1.080</td><td>5.104</td><td>0.273</td><td>0.740</td><td>0.904</td><td>0.962</td></tr><tr><td>Godard et al. [12]</td><td>\u2713</td><td>S</td><td>256\\times 512</td><td>0.138</td><td>1.186</td><td>5.650</td><td>0.234</td><td>0.813</td><td>0.930</td><td>0.969</td></tr><tr><td>Wong et al. [50]</td><td></td><td>S</td><td>256\\times 512</td><td>0.133</td><td>1.126</td><td>5.515</td><td>0.231</td><td>0.826</td><td>0.934</td><td>0.969</td></tr><tr><td>Pilzer et al. [38] Teacher</td><td></td><td>S</td><td>256\\times 512</td><td>0.098</td><td>0.831</td><td>4.656</td><td>0.202</td><td>0.882</td><td>0.948</td><td>0.973</td></tr><tr><td>Chen et al. [2]</td><td>\u2713</td><td>SC</td><td>256\\times 512</td><td>0.118</td><td>0.905</td><td>5.096</td><td>0.211</td><td>0.839</td><td>0.945</td><td>0.977</td></tr><tr><td>Godard et al. [13]</td><td>\u2713</td><td>S</td><td>192\\times 640</td><td>0.108</td><td>0.842</td><td>4.891</td><td>0.207</td><td>0.866</td><td>0.949</td><td>0.976</td></tr><tr><td>Watson et al. missing [49]</td><td>\u2713</td><td>S</td><td>192\\times 640</td><td>0.106</td><td>0.780</td><td>4.695</td><td>0.193</td><td>0.875</td><td>0.958</td><td>0.980</td></tr><tr><td>Ours</td><td>\u2713</td><td>S</td><td>192\\times 640</td><td>0.099</td><td>0.754</td><td>4.490</td><td>0.183</td><td>0.888</td><td>0.963</td><td>0.982</td></tr><tr><td>Pillai et al. [37]</td><td>\u2713</td><td>S</td><td>384\\times 1024</td><td>0.112</td><td>0.875</td><td>4.958</td><td>0.207</td><td>0.852</td><td>0.947</td><td>0.977</td></tr><tr><td>Godard et al. [13]</td><td>\u2713</td><td>S</td><td>320\\times 1024</td><td>0.105</td><td>0.822</td><td>4.692</td><td>0.199</td><td>0.876</td><td>0.954</td><td>0.977</td></tr><tr><td>Watson et al. missing [49]</td><td>\u2713</td><td>S</td><td>320\\times 1024</td><td>0.099</td><td>0.723</td><td>4.445</td><td>0.187</td><td>0.886</td><td>0.962</td><td>0.981</td></tr><tr><td>Zhu et al. [68] Finetuned</td><td>\u2713</td><td>S\\mathrm{C}^{\\dagger}</td><td>320\\times 1024</td><td>0.097</td><td>0.675</td><td>4.350</td><td>0.180</td><td>0.890</td><td>0.964</td><td>0.983</td></tr><tr><td>Ours</td><td>\u2713</td><td>S</td><td>320\\times 1024</td><td>0.093</td><td>0.671</td><td>4.297</td><td>0.178</td><td>0.899</td><td>0.965</td><td>0.983</td></tr><tr><td>Watson et al. missing [49] ResNet50</td><td>\u2713</td><td>S</td><td>320\\times 1024</td><td>0.096</td><td>0.710</td><td>4.393</td><td>0.185</td><td>0.890</td><td>0.962</td><td>0.981</td></tr><tr><td>Zhu et al. [68] Finetuned ResNet50</td><td>\u2713</td><td>S\\mathrm{C}^{\\dagger}</td><td>320\\times 1024</td><td>0.091</td><td>0.646</td><td>4.244</td><td>0.177</td><td>0.898</td><td>0.966</td><td>0.983</td></tr><tr><td>Ours ResNet50</td><td>\u2713</td><td>S</td><td>320\\times 1024</td><td>0.091</td><td>0.646</td><td>4.207</td><td>0.176</td><td>0.901</td><td>0.966</td><td>0.983</td></tr></tbody></table>", "caption": "Table 1: Quantitative results on the KITTI dataset [11] using thesplit of Eigen et al. [6]. Best results in each category are in bold.For red metrics, lower is better. And higer is better forblue metrics. Abbreviation in Data column: D refers to methodsthat are supervised by the ground truth depth, \\mathrm{D}^{\\dagger} use auxiliary depthsupervision from SLAM, \\mathrm{D}^{\\ast} use auxiliary depth supervision from syntheticdepth labels, C for supervision from segmentation labels, \\mathrm{C}^{\\dagger} forsupervision from predicted segmentation labels, S refers to the supervision from stereoimages and M for models trained by monocular video. PP represents post-processing[12]. The underlined model is our baseline. We annotate all themethods that use extra tricks, e.g., fine-tuning and teacher model.", "list_citation_info": ["[6] David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In ICCV, pages 2650\u20132658, 2015.", "[2] Po Yi Chen, Alexander H. Liu, Yen Cheng Liu, and Yu Chiang Frank Wang. Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation. In CVPR, pages 2624\u20132632, 2019.", "[8] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In CVPR, pages 2002\u20132011, 2018.", "[42] Chang Shu, Kun Yu, Zhixiang Duan, and Kuiyuan Yang. Feature-metric loss for self-supervised learning of depth and egomotion. In ECCV, pages 572\u2013588, 2020.", "[38] Andrea Pilzer, Stephane Lathuiliere, Nicu Sebe, and Elisa Ricci. Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation. In CVPR, pages 9768\u20139777, 2019.", "[10] Ravi Garg, B. G. Vijay Kumar, Gustavo Carneiro, and Ian Reid. Unsupervised CNN for single view depth estimation: Geometry to the rescue. In ECCV, pages 740\u2013756, 2016.", "[7] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NeurIPS, pages 2366\u20132374, 2014.", "[68] Shengjie Zhu, Garrick Brazil, and Xiaoming Liu. The edge of depth: Explicit constraints between segmentation and depth. In CVPR, pages 13116\u201313125, 2020.", "[31] Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun, Hongsheng Li, and Liang Lin. Single view stereo matching. In CVPR, pages 155\u2013163, 2018.", "[53] Nan Yang, Rui Wang, J\u00f6rg St\u00fcckler, and Daniel Cremers. Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry. In ECCV, pages 817\u2013833, 2018.", "[50] Alex Wong and Stefano Soatto. Bilateral cyclic constraint and adaptive regularization for unsupervised monocular depth prediction. In CVPR, pages 5644\u20135653, 2019.", "[49] Jamie Watson, Michael Firman, Gabriel J. Brostow, and Daniyar Turmukhambetov. Self-supervised monocular depth hints. In ICCV, pages 2162\u20132171, 2019.", "[13] Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into self-supervised monocular depth estimation. In ICCV, pages 3828\u20133838, 2019.", "[32] Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, Lina Liu, Yong Liu, Xinxin Chen, and Yi Yuan. HR-Depth: High resolution self-supervised monocular depth estimation. In AAAI, pages 2294\u20132301, 2021.", "[28] Yevhen Kuznietsov, J\u00f6rg St\u00fcckler, and Bastian Leibe. Semi-supervised deep learning for monocular depth map prediction. In CVPR, pages 6647\u20136655, 2017.", "[30] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In CVPR, pages 3354\u20133361, 2012.", "[12] Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with left-right consistency. In CVPR, pages 270\u2013279, 2017.", "[62] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera, Kejie Li, Harsh Agarwal, and Ian M. Reid. Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. In CVPR, pages 340\u2013349, 2018.", "[37] Sudeep Pillai, Rare\u015f Ambru\u015f, and Adrien Gaidon. SuperDepth: Self-supervised, super-resolved monocular depth estimation. In ICRA, pages 9250\u20139256, 2019."]}, {"table": "<table><thead><tr><th>Augmentation</th><th>Abs Rel</th><th>Sq Rel</th><th>RMSE</th><th>\\delta&lt;1.25</th></tr></thead><tbody><tr><th>RandErasing [65]</th><td>0.115</td><td>0.992</td><td>4.987</td><td>0.858</td></tr><tr><th>Cutout [5]</th><td>0.106</td><td>0.830</td><td>4.753</td><td>0.874</td></tr><tr><th>CutMix [59]</th><td>0.105</td><td>0.831</td><td>4.752</td><td>0.876</td></tr><tr><th>DataGrafting</th><td>0.102</td><td>0.782</td><td>4.581</td><td>0.883</td></tr></tbody></table>", "caption": "Table 3: Comparison against other similar augmentation methods. And the input size is 192\\times 640.", "list_citation_info": ["[65] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, pages 13001\u201313008, 2020.", "[5] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.", "[59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. CutMix: Regularization strategy to train strong classifiers with localizable features. In ICCV, pages 6023\u20136032, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>Abs Rel</th><th>Sq Rel</th><th>RMSE</th><th>RMSE log</th><th>\\delta&lt;1.25</th><th>\\delta&lt;1.25^{2}</th><th>\\delta&lt;1.25^{3}</th></tr></thead><tbody><tr><th>Monodepth2 [13]</th><td>0.355</td><td>0.673</td><td>1.252</td><td>0.373</td><td>0.485</td><td>0.771</td><td>0.907</td></tr><tr><th>DepthHint [49]</th><td>0.298</td><td>0.457</td><td>1.043</td><td>0.331</td><td>0.539</td><td>0.821</td><td>0.937</td></tr><tr><th>EdgeDepth [68]</th><td>0.292</td><td>0.437</td><td>1.018</td><td>0.319</td><td>0.563</td><td>0.834</td><td>0.941</td></tr><tr><th>Ours (EPCDepth)</th><td>0.247</td><td>0.277</td><td>0.818</td><td>0.285</td><td>0.605</td><td>0.869</td><td>0.961</td></tr></tbody></table>", "caption": "Table 8: Quantitative results on the NYU-Depth-v2 dataset.", "list_citation_info": ["[68] Shengjie Zhu, Garrick Brazil, and Xiaoming Liu. The edge of depth: Explicit constraints between segmentation and depth. In CVPR, pages 13116\u201313125, 2020.", "[13] Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into self-supervised monocular depth estimation. In ICCV, pages 3828\u20133838, 2019.", "[49] Jamie Watson, Michael Firman, Gabriel J. Brostow, and Daniyar Turmukhambetov. Self-supervised monocular depth hints. In ICCV, pages 2162\u20132171, 2019."]}]}