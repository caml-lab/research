{"title": "Improving 3D Object Detection with Channel-wise Transformer", "abstract": "Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Previous works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set abstraction and multi-scale feature fusion to produce powerful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal's keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors.", "authors": ["Hualian Sheng", " Sijia Cai", " Yuan Liu", " Bing Deng", " Jianqiang Huang", " Xian-Sheng Hua", " Min-Jian Zhao"], "pdf_url": "https://arxiv.org/abs/2108.10723", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th>Par.</th><td colspan=\"3\">3D Detection - Car</td></tr><tr><th>(M)</th><td>Easy</td><td>Mod.</td><td>Hard</td></tr><tr><th colspan=\"5\">LiDAR &amp; RGB</th></tr><tr><th>MV3D, CVPR 2017 [3]</th><th>-</th><td>74.97</td><td>63.63</td><td>54.00</td></tr><tr><th>ContFuse, ECCV 2018 [17]</th><th>-</th><td>83.68</td><td>68.78</td><td>61.67</td></tr><tr><th>AVOD-FPN, IROS 2018 [12]</th><th>-</th><td>83.07</td><td>71.76</td><td>65.73</td></tr><tr><th>F-PointNet, CVPR 2018 [21]</th><th>40</th><td>82.19</td><td>69.79</td><td>60.59</td></tr><tr><th>UberATG-MMF, CVPR 2019 [16]</th><th>-</th><td>88.40</td><td>77.43</td><td>70.22</td></tr><tr><th>3D-CVF at SPA, ECCV 2020 [38]</th><th>-</th><td>89.20</td><td>80.05</td><td>73.11</td></tr><tr><th>CLOCs, IROS 2020 [20]</th><th>-</th><td>88.94</td><td>80.67</td><td>77.15</td></tr><tr><th colspan=\"5\">LiDAR only</th></tr><tr><th>SECOND, Sensor 2018 [33]</th><th>20</th><td>83.34</td><td>72.55</td><td>65.82</td></tr><tr><th>PointPillars, CVPR 2019 [13]</th><th>18</th><td>82.58</td><td>74.31</td><td>68.99</td></tr><tr><th>STD, ICCV 2019 [37]</th><th>-</th><td>87.95</td><td>79.71</td><td>75.09</td></tr><tr><th>PointRCNN, CVPR 2019 [25]</th><th>16</th><td>86.96</td><td>75.64</td><td>70.70</td></tr><tr><th>3D IoU Loss, 3DV 2019 [40]</th><th>-</th><td>86.16</td><td>76.50</td><td>71.39</td></tr><tr><th>Part-A^{2}, PAMI 2020 [26]</th><th>226</th><td>87.81</td><td>78.49</td><td>73.51</td></tr><tr><th>SA-SSD, CVPR 2020 [10]</th><th>40.8</th><td>88.75</td><td>79.79</td><td>74.16</td></tr><tr><th>3DSSD, CVPR 2020 [36]</th><th>-</th><td>88.36</td><td>79.57</td><td>74.55</td></tr><tr><th>PV-RCNN, CVPR 2020 [24]</th><th>50</th><td>90.25</td><td>81.43</td><td>76.82</td></tr><tr><th>Voxel-RCNN, AAAI 2021 [4]</th><th>28</th><td>90.90</td><td>81.62</td><td>77.06</td></tr><tr><th>CT3D (Ours)</th><th>30</th><td>87.83</td><td>81.77</td><td>77.16</td></tr></tbody></table>", "caption": "Table 1: Performance comparisons with state-of-the-art methods on the KITTI test set. All results are reported by the average precision with 0.7 IoU threshold and 40 recall positions.", "list_citation_info": ["[20] Su Pang, Daniel Morris, and Hayder Radha. Clocs: Camera-lidar object candidates fusion for 3d object detection. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.", "[25] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013779, 2019.", "[36] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage object detector. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11040\u201311048, 2020.", "[16] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urtasun. Multi-task multi-sensor fusion for 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 7345\u20137353, 2019.", "[13] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12697\u201312705, 2019.", "[4] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. arXiv preprint arXiv:2012.15712, 2020.", "[10] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detection from point cloud. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11873\u201311882, 2020.", "[26] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020.", "[40] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo Yin, Yuchao Dai, and Ruigang Yang. Iou loss for 2d/3d object detection. In 2019 International Conference on 3D Vision (3DV), pages 85\u201394, 2019.", "[21] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object detection from rgb-d data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 918\u2013927, 2018.", "[12] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander. Joint 3d proposal generation and object detection from view aggregation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1\u20138, 2018.", "[24] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10529\u201310538, 2020.", "[3] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1907\u20131915, 2017.", "[17] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d object detection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 641\u2013656, 2018.", "[38] Jin Hyeok Yoo, Yecheol Kim, Ji Song Kim, and Jun Won Choi. 3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection. arXiv preprint arXiv:2004.12636, 3, 2020.", "[33] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.", "[37] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Std: Sparse-to-dense 3d object detector for point cloud. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1951\u20131960, 2019."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">3D Detection - Car</td></tr><tr><td>Easy</td><td>Mod.</td><td>Hard</td></tr><tr><th colspan=\"4\">LiDAR &amp; RGB</th></tr><tr><th>MV3D, CVPR 2017 [3]</th><td>71.29</td><td>62.68</td><td>56.56</td></tr><tr><th>ContFuse, ECCV 2018 [17]</th><td>-</td><td>73.25</td><td>-</td></tr><tr><th>AVOD-FPN, IROS 2018 [12]</th><td>-</td><td>74.44</td><td>-</td></tr><tr><th>F-PointNet, CVPR 2018 [21]</th><td>83.76</td><td>70.92</td><td>63.65</td></tr><tr><th>3D-CVF at SPA, ECCV 2020 [38]</th><td>89.67</td><td>79.88</td><td>78.47</td></tr><tr><th colspan=\"4\">LiDAR only</th></tr><tr><th>SECOND, Sensor 2018 [33]</th><td>88.61</td><td>78.62</td><td>77.22</td></tr><tr><th>PointPillars, CVPR 2019 [13]</th><td>86.62</td><td>76.06</td><td>68.91</td></tr><tr><th>STD, ICCV 2019 [37]</th><td>89.70</td><td>79.80</td><td>79.30</td></tr><tr><th>PointRCNN, CVPR 2019 [25]</th><td>88.88</td><td>78.63</td><td>77.38</td></tr><tr><th>SA-SSD, CVPR 2020 [10]</th><td>90.15</td><td>79.91</td><td>78.78</td></tr><tr><th>3DSSD, CVPR 2020 [36]</th><td>89.71</td><td>79.45</td><td>78.67</td></tr><tr><th>PV-RCNN, CVPR 2020 [24]</th><td>89.35</td><td>83.69</td><td>78.70</td></tr><tr><th>Voxel-RCNN, AAAI 2021 [4]</th><td>89.41</td><td>84.52</td><td>78.93</td></tr><tr><th>CT3D (Ours)</th><td>89.54</td><td>86.06</td><td>78.99</td></tr></tbody></table>", "caption": "Table 2: Performance comparisons with state-of-the-art methods on the KITTI val set. All results are reported by the average precision with 0.7 IoU threshold and 11 recall positions.", "list_citation_info": ["[36] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage object detector. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11040\u201311048, 2020.", "[25] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013779, 2019.", "[13] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12697\u201312705, 2019.", "[4] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. arXiv preprint arXiv:2012.15712, 2020.", "[10] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detection from point cloud. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11873\u201311882, 2020.", "[21] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object detection from rgb-d data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 918\u2013927, 2018.", "[12] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander. Joint 3d proposal generation and object detection from view aggregation. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1\u20138, 2018.", "[24] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10529\u201310538, 2020.", "[3] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1907\u20131915, 2017.", "[17] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d object detection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 641\u2013656, 2018.", "[38] Jin Hyeok Yoo, Yecheol Kim, Ji Song Kim, and Jun Won Choi. 3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection. arXiv preprint arXiv:2004.12636, 3, 2020.", "[33] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.", "[37] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Std: Sparse-to-dense 3d object detector for point cloud. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1951\u20131960, 2019."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Difficulty</th><th rowspan=\"2\">Method</th><td colspan=\"4\">3D Detection - Vehicle</td><td colspan=\"4\">BEV Detection - Vehicle</td></tr><tr><td>Overall</td><td>0-30m</td><td>30-50m</td><td>50m-Inf</td><td>Overall</td><td>0-30m</td><td>30-50m</td><td>50m-Inf</td></tr><tr><th rowspan=\"6\">LEVEL_1</th><th>PointPillar, CVPR 2019 [13]</th><td>56.62</td><td>81.01</td><td>51.75</td><td>27.94</td><td>75.57</td><td>92.10</td><td>74.06</td><td>55.47</td></tr><tr><th>MVF, CoRL 2020 [42]</th><td>62.93</td><td>86.30</td><td>60.02</td><td>36.02</td><td>80.40</td><td>93.59</td><td>79.21</td><td>63.09</td></tr><tr><th>Pillar-OD, arXiv 2020 [32]</th><td>69.80</td><td>88.53</td><td>66.50</td><td>42.93</td><td>87.11</td><td>95.78</td><td>84.87</td><td>72.12</td></tr><tr><th>PV-RCNN, CVPR 2020 [24]</th><td>70.30</td><td>91.92</td><td>69.21</td><td>42.17</td><td>82.96</td><td>97.35</td><td>82.99</td><td>64.97</td></tr><tr><th>Voxel-RCNN, AAAI 2021 [4]</th><td>75.59</td><td>92.49</td><td>74.09</td><td>53.15</td><td>88.19</td><td>97.62</td><td>87.34</td><td>77.70</td></tr><tr><th>CT3D (Ours)</th><td>76.30</td><td>92.51</td><td>75.07</td><td>55.36</td><td>90.50</td><td>97.64</td><td>88.06</td><td>78.89</td></tr><tr><th rowspan=\"3\">LEVEL_2</th><th>PV-RCNN, CVPR 2020 [24]</th><td>65.36</td><td>91.58</td><td>65.13</td><td>36.46</td><td>77.45</td><td>94.64</td><td>80.39</td><td>55.39</td></tr><tr><th>Voxel-RCNN, AAAI 2021 [4]</th><td>66.59</td><td>91.74</td><td>67.89</td><td>40.80</td><td>81.07</td><td>96.99</td><td>81.37</td><td>63.26</td></tr><tr><th>CT3D (Ours)</th><td>69.04</td><td>91.76</td><td>68.93</td><td>42.60</td><td>81.74</td><td>97.05</td><td>82.22</td><td>64.34</td></tr></tbody></table>", "caption": "Table 5: Performance comparisons with state-of-the-art methods on the Waymo dataset with 202 validation sequences (\\sim 40k samples) for vehicle detection.", "list_citation_info": ["[4] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. arXiv preprint arXiv:2012.15712, 2020.", "[13] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 12697\u201312705, 2019.", "[32] Yue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Caroline Pantofaru, Tom Funkhouser, and Justin Solomon. Pillar-based object detection for autonomous driving. arXiv preprint arXiv:2007.10323, 2020.", "[42] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Vasudevan. End-to-end multi-view fusion for 3d object detection in lidar point clouds. In Conference on Robot Learning, pages 923\u2013932, 2020.", "[24] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 10529\u201310538, 2020."]}]}