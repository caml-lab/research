{"title": "Split and Connect: A Universal Tracklet Booster for Multi-Object Tracking", "abstract": "Multi-object tracking (MOT) is an essential task in the computer vision field. With the fast development of deep learning technology in recent years, MOT has achieved great improvement. However, some challenges still remain, such as sensitiveness to occlusion, instability under different lighting conditions, non-robustness to deformable objects, etc. To address such common challenges in most of the existing trackers, in this paper, a tracklet booster algorithm is proposed, which can be built upon any other tracker. The motivation is simple and straightforward: split tracklets on potential ID-switch positions and then connect multiple tracklets into one if they are from the same object. In other words, the tracklet booster consists of two parts, i.e., Splitter and Connector. First, an architecture with stacked temporal dilated convolution blocks is employed for the splitting position prediction via label smoothing strategy with adaptive Gaussian kernels. Then, a multi-head self-attention based encoder is exploited for the tracklet embedding, which is further used to connect tracklets into larger groups. We conduct sufficient experiments on MOT17 and MOT20 benchmark datasets, which demonstrates promising results. Combined with the proposed tracklet booster, existing trackers usually can achieve large improvements on the IDF1 score, which shows the effectiveness of the proposed method.", "authors": ["Gaoang Wang", " Yizhou Wang", " Renshu Gu", " Weijie Hu", " Jenq-Neng Hwang"], "pdf_url": "https://arxiv.org/abs/2105.02426", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method (MOT17)</th><th>IDF1 (%) \\uparrow</th><th>MOTA (%) \\uparrow</th><th>MOTP (%) \\uparrow</th><th>MT (%) \\uparrow</th><th>ML (%) \\downarrow</th><th>IDS \\downarrow</th><th>FRAG \\downarrow</th><th>FPS \\uparrow</th></tr></thead><tbody><tr><td>IOU [63]</td><td>39.4</td><td>45.5</td><td>76.9</td><td>369</td><td>953</td><td>5,988</td><td>7,404</td><td>1,522.9</td></tr><tr><td>TBooster+IOU</td><td>45.1 (+5.7)</td><td>45.8</td><td>76.8</td><td>369</td><td>953</td><td>4,189</td><td>7,430</td><td>7.7</td></tr><tr><td>Tracktor_v2 [27]</td><td>55.1</td><td>56.3</td><td>78.8</td><td>498</td><td>831</td><td>1,987</td><td>3,763</td><td>1.5</td></tr><tr><td>TBooster+Tracktor_v2</td><td>59.2 (+4.1)</td><td>56.4</td><td>78.9</td><td>498</td><td>831</td><td>1,785</td><td>3,750</td><td>1.3</td></tr><tr><td>MPNTrack [21]</td><td>61.7</td><td>58.8</td><td>78.6</td><td>679</td><td>788</td><td>1,185</td><td>2,265</td><td>6.5</td></tr><tr><td>TBooster+MPNTrack</td><td>62.3 (+0.6)</td><td>58.9</td><td>78.7</td><td>682</td><td>790</td><td>1,198</td><td>2,209</td><td>4.2</td></tr><tr><td>CenterTrack [24]</td><td>59.6</td><td>61.5</td><td>78.9</td><td>621</td><td>752</td><td>2,583</td><td>4,965</td><td>17.0</td></tr><tr><td>TBooster+CenterTrack</td><td>63.3 (+3.7)</td><td>61.5</td><td>78.8</td><td>622</td><td>754</td><td>2,470</td><td>5,079</td><td>6.9</td></tr></tbody></table>", "caption": "TABLE I: Result on MOT17 testing set. For each pair of comparisons, the first row shows the original state-of-the-art (SOTA) method and the second row shows the corresponding method with the proposed TBooster (short for \u201ctracklet booster\u201d). The number in \u201c()\u201d besides the IDF1 score is the improvement after TBooster applied.", "list_citation_info": ["[63] E. Bochinski, V. Eiselein, and T. Sikora, \u201cHigh-speed tracking-by-detection without using image information,\u201d in International Workshop on Traffic and Street Surveillance for Safety and Security at IEEE AVSS 2017, Lecce, Italy, Aug. 2017. [Online]. Available: http://elvera.nue.tu-berlin.de/files/1517Bochinski2017.pdf", "[21] G. Bras\u00f3 and L. Leal-Taix\u00e9, \u201cLearning a neural solver for multiple object tracking,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 6247\u20136257.", "[24] X. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl, \u201cTracking objects as points,\u201d arXiv preprint arXiv:2004.01177, 2020.", "[27] P. Bergmann, T. Meinhardt, and L. Leal-Taix\u00e9, \u201cTracking without bells and whistles,\u201d in The IEEE International Conference on Computer Vision (ICCV), October 2019."]}, {"table": "<table><thead><tr><th>Method (MOT20)</th><th>IDF1 (%) \\uparrow</th><th>MOTA (%) \\uparrow</th><th>MOTP (%) \\uparrow</th><th>MT (%) \\uparrow</th><th>ML (%) \\downarrow</th><th>IDS \\downarrow</th><th>FRAG \\downarrow</th><th>FPS \\uparrow</th></tr></thead><tbody><tr><td>Tracktor [27]</td><td>52.7</td><td>52.6</td><td>79.9</td><td>365</td><td>331</td><td>1,648</td><td>4,374</td><td>1.2</td></tr><tr><td>TBooster+Tracktor</td><td>53.3 (+0.6)</td><td>52.6</td><td>79.8</td><td>365</td><td>329</td><td>1,734</td><td>4,389</td><td>0.6</td></tr><tr><td>UnsupTrack [64]</td><td>50.6</td><td>53.6</td><td>80.1</td><td>376</td><td>311</td><td>2,178</td><td>4,335</td><td>1.3</td></tr><tr><td>TBooster+UnsupTrack</td><td>54.3 (+3.7)</td><td>53.7</td><td>80.1</td><td>374</td><td>313</td><td>1,771</td><td>4,322</td><td>0.6</td></tr><tr><td>MOT20_TBC [65]</td><td>50.1</td><td>54.5</td><td>77.3</td><td>415</td><td>245</td><td>2,449</td><td>2,580</td><td>5.6</td></tr><tr><td>TBooster+MOT20_TBC</td><td>54.3 (+4.2)</td><td>54.6</td><td>77.3</td><td>416</td><td>247</td><td>1,771</td><td>2,679</td><td>0.8</td></tr><tr><td>GNNMatch [66]</td><td>49.0</td><td>54.5</td><td>79.4</td><td>407</td><td>317</td><td>2,038</td><td>2,456</td><td>0.1</td></tr><tr><td>TBooster+GNNMatch</td><td>53.4 (+4.4)</td><td>54.6</td><td>79.4</td><td>407</td><td>317</td><td>1.674</td><td>2,455</td><td>0.1</td></tr></tbody></table>", "caption": "TABLE II: Result on MOT20 testing set. For each pair of comparisons, the first row shows the original state-of-the-art (SOTA) method and the second row shows the corresponding method with the proposed TBooster (short for \u201ctracklet booster\u201d). The number in \u201c()\u201d besides the IDF1 score is the improvement after TBooster applied.", "list_citation_info": ["[66] I. Papakis, A. Sarkar, and A. Karpatne, \u201cGcnnmatch: Graph convolutional neural networks for multi-object tracking via sinkhorn normalization,\u201d arXiv preprint arXiv:2010.00067, 2020.", "[65] W. Ren, X. Wang, J. Tian, Y. Tang, and A. B. Chan, \u201cTracking-by-counting: Using network flows on crowd density maps for tracking multiple targets,\u201d IEEE Transactions on Image Processing, vol. 30, pp. 1439\u20131452, 2020.", "[27] P. Bergmann, T. Meinhardt, and L. Leal-Taix\u00e9, \u201cTracking without bells and whistles,\u201d in The IEEE International Conference on Computer Vision (ICCV), October 2019.", "[64] S. Karthik, A. Prabhu, and V. Gandhi, \u201cSimple unsupervised multi-object tracking,\u201d arXiv preprint arXiv:2006.02609, 2020."]}]}