{"title": "Clip4clip: An empirical study of clip for end to end video clip retrieval", "abstract": "Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. The CLIP (Contrastive Language-Image Pre-training), an image-language pre-training model, has demonstrated the power of visual concepts learning from web collected image-text datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner. Several questions are investigated via empirical studies: 1) Whether image feature is enough for video-text retrieval? 2) How a post-pretraining on a large-scale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyper-parameters sensitivity of the model on video-text retrieval task. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various video-text retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We release our code at https://github.com/ArrowLuo/CLIP4Clip.", "authors": ["Huaishao Luo", " Lei Ji", " Ming Zhong", " Yang Chen", " Wen Lei", " Nan Duan", " Tianrui Li"], "pdf_url": "https://arxiv.org/abs/2104.08860", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Methods</td><td>TrainD</td><td>E2E</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td><td>MnR\\downarrow</td></tr><tr><td>Multi Cues{}^{a}</td><td>M</td><td>\u2713</td><td>20.3</td><td>47.8</td><td>61.1</td><td>6</td><td>-</td></tr><tr><td>CE{}^{b}</td><td>M</td><td></td><td>19.8</td><td>49.0</td><td>63.8</td><td>6</td><td>-</td></tr><tr><td>SSB{}^{c}</td><td>H+M</td><td></td><td>28.4</td><td>60.0</td><td>72.9</td><td>4</td><td>-</td></tr><tr><td>NoiseE{}^{d}</td><td>H+M</td><td></td><td>20.3</td><td>49.0</td><td>63.3</td><td>6</td><td>-</td></tr><tr><td>CLIP-straight{}^{e}</td><td>W</td><td>\u2713</td><td>37.0</td><td>64.1</td><td>73.8</td><td>3</td><td>-</td></tr><tr><td>Frozen{}^{f}</td><td>CW+M</td><td>\u2713</td><td>33.7</td><td>64.7</td><td>76.3</td><td>3</td><td>-</td></tr><tr><td>TT-CE+{}^{g}</td><td>M</td><td></td><td>25.4</td><td>56.9</td><td>71.3</td><td>4</td><td>-</td></tr><tr><td>(Ours)-meanP</td><td>W+M</td><td>\u2713</td><td>46.2</td><td>76.1</td><td>84.6</td><td>2</td><td>10.0</td></tr><tr><td>(Ours)-seqLSTM</td><td>W+M</td><td>\u2713</td><td>46.2</td><td>75.3</td><td>84.5</td><td>2</td><td>10.2</td></tr><tr><td>(Ours)-seqTransf</td><td>W+M</td><td>\u2713</td><td>45.2</td><td>75.5</td><td>84.3</td><td>2</td><td>10.3</td></tr><tr><td>(Ours)-tightTransf</td><td>W+M</td><td>\u2713</td><td>40.0</td><td>71.5</td><td>82.1</td><td>2</td><td>13.3</td></tr></tbody></table>", "caption": "Table 2: Results of text-to-video retrieval on MSVD dataset. In the column \u2018TrainD\u2019, M, H, and W denote training on MSVD, HowTo100M Miech et al. (2019), and WIT Radford et al. (2021), and CW means CC3M Sharma et al. (2018) plus WebVid-2M Bain et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline methods are {}^{a}Multi Cues Mithun et al. (2018), {}^{b}CE Liu et al. (2019), {}^{c}SSB Patrick et al. (2021), {}^{d}NoiseE Amrani et al. (2021), {}^{e}CLIP-straight Portillo-Quintero et al. (2021), {}^{f}Frozen Bain et al. (2021), {}^{g}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Liu et al. (2019) Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. ICCV.", "Portillo-Quintero et al. (2021) Jes\u00fas Andr\u00e9s Portillo-Quintero, Jos\u00e9 Carlos Ortiz-Bayliss, and Hugo Terashima-Mar\u00edn. 2021. A straightforward framework for video retrieval using clip. arXiv preprint arXiv:2102.12443.", "Amrani et al. (2021) Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. 2021. Noise estimation using density estimation for self-supervised multimodal learning. In AAAI.", "Patrick et al. (2021) Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In ICLR.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Mithun et al. (2018) Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K Roy-Chowdhury. 2018. Learning joint embedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, pages 19\u201327."]}, {"table": "<table><tbody><tr><th>Methods</th><td>TrainD</td><td>E2E</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td><td>MnR\\downarrow</td></tr><tr><th>CT-SAN{}^{a}</th><td>L</td><td>\u2713</td><td>5.1</td><td>16.3</td><td>25.2</td><td>46.0</td><td>-</td></tr><tr><th>JSFusion{}^{b}</th><td>L</td><td>\u2713</td><td>9.1</td><td>21.2</td><td>34.1</td><td>36.0</td><td>-</td></tr><tr><th>CE{}^{c}</th><td>L</td><td></td><td>11.2</td><td>26.9</td><td>34.8</td><td>25.3</td><td>96.8</td></tr><tr><th>MMT{}^{d}</th><td>H+L</td><td></td><td>12.9</td><td>29.9</td><td>40.1</td><td>19.3</td><td>75.0</td></tr><tr><th>NoiseE{}^{e}</th><td>H+L</td><td></td><td>6.4</td><td>19.8</td><td>28.4</td><td>39.0</td><td>-</td></tr><tr><th>CLIP-straight{}^{f}</th><td>L</td><td>\u2713</td><td>11.3</td><td>22.7</td><td>29.2</td><td>56.5</td><td>-</td></tr><tr><th>MDMMT{}^{g}</th><td>MD+L</td><td></td><td>18.8</td><td>38.5</td><td>47.9</td><td>12.3</td><td>58.0</td></tr><tr><th>Frozen{}^{h}</th><td>CW+L</td><td>\u2713</td><td>15.0</td><td>30.8</td><td>39.8</td><td>20.0</td><td>-</td></tr><tr><th>HiT{}^{i}</th><td>H+L</td><td></td><td>14.0</td><td>31.2</td><td>41.6</td><td>18.5</td><td>-</td></tr><tr><th>TT-CE+{}^{j}</th><td>L</td><td></td><td>17.2</td><td>36.5</td><td>46.3</td><td>13.7</td><td>-</td></tr><tr><th>(Ours)-meanP</th><td>W+L</td><td>\u2713</td><td>20.7</td><td>38.9</td><td>47.2</td><td>13.0</td><td>65.3</td></tr><tr><th>(Ours)-seqLSTM</th><td>W+L</td><td>\u2713</td><td>21.6</td><td>41.8</td><td>49.8</td><td>11.0</td><td>58.0</td></tr><tr><th>(Ours)-seqTransf</th><td>W+L</td><td>\u2713</td><td>22.6</td><td>41.0</td><td>49.1</td><td>11.0</td><td>61.0</td></tr><tr><th>(Ours)-tightTransf</th><td>W+L</td><td>\u2713</td><td>18.9</td><td>37.8</td><td>46.7</td><td>13.0</td><td>61.6</td></tr></tbody></table>", "caption": "Table 3: Results of text-to-video retrieval on LSMDC dataset. In the column \u2018TrainD\u2019, L, H, and W denote training on LSMDC, HowTo100M Miech et al. (2019), and WIT Radford et al. (2021), MD used in Dzabraev et al. (2021) denotes a combined multidomain dataset containing MSR-VTT, LSMDC, HowTo100M, etc., and CW means CC3M Sharma et al. (2018) plus WebVid-2M Bain et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline methods are {}^{a}CT-SAN Yu et al. (2017), {}^{b}JSFusion Yu et al. (2018), {}^{c}CE Liu et al. (2019), {}^{d}MMT Gabeur et al. (2020), {}^{e}NoiseE Amrani et al. (2021), {}^{f}CLIP-straight Portillo-Quintero et al. (2021), {}^{g}MDMMT Dzabraev et al. (2021), {}^{h}Frozen Bain et al. (2021), {}^{i}HiT Liu et al. (2021), {}^{j}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Liu et al. (2019) Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. ICCV.", "Portillo-Quintero et al. (2021) Jes\u00fas Andr\u00e9s Portillo-Quintero, Jos\u00e9 Carlos Ortiz-Bayliss, and Hugo Terashima-Mar\u00edn. 2021. A straightforward framework for video retrieval using clip. arXiv preprint arXiv:2102.12443.", "Gabeur et al. (2020) Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval. In ECCV, volume 5.", "Amrani et al. (2021) Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. 2021. Noise estimation using density estimation for self-supervised multimodal learning. In AAAI.", "Yu et al. (2018) Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In ECCV, pages 487\u2013503.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL.", "Liu et al. (2021) Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. arXiv preprint arXiv:2103.15049.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Dzabraev et al. (2021) Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. 2021. Mdmmt: Multidomain multimodal transformer for video retrieval. arXiv preprint arXiv:2103.10699.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Yu et al. (2017) Youngjae Yu, Hyungjin Ko, Jongwook Choi, and Gunhee Kim. 2017. End-to-end concept word detection for video captioning, retrieval, and question answering. In CVPR, pages 3261\u20133269."]}, {"table": "<table><tbody><tr><th>Methods</th><td>TrainD</td><td>E2E</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@50\\uparrow</td><td>MdR\\downarrow</td><td>MnR\\downarrow</td></tr><tr><th>FSE{}^{a}</th><td>A</td><td></td><td>18.2</td><td>44.8</td><td>89.1</td><td>7.0</td><td>-</td></tr><tr><th>CE{}^{b}</th><td>A</td><td></td><td>18.2</td><td>47.7</td><td>91.4</td><td>6.0</td><td>23.1</td></tr><tr><th>HSE{}^{a}</th><td>A</td><td></td><td>20.5</td><td>49.3</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MMT{}^{c}</th><td>H+A</td><td></td><td>28.7</td><td>61.4</td><td>94.5</td><td>3.3</td><td>16.0</td></tr><tr><th>SSB{}^{d}</th><td>H+A</td><td></td><td>29.2</td><td>61.6</td><td>94.7</td><td>3.0</td><td>-</td></tr><tr><th>HiT{}^{e}</th><td>H+A</td><td></td><td>29.6</td><td>60.7</td><td>95.6</td><td>3.0</td><td>-</td></tr><tr><th>ClipBERT{}^{f}</th><td>C+G+A</td><td>\u2713</td><td>21.3</td><td>49.0</td><td>-</td><td>6.0</td><td>-</td></tr><tr><th>TT-CE+{}^{g}</th><td>A</td><td></td><td>23.5</td><td>57.2</td><td>96.1</td><td>4.0</td><td>-</td></tr><tr><th>(Ours)-meanP</th><td>W+A</td><td>\u2713</td><td>40.5</td><td>72.4</td><td>98.1</td><td>2.0</td><td>7.4</td></tr><tr><th>(Ours)-seqLSTM</th><td>W+A</td><td>\u2713</td><td>40.1</td><td>72.2</td><td>98.1</td><td>2.0</td><td>7.3</td></tr><tr><th>(Ours)-seqTransf</th><td>W+A</td><td>\u2713</td><td>40.5</td><td>72.4</td><td>98.2</td><td>2.0</td><td>7.5</td></tr><tr><th>(Ours)-tightTransf</th><td>W+A</td><td>\u2713</td><td>19.5</td><td>47.6</td><td>93.1</td><td>6.0</td><td>17.3</td></tr></tbody></table>", "caption": "Table 4: Results of text-to-video retrieval on ActivityNet dataset. In the column \u2018TrainD\u2019, A, H, W, C, and G denote training on ActivityNet, HowTo100M Miech et al. (2019), WIT Radford et al. (2021), COCO Captions Chen et al. (2015), and Visual Genome Captions Krishna et al. (2017b). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline methods are {}^{a}FSE,HSE Zhang et al. (2018), {}^{b}CE Liu et al. (2019), {}^{c}MMT Gabeur et al. (2020), {}^{d}SSB Patrick et al. (2021), {}^{e}HiT Liu et al. (2021), {}^{f}ClipBERT Lei et al. (2021), {}^{g}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Krishna et al. (2017b) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017b. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32\u201373.", "Liu et al. (2019) Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. ICCV.", "Gabeur et al. (2020) Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval. In ECCV, volume 5.", "Patrick et al. (2021) Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In ICLR.", "Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Liu et al. (2021) Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. arXiv preprint arXiv:2103.15049.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Zhang et al. (2018) Bowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-modal and hierarchical modeling of video and text. In ECCV, pages 385\u2013401."]}, {"table": "<table><tbody><tr><th>Methods</th><td>TrainD</td><td>E2E</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td><td>MnR\\downarrow</td></tr><tr><th>S2VT{}^{a}</th><td>D</td><td>\u2713</td><td>11.9</td><td>33.6</td><td>-</td><td>13.0</td><td>-</td></tr><tr><th>FSE{}^{b}</th><td>D</td><td></td><td>13.9</td><td>36.0</td><td>-</td><td>11.0</td><td>-</td></tr><tr><th>CE{}^{c}</th><td>D</td><td></td><td>16.1</td><td>41.1</td><td>-</td><td>8.3</td><td>43.7</td></tr><tr><th>ClipBERT{}^{d}{}^{\\dagger}</th><td>C+G+D</td><td>\u2713</td><td>20.4</td><td>48.0</td><td>60.8</td><td>6.0</td><td>-</td></tr><tr><th>Frozen{}^{e}{}^{\\dagger}</th><td>CW+D</td><td>\u2713</td><td>34.6</td><td>65.0</td><td>74.7</td><td>3.0</td><td>-</td></tr><tr><th>TT-CE+{}^{f}</th><td>D</td><td></td><td>21.6</td><td>48.6</td><td>62.9</td><td>6.0</td><td>-</td></tr><tr><th>(Ours)-meanP</th><td>W+D</td><td>\u2713</td><td>43.4</td><td>70.2</td><td>80.6</td><td>2.0</td><td>17.5</td></tr><tr><th>(Ours)-seqLSTM</th><td>W+D</td><td>\u2713</td><td>43.4</td><td>69.9</td><td>80.2</td><td>2.0</td><td>17.5</td></tr><tr><th>(Ours)-seqTransf</th><td>W+D</td><td>\u2713</td><td>42.8</td><td>68.5</td><td>79.2</td><td>2.0</td><td>18.9</td></tr><tr><th>(Ours)-tightTransf</th><td>W+D</td><td>\u2713</td><td>25.8</td><td>52.8</td><td>66.3</td><td>5.0</td><td>27.3</td></tr></tbody></table>", "caption": "Table 5: Results of text-to-video retrieval on DiDeMo dataset. In the column \u2018TrainD\u2019, D, H, W, C, and G denote training on DiDeMo, HowTo100M Miech et al. (2019), WIT Radford et al. (2021), COCO Captions Chen et al. (2015), and Visual Genome Captions Krishna et al. (2017b), CW means CC3M Sharma et al. (2018) plus WebVid-2M Bain et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. \\dagger means that the candidate video is concatenated using ground truth proposals. The baseline methods are {}^{a}S2VT Venugopalan et al. (2015), {}^{b}FSE Zhang et al. (2018), {}^{c}CE Liu et al. (2019), {}^{d}ClipBERT Lei et al. (2021), {}^{e}Frozen Bain et al. (2021), {}^{f}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Krishna et al. (2017b) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017b. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32\u201373.", "Liu et al. (2019) Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Venugopalan et al. (2015) Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond J. Mooney, and Kate Saenko. 2015. Translating videos to natural language using deep recurrent neural networks. In NAACL-HLT, pages 1494\u20131504.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. ICCV.", "Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL.", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Zhang et al. (2018) Bowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-modal and hierarchical modeling of video and text. In ECCV, pages 385\u2013401."]}, {"table": "<table><tbody><tr><td>Methods</td><td>TrainD</td><td>E2E</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MdR\\downarrow</td><td>MnR\\downarrow</td></tr><tr><td colspan=\"8\">Zero-shot</td></tr><tr><td>CLIP-straight{}^{a}</td><td>W</td><td>\u2713</td><td>27.2</td><td>51.7</td><td>62.6</td><td>5</td><td>-</td></tr><tr><td colspan=\"8\">Training-7K</td></tr><tr><td>HowTo100M{}^{b}</td><td>H+M</td><td>\u2713</td><td>16.8</td><td>41.7</td><td>55.1</td><td>8</td><td>-</td></tr><tr><td colspan=\"8\">Training-9K</td></tr><tr><td>CE{}^{c}</td><td>M</td><td></td><td>20.6</td><td>50.3</td><td>64.0</td><td>5.3</td><td>-</td></tr><tr><td>MMT{}^{d}</td><td>H+M</td><td></td><td>27.0</td><td>57.5</td><td>69.7</td><td>3.7</td><td>-</td></tr><tr><td>AVLnet{}^{e}</td><td>H+M</td><td></td><td>28.5</td><td>54.6</td><td>65.2</td><td>4</td><td>-</td></tr><tr><td>SSB{}^{f}</td><td>H+M</td><td></td><td>28.5</td><td>58.6</td><td>71.6</td><td>3</td><td>-</td></tr><tr><td>HiT{}^{g}</td><td>H+M</td><td></td><td>32.1</td><td>62.7</td><td>74.1</td><td>3</td><td>-</td></tr><tr><td>TT-CE+{}^{h}</td><td>M</td><td></td><td>32.1</td><td>62.7</td><td>75.0</td><td>3</td><td>-</td></tr><tr><td>\\cdashline1-8[3pt/4pt](Ours)-meanP</td><td>W+M</td><td>\u2713</td><td>43.1</td><td>70.5</td><td>81.2</td><td>2</td><td>12.4</td></tr><tr><td>(Ours)-seqLSTM</td><td>W+M</td><td>\u2713</td><td>42.8</td><td>71.0</td><td>80.4</td><td>2</td><td>12.3</td></tr><tr><td>(Ours)-seqTransf</td><td>W+M</td><td>\u2713</td><td>42.7</td><td>70.9</td><td>80.6</td><td>2</td><td>11.6</td></tr><tr><td>(Ours)-tightTransf</td><td>W+M</td><td>\u2713</td><td>40.6</td><td>69.5</td><td>79.5</td><td>2</td><td>13.6</td></tr></tbody></table>", "caption": "Table A1: Results of video-to-text retrieval on MSR-VTT dataset. \u2018Training-7K\u2019 follows the data splits from Miech et al. (2019) and \u2018Training-9K\u2019 follows the data splits from Gabeur et al. (2020). They have the same test set but different training set. The column \u2018TrainD\u2019 shows the datasets used for pre-training and training, where M, H, W denote MSR-VTT, HowTo100M Miech et al. (2019) and WIT Radford et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline methods are {}^{a}CLIP-straight Portillo-Quintero et al. (2021), {}^{b}HowTo100M Miech et al. (2019), {}^{c}CE Liu et al. (2019), {}^{d}MMT Gabeur et al. (2020), {}^{e}AVLnet Rouditchenko et al. (2020), {}^{f}SSB Patrick et al. (2021), {}^{g}HiT Liu et al. (2021), {}^{h}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Liu et al. (2019) Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Portillo-Quintero et al. (2021) Jes\u00fas Andr\u00e9s Portillo-Quintero, Jos\u00e9 Carlos Ortiz-Bayliss, and Hugo Terashima-Mar\u00edn. 2021. A straightforward framework for video retrieval using clip. arXiv preprint arXiv:2102.12443.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. ICCV.", "Gabeur et al. (2020) Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval. In ECCV, volume 5.", "Rouditchenko et al. (2020) Andrew Rouditchenko, Angie Boggust, David Harwath, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Rogerio Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, et al. 2020. Avlnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199.", "Patrick et al. (2021) Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In ICLR.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Liu et al. (2021) Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. arXiv preprint arXiv:2103.15049.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020."]}, {"table": "<table><thead><tr><th>Methods</th><th>TrainD</th><th>E2E</th><th>R@1\\uparrow</th><th>R@5\\uparrow</th><th>R@10\\uparrow</th><th>MdR\\downarrow</th><th>MnR\\downarrow</th></tr></thead><tbody><tr><td>CLIP-straight{}^{a}</td><td>W</td><td>\u2713</td><td>59.9</td><td>85.2</td><td>90.7</td><td>1</td><td>-</td></tr><tr><td>TT-CE+{}^{b}</td><td>W</td><td></td><td>27.1</td><td>55.3</td><td>67.1</td><td>4</td><td>-</td></tr><tr><td>(Ours)-meanP</td><td>W+M</td><td>\u2713</td><td>56.6</td><td>79.7</td><td>84.3</td><td>1</td><td>7.6</td></tr><tr><td>(Ours)-seqLSTM</td><td>W+M</td><td>\u2713</td><td>52.5</td><td>74.0</td><td>78.1</td><td>1</td><td>14.7</td></tr><tr><td>(Ours)-seqTransf</td><td>W+M</td><td>\u2713</td><td>62.0</td><td>87.3</td><td>92.6</td><td>1</td><td>4.3</td></tr><tr><td>(Ours)-tightTransf</td><td>W+M</td><td>\u2713</td><td>54.3</td><td>85.3</td><td>91.0</td><td>1</td><td>6.0</td></tr></tbody></table>", "caption": "Table A2: Results of video-to-text retrieval on MSVD dataset. In the column \u2018TrainD\u2019, M and W denote training on MSVD and WIT Radford et al. (2021), and CW means CC3M Sharma et al. (2018) plus WebVid-2M Bain et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline method is {}^{a}CLIP-straight Portillo-Quintero et al. (2021), {}^{b}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Portillo-Quintero et al. (2021) Jes\u00fas Andr\u00e9s Portillo-Quintero, Jos\u00e9 Carlos Ortiz-Bayliss, and Hugo Terashima-Mar\u00edn. 2021. A straightforward framework for video retrieval using clip. arXiv preprint arXiv:2102.12443.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020."]}, {"table": "<table><thead><tr><th>Methods</th><th>TrainD</th><th>E2E</th><th>R@1\\uparrow</th><th>R@5\\uparrow</th><th>R@10\\uparrow</th><th>MdR\\downarrow</th><th>MnR\\downarrow</th></tr></thead><tbody><tr><td>JSFusion{}^{a}</td><td>L</td><td>\u2713</td><td>12.3</td><td>28.6</td><td>38.9</td><td>20</td><td>-</td></tr><tr><td>CLIP-straight{}^{b}</td><td>L</td><td>\u2713</td><td>6.8</td><td>16.4</td><td>22.1</td><td>73</td><td>-</td></tr><tr><td>TT-CE+{}^{c}</td><td>L</td><td></td><td>17.5</td><td>36.0</td><td>45.0</td><td>14.3</td><td>-</td></tr><tr><td>(Ours)-meanP</td><td>W+L</td><td>\u2713</td><td>20.6</td><td>39.4</td><td>47.5</td><td>13</td><td>56.7</td></tr><tr><td>(Ours)-seqLSTM</td><td>W+L</td><td>\u2713</td><td>20.9</td><td>40.7</td><td>49.1</td><td>11</td><td>53.9</td></tr><tr><td>(Ours)-seqTransf</td><td>W+L</td><td>\u2713</td><td>20.8</td><td>39.0</td><td>48.6</td><td>12</td><td>54.2</td></tr><tr><td>(Ours)-tightTransf</td><td>W+L</td><td>\u2713</td><td>17.4</td><td>36.7</td><td>45.0</td><td>15</td><td>65.3</td></tr></tbody></table>", "caption": "Table A3: Results of video-to-text retrieval on LSMDC dataset. In the column \u2018TrainD\u2019, L and W denote training on LSMDC and WIT Radford et al. (2021), MD used in Dzabraev et al. (2021) denotes a combined multidomain dataset containing MSR-VTT, LSMDC, HowTo100M, etc., and CW means CC3M Sharma et al. (2018) plus WebVid-2M Bain et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline methods are {}^{a}JSFusion Yu et al. (2018), {}^{b}CLIP-straight Portillo-Quintero et al. (2021), {}^{c}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Portillo-Quintero et al. (2021) Jes\u00fas Andr\u00e9s Portillo-Quintero, Jos\u00e9 Carlos Ortiz-Bayliss, and Hugo Terashima-Mar\u00edn. 2021. A straightforward framework for video retrieval using clip. arXiv preprint arXiv:2102.12443.", "Yu et al. (2018) Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In ECCV, pages 487\u2013503.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Dzabraev et al. (2021) Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. 2021. Mdmmt: Multidomain multimodal transformer for video retrieval. arXiv preprint arXiv:2103.10699.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020."]}, {"table": "<table><thead><tr><th>Methods</th><th>TrainD</th><th>E2E</th><th>R@1\\uparrow</th><th>R@5\\uparrow</th><th>R@10\\uparrow</th><th>MdR\\downarrow</th><th>MnR\\downarrow</th></tr></thead><tbody><tr><th>FSE{}^{a}</th><td>A</td><td></td><td>16.7</td><td>43.1</td><td>-</td><td>7.0</td><td>-</td></tr><tr><th>CE{}^{b}</th><td>A</td><td></td><td>17.7</td><td>46.6</td><td>-</td><td>6.0</td><td>24.4</td></tr><tr><th>HSE{}^{a}</th><td>A</td><td></td><td>18.7</td><td>48.1</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MMT{}^{c}</th><td>H+A</td><td></td><td>28.9</td><td>61.1</td><td>-</td><td>4.0</td><td>17.1</td></tr><tr><th>SSB{}^{d}</th><td>H+A</td><td></td><td>28.7</td><td>60.8</td><td>-</td><td>2.0</td><td>-</td></tr><tr><th>TT-CE+{}^{e}</th><td>A</td><td></td><td>23.0</td><td>56.1</td><td>-</td><td>4.0</td><td>-</td></tr><tr><th>(Ours)-meanP</th><td>W+A</td><td>\u2713</td><td>42.5</td><td>74.1</td><td>85.8</td><td>2.0</td><td>6.6</td></tr><tr><th>(Ours)-seqLSTM</th><td>W+A</td><td>\u2713</td><td>42.6</td><td>73.4</td><td>85.6</td><td>2.0</td><td>6.7</td></tr><tr><th>(Ours)-seqTransf</th><td>W+A</td><td>\u2713</td><td>41.4</td><td>73.7</td><td>85.3</td><td>2.0</td><td>6.7</td></tr><tr><th>(Ours)-tightTransf</th><td>W+A</td><td>\u2713</td><td>18.9</td><td>49.6</td><td>65.8</td><td>6.0</td><td>16.3</td></tr></tbody></table>", "caption": "Table A4: Results of video-to-text retrieval on ActivityNet dataset. In the column \u2018TrainD\u2019, A, H, and W denote training on ActivityNet, HowTo100M Miech et al. (2019), and WIT Radford et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. The baseline methods are {}^{a}FSE,HSE Zhang et al. (2018), {}^{b}CE Liu et al. (2019), {}^{c}MMT Gabeur et al. (2020), {}^{d}SSB Patrick et al. (2021), {}^{e}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Liu et al. (2019) Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. ICCV.", "Gabeur et al. (2020) Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval. In ECCV, volume 5.", "Patrick et al. (2021) Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In ICLR.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Zhang et al. (2018) Bowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-modal and hierarchical modeling of video and text. In ECCV, pages 385\u2013401."]}, {"table": "<table><thead><tr><th>Methods</th><th>TrainD</th><th>E2E</th><th>R@1\\uparrow</th><th>R@5\\uparrow</th><th>R@10\\uparrow</th><th>MdR\\downarrow</th><th>MnR\\downarrow</th></tr></thead><tbody><tr><th>S2VT{}^{a}</th><td>D</td><td>\u2713</td><td>13.2</td><td>33.6</td><td>-</td><td>15.0</td><td>-</td></tr><tr><th>FSE{}^{b}</th><td>D</td><td></td><td>13.1</td><td>33.9</td><td>-</td><td>12.0</td><td>-</td></tr><tr><th>CE{}^{c}</th><td>D</td><td></td><td>15.6</td><td>40.9</td><td>-</td><td>8.2</td><td>42.4</td></tr><tr><th>TT-CE+{}^{d}</th><td>D</td><td></td><td>21.1</td><td>47.3</td><td>61.1</td><td>6.3</td><td>-</td></tr><tr><th>(Ours)-meanP</th><td>W+D</td><td>\u2713</td><td>42.5</td><td>70.6</td><td>80.2</td><td>2.0</td><td>11.6</td></tr><tr><th>(Ours)-seqLSTM</th><td>W+D</td><td>\u2713</td><td>42.4</td><td>69.2</td><td>79.2</td><td>2.0</td><td>11.8</td></tr><tr><th>(Ours)-seqTransf</th><td>W+D</td><td>\u2713</td><td>41.4</td><td>68.2</td><td>79.1</td><td>2.0</td><td>12.4</td></tr><tr><th>(Ours)-tightTransf</th><td>W+D</td><td>\u2713</td><td>21.5</td><td>51.1</td><td>64.8</td><td>5.0</td><td>22.4</td></tr></tbody></table>", "caption": "Table A5: Results of video-to-text retrieval on DiDeMo dataset. In the column \u2018TrainD\u2019, D and W denote training on DiDeMo and WIT Radford et al. (2021). The column \u2018E2E\u2019 with \u2713means training from raw video in an end-to-end manner. \\dagger means that the candidate video is concatenated using ground truth proposals. The baseline methods are {}^{a}S2VT Venugopalan et al. (2015), {}^{b}FSE Zhang et al. (2018), {}^{c}CE Liu et al. (2019), {}^{d}ClipBERT Lei et al. (2021), {}^{e}Frozen Bain et al. (2021), {}^{d}TT-CE+ Croitoru et al. (2021).", "list_citation_info": ["Liu et al. (2019) Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Venugopalan et al. (2015) Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond J. Mooney, and Kate Saenko. 2015. Translating videos to natural language using deep recurrent neural networks. In NAACL-HLT, pages 1494\u20131504.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv preprint arXiv:2104.00650.", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.", "Croitoru et al. (2021) Ioana Croitoru, Simion-Vlad Bogolin, Yang Liu, Samuel Albanie, Marius Leordeanu, Hailin Jin, and Andrew Zisserman. 2021. Teachtext: Crossmodal generalized distillation for text-video retrieval. arXiv preprint arXiv:2104.08271.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Zhang et al. (2018) Bowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-modal and hierarchical modeling of video and text. In ECCV, pages 385\u2013401."]}]}