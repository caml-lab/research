{"title": "Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation", "abstract": "Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to existing pre-trained segmentation models. Such a straightforward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost & Found leaderboard with a large margin. Our code is publicly available at this $\\href{https://github.com/shjung13/Standardized-max-logits}{link}$.", "authors": ["Sanghun Jung", " Jungsoo Lee", " Daehoon Gwak", " Sungha Choi", " Jaegul Choo"], "pdf_url": "https://arxiv.org/abs/2107.11264", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Models</th><th colspan=\"2\">Additional training</th><th rowspan=\"2\">UtilizingOoD Data</th><th rowspan=\"2\">mIoU</th><th colspan=\"2\">FS Lost &amp; Found</th><th colspan=\"2\">FS Static</th></tr><tr><th>Seg. Network</th><th>Extra Network</th><th>AP \\uparrow</th><th>FPR{}_{95}\\downarrow</th><th>AP \\uparrow</th><th>FPR{}_{95}\\downarrow</th></tr></thead><tbody><tr><td>MSP [18]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>80.30</td><td>1.77</td><td>44.85</td><td>12.88</td><td>39.83</td></tr><tr><td>Entropy [18]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>80.30</td><td>2.93</td><td>44.83</td><td>15.41</td><td>39.75</td></tr><tr><td>Density - Single-layer NLL [3]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>80.30</td><td>3.01</td><td>32.90</td><td>40.86</td><td>21.29</td></tr><tr><td>kNN Embedding - density [3]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>80.30</td><td>3.55</td><td>30.02</td><td>44.03</td><td>20.25</td></tr><tr><td>Density - Minimum NLL [3]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>80.30</td><td>4.25</td><td>47.15</td><td>62.14</td><td>17.43</td></tr><tr><td>Density - Logistic Regression [3]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>80.30</td><td>4.65</td><td>24.36</td><td>57.16</td><td>13.39</td></tr><tr><td>Image Resynthesis [26]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>81.40</td><td>5.70</td><td>48.05</td><td>29.60</td><td>27.13</td></tr><tr><td>Bayesian Deeplab [31]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>73.80</td><td>9.81</td><td>38.46</td><td>48.70</td><td>15.50</td></tr><tr><td>OoD Training - Void Class</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>70.40</td><td>10.29</td><td>22.11</td><td>45.00</td><td>19.40</td></tr><tr><td>Ours</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>80.33</td><td>31.05</td><td>21.52</td><td>53.11</td><td>19.64</td></tr><tr><td>Discriminative Outlier Detection Head [2]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>79.57</td><td>31.31</td><td>19.02</td><td>96.76</td><td>0.29</td></tr><tr><td>Dirichlet Deeplab [29]</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>70.50</td><td>34.28</td><td>47.43</td><td>31.3</td><td>84.60</td></tr></tbody></table>", "caption": "Table 1: Comparison with previous approaches reported in Fishyscapes Leaderboard. Models are sorted by the AP scores in Fishyscapes Lost &amp; Found test set. We achieve a new state-of-the-art performance among the approaches that do not require additional training on the segmentation network or OoD data on Fishyscapes Lost &amp; Found dataset. Bold fonts indicate the highest performance in its evaluation metric among approaches that do not 1) retrain segmentation networks, 2) train extra networks, and 3) utilize OoD data.", "list_citation_info": ["[26] Krzysztof Lis, Krishna Nakka, Pascal Fua, and Mathieu Salzmann. Detecting the unexpected via image resynthesis. In Proc. of IEEE international conference on computer vision (ICCV), pages 2151\u20132161, 2019.", "[3] Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland Siegwart, and Cesar Cadena. The fishyscapes benchmark: Measuring blind spots in semantic segmentation. arXiv preprint arXiv:1904.03215, 2019.", "[29] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In Proc. of the Advances in Neural Information Processing Systems (NeurIPS), pages 7047\u20137058, 2018.", "[2] Petra Bevandi\u0107, Ivan Kre\u0161o, Marin Or\u0161i\u0107, and Sini\u0161a \u0160egvi\u0107. Dense outlier detection and open-set recognition based on training with noisy negative images. arXiv preprint arXiv:2101.09193, 2021.", "[18] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proc. of the International Conference on Learning Representations (ICLR), 2017.", "[31] Jishnu Mukhoti and Yarin Gal. Evaluating bayesian deep learning methods for semantic segmentation. arXiv preprint arXiv:1811.12709, 2018."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Models</th><th rowspan=\"2\">mIoU</th><th colspan=\"3\">FS Lost &amp; Found</th><th colspan=\"3\">FS Static</th><th colspan=\"3\">Road Anomaly</th></tr><tr><th>AUROC \\uparrow</th><th>AP \\uparrow</th><th>FPR{}_{95} \\downarrow</th><th>AUROC \\uparrow</th><th>AP \\uparrow</th><th>FPR{}_{95} \\downarrow</th><th>AUROC \\uparrow</th><th>AP \\uparrow</th><th>FPR{}_{95} \\downarrow</th></tr></thead><tbody><tr><td>MSP [18]</td><td>80.33</td><td>86.99</td><td>6.02</td><td>45.63</td><td>88.94</td><td>14.24</td><td>34.10</td><td>73.76</td><td>20.59</td><td>68.44</td></tr><tr><td>Max Logit [16]</td><td>80.33</td><td>92.00</td><td>18.77</td><td>38.13</td><td>92.80</td><td>27.99</td><td>28.50</td><td>77.97</td><td>24.44</td><td>64.85</td></tr><tr><td>Entropy</td><td>80.33</td><td>88.32</td><td>13.91</td><td>44.85</td><td>89.99</td><td>21.78</td><td>33.74</td><td>75.12</td><td>22.38</td><td>68.15</td></tr><tr><td>kNN Embedding - Density [3]</td><td>80.30</td><td>-</td><td>4.1</td><td>22.30</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>{}^{\\dagger}SynthCP{}^{*} [39]</td><td>80.33</td><td>88.34</td><td>6.54</td><td>45.95</td><td>89.90</td><td>23.22</td><td>34.02</td><td>76.08</td><td>24.86</td><td>64.69</td></tr><tr><td>Ours</td><td>80.33</td><td>96.88</td><td>36.55</td><td>14.53</td><td>96.69</td><td>48.67</td><td>16.75</td><td>81.96</td><td>25.82</td><td>49.74</td></tr></tbody></table>", "caption": "Table 2: Comparison with other baselines in the Fishyscapes validation sets and the Road Anomaly dataset. {}^{\\dagger} denotes that the results are obtained from the official code with our pre-trained backbone and {}^{*} denotes that the model requires additional learnable parameters. Note that the performance of kNN Embedding - Density is provided from the Fishyscapes [3] team.", "list_citation_info": ["[16] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. arXiv preprint arXiv:1911.11132, 2020.", "[18] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proc. of the International Conference on Learning Representations (ICLR), 2017.", "[3] Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland Siegwart, and Cesar Cadena. The fishyscapes benchmark: Measuring blind spots in semantic segmentation. arXiv preprint arXiv:1904.03215, 2019.", "[39] Yingda Xia, Yi Zhang, Fengze Liu, Wei Shen, and Alan L. Yuille. Synthesize then compare: Detecting failures and anomalies for semantic segmentation. In Proc. of the European Conference on Computer Vision (ECCV), pages 145\u2013161, 2020."]}, {"table": "<table><thead><tr><th>Backbone</th><th>Models</th><th>mIoU</th><th>AUROC \\uparrow</th><th>AP \\uparrow</th><th>FPR{}_{95} \\downarrow</th></tr></thead><tbody><tr><td rowspan=\"3\">MobileNetV2 [37]</td><td>MSP</td><td rowspan=\"3\">75.70</td><td>86.00</td><td>2.60</td><td>48.05</td></tr><tr><td>Max Logit</td><td>91.89</td><td>7.15</td><td>36.24</td></tr><tr><td>Ours</td><td>96.18</td><td>16.95</td><td>16.63</td></tr><tr><td rowspan=\"3\">ShuffleNetV2 [28]</td><td>MSP</td><td rowspan=\"3\">72.71</td><td>86.33</td><td>4.06</td><td>45.68</td></tr><tr><td>Max Logit</td><td>90.06</td><td>8.67</td><td>45.36</td></tr><tr><td>Ours</td><td>95.26</td><td>14.42</td><td>23.17</td></tr><tr><td rowspan=\"3\">ResNet50 [15]</td><td>MSP</td><td rowspan=\"3\">77.76</td><td>86.25</td><td>3.50</td><td>45.03</td></tr><tr><td>Max Logit</td><td>89.47</td><td>8.95</td><td>48.99</td></tr><tr><td>Ours</td><td>95.24</td><td>18.54</td><td>19.57</td></tr></tbody></table>", "caption": "Table 5: Comparison with MSP and max logit on Fishyscapes Lost &amp; Found dataset. The backbone networks are trained with the output stride of 16.", "list_citation_info": ["[37] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proc. of IEEE conference on computer vision and pattern recognition (CVPR), pages 4510\u20134520, 2018.", "[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. of IEEE conference on computer vision and pattern recognition (CVPR), pages 770\u2013778, 2016.", "[28] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proc. of the European Conference on Computer Vision (ECCV), pages 116\u2013131, 2018."]}, {"table": "<table><thead><tr><th>Models</th><th>GFLOPs</th><th>Infer. Time (ms)</th></tr></thead><tbody><tr><td>ResNet-101 [15]</td><td>2139.86</td><td>60.54</td></tr><tr><td>Ours (SML)</td><td>2139.86</td><td>61.41</td></tr><tr><td>Ours (SML + B. Supp .)</td><td>2140.01</td><td>74.66</td></tr><tr><td>Ours (SML + B. Supp. + D. Smoothing)</td><td>2140.12</td><td>75.02</td></tr><tr><td>SynthCP [39]</td><td>4551.11</td><td>146.90</td></tr></tbody></table>", "caption": "Table 6: Comparison of computational cost. Metrics are measured with the image size of 2048\\times{1024} on NVIDIA GeForce RTX 3090 GPU. The inference time is averaged over 100 trials.", "list_citation_info": ["[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. of IEEE conference on computer vision and pattern recognition (CVPR), pages 770\u2013778, 2016.", "[39] Yingda Xia, Yi Zhang, Fengze Liu, Wei Shen, and Alan L. Yuille. Synthesize then compare: Detecting failures and anomalies for semantic segmentation. In Proc. of the European Conference on Computer Vision (ECCV), pages 145\u2013161, 2020."]}, {"table": "<table><thead><tr><th>Architectures</th><th>mIoU</th><th>Methods</th><th>AUROC \\uparrow</th><th>AP \\uparrow</th><th>FPR{}_{95} \\downarrow</th></tr></thead><tbody><tr><td rowspan=\"3\">{}^{\\dagger}EfficientPS [30]</td><td rowspan=\"3\">79.3</td><td>MSP</td><td>84.41</td><td>1.46</td><td>61.03</td></tr><tr><td>Max Logit</td><td>89.39</td><td>3.83</td><td>48.75</td></tr><tr><td>Ours</td><td>94.17</td><td>5.93</td><td>21.93</td></tr><tr><td rowspan=\"3\">DeeplabV3+w/ ResNeSt [42]</td><td rowspan=\"3\">79.1</td><td>MSP</td><td>87.23</td><td>7.89</td><td>57.67</td></tr><tr><td>Max Logit</td><td>91.91</td><td>22.58</td><td>51.12</td></tr><tr><td>Ours</td><td>95.32</td><td>31.38</td><td>30.37</td></tr></tbody></table>", "caption": "Table 8: Results of EfficientPS and DeeplabV3+ with ResNeSt backbone on Fishyscapes Lost &amp; Found validation set. {}^{\\dagger} denotes the results are obtained from the official code with their pre-trained networks.", "list_citation_info": ["[42] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020.", "[30] Rohit Mohan and Abhinav Valada. Efficientps: Efficient panoptic segmentation. International Journal of Computer Vision, 129(5):1551\u20131579, 2021."]}]}