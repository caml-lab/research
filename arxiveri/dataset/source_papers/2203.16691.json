{"title": "MAE-AST: Masked autoencoding audio spectrogram transformer", "abstract": "In this paper, we propose a simple yet powerful improvement over the recent Self-Supervised Audio Spectrogram Transformer (SSAST) model for speech and audio classification. Specifically, we leverage the insight that the SSAST uses a very high masking ratio (75%) during pretraining, meaning that the vast majority of self-attention compute is performed on mask tokens. We address this by integrating the encoder-decoder architecture from Masked Autoencoders are Scalable Vision Learners (MAE) into the SSAST, where a deep encoder operates on only unmasked input, and a shallow decoder operates on encoder outputs and mask tokens. We find that MAE-like pretraining can provide a 3x speedup and 2x memory usage reduction over the vanilla SSAST using current audio pretraining strategies with ordinary model and input sizes. When fine-tuning on downstream tasks, which only uses the encoder, we find that our approach outperforms the SSAST on a variety of downstream tasks. We further conduct comprehensive evaluations into different strategies of pretraining and explore differences in MAE-style pretraining between the visual and audio domains.", "authors": ["Alan Baade", " Puyuan Peng", " David Harwath"], "pdf_url": "https://arxiv.org/abs/2203.16691", "list_table_and_caption": [{"table": "<table><thead><tr><th>Model</th><th>Enc. Layers</th><th>Masking</th><th>AS</th><th>ESC</th><th>KS2</th><th>KS1</th><th>SID</th><th>ER</th></tr></thead><tbody><tr><th>SSAST Base Patch</th><th>12</th><th>Chunked</th><td>28.6</td><td>88.8</td><td>98.0</td><td>96.0</td><td>64.3</td><td>59.6</td></tr><tr><th>SSAST Base Frame</th><th>12</th><th>Random</th><td>-</td><td>85.9</td><td>98.1</td><td>96.7</td><td>80.8</td><td>60.5</td></tr><tr><th>MAE-AST Patch</th><th>6</th><th>Chunked</th><td>28.3</td><td>88.6</td><td>97.4</td><td>95.0</td><td>37.6</td><td>58.7</td></tr><tr><th>MAE-AST Frame</th><th>6</th><th>Random</th><td>25.9</td><td>88.0</td><td>97.8</td><td>96.6</td><td>58.6</td><td>60.2</td></tr><tr><th>MAE-AST Patch</th><th>12</th><th>Chunked</th><td>30.6</td><td>90.0</td><td>97.9</td><td>95.8</td><td>-</td><td>59.8</td></tr><tr><th>MAE-AST Frame</th><th>12</th><th>Random</th><td>23.0</td><td>88.9</td><td>98.0</td><td>97.3</td><td>63.3</td><td>62.1</td></tr></tbody></table>", "caption": "Table 1: Overall Results. We do not replicate SSAST Base Frame on AudioSet for fairness because the exact fine-tune parameters aren\u2019t published. Note that the numbers we report for the SSAST model differ from those reported by [8]. This is due to the fact that we train and test all models on our modified version of the AudioSet dataset.", "list_citation_info": ["[8] Y. Gong, C.-I. J. Lai, Y.-A. Chung, and J. Glass, \u201cSsast: Self-supervised audio spectrogram transformer,\u201d arXiv preprint arXiv:2110.09784, 2021."]}, {"table": "<table><thead><tr><th>Mask</th><th>Lay.</th><th>Sec/Ep</th><th>Mem. (MiB)</th><th>Speedup</th></tr></thead><tbody><tr><td>75%</td><td>6</td><td>7016</td><td>6291</td><td>1.80\\times</td></tr><tr><td>75%</td><td>12</td><td>8299</td><td>8227</td><td>2.96\\times</td></tr><tr><td>50%</td><td>12</td><td>12222</td><td>11639</td><td>2.01\\times</td></tr><tr><td>w/ [M]</td><td>6</td><td>12636</td><td>9871</td><td>-</td></tr><tr><td>w/ [M]</td><td>12</td><td>24577</td><td>17719</td><td>-</td></tr><tr><td>SSAST [8]</td><td>12</td><td>150755</td><td>41547</td><td>-</td></tr></tbody></table>", "caption": "Table 4: Pretraining Speed Ablation. Speed performance is tested for all models using a batch size of 32 and input audio length truncated to 10 seconds on a single Quadro RTX 8000. MAE-AST models use 2 decoder layers, which show high-quality results in table 2 and maximal fine-tune performance in the MAE paper [12]. We reimplement SSAST-like training with a decoder-only MAE-AST (w/ [M]) to account for significant library differences and highlight speed differences solely from the switch to an encoder-decoder architecture.", "list_citation_info": ["[12] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d arXiv:2111.06377, 2021.", "[8] Y. Gong, C.-I. J. Lai, Y.-A. Chung, and J. Glass, \u201cSsast: Self-supervised audio spectrogram transformer,\u201d arXiv preprint arXiv:2110.09784, 2021."]}]}