{"title": "An empirical study of training self-supervised vision transformers", "abstract": "This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.", "authors": ["Xinlei Chen", " Saining Xie", " Kaiming He"], "pdf_url": "https://arxiv.org/abs/2104.02057", "list_table_and_caption": [{"table": "<table><tbody><tr><td>framework</td><td>model</td><td>params</td><td>acc. (%)</td></tr><tr><td>linear probing:</td><td></td><td></td><td></td></tr><tr><td>iGPT [9]</td><td>iGPT-L</td><td>1362M</td><td>69.0</td></tr><tr><td>iGPT [9]</td><td>iGPT-XL</td><td>6801M</td><td>72.0</td></tr><tr><td>MoCo v3</td><td>ViT-B</td><td>86M</td><td>76.7</td></tr><tr><td>MoCo v3</td><td>ViT-L</td><td>304M</td><td>77.6</td></tr><tr><td>MoCo v3</td><td>ViT-H</td><td>632M</td><td>78.1</td></tr><tr><td>MoCo v3</td><td>ViT-BN-H</td><td>632M</td><td>79.1</td></tr><tr><td>MoCo v3</td><td>ViT-BN-L/7</td><td>304M</td><td>81.0</td></tr><tr><td>end-to-end fine-tuning:</td><td></td><td></td><td></td></tr><tr><td>masked patch pred. [16]</td><td>ViT-B</td><td>86M</td><td>79.9{}^{\\dagger}</td></tr><tr><td>MoCo v3</td><td>ViT-B</td><td>86M</td><td>83.2</td></tr><tr><td>MoCo v3</td><td>ViT-L</td><td>304M</td><td>84.1</td></tr></tbody></table>", "caption": "Table 1:    State-of-the-art Self-supervised Transformers in ImageNet classification, evaluated by linear probing (top panel) or end-to-end fine-tuning (bottom panel).Both iGPT [9] and masked patch prediction [16] belong to the masked auto-encoding paradigm. MoCo v3 is a contrastive learning method that compares two (224\\times224) crops.ViT-B, -L, -H are the Vision Transformers proposed in [16].ViT-BN is modified with BatchNorm, and \u201c/7\u201d denotes a patch size of 7\\times7. {}^{\\dagger}: pre-trained in JFT-300M.", "list_citation_info": ["[9] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020.", "[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021."]}, {"table": "<table><thead><tr><th>model</th><th>blocks</th><th>dim</th><th>heads</th><th>params</th></tr></thead><tbody><tr><td>ViT-Small</td><td>12</td><td>384</td><td>12</td><td>22 M</td></tr><tr><td>ViT-Base [16]</td><td>12</td><td>768</td><td>12</td><td>86 M</td></tr><tr><td>ViT-Large [16]</td><td>24</td><td>1024</td><td>16</td><td>304 M</td></tr><tr><td>ViT-Huge [16]</td><td>32</td><td>1280</td><td>16</td><td>632 M</td></tr></tbody></table>", "caption": "Table 2: Configurations of ViT models in our experiments. Here \u201cblocks\u201d is the number of Transformer blocks, \u201cdim\u201d is the input/output channel dimension of all blocks, and \u201cheads\u201d is the number of heads in multi-head attention. The MLP hidden dimension is 4\\timesdim.", "list_citation_info": ["[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021."]}, {"table": "<table><thead><tr><th>model</th><th><p>MoCo v3</p></th><th><p>SimCLR</p></th><th><p>BYOL</p></th><th><p>SwAV</p></th></tr></thead><tbody><tr><th>R-50, 800-ep</th><td>73.8</td><td>70.4</td><td>74.3</td><td>71.8</td></tr><tr><th>ViT-S, 300-ep</th><td>72.5</td><td><p>69.0</p></td><td><p>71.0</p></td><td><p>67.1</p></td></tr><tr><th>ViT-B, 300-ep</th><td>76.5</td><td><p>73.9</p></td><td><p>73.9</p></td><td><p>71.6</p></td></tr></tbody></table>", "caption": "Table 4: ViT-S/16 and ViT-B/16 in different self-supervised learning frameworks (ImageNet, linear probing).R-50 results of other frameworks are from the improved implementation in [13]. For fair comparisons, all are pre-trained with two 224\\times224 crops for each image (multi-crop training [7] could improve results, which is beyond the focus of this work).", "list_citation_info": ["[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.", "[13] Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. In CVPR, 2021."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">CIFAR-10 [26]</th><th colspan=\"3\">CIFAR-100 [26]</th><th colspan=\"3\">Oxford Flowers-102 [33]</th><th colspan=\"3\">Oxford-IIIT-Pets [35]</th></tr><tr><th>pre-train</th><th><p>ViT-B</p></th><th><p>ViT-L</p></th><th><p>ViT-H</p></th><th><p>ViT-B</p></th><th><p>ViT-L</p></th><th><p>ViT-H</p></th><th><p>ViT-B</p></th><th><p>ViT-L</p></th><th><p>ViT-H</p></th><th><p>ViT-B</p></th><th><p>ViT-L</p></th><th><p>ViT-H</p></th></tr></thead><tbody><tr><th>random init.</th><td><p>77.8</p></td><td><p>77.1</p></td><td><p>75.9</p></td><td><p>48.5</p></td><td><p>48.3</p></td><td><p>48.0</p></td><td><p>54.4</p></td><td><p>54.3</p></td><td><p>52.8</p></td><td><p>40.1</p></td><td><p>42.8</p></td><td><p>40.4</p></td></tr><tr><th>ImNet supervised [16]</th><td><p>98.1</p></td><td><p>97.9</p></td><td><p>n/a</p></td><td><p>87.1</p></td><td><p>86.4</p></td><td><p>n/a</p></td><td><p>89.5</p></td><td><p>89.7</p></td><td><p>n/a</p></td><td><p>93.8</p></td><td><p>93.6</p></td><td><p>n/a</p></td></tr><tr><th>ImNet self-sup., MoCo v3</th><td><p>98.9\\uparrow0.8</p></td><td><p>99.1\\uparrow1.2</p></td><td><p>99.1</p></td><td><p>90.5\\uparrow3.4</p></td><td><p>91.1\\uparrow4.7</p></td><td><p>91.2</p></td><td><p>97.7\\uparrow8.2</p></td><td><p>98.6\\uparrow8.9</p></td><td><p>98.8</p></td><td><p>93.2\\downarrow0.6</p></td><td><p>93.7\\uparrow0.1</p></td><td><p>94.2</p></td></tr></tbody></table>", "caption": "Table 6: Transfer learning accuracy (%) in four datasets. All entries are end-to-end fine-tuned [16]. Pre-training are performed in the ImageNet-1k training set.The models are ViT-B/16, ViT-L/16, and ViT-H/14. Results of ImageNet-supervised pre-training are from Table 3 in [16]. The arrows indicate the changes w.r.t. the ImageNet-supervised counterparts.", "list_citation_info": ["[35] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012.", "[26] Alex Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.", "[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.", "[33] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing, 2008."]}, {"table": "<table><thead><tr><th>case</th><th>pre-train</th><th>ViT-S</th><th>ViT-B</th><th>ViT-L</th></tr></thead><tbody><tr><th>masked patch pred. [16]</th><th>JFT-300M</th><td>-</td><td>79.9</td><td>-</td></tr><tr><th>DeiT [42]</th><th>-</th><td>79.9</td><td>81.8</td><td>n/a</td></tr><tr><th>MoCo v3</th><th>ImageNet-1k</th><td>81.4</td><td>83.2</td><td>84.1</td></tr></tbody></table>", "caption": "Table 5: End-to-end fine-tuning accuracy (%) in ImageNet-1k.", "list_citation_info": ["[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv:2012.12877, 2020.", "[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021."]}]}