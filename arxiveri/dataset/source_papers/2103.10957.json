{"title": "Efficient visual pretraining with contrastive detection", "abstract": "Self-supervised pretraining has been shown to yield powerful representations for transfer learning. These performance gains come at a large computational cost however, with state-of-the-art methods requiring an order of magnitude more computation than supervised pretraining. We tackle this computational bottleneck by introducing a new self-supervised objective, contrastive detection, which tasks representations with identifying object-level features across augmentations. This objective extracts a rich learning signal per image, leading to state-of-the-art transfer accuracy on a variety of downstream tasks, while requiring up to 10x less pretraining. In particular, our strongest ImageNet-pretrained model performs on par with SEER, one of the largest self-supervised systems to date, which uses 1000x more pretraining data. Finally, our objective seamlessly handles pretraining on more complex images such as those in COCO, closing the gap with supervised transfer learning from COCO to PASCAL.", "authors": ["Olivier J. H\u00e9naff", " Skanda Koppula", " Jean-Baptiste Alayrac", " Aaron van den Oord", " Oriol Vinyals", " Jo\u00e3o Carreira"], "pdf_url": "https://arxiv.org/abs/2103.10957", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"2\">Fine-tune 1\\times</td><td colspan=\"2\">Fine-tune 2\\times</td></tr><tr><th>method</th><td><p>AP{}^{\\text{bb}}_{\\text{~{}}}</p></td><td><p>AP{}^{\\text{mk}}_{\\text{~{}}}</p></td><td><p>AP{}^{\\text{bb}}_{\\text{~{}}}</p></td><td><p>AP{}^{\\text{mk}}_{\\text{~{}}}</p></td></tr><tr><th>Supervised</th><td><p>39.6</p></td><td><p>35.6</p></td><td><p>41.6</p></td><td><p>37.6</p></td></tr><tr><th>VADeR [48]</th><td><p>39.2</p></td><td><p>35.6</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>MoCo [24]</th><td><p>39.4</p></td><td><p>35.6</p></td><td><p>41.7</p></td><td><p>37.5</p></td></tr><tr><th>SimCLR [9]</th><td><p>39.7</p></td><td><p>35.8</p></td><td><p>41.6</p></td><td><p>37.4</p></td></tr><tr><th>MoCo v2 [11]</th><td><p>40.1</p></td><td><p>36.3</p></td><td><p>41.7</p></td><td><p>37.6</p></td></tr><tr><th>InfoMin [54]</th><td><p>40.6</p></td><td><p>36.7</p></td><td><p>42.5</p></td><td><p>38.4</p></td></tr><tr><th>PixPro [63]</th><td><p>41.4</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>BYOL [21]</th><td><p>41.6</p></td><td><p>37.2</p></td><td><p>42.4</p></td><td><p>38.0</p></td></tr><tr><th>SwAV [7]</th><td><p>41.6</p></td><td><p>37.8</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>DetCon{}_{S}</th><td><p>41.8</p></td><td><p>37.4</p></td><td><p>42.9</p></td><td><p>38.1</p></td></tr><tr><th>DetCon{}_{B}</th><td>42.7</td><td>38.2</td><td>43.4</td><td>38.7</td></tr></tbody></table>", "caption": "Table 2: Comparison to prior art: all methods are pretrained on ImageNet then fined-tuned on COCO for 12 epochs (1\\times  schedule) or 24 epochs (2\\times  schedule). ", "list_citation_info": ["[48] Pedro O Pinheiro, Amjad Almahairi, Ryan Y Benmaleck, Florian Golemo, and Aaron Courville. Unsupervised learning of dense visual representations. arXiv preprint arXiv:2011.05499, 2020.", "[54] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.", "[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.", "[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.", "[63] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. arXiv preprint arXiv:2011.10043, 2020.", "[24] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.", "[11] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.", "[21] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33, 2020."]}, {"table": "<table><thead><tr><th>pretrain</th><th><p>Data</p></th><th><p>Params</p></th><th><p>AP{}^{\\text{bb}}_{\\text{~{}}}</p></th><th><p>AP{}^{\\text{mk}}_{\\text{~{}}}</p></th></tr></thead><tbody><tr><th>Supervised [20]</th><td><p>IN-1M</p></td><td><p>250 M</p></td><td><p>45.9</p></td><td><p>41.0</p></td></tr><tr><th>    SEER [20]</th><td>IG-1B</td><td>693 M</td><td><p>48.5</p></td><td>43.2</td></tr><tr><th>DetCon{}_{B}</th><td><p>IN-1M</p></td><td><p>250 M</p></td><td>48.9</td><td><p>43.0</p></td></tr></tbody></table>", "caption": "Table 3: Comparison to large-scale transfer learning: all methods pretrain a backbone and transfer to COCO detection and instance segmentation using a Mask-RCNN. SEER trains on a billion Instagram images whereas DetCon{}_{S} trains on ImageNet (1.3 million images). SEER and the supervised baseline use the recent RegNet architecture [49], whereas DetCon{}_{S} uses a generic ResNet-200 (2\\times width). Despite this, DetCon pretraining matches the performance of large-scale SEER pretraining.", "list_citation_info": ["[49] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428\u201310436, 2020.", "[20] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in the wild. arXiv preprint arXiv:2103.01988, 2021."]}]}