{"title": "A free lunch for unsupervised domain adaptive object detection without source data", "abstract": "Unsupervised domain adaptation (UDA) assumes that source and target domain data are freely available and usually trained together to reduce the domain gap. However, considering the data privacy and the inefficiency of data transmission, it is impractical in real scenarios. Hence, it draws our eyes to optimize the network in the target domain without accessing labeled source data. To explore this direction in object detection, for the first time, we propose a source data-free domain adaptive object detection (SFOD) framework via modeling it into a problem of learning with noisy labels. Generally, a straightforward method is to leverage the pre-trained network from the source domain to generate the pseudo labels for target domain optimization. However, it is difficult to evaluate the quality of pseudo labels since no labels are available in target domain. In this paper, self-entropy descent (SED) is a metric proposed to search an appropriate confidence threshold for reliable pseudo label generation without using any handcrafted labels. Nonetheless, completely clean labels are still unattainable. After a thorough experimental analysis, false negatives are found to dominate in the generated noisy labels. Undoubtedly, false negatives mining is helpful for performance improvement, and we ease it to false negatives simulation through data augmentation like Mosaic. Extensive experiments conducted in four representative adaptation tasks have demonstrated that the proposed framework can easily achieve state-of-the-art performance. From another view, it also reminds the UDA community that the labeled source data are not fully exploited in the existing methods.", "authors": ["Xianfeng Li", " Weijie Chen", " Di Xie", " Shicai Yang", " Peng Yuan", " Shiliang Pu", " Yueting Zhuang"], "pdf_url": "https://arxiv.org/abs/2012.05400", "list_table_and_caption": [{"table": "<table><thead><tr><th>Methods</th><th>AP of Car</th></tr></thead><tbody><tr><th>Source only</th><td>36.4</td></tr><tr><th>DA-Faster (Chen et al. 2018)</th><td>38.5</td></tr><tr><th>SW-Faster (Saito et al. 2019)</th><td>37.9</td></tr><tr><th>MAF (Z.He and L.Zhang 2019)</th><td>41.0</td></tr><tr><th>AT-Faster (He and Zhang 2020)</th><td>42.1</td></tr><tr><th>Noise Labeling (Khodabandeh et al. 2019)</th><td>43.0</td></tr><tr><th>DA-Detection (Hsu et al. 2020)</th><td>43.9</td></tr><tr><th>SFOD (SED)</th><td>43.6</td></tr><tr><th>SFOD-Mosaic (SED)</th><td>44.6</td></tr><tr><th>SFOD (Ideal)</th><td>43.7</td></tr><tr><th>SFOD-Mosaic (Ideal)</th><td>44.6</td></tr><tr><th>Oracle</th><td>58.5</td></tr></tbody></table>", "caption": "Table 1: Results of adaptation to a new sense, i.e., from KITTI dataset to Cityscapes dataset.", "list_citation_info": ["Hsu et al. (2020) Hsu, H.-K.; Yao, C.-H.; Tsai, Y.-H.; Hung, W.-C.; Tseng, H.; Singh, M.; and Yang, M.-H. 2020. Progressive Domain Adaptation for Object Detection. In WACV.", "Z.He and L.Zhang (2019) Z.He; and L.Zhang. 2019. Multi-adversarial Faster-RCNN for Unrestricted Object Detection. In ICCV.", "Chen et al. (2018) Chen, Y.; Li, W.; Sakaridis, C.; Dai, D.; and Gool, L. V. 2018. Domain Adaptive Faster R-CNN for Object Detection in the Wild. In CVPR.", "Saito et al. (2019) Saito, K.; Ushiku, Y.; Harada, T.; and Saenko, K. 2019. Strong-Weak Distribution Alignment for Adaptive Object Detection. In CVPR.", "Khodabandeh et al. (2019) Khodabandeh, M.; Vahdat, A.; Ranjbar, M.; and Macready, W. G. 2019. A Robust Learning Approach to Domain Adaptive Object Detection. In ICCV.", "He and Zhang (2020) He, Z.; and Zhang, L. 2020. Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN. In ECCV."]}, {"table": "<table><thead><tr><th>Methods</th><th>AP of Car</th></tr></thead><tbody><tr><th>Source only</th><td>33.7</td></tr><tr><th>DA-Faster (Chen et al. 2018)</th><td>38.5</td></tr><tr><th>MAF (Z.He and L.Zhang 2019)</th><td>41.1</td></tr><tr><th>AT-Faster (He and Zhang 2020)</th><td>42.1</td></tr><tr><th>Noise Labelling (Khodabandeh et al. 2019)</th><td>43.0</td></tr><tr><th>SFOD (SED)</th><td>42.3</td></tr><tr><th>SFOD-Mosaic (SED)</th><td>42.9</td></tr><tr><th>SFOD (Ideal)</th><td>42.5</td></tr><tr><th>SFOD-Mosaic (Ideal)</th><td>43.1</td></tr><tr><th>Oracle</th><td>58.5</td></tr></tbody></table>", "caption": "Table 2: Results of adaptation from synthetic to real images, i.e.,from Sim10k dataset to Cityscapes dataset.", "list_citation_info": ["Chen et al. (2018) Chen, Y.; Li, W.; Sakaridis, C.; Dai, D.; and Gool, L. V. 2018. Domain Adaptive Faster R-CNN for Object Detection in the Wild. In CVPR.", "Z.He and L.Zhang (2019) Z.He; and L.Zhang. 2019. Multi-adversarial Faster-RCNN for Unrestricted Object Detection. In ICCV.", "He and Zhang (2020) He, Z.; and Zhang, L. 2020. Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN. In ECCV.", "Khodabandeh et al. (2019) Khodabandeh, M.; Vahdat, A.; Ranjbar, M.; and Macready, W. G. 2019. A Robust Learning Approach to Domain Adaptive Object Detection. In ICCV."]}, {"table": "<table><tbody><tr><th>Methods</th><td>truck</td><td>car</td><td>rider</td><td>person</td><td>train</td><td>motor</td><td>bicycle</td><td>bus</td><td>mAP</td></tr><tr><th>Source only</th><td>14.0</td><td>40.7</td><td>24.4</td><td>22.4</td><td>-</td><td>14.5</td><td>20.5</td><td>16.1</td><td>21.8</td></tr><tr><th>DA-Faster (Chen et al. 2018)</th><td>14.3</td><td>44.6</td><td>26.5</td><td>29.4</td><td>-</td><td>15.8</td><td>20.6</td><td>16.8</td><td>24.0</td></tr><tr><th>SW-Faster (Saito et al. 2019)</th><td>15.2</td><td>45.7</td><td>29.5</td><td>30.2</td><td>-</td><td>17.1</td><td>21.2</td><td>18.4</td><td>25.3</td></tr><tr><th>CR-DA-DET (Xu et al. 2020)</th><td>19.5</td><td>46.3</td><td>31.3</td><td>31.4</td><td>-</td><td>17.3</td><td>23.8</td><td>18.9</td><td>26.9</td></tr><tr><th>SFOD (SED)</th><td>20.4</td><td>48.8</td><td>32.4</td><td>31.0</td><td>-</td><td>15.0</td><td>24.3</td><td>21.3</td><td>27.6</td></tr><tr><th>SFOD-Mosaic (SED)</th><td>20.6</td><td>50.4</td><td>32.6</td><td>32.4</td><td>-</td><td>18.9</td><td>25.0</td><td>23.4</td><td>29.0</td></tr><tr><th>SFOD (Ideal)</th><td>20.0</td><td>46.8</td><td>32.1</td><td>31.5</td><td>-</td><td>16.3</td><td>25.1</td><td>21.8</td><td>27.7</td></tr><tr><th>SFOD-Mosaic (Ideal)</th><td>20.6</td><td>50.4</td><td>32.6</td><td>32.4</td><td>-</td><td>18.9</td><td>25.0</td><td>23.4</td><td>29.0</td></tr><tr><th>Oracle</th><td>53.4</td><td>53.5</td><td>42.8</td><td>41.9</td><td>-</td><td>37.3</td><td>38.8</td><td>58.1</td><td>47.1</td></tr></tbody></table>", "caption": "Table 3: Results of adaptation to a large-scale dataset, i.e.,from Cityscapes dataset to BDD100k daytime dataset.", "list_citation_info": ["Chen et al. (2018) Chen, Y.; Li, W.; Sakaridis, C.; Dai, D.; and Gool, L. V. 2018. Domain Adaptive Faster R-CNN for Object Detection in the Wild. In CVPR.", "Xu et al. (2020) Xu, C.; Zhao, X.; Jin, X.; and Wei, X. 2020. Exploring Categorical Regularization for Domain Adaptive Object Detection. In CVPR.", "Saito et al. (2019) Saito, K.; Ushiku, Y.; Harada, T.; and Saenko, K. 2019. Strong-Weak Distribution Alignment for Adaptive Object Detection. In CVPR."]}, {"table": "<table><tbody><tr><th>Methods</th><th>defoggy</th><td>truck</td><td>car</td><td>rider</td><td>person</td><td>train</td><td>motor</td><td>bicycle</td><td>bus</td><td>mAP</td></tr><tr><th>Source only</th><th>\\times</th><td>11.6</td><td>38.7</td><td>31.4</td><td>23.6</td><td>9.4</td><td>17.3</td><td>27.4</td><td>19.0</td><td>22.3</td></tr><tr><th>DA-Faster (Chen et al. 2018)</th><th>\\times</th><td>19.5</td><td>43.5</td><td>36.5</td><td>28.7</td><td>12.6</td><td>24.8</td><td>29.1</td><td>33.1</td><td>28.5</td></tr><tr><th>MAF (Z.He and L.Zhang 2019)</th><th>\\times</th><td>23.8</td><td>43.9</td><td>39.5</td><td>28.2</td><td>33.3</td><td>29.2</td><td>33.9</td><td>39.9</td><td>34.0</td></tr><tr><th>SW-Faster (Saito et al. 2019)</th><th>\\times</th><td>23.7</td><td>47.3</td><td>42.2</td><td>32.3</td><td>27.8</td><td>28.3</td><td>35.4</td><td>41.3</td><td>34.8</td></tr><tr><th>DA-Detection (Hsu et al. 2020)</th><th>\\surd</th><td>24.3</td><td>54.4</td><td>45.5</td><td>36.0</td><td>25.8</td><td>29.1</td><td>35.9</td><td>44.1</td><td>36.9</td></tr><tr><th>CR-DA-DET (Xu et al. 2020)</th><th>\\times</th><td>27.2</td><td>49.2</td><td>43.8</td><td>32.9</td><td>36.4</td><td>30.3</td><td>34.6</td><td>45.1</td><td>37.4</td></tr><tr><th>AT-Faster (He and Zhang 2020)</th><th>\\times</th><td>23.7</td><td>50.0</td><td>47.0</td><td>34.6</td><td>38.7</td><td>33.4</td><td>38.8</td><td>43.3</td><td>38.7</td></tr><tr><th>SFOD (SED)</th><th>\\times</th><td>21.7</td><td>44.0</td><td>40.4</td><td>32.6</td><td>11.8</td><td>25.3</td><td>34.5</td><td>34.3</td><td>30.6</td></tr><tr><th>SFOD-Mosaic (SED)</th><th>\\times</th><td>25.5</td><td>44.5</td><td>40.7</td><td>33.2</td><td>22.2</td><td>28.4</td><td>34.1</td><td>39.0</td><td>33.5</td></tr><tr><th>SFOD (Ideal)</th><th>\\times</th><td>22.3</td><td>44.0</td><td>38.2</td><td>31.4</td><td>15.1</td><td>25.7</td><td>34.6</td><td>36.8</td><td>31.0</td></tr><tr><th>SFOD-Mosaic (Ideal)</th><th>\\times</th><td>25.5</td><td>44.5</td><td>40.7</td><td>33.2</td><td>22.2</td><td>28.4</td><td>34.1</td><td>39.0</td><td>33.5</td></tr><tr><th>SFOD-Defoggy (SED)</th><th>\\surd</th><td>28.4</td><td>50.9</td><td>41.6</td><td>32.2</td><td>15.9</td><td>28.1</td><td>36.0</td><td>40.1</td><td>34.2</td></tr><tr><th>SFOD-Mosaic-Defoggy (SED)</th><th>\\surd</th><td>27.9</td><td>51.7</td><td>44.7</td><td>33.2</td><td>21.3</td><td>28.6</td><td>37.3</td><td>45.9</td><td>36.3</td></tr><tr><th>SFOD-Defoggy (Ideal)</th><th>\\surd</th><td>26.2</td><td>50.6</td><td>41.8</td><td>32.5</td><td>24.4</td><td>28.7</td><td>36.1</td><td>40.5</td><td>35.1</td></tr><tr><th>SFOD-Masoic-Defoggy (Ideal)</th><th>\\surd</th><td>30.4</td><td>51.9</td><td>44.4</td><td>34.1</td><td>25.7</td><td>30.3</td><td>37.2</td><td>41.8</td><td>37.0</td></tr><tr><th>Oracle</th><th>\\times</th><td>38.1</td><td>49.8</td><td>53.1</td><td>33.1</td><td>37.4</td><td>41.1</td><td>57.4</td><td>48.2</td><td>44.8</td></tr></tbody></table>", "caption": "Table 4: Results of adaptation from normal to foggy dataset, i.e.,from Cityscapes dataset to Foggy Cityscapes dataset.", "list_citation_info": ["Hsu et al. (2020) Hsu, H.-K.; Yao, C.-H.; Tsai, Y.-H.; Hung, W.-C.; Tseng, H.; Singh, M.; and Yang, M.-H. 2020. Progressive Domain Adaptation for Object Detection. In WACV.", "Xu et al. (2020) Xu, C.; Zhao, X.; Jin, X.; and Wei, X. 2020. Exploring Categorical Regularization for Domain Adaptive Object Detection. In CVPR.", "Z.He and L.Zhang (2019) Z.He; and L.Zhang. 2019. Multi-adversarial Faster-RCNN for Unrestricted Object Detection. In ICCV.", "Chen et al. (2018) Chen, Y.; Li, W.; Sakaridis, C.; Dai, D.; and Gool, L. V. 2018. Domain Adaptive Faster R-CNN for Object Detection in the Wild. In CVPR.", "Saito et al. (2019) Saito, K.; Ushiku, Y.; Harada, T.; and Saenko, K. 2019. Strong-Weak Distribution Alignment for Adaptive Object Detection. In CVPR.", "He and Zhang (2020) He, Z.; and Zhang, L. 2020. Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN. In ECCV."]}]}