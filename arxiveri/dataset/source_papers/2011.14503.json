{"title": "End-to-end video instance segmentation with transformers", "abstract": "Video instance segmentation (VIS) is the task that requires simultaneously classifying, segmenting and tracking object instances of interest in video. Recent methods typically develop sophisticated pipelines to tackle this task. Here, we propose a new video instance segmentation framework built upon Transformers, termed VisTR, which views the VIS task as a direct end-to-end parallel sequence decoding/prediction problem. Given a video clip consisting of multiple image frames as input, VisTR outputs the sequence of masks for each instance in the video in order directly. At the core is a new, effective instance sequence matching and segmentation strategy, which supervises and segments instances at the sequence level as a whole. VisTR frames the instance segmentation and tracking in the same perspective of similarity learning, thus considerably simplifying the overall pipeline and is significantly different from existing approaches. Without bells and whistles, VisTR achieves the highest speed among all existing VIS models, and achieves the best result among methods using single model on the YouTube-VIS dataset. For the first time, we demonstrate a much simpler and faster video instance segmentation framework built upon Transformers, achieving competitive accuracy. We hope that VisTR can motivate future research for more video understanding tasks.", "authors": ["Yuqing Wang", " Zhaoliang Xu", " Xinlong Wang", " Chunhua Shen", " Baoshan Cheng", " Hao Shen", " Huaxia Xia"], "pdf_url": "https://arxiv.org/abs/2011.14503", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>backbone</td><td>FPS</td><td>AP</td><td>\\rm AP_{50}</td><td>\\rm AP_{75}</td><td>\\rm AR_{1}</td><td>\\rm AR_{10}</td></tr><tr><td>DeepSORT[28]</td><td>ResNet-50</td><td>-</td><td>26.1</td><td>42.9</td><td>26.1</td><td>27.8</td><td>31.3</td></tr><tr><td>FEELVOS[24]</td><td>ResNet-50</td><td>-</td><td>26.9</td><td>42.0</td><td>29.7</td><td>29.9</td><td>33.4</td></tr><tr><td>OSMN[31]</td><td>ResNet-50</td><td>-</td><td>27.5</td><td>45.1</td><td>29.1</td><td>28.6</td><td>33.1</td></tr><tr><td>MaskTrack R-CNN[30]</td><td>ResNet-50</td><td>20.0</td><td>30.3</td><td>51.1</td><td>32.6</td><td>31.0</td><td>35.5</td></tr><tr><td>STEm-Seg[1]</td><td>ResNet-50</td><td>-</td><td>30.6</td><td>50.7</td><td>33.5</td><td>31.6</td><td>37.1</td></tr><tr><td>STEm-Seg[1]</td><td>ResNet-101</td><td>2.1</td><td>34.6</td><td>55.8</td><td>37.9</td><td>34.4</td><td>41.6</td></tr><tr><td>MaskProp[2]</td><td>ResNet-50</td><td>-</td><td>40.0</td><td>-</td><td>42.9</td><td>-</td><td>-</td></tr><tr><td>MaskProp[2]</td><td>ResNet-101</td><td>-</td><td>42.5</td><td>-</td><td>45.6</td><td>-</td><td>-</td></tr><tr><td>VisTR</td><td>ResNet-50</td><td>30.0/69.9</td><td>36.2</td><td>59.8</td><td>36.9</td><td>37.2</td><td>42.4</td></tr><tr><td>VisTR</td><td>ResNet-101</td><td>27.7/57.7</td><td>40.1</td><td>64.0</td><td>45.0</td><td>38.3</td><td>44.9</td></tr></tbody></table>", "caption": "Table 2: Video instance segmentation AP (%) on the YouTube-VIS [30] validation dataset. Note that,for the first three methods, we have cited the results reported by the re-implementations in [30] for VIS. Other results are adopted from their original paper.For the speed of VisTR we report the FPS results with and without the data loading process.Here we naively load the images serially, taking unnecessarily long time.The data loading process can be much faster by parallelizing.", "list_citation_info": ["[1] Ali Athar, Sabarinath Mahadevan, Aljo\u0161a O\u0161ep, Laura Leal-Taix\u00e9, and Bastian Leibe. Stem-seg: Spatio-temporal embeddings for instance segmentation in videos. In Proc. Eur. Conf. Comp. Vis., 2020.", "[30] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In Proc. IEEE Int. Conf. Comp. Vis., 2019.", "[24] Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 9481\u20139490, 2019.", "[28] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In Proc. IEEE Int. Conf. Image Process., pages 3645\u20133649, 2017.", "[31] Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos K Katsaggelos. Efficient video object segmentation via network modulation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 6499\u20136507, 2018.", "[2] Gedas Bertasius and Lorenzo Torresani. Classifying, segmenting, and tracking object instances in video with mask propagation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 9739\u20139748, 2020."]}]}