{"title": "Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection", "abstract": "The objective of this paper is to learn context- and depth-aware feature representation to solve the problem of monocular 3D object detection. We make following contributions: (i) rather than appealing to the complicated pseudo-LiDAR based approach, we propose a depth-conditioned dynamic message propagation (DDMP) network to effectively integrate the multi-scale depth information with the image context;(ii) this is achieved by first adaptively sampling context-aware nodes in the image context and then dynamically predicting hybrid depth-dependent filter weights and affinity matrices for propagating information; (iii) by augmenting a center-aware depth encoding (CDE) task, our method successfully alleviates the inaccurate depth prior; (iv) we thoroughly demonstrate the effectiveness of our proposed approach and show state-of-the-art results among the monocular-based approaches on the KITTI benchmark dataset. Particularly, we rank $1^{st}$ in the highly competitive KITTI monocular 3D object detection track on the submission day (November 16th, 2020). Code and models are released at \\url{https://github.com/fudan-zvg/DDMP}", "authors": ["Li Wang", " Liang Du", " Xiaoqing Ye", " Yanwei Fu", " Guodong Guo", " Xiangyang Xue", " Jianfeng Feng", " Li Zhang"], "pdf_url": "https://arxiv.org/abs/2103.16470", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Reference</th><th rowspan=\"2\">Speed (FPS)</th><th colspan=\"3\">\\rm AP_{3D}</th><th colspan=\"3\">\\rm AP_{BEV}</th><th rowspan=\"2\">GPU</th></tr><tr><th>Mod.</th><th>Easy</th><th>Hard</th><th>Mod.</th><th>Easy</th><th>Hard</th></tr></thead><tbody><tr><td>FQNet[25]</td><td>CVPR 2019</td><td>2</td><td>1.51</td><td>2.77</td><td>1.01</td><td>3.23</td><td>5.40</td><td>2.46</td><td>1080Ti</td></tr><tr><td>ROI-10D[28]</td><td>CVPR 2019</td><td>5</td><td>2.02</td><td>4.32</td><td>1.46</td><td>4.91</td><td>9.78</td><td>3.74</td><td>-</td></tr><tr><td>MonoDIS[38]</td><td>ICCV 2019</td><td>-</td><td>7.94</td><td>10.37</td><td>6.40</td><td>13.19</td><td>17.23</td><td>11.12</td><td>Tesla V100</td></tr><tr><td>MonoPair[6]</td><td>CVPR 2020</td><td>17</td><td>9.99</td><td>13.04</td><td>8.65</td><td>14.83</td><td>19.28</td><td>12.89</td><td>-</td></tr><tr><td>UR3D [37]</td><td>ECCV 2020</td><td>8</td><td>8.61</td><td>15.58</td><td>6.00</td><td>12.51</td><td>21.85</td><td>9.2</td><td>GTX Titan X</td></tr><tr><td>RTM3D[24]</td><td>ECCV 2020</td><td>20</td><td>10.34</td><td>14.41</td><td>8.77</td><td>14.20</td><td>19.17</td><td>11.99</td><td>1080Ti</td></tr><tr><td>AM3D[27]</td><td>ICCV 2019</td><td>3</td><td>10.74</td><td>16.50</td><td>9.52</td><td>17.32</td><td>25.30</td><td>14.91</td><td>1080Ti</td></tr><tr><td>DA-3Ddet[48]</td><td>ECCV 2020</td><td>3</td><td>11.50</td><td>16.77</td><td>8.93</td><td>15.90</td><td>23.35</td><td>12.11</td><td>Titan RTX</td></tr><tr><td>\\rm D^{4}LCN[8]</td><td>CVPR 2020</td><td>5</td><td>11.72</td><td>16.65</td><td>9.51</td><td>16.02</td><td>22.51</td><td>12.55</td><td>1080Ti</td></tr><tr><td>Kinematic3D[2]</td><td>ECCV 2020</td><td>8</td><td>12.72</td><td>19.07</td><td>9.17</td><td>17.52</td><td>26.69</td><td>13.10</td><td>-</td></tr><tr><td>Ours</td><td>-</td><td>6</td><td>12.78</td><td>19.71</td><td>9.80</td><td>17.89</td><td>28.08</td><td>13.44</td><td>Tesla V100</td></tr></tbody></table>", "caption": "Table 1: Comparison with SoTA methods on the KITTI test set at IoU = 0.7. Our DDMP-3D achieves new SoTA performance.", "list_citation_info": ["[8] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided convolutions for monocular 3d object detection. arXiv preprint, 2019.", "[28] Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape. In CVPR, 2019.", "[2] Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt Schiele. Kinematic 3d object detection in monocular video. arXiv preprint, 2020.", "[27] Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Wanli Ouyang, and Xin Fan. Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving. In ICCV, 2019.", "[6] Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li. Monopair: Monocular 3d object detection using pairwise spatial relationships. In CVPR, 2020.", "[37] Xuepeng Shi, Zhixiang Chen, and Tae-Kyun Kim. Distance-normalized unified representation for monocular 3d object detection. In ECCV, 2020.", "[25] Lijie Liu, Jiwen Lu, Chunjing Xu, Qi Tian, and Jie Zhou. Deep fitting degree scoring network for monocular 3d object detection. In CVPR, 2019.", "[38] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Manuel L\u00f3pez-Antequera, and Peter Kontschieder. Disentangling monocular 3d object detection. In ICCV, 2019.", "[24] Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao. Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving. In ECCV, 2020.", "[48] Xiaoqing Ye, Liang Du, Yifeng Shi, Yingying Li, Xiao Tan, Jianfeng Feng, Errui Ding, and Shilei Wen. Monocular 3d object detection via feature domain adaptation. In ECCV, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">Val1 [AP|_{R40} / AP|_{R11}]</td><td colspan=\"3\">Val2 [AP|_{R40} / AP|_{R11}]</td></tr><tr><td>Mod.</td><td>Easy</td><td>Hard</td><td>Mod.</td><td>Easy</td><td>Hard</td></tr><tr><th>OFT-Net[35]</th><td>\u2003-\u2003/ 3.27</td><td>\u2003-\u2003/ 4.07</td><td>\u2003-\u2003/ 3.29</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>FQNet[25]</th><td>\u2003-\u2003/ 5.50</td><td>\u2003-\u2003/ 5.98</td><td>\u2003-\u2003/ 4.75</td><td>\u2003-\u2003/ 5.11</td><td>\u2003-\u2003/ 5.45</td><td>\u2003-\u2003/ 4.45</td></tr><tr><th>ROI-10D[28]</th><td>\u2003-\u2003/ 6.93</td><td>\u2003-\u2003/ 10.25</td><td>\u2003-\u2003/ 6.18</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>MonoDIS[38]</th><td>\u2003-\u2003/ 7.60</td><td>\u2003-\u2003/ 11.06</td><td>\u2003-\u2003/ 6.37</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>MonoGRNet[32]</th><td>7.56 / 10.19</td><td>11.90 / 13.88</td><td>5.76 / 7.62</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>GS3D[23]</th><td>\u2003-\u2003/ 10.97</td><td>\u2003-\u2003/ 13.46</td><td>\u2003-\u2003/ 10.38</td><td>\u2003-\u2003/ 10.51</td><td>\u2003-\u2003/ 11.63</td><td>\u2003-\u2003/ 10.51</td></tr><tr><th>shift R-CNN[30]</th><td>\u2003-\u2003/ 11.29</td><td>\u2003-\u2003/ 13.84</td><td>\u2003-\u2003/ 11.08</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>MonoPSR[22]</th><td>\u2003-\u2003/ 11.48</td><td>\u2003-\u2003/ 12.75</td><td>\u2003-\u2003/ 8.59</td><td>\u2003-\u2003/ 12.24</td><td>\u2003-\u2003/ 13.94</td><td>\u2003-\u2003/ 10.77</td></tr><tr><th>SS3D[19]</th><td>\u2003-\u2003/ 13.15</td><td>\u2003-\u2003/ 14.52</td><td>\u2003-\u2003/ 11.85</td><td>\u2003-\u2003/ 8.42</td><td>\u2003-\u2003/ 9.45</td><td>\u2003-\u2003/ 7.34</td></tr><tr><th>RTM3D[24]</th><td>\u2003-\u2003/ 16.86</td><td>\u2003-\u2003/ 20.77</td><td>\u2003-\u2003/ 16.63</td><td>\u2003-\u2003/ 16.29</td><td>\u2003-\u2003/ 19.47</td><td>\u2003-\u2003/ 15.57</td></tr><tr><th>M3D-RPN[1]</th><td>11.07 / 17.06</td><td>14.53 / 20.27</td><td>8.65 / 15.21</td><td>10.07 / 16.48</td><td>14.51 / 20.40</td><td>7.51 / 13.34</td></tr><tr><th>Pseudo-LiDAR[44]</th><td>\u2003-\u2003/18.50</td><td>\u2003-\u2003/ 28.20</td><td>\u2003-\u2003/ 16.40</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>Decoupled-3D[3]</th><td>\u2003-\u2003/ 18.68</td><td>\u2003-\u2003/ 26.95</td><td>\u2003-\u2003/ 15.82</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>UR3D [37]</th><td>13.35 / 18.76</td><td>23.24 / 28.05</td><td>10.15 / 16.55</td><td>11.10 / 16.75</td><td>22.15 / 26.30</td><td>9.15 / 13.60</td></tr><tr><th>AM3D[27]</th><td>\u2003-\u2003/ 21.09</td><td>\u2003-\u2003/ 32.23</td><td>\u2003-\u2003/ 17.26</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td><td>\u2003-\u2003/\u2003-</td></tr><tr><th>\\rm D^{4}LCN[8]</th><td>16.20 / 21.71</td><td>22.32 / 26.97</td><td>12.30 / 18.22</td><td>\u2003-\u2003/ 19.54</td><td>\u2003-\u2003/ 24.29</td><td>\u2003-\u2003/ 16.38</td></tr><tr><th>Ours</th><td>20.39 / 23.12</td><td>28.12 / 31.14</td><td>16.34 / 19.45</td><td>19.91 / 22.92</td><td>29.17 / 30.66</td><td>15.26 / 18.75</td></tr></tbody></table>", "caption": "Table 2: Comparison on the KITTI \u201cval1\u201d, \u201cval2\u201d set. We report the average precision (in %) including AP|_{R40} and AP|_{R11} of \u201cCar\u201d on 3D object detection (\\rm AP_{3D}) at IoU = 0.7.", "list_citation_info": ["[8] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided convolutions for monocular 3d object detection. arXiv preprint, 2019.", "[35] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Orthographic feature transform for monocular 3d object detection. In BMVC, 2019.", "[28] Fabian Manhardt, Wadim Kehl, and Adrien Gaidon. Roi-10d: Monocular lifting of 2d detection to 6d pose and metric shape. In CVPR, 2019.", "[32] Zengyi Qin, Jinglu Wang, and Yan Lu. Monogrnet: A geometric reasoning network for monocular 3d object localization. In AAAI, 2019.", "[30] Andretti Naiden, Vlad Paunescu, Gyeongmo Kim, ByeongMoon Jeon, and Marius Leordeanu. Shift r-cnn: Deep monocular 3d object detection with closed-form geometric constraints. In ICIP, 2019.", "[23] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and Xiaogang Wang. Gs3d: An efficient 3d object detection framework for autonomous driving. In CVPR, 2019.", "[22] Jason Ku, Alex D Pon, and Steven L Waslander. Monocular 3d object detection leveraging accurate proposals and shape reconstruction. In CVPR, 2019.", "[27] Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Wanli Ouyang, and Xin Fan. Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving. In ICCV, 2019.", "[37] Xuepeng Shi, Zhixiang Chen, and Tae-Kyun Kim. Distance-normalized unified representation for monocular 3d object detection. In ECCV, 2020.", "[1] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d region proposal network for object detection. In ICCV, 2019.", "[25] Lijie Liu, Jiwen Lu, Chunjing Xu, Qi Tian, and Jie Zhou. Deep fitting degree scoring network for monocular 3d object detection. In CVPR, 2019.", "[44] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In CVPR, 2019.", "[38] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Manuel L\u00f3pez-Antequera, and Peter Kontschieder. Disentangling monocular 3d object detection. In ICCV, 2019.", "[24] Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao. Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving. In ECCV, 2020.", "[3] Yingjie Cai, Buyu Li, Zeyu Jiao, Hongsheng Li, Xingyu Zeng, and Xiaogang Wang. Monocular 3d object detection with decoupled structured polygon estimation and height-guided depth estimation. arXiv preprint, 2020.", "[19] Eskil J\u00f6rgensen, Christopher Zach, and Fredrik Kahl. Monocular 3d object detection and box fitting trained end-to-end using intersection-over-union loss. arXiv preprint, 2019."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">Cyclist</th><th colspan=\"3\">Pedestrian</th></tr><tr><th>Mod.</th><th>Easy</th><th>Hard</th><th>Mod.</th><th>Easy</th><th>Hard</th></tr></thead><tbody><tr><th>\\rm D^{4}LCN[8]</th><td>4.41 / 1.67</td><td>5.85 / 2.45</td><td>4.14 / 1.36</td><td>11.23 / 3.42</td><td>12.95 / 4.55</td><td>11.05 / 2.83</td></tr><tr><th>Baseline</th><td>4.07 / 0.17</td><td>4.50 / 0.32</td><td>4.08 / 0.17</td><td>6.93 / 2.32</td><td>8.11 / 3.05</td><td>6.78 / 1.81</td></tr><tr><th>Ours</th><td>6.47 / 2.50</td><td>8.01 / 4.18</td><td>6.27 / 2.32</td><td>12.11 / 3.55</td><td>14.42 / 4.93</td><td>12.05 / 3.01</td></tr></tbody></table>", "caption": "Table 3: 3D object detection (\\rm AP_{3D}) performance for \u201cCyclist\u201d and \u201cPedestrian\u201d on KITTI val split and test set (\u201cval1\u201d/test).", "list_citation_info": ["[8] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided convolutions for monocular 3d object detection. arXiv preprint, 2019."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Depth</th><th rowspan=\"2\">Method</th><th colspan=\"3\">\\rm AP_{3D} / \\rm AP_{BEV}</th></tr><tr><th>Mod.</th><th>Easy</th><th>Hard</th></tr></thead><tbody><tr><th rowspan=\"3\">DORN[13]</th><th>\\rm D^{4}LCN</th><td>21.71 / 19.54</td><td>26.97 / 24.29</td><td>18.22 / 16.38</td></tr><tr><th>Baseline</th><td>18.82 / 24.18</td><td>26.03 / 33.06</td><td>16.27 / 19.63</td></tr><tr><th>Ours</th><td>23.12 / 27.46</td><td>31.14 / 37.71</td><td>19.45 / 24.53</td></tr><tr><th rowspan=\"3\">PSMNet[4]</th><th>\\rm D^{4}LCN</th><td>25.41 / \u2003-</td><td>30.03 / \u2003-</td><td>21.63 / \u2003-</td></tr><tr><th>Baseline</th><td>25.41 / 33.34</td><td>35.26 / 46.75</td><td>20.69 / 27.27</td></tr><tr><th>Ours</th><td>30.83 / 36.20</td><td>41.76 / 52.87</td><td>24.78 / 29.34</td></tr></tbody></table>", "caption": "Table 6: Comparison of different depth estimators on val split set at IoU = 0.7. The first, second, and third rows show the results of \\rm D^{4}LCN [8], the baseline, and our method.", "list_citation_info": ["[8] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided convolutions for monocular 3d object detection. arXiv preprint, 2019.", "[13] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In CVPR, 2018.", "[4] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In CVPR, 2018."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Image input</th><th rowspan=\"2\">Depth map input</th><td colspan=\"3\">\\rm AP_{3D}</td><td colspan=\"3\">\\rm AP_{BEV}</td></tr><tr><td>Mod.</td><td>Easy</td><td>Hard</td><td>Mod.</td><td>Easy</td><td>Hard</td></tr><tr><th>3DNet</th><th rowspan=\"2\">\u2713</th><th rowspan=\"2\">-</th><td>14.61</td><td>17.94</td><td>12.74</td><td>19.89</td><td>24.87</td><td>16.14</td></tr><tr><th>3DNet w/ DGMN [51]</th><td>16.98</td><td>20.12</td><td>15.17</td><td>21.49</td><td>26.40</td><td>17.96</td></tr><tr><th>Baseline (3DNet + Depth)</th><th rowspan=\"2\">\u2713</th><th rowspan=\"2\">\u2713</th><td>18.82</td><td>26.03</td><td>16.27</td><td>24.18</td><td>33.06</td><td>19.63</td></tr><tr><th>3DNet w/ DGMN [51] +Depth</th><td>19.59</td><td>27.78</td><td>16.48</td><td>25.30</td><td>35.59</td><td>20.32</td></tr><tr><th>DDMP</th><th rowspan=\"3\">\u2713</th><th rowspan=\"3\">\u2713</th><td>22.84</td><td>28.12</td><td>19.09</td><td>27.05</td><td>37.11</td><td>24.20</td></tr><tr><th>DDMP + CDE</th><td>23.13</td><td>31.14</td><td>19.45</td><td>27.46</td><td>37.71</td><td>24.53</td></tr><tr><th>DDMP (Softmax) + CDE</th><td>23.17</td><td>32.40</td><td>19.35</td><td>27.85</td><td>42.05</td><td>24.91</td></tr></tbody></table>", "caption": "Table 10: Comparison results (3D \u201cCar\u201d detection) of different message integration positions on val split set (IoU = 0.7).", "list_citation_info": ["[51] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dynamic graph message passing networks. In CVPR, 2020."]}]}