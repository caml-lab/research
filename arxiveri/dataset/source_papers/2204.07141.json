{"title": "Masked Siamese Networks for Label-Efficient Learning", "abstract": "We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our approach matches the representation of an image view containing randomly masked patches to the representation of the original unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures, while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance, on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4% top-1 accuracy, and with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark. Our code is publicly available.", "authors": ["Mahmoud Assran", " Mathilde Caron", " Ishan Misra", " Piotr Bojanowski", " Florian Bordes", " Pascal Vincent", " Armand Joulin", " Michael Rabbat", " Nicolas Ballas"], "pdf_url": "https://arxiv.org/abs/2204.07141", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th></th><th></th><td colspan=\"3\">Images per Class</td></tr><tr><th>Method</th><th>Architecture</th><th>Epochs</th><td>1</td><td>2</td><td>5</td></tr><tr><th rowspan=\"2\">iBOT (Zhou et al., 2021)</th><th>ViT-S/16</th><th>800</th><td>40.4 \\pm 0.5</td><td>50.8 \\pm 0.8</td><td>59.9 \\pm 0.2</td></tr><tr><th>ViT-B/16</th><th>400</th><td>46.1 \\pm 0.3</td><td>56.2 \\pm 0.7</td><td>64.7 \\pm 0.3</td></tr><tr><th rowspan=\"4\">DINO (Caron et al., 2021)</th><th>ViT-S/16</th><th>800</th><td>38.9 \\pm 0.4</td><td>48.9 \\pm 0.3</td><td>58.5 \\pm 0.1</td></tr><tr><th>ViT-B/16</th><th>400</th><td>41.8 \\pm 0.3</td><td>51.9 \\pm 0.6</td><td>61.4 \\pm 0.2</td></tr><tr><th>ViT-S/8</th><th>800</th><td>45.5 \\pm 0.4</td><td>56.0 \\pm 0.7</td><td>64.7 \\pm 0.4</td></tr><tr><th>ViT-B/8</th><th>300</th><td>45.8 \\pm 0.5</td><td>55.9 \\pm 0.6</td><td>64.6 \\pm 0.2</td></tr><tr><th rowspan=\"3\">MAE (He et al., 2021)</th><th>ViT-B/16</th><th>1600</th><td>8.2 \\pm 0.3</td><td>25.0 \\pm 0.3</td><td>40.5 \\pm 0.2</td></tr><tr><th>ViT-L/16</th><th>1600</th><td>12.3 \\pm 0.2</td><td>19.3 \\pm 1.8</td><td>42.3 \\pm 0.3</td></tr><tr><th>ViT-H/14</th><th>1600</th><td>11.6 \\pm 0.4</td><td>18.6 \\pm 0.2</td><td>32.8 \\pm 0.2</td></tr><tr><th rowspan=\"5\">MSN (Ours)</th><th>ViT-S/16</th><th>800</th><td>47.1 \\pm 0.1</td><td>55.8 \\pm 0.6</td><td>62.8 \\pm 0.3</td></tr><tr><th>ViT-B/16</th><th>600</th><td>49.8 \\pm 0.2</td><td>58.9 \\pm 0.4</td><td>65.5 \\pm 0.3</td></tr><tr><th>ViT-B/8</th><th>600</th><td>55.1 \\pm 0.1</td><td>64.9 \\pm 0.7</td><td>71.6 \\pm 0.3</td></tr><tr><th>ViT-B/4</th><th>300</th><td>54.3 \\pm 0.4</td><td>64.6 \\pm 0.7</td><td>72.4 \\pm 0.3</td></tr><tr><th>ViT-L/7</th><th>200</th><td>57.1 \\pm 0.6</td><td>66.4 \\pm 0.6</td><td>72.1 \\pm 0.2</td></tr></tbody></table>", "caption": "Table 1: Extreme low-shot. We evaluate the label-efficiency of self-supervised models pretrained on the ImageNet-1K dataset. For evaluation, we use an extremely small number of the ImageNet-1K labels and report the mean top-1 accuracy and standard deviation across 3 random splits of the data.", "list_citation_info": ["He et al. (2021) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.", "Caron et al. (2021) Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "Zhou et al. (2021) Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Architecture</th><th>Params.</th><th>Top 1</th></tr></thead><tbody><tr><td colspan=\"4\">Comparing similar architectures</td></tr><tr><td>Barlow-Tw. (Zbontar et al., 2021)</td><td>RN50</td><td>24M</td><td>55.0</td></tr><tr><td>SimCLRv2 (Chen et al., 2020c)</td><td>RN50</td><td>24M</td><td>57.9</td></tr><tr><td>PAWS (Assran et al., 2021)</td><td>RN50</td><td>24M</td><td>66.5</td></tr><tr><td>DINO (Caron et al., 2021)</td><td>ViT-S/16</td><td>22M</td><td>64.5</td></tr><tr><td>iBOT (Zhou et al., 2021)</td><td>ViT-S/16</td><td>22M</td><td>65.9</td></tr><tr><td>MSN</td><td>ViT-S/16</td><td>22M</td><td>67.2</td></tr><tr><td colspan=\"4\">Comparing larger architectures</td></tr><tr><td>BYOL (Grill et al., 2020)</td><td>RN200 (2\\times)</td><td>250M</td><td>71.2</td></tr><tr><td>SimCLRv2 (Chen et al., 2020c)</td><td>RN151+SK (3\\times)</td><td>795M</td><td>74.9</td></tr><tr><td>iBOT (Zhou et al., 2021){}^{\\dagger}</td><td>ViT-B/16</td><td>86M</td><td>69.7</td></tr><tr><td>DINO (Caron et al., 2021){}^{\\dagger}</td><td>ViT-B/8</td><td>86M</td><td>70.0</td></tr><tr><td rowspan=\"2\">MSN</td><td>ViT-L/7</td><td>304M</td><td>75.1</td></tr><tr><td>ViT-B/4</td><td>86M</td><td>75.7</td></tr></tbody></table>", "caption": "Table 2: Low-shot evaluation on ImageNet-1K using 1% of the labels (approximately 13 images per class). {}^{\\dagger}Indicates evaluations we computed using publicly available models.", "list_citation_info": ["Chen et al. (2020c) Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020c.", "Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.", "Zbontar et al. (2021) Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.", "Caron et al. (2021) Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "Zhou et al. (2021) Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.", "Assran et al. (2021) Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. In ICCV, 2021."]}, {"table": "<table><tbody><tr><td>Method</td><td>Architecture</td><td>Params.</td><td>Epochs</td><td>Top 1</td></tr><tr><td colspan=\"5\">Comparing similar architectures</td></tr><tr><td>SimCLRv2 (Chen et al., 2020c)</td><td>RN50</td><td>24M</td><td>800</td><td>71.7</td></tr><tr><td>BYOL (Grill et al., 2020)</td><td>RN50</td><td>24M</td><td>1000</td><td>74.4</td></tr><tr><td>DINO (Caron et al., 2021)</td><td>ViT-S/16</td><td>22M</td><td>800</td><td>77.0</td></tr><tr><td>iBOT (Zhou et al., 2021)</td><td>ViT-S/16</td><td>22M</td><td>800</td><td>77.9</td></tr><tr><td>MSN</td><td>ViT-S/16</td><td>22M</td><td>600</td><td>76.9</td></tr><tr><td colspan=\"5\">Comparing larger architectures</td></tr><tr><td>MAE (He et al., 2021)</td><td>ViT-H/14</td><td>632M</td><td>1600</td><td>76.6</td></tr><tr><td>BYOL (Grill et al., 2020)</td><td>RN200 (2\\times)</td><td>250M</td><td>800</td><td>79.6</td></tr><tr><td>SimCLRv2 (Chen et al., 2020c)</td><td>RN151+SK (3\\times)</td><td>795M</td><td>800</td><td>79.8</td></tr><tr><td>iBOT (Zhou et al., 2021)</td><td>ViT-B/16</td><td>86M</td><td>400</td><td>79.4</td></tr><tr><td>DINO (Caron et al., 2021)</td><td>ViT-B/8</td><td>86M</td><td>300</td><td>80.1</td></tr><tr><td>MoCov3 (Chen et al., 2021)</td><td>ViT-BN-L/7</td><td>304M</td><td>300</td><td>81.0</td></tr><tr><td>MSN</td><td>ViT-L/7</td><td>304M</td><td>200</td><td>80.7</td></tr></tbody></table>", "caption": "Table 3: Linear evaluation on ImageNet-1K using 100% of the labels.", "list_citation_info": ["Chen et al. (2020c) Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020c.", "Grill et al. (2020) Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.", "Caron et al. (2021) Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "He et al. (2021) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.", "Zhou et al. (2021) Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.", "Chen et al. (2021) Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021."]}, {"table": "<table><thead><tr><th>Initialization</th><th>Pretrain Epochs</th><th>Top 1</th></tr></thead><tbody><tr><th>DINO (Caron et al., 2021)</th><td>800</td><td>83.6</td></tr><tr><th>BEiT (Bao et al., 2021)</th><td>800</td><td>83.2</td></tr><tr><th>iBOT (He et al., 2021)</th><td>800</td><td>83.8</td></tr><tr><th>MAE (He et al., 2021)</th><td>1600</td><td>83.6</td></tr><tr><th>SimMIM (Xie et al., 2021)</th><td>-</td><td>83.8</td></tr><tr><th>MaskFeat (Wei et al., 2021)</th><td>-</td><td>84.0</td></tr><tr><th>Data2Vec (Baevski et al., 2022)</th><td>800</td><td>84.2</td></tr><tr><th>MSN</th><td>600</td><td>83.4</td></tr></tbody></table>", "caption": "Table 4: End-to-end fine-tuning of a ViT-B/16 encoder on ImageNet-1K using 100% of the labels. MSN obtains competitive performance with both joint-embedding approaches and auto-encoding approaches.", "list_citation_info": ["Baevski et al. (2022) Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022.", "Wei et al. (2021) Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. arXiv preprint arXiv:2112.09133, 2021.", "Caron et al. (2021) Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "Xie et al. (2021) Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. arXiv preprint arXiv:2111.09886, 2021.", "He et al. (2021) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.", "Bao et al. (2021) Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021."]}, {"table": "<table><thead><tr><th></th><th>IN-A</th><th>IN-R</th><th>IN-Sketch</th><th>IN-C</th></tr><tr><th></th><th>(top-1 \\uparrow)</th><th>(top-1 \\uparrow)</th><th>(top-1 \\uparrow)</th><th>(mCE \\downarrow)</th></tr></thead><tbody><tr><th>Supervised ResNet50</th><td>0.04</td><td>36.11</td><td>24.2</td><td>76.7</td></tr><tr><th>MAE ViT-B/16 (He et al., 2021)</th><td>35.9</td><td>48.3</td><td>34.5</td><td>51.7</td></tr><tr><th>MSN ViT-B/16</th><td>37.5</td><td>50.0</td><td>36.3</td><td>46.6</td></tr></tbody></table>", "caption": "Table 14: Evaluation on alternative ImageNet validation sets. We evaluate the performance of a fine-tuned ViT-B/16 model on four alternative ImageNet validation sets: ImageNet-A, ImageNet-R, ImageNet-Sketch, and ImageNet-C. The metric used for the first three (-A, -R, and -Sketch) is top-1 accuracy on the validation set. On ImageNet-C, performance is measured in terms of mean Corruption Error (mCE) (Hendrycks &amp; Dietterich, 2019).", "list_citation_info": ["He et al. (2021) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.", "Hendrycks & Dietterich (2019) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019."]}]}