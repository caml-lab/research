{"title": "Removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning", "abstract": "Unsupervised image captioning is a challenging task that aims at generating captions without the supervision of image-sentence pairs, but only with images and sentences drawn from different sources and object labels detected from the images. In previous work, pseudo-captions, i.e., sentences that contain the detected object labels, were assigned to a given image. The focus of the previous work was on the alignment of input images and pseudo-captions at the sentence level. However, pseudo-captions contain many words that are irrelevant to a given image. In this work, we investigate the effect of removing mismatched words from image-sentence alignment to determine how they make this task difficult. We propose a simple gating mechanism that is trained to align image features with only the most reliable words in pseudo-captions: the detected object labels. The experimental results show that our proposed method outperforms the previous methods without introducing complex sentence-level learning objectives. Combined with the sentence-level alignment method of previous work, our method further improves its performance. These results confirm the importance of careful alignment in word-level details.", "authors": ["Ukyo Honda", " Yoshitaka Ushiku", " Atsushi Hashimoto", " Taro Watanabe", " Yuji Matsumoto"], "pdf_url": "https://arxiv.org/abs/2104.13872", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th>Training Text</th><th>Object Detector</th><th>Image Encoder</th><th>Text Decoder</th></tr></thead><tbody><tr><th>A Feng et al. (2019)</th><td>SS</td><td>Faster-RCNN trained on OpneImages-v2</td><td>Inception-v4</td><td>1-layer LSTM of 512 dimensions</td></tr><tr><th>B Laina et al. (2019)</th><td>GCC</td><td>Faster-RCNN trained on OpneImages-v4</td><td>ResNet-101</td><td>1-layer GRU of 200 dimensions</td></tr></tbody></table>", "caption": "Table 1: Summary of the difference in the experimental settings.", "list_citation_info": ["Laina et al. (2019) Iro Laina, Christian Rupprecht, and Nassir Navab. 2019. Towards unsupervised image captioning with shared multimodal embeddings. In ICCV.", "Feng et al. (2019) Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Unsupervised image captioning. In CVPR."]}, {"table": "<table><thead><tr><th></th><th></th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>METEOR</th><th>ROUGE-L</th><th>CIDEr</th><th>SPICE</th></tr></thead><tbody><tr><th rowspan=\"2\">A</th><th>Feng et al. (2019)</th><td>41.0</td><td>22.5</td><td>11.2</td><td>5.6</td><td>12.4</td><td>28.7</td><td>28.6</td><td>8.1</td></tr><tr><th>Ours</th><td>49.5 \\pm 0.7</td><td>27.3 \\pm 1.2</td><td>13.1 \\pm 0.8</td><td>6.3 \\pm 0.5</td><td>14.0 \\pm 0.1</td><td>34.5 \\pm 0.3</td><td>31.9 \\pm 1.0</td><td>8.6 \\pm 0.2</td></tr><tr><th rowspan=\"2\">B</th><th>Laina et al. (2019)</th><td></td><td></td><td></td><td>6.5</td><td>12.9</td><td>35.1</td><td>22.7</td><td></td></tr><tr><th>Ours</th><td>50.4 \\pm 1.5</td><td>29.5 \\pm 0.8</td><td>14.4 \\pm 0.5</td><td>7.6 \\pm 0.4</td><td>13.5 \\pm 0.3</td><td>37.3 \\pm 0.2</td><td>31.8 \\pm 0.7</td><td>8.4 \\pm 0.1</td></tr></tbody></table>", "caption": "Table 2: Comparison with the state-of-the-art results on the experimental settings A and B. The scores of our model are the mean \\pm standard deviation of five runs. The scores obtained for BLEU-1 to 3 and SPICE are not provided in the original paper of Laina et al. (2019).", "list_citation_info": ["Laina et al. (2019) Iro Laina, Christian Rupprecht, and Nassir Navab. 2019. Towards unsupervised image captioning with shared multimodal embeddings. In ICCV.", "Feng et al. (2019) Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Unsupervised image captioning. In CVPR."]}, {"table": "<table><thead><tr><th></th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>METEOR</th><th>ROUGE-L</th><th>CIDEr</th><th>SPICE</th></tr></thead><tbody><tr><th>Feng et al. (2019)</th><td>41.0</td><td>22.5</td><td>11.2</td><td>5.6</td><td>12.4</td><td>28.7</td><td>28.6</td><td>8.1</td></tr><tr><th>Ours</th><td>49.5 \\pm 0.7</td><td>27.3 \\pm 1.2</td><td>13.1 \\pm 0.8</td><td>6.3 \\pm 0.5</td><td>14.0 \\pm 0.1</td><td>34.5 \\pm 0.3</td><td>31.9 \\pm 1.0</td><td>8.6 \\pm 0.2</td></tr><tr><th>Ours + Feng et al. (2019)</th><td>50.9 \\pm 0.1</td><td>28.0 \\pm 0.1</td><td>14.0 \\pm 0.1</td><td>7.1 \\pm 0.0</td><td>14.1 \\pm 0.0</td><td>35.2 \\pm 0.1</td><td>35.7 \\pm 0.1</td><td>9.2 \\pm 0.0</td></tr></tbody></table>", "caption": "Table 4: Results of combining our method with previous methods Feng et al. (2019). Scores of our model and the combined model are the mean \\pm standard deviation of five runs. We marked in bold the scores within the standard deviation of the best scores.", "list_citation_info": ["Feng et al. (2019) Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Unsupervised image captioning. In CVPR."]}, {"table": "<table><thead><tr><th></th><th></th><th>Precision</th><th>Recall</th><th>F1</th></tr></thead><tbody><tr><th rowspan=\"3\">Detected</th><th>Feng et al. (2019)</th><td>56.6</td><td>57.4</td><td>55.4</td></tr><tr><th>Ours</th><td>51.0</td><td>56.7</td><td>51.6</td></tr><tr><th>Ours + Feng et al. (2019)</th><td>54.0</td><td>61.8</td><td>55.4</td></tr><tr><th rowspan=\"3\">Others</th><th>Feng et al. (2019)</th><td>22.3</td><td>17.0</td><td>18.8</td></tr><tr><th>Ours</th><td>27.8</td><td>21.9</td><td>23.4</td></tr><tr><th>Ours + Feng et al. (2019)</th><td>29.9</td><td>21.9</td><td>24.2</td></tr></tbody></table>", "caption": "Table 5: Bag-of-words matching scores with respect to detected object labels and the other words.", "list_citation_info": ["Feng et al. (2019) Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Unsupervised image captioning. In CVPR."]}, {"table": "<table><thead><tr><th></th><th></th><th>Word Type</th><th>Frequency</th></tr></thead><tbody><tr><th rowspan=\"3\">Object</th><th>Feng et al. (2019)</th><td>205</td><td>20013</td></tr><tr><th>Ours</th><td>306</td><td>15052</td></tr><tr><th>Ours + Feng et al. (2019)</th><td>239</td><td>18226</td></tr><tr><th rowspan=\"3\">Others</th><th>Feng et al. (2019)</th><td>827</td><td>24865</td></tr><tr><th>Ours</th><td>169</td><td>83693</td></tr><tr><th>Ours + Feng et al. (2019)</th><td>121</td><td>110358</td></tr></tbody></table>", "caption": "Table 6: Analysis of generated captions with respect to object labels and the other words. Word Type is the number of unique words, and Frequency is the mean of the frequency of the words in the training text corpus.", "list_citation_info": ["Feng et al. (2019) Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Unsupervised image captioning. In CVPR."]}]}