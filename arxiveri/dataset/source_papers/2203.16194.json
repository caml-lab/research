{"title": "FlowFormer: A transformer architecture for optical flow", "abstract": "We introduce optical Flow transFormer, dubbed as FlowFormer, a transformer-based neural network architecture for learning optical flow. FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the cost tokens into a cost memory with alternate-group transformer (AGT) layers in a novel latent space, and decodes the cost memory via a recurrent transformer decoder with dynamic positional cost queries. On the Sintel benchmark, FlowFormer achieves 1.159 and 2.088 average end-point-error (AEPE) on the clean and final pass, a 16.5% and 15.5% error reduction from the best published result (1.388 and 2.47). Besides, FlowFormer also achieves strong generalization performance. Without being trained on Sintel, FlowFormer achieves 1.01 AEPE on the clean pass of Sintel training set, outperforming the best published result (1.29) by 21.7%.", "authors": ["Zhaoyang Huang", " Xiaoyu Shi", " Chao Zhang", " Qiang Wang", " Ka Chun Cheung", " Hongwei Qin", " Jifeng Dai", " Hongsheng Li"], "pdf_url": "https://arxiv.org/abs/2203.16194", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Training Data</th><th rowspan=\"2\">Method</th><td colspan=\"2\">Sintel (train)</td><td colspan=\"2\">KITTI-15 (train)</td><td colspan=\"2\">Sintel (test)</td><td>KITTI-15 (test)</td></tr><tr><td>Clean</td><td>Final</td><td>F1-epe</td><td>F1-all</td><td>Clean</td><td>Final</td><td>F1-all</td></tr><tr><th rowspan=\"3\">A</th><th>Perceiver IO [24]</th><td>1.81</td><td>2.42</td><td>4.98</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PWC-Net [42]</th><td>2.17</td><td>2.91</td><td>5.76</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RAFT [46]</th><td>1.95</td><td>2.57</td><td>4.23</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th rowspan=\"9\">C+T</th><th>HD3 [55]</th><td>3.84</td><td>8.77</td><td>13.17</td><td>24.0</td><td>-</td><td>-</td><td>-</td></tr><tr><th>LiteFlowNet [21]</th><td>2.48</td><td>4.04</td><td>10.39</td><td>28.5</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PWC-Net [42]</th><td>2.55</td><td>3.93</td><td>10.35</td><td>33.7</td><td>-</td><td>-</td><td>-</td></tr><tr><th>LiteFlowNet2 [22]</th><td>2.24</td><td>3.78</td><td>8.97</td><td>25.9</td><td>-</td><td>-</td><td>-</td></tr><tr><th>S-Flow [57]</th><td>1.30</td><td>2.59</td><td>4.60</td><td>15.9</td><td></td><td></td><td></td></tr><tr><th>RAFT [46]</th><td>1.43</td><td>2.71</td><td>5.04</td><td>17.4</td><td>-</td><td>-</td><td>-</td></tr><tr><th>FM-RAFT [26]</th><td>1.29</td><td>2.95</td><td>6.80</td><td>19.3</td><td>-</td><td>-</td><td>-</td></tr><tr><th>GMA [25]</th><td>1.30</td><td>2.74</td><td>4.69</td><td>17.1</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>\\mathbf{1.01}</td><td>\\mathbf{2.40}</td><td>\\mathbf{4.09}^{{\\dagger}}</td><td>\\mathbf{14.72}^{{\\dagger}}</td><td>-</td><td>-</td><td>-</td></tr><tr><th rowspan=\"11\">C+T+S+K+H</th><th>LiteFlowNet2 [22]</th><td>(1.30)</td><td>(1.62)</td><td>(1.47)</td><td>(4.8)</td><td>3.48</td><td>4.69</td><td>7.74</td></tr><tr><th>PWC-Net+ [43]</th><td>(1.71)</td><td>(2.34)</td><td>(1.50)</td><td>(5.3)</td><td>3.45</td><td>4.60</td><td>7.72</td></tr><tr><th>VCN [53]</th><td>(1.66)</td><td>(2.24)</td><td>(1.16)</td><td>(4.1)</td><td>2.81</td><td>4.40</td><td>6.30</td></tr><tr><th>MaskFlowNet [59]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>2.52</td><td>4.17</td><td>6.10</td></tr><tr><th>S-Flow [57]</th><td>(0.69)</td><td>(1.10)</td><td>(0.69)</td><td>(1.60)</td><td>1.50</td><td>2.67</td><td>\\mathbf{4.64}</td></tr><tr><th>RAFT [46]</th><td>(0.76)</td><td>(1.22)</td><td>(0.63)</td><td>(1.5)</td><td>1.94</td><td>3.18</td><td>5.10</td></tr><tr><th>FM-RAFT [26]</th><td>(0.79)</td><td>(1.70)</td><td>(0.75)</td><td>(2.1)</td><td>1.72</td><td>3.60</td><td>6.17</td></tr><tr><th>GMA [25]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>1.40</td><td>2.88</td><td>5.15</td></tr><tr><th>Ours</th><td>(0.48)</td><td>(0.74)</td><td>(0.53)</td><td>(1.11)</td><td>\\mathbf{1.16}</td><td>\\mathbf{2.09}</td><td>4.68^{\\dagger}</td></tr><tr><th>RAFT* [46]</th><td>(0.77)</td><td>(1.27)</td><td>-</td><td>-</td><td>1.61</td><td>2.86</td><td>-</td></tr><tr><th>GMA* [25]</th><td>(0.62)</td><td>(1.06)</td><td>(0.57)</td><td>(1.2)</td><td>1.39</td><td>2.47</td><td>-</td></tr></tbody></table>", "caption": "Table 1:  Experiments on Sintel [3] and KITTI [14] datasets. \u2018A\u2019 denotes the autoflow dataset. \u2018C + T\u2019 denotes training only on the FlyingChairs and FlyingThings datasets. \u2018+ S + K + H\u2019 denotes finetuning on the combination of Sintel, KITTI, and HD1K training sets. * denotes that the methods use the warm-start strategy [46], which relies on previous image frames in a video. {}^{\\dagger} is estimated via the tile technique elaborated in the supplementary.Our FlowFormer achieves best generalization performance (C+T) and ranks 1st on the Sintel benchmark (C+T+S+K+H).", "list_citation_info": ["[25] Jiang, S., Campbell, D., Lu, Y., Li, H., Hartley, R.: Learning to estimate hidden motions with global motion aggregation. arXiv preprint arXiv:2104.02409 (2021)", "[53] Yang, G., Ramanan, D.: Volumetric correspondence networks for optical flow. Advances in neural information processing systems 32, 794\u2013805 (2019)", "[42] Sun, D., Yang, X., Liu, M.Y., Kautz, J.: Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 8934\u20138943 (2018)", "[3] Butler, D.J., Wulff, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie for optical flow evaluation. In: European conference on computer vision. pp. 611\u2013625. Springer (2012)", "[26] Jiang, S., Lu, Y., Li, H., Hartley, R.: Learning optical flow from a few matches. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 16592\u201316600 (2021)", "[43] Sun, D., Yang, X., Liu, M.Y., Kautz, J.: Models matter, so does training: An empirical study of cnns for optical flow estimation. IEEE transactions on pattern analysis and machine intelligence 42(6), 1408\u20131423 (2019)", "[22] Hui, T.W., Tang, X., Loy, C.C.: A lightweight optical flow cnn\u2014revisiting data fidelity and regularization. IEEE transactions on pattern analysis and machine intelligence 43(8), 2555\u20132569 (2020)", "[57] Zhang, F., Woodford, O.J., Prisacariu, V.A., Torr, P.H.: Separable flow: Learning motion cost volumes for optical flow estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10807\u201310817 (2021)", "[24] Jaegle, A., Borgeaud, S., Alayrac, J.B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., et al.: Perceiver io: A general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795 (2021)", "[55] Yin, Z., Darrell, T., Yu, F.: Hierarchical discrete distribution decomposition for match density estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6044\u20136053 (2019)", "[21] Hui, T.W., Tang, X., Loy, C.C.: Liteflownet: A lightweight convolutional neural network for optical flow estimation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 8981\u20138989 (2018)", "[59] Zhao, S., Sheng, Y., Dong, Y., Chang, E.I., Xu, Y., et al.: Maskflownet: Asymmetric feature matching with learnable occlusion mask. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6278\u20136287 (2020)", "[14] Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: The kitti dataset. The International Journal of Robotics Research 32(11), 1231\u20131237 (2013)", "[46] Teed, Z., Deng, J.: Raft: Recurrent all-pairs field transforms for optical flow. In: European conference on computer vision. pp. 402\u2013419. Springer (2020)"]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Sintel (train)</td><td colspan=\"2\">KITTI-15 (train)</td><td rowspan=\"2\">parameters</td></tr><tr><td>Clean</td><td>Final</td><td>F1-epe</td><td>F1-all</td></tr><tr><td>GMA [25]</td><td>1.30 (+30%)</td><td>2.74 (+12%)</td><td>4.69 (+15%)</td><td>17.1 (+16%)</td><td>5.9M</td></tr><tr><td>Ours (small)</td><td>1.20 (+20%)</td><td>2.64 (+8%)</td><td>4.57 (+12%)</td><td>16.62 (+13%)</td><td>6.2M</td></tr><tr><td>GMA-L [25]</td><td>1.33 (+33%)</td><td>2.56 (+4%)</td><td>4.40 (+8%)</td><td>15.93 (+8%)</td><td>17.0M</td></tr><tr><td>GMA-Twins [25]</td><td>1.15 (+15%)</td><td>2.73 (+11%)</td><td>4.98 (+22%)</td><td>16.82 (+14%)</td><td>14.2M</td></tr><tr><td>Ours</td><td>\\mathbf{1.00}</td><td>\\mathbf{2.45}</td><td>\\mathbf{4.09}</td><td>\\mathbf{14.72}</td><td>18.2M</td></tr></tbody></table>", "caption": "Table 3:  FlowFormer v.s. GMA.Ours (small) is a small version of FlowFormer and uses the CNN image feature encoder of GMA.GMA-L is a large version of GMA.GMA-Twins replace its CNN image feature encoder with pre-trained Twins.(+x%) indicates that this model obtains x% larger error than ours.", "list_citation_info": ["[25] Jiang, S., Campbell, D., Lu, Y., Li, H., Hartley, R.: Learning to estimate hidden motions with global motion aggregation. arXiv preprint arXiv:2104.02409 (2021)"]}]}