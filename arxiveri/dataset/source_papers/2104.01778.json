{"title": "AST: Audio Spectrogram Transformer", "abstract": "In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.", "authors": ["Yuan Gong", " Yu-An Chung", " James Glass"], "pdf_url": "https://arxiv.org/abs/2104.01778", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th># Params</th><th>ImageNet</th><th>AudioSet</th></tr></thead><tbody><tr><th>ViT Base [11]</th><td>86M</td><td>0.846</td><td>0.320</td></tr><tr><th>ViT Large [11]*</th><td>307M</td><td>0.851</td><td>0.330</td></tr><tr><th>DeiT w/o Distill [12]</th><td>86M</td><td>0.829</td><td>0.330</td></tr><tr><th>DeiT w/ Distill (Used)</th><td>87M</td><td>0.852</td><td>0.347</td></tr></tbody></table>", "caption": "Table 3: Performance of AST models initialized with different ViT weights on balanced AudioSet and corresponding ViT models\u2019 top-1 accuracy on ImageNet 2012. (* Model is trained without patch split overlap due to memory limitation.)", "list_citation_info": ["[12] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou, \u201cTraining data-efficient image transformers & distillation through attention,\u201d arXiv preprint arXiv:2012.12877, 2020.", "[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021."]}, {"table": "<table><thead><tr><th></th><th>ESC-50</th><th>Speech Commands V2 (35 classes)</th></tr></thead><tbody><tr><th>SOTA-S</th><td>86.5 [33]</td><td>97.4 [34]</td></tr><tr><th>SOTA-P</th><td>94.7 [7]</td><td>97.7 [35]</td></tr><tr><th>AST-S</th><td>88.7\\pm0.7</td><td>98.11\\pm0.05</td></tr><tr><th>AST-P</th><td>95.6\\pm0.4</td><td>97.88\\pm0.03</td></tr></tbody></table>", "caption": "Table 7: Comparing AST and SOTA models on ESC-50 and Speech Commands. \u201c-S\u201d and \u201c-P\u201d denotes model trained without and with additional audio data, respectively.", "list_citation_info": ["[35] J. Lin, K. Kilgour, D. Roblek, and M. Sharifi, \u201cTraining keyword spotters with limited and synthesized speech data,\u201d in ICASSP, 2020.", "[33] H. B. Sailor, D. M. Agrawal, and H. A. Patil, \u201cUnsupervised filterbank learning using convolutional restricted boltzmann machine for environmental sound classification.\u201d in Interspeech, 2017.", "[7] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, \u201cPANNs: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE/ACM TASLP, vol. 28, pp. 2880\u20132894, 2020.", "[34] S. Majumdar and B. Ginsburg, \u201cMatchboxnet\u20131d time-channel separable convolutional neural network architecture for speech commands recognition,\u201d arXiv preprint arXiv:2004.08531, 2020."]}]}