{"title": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "abstract": "Our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We address both these challenges in this paper. We propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.", "authors": ["Max Bain", " Arsha Nagrani", " G\u00fcl Varol", " Andrew Zisserman"], "pdf_url": "https://arxiv.org/abs/2104.00650", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>E2E\\dagger</td><td>Vis Enc. Init.</td><td>Visual-Text PT</td><td>#pairs PT</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><td>JSFusion [75]</td><td>\\checkmark</td><td>-</td><td>-</td><td>-</td><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td></tr><tr><td>ActBERT [80]</td><td>\\checkmark</td><td>VisGenome</td><td>HowTo100M</td><td>136M</td><td>16.3</td><td>42.8</td><td>56.9</td><td>10.0</td></tr><tr><td>HERO [34]</td><td>\\checkmark</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><td>VidTranslate [28]</td><td>\\checkmark</td><td>IG65M</td><td>HowTo100M</td><td>136M</td><td>14.7</td><td>-</td><td>52.8</td><td></td></tr><tr><td>NoiseEst. [2]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>17.4</td><td>41.6</td><td>53.6</td><td>8.0</td></tr><tr><td>\\rowcoloraliceblue CE [38]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td></tr><tr><td>UniVL [40]</td><td>\u2717</td><td>-</td><td>HowTo100M</td><td>136M</td><td>21.2</td><td>49.6</td><td>63.1</td><td>6.0</td></tr><tr><td>ClipBERT [32]</td><td>\u2713</td><td>-</td><td>COCO, VisGenome</td><td>5.6M</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><td>AVLnet [55]</td><td>\u2717</td><td>ImageNet, Kinetics</td><td>HowTo100M</td><td>136M</td><td>27.1</td><td>55.6</td><td>66.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue MMT [21]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>HowTo100M</td><td>136M</td><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><td>\\rowcoloraliceblue T2VLAD [69]</td><td>\u2717</td><td>Numerous experts\\dagger</td><td>-</td><td></td><td>29.5</td><td>59.0</td><td>70.1</td><td>4.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>-</td><td>-</td><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td></tr><tr><td>Support Set [48]</td><td>\u2717</td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M</td><td>3M</td><td>25.5</td><td>54.5</td><td>66.1</td><td>4.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>32.5</td><td>61.5</td><td>71.2</td><td>3.0</td></tr><tr><td>Zero-shot</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HT MIL-NCE [44]</td><td>\\checkmark</td><td>-</td><td>HowTo100M</td><td>136M</td><td>7.5</td><td>21.2</td><td>29.6</td><td>38.0</td></tr><tr><td>SupportSet [48]</td><td></td><td>IG65M, ImageNet</td><td>HowTo100M</td><td>136M</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M</td><td>5.5M</td><td>23.2</td><td>44.6</td><td>56.6</td><td>7.0</td></tr><tr><td>Ours</td><td>\u2713</td><td>ImageNet</td><td>CC3M,WV-2M,COCO</td><td>6.1M</td><td>24.7</td><td>46.9</td><td>57.2</td><td>7.0</td></tr></tbody></table>", "caption": "Table 4: Comparison to state-of-the-art results on MSR-VTT for text-to-video retrieval, 1k-A split. \\daggerE2E: Works trained on pixels directly, without using pre-extracted expert features trained for other tasks. Vis Enc. Init.: Datasets used for pretraining visual encoders for tasks other than visual-text retrieval, eg object classification. Visual-Text PT: Visual-text pretraining data. Rows highlighted in blue use additional modalities such as sound and speech from the MSR-VTT test videos. \\dagger Object, Motion, Face, Scene, Speech, OCR and Sound classification features.", "list_citation_info": ["[69] Xiaohan Wang, Linchao Zhu, and Yi Yang. T2vlad: Global-local sequence alignment for text-video retrieval, 2021.", "[2] Elad Amrani, Rami Ben Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. arXiv preprint arXiv:2003.03186, 2020.", "[75] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In ECCV, 2018.", "[21] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, 2020.", "[40] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and Ming Zhou. UniVL: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020.", "[32] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. arXiv preprint arXiv:2102.06183, 2021.", "[55] Andrew Rouditchenko, Angie Boggust, David Harwath, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Rogerio Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, et al. AVLnet: Learning audio-visual language representations from instructional videos. arXiv preprint arXiv:2006.09199, 2020.", "[44] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, 2019.", "[28] Bruno Korbar, Fabio Petroni, Rohit Girdhar, and Lorenzo Torresani. Video understanding as machine translation. arXiv preprint arXiv:2006.07203, 2020.", "[34] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. EMNLP, 2020.", "[48] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020.", "[38] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In Proc. BMVC, 2019.", "[80] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In CVPR, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>R@1</th><th>R@5</th><th>R@10</th><th>MedR</th></tr></thead><tbody><tr><th>VSE [27]</th><td>12.3</td><td>30.1</td><td>42.3</td><td>14.0</td></tr><tr><th>VSE++ [20]</th><td>15.4</td><td>39.6</td><td>53.0</td><td>9.0</td></tr><tr><th>Multi. Cues [45]</th><td>20.3</td><td>47.8</td><td>61.1</td><td>6.0</td></tr><tr><th>CE [38]</th><td>19.8</td><td>49.0</td><td>63.8</td><td>6.0</td></tr><tr><th>Support Set [48]</th><td>23.0</td><td>52.8</td><td>65.8</td><td>5.0</td></tr><tr><th>Support Set [48] (HowTo PT)</th><td>28.4</td><td>60.0</td><td>72.9</td><td>4.0</td></tr><tr><th>Ours</th><td>33.7</td><td>64.7</td><td>76.3</td><td>3.0</td></tr></tbody></table>", "caption": "Table 5: Text-to-video retrieval results on the MSVD [10] test set.", "list_citation_info": ["[10] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190\u2013200, 2011.", "[20] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017.", "[45] Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit K Roy-Chowdhury. Learning joint embedding with multimodal cues for cross-modal video-text retrieval. In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, pages 19\u201327, 2018.", "[27] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel. Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.", "[38] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In Proc. BMVC, 2019.", "[48] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><th>GT prop.</th><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><th>S2VT [64]</th><th></th><td>11.9</td><td>33.6</td><td>-</td><td>13.0</td></tr><tr><th>FSE [77]</th><th></th><td>13.9</td><td>36.0</td><td>-</td><td>11.0</td></tr><tr><th>CE [38]</th><th></th><td>16.1</td><td>41.1</td><td>-</td><td>8.3</td></tr><tr><th>ClipBERT [32]</th><th>\u2713</th><td>20.4</td><td>44.5</td><td>56.7</td><td>7.0</td></tr><tr><th>Ours</th><th></th><td>31.0</td><td>59.8</td><td>72.4</td><td>3.0</td></tr><tr><th>Ours</th><th>\u2713</th><td>34.6</td><td>65.0</td><td>74.7</td><td>3.0</td></tr><tr><th>Zero-shot</th><th></th><td></td><td></td><td></td><td></td></tr><tr><th>Ours</th><th></th><td>21.1</td><td>46.0</td><td>56.2</td><td>7.0</td></tr><tr><th>Ours</th><th>\u2713</th><td>20.2</td><td>46.4</td><td>58.5</td><td>7.0</td></tr></tbody></table>", "caption": "Table 6: Text-to-video retrieval results on the DiDeMo test set. We show results with and without ground truth proposals (GT prop.) as well as with finetuning and without (zero-shot).", "list_citation_info": ["[64] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, 2014.", "[77] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In ECCV, 2018.", "[38] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In Proc. BMVC, 2019.", "[32] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. arXiv preprint arXiv:2102.06183, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>R@1</th><th>R@5</th><th>R@10</th><th>MedR</th></tr></thead><tbody><tr><th>JSFusion [75]</th><td>9.1</td><td>21.2</td><td>34.1</td><td>36.0</td></tr><tr><th>MEE [43]</th><td>9.3</td><td>25.1</td><td>33.4</td><td>27.0</td></tr><tr><th>CE [38]</th><td>11.2</td><td>26.9</td><td>34.8</td><td>25.3</td></tr><tr><th>MMT (HowTo100M) [21]</th><td>12.9</td><td>29.9</td><td>40.1</td><td>19.3</td></tr><tr><th>Ours</th><td>15.0</td><td>30.8</td><td>40.3</td><td>20.0</td></tr></tbody></table>", "caption": "Table 7: Text-to-video retrieval results on the LSMDC test set.", "list_citation_info": ["[75] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In ECCV, 2018.", "[38] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In Proc. BMVC, 2019.", "[21] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, 2020.", "[43] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Vis PT. size</th><th>R@1</th><th>R@5</th><th>R@10</th></tr></thead><tbody><tr><th>SCANM [31]</th><td>VisGenObj (3.8M)</td><td>48.6</td><td>77.7</td><td>85.2</td></tr><tr><th>IMRAM [11]</th><td>VisGenObj (3.8M)</td><td>53.9</td><td>79.4</td><td>87.2</td></tr><tr><th>SGRAF [18]</th><td>VisGenObj (3.8M)</td><td>58.5</td><td>83.0</td><td>88.8</td></tr><tr><th>Ours</th><td>CC (3.0M)</td><td>54.2</td><td>83.2</td><td>89.8</td></tr><tr><th>Ours</th><td>CC,WV-2M (5.5M)</td><td>61.0</td><td>87.5</td><td>92.7</td></tr></tbody></table>", "caption": "Table 8: Text-to-image retrieval results on the Flickr30K test set. ++ indicates additional datasets: COCO Captions, SBU Captions. VisGenObjects denotes Visual Genome object bounding box annotations used to pretrain an FRCNN object feature extractor.", "list_citation_info": ["[31] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching, 2018.", "[18] Haiwen Diao, Ying Zhang, Lin Ma, and Huchuan Lu. Similarity reasoning and filtration for image-text matching, 2021.", "[11] Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, and Jungong Han. IMRAM: Iterative matching with recurrent attention memory for cross-modal image-text retrieval, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>E2E</th><th>VT PT</th><th>R@1</th><th>R@5</th><th>MedR</th></tr></thead><tbody><tr><th>FSE</th><th></th><th></th><td>18.2</td><td>44.8</td><td>8.3</td></tr><tr><th>CE [38]</th><th></th><th></th><td>18.2</td><td>47.7</td><td>13.0</td></tr><tr><th>CLIPBERT</th><th>\u2713</th><th></th><td>21.3</td><td>49.0</td><td>6.0</td></tr><tr><th>MMT</th><th></th><th></th><td>22.7</td><td>54.2</td><td>5.0</td></tr><tr><th>SupportSet [48]</th><th></th><th></th><td>26.8</td><td>58.1</td><td>3.0</td></tr><tr><th>MMT [21]</th><th></th><th>HowTo</th><td>28.7</td><td>61.4</td><td>3.0</td></tr><tr><th>SupportSet [48]</th><th></th><th>HowTo</th><td>29.2</td><td>61.6</td><td>3.0</td></tr><tr><th>Ours</th><th>\u2713</th><th>CC,WebVid-2M</th><td>28.8</td><td>60.9</td><td>3.0</td></tr></tbody></table>", "caption": "Table 10: Text-to-video retrieval results on the ActivityNet val1k set. R@k: Recall@K. MedR: Median Rank.", "list_citation_info": ["[38] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In Proc. BMVC, 2019.", "[21] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, 2020.", "[48] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824, 2020."]}, {"table": "<table><thead><tr><th>Attention Method</th><th>R@1</th><th>R@10</th><th>MedR</th></tr></thead><tbody><tr><th>Divided Space-Time [32]</th><td>13.0</td><td>40.2</td><td>18.0</td></tr><tr><th>Ours</th><td>14.6</td><td>42.7</td><td>16.0</td></tr></tbody></table>", "caption": "Table 13: Space-time attention method: Zero-shot results are presented on 1K-A MSR-VTT test set for text-video retrieval. The models were trained on WebVid-2M.", "list_citation_info": ["[32] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. arXiv preprint arXiv:2102.06183, 2021."]}]}