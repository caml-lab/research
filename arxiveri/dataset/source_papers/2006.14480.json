{"title": "One thousand and one hours: Self-driving motion prediction dataset", "abstract": "Motivated by the impact of large-scale datasets on ML systems we present the largest self-driving dataset for motion prediction to date, containing over 1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles along a fixed route in Palo Alto, California, over a four-month period. It consists of 170,000 scenes, where each scene is 25 seconds long and captures the perception output of the self-driving system, which encodes the precise positions and motions of nearby vehicles, cyclists, and pedestrians over time. On top of this, the dataset contains a high-definition semantic map with 15,242 labelled elements and a high-definition aerial view over the area. We show that using a dataset of this size dramatically improves performance for key self-driving problems. Combined with the provided software kit, this collection forms the largest and most detailed dataset to date for the development of self-driving machine learning tasks, such as motion forecasting, motion planning and simulation. The full dataset is available at http://level5.lyft.com/.", "authors": ["John Houston", " Guido Zuidhof", " Luca Bergamini", " Yawei Ye", " Long Chen", " Ashesh Jain", " Sammy Omari", " Vladimir Iglovikov", " Peter Ondruska"], "pdf_url": "https://arxiv.org/abs/2006.14480", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Name</th><th>Size</th><th>Scenes</th><td>Map</td><td>Annotations</td><td>Task</td></tr><tr><th rowspan=\"2\">KITTI [1]</th><th rowspan=\"2\">6h</th><th rowspan=\"2\">50</th><td rowspan=\"2\">None</td><td>3D bounding</td><td rowspan=\"2\">Perception</td></tr><tr><td>boxes</td></tr><tr><th>Oxford RobotCar [8]</th><th>71h</th><th>100</th><td>None</td><td>-</td><td>Perception</td></tr><tr><th rowspan=\"2\">Waymo Open Dataset [9]</th><th rowspan=\"2\">10h</th><th rowspan=\"2\">1000</th><td rowspan=\"2\">None</td><td>3D bounding</td><td rowspan=\"2\">Perception</td></tr><tr><td>boxes</td></tr><tr><th>ApolloScape Scene</th><th rowspan=\"2\">2h</th><th rowspan=\"2\">-</th><td rowspan=\"2\">None</td><td>3D bounding</td><td rowspan=\"2\">Perception</td></tr><tr><th>Parsing [10]</th><td>boxes</td></tr><tr><th>Argoverse 3D Tracking</th><th rowspan=\"2\">1h</th><th rowspan=\"2\">113</th><td>Lane center lines,</td><td>3D bounding</td><td rowspan=\"2\">Perception</td></tr><tr><th>v1.1 [2]</th><td>lane connectivity</td><td>boxes</td></tr><tr><th>Lyft Perception Dataset</th><th rowspan=\"2\">2.5h</th><th rowspan=\"2\">366</th><td>Rasterised</td><td>3D bounding</td><td rowspan=\"2\">Perception</td></tr><tr><th>[3]</th><td>road geometry</td><td>boxes</td></tr><tr><th rowspan=\"2\">nuScenes [11]</th><th rowspan=\"2\">6h</th><th rowspan=\"2\">1000</th><td>Rasterised</td><td>3D bounding</td><td>Perception,</td></tr><tr><td>road geometry</td><td>boxes, trajectories</td><td>Prediction</td></tr><tr><th>ApolloScape Trajectory</th><th rowspan=\"2\">2h</th><th rowspan=\"2\">103</th><td rowspan=\"2\">None</td><td rowspan=\"2\">Trajectories</td><td rowspan=\"2\">Prediction</td></tr><tr><th>[12]</th></tr><tr><th>Argoverse Forecasting</th><th rowspan=\"2\">320h</th><th rowspan=\"2\">324k</th><td>Lane center lines,</td><td rowspan=\"2\">Trajectories</td><td rowspan=\"2\">Prediction</td></tr><tr><th>v1.1 [2]</th><td>lane connectivity</td></tr><tr><th rowspan=\"4\">Ours</th><th rowspan=\"4\">1,118h</th><th rowspan=\"4\">170k</th><td>Road geometry,</td><td rowspan=\"4\">Trajectories</td><td></td></tr><tr><td>aerial map,</td><td>Prediction,</td></tr><tr><td>crosswalks,</td><td>Planning</td></tr><tr><td>traffic lights state, \u2026</td><td></td></tr></tbody></table>", "caption": "Table 1: A comparison of various self-driving datasets available today. Our dataset surpasses all others in terms of size, as well as level of detail of the semantic map (see Section 3).", "list_citation_info": ["Wang et al. [2019] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang. The apolloscape open dataset for autonomous driving and its application. Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019.", "Chang et al. [2019] M. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett, D. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays. Argoverse: 3d tracking and forecasting with rich maps. Int. Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.", "Kesten et al. [2019] R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira, M. Yuan, B. Low, A. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C. Tao, L. Platinsky, W. Jiang, and V. Shet. Lyft level 5 av dataset 2019. 2019.", "Maddern et al. [2017] W. Maddern, G. Pascoe, C. Linegar, and P. Newman. 1 year, 1000km: The oxford robotcar dataset. Int. Journal of Robotics Research (IJRR), 2017.", "Geiger et al. [2013] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. Int. Journal of Robotics Research (IJRR), 2013.", "Caesar et al. [2019] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom. nuscenes: A multimodal dataset for autonomous driving. arXiv preprint arXiv:1903.11027, 2019.", "Sun et al. [2019] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. 2019.", "Ma et al. [2019] Y. Ma, X. Zhu, S. Zhang, R. Yang, W. Wang, and D. Manocha. Trafficpredict: Trajectory prediction for heterogeneous traffic-agents. AAAI Conference on Artificial Intelligence, 2019."]}]}