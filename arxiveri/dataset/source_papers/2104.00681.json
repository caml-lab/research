{"title": "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video", "abstract": "We present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both accuracy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time.", "authors": ["Jiaming Sun", " Yiming Xie", " Linghao Chen", " Xiaowei Zhou", " Hujun Bao"], "pdf_url": "https://arxiv.org/abs/2104.00681", "list_table_and_caption": [{"table": "<table><tr><td> Method</td><td>Layer</td><td>Comp \\downarrow</td><td>Acc \\downarrow</td><td>Recall \\uparrow</td><td>Prec \\uparrow</td><td>F-score \\uparrow</td><td>Time (ms) \\downarrow</td></tr><tr><td>MVDepthNet [48]</td><td>single</td><td>0.040</td><td>0.240</td><td>0.831</td><td>0.208</td><td>0.329</td><td>48</td></tr><tr><td>GPMVS [13]</td><td>single</td><td>0.031</td><td>0.879</td><td>0.871</td><td>0.188</td><td>0.304</td><td>51</td></tr><tr><td>DPSNet [14]</td><td>single</td><td>0.045</td><td>0.284</td><td>0.793</td><td>0.223</td><td>0.344</td><td>322</td></tr><tr><td>COLMAP [37]</td><td>single</td><td>0.069</td><td>0.135</td><td>0.634</td><td>0.505</td><td>0.558</td><td>2076</td></tr><tr><td>Ours</td><td>single</td><td>0.128</td><td>0.054</td><td>0.479</td><td>0.684</td><td>0.562</td><td>30</td></tr><tr><td>Atlas [30]</td><td>double</td><td>0.062</td><td>0.128</td><td>0.732</td><td>0.382</td><td>0.499</td><td>292</td></tr><tr><td>Ours</td><td>double</td><td>0.106</td><td>0.073</td><td>0.609</td><td>0.450</td><td>0.516</td><td>30</td></tr><tr><td> DeepV2D [44]</td><td>single</td><td>0.057</td><td>0.239</td><td>0.646</td><td>0.329</td><td>0.431</td><td>347</td></tr><tr><td>Consistent Depth [28]</td><td>single</td><td>0.091</td><td>0.344</td><td>0.461</td><td>0.266</td><td>0.331</td><td>2321</td></tr><tr><td>Ours</td><td>single</td><td>0.120</td><td>0.062</td><td>0.428</td><td>0.592</td><td>0.494</td><td>30</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: 3D geometry metrics on ScanNet.We use two different training/validation splits following Atlas [30] (top block) and BA-Net [42] (bottom block).We elaborate the meaning of the single and double layer in the supplementary material.", "list_citation_info": ["[37] Johannes L. Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise View Selection for Unstructured Multi-View Stereo. In ECCV. 2016.", "[30] Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End-to-End 3D Scene Reconstruction from Posed Images. In ECCV, 2020.", "[48] Kaixuan Wang and Shaojie Shen. MVDepthNet: Real-Time Multiview Depth Estimation Neural Network. In 3DV, 2018.", "[28] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent Video Depth Estimation. ACM TOG, 2020.", "[42] Chengzhou Tang and Ping Tan. BA-Net: Dense Bundle Adjustment Networks. In ICLR, 2019.", "[13] Yuxin Hou, Juho Kannala, and Arno Solin. Multi-view stereo by temporal nonparametric fusion. In ICCV, 2019.", "[44] Zachary Teed and Jia Deng. DeepV2D: Video to Depth with Differentiable Structure from Motion. In ICLR, 2020.", "[14] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: End-to-end Deep Plane Sweep Stereo. In ICLR, 2019."]}, {"table": "<table><tr><td> Method</td><td>Abs Rel \\downarrow</td><td>Abs Diff \\downarrow</td><td>Sq Rel \\downarrow</td><td>RMSE \\downarrow</td><td>\\delta&lt;1.25 \\uparrow</td><td>Comp \\uparrow</td></tr><tr><td>COLMAP [37]</td><td>0.137</td><td>0.264</td><td>0.138</td><td>0.502</td><td>83.4</td><td>0.871</td></tr><tr><td>MVDepthNet [48]</td><td>0.098</td><td>0.191</td><td>0.061</td><td>0.293</td><td>89.6</td><td>0.928</td></tr><tr><td>GPMVS [13]</td><td>0.130</td><td>0.239</td><td>0.339</td><td>0.472</td><td>90.6</td><td>0.928</td></tr><tr><td>DPSNet [14]</td><td>0.087</td><td>0.158</td><td>0.035</td><td>0.232</td><td>92.5</td><td>0.928</td></tr><tr><td>Atlas [30]</td><td>0.065</td><td>0.123</td><td>0.045</td><td>0.251</td><td>93.6</td><td>0.999</td></tr><tr><td>Ours</td><td>0.065</td><td>0.106</td><td>0.031</td><td>0.195</td><td>94.8</td><td>0.909</td></tr><tr><td> Method</td><td>Abs Rel \\downarrow</td><td>Sq Rel \\downarrow</td><td>RMSE \\downarrow</td><td>RMSE log \\downarrow</td><td>Sc Inv \\downarrow</td><td>-</td></tr><tr><td>DeMoN [45]</td><td>0.231</td><td>0.520</td><td>0.761</td><td>0.289</td><td>0.284</td><td>-</td></tr><tr><td>BA-Net [42]</td><td>0.161</td><td>0.092</td><td>0.346</td><td>0.214</td><td>0.184</td><td>-</td></tr><tr><td>DeepV2D [44]</td><td>0.057</td><td>0.010</td><td>0.168</td><td>0.080</td><td>0.077</td><td>-</td></tr><tr><td>Consistent Depth [28]</td><td>0.073</td><td>0.037</td><td>0.217</td><td>0.105</td><td>0.103</td><td>-</td></tr><tr><td>Ours</td><td>0.047</td><td>0.024</td><td>0.164</td><td>0.093</td><td>0.092</td><td>-</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 2: 2D depth metrics on ScanNet.We use two different training/validation splits following Atlas [30] (top block) and BA-Net [42] (bottom block).", "list_citation_info": ["[37] Johannes L. Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise View Selection for Unstructured Multi-View Stereo. In ECCV. 2016.", "[30] Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay Badrinarayanan, and Andrew Rabinovich. Atlas: End-to-End 3D Scene Reconstruction from Posed Images. In ECCV, 2020.", "[48] Kaixuan Wang and Shaojie Shen. MVDepthNet: Real-Time Multiview Depth Estimation Neural Network. In 3DV, 2018.", "[45] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. DeMoN: Depth and Motion Network for Learning Monocular Stereo. In CVPR, 2017.", "[42] Chengzhou Tang and Ping Tan. BA-Net: Dense Bundle Adjustment Networks. In ICLR, 2019.", "[28] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent Video Depth Estimation. ACM TOG, 2020.", "[13] Yuxin Hou, Juho Kannala, and Arno Solin. Multi-view stereo by temporal nonparametric fusion. In ICCV, 2019.", "[44] Zachary Teed and Jia Deng. DeepV2D: Video to Depth with Differentiable Structure from Motion. In ICLR, 2020.", "[14] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: End-to-end Deep Plane Sweep Stereo. In ICLR, 2019."]}, {"table": "<table><tr><td> Method</td><td>Comp \\downarrow</td><td>Acc \\downarrow</td><td>Recall \\uparrow</td><td>Prec \\uparrow</td><td>F-score \\uparrow</td></tr><tr><td>DeepV2D [44]</td><td>0.180</td><td>0.518</td><td>0.175</td><td>0.087</td><td>0.115</td></tr><tr><td>CNMNet [26]</td><td>0.150</td><td>0.398</td><td>0.246</td><td>0.111</td><td>0.149</td></tr><tr><td>Ours</td><td>0.228</td><td>0.100</td><td>0.227</td><td>0.389</td><td>0.282</td></tr><tr><td> Method</td><td>\\delta&lt;1.25\\uparrow</td><td>Abs Rel \\downarrow</td><td>Sq Rel \\downarrow</td><td>RMSE \\downarrow</td><td>Time \\downarrow</td></tr><tr><td>DeMoN [45]</td><td>31.88</td><td>0.3888</td><td>0.4198</td><td>0.8549</td><td>110</td></tr><tr><td>MVSNet [53]</td><td>64.09</td><td>0.2339</td><td>0.1904</td><td>0.5078</td><td>1050</td></tr><tr><td>N-RGBD [24]</td><td>69.26</td><td>0.1758</td><td>0.1123</td><td>0.4408</td><td>202</td></tr><tr><td>MVDNet [48]</td><td>71.79</td><td>0.1925</td><td>0.2350</td><td>0.4585</td><td>48</td></tr><tr><td>DPSNet [14]</td><td>70.96</td><td>0.1991</td><td>0.1420</td><td>0.4382</td><td>322</td></tr><tr><td>DeepV2D [44]</td><td>42.80</td><td>0.4370</td><td>0.5530</td><td>0.8690</td><td>347</td></tr><tr><td>CNMNet [26]</td><td>76.64</td><td>0.1612</td><td>0.0832</td><td>0.3614</td><td>80</td></tr><tr><td>Ours</td><td>82.00</td><td>0.1550</td><td>0.1040</td><td>0.3470</td><td>30</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 3: 3D geometry metrics (top block) and 2D depth metrics (bottom block) on 7-Scenes. Time is measured in milliseconds.", "list_citation_info": ["[24] Chao Liu, Jinwei Gu, Kihwan Kim, Srinivasa G Narasimhan, and Jan Kautz. Neural RGB->>> D Sensing: Depth and uncertainty from a video camera. In CVPR, 2019.", "[53] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. MVSNet: Depth Inference for Unstructured Multi-View Stereo. In ECCV, 2018.", "[26] Xiaoxiao Long, Lingjie Liu, Christian Theobalt, and Wenping Wang. Occlusion-Aware Depth Estimation with Adaptive Normal Constraints. In ECCV, 2020.", "[48] Kaixuan Wang and Shaojie Shen. MVDepthNet: Real-Time Multiview Depth Estimation Neural Network. In 3DV, 2018.", "[45] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and Thomas Brox. DeMoN: Depth and Motion Network for Learning Monocular Stereo. In CVPR, 2017.", "[44] Zachary Teed and Jia Deng. DeepV2D: Video to Depth with Differentiable Structure from Motion. In ICLR, 2020.", "[14] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: End-to-end Deep Plane Sweep Stereo. In ICLR, 2019."]}]}