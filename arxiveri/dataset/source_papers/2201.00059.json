{"title": "icaps: Iterative Category-Level Object Pose and Shape Estimation", "abstract": "This paper proposes a category-level 6D object pose and shape estimation approach iCaps, which allows tracking 6D poses of unseen objects in a category and estimating their 3D shapes. We develop a category-level auto-encoder network using depth images as input, where feature embeddings from the auto-encoder encode poses of objects in a category. The auto-encoder can be used in a particle filter framework to estimate and track 6D poses of objects in a category. By exploiting an implicit shape representation based on signed distance functions, we build a LatentNet to estimate a latent representation of the 3D shape given the estimated pose of an object. Then the estimated pose and shape can be used to update each other in an iterative way. Our category-level 6D object pose and shape estimation pipeline only requires 2D detection and segmentation for initialization. We evaluate our approach on a publicly available dataset and demonstrate its effectiveness. In particular, our method achieves comparably high accuracy on shape estimation.", "authors": ["Xinke Deng", " Junyi Geng", " Timothy Bretl", " Yu Xiang", " Dieter Fox"], "pdf_url": "https://arxiv.org/abs/2201.00059", "list_table_and_caption": [{"table": "<p>MethodNOCS [11]CASS [12]FS-Net [13]OursICP [46]6Pack [14]CAPTRA [15]BundleTrack [16]OursInputRGBDRGBDRGBDRGBDDepthRGBDRGBDRGBDRGBDSettingSingle frameTrackingInitialization---2D det. &amp; seg.6D GTPert 6D GTPert 6D GTPert 6D GT2D det. &amp; seg.bottle5^{\\circ}5\\text{cm}5.518.5642.1916.0510.124.579.4686.516.95IoU2548.784.94-99.0529.991.1-100100Rot. err. (deg)25.614.6-51.414815.63.291.69.72Trans. err. (cm)14.418.39-2.2515.742.62.32.2bowl5^{\\circ}5\\text{cm}62.244.4659.1656.5640.35579.299.670.37IoU2599.695.58-99.8679.7100-99.999.94Rot. err. (deg)4.75.05-13.89195.23.51.75.49Trans. err. (cm)1.23.46-1.74.71.71.432.11.49camera5^{\\circ}5\\text{cm}0.60.441.769.212.610.10.4185.89.32IoU2590.688.93-98.953.187.6-99.9100Rot. err. (deg)33.827.61-48.7180.535.717.82313.69Trans. err. (cm)3.16.09-3.1212.25.635.532.12.72can5^{\\circ}5\\text{cm}7.132.9840.5519.8717.222.664.799.245IoU257789.94-95.140.592.6-10098.32Rot. err. (deg)16.98.85-53.2347.113.93.431.58.52Trans. err. (cm)45.5-1.899.44.85.692.12.64laptop5^{\\circ}5\\text{cm}25.538.2216.599.4614.863.594.0399.926.07IoU2594.799.93-75.3950.998.1-99.999.5Rot. err. (deg)8.68.25-11.1837.74.72.241.58.76Trans. err. (cm)2.43.5-3.189.22.51.482.23.19mug5^{\\circ}5\\text{cm}0.91.458.7422.566.224.155.1753.621.82IoU2582.882.19-99.9627.795.2-99.9100Rot. err. (deg)31.529.49-38.2456.321.35.365.210.69Trans. err. (cm)415.04-1.659.22.30.792.21.31overall5^{\\circ}5\\text{cm}1723.528.222.2816.933.362.1687.431.59IoU2582.284.295.194.634794.2-99.999.63Rot. err. (deg)20.215.64-36.1148.1165.942.49.48Trans. err. (cm)4.98.66-2.310.53.57.922.12.26\u2022{}^{\\dagger}For tracking, BundleTrack, 6-PACK, ICP require gound truth (GT) 6D pose for initialization. CAPTRA perturbed (Pert) the GT 6D pose for initialization. Our method only requires 2D detection and segmentation for particle sampling, indicating its effectiveness when the initial 6D pose is not available.\u2022{}^{\\dagger}The notation \u201c-\u2018\u2019 means that the corresponding metric was not reported.</p>", "caption": "TABLE I: Category-level 6D object pose estimation results on NOCS-REAL275 dataset.", "list_citation_info": ["[12] D. Chen, J. Li, Z. Wang, and K. Xu, \u201cLearning canonical shape space for category-level 6D object pose and size estimation,\u201d in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11\u2009973\u201311\u2009982.", "[15] Y. Weng, H. Wang, Q. Zhou, Y. Qin, Y. Duan, Q. Fan, B. Chen, H. Su, and L. J. Guibas, \u201cCaptra: Category-level pose tracking for rigid and articulated objects from point clouds,\u201d arXiv preprint arXiv:2104.03437, 2021.", "[11] H. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas, \u201cNormalized object coordinate space for category-level 6D object pose and size estimation,\u201d in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit., 2019, pp. 2642\u20132651.", "[16] B. Wen and K. Bekris, \u201cBundletrack: 6d pose tracking for novel objects without instance or category-level 3d models,\u201d Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2021.", "[13] W. Chen, X. Jia, H. J. Chang, J. Duan, L. Shen, and A. Leonardis, \u201cFs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism,\u201d in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit., 2021, pp. 1581\u20131590.", "[14] C. Wang, R. Mart\u00edn-Mart\u00edn, D. Xu, J. Lv, C. Lu, L. Fei-Fei, S. Savarese, and Y. Zhu, \u201c6-pack: Category-level 6D pose tracker with anchor-based keypoints,\u201d in Proc. IEEE Int. Conf. Robot. Autom., 2020.", "[46] Q.-Y. Zhou, J. Park, and V. Koltun, \u201cOpen3D: A modern library for 3D data processing,\u201d arXiv:1801.09847, 2018."]}, {"table": "<p>CASS[12]Shape-Prior [18]FS-Net [13]Oursbottle0.753.441.20.113bowl0.381.210.390.043camera0.778.890.440.272can0.421.560.620.237laptop3.732.912.230.882mug0.321.020.290.131overall1.063.170.860.237</p>", "caption": "TABLE II: Evaluation of point cloud reconstruction accuracy with Chamfer Distance (CD) (\\times 10^{-3}) metric.", "list_citation_info": ["[18] M. Tian, M. H. Ang, and G. H. Lee, \u201cShape prior deformation for categorical 6d object pose and size estimation,\u201d in Proc. Eur. Conf. Comput. Vis. Springer, 2020, pp. 530\u2013546.", "[12] D. Chen, J. Li, Z. Wang, and K. Xu, \u201cLearning canonical shape space for category-level 6D object pose and size estimation,\u201d in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11\u2009973\u201311\u2009982.", "[13] W. Chen, X. Jia, H. J. Chang, J. Duan, L. Shen, and A. Leonardis, \u201cFs-net: Fast shape-based network for category-level 6d object pose estimation with decoupled rotation mechanism,\u201d in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit., 2021, pp. 1581\u20131590."]}]}