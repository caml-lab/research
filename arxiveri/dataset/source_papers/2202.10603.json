{"title": "Disentangling light fields for super-resolution and disparity estimation", "abstract": "Light field (LF) cameras record both intensity and directions of light rays, and encode 3D scenes into 4D LF images. Recently, many convolutional neural networks (CNNs) have been proposed for various LF image processing tasks. However, it is challenging for CNNs to effectively process LF images since the spatial and angular information are highly inter-twined with varying disparities. In this paper, we propose a generic mechanism to disentangle these coupled information for LF image processing. Specifically, we first design a class of domain-specific convolutions to disentangle LFs from different dimensions, and then leverage these disentangled features by designing task-specific modules. Our disentangling mechanism can well incorporate the LF structure prior and effectively handle 4D LF data. Based on the proposed mechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp) for spatial super-resolution, angular super-resolution and disparity estimation. Experimental results show that our networks achieve state-of-the-art performance on all these three tasks, which demonstrates the effectiveness, efficiency, and generality of our disentangling mechanism. Project page: https://yingqianwang.github.io/DistgLF/.", "authors": ["Yingqian Wang", " Longguang Wang", " Gaochang Wu", " Jungang Yang", " Wei An", " Jingyi Yu", " Yulan Guo"], "pdf_url": "https://arxiv.org/abs/2202.10603", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"5\">2\\times</td><td colspan=\"5\">4\\times</td></tr><tr><td><p>EPFL</p></td><td><p>HCInew</p></td><td><p>HCIold</p></td><td><p>INRIA</p></td><td><p>STFgantry</p></td><td><p>EPFL</p></td><td><p>HCInew</p></td><td><p>HCIold</p></td><td><p>INRIA</p></td><td><p>STFgantry</p></td></tr><tr><td>Bicubic</td><td><p>29.50/0.935</p></td><td><p>31.69/0.934</p></td><td><p>37.46/0.978</p></td><td><p>31.10/0.956</p></td><td><p>30.82/0.947</p></td><td><p>25.14/0.831</p></td><td><p>27.61/0.851</p></td><td><p>32.42/0.934</p></td><td><p>26.82/0.886</p></td><td><p>25.93/0.843</p></td></tr><tr><td>VDSR [79]</td><td><p>32.50/0.960</p></td><td><p>34.37/0.956</p></td><td><p>40.61/0.987</p></td><td><p>34.43/0.974</p></td><td><p>35.54/0.979</p></td><td><p>27.25/0.878</p></td><td><p>29.31/0.883</p></td><td><p>34.81/0.952</p></td><td><p>29.19/0.921</p></td><td><p>28.51/0.901</p></td></tr><tr><td>EDSR [39]</td><td><p>33.09/0.963</p></td><td><p>34.83/0.960</p></td><td><p>41.01/0.988</p></td><td><p>34.97/0.977</p></td><td><p>36.29/0.982</p></td><td><p>27.84/0.886</p></td><td><p>29.60/0.887</p></td><td><p>35.18/0.954</p></td><td><p>29.66/0.926</p></td><td><p>28.70/0.908</p></td></tr><tr><td>RCAN [74]</td><td><p>33.16/0.964</p></td><td><p>34.98/0.960</p></td><td><p>41.05/0.988</p></td><td><p>35.01/0.977</p></td><td><p>36.33/0.983</p></td><td><p>27.88/0.886</p></td><td><p>29.63/0.888</p></td><td><p>35.20/0.954</p></td><td><p>29.76/0.927</p></td><td><p>28.90/0.911</p></td></tr><tr><td>LFBM5D [33]</td><td><p>31.15/0.955</p></td><td><p>33.72/0.955</p></td><td><p>39.62/0.985</p></td><td><p>32.85/0.970</p></td><td><p>33.55/0.972</p></td><td><p>26.61/0.870</p></td><td><p>29.13/0.882</p></td><td><p>34.23/0.951</p></td><td><p>28.49/0.914</p></td><td><p>28.30/0.900</p></td></tr><tr><td>GB [34]</td><td><p>31.22/0.959</p></td><td><p>35.25/0.969</p></td><td><p>40.21/0.988</p></td><td><p>32.76/0.972</p></td><td><p>35.44/0.984</p></td><td><p>26.02/0.863</p></td><td><p>28.92/0.884</p></td><td><p>33.74/0.950</p></td><td><p>27.73/0.909</p></td><td><p>28.11/0.901</p></td></tr><tr><td>resLF [25]</td><td><p>33.62/0.971</p></td><td><p>36.69/0.974</p></td><td><p>43.42/0.993</p></td><td><p>35.39/0.981</p></td><td><p>38.36/0.990</p></td><td><p>28.27/0.904</p></td><td><p>30.73/0.911</p></td><td><p>36.71/0.968</p></td><td><p>30.34/0.941</p></td><td><p>30.19/0.937</p></td></tr><tr><td>LFSSR [44]</td><td><p>33.69/0.975</p></td><td><p>36.86/0.975</p></td><td><p>43.75/0.994</p></td><td><p>35.27/0.983</p></td><td><p>38.07/0.990</p></td><td><p>28.27/0.908</p></td><td><p>30.72/0.912</p></td><td><p>36.70/0.969</p></td><td><p>30.31/0.945</p></td><td><p>30.15/0.939</p></td></tr><tr><td>LF-ATO [40]</td><td><p>34.27/0.976</p></td><td><p>37.24/0.977</p></td><td><p>44.20/0.994</p></td><td><p>36.15/0.984</p></td><td><p>39.64/0.993</p></td><td><p>28.52/0.912</p></td><td><p>30.88/0.914</p></td><td><p>37.00/0.970</p></td><td><p>30.71/0.949</p></td><td><p>30.61/0.943</p></td></tr><tr><td>LF-InterNet [29]</td><td><p>34.14/0.976</p></td><td><p>37.28/0.977</p></td><td><p>44.45/0.995</p></td><td><p>35.80/0.985</p></td><td><p>38.72/0.992</p></td><td><p>28.67/0.914</p></td><td><p>30.98/0.917</p></td><td><p>37.11/0.972</p></td><td><p>30.64/0.949</p></td><td><p>30.53/0.943</p></td></tr><tr><td>LF-DFnet [45]</td><td><p>34.44/0.977</p></td><td><p>37.44/0.979</p></td><td><p>44.23/0.994</p></td><td><p>36.36/0.984</p></td><td><p>39.61/0.994</p></td><td><p>28.77/0.917</p></td><td><p>31.23/0.920</p></td><td><p>37.32/0.972</p></td><td><p>30.83/0.950</p></td><td><p>31.15/0.949</p></td></tr><tr><td>DistgSSR (ours)</td><td><p>34.80/0.979</p></td><td><p>37.95/0.980</p></td><td><p>44.92/0.995</p></td><td><p>36.58/0.986</p></td><td><p>40.37/0.994</p></td><td><p>28.98/0.919</p></td><td><p>31.38/0.922</p></td><td><p>37.55/0.973</p></td><td><p>30.99/0.952</p></td><td><p>31.63/0.953</p></td></tr></table>", "caption": "TABLE III: PSNR/SSIM values achieved by different methods for 2\\times and 4\\timesSR. The best results are in bold faces.", "list_citation_info": ["[33] M. Alain and A. Smolic, \u201cLight field super-resolution via lfbm5d sparse coding,\u201d in IEEE International Conference on Image Processing (ICIP), 2018, pp. 2501\u20132505.", "[40] J. Jin, J. Hou, J. Chen, and S. Kwong, \u201cLight field spatial super-resolution via deep combinatorial geometry embedding and structural consistency regularization,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 2260\u20132269.", "[39] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, \u201cEnhanced deep residual networks for single image super-resolution,\u201d in IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2017, pp. 136\u2013144.", "[25] S. Zhang, Y. Lin, and H. Sheng, \u201cResidual networks for light field image super-resolution,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 11\u2009046\u201311\u2009055.", "[45] Y. Wang, J. Yang, L. Wang, X. Ying, T. Wu, W. An, and Y. Guo, \u201cLight field image super-resolution using deformable convolution,\u201d IEEE Transactions on Image Processing, 2020.", "[74] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, \u201cImage super-resolution using very deep residual channel attention networks,\u201d in European Conference on Computer Vision (ECCV), 2018, pp. 286\u2013301.", "[44] H. W. F. Yeung, J. Hou, X. Chen, J. Chen, Z. Chen, and Y. Y. Chung, \u201cLight field spatial super-resolution using deep efficient spatial-angular separable convolution,\u201d IEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2319\u20132330, 2018.", "[29] Y. Wang, L. Wang, J. Yang, W. An, J. Yu, and Y. Guo, \u201cSpatial-angular interaction for light field image super-resolution,\u201d in European Conference on Computer Vision (ECCV), 2020.", "[79] J. Kim, J. Kwon Lee, and K. Mu Lee, \u201cAccurate image super-resolution using very deep convolutional networks,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 1646\u20131654.", "[34] M. Rossi and P. Frossard, \u201cGeometry-consistent light field super-resolution via graph-based regularization,\u201d IEEE Transactions on Image Processing, vol. 27, no. 9, pp. 4207\u20134218, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">2\\times</td><td colspan=\"3\">4\\times</td></tr><tr><td>#Param.</td><td>FLOPs</td><td>PSNR</td><td>#Param.</td><td>FLOPs</td><td>PSNR</td></tr><tr><td>EDSR [39]</td><td>38.6M</td><td>988.9G</td><td>36.04</td><td>38.9M</td><td>1017 G</td><td>30.20</td></tr><tr><td>RCAN [74]</td><td>15.3M</td><td>392.8G</td><td>36.11</td><td>15.4M</td><td>408.5G</td><td>30.27</td></tr><tr><td>LFSSR [44]</td><td>0.81M</td><td>25.70G</td><td>37.53</td><td>1.61M</td><td>128.4G</td><td>31.23</td></tr><tr><td>resLF [25]</td><td>6.35M</td><td>37.06G</td><td>37.50</td><td>6.79M</td><td>39.70G</td><td>31.25</td></tr><tr><td>LF-ATO [40]</td><td>1.51M</td><td>597.7G</td><td>38.30</td><td>1.66M</td><td>687.0G</td><td>31.54</td></tr><tr><td>LF-InterNet [29]</td><td>4.80M</td><td>47.46G</td><td>38.08</td><td>5.23M</td><td>50.10G</td><td>31.59</td></tr><tr><td>LF-DFnet [45]</td><td>3.94M</td><td>57.22G</td><td>38.42</td><td>3.99M</td><td>57.31G</td><td>31.86</td></tr><tr><td>DistgSSR_32</td><td>0.88M</td><td>16.06G</td><td>38.33</td><td>0.90M</td><td>16.40G</td><td>31.64</td></tr><tr><td>DistgSSR_64</td><td>3.53M</td><td>64.11G</td><td>38.92</td><td>3.58M</td><td>65.41G</td><td>32.11</td></tr></table>", "caption": "TABLE IV: Comparisons of the number of parameters (#Param.) and FLOPs for 2\\times and 4\\timesSR. Note that, FLOPs is calculated on an input LF with an angular resolution of 5\\times5 and a spatial resolution of 32\\times32. The PSNR scores are averaged over 5 test datasets.", "list_citation_info": ["[39] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, \u201cEnhanced deep residual networks for single image super-resolution,\u201d in IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2017, pp. 136\u2013144.", "[25] S. Zhang, Y. Lin, and H. Sheng, \u201cResidual networks for light field image super-resolution,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 11\u2009046\u201311\u2009055.", "[45] Y. Wang, J. Yang, L. Wang, X. Ying, T. Wu, W. An, and Y. Guo, \u201cLight field image super-resolution using deformable convolution,\u201d IEEE Transactions on Image Processing, 2020.", "[44] H. W. F. Yeung, J. Hou, X. Chen, J. Chen, Z. Chen, and Y. Y. Chung, \u201cLight field spatial super-resolution using deep efficient spatial-angular separable convolution,\u201d IEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2319\u20132330, 2018.", "[74] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, \u201cImage super-resolution using very deep residual channel attention networks,\u201d in European Conference on Computer Vision (ECCV), 2018, pp. 286\u2013301.", "[29] Y. Wang, L. Wang, J. Yang, W. An, J. Yu, and Y. Guo, \u201cSpatial-angular interaction for light field image super-resolution,\u201d in European Conference on Computer Vision (ECCV), 2020.", "[40] J. Jin, J. Hou, J. Chen, and S. Kwong, \u201cLight field spatial super-resolution via deep combinatorial geometry embedding and structural consistency regularization,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 2260\u20132269."]}, {"table": "<table><tr><td>Method</td><td>HCInew</td><td>HCIold</td><td>30scenes</td><td>Occlusion</td><td>Reflective</td></tr><tr><td>LFEPICNN [20]</td><td><p>26.64/0.744</p></td><td><p>31.43/0.850</p></td><td><p>33.66/0.918</p></td><td><p>32.72/0.924</p></td><td><p>34.76/0.930</p></td></tr><tr><td>ShearedEPI [22]</td><td><p>31.84/0.898</p></td><td><p>37.61/0.942</p></td><td><p>39.17/0.975</p></td><td><p>34.41/0.955</p></td><td><p>36.38/0.944</p></td></tr><tr><td>P4DCNN [53]</td><td><p>29.61/0.819</p></td><td><p>35.73/0.898</p></td><td><p>38.22/0.970</p></td><td><p>35.42/0.962</p></td><td><p>35.96/0.942</p></td></tr><tr><td>Kanlantari [47]</td><td><p>32.85/0.909</p></td><td><p>38.58/0.944</p></td><td><p>41.40/0.982</p></td><td><p>37.25/0.972</p></td><td><p>38.09/0.953</p></td></tr><tr><td>Yeung [54]</td><td><p>32.30/0.900</p></td><td><p>39.69/0.941</p></td><td><p>42.77/0.986</p></td><td><p>38.88/0.980</p></td><td><p>38.33/0.960</p></td></tr><tr><td>LFASR-geo [71]</td><td><p>34.60/0.937</p></td><td><p>40.84/0.960</p></td><td><p>42.53/0.985</p></td><td><p>38.36/0.977</p></td><td><p>38.20/0.955</p></td></tr><tr><td>FS-GAF [49]</td><td><p>37.14/0.966</p></td><td><p>41.80/0.974</p></td><td><p>42.75/0.986</p></td><td><p>38.51/0.979</p></td><td><p>38.35/0.957</p></td></tr><tr><td>DistgASR</td><td><p>34.70/0.974</p></td><td><p>42.18/0.978</p></td><td><p>43.67/0.995</p></td><td><p>39.46/0.991</p></td><td><p>39.11/0.978</p></td></tr></table><p>Note: All compared methods except LFEPICNN [20] were retrained on the same</p><p>datasets as our method.</p>", "caption": "TABLE VI: PSNR values achieved by different methods for 2\\times 2\\rightarrow 7\\times 7 angular SR. The best results are in bold faces.", "list_citation_info": ["[47] N. K. Kalantari, T.-C. Wang, and R. Ramamoorthi, \u201cLearning-based view synthesis for light field cameras,\u201d ACM Transactions on Graphics, vol. 35, no. 6, pp. 1\u201310, 2016.", "[71] J. Jin, J. Hou, H. Yuan, and S. Kwong, \u201cLearning light field angular super-resolution via a geometry-aware network,\u201d in AAAI Conference on Artificial Intelligence, 2020.", "[49] J. Jin, J. Hou, J. Chen, H. Zeng, S. Kwong, and J. Yu, \u201cDeep coarse-to-fine dense light field reconstruction with flexible sampling and geometry-aware fusion,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[20] G. Wu, M. Zhao, L. Wang, Q. Dai, T. Chai, and Y. Liu, \u201cLight field reconstruction using deep convolutional network on epi,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6319\u20136327.", "[22] G. Wu, Y. Liu, Q. Dai, and T. Chai, \u201cLearning sheared epi structure for light field reconstruction,\u201d IEEE Transactions on Image Processing, vol. 28, no. 7, pp. 3261\u20133273, 2019.", "[53] Y. Wang, F. Liu, Z. Wang, G. Hou, Z. Sun, and T. Tan, \u201cEnd-to-end view synthesis for light field imaging with pseudo 4dcnn,\u201d in European Conference on Computer Vision (ECCV), 2018, pp. 333\u2013348.", "[54] H. Wing Fung Yeung, J. Hou, J. Chen, Y. Ying Chung, and X. Chen, \u201cFast light field reconstruction with deep coarse-to-fine modeling of spatial-angular clues,\u201d in European Conference on Computer Vision (ECCV), 2018, pp. 137\u2013152."]}, {"table": "<table><tr><td rowspan=\"2\"></td><td colspan=\"4\">Backgammon</td><td colspan=\"4\">Dots</td><td colspan=\"4\">Pyramids</td><td colspan=\"4\">Stripes</td></tr><tr><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td></tr><tr><td>CAE [6]</td><td><p>3.924</p></td><td>4.313</td><td><p>17.32</p></td><td><p>6.074</p></td><td><p>12.40</p></td><td><p>42.50</p></td><td><p>83.70</p></td><td><p>5.082</p></td><td><p>1.681</p></td><td><p>7.162</p></td><td><p>27.54</p></td><td><p>0.048</p></td><td><p>7.872</p></td><td><p>16.90</p></td><td><p>39.95</p></td><td><p>3.556</p></td></tr><tr><td>PS-RF [5]</td><td><p>7.142</p></td><td><p>13.94</p></td><td><p>74.66</p></td><td><p>6.892</p></td><td><p>7.975</p></td><td><p>17.54</p></td><td><p>78.80</p></td><td><p>8.338</p></td><td>0.107</td><td><p>6.235</p></td><td><p>83.23</p></td><td><p>0.043</p></td><td><p>2.964</p></td><td><p>5.790</p></td><td><p>41.65</p></td><td><p>1.382</p></td></tr><tr><td>SPO [62]</td><td><p>3.781</p></td><td><p>8.639</p></td><td><p>49.94</p></td><td><p>4.587</p></td><td><p>16.27</p></td><td><p>35.06</p></td><td><p>58.07</p></td><td><p>5.238</p></td><td><p>0.861</p></td><td><p>6.263</p></td><td><p>79.20</p></td><td><p>0.043</p></td><td><p>14.97</p></td><td><p>15.46</p></td><td><p>21.87</p></td><td><p>6.955</p></td></tr><tr><td>OBER-crossANP [64]</td><td><p>3.413</p></td><td><p>4.952</p></td><td>13.66</td><td><p>4.700</p></td><td>0.974</td><td><p>37.66</p></td><td><p>73.13</p></td><td><p>1.757</p></td><td><p>0.364</p></td><td><p>1.130</p></td><td><p>8.171</p></td><td><p>0.008</p></td><td><p>3.065</p></td><td><p>9.352</p></td><td><p>44.72</p></td><td><p>1.435</p></td></tr><tr><td>EPN+OS+GC [82]</td><td>3.328</td><td><p>10.56</p></td><td><p>55.98</p></td><td><p>3.699</p></td><td><p>39.25</p></td><td><p>82.74</p></td><td><p>84.91</p></td><td><p>22.37</p></td><td><p>0.242</p></td><td><p>3.169</p></td><td><p>28.56</p></td><td><p>0.018</p></td><td><p>18.54</p></td><td><p>19.60</p></td><td><p>28.17</p></td><td><p>8.731</p></td></tr><tr><td>Epinet-fcn [24]</td><td><p>3.580</p></td><td><p>6.289</p></td><td><p>20.89</p></td><td><p>3.629</p></td><td><p>3.183</p></td><td><p>12.73</p></td><td><p>41.05</p></td><td><p>1.635</p></td><td><p>0.192</p></td><td><p>0.913</p></td><td><p>11.87</p></td><td><p>0.008</p></td><td>2.462</td><td>3.115</td><td>15.67</td><td><p>0.950</p></td></tr><tr><td>EPI-Shift [27]</td><td><p>22.89</p></td><td><p>40.53</p></td><td><p>70.58</p></td><td><p>12.79</p></td><td><p>43.92</p></td><td><p>53.18</p></td><td><p>74.55</p></td><td><p>13.15</p></td><td><p>1.242</p></td><td><p>7.315</p></td><td><p>40.48</p></td><td><p>0.037</p></td><td><p>22.72</p></td><td><p>47.70</p></td><td><p>78.95</p></td><td><p>1.686</p></td></tr><tr><td>EPI_ORM [69]</td><td><p>3.988</p></td><td><p>7.238</p></td><td><p>34.32</p></td><td>3.411</td><td><p>36.10</p></td><td><p>47.93</p></td><td><p>65.71</p></td><td><p>14.48</p></td><td><p>0.324</p></td><td><p>1.301</p></td><td><p>19.06</p></td><td><p>0.016</p></td><td><p>6.871</p></td><td><p>13.94</p></td><td><p>55.14</p></td><td><p>1.744</p></td></tr><tr><td>DistgDisp (ours)</td><td><p>5.824</p></td><td><p>10.54</p></td><td><p>26.17</p></td><td><p>4.712</p></td><td><p>1.826</p></td><td>4.464</td><td>25.37</td><td>1.367</td><td><p>0.108</p></td><td>0.539</td><td>4.953</td><td>0.004</td><td><p>3.913</p></td><td><p>6.885</p></td><td><p>19.25</p></td><td>0.917</td></tr><tr><td rowspan=\"2\"></td><td colspan=\"4\">Boxes</td><td colspan=\"4\">Cotton</td><td colspan=\"4\">Dino</td><td colspan=\"4\">Sideboard</td></tr><tr><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td></tr><tr><td>CAE [6]</td><td><p>17.89</p></td><td><p>40.40</p></td><td><p>72.69</p></td><td><p>8.424</p></td><td><p>3.369</p></td><td><p>15.50</p></td><td><p>59.22</p></td><td><p>1.506</p></td><td><p>4.968</p></td><td><p>21.30</p></td><td><p>61.06</p></td><td><p>0.382</p></td><td><p>9.845</p></td><td><p>26.85</p></td><td><p>56.92</p></td><td><p>0.876</p></td></tr><tr><td>PS_RF [5]</td><td><p>18.95</p></td><td><p>35.23</p></td><td><p>76.39</p></td><td><p>9.043</p></td><td><p>2.425</p></td><td><p>14.98</p></td><td><p>70.41</p></td><td><p>1.161</p></td><td><p>4.379</p></td><td><p>16.44</p></td><td><p>75.97</p></td><td><p>0.751</p></td><td><p>11.75</p></td><td><p>36.28</p></td><td><p>79.98</p></td><td><p>1.945</p></td></tr><tr><td>SPO [62]</td><td><p>15.89</p></td><td><p>29.52</p></td><td><p>73.23</p></td><td><p>9.107</p></td><td><p>2.594</p></td><td><p>13.71</p></td><td><p>69.05</p></td><td><p>1.313</p></td><td><p>2.184</p></td><td><p>16.36</p></td><td><p>69.87</p></td><td><p>0.310</p></td><td><p>9.297</p></td><td><p>28.81</p></td><td><p>73.36</p></td><td><p>1.024</p></td></tr><tr><td>OBER-crossANP [64]</td><td>10.76</td><td>17.92</td><td><p>44.96</p></td><td><p>4.750</p></td><td><p>1.108</p></td><td><p>7.722</p></td><td><p>36.79</p></td><td><p>0.555</p></td><td><p>2.070</p></td><td><p>6.161</p></td><td><p>22.76</p></td><td><p>0.336</p></td><td><p>5.671</p></td><td><p>12.48</p></td><td><p>32.79</p></td><td><p>0.941</p></td></tr><tr><td>EPN+OS+GC [82]</td><td><p>15.30</p></td><td><p>29.01</p></td><td><p>67.35</p></td><td><p>9.314</p></td><td><p>2.060</p></td><td><p>9.767</p></td><td><p>54.85</p></td><td><p>1.406</p></td><td><p>2.877</p></td><td><p>12.79</p></td><td><p>58.79</p></td><td><p>0.565</p></td><td><p>7.997</p></td><td><p>23.87</p></td><td><p>66.35</p></td><td><p>1.744</p></td></tr><tr><td>Epinet-fcn [24]</td><td><p>12.84</p></td><td><p>19.76</p></td><td><p>49.04</p></td><td><p>6.240</p></td><td><p>0.508</p></td><td><p>2.310</p></td><td><p>28.06</p></td><td><p>0.191</p></td><td>1.286</td><td>3.452</td><td><p>22.40</p></td><td><p>0.167</p></td><td><p>4.801</p></td><td><p>12.08</p></td><td><p>41.88</p></td><td><p>0.827</p></td></tr><tr><td>EPI-Shift [27]</td><td><p>25.95</p></td><td><p>44.14</p></td><td><p>74.36</p></td><td><p>9.790</p></td><td><p>2.176</p></td><td><p>10.68</p></td><td><p>46.86</p></td><td><p>0.475</p></td><td><p>5.964</p></td><td><p>22.14</p></td><td><p>64.16</p></td><td><p>0.392</p></td><td><p>11.80</p></td><td><p>36.64</p></td><td><p>73.42</p></td><td><p>1.261</p></td></tr><tr><td>EPI_ORM [69]</td><td><p>13.37</p></td><td><p>25.33</p></td><td><p>59.68</p></td><td><p>4.189</p></td><td><p>0.856</p></td><td><p>5.564</p></td><td><p>42.94</p></td><td><p>0.287</p></td><td><p>2.814</p></td><td><p>8.993</p></td><td><p>41.04</p></td><td><p>0.336</p></td><td><p>5.583</p></td><td><p>14.61</p></td><td><p>52.59</p></td><td><p>0.778</p></td></tr><tr><td>DistgDisp (ours)</td><td><p>13.31</p></td><td><p>21.13</p></td><td>41.62</td><td>3.325</td><td>0.489</td><td>1.478</td><td>7.594</td><td>0.184</td><td><p>1.414</p></td><td><p>4.018</p></td><td>20.46</td><td>0.099</td><td>4.051</td><td>9.575</td><td>28.28</td><td>0.713</td></tr><tr><td rowspan=\"2\"></td><td colspan=\"4\">Bedroom</td><td colspan=\"4\">Bicycle</td><td colspan=\"4\">Herbs</td><td colspan=\"4\">Origami</td></tr><tr><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td><td>BP07</td><td>BP03</td><td>BP01</td><td>MSE</td></tr><tr><td>CAE [6]</td><td><p>5.788</p></td><td><p>25.36</p></td><td><p>68.59</p></td><td><p>0.234</p></td><td><p>11.22</p></td><td><p>23.62</p></td><td><p>59.64</p></td><td><p>5.135</p></td><td><p>9.550</p></td><td><p>23.16</p></td><td><p>59.24</p></td><td><p>11.67</p></td><td><p>10.03</p></td><td><p>28.35</p></td><td><p>64.16</p></td><td><p>1.778</p></td></tr><tr><td>PS_RF [5]</td><td><p>6.015</p></td><td><p>22.45</p></td><td><p>80.68</p></td><td><p>0.288</p></td><td><p>17.17</p></td><td><p>32.32</p></td><td><p>79.80</p></td><td><p>7.926</p></td><td><p>10.48</p></td><td><p>21.90</p></td><td><p>66.47</p></td><td><p>15.25</p></td><td><p>13.57</p></td><td><p>36.45</p></td><td><p>80.32</p></td><td><p>2.393</p></td></tr><tr><td>SPO [62]</td><td><p>4.864</p></td><td><p>23.53</p></td><td><p>72.37</p></td><td><p>0.209</p></td><td><p>10.91</p></td><td><p>26.90</p></td><td><p>71.13</p></td><td><p>5.570</p></td><td><p>8.260</p></td><td><p>30.62</p></td><td><p>86.62</p></td><td><p>11.23</p></td><td><p>11.69</p></td><td><p>32.71</p></td><td><p>75.58</p></td><td><p>2.032</p></td></tr><tr><td>OBER-crossANP [64]</td><td><p>3.329</p></td><td><p>9.558</p></td><td><p>28.91</p></td><td><p>0.185</p></td><td>8.683</td><td>16.17</td><td><p>37.83</p></td><td><p>4.314</p></td><td><p>7.120</p></td><td><p>14.06</p></td><td><p>36.83</p></td><td><p>10.44</p></td><td><p>8.665</p></td><td><p>20.03</p></td><td><p>42.16</p></td><td><p>1.493</p></td></tr><tr><td>EPN+OS+GC [82]</td><td><p>7.543</p></td><td><p>16.76</p></td><td><p>58.93</p></td><td><p>1.188</p></td><td><p>11.60</p></td><td><p>24.86</p></td><td><p>64.10</p></td><td><p>6.411</p></td><td><p>9.190</p></td><td><p>25.72</p></td><td><p>67.13</p></td><td><p>11.58</p></td><td><p>10.75</p></td><td><p>27.09</p></td><td><p>67.35</p></td><td><p>10.09</p></td></tr><tr><td>Epinet-fcn [24]</td><td><p>2.403</p></td><td><p>6.921</p></td><td><p>33.99</p></td><td><p>0.213</p></td><td><p>9.896</p></td><td><p>18.05</p></td><td><p>46.37</p></td><td><p>4.682</p></td><td><p>12.10</p></td><td><p>28.95</p></td><td><p>62.67</p></td><td><p>9.700</p></td><td><p>5.918</p></td><td><p>14.37</p></td><td><p>45.93</p></td><td><p>1.466</p></td></tr><tr><td>EPI-Shift [27]</td><td><p>8.297</p></td><td><p>21.51</p></td><td><p>55.45</p></td><td><p>0.284</p></td><td><p>20.79</p></td><td><p>39.59</p></td><td><p>68.48</p></td><td><p>6.920</p></td><td><p>14.19</p></td><td><p>26.66</p></td><td><p>56.98</p></td><td><p>17.01</p></td><td><p>11.52</p></td><td><p>33.75</p></td><td><p>73.45</p></td><td><p>1.690</p></td></tr><tr><td>EPI_ORM [69]</td><td><p>5.492</p></td><td><p>14.66</p></td><td><p>51.02</p></td><td><p>0.298</p></td><td><p>11.12</p></td><td><p>21.20</p></td><td><p>51.22</p></td><td><p>3.489</p></td><td><p>8.515</p></td><td><p>24.60</p></td><td><p>68.79</p></td><td>4.468</td><td><p>8.661</p></td><td><p>22.95</p></td><td><p>56.57</p></td><td><p>1.826</p></td></tr><tr><td>DistgDisp (ours)</td><td>2.349</td><td>5.925</td><td>17.66</td><td>0.111</td><td><p>9.856</p></td><td><p>17.58</p></td><td>35.72</td><td>3.419</td><td>6.846</td><td>12.44</td><td>24.44</td><td><p>6.846</p></td><td>4.270</td><td>9.816</td><td>28.42</td><td>1.053</td></tr></table>", "caption": "TABLE VII: Comparative results achieved by different LF disparity estimation methods on the HCI benchmark. The best results are in bold faces.", "list_citation_info": ["[5] H.-G. Jeon, J. Park, G. Choe, J. Park, Y. Bok, Y.-W. Tai, and I. S. Kweon, \u201cDepth from a light field image with learning-based matching costs,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 2, pp. 297\u2013310, 2018.", "[69] K. Li, J. Zhang, R. Sun, X. Zhang, and J. Gao, \u201cEpi-based oriented relation networks for light field depth estimation,\u201d in British Machine Vision Conference (BMVC), 2020.", "[24] C. Shin, H.-G. Jeon, Y. Yoon, I. So Kweon, and S. Joo Kim, \u201cEpinet: A fully-convolutional neural network using epipolar geometry for depth from light field images,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4748\u20134757.", "[62] S. Zhang, H. Sheng, C. Li, J. Zhang, and Z. Xiong, \u201cRobust depth estimation for light field via spinning parallelogram operator,\u201d Computer Vision and Image Understanding, vol. 145, pp. 148\u2013159, 2016.", "[27] T. Leistner, H. Schilling, R. Mackowiak, S. Gumhold, and C. Rother, \u201cLearning to think outside the box: Wide-baseline light field depth estimation with epi-shift,\u201d in International Conference on 3D Vision (3DV), 2019, pp. 249\u2013257.", "[82] Y. Luo, W. Zhou, J. Fang, L. Liang, H. Zhang, and G. Dai, \u201cEpi-patch based convolutional neural network for depth estimation on 4d light field,\u201d in International Conference on Neural Information Processing (ICNIP), 2017, pp. 642\u2013652.", "[64] H. Schilling, M. Diebold, C. Rother, and B. J\u00e4hne, \u201cTrust your model: Light field depth estimation with inline occlusion handling,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4530\u20134538.", "[6] Williem, I. K. Park, and K. M. Lee, \u201cRobust light field depth estimation using occlusion-noise aware data costs,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484\u20132497, 2018."]}]}