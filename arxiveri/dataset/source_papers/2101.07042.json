{"title": "Claster: Clustering with reinforcement learning for zero-shot action recognition", "abstract": "Zero-shot action recognition is the task of recognizingaction classes without visual examples, only with a seman-tic embedding which relates unseen to seen classes. Theproblem can be seen as learning a function which general-izes well to instances of unseen classes without losing dis-crimination between classes. Neural networks can modelthe complex boundaries between visual classes, which ex-plains their success as supervised models. However, inzero-shot learning, these highly specialized class bound-aries may not transfer well from seen to unseen classes.In this paper we propose a centroid-based representation,which clusters visual and semantic representation, consid-ers all training samples at once, and in this way generaliz-ing well to instances from unseen classes. We optimize theclustering using Reinforcement Learning which we show iscritical for our approach to work. We call the proposedmethod CLASTER and observe that it consistently outper-forms the state-of-the-art in all standard datasets, includ-ing UCF101, HMDB51 and Olympic Sports; both in thestandard zero-shot evaluation and the generalized zero-shotlearning. Further, we show that our model performs com-petitively in the image domain as well, outperforming thestate-of-the-art in many settings.", "authors": ["Shreyank N Gowda", " Laura Sevilla-Lara", " Frank Keller", " Marcus Rohrbach"], "pdf_url": "https://arxiv.org/abs/2101.07042", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>SE</th><th>Olympics</th><th>HMDB51</th><th>UCF101</th></tr></thead><tbody><tr><th>SJE [1]</th><td>M</td><td>47.5 \\pm 14.8</td><td>-</td><td>12.0 \\pm 1.2</td></tr><tr><th>Bi-Dir GAN [29]</th><td>M</td><td>53.2 \\pm 10.5</td><td>-</td><td>24.7 \\pm 3.7</td></tr><tr><th>IAF [29]</th><td>M</td><td>54.9 \\pm 11.7</td><td>-</td><td>26.1 \\pm 2.9</td></tr><tr><th>GGM [30]</th><td>M</td><td>57.9 \\pm 14.1</td><td>-</td><td>24.5 \\pm 2.9</td></tr><tr><th>OD [26]</th><td>M</td><td>65.9 \\pm 8.1</td><td>-</td><td>38.3 \\pm 3.0</td></tr><tr><th>WGAN [43]</th><td>M</td><td>64.7 \\pm 7.5</td><td>-</td><td>37.5 \\pm 3.1</td></tr><tr><th>CLASTER (ours)</th><td>M</td><td>67.4 \\pm 7.8</td><td>-</td><td>51.8 \\pm 2.8</td></tr><tr><th>SJE [1]</th><td>W</td><td>28.6 \\pm 4.9</td><td>13.3 \\pm 2.4</td><td>9.9 \\pm 1.4</td></tr><tr><th>IAF [29]</th><td>W</td><td>39.8 \\pm 11.6</td><td>19.2 \\pm 3.7</td><td>22.2 \\pm 2.7</td></tr><tr><th>Bi-Dir GAN [29]</th><td>W</td><td>40.2 \\pm 10.6</td><td>21.3 \\pm 3.2</td><td>21.8 \\pm 3.6</td></tr><tr><th>GGM [30]</th><td>W</td><td>41.3 \\pm 11.4</td><td>20.7 \\pm 3.1</td><td>20.3 \\pm 1.9</td></tr><tr><th>WGAN [43]</th><td>W</td><td>47.1 \\pm 6.4</td><td>29.1 \\pm 3.8</td><td>25.8 \\pm 3.2</td></tr><tr><th>OD [26]</th><td>W</td><td>50.5 \\pm 6.9</td><td>30.2 \\pm 2.7</td><td>26.9 \\pm 2.8</td></tr><tr><th>PS-GNN [13]</th><td>W</td><td>61.8 \\pm 6.8</td><td>32.6 \\pm 2.9</td><td>43.0 \\pm 4.9</td></tr><tr><th>E2E [4]*</th><td>W</td><td>61.4 \\pm 5.5</td><td>33.1 \\pm 3.4</td><td>46.2 \\pm 3.8</td></tr><tr><th>CLASTER (ours)</th><td>W</td><td>63.8 \\pm 5.7</td><td>36.6 \\pm 4.6</td><td>46.7 \\pm 5.4</td></tr><tr><th>CLASTER (ours)</th><td>S</td><td>64.2 \\pm 3.3</td><td>41.8 \\pm 2.1</td><td>50.2 \\pm 3.8</td></tr><tr><th>CLASTER (ours)</th><td>C</td><td>67.7 \\pm 2.7</td><td>42.6 \\pm 2.6</td><td>52.7 \\pm 2.2</td></tr><tr><th>ER [6]</th><td>ED</td><td>60.2 \\pm 8.9</td><td>35.3 \\pm 4.6</td><td>51.8 \\pm 2.9</td></tr><tr><th>CLASTER (ours)</th><td>ED</td><td>68.4 \\pm 4.1</td><td>43.2 \\pm 1.9</td><td>53.9 \\pm 2.5</td></tr></tbody></table>", "caption": "Table 2: Results on ZSL. SE: semantic embedding, M: manual representation, W: word2vec embedding, S: sentence2vec, C: Combination of embeddings. The proposed CLASTER outperforms previous state-of-the-art across tasks and datasets. ", "list_citation_info": ["[4] Brattoli, B., Tighe, J., Zhdanov, F., Perona, P., Chalupka, K.: Rethinking zero-shot video classification: End-to-end training for realistic applications. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4613\u20134623 (2020)", "[26] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)", "[1] Akata, Z., Reed, S., Walter, D., Lee, H., Schiele, B.: Evaluation of output embeddings for fine-grained image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2927\u20132936 (2015)", "[13] Gao, J., Zhang, T., Xu, C.: Learning to model relationships for zero-shot video classification. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020)", "[30] Mishra, A., Verma, V.K., Reddy, M.S.K., Arulkumar, S., Rai, P., Mittal, A.: A generative approach to zero-shot and few-shot action recognition. In: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 372\u2013380. IEEE (2018)", "[6] Chen, S., Huang, D.: Elaborative rehearsal for zero-shot action recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13638\u201313647 (2021)", "[43] Xian, Y., Lorenz, T., Schiele, B., Akata, Z.: Feature generating networks for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5542\u20135551 (2018)", "[29] Mishra, A., Pandey, A., Murthy, H.A.: Zero-shot learning for action recognition using synthesized features. Neurocomputing 390, 117\u2013130 (2020)"]}, {"table": "<table><tbody><tr><th>Method</th><td>SE</td><td>Olympics</td><td>HMDB51</td><td>UCF101</td></tr><tr><th>Bi-Dir GAN [29]</th><td>M</td><td>44.2 \\pm 11.2</td><td>-</td><td>22.7 \\pm 2.5</td></tr><tr><th>IAF [29]</th><td>M</td><td>48.4 \\pm 7.0</td><td>-</td><td>25.9 \\pm 2.6</td></tr><tr><th>GGM [30]</th><td>M</td><td>52.4 \\pm 12.2</td><td>-</td><td>23.7 \\pm 1.2</td></tr><tr><th>WGAN [43]</th><td>M</td><td>59.9 \\pm 5.3</td><td>-</td><td>44.4 \\pm 3.0</td></tr><tr><th>OD[26]</th><td>M</td><td>66.2 \\pm 6.3</td><td>-</td><td>49.4 \\pm 2.4</td></tr><tr><th>CLASTER (ours)</th><td>M</td><td>68.8 \\pm 6.6</td><td>-</td><td>50.9 \\pm 3.2</td></tr><tr><th>IAF [29]</th><td>W</td><td>30.2 \\pm 11.1</td><td>15.6 \\pm 2.2</td><td>20.2 \\pm 2.6</td></tr><tr><th>Bi-Dir GAN [29]</th><td>W</td><td>32.2 \\pm 10.5</td><td>  7.5 \\pm 2.4</td><td>17.2 \\pm 2.3</td></tr><tr><th>SJE [1]</th><td>W</td><td>32.5 \\pm 6.7</td><td>10.5 \\pm 2.4</td><td>  8.9 \\pm 2.2</td></tr><tr><th>GGM[30]</th><td>W</td><td>42.2 \\pm 10.2</td><td>20.1 \\pm 2.1</td><td>17.5 \\pm 2.2</td></tr><tr><th>WGAN [43]</th><td>W</td><td>46.1 \\pm 3.7</td><td>32.7 \\pm 3.4</td><td>32.4 \\pm 3.3</td></tr><tr><th>PS-GNN [13]</th><td>W</td><td>52.9 \\pm 6.2</td><td>24.2 \\pm 3.3</td><td>35.1 \\pm 4.6</td></tr><tr><th>OD [26]</th><td>W</td><td>53.1 \\pm 3.6</td><td>36.1 \\pm 2.2</td><td>37.3 \\pm 2.1</td></tr><tr><th>CLASTER (ours)</th><td>W</td><td>58.1 \\pm 2.4</td><td>42.4 \\pm 3.6</td><td>42.1 \\pm 2.6</td></tr><tr><th>CLASTER (ours)</th><td>S</td><td>58.7 \\pm 3.1</td><td>47.4 \\pm 2.8</td><td>48.3 \\pm 3.1</td></tr><tr><th>CLASTER (ours)</th><td>C</td><td>69.1 \\pm 5.4</td><td>48.0 \\pm 2.4</td><td>51.3 \\pm 3.5</td></tr></tbody></table>", "caption": "Table 3: Results on GZSL. SE: semantic embedding, M: manual representation, W: word2vec embedding, S: sentence2vec, C: combination of embeddings. The seen and unseen class accuracies are listed in the supplementary material.", "list_citation_info": ["[26] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)", "[1] Akata, Z., Reed, S., Walter, D., Lee, H., Schiele, B.: Evaluation of output embeddings for fine-grained image classification. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2927\u20132936 (2015)", "[13] Gao, J., Zhang, T., Xu, C.: Learning to model relationships for zero-shot video classification. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020)", "[30] Mishra, A., Verma, V.K., Reddy, M.S.K., Arulkumar, S., Rai, P., Mittal, A.: A generative approach to zero-shot and few-shot action recognition. In: 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 372\u2013380. IEEE (2018)", "[43] Xian, Y., Lorenz, T., Schiele, B., Akata, Z.: Feature generating networks for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5542\u20135551 (2018)", "[29] Mishra, A., Pandey, A., Murthy, H.A.: Zero-shot learning for action recognition using synthesized features. Neurocomputing 390, 117\u2013130 (2020)"]}, {"table": "<table><tbody><tr><th>Pairs</th><td>Dataset</td><td>t-value</td><td>Statistical significance(p&lt;0.05)</td><td>Type</td></tr><tr><th>CLASTER and OD [26]</th><td>UCF101</td><td>-15.77</td><td>Significant, p&lt;0.00001</td><td>ZSL</td></tr><tr><th>CLASTER and WGAN [43]</th><td>UCF101</td><td>-9.08</td><td>Significant, p&lt;0.00001</td><td>ZSL</td></tr><tr><th>CLASTER and E2E [4]</th><td>UCF101</td><td>-0.67</td><td>Not Significant, p = 0.26</td><td>ZSL</td></tr><tr><th>OD [26] and WGAN [43]</th><td>UCF101</td><td>-1.70</td><td>Not Significant, p=0.12278</td><td>ZSL</td></tr><tr><th>CLASTER and OD [26]</th><td>HMDB51</td><td>-4.33</td><td>Significant, p=0.00189</td><td>ZSL</td></tr><tr><th>CLASTER and WGAN [43]</th><td>HMDB51</td><td>-5.54</td><td>Significant, p=0.00036</td><td>ZSL</td></tr><tr><th>CLASTER and E2E [4]</th><td>HMDB51</td><td>-3.77</td><td>Significant, p = 0.00219</td><td>ZSL</td></tr><tr><th>OD [26] and WGAN [43]</th><td>HMDB51</td><td>-3.71</td><td>Significant, p=0.00483</td><td>ZSL</td></tr><tr><th>CLASTER and OD [26]</th><td>Olympics</td><td>-9.06</td><td>Significant, p&lt;0.00001</td><td>ZSL</td></tr><tr><th>CLASTER and WGAN [43]</th><td>Olympics</td><td>-11.73</td><td>Significant, p&lt;0.00001</td><td>ZSL</td></tr><tr><th>CLASTER and E2E [4]</th><td>Olympics</td><td>-2.72</td><td>Significant, p = 0.012</td><td>ZSL</td></tr><tr><th>OD [26] and WGAN [43]</th><td>Olympics</td><td>-2.47</td><td>Significant, p=0.03547</td><td>ZSL</td></tr><tr><th>CLASTER and OD [26]</th><td>UCF101</td><td>-4.51</td><td>Significant, p=0.00148</td><td>GZSL</td></tr><tr><th>CLASTER and WGAN [43]</th><td>UCF101</td><td>-5.49</td><td>Significant, p=0.00039</td><td>GZSL</td></tr><tr><th>OD [26] and WGAN [43]</th><td>UCF101</td><td>-3.16</td><td>Significant, p=0.01144</td><td>GZSL</td></tr><tr><th>CLASTER and OD [26]</th><td>HMDB51</td><td>-5.08</td><td>Significant, p=0.00066</td><td>GZSL</td></tr><tr><th>CLASTER and WGAN [43]</th><td>HMDB51</td><td>-7.51</td><td>Significant, p=0.00004</td><td>GZSL</td></tr><tr><th>OD [26] and WGAN [43]</th><td>HMDB51</td><td>-5.27</td><td>Significant, p=0.00051</td><td>GZSL</td></tr><tr><th>CLASTER and OD [26]</th><td>Olympics</td><td>-5.79</td><td>Significant, p=0.00026</td><td>GZSL</td></tr><tr><th>CLASTER and WGAN [43]</th><td>Olympics</td><td>-8.39</td><td>Significant, p=0.00002</td><td>GZSL</td></tr><tr><th>OD [26] and WGAN [43]</th><td>Olympics</td><td>-6.22</td><td>Significant, p=0.00014</td><td>GZSL</td></tr></tbody></table>", "caption": "Table 5: Comparison of the t-test for different pairs of models on the same random split. Lower the value of \u2019p\u2019, higher the significance. As we can see, our results are statistically significant in comparison to both OD [26] and WGAN [43] in both ZSL and GZSL. For GZSL, OD [26] also achieves results that are significant in comparison to WGAN [43]. ", "list_citation_info": ["[4] Brattoli, B., Tighe, J., Zhdanov, F., Perona, P., Chalupka, K.: Rethinking zero-shot video classification: End-to-end training for realistic applications. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4613\u20134623 (2020)", "[26] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)", "[43] Xian, Y., Lorenz, T., Schiele, B., Akata, Z.: Feature generating networks for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5542\u20135551 (2018)"]}, {"table": "<table><thead><tr><th>Models</th><th>Setting</th><th>Olympics</th><th>HMDB51</th><th>UCF101</th></tr></thead><tbody><tr><td>Ours and WGAN [43]</td><td>ZSL</td><td>17.5 \\pm 4.5</td><td>7.0 \\pm3.8</td><td>17.4 \\pm 5.7</td></tr><tr><td>Ours and OD [26]</td><td>ZSL</td><td>13.6 \\pm 4.5</td><td>2.4 \\pm 1.6</td><td>14.3 \\pm 2.7</td></tr><tr><td>Ours and E2E [4]</td><td>ZSL</td><td>2.6 \\pm 2.8</td><td>3.7 \\pm 2.8</td><td>0.4 \\pm 1.8</td></tr><tr><td>Ours and WGAN [43]</td><td>GZSL</td><td>11.2 \\pm 4.0</td><td>9.3 \\pm 3.7</td><td>8.1 \\pm 4.4</td></tr><tr><td>Ours and OD [26]</td><td>GZSL</td><td>4.6 \\pm 2.4</td><td>5.2 \\pm 3.1</td><td>2.7 \\pm 1.8</td></tr></tbody></table>", "caption": "Table 6: Comparing the average of the difference in performance for recent state-of-the-art approaches in zero-shot and generalized zero-shot action recognition on the same splits. All results were computed using sen2vec as the embedding. We can see that we outperform recent approaches in every scenario.", "list_citation_info": ["[4] Brattoli, B., Tighe, J., Zhdanov, F., Perona, P., Chalupka, K.: Rethinking zero-shot video classification: End-to-end training for realistic applications. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4613\u20134623 (2020)", "[26] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)", "[43] Xian, Y., Lorenz, T., Schiele, B., Akata, Z.: Feature generating networks for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5542\u20135551 (2018)"]}, {"table": "<table><tbody><tr><td>Model</td><td>E</td><td colspan=\"3\">Olympics</td><td colspan=\"3\">HMDB51</td><td colspan=\"3\">UCF-101</td></tr><tr><td></td><td></td><td>u</td><td>s</td><td>H</td><td>u</td><td>s</td><td>H</td><td>u</td><td>s</td><td>H</td></tr><tr><td>WGAN [43]</td><td>A</td><td>50.8</td><td>71.4</td><td>59.4</td><td>-</td><td>-</td><td>-</td><td>30.4</td><td>83.6</td><td>44.6</td></tr><tr><td>OD [26]</td><td>A</td><td>61.8</td><td>71.1</td><td>66.1</td><td>-</td><td>-</td><td>-</td><td>36.2</td><td>76.1</td><td>49.1</td></tr><tr><td>CLASTER</td><td>A</td><td>66.2</td><td>71.7</td><td>68.8</td><td>-</td><td>-</td><td>-</td><td>40.2</td><td>69.4</td><td>50.9</td></tr><tr><td>WGAN [43]</td><td>W</td><td>35.4</td><td>65.6</td><td>46.0</td><td>23.1</td><td>55.1</td><td>32.5</td><td>20.6</td><td>73.9</td><td>32.2</td></tr><tr><td>OD [26]</td><td>W</td><td>41.3</td><td>72.5</td><td>52.6</td><td>25.9</td><td>55.8</td><td>35.4</td><td>25.3</td><td>74.1</td><td>37.7</td></tr><tr><td>CLASTER</td><td>W</td><td>49.2</td><td>71.1</td><td>58.1</td><td>35.5</td><td>52.8</td><td>42.4</td><td>30.4</td><td>68.9</td><td>42.1</td></tr><tr><td>WGAN [43]</td><td>S</td><td>36.1</td><td>66.2</td><td>46.7</td><td>28.6</td><td>57.8</td><td>38.2</td><td>27.5</td><td>74.7</td><td>40.2</td></tr><tr><td>OD [26]</td><td>S</td><td>42.9</td><td>73.5</td><td>54.1</td><td>33.4</td><td>57.8</td><td>42.3</td><td>32.7</td><td>75.9</td><td>45.7</td></tr><tr><td>CLASTER</td><td>S</td><td>49.9</td><td>71.3</td><td>58.7</td><td>42.7</td><td>53.2</td><td>47.4</td><td>36.9</td><td>69.8</td><td>48.3</td></tr><tr><td>CLASTER</td><td>C</td><td>66.8</td><td>71.6</td><td>69.1</td><td>43.7</td><td>53.3</td><td>48.0</td><td>40.8</td><td>69.3</td><td>51.3</td></tr></tbody></table>", "caption": "Table 8: Seen and unseen accuracies for CLASTER on different datasets using different embeddings. \u2019E\u2019 corresponds to the type of embedding used, wherein \u2019A\u2019, \u2019W\u2019, \u2019S\u2019 and \u2019C\u2019 refers to manual annotations, word2vec, sen2vec and combination of the embeddings respectively. \u2019u\u2019, \u2019s\u2019 and \u2019H\u2019 corresponds to average unseen accuracy, average seen accuracy and the harmonic mean of the two. All the reported results are on the same splits.", "list_citation_info": ["[26] Mandal, D., Narayan, S., Dwivedi, S.K., Gupta, V., Ahmed, S., Khan, F.S., Shao, L.: Out-of-distribution detection for generalized zero-shot action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9985\u20139993 (2019)", "[43] Xian, Y., Lorenz, T., Schiele, B., Akata, Z.: Feature generating networks for zero-shot learning. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5542\u20135551 (2018)"]}]}