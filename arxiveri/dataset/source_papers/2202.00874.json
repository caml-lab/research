{"title": "HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection", "abstract": "Audio classification is an important task of mapping audio samples into their corresponding labels. Recently, the transformer model with self-attention mechanisms has been adopted in this field. However, existing audio transformers require large GPU memories and long training time, meanwhile relying on pretrained vision models to achieve high performance, which limits the model's scalability in audio tasks. To combat these problems, we introduce HTS-AT: an audio transformer with a hierarchical structure to reduce the model size and training time. It is further combined with a token-semantic module to map final outputs into class featuremaps, thus enabling the model for the audio event detection (i.e. localization in time). We evaluate HTS-AT on three datasets of audio classification where it achieves new state-of-the-art (SOTA) results on AudioSet and ESC-50, and equals the SOTA on Speech Command V2. It also achieves better performance in event localization than the previous CNN-based models. Moreover, HTS-AT requires only 35% model parameters and 15% training time of the previous audio transformer. These results demonstrate the high performance and high efficiency of HTS-AT.", "authors": ["Ke Chen", " Xingjian Du", " Bilei Zhu", " Zejun Ma", " Taylor Berg-Kirkpatrick", " Shlomo Dubnov"], "pdf_url": "https://arxiv.org/abs/2202.00874", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td>Pretrain</td><td>#Params.</td><td>mAP</td><td>Ensemble-mAP</td></tr><tr><td>Baseline [7]</td><td>\u2717</td><td>2.6M</td><td>0.314</td><td>-</td></tr><tr><td>DeepRes [9]</td><td>\u2717</td><td>26M</td><td>0.392</td><td>-</td></tr><tr><td>PANN [11]</td><td>\u2717</td><td>81M</td><td>0.434</td><td>-</td></tr><tr><td>PSLA{}^{P} [12]</td><td>\u2713</td><td>13.6M</td><td>0.444</td><td>0.474</td></tr><tr><td>AST [14]</td><td>\u2717</td><td>87M</td><td>0.366</td><td>-</td></tr><tr><td>AST{}^{P} [14]</td><td>\u2713</td><td>87M</td><td>0.459</td><td>0.475 (0.485<sup>2</sup><sup>2</sup>2AST provides a second bigger ensemble result by using models with different patch settings, which is partially comparable with our settings.)</td></tr><tr><td>HTS-AT{}^{H}</td><td>\u2717</td><td>28.8M</td><td>0.440</td><td>-</td></tr><tr><td>HTS-AT{}^{HC}</td><td>\u2717</td><td>31M</td><td>0.453</td><td>-</td></tr><tr><td>HTS-AT{}^{HCP}</td><td>\u2713</td><td>31M</td><td>0.471</td><td>0.487</td></tr></tbody></table>", "caption": "Table 1: The mAP results on AudioSet evaluation set.", "list_citation_info": ["[14] Yuan Gong, Yu-An Chung, and James Glass, \u201cAst: Audio spectrogram transformer,\u201d in Interspeech 2021.", "[7] Jort F. Gemmeke, Daniel P. W. Ellis, and Dylan Freedman et al., \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in ICASSP 2017.", "[11] Qiuqiang Kong, Yin Cao, and Turab Iqbal et al., \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE TASLP 2020.", "[12] Yuan Gong, Yu-An Chung, and James Glass, \u201cPsla: Improving audio tagging with pretraining, sampling, labeling, and aggregation,\u201d IEEE TASLP 2021.", "[9] Logan Ford, Hao Tang, and Fran\u00e7ois Grondin et al., \u201cA deep residual network for large-scale acoustic scene analysis,\u201d in Interspeech 2019."]}, {"table": "<table><thead><tr><th>Model</th><th>ESC-50 Acc.(%)</th><th>Model</th><th>SCV2 Acc.(%)</th></tr></thead><tbody><tr><th>PANN [11]</th><td>90.5</td><th>RES-15 [21]</th><td>97.0</td></tr><tr><th>AST [14]</th><td>95.6 \\pm 0.4</td><th>AST [14]</th><td>98.1 \\pm 0.05</td></tr><tr><th>ERANN [22]</th><td>96.1</td><th>KWT-2 [23]</th><td>97.3 \\pm 0.03</td></tr><tr><th>HTS-AT</th><td>97.0 \\pm 0.2</td><th>HTS-AT</th><td>98.0 \\pm 0.03</td></tr></tbody></table>", "caption": "Table 2: The accuracy score results on ESC-50 dataset and Speech Command V2 (SCV2).", "list_citation_info": ["[14] Yuan Gong, Yu-An Chung, and James Glass, \u201cAst: Audio spectrogram transformer,\u201d in Interspeech 2021.", "[21] Roman Vygon and Nikolay Mikhaylovskiy, \u201cLearning efficient representations for keyword spotting with triplet loss,\u201d in SPECOM 2021. Springer.", "[22] Sergey Verbitskiy and Viacheslav Vyshegorodtsev, \u201cEranns: Efficient residual audio neural networks for audio pattern recognition,\u201d CoRR, vol. abs/2106.01621, 2021.", "[23] Axel Berg, Mark O\u2019Connor, and Miguel Tairum Cruz, \u201cKeyword transformer: A self-attention model for keyword spotting,\u201d in Interspeech 2021.", "[11] Qiuqiang Kong, Yin Cao, and Turab Iqbal et al., \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE TASLP 2020."]}, {"table": "<table><thead><tr><th>Model</th><th>Alarm</th><th>Blender</th><th>Cat</th><th>Dishes</th><th>Dog</th><th>Shaver</th><th>Frying</th><th>Water</th><th>Speech</th><th>Cleaner</th><th>Average</th></tr></thead><tbody><tr><th>PANN [11]</th><td>34.3</td><td>42.4</td><td>36.3</td><td>17.6</td><td>35.8</td><td>23.8</td><td>9.3</td><td>30.6</td><td>69.7</td><td>51.0</td><td>35.1</td></tr><tr><th>HTS-AT</th><td>48.6</td><td>52.9</td><td>67.7</td><td>25.0</td><td>48.0</td><td>42.9</td><td>60.3</td><td>43.0</td><td>46.8</td><td>49.1</td><td>48.4</td></tr><tr><th>HTS-AT - Ensemble</th><td>47.5</td><td>55.1</td><td>72.4</td><td>30.9</td><td>49.7</td><td>41.9</td><td>63.2</td><td>44.3</td><td>51.3</td><td>50.6</td><td>50.7</td></tr><tr><th>Zheng et al.* [24]</th><td>41.4</td><td>54.1</td><td>72.4</td><td>29.4</td><td>47.8</td><td>61.01</td><td>49.2</td><td>33.7</td><td>69.5</td><td>65.5</td><td>52.4</td></tr><tr><th>Kim et al.* [24]</th><td>34.7</td><td>59.8</td><td>71.6</td><td>40.4</td><td>47.3</td><td>26.2</td><td>61.8</td><td>32.8</td><td>64.9</td><td>66.7</td><td>50.6</td></tr><tr><th>Lu et al.* [24]</th><td>37.1</td><td>41.4</td><td>62.5</td><td>40.6</td><td>39.7</td><td>46.5</td><td>46.5</td><td>34.5</td><td>54.5</td><td>46.9</td><td>45.0</td></tr></tbody></table>", "caption": "Table 3: The event-based F1-scores of each class on the DESED test set. Models with * are from DCASE 2021 [24], which are partial references since they use extra training data and are evaluated on DESED test set and its another private subset.", "list_citation_info": ["[24] \u201cDcase 2021 challenge task 4: Sound event detection and separation in domestic environments,\u201d http://dcase.community/challenge2021.", "[11] Qiuqiang Kong, Yin Cao, and Turab Iqbal et al., \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE TASLP 2020."]}]}