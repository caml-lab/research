{"title": "Neural architecture search for image super-resolution using densely constructed search space: Deconas", "abstract": "The recent progress of deep convolutional neural networks has enabled great success in single image super-resolution (SISR) and many other vision tasks. Their performances are also being increased by deepening the networks and developing more sophisticated network structures. However, finding an optimal structure for the given problem is a difficult task, even for human experts. For this reason, neural architecture search (NAS) methods have been introduced, which automate the procedure of constructing the structures. In this paper, we expand the NAS to the super-resolution domain and find a lightweight densely connected network named DeCoNASNet. We use a hierarchical search strategy to find the best connection with local and global features. In this process, we define a complexity-based penalty for solving image super-resolution, which can be considered a multi-objective problem. Experiments show that our DeCoNASNet outperforms the state-of-the-art lightweight super-resolution networks designed by handcraft methods and existing NAS-based design.", "authors": ["Joon Young Ahn", " Nam Ik Cho"], "pdf_url": "https://arxiv.org/abs/2104.09048", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td>Params</td><td>Set 5</td><td>Set 14</td><td>B100</td><td>Uban100</td><td>Design time</td></tr><tr><th>Bicubic</th><td>-</td><td>33.66 / 0.9299</td><td>30.24 / 0.8688</td><td>29.56 / 0.8431</td><td>26.88 / 0.8403</td><td>-</td></tr><tr><th>SRCNN [6]</th><td>57K</td><td>36.66 / 0.9542</td><td>32.45 / 0.9067</td><td>31.36 / 0.8879</td><td>29.50 / 0.8946</td><td>-</td></tr><tr><th>VDSR [9]</th><td>665K</td><td>37.53 / 0.9587</td><td>33.03 / 0.9124</td><td>31.90 / 0.8960</td><td>30.76 / 0.9140</td><td>-</td></tr><tr><th>LapSRN [15]</th><td>813K</td><td>37.52 / 0.9591</td><td>33.08 / 0.9130</td><td>31.80 / 0.8950</td><td>30.41 / 0.9101</td><td>-</td></tr><tr><th>MemNet [12]</th><td>677K</td><td>37.78 / 0.9597</td><td>33.28 / 0.9142</td><td>32.08 / 0.8978</td><td>31.31 / 0.9195</td><td>-</td></tr><tr><th>SelNet [17]</th><td>970K</td><td>37.89 / 0.9598</td><td>33.61 / 0.9160</td><td>32.08 / 0.8984</td><td>- / -</td><td>-</td></tr><tr><th>CARN [14]</th><td>1,582K</td><td>37.76 / 0.9590</td><td>33.52 / 0.9166</td><td>32.09 / 0.8978</td><td>31.92 / 0.9256</td><td>-</td></tr><tr><th>MoreMNAS-A [29]</th><td>1,039K</td><td>37.63 / 0.9584</td><td>33.23 / 0.9138</td><td>31.95 / 0.8961</td><td>31.24 / 0.9187</td><td>56 GPU days</td></tr><tr><th>FALSR-A [30]</th><td>1,021K</td><td>37.82 / 0.9595</td><td>33.55 / 0.9168</td><td>32.12 / 0.8987</td><td>31.93 / 0.9256</td><td>24 GPU days</td></tr><tr><th>DeCoNASNet (ours)</th><td>1,713K</td><td>37.96 / 0.9594</td><td>33.63 / 0.9175</td><td>32.15 / 0.8986</td><td>32.03 / 0.9265</td><td>12 GPU hours</td></tr></tbody></table>", "caption": "TABLE II: Public benchmark test results (PSNR/SSIM) for \\times 2 SR. The red color means the best performance and the blue means the second best. The \u201cDesign time\u201d at the last column indicates the times taken by the NAS approaches.", "list_citation_info": ["[30] X. Chu, B. Zhang, H. Ma, R. Xu, J. Li, and Q. Li, \u201cFast, accurate and lightweight super-resolution with neural architecture search,\u201d arXiv preprint arXiv:1901.07261, 2019.", "[29] X. Chu, B. Zhang, R. Xu, and H. Ma, \u201cMulti-objective reinforced evolution in mobile neural architecture search,\u201d arXiv preprint arXiv:1901.01074, 2019.", "[17] J.-S. Choi and M. Kim, \u201cA deep convolutional neural network with selection units for super-resolution,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017, pp. 154\u2013160.", "[15] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang, \u201cDeep laplacian pyramid networks for fast and accurate super-resolution,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 624\u2013632.", "[9] J. Kim, J. Kwon Lee, and K. Mu Lee, \u201cAccurate image super-resolution using very deep convolutional networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 1646\u20131654.", "[12] Y. Tai, J. Yang, X. Liu, and C. Xu, \u201cMemnet: A persistent memory network for image restoration,\u201d in Proceedings of the IEEE international conference on computer vision, 2017, pp. 4539\u20134547.", "[14] N. Ahn, B. Kang, and K.-A. Sohn, \u201cFast, accurate, and lightweight super-resolution with cascading residual network,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 252\u2013268.", "[6] C. Dong, C. C. Loy, K. He, and X. Tang, \u201cLearning a deep convolutional network for image super-resolution,\u201d in European conference on computer vision. Springer, 2014, pp. 184\u2013199."]}]}