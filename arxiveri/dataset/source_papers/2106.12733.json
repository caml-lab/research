{"title": "Feature completion for occluded person re-identification", "abstract": "Person re-identification (reID) plays an important role in computer vision. However, existing methods suffer from performance degradation in occluded scenes. In this work, we propose an occlusion-robust block, Region Feature Completion (RFC), for occluded reID. Different from most previous works that discard the occluded regions, RFC block can recover the semantics of occluded regions in feature space. Firstly, a Spatial RFC (SRFC) module is developed. SRFC exploits the long-range spatial contexts from non-occluded regions to predict the features of occluded regions. The unit-wise prediction task leads to an encoder/decoder architecture, where the region-encoder models the correlation between non-occluded and occluded region, and the region-decoder utilizes the spatial correlation to recover occluded region features. Secondly, we introduce Temporal RFC (TRFC) module which captures the long-term temporal contexts to refine the prediction of SRFC. RFC block is lightweight, end-to-end trainable and can be easily plugged into existing CNNs to form RFCnet. Extensive experiments are conducted on occluded and commonly holistic reID benchmarks. Our method significantly outperforms existing methods on the occlusion datasets, while remains top even superior performance on holistic datasets. The source code is available at https://github.com/blue-blue272/OccludedReID-RFCnet.", "authors": ["Ruibing Hou", " Bingpeng Ma", " Hong Chang", " Xinqian Gu", " Shiguang Shan", " Xilin Chen"], "pdf_url": "https://arxiv.org/abs/2106.12733", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Backbone</td><td colspan=\"2\">Loss Function</td><td colspan=\"2\">External Clues</td><td colspan=\"4\">Occluded-DukeMTMC</td></tr><tr><td>CE</td><td>Triplet</td><td>key-points</td><td>Foreground</td><td>mAP</td><td>top-1</td><td>top-5</td><td>top-10</td></tr><tr><td>Part Aligned [50]</td><td>GoogLeNet</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>20.2</td><td>28.8</td><td>44.6</td><td>51.0</td></tr><tr><td>RFCnet</td><td>GoogLeNet</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>36.0</td><td>44.6</td><td>61.2</td><td>67.2</td></tr><tr><td>SFR [57]</td><td>ResNet50</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>32.0</td><td>42.3</td><td>60.3</td><td>67.3</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>43.4</td><td>53.2</td><td>68.5</td><td>73.4</td></tr><tr><td>Part Bilinear [58]</td><td>GoogLeNet</td><td>\\times</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>-</td><td>36.9</td><td>-</td><td>-</td></tr><tr><td>RFCnet</td><td>GoogLeNet</td><td>\\times</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>37.1</td><td>46.3</td><td>61.9</td><td>68.6</td></tr><tr><td>HACNN [51]</td><td>Inception</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>26.0</td><td>34.4</td><td>51.9</td><td>59.4</td></tr><tr><td>RFCnet</td><td>Inception</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>37.1</td><td>41.0</td><td>56.8</td><td>62.1</td></tr><tr><td>Random Erasing [59]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>30.0</td><td>40.5</td><td>59.6</td><td>66.8</td></tr><tr><td>DSR [60]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>30.4</td><td>40.8</td><td>58.2</td><td>65.2</td></tr><tr><td>Adver Occluded [61]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>32.2</td><td>44.5</td><td>-</td><td>-</td></tr><tr><td>PCB [47]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>33.7</td><td>42.6</td><td>57.1</td><td>62.9</td></tr><tr><td>PCB+RPP [47]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>35.0</td><td>46.8</td><td>61.1</td><td>67.3</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>44.8</td><td>52.4</td><td>68.3</td><td>73.4</td></tr><tr><td>FD-GAN [62]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>-</td><td>40.8</td><td>-</td><td>-</td></tr><tr><td>PGFA [2]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>37.3</td><td>51.4</td><td>68.6</td><td>74.9</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>46.1</td><td>54.6</td><td>70.3</td><td>75.9</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>54.5</td><td>63.9</td><td>77.6</td><td>82.1</td></tr></table>", "caption": "TABLE IV: Comparison with state-of-the-arts on on image occluded reID dataset, Occluded-DukeMTMC. The \u201cCE\u201d and \u201cTriplet\u201d indicate whether the methods use the cross-entropy loss and triplet loss to train. The\u201cKey-points\u201d and \u201cForeground\u201d indicate whether the methods rely on extra supervision information from human pose model and human parsing model.", "list_citation_info": ["[2] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, \u201cPose-guided feature alignment for occluded person re-identification,\u201d in IEEE International Conference on Computer Vision, pp. 542\u2013551, 2019.", "[62] Y. Ge, Z. Li, H. Zhao, G. Yin, S. Yi, and X. Wang, \u201cFd-gan: Pose-guided feature distilling gan for robust person re-identification.,\u201d in Advances in neural information processing systems, pp. 1222\u20131233, 2018.", "[47] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, \u201cBeyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline),\u201d in European Conference on Computer Vision, pp. 480\u2013496, 2018.", "[58] Y. Suh, J. Wang, S. Tang, T. Mei, and K. M. Lee, \u201cPart-aligned bilinear representations for person re-identification.,\u201d in European Conference on Computer Vision, pp. 402\u2013419, 2018.", "[59] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, \u201cRandom erasing data augmentation,\u201d in AAAI Conference on Artificial Intelligence, vol. 34, pp. 13001\u201313008, 2020.", "[51] W. Li, X. Zhu, and S. Gong, \u201cHarmonious attention network for person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 2285 \u2013 2294, 2018.", "[60] L. He, J. Liang, H. Li, and Z. Sun, \u201cDeep spatial feature reconstruction for partial person re-identification: Alignment-free approach,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 7073\u20137082, 2018.", "[57] L. He, Z. Sun, Y. Zhu, and Y. Wang., \u201cRecognizing partial biometric patterns.,\u201d arXiv preprint arXiv:1810.07399, 2018.", "[50] L. Zhao, X. Li, J. Wang, and Y. Zhuang, \u201cDeeply-learned part-aligned representations for person re-identification,\u201d in IEEE International Conference on Computer Vision, pp. 3239 \u2013 3248, 2017.", "[61] H. Huang, D. Li, Z. Zhang, X. Chen, and K. Huang, \u201cAdversarially occluded samples for person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 5098\u20135107, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Backbone</td><td colspan=\"2\">Loss Function</td><td colspan=\"2\">External Clues</td><td colspan=\"4\">Occluded-DukeMTMC-VideoReID</td></tr><tr><td>CE</td><td>Triplet</td><td>key-points</td><td>Foreground</td><td>mAP</td><td>top-1</td><td>top-5</td><td>top-10</td></tr><tr><td>RCN [38]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>62.4</td><td>60.9</td><td>83.3</td><td>88.1</td></tr><tr><td>TriNet [56]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>64.1</td><td>63.1</td><td>82.6</td><td>87.4</td></tr><tr><td>STAN [45]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>69.4</td><td>69.4</td><td>88.1</td><td>91.7</td></tr><tr><td>QAN [44]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>74.8</td><td>75.1</td><td>90.6</td><td>93.4</td></tr><tr><td>RQEN [46]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>74.9</td><td>73.5</td><td>90.5</td><td>94.1</td></tr><tr><td>VRSTC [5]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>76.7</td><td>76.9</td><td>90.3</td><td>94.2</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>90.1</td><td>90.5</td><td>98.6</td><td>98.9</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>92.0</td><td>93.0</td><td>98.6</td><td>99.1</td></tr></table>", "caption": "TABLE V: Comparison with state-of-the-arts on video occluded reID dataset, Occluded-DukeMTMC-VideoReID. For fair comparison, we reproduce the other methods which use ResNet50 as the backbone network.", "list_citation_info": ["[38] N. McLaughlin, J. M. del Rincon, and P. C. Miller, \u201cRecurrent convolutional network for video-based person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 1325\u20131334, 2016.", "[5] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, \u201cVRSTC: Occlusion-free video person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 7183\u20137192, 2019.", "[44] Y. Liu, J. Yan, and W. Ouyang, \u201cQuality aware network for set to set recognition,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 4694\u20134703, 2017.", "[46] G. Song, B. Leng, Y. Liu, C. Hetang, and S. Cai, \u201cRegion-based quality estimation network for large-scale person re-identification,\u201d in AAAI Conference on Artificial Intelligence, vol. 32, 2018.", "[56] A. Hermans, L. Beyer, and B. Leibe, \u201cIn defense of the triplet loss for person reidentification,\u201d arXiv preprint arXiv: 1703.07737, 2017.", "[45] S. Li, S. Bak, P. Carr, C. Hetang, and X. Wang., \u201cDiversity regularized spatiotemporal attention for video-based person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 369\u2013378, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Backbone</td><td colspan=\"2\">Loss Function</td><td colspan=\"2\">External Clues</td><td colspan=\"2\">Market-1501</td><td colspan=\"2\">DukeMTMC</td><td colspan=\"2\">CUHK03-NP</td><td colspan=\"2\">MSMT17</td></tr><tr><td>CE</td><td>Triplet</td><td>KP</td><td>F</td><td>mAP</td><td>top-1</td><td>mAP</td><td>top-1</td><td>mAP</td><td>top-1</td><td>mAP</td><td>top-1</td></tr><tr><td>PCB [47]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>77.4</td><td>92.3</td><td>66.1</td><td>81.8</td><td>54.2</td><td>61.3</td><td>-</td><td>-</td></tr><tr><td>AO [61]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>78.3</td><td>86.5</td><td>62.1</td><td>79.1</td><td>56.1</td><td>54.6</td><td>-</td><td>-</td></tr><tr><td>PCB+RPP [47]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>81.6</td><td>93.8</td><td>69.2</td><td>83.3</td><td>57.5</td><td>63.7</td><td>-</td><td>-</td></tr><tr><td>CASN [71]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>82.8</td><td>94.4</td><td>73.7</td><td>87.7</td><td>64.4</td><td>71.5</td><td>-</td><td>-</td></tr><tr><td>IAnet [72]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>83.1</td><td>94.4</td><td>73.4</td><td>87.1</td><td>-</td><td>-</td><td>46.8</td><td>75.5</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>84.8</td><td>94.5</td><td>76.5</td><td>87.7</td><td>69.7</td><td>73.3</td><td>51.5</td><td>76.4</td></tr><tr><td>PGFA [2]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>76.8</td><td>91.2</td><td>65.5</td><td>82.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>\\times</td><td>85.7</td><td>94.5</td><td>76.6</td><td>87.7</td><td>69.9</td><td>73.5</td><td>52.7</td><td>77.7</td></tr><tr><td>VPM [3]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>80.8</td><td>93.0</td><td>72.6</td><td>83.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BDB [73]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>84.3</td><td>94.2</td><td>72.1</td><td>86.8</td><td>69.3</td><td>72.8</td><td>-</td><td>-</td></tr><tr><td>Pymamid [74]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>88.2</td><td>95.7</td><td>79.0</td><td>89.0</td><td>74.8</td><td>78.9</td><td>-</td><td>-</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>88.2</td><td>95.1</td><td>80.1</td><td>89.5</td><td>76.5</td><td>79.9</td><td>59.2</td><td>81.7</td></tr><tr><td>FPR [4]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>86.6</td><td>95.4</td><td>78.4</td><td>88.6</td><td>72.3</td><td>76.1</td><td>-</td><td>-</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>88.6</td><td>95.1</td><td>80.6</td><td>89.9</td><td>77.3</td><td>80.9</td><td>59.3</td><td>81.9</td></tr><tr><td>DSA [27]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>87.6</td><td>95.7</td><td>74.3</td><td>86.2</td><td>73.1</td><td>78.2</td><td>-</td><td>-</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>88.7</td><td>95.1</td><td>80.5</td><td>90.0</td><td>76.5</td><td>80.0</td><td>59.6</td><td>81.9</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>89.2</td><td>95.2</td><td>80.7</td><td>90.7</td><td>78.0</td><td>81.1</td><td>60.2</td><td>82.0</td></tr></table>", "caption": "TABLE VI: Comparison with state-of-the-arts on Market-1501, DukeMTMC, CUHK03 and MSMT17 datasets. \u201cKP\u201d denotes key-points and \u201cF\u201d denotes Foreground.", "list_citation_info": ["[2] J. Miao, Y. Wu, P. Liu, Y. Ding, and Y. Yang, \u201cPose-guided feature alignment for occluded person re-identification,\u201d in IEEE International Conference on Computer Vision, pp. 542\u2013551, 2019.", "[4] L. He, Y. Wang, W. Liu, H. Zhao, Z. Sun, and J. Feng, \u201cForeground-aware pyramid reconstruction for alignment-free occluded person re-identification,\u201d in IEEE International Conference on Computer Vision, pp. 8450\u20138459, 2019.", "[3] Y. Sun, Q. Xu, Y. Li, C. Zhang, Y. Li, S. Wang, and J. Sun, \u201cPerceive where to focus: Learning visibility-aware part-level features for partial person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 393\u2013402, 2019.", "[47] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, \u201cBeyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline),\u201d in European Conference on Computer Vision, pp. 480\u2013496, 2018.", "[73] Z. Dai, M. Chen, X. Gu, S. Zhu, and P. Tan, \u201cBatch dropblock network for person re-identification and beyond,\u201d in IEEE International Conference on Computer Vision, pp. 3691\u20133701, 2019.", "[27] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, \u201cDensely semantically aligned person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 667\u2013676, 2019.", "[71] M. Zheng, S. Karanam, Z. Wu, and R. J. Radke, \u201cRe-identification with consistent attentive siamese networks,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 5735\u20135744, 2019.", "[74] F. Zheng, C. Deng, X. Sun, X. Jiang, X. Guo, Z. Yu, F. Huang, and R. Ji, \u201cPyramidal person re-identification via multi-loss dynamic training,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 8514\u20138522, 2019.", "[72] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, \u201cInteraction-and-aggregation network for person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 9317\u20139326, 2019.", "[61] H. Huang, D. Li, Z. Zhang, X. Chen, and K. Huang, \u201cAdversarially occluded samples for person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 5098\u20135107, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Backbone</td><td colspan=\"2\">Loss Function</td><td colspan=\"2\">External Clues</td><td colspan=\"2\">Mars</td><td colspan=\"2\">Duke-VideoReID</td></tr><tr><td>CE</td><td>Triplet</td><td>key-points</td><td>Foreground</td><td>mAP</td><td>top-1</td><td>mAP</td><td>top-1</td></tr><tr><td>STAN [45]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>65.8</td><td>82.3</td><td>-</td><td>-</td></tr><tr><td>EUG [9]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>67.4</td><td>80.8</td><td>78.3</td><td>83.6</td></tr><tr><td>M3D [75]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>74.1</td><td>84.4</td><td>-</td><td>-</td></tr><tr><td>Snippet [41]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>76.1</td><td>86.3</td><td>-</td><td>-</td></tr><tr><td>TAFD [76]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>78.2</td><td>87.0</td><td>-</td><td>-</td></tr><tr><td>GLTP [43]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>78.5</td><td>87.0</td><td>93.7</td><td>96.3</td></tr><tr><td>VRSTC [5]</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>82.3</td><td>88.5</td><td>93.8</td><td>95.0</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>\\times</td><td>83.1</td><td>88.6</td><td>95.5</td><td>95.6</td></tr><tr><td>COSAM [77]</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>79.9</td><td>84.9</td><td>93.7</td><td>96.2</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>\\times</td><td>85.7</td><td>90.5</td><td>96.6</td><td>96.8</td></tr><tr><td>RFCnet</td><td>ResNet50</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>86.3</td><td>90.7</td><td>97.0</td><td>97.6</td></tr></table>", "caption": "TABLE VII: Comparison with related methods on MARS and DukeMTMC-VideoReID datasets.", "list_citation_info": ["[41] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, \u201cVideo person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 1169\u20131178, 2018.", "[76] Y. Zhao, X. Shen, Z. Jin, H. Lu, and X.-s. Hua, \u201cAttribute-driven feature disentangling and temporal aggregation for video person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 4913\u20134922, 2019.", "[43] J. Li, J. Wang, Q. Tian, W. Gao, and S. Zhang, \u201cGlobal-local temporal representations for video person re-identification,\u201d in IEEE International Conference on Computer Vision, 2019.", "[5] R. Hou, B. Ma, H. Chang, X. Gu, S. Shan, and X. Chen, \u201cVRSTC: Occlusion-free video person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 7183\u20137192, 2019.", "[75] J. Li, S. Zhang, and T. Huang, \u201cMultiscale 3d convolution network for video based person reidentification,\u201d in AAAI, 2019.", "[77] A. Subramaniam, A. Nambiar, and A. Mittal, \u201cCo-segmentation inspired attention networks for video-based person re-identification,\u201d in IEEE International Conference on Computer Vision, pp. 562\u2013572, 2019.", "[45] S. Li, S. Bak, P. Carr, C. Hetang, and X. Wang., \u201cDiversity regularized spatiotemporal attention for video-based person re-identification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 369\u2013378, 2018.", "[9] Y. Wu, Y. Lin, X. Dong, Y. Yan, W. Quyang, and Y. Yang, \u201cExploit the unknown gradually: One-shot video-based person re-identification by stepwise learning,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 5177\u20135186, 2018."]}]}