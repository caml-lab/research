{"title": "Boosting light-weight depth estimation via knowledge distillation", "abstract": "Monocular depth estimation (MDE) methods are often either too computationally expensive or not accurate enough due to the trade-off between model complexity and inference performance. In this paper, we propose a lightweight network that can accurately estimate depth maps using minimal computing resources. We achieve this by designing a compact model architecture that maximally reduces model complexity. To improve the performance of our lightweight network, we adopt knowledge distillation (KD) techniques. We consider a large network as an expert teacher that accurately estimates depth maps on the target domain. The student, which is the lightweight network, is then trained to mimic the teacher's predictions. However, this KD process can be challenging and insufficient due to the large model capacity gap between the teacher and the student. To address this, we propose to use auxiliary unlabeled data to guide KD, enabling the student to better learn from the teacher's predictions. This approach helps fill the gap between the teacher and the student, resulting in improved data-driven learning. Our extensive experiments show that our method achieves comparable performance to state-of-the-art methods while using only 1% of their parameters. Furthermore, our method outperforms previous lightweight methods regarding inference accuracy, computational efficiency, and generalizability.", "authors": ["Junjie Hu", " Chenyou Fan", " Hualie Jiang", " Xiyue Guo", " Yuan Gao", " Xiangyong Lu", " Tin Lun Lam"], "pdf_url": "https://arxiv.org/abs/2105.06143", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><th>Backbone</th><td>Original set</td><td>Auxiliary set</td><td>Params (M) \\downarrow</td><td>RMSE \\downarrow</td><td>REL \\downarrow</td><td>\\log 10 \\downarrow</td><td>\\delta_{1} \\uparrow</td></tr><tr><th>Laina et al. [20]</th><th>ResNet-50</th><td>NYU-v2</td><td>\u2013</td><td>60.6</td><td>0.573</td><td>0.127</td><td>0.055</td><td>0.811</td></tr><tr><th>Hu et al. [16]</th><th>ResNet-50</th><td>NYU-v2</td><td>\u2013</td><td>63.6</td><td>0.555</td><td>0.126</td><td>0.054</td><td>0.843</td></tr><tr><th>Zhang et al. [48]</th><th>ResNet-50</th><td>NYU-v2</td><td>\u2013</td><td>95.4</td><td>0.497</td><td>0.121</td><td>-</td><td>0.846</td></tr><tr><th>Fu et al. [10]</th><th>ResNet-101</th><td>NYU-v2</td><td>\u2013</td><td>110.0</td><td>0.509</td><td>0.115</td><td>0.051</td><td>0.828</td></tr><tr><th>Hu et al. [16]</th><th>SeNet-154</th><td>NYU-v2</td><td>\u2013</td><td>149.8</td><td>0.530</td><td>0.115</td><td>0.051</td><td>0.866</td></tr><tr><th>Chen et al. [4]</th><th>SeNet-154</th><td>NYU-v2</td><td>\u2013</td><td>210.3</td><td>0.514</td><td>0.111</td><td>0.048</td><td>0.878</td></tr><tr><th>Chen et al.[3]</th><th>ResNet-101</th><td>NYU-v2</td><td>HC labeled data</td><td>163.4</td><td>0.376</td><td>0.098</td><td>0.042</td><td>0.899</td></tr><tr><th>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U})</th><th>MobileNet-V2</th><td>NYU-v2</td><td>Unlabeled ScanNet</td><td>1.7</td><td>0.482</td><td>0.131</td><td>0.056</td><td>0.837</td></tr><tr><th>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U}^{{}^{\\prime}})</th><th>MobileNet-V2</th><td>NYU-v2</td><td>Labeled ScanNet</td><td>1.7</td><td>0.461</td><td>0.121</td><td>0.052</td><td>0.855</td></tr></tbody></table>", "caption": "TABLE II: Quantitative comparisons between our method and other approaches built on large networks on the NYU-v2 dataset.", "list_citation_info": ["[48] Z. Zhang, Z. Cui, C. Xu, Y. Yan, N. Sebe, and J. Yang, \u201cPattern-affinitive propagation across depth, surface normal and semantic segmentation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 4106\u20134115.", "[10] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao, \u201cDeep ordinal regression network for monocular depth estimation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 2002\u20132011.", "[4] X. Chen and Z. Zha, \u201cStructure-aware residual pyramid network for monocular depth estimation,\u201d in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI), 2019, pp. 694\u2013700.", "[3] T. Chen, S. An, Y. Zhang, C. Ma, H. Wang, X. Guo, and W. Zheng, \u201cImproving monocular depth estimation by leveraging structural awareness and complementary datasets,\u201d in Proceedings of the European conference on computer vision (ECCV), vol. 12359. Springer, 2020, pp. 90\u2013108.", "[16] J. Hu, M. Ozay, Y. Zhang, and T. Okatani, \u201cRevisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries,\u201d in IEEE Winter Conference on Applications of Computer Vision (WACV), 2019, pp. 1043\u20131051.", "[20] L. Iro, R. Christian, B. Vasileios, T. Federico, and N. Nassir, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in International Conference on 3D Vision (3DV), 2016, pp. 239\u2013248."]}, {"table": "<table><thead><tr><th>Method</th><th>Backbone</th><th>Params (M)</th><th>GPU [ms]</th><th>REL</th><th>\\delta_{1}</th></tr></thead><tbody><tr><td>Fast-depth [43]</td><td>MobileNet</td><td>3.9</td><td>7</td><td>-</td><td>0.775</td></tr><tr><td>Joint-depth [30]</td><td>MobileNet-V2</td><td>3.1</td><td>21</td><td>0.149</td><td>0.790</td></tr><tr><td>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U})</td><td>MobileNet-V2</td><td>1.7</td><td>11</td><td>0.131</td><td>0.837</td></tr><tr><td>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U}^{{}^{\\prime}})</td><td>MobileNet-V2</td><td>1.7</td><td>11</td><td>0.121</td><td>0.855</td></tr></tbody></table>", "caption": "TABLE III: Quantitative comparison of light-weight approaches on the NYU-v2 dataset. The best and the second best results are highlighted in red and blue, respectively.", "list_citation_info": ["[30] V. Nekrasov, T. Dharmasiri, A. Spek, T. Drummond, C. Shen, and I. Reid, \u201cReal-time joint semantic segmentation and depth estimation using asymmetric annotations,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2019, pp. 7101\u20137107.", "[43] D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V. Sze, \u201cFastdepth: Fast monocular depth estimation on embedded systems,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2019, pp. 6101\u20136108."]}, {"table": "<table><thead><tr><th>Method</th><th>RMSE</th><th>REL</th><th>\\delta_{1}</th></tr></thead><tbody><tr><th>Fast-depth [43]</th><td>0.662</td><td>0.376</td><td>0.404</td></tr><tr><th>Joint-depth [30]</th><td>0.634</td><td>0.338</td><td>0.454</td></tr><tr><th>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U})</th><td>0.577</td><td>0.338</td><td>0.430</td></tr><tr><th>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U^{\\prime}})</th><td>0.531</td><td>0.306</td><td>0.446</td></tr></tbody></table>", "caption": "TABLE IV: The results of different methods on the SUNRGBD dataset. The best and the second best results are highlighted in red and blue, respectively.", "list_citation_info": ["[30] V. Nekrasov, T. Dharmasiri, A. Spek, T. Drummond, C. Shen, and I. Reid, \u201cReal-time joint semantic segmentation and depth estimation using asymmetric annotations,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2019, pp. 7101\u20137107.", "[43] D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V. Sze, \u201cFastdepth: Fast monocular depth estimation on embedded systems,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2019, pp. 6101\u20136108."]}, {"table": "<table><thead><tr><th>Method</th><th>fr1/360</th><th>fr1/desk</th><th>fr1/desk2</th><th>fr1/rpy</th><th>fr1/xyz</th></tr></thead><tbody><tr><th>Fast-depth [43]</th><td>0.548</td><td>0.308</td><td>0.358</td><td>0.333</td><td>0.287</td></tr><tr><th>Joint-depth [30]</th><td>0.512</td><td>0.410</td><td>0.441</td><td>0.552</td><td>0.583</td></tr><tr><th>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U})</th><td>0.615</td><td>0.442</td><td>0.498</td><td>0.611</td><td>0.486</td></tr><tr><th>Ours N_{s}(\\mathcal{X}\\cup\\mathcal{U^{\\prime}})</th><td>0.854</td><td>0.695</td><td>0.772</td><td>0.679</td><td>0.905</td></tr></tbody></table>", "caption": "TABLE V: The \\delta_{1} accuracy of different methods on the five sequences from TUM dataset. The best and the second best results are highlighted in red and blue, respectively.", "list_citation_info": ["[30] V. Nekrasov, T. Dharmasiri, A. Spek, T. Drummond, C. Shen, and I. Reid, \u201cReal-time joint semantic segmentation and depth estimation using asymmetric annotations,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2019, pp. 7101\u20137107.", "[43] D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V. Sze, \u201cFastdepth: Fast monocular depth estimation on embedded systems,\u201d in IEEE International Conference on Robotics and Automation (ICRA), 2019, pp. 6101\u20136108."]}]}