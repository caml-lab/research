{"title": "Memory-guided unsupervised image-to-image translation", "abstract": "We present a novel unsupervised framework for instance-level image-to-image translation. Although recent advances have been made by incorporating additional object annotations, existing methods often fail to handle images with multiple disparate objects. The main cause is that, during inference, they apply a global style to the whole image and do not consider the large style discrepancy between instance and background, or within instances. To address this problem, we propose a class-aware memory network that explicitly reasons about local style variations. A key-values memory structure, with a set of read/update operations, is introduced to record class-wise style variations and access them without requiring an object detector at the test time. The key stores a domain-agnostic content representation for allocating memory items, while the values encode domain-specific style representations. We also present a feature contrastive loss to boost the discriminative power of memory items. We show that by incorporating our memory, we can transfer class-aware and accurate style representations across domains. Experimental results demonstrate that our model outperforms recent instance-level methods and achieves state-of-the-art performance.", "authors": ["Somi Jeong", " Youngjung Kim", " Eungbean Lee", " Kwanghoon Sohn"], "pdf_url": "https://arxiv.org/abs/2104.05170", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"2\">CycleGAN [49]</td><td colspan=\"2\">UNIT [23]</td><td colspan=\"2\">MUNIT [13]</td><td colspan=\"2\">DRIT [22]</td><td colspan=\"2\">INIT [38]</td><td colspan=\"2\">DUNIT [1]</td><td colspan=\"2\">Ours</td></tr><tr><th></th><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td></tr><tr><th>sunny\\rightarrownight</th><td>0.014</td><td>1.026</td><td>0.082</td><td>1.030</td><td>1.159</td><td>1.278</td><td>1.058</td><td>1.224</td><td>1.060</td><td>1.118</td><td>1.166</td><td>1.259</td><td>1.176</td><td>1.271</td></tr><tr><th>night\\rightarrowsunny</th><td>0.012</td><td>1.023</td><td>0.027</td><td>1.024</td><td>1.036</td><td>1.051</td><td>1.024</td><td>1.099</td><td>1.045</td><td>1.080</td><td>1.083</td><td>1.108</td><td>1.115</td><td>1.130</td></tr><tr><th>sunny\\rightarrowrainy</th><td>0.011</td><td>1.073</td><td>0.097</td><td>1.075</td><td>1.012</td><td>1.146</td><td>1.007</td><td>1.207</td><td>1.036</td><td>1.152</td><td>1.029</td><td>1.225</td><td>1.092</td><td>1.213</td></tr><tr><th>sunny\\rightarrowcloudy</th><td>0.014</td><td>1.097</td><td>0.081</td><td>1.134</td><td>1.008</td><td>1.095</td><td>1.025</td><td>1.104</td><td>1.040</td><td>1.142</td><td>1.033</td><td>1.149</td><td>1.052</td><td>1.218</td></tr><tr><th>cloudy\\rightarrowsunny</th><td>0.090</td><td>1.033</td><td>0.219</td><td>1.046</td><td>1.026</td><td>1.321</td><td>1.046</td><td>1.249</td><td>1.016</td><td>1.460</td><td>1.077</td><td>1.472</td><td>1.136</td><td>1.489</td></tr><tr><th>Average</th><td>0.025</td><td>1.057</td><td>0.087</td><td>1.055</td><td>1.032</td><td>1.166</td><td>1.031</td><td>1.164</td><td>1.043</td><td>1.179</td><td>1.079</td><td>1.223</td><td>1.112</td><td>1.254</td></tr></tbody></table>", "caption": "Table 1: Quantitative evaluation on INIT dataset [38].We perform bidirectional translation for each domain pair.We measure CIS and IS (higher is better).Our results attain the best results.", "list_citation_info": ["[49] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Int. Conf. Comput. Vis., 2017.", "[23] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Adv. Neural Inform. Process. Syst., 2017.", "[1] Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based unsupervised image-to-image translation. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.", "[38] Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, and Thomas S Huang. Towards instance-level image-to-image translation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.", "[22] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In Eur. Conf. Comput. Vis., 2018.", "[13] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In Eur. Conf. Comput. Vis., 2018."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th>sunny</th><th>sunny</th><th>sunny</th><th rowspan=\"2\">Average</th></tr><tr><th>\\rightarrownight</th><th>\\rightarrowrainy</th><th>\\rightarrowcloudy</th></tr></thead><tbody><tr><th>CycleGAN [49]</th><td>0.016</td><td>0.008</td><td>0.011</td><td>0.012</td></tr><tr><th>UNIT [23]</th><td>0.067</td><td>0.062</td><td>0.068</td><td>0.066</td></tr><tr><th>MUNIT [13]</th><td>0.292</td><td>0.239</td><td>0.211</td><td>0.247</td></tr><tr><th>DRIT [22]</th><td>0.231</td><td>0.173</td><td>0.166</td><td>0.190</td></tr><tr><th>INIT [38]</th><td>0.330</td><td>0.267</td><td>0.224</td><td>0.274</td></tr><tr><th>DUNIT [1]</th><td>0.338</td><td>0.298</td><td>0.225</td><td>0.287</td></tr><tr><th>Ours</th><td>0.346</td><td>0.316</td><td>0.251</td><td>0.304</td></tr><tr><th>Real images</th><td>0.573</td><td>0.489</td><td>0.465</td><td>0.509</td></tr></tbody></table>", "caption": "Table 2: Quantitative evaluation with average LPIPS metric.The LPIPS metric calculates the diversity scores.", "list_citation_info": ["[49] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Int. Conf. Comput. Vis., 2017.", "[23] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Adv. Neural Inform. Process. Syst., 2017.", "[1] Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based unsupervised image-to-image translation. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.", "[38] Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, and Thomas S Huang. Towards instance-level image-to-image translation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.", "[22] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In Eur. Conf. Comput. Vis., 2018.", "[13] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In Eur. Conf. Comput. Vis., 2018."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"5\">KITTI \\rightarrow Cityscapes</th></tr><tr><th>Pers.</th><th>Car</th><th>Truc.</th><th>Bic.</th><th>mAP</th></tr></thead><tbody><tr><th>DT [14]</th><td>28.5</td><td>40.7</td><td>25.9</td><td>29.7</td><td>31.2</td></tr><tr><th>DAF [2]</th><td>39.2</td><td>40.2</td><td>25.7</td><td>48.9</td><td>38.5</td></tr><tr><th>DARL [17]</th><td>46.4</td><td>58.7</td><td>27.0</td><td>49.1</td><td>45.3</td></tr><tr><th>DAOD [36]</th><td>47.3</td><td>59.1</td><td>28.3</td><td>49.6</td><td>46.1</td></tr><tr><th>DUNIT w/o IC [1]</th><td>56.2</td><td>59.5</td><td>24.9</td><td>48.2</td><td>47.2</td></tr><tr><th>DUNIT w/ IC [1]</th><td>60.7</td><td>65.1</td><td>32.7</td><td>57.7</td><td>54.1</td></tr><tr><th>Ours</th><td>58.3</td><td>68.2</td><td>33.4</td><td>58.4</td><td>54.6</td></tr></tbody></table>", "caption": "Table 3: Quantitative results for domain adaptive detection.We report per-class AP for KITTI\\rightarrowCityscapes case.", "list_citation_info": ["[36] Adrian Lopez Rodriguez and Krystian Mikolajczyk. Domain adaptation for object detection via style consistency. arXiv preprint arXiv:1911.10033, 2019.", "[17] Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim. Diversify and match: A domain adaptive representation learning paradigm for object detection. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.", "[1] Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based unsupervised image-to-image translation. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.", "[2] Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object detection in the wild. In IEEE Conf. Comput. Vis. Pattern Recog., 2018.", "[14] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-domain weakly-supervised object detection through progressive domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., 2018."]}]}