{"title": "End-to-end object detection with adaptive clustering transformer", "abstract": "End-to-end Object Detection with Transformer (DETR)proposes to perform object detection with Transformer and achieve comparable performance with two-stage object detection like Faster-RCNN. However, DETR needs huge computational resources for training and inference due to the high-resolution spatial input. In this paper, a novel variant of transformer named Adaptive Clustering Transformer(ACT) has been proposed to reduce the computation cost for high-resolution input. ACT cluster the query features adaptively using Locality Sensitive Hashing (LSH) and ap-proximate the query-key interaction using the prototype-key interaction. ACT can reduce the quadratic O(N2) complexity inside self-attention into O(NK) where K is the number of prototypes in each layer. ACT can be a drop-in module replacing the original self-attention module without any training. ACT achieves a good balance between accuracy and computation cost (FLOPs). The code is available as supplementary for the ease of experiment replication and verification. Code is released at \\url{https://github.com/gaopengcuhk/SMCA-DETR/}", "authors": ["Minghang Zheng", " Peng Gao", " Renrui Zhang", " Kunchang Li", " Xiaogang Wang", " Hongsheng Li", " Hao Dong"], "pdf_url": "https://arxiv.org/abs/2011.09315", "list_table_and_caption": [{"table": "<table><tr><td>Model</td><td>GFLOPs</td><td>AP</td><td>AP{}_{L}</td><td>AP{}_{M}</td><td>AP{}_{S}</td></tr><tr><td>Backbone (ResNet50-DC5)</td><td>110.7</td><td></td><td></td><td></td><td></td></tr><tr><td>DETR-DC5 [Carion et al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, andZagoruyko]</td><td>+73.4</td><td>43.3</td><td>61.1</td><td>47.3</td><td>22.5</td></tr><tr><td>Faster RCNN-DC5 [Ren et al.(2015)Ren, He, Girshick, and Sun]</td><td>+209.3</td><td>41.1</td><td>55.0</td><td>45.9</td><td>22.9</td></tr><tr><td>ACT (L=32)</td><td>+58.2</td><td>42.6</td><td>61.1</td><td>46.8</td><td>21.4</td></tr><tr><td>ACT (L=24)</td><td>+53.1</td><td>41.3</td><td>60.6</td><td>45.9</td><td>19.2</td></tr><tr><td>ACT (L=20)</td><td>+49.4</td><td>39.7</td><td>60.3</td><td>44.2</td><td>16.9</td></tr><tr><td>ACT (L=16)</td><td>+45.0</td><td>37.1</td><td>58.8</td><td>41.3</td><td>13.9</td></tr><tr><td>ACT+MTKD (L=32)</td><td>+58.2</td><td>43.1</td><td>61.4</td><td>47.1</td><td>22.2</td></tr><tr><td>ACT+MTKD (L=24)</td><td>+53.1</td><td>42.3</td><td>61.0</td><td>46.4</td><td>21.3</td></tr><tr><td>ACT+MTKD (L=20)</td><td>+49.5</td><td>41.8</td><td>60.7</td><td>45.6</td><td>20.6</td></tr><tr><td>ACT+MTKD (L=16)</td><td>+45.1</td><td>40.6</td><td>59.7</td><td>44.3</td><td>18.5</td></tr></table>", "caption": "Table 1: We compare the AP of our model with DETR-DC5 and Faster RCNN in detail. DETR-DC5 and Faster RCNN use dilated ResNet-50 as the backbone. The sign \u2018+\u2019 in GFLOPs column refers to the flops increased relative to the backbone. We refer to the bbox AP of the large, medium, and large size instances as AP{}_{L}, AP{}_{M}, and AP{}_{S} respectively.", "list_citation_info": ["[Carion et al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and Zagoruyko] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872, 2020.", "[Ren et al.(2015)Ren, He, Girshick, and Sun] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91\u201399, 2015."]}]}