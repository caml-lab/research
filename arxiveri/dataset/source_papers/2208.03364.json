{"title": "Glass: Global to local attention for scene-text spotting", "abstract": "In recent years, the dominant paradigm for text spotting is to combine the tasks of text detection and recognition into a single end-to-end framework. Under this paradigm, both tasks are accomplished by operating over a shared global feature map extracted from the input image. Among the main challenges that end-to-end approaches face is the performance degradation when recognizing text across scale variations (smaller or larger text), and arbitrary word rotation angles. In this work, we address these challenges by proposing a novel global-to-local attention mechanism for text spotting, termed GLASS, that fuses together global and local features. The global features are extracted from the shared backbone, preserving contextual information from the entire image, while the local features are computed individually on resized, high-resolution rotated word crops. The information extracted from the local crops alleviates much of the inherent difficulties with scale and word rotation. We show a performance analysis across scales and angles, highlighting improvement over scale and angle extremities. In addition, we introduce an orientation-aware loss term supervising the detection task, and show its contribution to both detection and recognition performance across all angles. Finally, we show that GLASS is general by incorporating it into other leading text spotting architectures, improving their text spotting performance. Our method achieves state-of-the-art results on multiple benchmarks, including the newly released TextOCR.", "authors": ["Roi Ronen", " Shahar Tsiper", " Oron Anschel", " Inbal Lavi", " Amir Markovitz", " R. Manmatha"], "pdf_url": "https://arxiv.org/abs/2208.03364", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"3\">Method</th><th colspan=\"6\">ICDAR 2015</th><th colspan=\"4\">Total-Text</th><th>TextOCR</th></tr><tr><th colspan=\"3\">Word Spotting</th><th colspan=\"3\">End-to-End</th><th colspan=\"2\">Word Spotting</th><th colspan=\"2\">End-to-End</th><th rowspan=\"2\">End-to-End</th></tr><tr><th>S</th><th>W</th><th>G</th><th>S</th><th>W</th><th>G</th><th>None</th><th>Full</th><th>None</th><th>Full</th></tr></thead><tbody><tr><th>TextDragon [8]</th><td>86.2</td><td>81.6</td><td>68.0</td><td>82.5</td><td>78.3</td><td>65.2</td><td>-</td><td>-</td><td>48.8</td><td>71.8</td><td>-</td></tr><tr><th>ABCNet v2 [29]</th><td>-</td><td>-</td><td>-</td><td>82.7</td><td>78.5</td><td>73.0</td><td>70.4</td><td>78.1</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MTSv3* [20]</th><td>83.1</td><td>79.1</td><td>75.1</td><td>83.3</td><td>78.1</td><td>74.2</td><td>-</td><td>-</td><td>71.2</td><td>78.4</td><td>50.8</td></tr><tr><th>Text Perc. [37]</th><td>84.1</td><td>79.4</td><td>67.9</td><td>80.5</td><td>76.6</td><td>65.1</td><td>69.7</td><td>78.3</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CRAFTS [3]</th><td>-</td><td>-</td><td>-</td><td>83.1</td><td>82.1</td><td>74.9</td><td>-</td><td>-</td><td>78.7</td><td>-</td><td>-</td></tr><tr><th>MANGO*{}^{\\dagger} [36]</th><td>85.2</td><td>81.1</td><td>74.6</td><td>85.4</td><td>80.1</td><td>73.9</td><td>72.9</td><td>83.6</td><td>68.9{}^{\\ddagger}</td><td>78.9{}^{\\ddagger}</td><td>-</td></tr><tr><th>YAMTS* [16]</th><td>86.8</td><td>82.4</td><td>76.7</td><td>85.3</td><td>79.8</td><td>74.0</td><td>-</td><td>-</td><td>71.1</td><td>78.4</td><td>-</td></tr><tr><th>Ours*</th><td>86.8</td><td>82.5</td><td>78.8</td><td>84.7</td><td>80.1</td><td>76.3</td><td>79.9</td><td>86.2</td><td>76.6</td><td>83.0</td><td>67.1</td></tr></tbody></table>", "caption": "Table 1: Results for ICDAR 2015, Total-Text and TextOCR datasets. \u2018S\u2019, \u2018W\u2019 and \u2018G\u2019 refer to strong, weak andgeneric lexicons. \u201cNone\u201d refers to recognition without any lexicon. \u201cFull\u201d lexicon contains all the words in the test set. (*) refers to using specific lexicons from [20].({}^{\\dagger}) indicates IoU of 0.1 was used instead of 0.5 during evaluation.({}^{\\ddagger}) represents results obtained using method\u2019s official source code.", "list_citation_info": ["[8] Feng, W., He, W., Yin, F., Zhang, X.Y., Liu, C.L.: TextDragon: An end-to-end framework for arbitrary shaped text spotting. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 9076\u20139085 (2019)", "[29] Liu, Y., Shen, C., Jin, L., He, T., Chen, P., Liu, C., Chen, H.: ABCNet v2: Adaptive bezier-curve network for real-time end-to-end text spotting. arXiv preprint arXiv:2105.03620 (2021)", "[36] Qiao, L., Chen, Y., Cheng, Z., Xu, Y., Niu, Y., Pu, S., Wu, F.: MANGO: A mask attention guided one-stage scene text spotter. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, issue 3, pp. 2467\u20132476 (2021)", "[20] Liao, M., Pang, G., Huang, J., Hassner, T., Bai, X.: Mask TextSpotter v3: Segmentation proposal network for robust scene text spotting. In: Proceedings of the European Conference on Computer Vision. pp. 706\u2013722. Springer (2020)", "[3] Baek, Y., Shin, S., Baek, J., Park, S., Lee, J., Nam, D., Lee, H.: Character region attention for text spotting. In: Proceedings of the European Conference on Computer Vision. pp. 504\u2013521. Springer (2020)", "[16] Krylov, I., Nosov, S., Sovrasov, V.: Open images v5 text annotation and yet another mask text spotter. arXiv preprint arXiv:2106.12326 (2021)", "[37] Qiao, L., Tang, S., Cheng, Z., Xu, Y., Niu, Y., Pu, S., Wu, F.: Text perceptron: Towards end-to-end arbitrary-shaped text spotting. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, issue 07, pp. 11899\u201311907 (2020)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">Total-Text</th><th colspan=\"2\">ICDAR 2015</th></tr><tr><th>E2E Hmean</th><th>FPS</th><th>E2E Hmean</th><th>FPS</th></tr></thead><tbody><tr><th>MTSv3 [20]</th><td>67.5</td><td>2.2</td><td>68.5</td><td>2.6</td></tr><tr><th>     + GLASS</th><td>69.8 (+2.3)</td><td>2.0</td><td>72.3 (+3.8)</td><td>2.3</td></tr><tr><th>ABCNet v2 [29]</th><td>67.6</td><td>6.5</td><td>-</td><td>-</td></tr><tr><th>     + GLASS</th><td>71.3 (+3.7)</td><td>6.0</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 2: GLASS results with Mask TextSpotter v3 (MTSv3) [20] and ABCNet v2 [29]. First and third rows are results reproduced using the official MTSv3 and ABCNet v2 implementations. The second and fourth rows show the effect of incorporating GLASS into MTSv3 and ABCNet v2. ", "list_citation_info": ["[20] Liao, M., Pang, G., Huang, J., Hassner, T., Bai, X.: Mask TextSpotter v3: Segmentation proposal network for robust scene text spotting. In: Proceedings of the European Conference on Computer Vision. pp. 706\u2013722. Springer (2020)", "[29] Liu, Y., Shen, C., Jin, L., He, T., Chen, P., Liu, C., Chen, H.: ABCNet v2: Adaptive bezier-curve network for real-time end-to-end text spotting. arXiv preprint arXiv:2105.03620 (2021)"]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th>Detection</th><th>Recognition</th><th colspan=\"2\">GLASS</th><th>Total #</th><th rowspan=\"2\">FPS</th></tr><tr><td>Branch</td><td>Head</td><td>ResNet34</td><td>Fusion</td><td>Parameters</td></tr><tr><th>Baseline</th><th>48.8M</th><th>3.2M</th><th>-</th><th>-</th><th>52M</th><th>2.7</th></tr><tr><th>\u2003+ GLASS</th><td>48.8M</td><td>3.2M</td><td>10.5M</td><td>1.5M</td><td>64M</td><td>3.0</td></tr><tr><th>MTSv3 [20]</th><th>41.3M</th><th>4.0M</th><th>-</th><th>-</th><th>45.3M</th><th>2.3</th></tr><tr><th>\u2003+ GLASS</th><td>41.3M</td><td>4.0M</td><td>10.5M</td><td>1.5M</td><td>57.3M</td><td>2.6</td></tr><tr><th>ABCNet v2 [29]</th><th>44.7M</th><th>3.0M</th><th>-</th><th>-</th><th>47.7M</th><th>6.0</th></tr><tr><th>\u2003+ GLASS</th><td>44.7M</td><td>3.0M</td><td>10.5M</td><td>1.5M</td><td>59.7M</td><td>6.5</td></tr></tbody></table>", "caption": "Table 7: Model\u2019s number of parameters. \u201cFPS\u201d column states the frames-per-second measured for Total-Text dataset.", "list_citation_info": ["[20] Liao, M., Pang, G., Huang, J., Hassner, T., Bai, X.: Mask TextSpotter v3: Segmentation proposal network for robust scene text spotting. In: Proceedings of the European Conference on Computer Vision. pp. 706\u2013722. Springer (2020)", "[29] Liu, Y., Shen, C., Jin, L., He, T., Chen, P., Liu, C., Chen, H.: ABCNet v2: Adaptive bezier-curve network for real-time end-to-end text spotting. arXiv preprint arXiv:2105.03620 (2021)"]}, {"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><td colspan=\"5\">Validation set</td><td colspan=\"2\">Test set</td></tr><tr><td colspan=\"3\">Detection</td><td>Word</td><td rowspan=\"2\">End-to-End</td><td>Word</td><td rowspan=\"2\">End-to-End</td></tr><tr><td>R</td><td>P</td><td>H</td><td>Spotting</td><td>Spotting</td></tr><tr><th>MTSv3 [20]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>50.8</td></tr><tr><th>Baseline</th><td>73.3</td><td>82.5</td><td>77.6</td><td>60.6</td><td>55.6</td><td>58.0</td><td>56.8</td></tr><tr><th>GLASS</th><td>71.5</td><td>84.3</td><td>77.4</td><td>71.3</td><td>64.8</td><td>70.4</td><td>67.1</td></tr></tbody></table>", "caption": "Table 9: Results on the TextOCR validation and test datasets.R, P, and H refer to recall, precision and H-mean.No lexicon is used.Our method with GLASS module outperforms Mask TextSpotter v3 on the test set, noting both approaches were optimized on similar data, including TextOCR train data [42].On TextOCR validation dataset, our method with the GLASS component surpasses the baseline by a large margin for end-to-end recognition and word spotting metrics.", "list_citation_info": ["[42] Singh, A., Pang, G., Toh, M., Huang, J., Galuba, W., Hassner, T.: Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8802\u20138812 (2021)", "[20] Liao, M., Pang, G., Huang, J., Hassner, T., Bai, X.: Mask TextSpotter v3: Segmentation proposal network for robust scene text spotting. In: Proceedings of the European Conference on Computer Vision. pp. 706\u2013722. Springer (2020)"]}]}