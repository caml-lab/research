{"title": "The OMG-Emotion Behavior Dataset", "abstract": "This paper is the basis paper for the accepted IJCNN challenge One-Minute Gradual-Emotion Recognition (OMG-Emotion) by which we hope to foster long-emotion classification using neural models for the benefit of the IJCNN community. The proposed corpus has as the novelty the data collection and annotation strategy based on emotion expressions which evolve over time into a specific context. Different from other corpora, we propose a novel multimodal corpus for emotion expression recognition, which uses gradual annotations with a focus on contextual emotion expressions. Our dataset was collected from Youtube videos using a specific search strategy based on restricted keywords and filtering which guaranteed that the data follow a gradual emotion expression transition, i.e. emotion expressions evolve over time in a natural and continuous fashion. We also provide an experimental protocol and a series of unimodal baseline experiments which can be used to evaluate deep and recurrent neural models in a fair and standard manner.", "authors": ["Pablo Barros", " Nikhil Churamani", " Egor Lakomkin", " Henrique Siqueira", " Alexander Sutherland", " Stefan Wermter"], "pdf_url": "https://arxiv.org/abs/1803.05434", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>Modalities</th><th>Annotation Domain</th><th>Samples</th><th>Annotation Level</th><th>Annotators</th><th>Annotation Strategy</th><th>Scenario</th></tr></thead><tbody><tr><td>IEMOCAP [8]</td><td>A, V, L</td><td>Dimensional, 9 Emotions</td><td>6000</td><td>Utterance</td><td>5</td><td>Contextual</td><td>Indoor</td></tr><tr><td>MOSI [9]</td><td>A, V, L</td><td>Head gestures, Sentiment</td><td>2199</td><td>Video Clip</td><td>5 per video</td><td>Instance</td><td>Wild</td></tr><tr><td>EmoReact [10]</td><td>A, V</td><td>17 Emotions</td><td>1102</td><td>Video Clip</td><td>1 per video</td><td>Instance</td><td>Wild</td></tr><tr><td>GIFGIF+ [11]</td><td>V</td><td>17 Emotions</td><td>25.544</td><td>Video Clip</td><td>500+ per video</td><td>Instance</td><td>Wild</td></tr><tr><td>Aff-Challenge [12]</td><td>A, V</td><td>Dimensional</td><td>400</td><td>videos</td><td>1</td><td>Contextual</td><td>Wild</td></tr><tr><td>EMOTIW [13]</td><td>A, V</td><td>7 Emotions</td><td>1809</td><td>Videos</td><td>3</td><td>Instance</td><td>Wild</td></tr><tr><td>OMG-Emotion</td><td>A, V, L</td><td>Dimensional, 7 Emotions</td><td>2400</td><td>Utterance</td><td>5 per video</td><td>Contextual</td><td>Wild</td></tr></tbody></table>", "caption": "TABLE I: Summary of recent emotion expression corpora which are similar to the proposed OMG-Emotion dataset ", "list_citation_info": ["[11] O. O. R. Weixuan Chen and R. W. Picard, \u201cGifgif+: Collecting emotional animated gifs with clustered multi-task learning,\u201d in 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII). ACII, 2017.", "[12] S. Zafeiriou, D. Kollias, M. Nicolaou, A. Papaioannou, G. Zhao, and I. Kotsia, \u201cAff-wild: Valence and arousal in-the-wild challenge,\u201d 2017.", "[10] B. Nojavanasghari, T. Baltru\u0161aitis, C. E. Hughes, and L.-P. Morency, \u201cEmoReact: a multimodal approach and dataset for recognizing emotional responses in children,\u201d in Proceedings of the 18th ACM International Conference on Multimodal Interaction. ACM, 2016, pp. 137\u2013144.", "[8] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIEMOCAP: Interactive emotional dyadic motion capture database,\u201d Language Resources and Evaluation, vol. 42, no. 4, p. 335, 2008.", "[9] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, \u201cMosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos,\u201d arXiv preprint arXiv:1606.06259, 2016.", "[13] A. Dhall, R. Goecke, S. Ghosh, J. Joshi, J. Hoey, and T. Gedeon, \u201cFrom individual to group-level emotion recognition: Emotiw 5.0,\u201d in Proceedings of the 19th ACM International Conference on Multimodal Interaction. ACM, 2017, pp. 524\u2013528."]}, {"table": "<table><tbody><tr><td>Model</td><td>Emotion</td><td colspan=\"2\">Arousal</td><td colspan=\"2\">Valence</td></tr><tr><td></td><td>F1-Score</td><td>MSE</td><td>CCC</td><td>MSE</td><td>CCC</td></tr><tr><td>SVM A</td><td>0.33</td><td>0.04</td><td>0.15</td><td>0.10</td><td>0.21</td></tr><tr><td>RF T</td><td>0.39</td><td>0.06</td><td>0.15</td><td>0.10</td><td>0.04</td></tr><tr><td>Vision - Face Channel [19]</td><td>0.37</td><td>0.05</td><td>0.12</td><td>0.12</td><td>0.23</td></tr><tr><td>Vision - Audio Channel [19]</td><td>0.33</td><td>0.05</td><td>0.08</td><td>0.12</td><td>0.10</td></tr></tbody></table>", "caption": "TABLE III: Baseline results. ", "list_citation_info": ["[19] P. Barros and S. Wermter, \u201cDeveloping crossmodal expression recognition based on a deep neural model,\u201d Adaptive Behavior, vol. 24, no. 5, pp. 373\u2013396, 2016."]}]}