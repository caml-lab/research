{"title": "EnD: Entangling and Disentangling Deep Representations for Bias Correction", "abstract": "Artificial neural networks perform state-of-the-art in an ever-growing number of tasks, and nowadays they are used to solve an incredibly large variety of tasks. There are problems, like the presence of biases in the training data, which question the generalization capability of these models. In this work we propose EnD, a regularization strategy whose aim is to prevent deep models from learning unwanted biases. In particular, we insert an \"information bottleneck\" at a certain point of the deep neural network, where we disentangle the information about the bias, still letting the useful information for the training task forward-propagating in the rest of the model. One big advantage of EnD is that we do not require additional training complexity (like decoders or extra layers in the model), since it is a regularizer directly applied on the trained model. Our experiments show that EnD effectively improves the generalization on unbiased test sets, and it can be effectively applied on real-case scenarios, like removing hidden biases in the COVID-19 detection from radiographic images.", "authors": ["Enzo Tartaglione", " Carlo Alberto Barbano", " Marco Grangetto"], "pdf_url": "https://arxiv.org/abs/2103.02023", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"4\">\\rho values</th></tr><tr><th>0.999</th><th>0.997</th><th>0.995</th><th>0.990</th></tr></thead><tbody><tr><th>Vanilla</th><td>10.4</td><td>33.4</td><td>72.1</td><td>89.1</td></tr><tr><th>HEX [32]</th><td>10.8</td><td>16.6</td><td>19.7</td><td>24.7</td></tr><tr><th>LearnedMixin [6]</th><td>12.1</td><td>50.2</td><td>78.2</td><td>88.3</td></tr><tr><th>RUBi [5]</th><td>13.7</td><td>43.0</td><td>90.4</td><td>93.6</td></tr><tr><th>ReBias [4]</th><td>22.7</td><td>64.2</td><td>76.0</td><td>88.1</td></tr><tr><th>EnD</th><td>52.30</td><td>83.70</td><td>93.92</td><td>96.02</td></tr><tr><th></th><td>\\pm 2.39</td><td>\\pm 1.03</td><td>\\pm 0.35</td><td>\\pm 0.08</td></tr></tbody></table>", "caption": "Table 1: Biased MNIST performance on the unbiased test set.", "list_citation_info": ["[5] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal biases for visual question answering. In Advances in neural information processing systems, pages 841\u2013852, 2019.", "[6] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4067\u20134080. Association for Computational Linguistics, 2019.", "[4] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In International Conference on Machine Learning (ICML), 2020.", "[32] Haohan Wang, Zexue He, Zachary L. Lipton, and Eric P. Xing. Learning robust representations by projecting superficial statistics out. In International Conference on Learning Representations, 2019."]}, {"table": "<table><tbody><tr><td></td><td></td><td>Method</td><td>Unbiased</td><td>Bias-conflicting</td></tr><tr><td rowspan=\"4\">Learn</td><td rowspan=\"4\">HairColor</td><td>Vanilla</td><td>70.25 \\pm 0.35</td><td>52.52 \\pm 0.19</td></tr><tr><td>Group DRO [23]</td><td>85.43 \\pm 0.53</td><td>83.40 \\pm 0.67</td></tr><tr><td>LfF[21]</td><td>84.24 \\pm 0.37</td><td>81.24 \\pm 1.38</td></tr><tr><td>EnD</td><td>91.21 \\pm 0.22</td><td>87.45 \\pm 1.06</td></tr><tr><td rowspan=\"5\">Learn</td><td rowspan=\"4\">HeavyMakeup</td><td>Vanilla</td><td>62.00 \\pm 0.02</td><td>33.75 \\pm 0.28</td></tr><tr><td>Group DRO [23]</td><td>64.88 \\pm 0.42</td><td>50.24 \\pm 0.68</td></tr><tr><td>LfF[21]</td><td>66.20 \\pm 1.21</td><td>45.48 \\pm 4.33</td></tr><tr><td>EnD</td><td>75.93 \\pm 1.31</td><td>53.70 \\pm 5.24</td></tr></tbody></table>", "caption": "Table 3: Performance on CelebA.", "list_citation_info": ["[23] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2019.", "[21] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: Training debiased classifier from biased classifier. In Advances in Neural Information Processing Systems, 2020."]}, {"table": "<table><thead><tr><th></th><th rowspan=\"2\">Method</th><th colspan=\"2\">Trained on EB1</th><th colspan=\"2\">Trained on EB2</th></tr><tr><th></th><th>EB2</th><th>Test</th><th>EB1</th><th>Test</th></tr></thead><tbody><tr><th rowspan=\"5\">Learn Gender</th><th>Vanilla</th><td>59.86</td><td>84.42</td><td>57.84</td><td>69.75</td></tr><tr><th>BlindEye [1]</th><td>63.74</td><td>85.56</td><td>57.33</td><td>69.90</td></tr><tr><th>Kim et al. [16]</th><td>68.00</td><td>86.66</td><td>64.18</td><td>74.50</td></tr><tr><th>EnD</th><td>65.49</td><td>87.15</td><td>69.40</td><td>78.19</td></tr><tr><th></th><td>\\pm 0.81</td><td>\\pm 0.31</td><td>\\pm 2.01</td><td>\\pm 1.18</td></tr><tr><th rowspan=\"5\">Learn Age</th><th>Vanilla</th><td>54.30</td><td>77.17</td><td>48.91</td><td>61.97</td></tr><tr><th>BlindEye [1]</th><td>66.80</td><td>75.13</td><td>64.16</td><td>62.40</td></tr><tr><th>Kim et al. [16]</th><td>65.27</td><td>77.43</td><td>62.18</td><td>63.04</td></tr><tr><th>EnD</th><td>76.04</td><td>80.15</td><td>74.25</td><td>78.80</td></tr><tr><th></th><td>\\pm 0.25</td><td>\\pm 0.96</td><td>\\pm 2.26</td><td>\\pm 1.48</td></tr></tbody></table>", "caption": "Table 4: Performance on IMDB Face. When gender is learned, age is the bias, and when age is learned the gender is the bias.", "list_citation_info": ["[16] Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn: Training deep neural networks with biased data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[1] Mohsan Alvi, Andrew Zisserman, and Christoffer Nell\u00e5ker. Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings. In Proceedings of the European Conference on Computer Vision (ECCV), pages 0\u20130, 2018."]}]}