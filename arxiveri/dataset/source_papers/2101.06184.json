{"title": "Temporal-Relational Crosstransformers for Few-Shot Action Recognition", "abstract": "We propose a novel approach to few-shot action recognition, finding temporally-corresponding frame tuples between the query and videos in the support set. Distinct from previous few-shot works, we construct class prototypes using the CrossTransformer attention mechanism to observe relevant sub-sequences of all support videos, rather than using class averages or single best matches. Video representations are formed from ordered tuples of varying numbers of frames, which allows sub-sequences of actions at different speeds and temporal offsets to be compared.\n  Our proposed Temporal-Relational CrossTransformers (TRX) achieve state-of-the-art results on few-shot splits of Kinetics, Something-Something V2 (SSv2), HMDB51 and UCF101. Importantly, our method outperforms prior work on SSv2 by a wide margin (12%) due to the its ability to model temporal relations. A detailed ablation showcases the importance of matching to multiple support set videos and learning higher-order relational CrossTransformers.", "authors": ["Toby Perrett", " Alessandro Masullo", " Tilo Burghardt", " Majid Mirmehdi", " Dima Damen"], "pdf_url": "https://arxiv.org/abs/2101.06184", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>Kinetics</th><th>SSv2{}^{\\dagger}</th><th>SSv2{}^{*}</th><th>HMDB</th><th>UCF</th></tr></thead><tbody><tr><th>CMN [31]</th><td>78.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CMN-J [32]</th><td>78.9</td><td>48.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>TARN [3]</th><td>78.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ARN [27]</th><td>82.4</td><td>-</td><td>-</td><td>60.6</td><td>83.1</td></tr><tr><th>OTAM [4]</th><td>85.8</td><td>-</td><td>52.3</td><td>-</td><td>-</td></tr><tr><th>TRX (Ours)</th><td>85.9</td><td>59.1</td><td>64.6</td><td>75.6</td><td>96.1</td></tr></tbody></table>", "caption": "Table 1: Results on 5-way 5-shot benchmarks of Kinetics (split from [32]), SSv2 ({}^{\\dagger}: split from [32], {}^{*}: split from [4]), HMDB51 and UCF101 (both splits from [27]).", "list_citation_info": ["[27] Hongguang Zhang, Li Zhang, Xiaojuan Qi, Hongdong Li, Philip H S Torr, and Piotr Koniusz. Few-shot Action Recognition with Permutation-invariant Attention. In European Conference on Computer Vision, 2020.", "[3] Mina Bishay, Georgios Zoumpourlis, and Ioannis Patras. TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition. In British Machine Vision Conference, 2019.", "[4] Kaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, and Juan Carlos Niebles. Few-Shot Video Classification via Temporal Alignment. In Computer Vision and Pattern Recognition, 2020.", "[31] Linchao Zhu and Yi Yang. Compound Memory Networks for Few-Shot Video Classification. In European Conference on Computer Vision, 2018.", "[32] Linchao Zhu and Yi Yang. Label Independent Memory for Semi-Supervised Few-shot Video Classification. Transactions on Pattern Analysis and Machine Intelligence, 14(8), 2020."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"5\">Shot</th></tr><tr><th>Dataset</th><th>Method</th><th>    1</th><th>    2</th><th>    3</th><th>    4</th><th>    5</th></tr></thead><tbody><tr><th></th><th>CMN [31]</th><td>60.5</td><td>-</td><td>-</td><td>-</td><td>78.9</td></tr><tr><th></th><th>CMN-J [32]</th><td>60.5</td><td>70.0</td><td>75.6</td><td>77.3</td><td>78.9</td></tr><tr><th></th><th>TARN [3]</th><td>64.8</td><td>-</td><td>-</td><td>-</td><td>78.5</td></tr><tr><th>Kinetics</th><th>ARN [27]</th><td>63.7</td><td>-</td><td>-</td><td>-</td><td>82.4</td></tr><tr><th></th><th>OTAM [4]</th><td>73.0</td><td>-</td><td>-</td><td>-</td><td>85.8</td></tr><tr><th></th><th>Ours - TRX \\Omega{=}\\{1\\}</th><td>63.6</td><td>75.4</td><td>80.1</td><td>82.4</td><td>85.2</td></tr><tr><th></th><th>Ours - TRX \\Omega{=}\\{2,3\\}</th><td>63.6</td><td>76.2</td><td>81.8</td><td>83.4</td><td>85.9</td></tr><tr><th></th><th>CMN-J [32]</th><td>36.2</td><td>42.1</td><td>44.6</td><td>47.0</td><td>48.8</td></tr><tr><th>SSv2{}^{*}</th><th>Ours - TRX \\Omega{=}\\{1\\}</th><td>34.9</td><td>43.4</td><td>47.6</td><td>50.9</td><td>53.3</td></tr><tr><th></th><th>Ours - TRX \\Omega{=}\\{2,3\\}</th><td>36.0</td><td>46.0</td><td>51.9</td><td>54.9</td><td>59.1</td></tr><tr><th></th><th>OTAM [4]</th><td>42.8</td><td>-</td><td>-</td><td>-</td><td>52.3</td></tr><tr><th>SSv2{}^{\\dagger}</th><th>Ours - TRX \\Omega{=}\\{1\\}</th><td>38.8</td><td>49.7</td><td>54.4</td><td>58.0</td><td>60.6</td></tr><tr><th></th><th>Ours - TRX \\Omega{=}\\{2,3\\}</th><td>42.0</td><td>53.1</td><td>57.6</td><td>61.1</td><td>64.6</td></tr></tbody></table>", "caption": "Table 5: Comparison to few-shot video works on Kinetics (split from [32]) and Something-Something V2 (SSv2) ({}^{\\dagger}: split from [32] {}^{*}: split from [4]). Results are reported as the shot, \\ienumber of support set videos per class, increases from 1 to 5. -: Results not available in published works.", "list_citation_info": ["[27] Hongguang Zhang, Li Zhang, Xiaojuan Qi, Hongdong Li, Philip H S Torr, and Piotr Koniusz. Few-shot Action Recognition with Permutation-invariant Attention. In European Conference on Computer Vision, 2020.", "[3] Mina Bishay, Georgios Zoumpourlis, and Ioannis Patras. TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot Action Recognition. In British Machine Vision Conference, 2019.", "[4] Kaidi Cao, Jingwei Ji, Zhangjie Cao, Chien-Yi Chang, and Juan Carlos Niebles. Few-Shot Video Classification via Temporal Alignment. In Computer Vision and Pattern Recognition, 2020.", "[31] Linchao Zhu and Yi Yang. Compound Memory Networks for Few-Shot Video Classification. In European Conference on Computer Vision, 2018.", "[32] Linchao Zhu and Yi Yang. Label Independent Memory for Semi-Supervised Few-shot Video Classification. Transactions on Pattern Analysis and Machine Intelligence, 14(8), 2020."]}]}