{"title": "Swin Transformer V2: Scaling up Capacity and Resolution", "abstract": "Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536$\\times$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{https://github.com/microsoft/Swin-Transformer}.", "authors": ["Ze Liu", " Han Hu", " Yutong Lin", " Zhuliang Yao", " Zhenda Xie", " Yixuan Wei", " Jia Ning", " Yue Cao", " Zheng Zhang", " Li Dong", " Furu Wei", " Baining Guo"], "pdf_url": "https://arxiv.org/abs/2111.09883", "list_table_and_caption": [{"table": "<table><tr><td> </td><td>ImageNet*</td><td colspan=\"4\">ImageNet{}^{\\dagger}</td><td colspan=\"2\">COCO</td><td colspan=\"2\">ADE20k</td><td></td></tr><tr><td>method</td><td> W8, I256top-1 acc </td><td> W12, I384top-1 acc </td><td> W16, I512top-1 acc </td><td> W20, I640top-1 acc </td><td> W24, I768top-1 acc </td><td> W16AP{}^{\\text{box}} </td><td> W32AP{}^{\\text{box}} </td><td> W16mIoU </td><td> W20mIoU </td><td> W32mIoU </td></tr><tr><td>Parameterized position bias [46]</td><td>81.7</td><td>79.4/82.7</td><td>77.2/83.0</td><td>73.2/83.2</td><td>68.7/83.2</td><td>50.8</td><td>50.9</td><td>45.5</td><td>45.8</td><td>44.5</td></tr><tr><td>Linear-Spaced CPB</td><td> 81.7(+0.0) </td><td> 82.0/82.9(+2.6/+0.2) </td><td> 81.2/83.3(+4.0/+0.3) </td><td> 79.8/83.6(+6.6/+0.4) </td><td> 77.6/83.7(+8.9/+0.5) </td><td> 50.9(+0.1) </td><td> 51.7(+0.8) </td><td> 47.0(+1.5) </td><td> 47.4(+1.6) </td><td> 47.2(+2.7) </td></tr><tr><td>Log-Spaced CPB</td><td> 81.8(+0.1) </td><td> 82.4/83.2(+3.0/+0.5) </td><td> 81.7/83.8(+4.5/+0.8) </td><td> 80.4/84.0(+7.2/+0.8) </td><td> 79.1/84.2(+10.4/+1.0) </td><td> 51.1(+0.3) </td><td> 51.8(+0.9) </td><td> 47.0(+1.5) </td><td> 47.7(+1.9) </td><td> 47.8(+3.3) </td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Comparison of different position bias computation approaches using Swin-T. * indicates the top-1 accuracy on ImageNet-1k trained from scratch. The models in * column will be used for testing on the ImageNet-1K image classification task using larger image/window resolutions, marked by {\\dagger}. For these results, we report both the results w.o./with fine-tuning. These models are also used for fine-tuning on COCO object detection and ADE20K semantic segmentation tasks.", "list_citation_info": ["[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021."]}, {"table": "<table><tr><td> Method</td><td>param</td><td> pre-trainimages </td><td> pre-trainlength (#im) </td><td> pre-trainim size </td><td> pre-traintime </td><td> fine-tuneim size </td><td> ImageNet-1K-V1top-1 acc </td><td> ImaegNet-1K-V2top-1 acc </td></tr><tr><td>SwinV1-B</td><td>88M</td><td>IN-22K-14M</td><td>1.3B</td><td>224{}^{2}</td><td>&lt;30{}^{\\dagger}</td><td>384{}^{2}</td><td>86.4</td><td>76.58</td></tr><tr><td>SwinV1-L</td><td>197M</td><td>IN-22K-14M</td><td>1.3B</td><td>224{}^{2}</td><td>&lt;10{}^{\\dagger}</td><td>384{}^{2}</td><td>87.3</td><td>77.46</td></tr><tr><td>ViT-G [80]</td><td>1.8B</td><td>JFT-3B</td><td>164B</td><td>224{}^{2}</td><td>&gt;30k</td><td>518{}^{2}</td><td>90.45</td><td>83.33</td></tr><tr><td>V-MoE [56]</td><td>14.7B*</td><td>JFT-3B</td><td>-</td><td>224{}^{2}</td><td>16.8k</td><td>518{}^{2}</td><td>90.35</td><td>-</td></tr><tr><td>CoAtNet-7 [17]</td><td>2.44B</td><td>JFT-3B</td><td>-</td><td>224{}^{2}</td><td>20.1k</td><td>512{}^{2}</td><td>90.88</td><td>-</td></tr><tr><td>SwinV2-B</td><td>88M</td><td>IN-22K-14M</td><td>1.3B</td><td>192{}^{2}</td><td>&lt;30{}^{\\dagger}</td><td>384{}^{2}</td><td>87.1</td><td>78.08</td></tr><tr><td>SwinV2-L</td><td>197M</td><td>IN-22K-14M</td><td>1.3B</td><td>192{}^{2}</td><td>&lt;20{}^{\\dagger}</td><td>384{}^{2}</td><td>87.7</td><td>78.31</td></tr><tr><td>SwinV2-G</td><td>3.0B</td><td>IN-22K-ext-70M</td><td>3.5B</td><td>192{}^{2}</td><td>&lt;0.5k{}^{\\dagger}</td><td>640{}^{2}</td><td>90.17</td><td>84.00</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 2: Comparison with previous largest vision models on ImageNet-1K V1 and V2 classification. * indicates the sparse model; the \u201cpre-train time\u201d column is measured by the TPUv3 core days with numbers copied from the original papers. {\\dagger} That of SwinV2-G is estimated according to training iterations and FLOPs.", "list_citation_info": ["[17] Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes, 2021.", "[80] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers, 2021.", "[56] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts, 2021."]}, {"table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\"> trainI(W) size </td><td rowspan=\"2\"> testI(W) size </td><td colspan=\"2\">mini-val (AP)</td><td colspan=\"2\">test-dev (AP)</td></tr><tr><td>box</td><td>mask</td><td>box</td><td>mask</td></tr><tr><td>CopyPaste[25]</td><td>1280(-)</td><td>1280(-)</td><td>57.0</td><td>48.9</td><td>57.3</td><td>49.1</td></tr><tr><td>SwinV1-L[46]</td><td>800(7)</td><td>ms(7)</td><td>58.0</td><td>50.4</td><td>58.7</td><td>51.1</td></tr><tr><td>YOLOR[66]</td><td>1280(-)</td><td>1280(-)</td><td>-</td><td>-</td><td>57.3</td><td>-</td></tr><tr><td>CBNet[43]</td><td>1400(7)</td><td>ms(7)</td><td>59.6</td><td>51.8</td><td>60.1</td><td>52.3</td></tr><tr><td>DyHead[16]</td><td>1200(-)</td><td>ms(-)</td><td>60.3</td><td>-</td><td>60.6</td><td>-</td></tr><tr><td>SoftTeacher[74]</td><td>1280(12)</td><td>ms(12)</td><td>60.7</td><td>52.5</td><td>61.3</td><td>53.0</td></tr><tr><td rowspan=\"3\"> SwinV2-L(HTC++) </td><td rowspan=\"3\">1536(32)</td><td>1100(32)</td><td>58.8</td><td>51.1</td><td>-</td><td>-</td></tr><tr><td>1100 (48)</td><td>58.9</td><td>51.2</td><td>-</td><td>-</td></tr><tr><td>ms (48)</td><td>60.2</td><td>52.1</td><td>60.8</td><td>52.7</td></tr><tr><td rowspan=\"3\"> SwinV2-G(HTC++) </td><td rowspan=\"3\">1536(32)</td><td>1100(32)</td><td>61.7</td><td>53.3</td><td>-</td><td>-</td></tr><tr><td>1100 (48)</td><td>61.9</td><td>53.4</td><td>-</td><td>-</td></tr><tr><td>ms (48)</td><td>62.5</td><td>53.7</td><td>63.1</td><td>54.4</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 3: Comparison with previous best results on COCO object detection and instance segmentation. I(W) indicates the image and window size. ms indicate multi-scale testing is employed.", "list_citation_info": ["[25] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. arXiv preprint arXiv:2012.07177, 2020.", "[43] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang, Zhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. Cbnetv2: A composite backbone network architecture for object detection, 2021.", "[16] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions, 2021.", "[74] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher, 2021.", "[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.", "[66] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. You only learn one representation: Unified network for multiple tasks, 2021."]}, {"table": "<table><tr><td> Method</td><td> train I(W) size </td><td> test I(W) size </td><td>mIoU</td></tr><tr><td>SwinV1-L[46]</td><td>640(7)</td><td>640(7)</td><td>53.5*</td></tr><tr><td>Focal-L[75]</td><td>640(40)</td><td>640(40)</td><td>55.4*</td></tr><tr><td>CSwin-L[21]</td><td>640(40)</td><td>640(40)</td><td>55.7*</td></tr><tr><td>MaskFormer[13]</td><td>640(7)</td><td>640(7)</td><td>55.6*</td></tr><tr><td>FaPN[33]</td><td>640(7)</td><td>640(7)</td><td>56.7*</td></tr><tr><td>BEiT[4]</td><td>640(40)</td><td>640(40)</td><td>58.4*</td></tr><tr><td> SwinV2-L(UperNet) </td><td>640(40)</td><td>640(40)</td><td>55.9*</td></tr><tr><td rowspan=\"3\"> SwinV2-G(UperNet) </td><td rowspan=\"3\">640(40)</td><td>640(40)</td><td>59.1</td></tr><tr><td>896 (56)</td><td>59.3</td></tr><tr><td>896 (56)</td><td>59.9*</td></tr><tr><td> </td><td></td><td></td><td></td></tr></table>", "caption": "Table 4: Comparison with previous best results on ADE20K semantic segmentation. * indicates multi-scale testing is used.", "list_citation_info": ["[13] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. arXiv, 2021.", "[75] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers, 2021.", "[21] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows, 2021.", "[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.", "[33] Shihua Huang, Zhichao Lu, Ran Cheng, and Cheng He. Fapn: Feature-aligned pyramid network for dense image prediction, 2021.", "[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers, 2021."]}, {"table": "<table><tr><td> Method</td><td> train I(W) size </td><td> test I(W) size </td><td>views</td><td>top-1</td></tr><tr><td>ViViT[2]</td><td> -(-) </td><td>-(-)</td><td>4\\times3</td><td>84.8</td></tr><tr><td>SwinV1-L[47]</td><td> 480(12){}^{2}\\times16(8) </td><td> 480(12){}^{2}\\times16(8) </td><td>10\\times5</td><td>84.9</td></tr><tr><td>TokenLearner[57]</td><td> 256(8){}^{2}\\times64(64) </td><td> 256(8){}^{2}\\times64(64) </td><td>4\\times3</td><td>85.4</td></tr><tr><td rowspan=\"3\"> Video-SwinV2-G </td><td rowspan=\"3\"> 320(20){}^{2}\\times8(8) </td><td> 320(20){}^{2}\\times8(8) </td><td>1\\times1</td><td>83.2</td></tr><tr><td> 384(24){}^{2}\\times8(8) </td><td>1\\times1</td><td>83.4</td></tr><tr><td> 384(24){}^{2}\\times8(8) </td><td>4\\times5</td><td>86.8</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 5: Comparison with previous best results on Kinetics-400 video action classification.", "list_citation_info": ["[47] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer, 2021.", "[57] Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: What can 8 learned tokens do for images and videos?, 2021.", "[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. Vivit: A video vision transformer, 2021."]}, {"table": "<table><tr><td> Backbone</td><td>pre-norm</td><td> sandwich  [20]</td><td> post-norm  [65]</td><td> our </td></tr><tr><td>Swin-S</td><td>83.2</td><td>82.6</td><td>83.3</td><td>83.6</td></tr><tr><td>Swin-B</td><td>83.6</td><td>-</td><td>83.6</td><td>84.1</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 7: Comparison with other normalization methods. The post-norm method diverges at the default learning rate, and we use 1/4 of the default learning rate for this method. Sandwich performs worse than ours, probably because it sacrifices expressiveness.", "list_citation_info": ["[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017.", "[20] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. arXiv preprint arXiv:2105.13290, 2021."]}]}