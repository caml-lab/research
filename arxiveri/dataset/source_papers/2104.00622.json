{"title": "RGB-D Local Implicit Function for Depth Completion of Transparent Objects", "abstract": "Majority of the perception methods in robotics require depth information provided by RGB-D cameras. However, standard 3D sensors fail to capture depth of transparent objects due to refraction and absorption of light. In this paper, we introduce a new approach for depth completion of transparent objects from a single RGB-D image. Key to our approach is a local implicit neural representation built on ray-voxel pairs that allows our method to generalize to unseen objects and achieve fast inference speed. Based on this representation, we present a novel framework that can complete missing depth given noisy RGB-D input. We further improve the depth estimation iteratively using a self-correcting refinement model. To train the whole pipeline, we build a large scale synthetic dataset with transparent objects. Experiments demonstrate that our method performs significantly better than the current state-of-the-art methods on both synthetic and real world data. In addition, our approach improves the inference speed by a factor of 20 compared to the previous best method, ClearGrasp. Code and dataset will be released at https://research.nvidia.com/publication/2021-03_RGB-D-Local-Implicit.", "authors": ["Luyang Zhu", " Arsalan Mousavian", " Yu Xiang", " Hammad Mazhar", " Jozef van Eenbergen", " Shoubhik Debnath", " Dieter Fox"], "pdf_url": "https://arxiv.org/abs/2104.00622", "list_table_and_caption": [{"table": "<table><thead><tr><th>Methods</th><th>RMSE\\downarrow</th><th>REL\\downarrow</th><th>MAE\\downarrow</th><th>\\delta_{1.05}\\uparrow</th><th>\\delta_{1.10}\\uparrow</th><th>\\delta_{1.25}\\uparrow</th></tr><tr><th></th><th colspan=\"6\">Cleargrasp Syn-known</th></tr></thead><tbody><tr><th>RGBD-FCN{}_{\\text{ours}}</th><td>0.028</td><td>0.039</td><td>0.021</td><td>76.53</td><td>91.82</td><td>99.00</td></tr><tr><th>NLSPN [38]</th><td>0.136</td><td>0.231</td><td>0.113</td><td>19.02</td><td>35.95</td><td>70.43</td></tr><tr><th>NLSPN{}_{\\text{ours}} [38]</th><td>0.026</td><td>0.041</td><td>0.021</td><td>74.89</td><td>89.95</td><td>98.59</td></tr><tr><th>CG [46]</th><td>0.041</td><td>0.055</td><td>0.031</td><td>69.43</td><td>89.17</td><td>96.74</td></tr><tr><th>CG{}_{\\text{ours}} [46]</th><td>0.034</td><td>0.045</td><td>0.026</td><td>73.53</td><td>92.68</td><td>98.25</td></tr><tr><th>Ours</th><td>0.012</td><td>0.017</td><td>0.009</td><td>94.79</td><td>98.52</td><td>99.67</td></tr><tr><th></th><td colspan=\"6\">Cleargrasp Syn-novel</td></tr><tr><th>RGBD-FCN{}_{\\text{ours}}</th><td>0.033</td><td>0.058</td><td>0.028</td><td>52.40</td><td>85.64</td><td>98.94</td></tr><tr><th>NLSPN [38]</th><td>0.132</td><td>0.239</td><td>0.106</td><td>16.25</td><td>32.13</td><td>64.78</td></tr><tr><th>NLSPN{}_{\\text{ours}} [38]</th><td>0.029</td><td>0.049</td><td>0.024</td><td>64.83</td><td>88.20</td><td>98.57</td></tr><tr><th>CG [46]</th><td>0.044</td><td>0.074</td><td>0.038</td><td>41.37</td><td>79.20</td><td>97.29</td></tr><tr><th>CG{}_{\\text{ours}} [46]</th><td>0.037</td><td>0.062</td><td>0.032</td><td>50.27</td><td>84.00</td><td>98.39</td></tr><tr><th>Ours</th><td>0.028</td><td>0.045</td><td>0.023</td><td>68.62</td><td>89.10</td><td>99.20</td></tr><tr><th></th><td colspan=\"6\">Cleargrasp Real-known</td></tr><tr><th>RGBD-FCN{}_{\\text{ours}}</th><td>0.054</td><td>0.087</td><td>0.048</td><td>36.32</td><td>67.11</td><td>96.26</td></tr><tr><th>NLSPN [38]</th><td>0.149</td><td>0.228</td><td>0.127</td><td>14.04</td><td>26.67</td><td>54.32</td></tr><tr><th>NLSPN{}_{\\text{ours}} [38]</th><td>0.056</td><td>0.086</td><td>0.048</td><td>40.60</td><td>67.68</td><td>96.25</td></tr><tr><th>CG [46]</th><td>0.039</td><td>0.051</td><td>0.029</td><td>72.62</td><td>86.96</td><td>95.58</td></tr><tr><th>CG{}_{\\text{ours}} [46]</th><td>0.032</td><td>0.042</td><td>0.024</td><td>74.63</td><td>90.69</td><td>98.33</td></tr><tr><th>Ours</th><td>0.028</td><td>0.033</td><td>0.020</td><td>82.37</td><td>92.98</td><td>98.63</td></tr><tr><th></th><td colspan=\"6\">Cleargrasp Real-novel</td></tr><tr><th>RGBD-FCN{}_{\\text{ours}}</th><td>0.042</td><td>0.070</td><td>0.037</td><td>42.45</td><td>75.68</td><td>99.02</td></tr><tr><th>NLSPN [38]</th><td>0.145</td><td>0.240</td><td>0.123</td><td>13.77</td><td>25.81</td><td>51.59</td></tr><tr><th>NLSPN{}_{\\text{ours}} [38]</th><td>0.036</td><td>0.059</td><td>0.030</td><td>51.97</td><td>84.82</td><td>99.52</td></tr><tr><th>CG [46]</th><td>0.034</td><td>0.045</td><td>0.025</td><td>76.72</td><td>91.00</td><td>97.63</td></tr><tr><th>CG{}_{\\text{ours}} [46]</th><td>0.027</td><td>0.039</td><td>0.022</td><td>79.5</td><td>93.00</td><td>99.28</td></tr><tr><th>Ours</th><td>0.025</td><td>0.036</td><td>0.020</td><td>76.21</td><td>94.01</td><td>99.35</td></tr></tbody></table>", "caption": "Table 1: Quantitative Comparison to state-of-the-art methods. \\downarrow means lower is better, \\uparrow means higher is better. Please refer to the text for more details.", "list_citation_info": ["[38] Jinsun Park, Kyungdon Joo, Zhe Hu, Chi-Kuei Liu, and In So Kweon. Non-local spatial propagation network for depth completion. In European Conference on Computer Vision (ECCV), 2020.", "[46] Shreeyak Sajjan, Matthew Moore, Mike Pan, Ganesh Nagaraja, Johnny Lee, Andy Zeng, and Shuran Song. Clear grasp: 3d shape estimation of transparent objects for manipulation. In IEEE International Conference on Robotics and Automation (ICRA), pages 3634\u20133642, 2020."]}]}