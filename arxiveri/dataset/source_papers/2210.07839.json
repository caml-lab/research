{"title": "Contrastive audio-visual masked autoencoder", "abstract": "In this paper, we first extend the recent Masked Auto-Encoder (MAE) model from a single modality to audio-visual multi-modalities. Subsequently, we propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining contrastive learning and masked data modeling, two major self-supervised learning frameworks, to learn a joint and coordinated audio-visual representation. Our experiments show that the contrastive audio-visual correspondence learning objective not only enables the model to perform audio-visual retrieval tasks, but also helps the model learn a better joint representation. As a result, our fully self-supervised pretrained CAV-MAE achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the previous best supervised pretrained model on AudioSet in the audio-visual event classification task. Code and pretrained models are at https://github.com/yuangongnd/cav-mae.", "authors": ["Yuan Gong", " Andrew Rouditchenko", " Alexander H. Liu", " David Harwath", " Leonid Karlinsky", " Hilde Kuehne", " James Glass"], "pdf_url": "https://arxiv.org/abs/2210.07839", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td rowspan=\"2\">Pretrain</td><td colspan=\"3\">AudioSet-20K (mAP)</td><td colspan=\"3\">AudioSet-2M (mAP)</td><td colspan=\"3\">VGGSound (Acc)</td></tr><tr><th></th><td>A</td><td>V</td><td>A-V</td><td>A</td><td>V</td><td>A-V</td><td>A</td><td>V</td><td>A-V</td></tr><tr><th colspan=\"11\">Existing Audio-Based Models</th></tr><tr><th>PANNs (Kong et al., 2020)</th><td>-</td><td>27.8</td><td>-</td><td>-</td><td>43.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AST (Gong et al., 2021a)</th><td>IN SL</td><td>34.7</td><td>-</td><td>-</td><td>45.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HTS-AT Chen et al. (2022)</th><td>IN SL</td><td>-</td><td>-</td><td>-</td><td>47.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PaSST Koutini et al. (2021)</th><td>IN SL</td><td>-</td><td>-</td><td>-</td><td>47.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SSAST (Gong et al., 2022)</th><td>SSL</td><td>31.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MAE-AST (Baade et al., 2022)</th><td>SSL</td><td>30.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Audio-MAE<sup>\u2020</sup>(vanilla) (Xu et al., 2022)</th><td>SSL</td><td>36.6</td><td>-</td><td>-</td><td>46.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Audio-MAE<sup>\u2020</sup> (Xu et al., 2022)</th><td>SSL</td><td>37.1</td><td>-</td><td>-</td><td>47.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Chen et al. (2020)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>48.8</td><td>-</td><td>-</td></tr><tr><th>AudioSlowFast (Kazakos et al., 2021)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>50.1</td><td>-</td><td>-</td></tr><tr><th colspan=\"11\">Existing Audio-Visual Models</th></tr><tr><th>GBlend<sup>\u2020*</sup> (Wang et al., 2020)</th><td>-</td><td>29.1</td><td>22.1</td><td>37.8</td><td>32.4</td><td>18.8</td><td>41.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Perceiver<sup>\u2020</sup> (Jaegle et al., 2021)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>38.4</td><td>25.8</td><td>44.2</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Attn AV (Fayek &amp; Kumar, 2020)</th><td>IN SL</td><td>-</td><td>-</td><td>-</td><td>38.4</td><td>25.7</td><td>46.2</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MBT<sup>\u2020*</sup> (Nagrani et al., 2021)</th><td>IN SL</td><td>31.3</td><td>27.7</td><td>43.9</td><td>44.3</td><td>32.3</td><td>52.1</td><td>52.3</td><td>51.2</td><td>64.1</td></tr><tr><th colspan=\"11\">Our Single-Modal MAE</th></tr><tr><th>Audio-MAE</th><td>SSL</td><td>34.2</td><td>-</td><td rowspan=\"2\">36.7<sup>ens</sup></td><td>44.9</td><td>-</td><td rowspan=\"2\">46.9<sup>ens</sup></td><td>57.7</td><td>-</td><td rowspan=\"2\">63.1<sup>ens</sup></td></tr><tr><th>Visual-MAE</th><td>SSL</td><td>-</td><td>15.7</td><td>-</td><td>24.2</td><td>-</td><td>45.7</td></tr><tr><th colspan=\"11\">Our Contrastive Audio-Visual Learning</th></tr><tr><th>CAV</th><td>SSL</td><td>34.6</td><td>18.4</td><td>38.5</td><td>43.6</td><td>24.4</td><td>48.1</td><td>57.3</td><td>45.1</td><td>64.1</td></tr><tr><th colspan=\"11\">Our Multi-Modal MAE</th></tr><tr><th>Vanilla AV-MAE</th><td>SSL</td><td>32.7</td><td>15.8</td><td>36.5</td><td>43.7</td><td>24.0</td><td>48.3</td><td>56.4</td><td>45.4</td><td>63.4</td></tr><tr><th>AV-MAE</th><td>SSL</td><td>33.4</td><td>15.1</td><td>37.4</td><td>44.8</td><td>24.0</td><td>49.6</td><td>57.2</td><td>45.3</td><td>64.1</td></tr><tr><th>CAV-MAE</th><td>SSL</td><td>36.8</td><td>18.7</td><td>40.5</td><td>45.8</td><td>25.6</td><td>50.5</td><td>59.2</td><td>46.6</td><td>65.4</td></tr><tr><th>CAV-MAE<sup>Scale+</sup></th><td>SSL</td><td>37.7</td><td>19.8</td><td>42.0</td><td>46.6</td><td>26.2</td><td>51.2</td><td>59.5</td><td>47.0</td><td>65.5</td></tr></tbody></table>", "caption": "Table 1: Comparing audio-visual classification performance on AudioSet and VGGSound. <br/>IN SL=ImageNet supervised learning; SSL=self-supervised learning; <sup>\u2020</sup>Industry-level computation. <sup>*</sup>Nonstandard data split; <sup>ens</sup>Ensemble of single-modal models. We bold the best methods without supervised pretraining, and underline the overall best methods.", "list_citation_info": ["Gong et al. (2022) Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. Ssast: Self-supervised audio spectrogram transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10699\u201310709, 2022.", "Kazakos et al. (2021) Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Slow-fast auditory streams for audio recognition. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 855\u2013859. IEEE, 2021.", "Koutini et al. (2021) Khaled Koutini, Jan Schl\u00fcter, Hamid Eghbal-zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021.", "Baade et al. (2022) Alan Baade, Puyuan Peng, and David Harwath. Mae-ast: Masked autoencoding audio spectrogram transformer. In Proc. Interspeech 2022, 2022.", "Xu et al. (2022) Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, Christoph Feichtenhofer, et al. Masked autoencoders that listen. arXiv preprint arXiv:2207.06405, 2022.", "Gong et al. (2021a) Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pp. 571\u2013575, 2021a. doi: 10.21437/Interspeech.2021-698.", "Jaegle et al. (2021) Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pp. 4651\u20134664. PMLR, 2021.", "Chen et al. (2022) Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 646\u2013650. IEEE, 2022.", "Chen et al. (2020) Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale audio-visual dataset. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 721\u2013725, 2020.", "Nagrani et al. (2021) Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. Advances in Neural Information Processing Systems, 34:14200\u201314213, 2021.", "Fayek & Kumar (2020) Haytham M Fayek and Anurag Kumar. Large scale audiovisual learning of sounds with weakly labeled data. arXiv preprint arXiv:2006.01595, 2020.", "Wang et al. (2020) Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12695\u201312705, 2020.", "Kong et al. (2020) Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:2880\u20132894, 2020."]}]}