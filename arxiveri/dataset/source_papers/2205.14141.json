{"title": "Contrastive learning rivals masked image modeling in fine-tuning via feature distillation", "abstract": "Masked image modeling (MIM) learns representations with remarkably good fine-tuning performances, overshadowing previous prevalent pre-training approaches such as image classification, instance contrastive learning, and image-text alignment. In this paper, we show that the inferior fine-tuning performance of these pre-training approaches can be significantly improved by a simple post-processing in the form of feature distillation (FD). The feature distillation converts the old representations to new representations that have a few desirable properties just like those representations produced by MIM. These properties, which we aggregately refer to as optimization friendliness, are identified and analyzed by a set of attention- and optimization-related diagnosis tools. With these properties, the new representations show strong fine-tuning performance. Specifically, the contrastive self-supervised learning methods are made as competitive in fine-tuning as the state-of-the-art masked image modeling (MIM) algorithms. The CLIP models' fine-tuning performance is also significantly improved, with a CLIP ViT-L model reaching 89.0% top-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter SwinV2-G model, the fine-tuning accuracy is improved by +1.5 mIoU / +1.1 mAP to 61.4 mIoU / 64.2 mAP on ADE20K semantic segmentation and COCO object detection, respectively, creating new records on both benchmarks. More importantly, our work provides a way for the future research to focus more effort on the generality and scalability of the learnt representations without being pre-occupied with optimization friendliness since it can be enhanced rather easily. The code will be available at https://github.com/SwinTransformer/Feature-Distillation.", "authors": ["Yixuan Wei", " Han Hu", " Zhenda Xie", " Zheng Zhang", " Yue Cao", " Jianmin Bao", " Dong Chen", " Baining Guo"], "pdf_url": "https://arxiv.org/abs/2205.14141", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">res.</td><td rowspan=\"2\">F. D.</td><td colspan=\"2\">IN-1K</td><td rowspan=\"2\">ADE20K</td></tr><tr><td>f.t.</td><td>linear</td></tr><tr><td>BEiT Bao et al., (2021)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>83.2</td><td>37.6</td><td>47.1</td></tr><tr><td>MAE He et al., (2021)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>83.6</td><td>68.0</td><td>48.1</td></tr><tr><td>SimMIM Xie et al., (2022)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>83.8</td><td>56.7</td><td>47.6</td></tr><tr><td>SimMIM Xie et al., (2022)</td><td>Swin-B</td><td>224^{2}</td><td></td><td>84.8</td><td>24.8</td><td>48.3</td></tr><tr><td>WiSE-FT CLIP Wortsman et al., (2021)</td><td>ViT-L</td><td>336{}^{2}</td><td></td><td>87.1</td><td>-</td><td>-</td></tr><tr><td>DINO Caron et al., (2021)</td><td>ViT-B</td><td>224^{2}</td><td></td><td>82.8</td><td>78.2</td><td>46.2</td></tr><tr><td>FD-DINO</td><td>ViT-B</td><td>224^{2}</td><td>\\checkmark</td><td>83.8 (+1.0)</td><td>76.1</td><td>47.7 (+1.5)</td></tr><tr><td>EsViT Li et al., 2022a </td><td>Swin-B</td><td>224^{2}</td><td></td><td>83.9</td><td>81.3</td><td>47.3</td></tr><tr><td>FD-EsViT</td><td>Swin-B</td><td>224^{2}</td><td>\\checkmark</td><td>85.1 (+1.2)</td><td>80.4</td><td>48.9 (+1.6)</td></tr><tr><td>DeiT Touvron et al., (2020)</td><td rowspan=\"2\">ViT-B</td><td>224^{2}</td><td></td><td>81.8</td><td>-</td><td>47.0</td></tr><tr><td>FD-DeiT</td><td>224^{2}</td><td>\\checkmark</td><td>83.0 (+1.2)</td><td>-</td><td>48.0  (+1.0)</td></tr><tr><td>CLIP Radford et al., (2021)</td><td rowspan=\"2\">ViT-B</td><td>224^{2}</td><td></td><td>82.9</td><td>79.5</td><td>49.5</td></tr><tr><td>FD-CLIP</td><td>224^{2}</td><td>\\checkmark</td><td>84.9 (+2.0)</td><td>80.3</td><td>52.8 (+3.3)</td></tr><tr><td>CLIP Radford et al., (2021)</td><td rowspan=\"3\">ViT-L</td><td>224^{2}</td><td></td><td>86.1</td><td>83.5</td><td>53.5</td></tr><tr><td>FD-CLIP</td><td>224^{2}</td><td>\\checkmark</td><td>87.7 (+1.6)</td><td>84.8</td><td>55.7 (+2.2)</td></tr><tr><td>FD-CLIP*</td><td>336{}^{2}</td><td>\\checkmark</td><td>89.0</td><td>-</td><td>-</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Feature distillation improves fine-tuning performance. * uses the same model in the upper row, but with an additional inter-mediate fine-tuning step on ImageNet-22K image classification.", "list_citation_info": ["Caron et al., (2021) Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021). Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294.", "Touvron et al., (2020) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. (2020). Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877.", "Radford et al., (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. (2021). Learning transferable visual models from natural language supervision.", "Xie et al., (2022) Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., and Hu, H. (2022). Simmim: A simple framework for masked image modeling. In International Conference on Computer Vision and Pattern Recognition (CVPR).", "Bao et al., (2021) Bao, H., Dong, L., and Wei, F. (2021). Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254.", "Wortsman et al., (2021) Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., and Schmidt, L. (2021). Robust fine-tuning of zero-shot models.", "(26) Li, C., Yang, J., Zhang, P., Gao, M., Xiao, B., Dai, X., Yuan, L., and Gao, J. (2022a). Efficient self-supervised vision transformers for representation learning. International Conference on Learning Representations (ICLR).", "He et al., (2021) He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., and Girshick, R. (2021). Masked autoencoders are scalable vision learners. arXiv:2111.06377."]}, {"table": "<table><tr><td> Method</td><td>IN-1K (%)</td><td>COCO (AP{}_{\\text{box}})</td><td>COCO (AP{}_{\\text{mask}})</td><td>ADE20K (mIoU)</td></tr><tr><td>GLIPv2-CoSwin-H Zhang et al., 2022b </td><td>-</td><td>62.4</td><td>-</td><td>-</td></tr><tr><td>Florence-CoSwin-H Yuan et al., (2021)</td><td>-</td><td>62.4</td><td>-</td><td>-</td></tr><tr><td>DINO-Swin-L Zhang et al., 2022a </td><td>-</td><td>63.3</td><td>-</td><td>-</td></tr><tr><td>MaskDINO-Swin-L Li et al., 2022b </td><td>-</td><td>-</td><td>54.7</td><td>60.8</td></tr><tr><td>ViT-Adapter-L Chen et al., (2022)</td><td>-</td><td>-</td><td>-</td><td>60.5</td></tr><tr><td>SwinV2-G Liu et al., 2021a </td><td>89.2</td><td>63.1</td><td>54.4</td><td>59.9</td></tr><tr><td>FD-SwinV2-G</td><td>89.4 (+0.2)</td><td>64.2 (+1.1)</td><td>55.4 (+1.0)</td><td>61.4 (+1.5)</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 2: Feature distillation improves the state-of-the-art SwinV2-G model.", "list_citation_info": ["(52) Zhang, H., Zhang, P., Hu, X., Chen, Y.-C., Li, L. H., Dai, X., Wang, L., Yuan, L., Hwang, J.-N., and Gao, J. (2022b). Glipv2: Unifying localization and vision-language understanding.", "Yuan et al., (2021) Yuan, L., Chen, D., Chen, Y.-L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al. (2021). Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432.", "(27) Li, F., Zhang, H., xu, H., Liu, S., Zhang, L., Ni, L. M., and Shum, H.-Y. (2022b). Mask dino: Towards a unified transformer-based framework for object detection and segmentation.", "Chen et al., (2022) Chen, Z., Duan, Y., Wang, W., He, J., Lu, T., Dai, J., and Qiao, Y. (2022). Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534.", "(51) Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L. M., and Shum, H.-Y. (2022a). Dino: Detr with improved denoising anchor boxes for end-to-end object detection.", "(29) Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L., et al. (2021a). Swin transformer v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883."]}]}