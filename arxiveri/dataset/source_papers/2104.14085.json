{"title": "Bridge to answer: Structure-aware graph interaction network for video question answering", "abstract": "This paper presents a novel method, termed Bridge to Answer, to infer correct answers for questions about a given video by leveraging adequate graph interactions of heterogeneous crossmodal graphs. To realize this, we learn question conditioned visual graphs by exploiting the relation between video and question to enable each visual node using question-to-visual interactions to encompass both visual and linguistic cues. In addition, we propose bridged visual-to-visual interactions to incorporate two complementary visual information on appearance and motion by placing the question graph as an intermediate bridge. This bridged architecture allows reliable message passing through compositional semantics of the question to generate an appropriate answer. As a result, our method can learn the question conditioned visual representations attributed to appearance and motion that show powerful capability for video question answering. Extensive experiments prove that the proposed method provides effective and superior performance than state-of-the-art methods on several benchmarks.", "authors": ["Jungin Park", " Jiyoung Lee", " Kwanghoon Sohn"], "pdf_url": "https://arxiv.org/abs/2104.14085", "list_table_and_caption": [{"table": "<table><tbody><tr><td> Model</td><td><p>Action</p></td><td><p>Trans.</p></td><td><p>Frame</p></td><td><p>Count</p></td></tr><tr><td><p>ST-TP [11]</p></td><td><p>62.9</p></td><td><p>69.4</p></td><td><p>49.5</p></td><td><p>4.32</p></td></tr><tr><td><p>Co-mem [6]</p></td><td><p>68.2</p></td><td><p>74.3</p></td><td><p>51.5</p></td><td><p>4.10</p></td></tr><tr><td><p>PSAC [24]</p></td><td><p>70.4</p></td><td><p>76.9</p></td><td><p>55.7</p></td><td><p>4.27</p></td></tr><tr><td><p>HME [5]</p></td><td><p>73.9</p></td><td><p>77.8</p></td><td><p>53.8</p></td><td><p>4.02</p></td></tr><tr><td><p>L-GCN [10]</p></td><td><p>74.3</p></td><td><p>81.1</p></td><td><p>56.3</p></td><td><p>3.95</p></td></tr><tr><td><p>QueST [12]</p></td><td><p>75.9</p></td><td><p>81.0</p></td><td>59.7</td><td><p>4.19</p></td></tr><tr><td><p>HCRN [18]</p></td><td><p>75.0</p></td><td><p>81.4</p></td><td><p>55.9</p></td><td><p>3.82</p></td></tr><tr><td>Ours</td><td>75.9</td><td>82.6</td><td><p>57.5</p></td><td>3.71</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 2: Performance comparison for several tasks on TGIF-QA [11] dataset with state-of-the-art methods.The lower the better for count.", "list_citation_info": ["[5] Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. IEEE Conf. Comput. Vis. Pattern Recog., pages 1999\u20132007, 2019.", "[18] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. IEEE Conf. Comput. Vis. Pattern Recog., pages 9969\u20139978, 2020.", "[11] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. IEEE Conf. Comput. Vis. Pattern Recog., pages 2758\u20132766, 2017.", "[6] Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. Motion-appearance co-memory networks for video question answering. IEEE Conf. Comput. Vis. Pattern Recog., 2018.", "[12] Jianwen Jiang, Ziqiang Chen, HaoJie Lin, Xibin Zhao, and Yue Gao. Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering. AAAI, pages 11101\u201311108, 2020.", "[24] Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, and Chuang Gan. Beyond rnns: Positional self-attention with co-attention for video question answering. AAAI, 2019.", "[10] Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. Location-aware graph convolutional networks for video question answering. AAAI, 2020."]}, {"table": "<table><thead><tr><th> Model</th><th><p>MSVD-QA</p></th><th><p>MSRVTT</p></th></tr></thead><tbody><tr><td><p>AMU [38]</p></td><td><p>32.0</p></td><td><p>32.5</p></td></tr><tr><td><p>HRA [2]</p></td><td><p>34.4</p></td><td><p>35.0</p></td></tr><tr><td><p>Co-mem [6]</p></td><td><p>31.7</p></td><td><p>31.9</p></td></tr><tr><td><p>HME [5]</p></td><td><p>33.7</p></td><td><p>33.0</p></td></tr><tr><td><p>L-GCN [10]</p></td><td><p>34.3</p></td><td><p>-</p></td></tr><tr><td><p>QueST [12]</p></td><td><p>36.1</p></td><td><p>34.6</p></td></tr><tr><td><p>HCRN [18]</p></td><td><p>36.1</p></td><td><p>35.6</p></td></tr><tr><td>Ours</td><td>37.2</td><td>36.9</td></tr><tr><td> </td><td></td><td></td></tr></tbody></table>", "caption": "Table 3: Performance comparison for open-ended questions on MSVD-QA [38] and MSRVTT-QA [39] datasets with state-of-the-art methods.", "list_citation_info": ["[5] Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. Heterogeneous memory enhanced multimodal attention model for video question answering. IEEE Conf. Comput. Vis. Pattern Recog., pages 1999\u20132007, 2019.", "[39] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. IEEE Conf. Comput. Vis. Pattern Recog., pages 5288\u20135296, 2016.", "[18] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. IEEE Conf. Comput. Vis. Pattern Recog., pages 9969\u20139978, 2020.", "[6] Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. Motion-appearance co-memory networks for video question answering. IEEE Conf. Comput. Vis. Pattern Recog., 2018.", "[12] Jianwen Jiang, Ziqiang Chen, HaoJie Lin, Xibin Zhao, and Yue Gao. Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering. AAAI, pages 11101\u201311108, 2020.", "[2] Muhammad Iqbal Hasan Chowdhury, Kien Nguyen, Sridha Sridharan, and Clinton Fookes. Hierarchical relational attention for video question answering. IEEE Int. Conf. Image Process., pages 599\u2013603, 2018.", "[10] Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. Location-aware graph convolutional networks for video question answering. AAAI, 2020.", "[38] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. ACM Int. Conf. Multimedia, pages 1645\u20131653, 2017."]}, {"table": "<table><tbody><tr><th> Model</th><td>Act.</td><td>Trans.</td><td>F.QA</td><td>Count</td></tr><tr><th>Input conditioning</th><td></td><td></td><td></td><td></td></tr><tr><th>\u2003w/o appearance</th><td>72.8</td><td>77.2</td><td>-</td><td>4.01</td></tr><tr><th>\u2003w/o motion</th><td>68.2</td><td>75.5</td><td>57.3</td><td>4.21</td></tr><tr><th>Interaction</th><td></td><td></td><td></td><td></td></tr><tr><th>\u2003w/o Q2V, V2V</th><td>69.4</td><td>75.3</td><td>51.4</td><td>3.97</td></tr><tr><th>\u2003w/o Q2A</th><td>73.9</td><td>80.1</td><td>52.7</td><td>3.87</td></tr><tr><th>\u2003w/o Q2M</th><td>73.1</td><td>78.2</td><td>56.8</td><td>3.90</td></tr><tr><th>\u2003w/o Q2V</th><td>72.3</td><td>76.3</td><td>52.6</td><td>3.95</td></tr><tr><th>\u2003w/o A2M</th><td>74.7</td><td>81.4</td><td>56.1</td><td>3.84</td></tr><tr><th>\u2003w/o M2A</th><td>74.8</td><td>80.9</td><td>56.9</td><td>3.86</td></tr><tr><th>\u2003w/o V2V</th><td>74.1</td><td>78.5</td><td>56.5</td><td>3.89</td></tr><tr><th>Bridge conditioning</th><td></td><td></td><td></td><td></td></tr><tr><th>\u2003w/o bridge</th><td>75.1</td><td>81.7</td><td>56.9</td><td>3.83</td></tr><tr><th>Parameter \\lambda</th><td></td><td></td><td></td><td></td></tr><tr><th>\u2003\\lambda=1</th><td>75.1</td><td>81.5</td><td>56.2</td><td>3.80</td></tr><tr><th>\u2003\\lambda=5</th><td>75.3</td><td>81.8</td><td>57.2</td><td>3.77</td></tr><tr><th>\u2003\\lambda=10</th><td>75.9</td><td>82.6</td><td>57.5</td><td>3.71</td></tr><tr><th>\u2003\\lambda=20</th><td>75.4</td><td>82.2</td><td>57.1</td><td>3.73</td></tr><tr><th>\u2003Full model</th><td>75.9</td><td>82.6</td><td>57.5</td><td>3.71</td></tr><tr><th> </th><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 4: Ablation studies for input conditioning, interaction, and the value of \\lambda on TGIF-QA dataset [11]. Act.: Action; Trans.: Transition; F.QA: Frame QA. When not explicitly specified, we use \\lambda=10. The lower the better for count.", "list_citation_info": ["[11] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. IEEE Conf. Comput. Vis. Pattern Recog., pages 2758\u20132766, 2017."]}, {"table": "<table><thead><tr><th> Parameter \\lambda</th><th><p>MSVD-QA</p></th><th><p>MSRVTT</p></th></tr></thead><tbody><tr><td><p>\\lambda=1</p></td><td><p>35.3</p></td><td><p>33.8</p></td></tr><tr><td><p>\\lambda=5</p></td><td><p>36.9</p></td><td><p>35.0</p></td></tr><tr><td><p>\\lambda=10</p></td><td><p>\\mathbf{37.2}</p></td><td><p>36.6</p></td></tr><tr><td><p>\\lambda=20</p></td><td><p>36.5</p></td><td><p>\\mathbf{36.9}</p></td></tr><tr><td> </td><td></td><td></td></tr></tbody></table>", "caption": "Table 5: Performance comparison with different \\lambda values on MSVD-QA [38] and MSRVTT-QA [39] datasets.", "list_citation_info": ["[38] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. ACM Int. Conf. Multimedia, pages 1645\u20131653, 2017.", "[39] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. IEEE Conf. Comput. Vis. Pattern Recog., pages 5288\u20135296, 2016."]}]}