{"title": "Masked Autoencoders are Scalable Vision Learners", "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.", "authors": ["Kaiming He", " Xinlei Chen", " Saining Xie", " Yanghao Li", " Piotr Doll\u00e1r", " Ross Girshick"], "pdf_url": "https://arxiv.org/abs/2111.06377", "list_table_and_caption": [{"table": "<table><thead><tr><th>method</th><th>pre-train data</th><th>ViT-B</th><th>ViT-L</th><th>ViT-H</th><th>ViT-H{}_{\\text{448}}</th></tr></thead><tbody><tr><th>scratch, our impl.</th><th>-</th><td>82.3</td><td>82.6</td><td>83.1</td><td>-</td></tr><tr><th>DINO [5]</th><th>IN1K</th><td><p>82.8</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>MoCo v3 [9]</th><th>IN1K</th><td><p>83.2</p></td><td><p>84.1</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>BEiT [2]</th><th>IN1K+DALLE</th><td><p>83.2</p></td><td><p>85.2</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>MAE</th><th>IN1K</th><td>83.6</td><td>85.9</td><td>86.9</td><td>87.8</td></tr></tbody></table>", "caption": "Table 3: Comparisons with previous results on ImageNet-1K. The pre-training data is the ImageNet-1K training set (except the tokenizer in BEiT was pre-trained on 250M DALLE data [50]). All self-supervised methods are evaluated by end-to-end fine-tuning. The ViT models are B/16, L/16, H/14 [16]. The best for each column is underlined. All results are on an image size of 224, except for ViT-H with an extra result on 448. Here our MAE reconstructs normalized pixels and is pre-trained for 1600 epochs.", "list_citation_info": ["[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.", "[9] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised Vision Transformers. In ICCV, 2021.", "[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.", "[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers. arXiv:2106.08254, 2021. Accessed in June 2021.", "[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021."]}, {"table": "<table><thead><tr><th>dataset</th><th>ViT-B</th><th>ViT-L</th><th>ViT-H</th><th>ViT-H{}_{\\text{448}}</th><th>prev best</th></tr></thead><tbody><tr><th>iNat 2017</th><td><p>70.5</p></td><td><p>75.7</p></td><td><p>79.3</p></td><td>83.4</td><td><p>75.4 [55]</p></td></tr><tr><th>iNat 2018</th><td><p>75.4</p></td><td><p>80.1</p></td><td><p>83.0</p></td><td>86.8</td><td><p>81.2 [54]</p></td></tr><tr><th>iNat 2019</th><td><p>80.5</p></td><td><p>83.4</p></td><td><p>85.7</p></td><td>88.3</td><td><p>84.1 [54]</p></td></tr><tr><th>Places205</th><td><p>63.9</p></td><td><p>65.8</p></td><td><p>65.9</p></td><td>66.8</td><td><p>66.0 [19]{}^{\\dagger}</p></td></tr><tr><th>Places365</th><td><p>57.9</p></td><td><p>59.4</p></td><td><p>59.8</p></td><td>60.3</td><td><p>58.0 [40]{}^{\\ddagger}</p></td></tr></tbody></table>", "caption": "Table 6: Transfer learning accuracy on classification datasets, using MAE pre-trained on IN1K and then fine-tuned.We provide system-level comparisons with the previous best results.<br/>{}^{\\dagger}: pre-trained on 1 billion images. {}^{\\ddagger}: pre-trained on 3.5 billion images.", "list_citation_info": ["[55] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy. arXiv:1906.06423, 2019.", "[19] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, and Piotr Bojanowski. Self-supervised pretraining of visual features in the wild. arXiv:2103.01988, 2021.", "[54] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Grafit: Learning fine-grained image representations with coarse labels. In ICCV, 2021.", "[40] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In ECCV, 2018."]}, {"table": "<table><thead><tr><th><p>config</p></th><th><p>value</p></th></tr></thead><tbody><tr><td><p>optimizer</p></td><td><p>AdamW [39]</p></td></tr><tr><td><p>base learning rate</p></td><td><p>1.5e-4</p></td></tr><tr><td><p>weight decay</p></td><td><p>0.05</p></td></tr><tr><td><p>optimizer momentum</p></td><td><p>\\beta_{1},\\beta_{2}{=}0.9,0.95 [6]</p></td></tr><tr><td><p>batch size</p></td><td><p>4096</p></td></tr><tr><td><p>learning rate schedule</p></td><td><p>cosine decay [38]</p></td></tr><tr><td><p>warmup epochs [20]</p></td><td><p>40</p></td></tr><tr><td><p>augmentation</p></td><td><p>RandomResizedCrop</p></td></tr></tbody></table>", "caption": "Table 8: Pre-training setting.", "list_citation_info": ["[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020.", "[38] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017.", "[20] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017.", "[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019."]}, {"table": "<table><thead><tr><th><p>config</p></th><th><p>value</p></th></tr></thead><tbody><tr><td><p>optimizer</p></td><td><p>AdamW</p></td></tr><tr><td><p>base learning rate</p></td><td><p>1e-3</p></td></tr><tr><td><p>weight decay</p></td><td><p>0.05</p></td></tr><tr><td><p>optimizer momentum</p></td><td><p>\\beta_{1},\\beta_{2}{=}0.9,0.999</p></td></tr><tr><td><p>layer-wise lr decay [10, 2]</p></td><td><p>0.75</p></td></tr><tr><td><p>batch size</p></td><td><p>1024</p></td></tr><tr><td><p>learning rate schedule</p></td><td><p>cosine decay</p></td></tr><tr><td><p>warmup epochs</p></td><td><p>5</p></td></tr><tr><td><p>training epochs</p></td><td><p>100 (B), 50 (L/H)</p></td></tr><tr><td><p>augmentation</p></td><td><p>RandAug (9, 0.5) [12]</p></td></tr><tr><td><p>label smoothing [52]</p></td><td><p>0.1</p></td></tr><tr><td><p>mixup [69]</p></td><td><p>0.8</p></td></tr><tr><td><p>cutmix [68]</p></td><td><p>1.0</p></td></tr><tr><td><p>drop path [30]</p></td><td><p>0.1 (B/L) 0.2 (H)</p></td></tr></tbody></table>", "caption": "Table 9: End-to-end fine-tuning setting.", "list_citation_info": ["[10] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020.", "[68] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.", "[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In CVPR Workshops, 2020.", "[69] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018.", "[52] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.", "[30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016."]}, {"table": "<table><thead><tr><th><p>config</p></th><th><p>value</p></th></tr></thead><tbody><tr><td><p>optimizer</p></td><td><p>LARS [66]</p></td></tr><tr><td><p>base learning rate</p></td><td><p>0.1</p></td></tr><tr><td><p>weight decay</p></td><td><p>0</p></td></tr><tr><td><p>optimizer momentum</p></td><td><p>0.9</p></td></tr><tr><td><p>batch size</p></td><td><p>16384</p></td></tr><tr><td><p>learning rate schedule</p></td><td><p>cosine decay</p></td></tr><tr><td><p>warmup epochs</p></td><td><p>10</p></td></tr><tr><td><p>training epochs</p></td><td><p>90</p></td></tr><tr><td><p>augmentation</p></td><td><p>RandomResizedCrop</p></td></tr></tbody></table>", "caption": "Table 10: Linear probing setting. We use LARS with a large batch for faster training; SGD works similarly with a 4096 batch.", "list_citation_info": ["[66] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv:1708.03888, 2017."]}, {"table": "<table><thead><tr><th><p>config</p></th><th><p>value</p></th></tr></thead><tbody><tr><td><p>optimizer</p></td><td><p>AdamW</p></td></tr><tr><td><p>base learning rate</p></td><td><p>1e-4</p></td></tr><tr><td><p>weight decay</p></td><td><p>0.3</p></td></tr><tr><td><p>optimizer momentum</p></td><td><p>\\beta_{1},\\beta_{2}{=}0.9,0.95</p></td></tr><tr><td><p>batch size</p></td><td><p>4096</p></td></tr><tr><td><p>learning rate schedule</p></td><td><p>cosine decay</p></td></tr><tr><td><p>warmup epochs</p></td><td><p>20</p></td></tr><tr><td><p>training epochs</p></td><td><p>300 (B), 200 (L/H)</p></td></tr><tr><td><p>augmentation</p></td><td><p>RandAug (9, 0.5) [12]</p></td></tr><tr><td><p>label smoothing [52]</p></td><td><p>0.1</p></td></tr><tr><td><p>mixup [69]</p></td><td><p>0.8</p></td></tr><tr><td><p>cutmix [68]</p></td><td><p>1.0</p></td></tr><tr><td><p>drop path [30]</p></td><td><p>0.1 (B), 0.2 (L/H)</p></td></tr><tr><td><p>exp. moving average (EMA)</p></td><td><p>0.9999</p></td></tr></tbody></table>", "caption": "Table 11: Supervised training ViT from scratch.", "list_citation_info": ["[68] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.", "[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In CVPR Workshops, 2020.", "[69] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR, 2018.", "[52] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.", "[30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016."]}, {"table": "<table><thead><tr><th>method</th><th>model</th><th>params</th><th>acc</th></tr></thead><tbody><tr><td>iGPT [6]</td><td>iGPT-L</td><td>1362 M</td><td>69.0</td></tr><tr><td>iGPT [6]</td><td>iGPT-XL</td><td>6801 M</td><td>72.0</td></tr><tr><td>BEiT [2]</td><td>ViT-L</td><td>304 M</td><td>52.1{{}^{\\dagger}}</td></tr><tr><td>MAE</td><td>ViT-B</td><td>86 M</td><td>68.0</td></tr><tr><td>MAE</td><td>ViT-L</td><td>304 M</td><td>75.8</td></tr><tr><td>MAE</td><td>ViT-H</td><td>632 M</td><td>76.6</td></tr></tbody></table>", "caption": "Table 12: Linear probing results of masked encoding methods. Our fine-tuning results are in Table 3. {{}^{\\dagger}}: our implementation.", "list_citation_info": ["[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers. arXiv:2106.08254, 2021. Accessed in June 2021.", "[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020."]}, {"table": "<table><tbody><tr><th>dataset</th><td>ViT-B</td><td>ViT-L</td><td>ViT-H</td><td>ViT-H{}_{\\text{448}}</td><td>prev best</td></tr><tr><th>IN-Corruption \\downarrow [27]</th><td><p>51.7</p></td><td><p>41.8</p></td><td>33.8</td><td><p>36.8</p></td><td><p>42.5 [32]</p></td></tr><tr><th>IN-Adversarial [28]</th><td><p>35.9</p></td><td><p>57.1</p></td><td><p>68.2</p></td><td>76.7</td><td><p>35.8 [41]</p></td></tr><tr><th>IN-Rendition [26]</th><td><p>48.3</p></td><td><p>59.9</p></td><td><p>64.4</p></td><td>66.5</td><td><p>48.7 [41]</p></td></tr><tr><th>IN-Sketch [60]</th><td><p>34.5</p></td><td><p>45.3</p></td><td><p>49.6</p></td><td>50.9</td><td><p>36.0 [41]</p></td></tr><tr><th colspan=\"3\">our supervised training baselines:</th><td></td><td></td><td></td></tr><tr><th>IN-Corruption \\downarrow</th><td>45.8</td><td>42.3</td><td>41.3</td><td></td><td></td></tr><tr><th>IN-Adversarial</th><td>27.2</td><td>29.6</td><td>33.1</td><td></td><td></td></tr><tr><th>IN-Rendition</th><td>49.4</td><td>50.9</td><td>50.3</td><td></td><td></td></tr><tr><th>IN-Sketch</th><td>35.6</td><td>37.5</td><td>38.0</td><td></td><td></td></tr></tbody></table>", "caption": "Table 13: Robustness evaluation on ImageNet variants (top-1 accuracy, except for IN-C [27] which evaluates mean corruption error).We test the same MAE models (Table 3) on different ImageNet validation sets, without any specialized fine-tuning. We provide system-level comparisons with the previous best results.", "list_citation_info": ["[60] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019.", "[32] Insoo Kim, Seungju Han, Ji-won Baek, Seong-Jin Park, Jae-Joon Han, and Jinwoo Shin. Quality-agnostic image recognition via invertible decoder. In CVPR, 2021.", "[26] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, 2021.", "[28] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In CVPR, 2021.", "[27] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In ICLR, 2019.", "[41] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision transformer. arXiv:2105.07926, 2021."]}]}