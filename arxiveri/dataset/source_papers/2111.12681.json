{"title": "Violet: End-to-end video-language transformers with masked visual-token modeling", "abstract": "A great challenge in video-language (VidL) modeling lies in the disconnection between fixed video representations extracted from image/video understanding models and downstream VidL data. Recent studies try to mitigate this disconnection via end-to-end training. To make it computationally feasible, prior works tend to \"imagify\" video inputs, i.e., a handful of sparsely sampled frames are fed into a 2D CNN, followed by a simple mean-pooling or concatenation to obtain the overall video representations. Although achieving promising results, such simple approaches may lose temporal information that is essential for performing downstream VidL tasks. In this work, we present VIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video transformer to explicitly model the temporal dynamics of video inputs. Further, unlike previous studies that found pre-training tasks on video inputs (e.g., masked frame modeling) not very effective, we design a new pre-training task, Masked Visual-token Modeling (MVM), for better video modeling. Specifically, the original video frame patches are \"tokenized\" into discrete visual tokens, and the goal is to recover the original visual tokens based on the masked patches. Comprehensive analysis demonstrates the effectiveness of both explicit temporal modeling via video transformer and MVM. As a result, VIOLET achieves new state-of-the-art performance on 5 video question answering tasks and 4 text-to-video retrieval tasks.", "authors": ["Tsu-Jui Fu", " Linjie Li", " Zhe Gan", " Kevin Lin", " William Yang Wang", " Lijuan Wang", " Zicheng Liu"], "pdf_url": "https://arxiv.org/abs/2111.12681", "list_table_and_caption": [{"table": "<table><tr><td></td><td colspan=\"3\">TGIF</td><td colspan=\"2\">MSRVTT</td><td colspan=\"2\">LSMDC</td><td>MSVD</td></tr><tr><td>Method</td><td>Action</td><td>Transition</td><td>Frame</td><td>MC</td><td>QA</td><td>MC</td><td>FiB</td><td>QA</td></tr><tr><td>ClipBERT [18]</td><td>82.8</td><td>87.8</td><td>60.3</td><td>88.2</td><td>37.4</td><td>-</td><td>-</td><td>-</td></tr><tr><td>JustAsk [68]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>41.5</td><td>-</td><td>-</td><td>46.3</td></tr><tr><td>MERLOT [19]</td><td>94.0</td><td>96.2</td><td>69.5</td><td>90.9</td><td>43.1</td><td>81.7</td><td>52.9</td><td>-</td></tr><tr><td>VIOLET</td><td>92.5</td><td>95.7</td><td>68.9</td><td>91.9</td><td>43.9</td><td>82.8</td><td>53.7</td><td>47.9</td></tr></table>", "caption": "Table 2: Comparison with SOTA methods on video question answering. We gray out MERLOT due to its excessive computational cost (e.g., 30K TPU hours vs. 2K GPU hours (ours) for pre-training and frame resolution 704 vs. 224 for downstream tasks).", "list_citation_info": ["[19] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. MERLOT: Multimodal Neural Script Knowledge Models. In Conference on Neural Information Processing Systems (NeurIPS), 2021.", "[18] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[68] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just Ask: Learning to Answer Questions from Millions of Narrated Videos. In International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tr><td>Video</td><td>TGIF-</td><td>TGIF-</td><td>MSRVTT-</td><td>DiDeMo-</td></tr><tr><td>Encoding</td><td>Action</td><td>Transition</td><td>Retrieval</td><td>Retrieval</td></tr><tr><td colspan=\"5\">Random initialized visual encoder</td></tr><tr><td>Mean</td><td>72.1</td><td>83.5</td><td>08.4 / 22.7 / 35.3</td><td>09.1 / 24.9 / 36.7</td></tr><tr><td>Concat</td><td>72.9</td><td>83.7</td><td>09.0 / 23.5 / 35.5</td><td>09.4 / 25.8 / 38.1</td></tr><tr><td>VT</td><td>73.6</td><td>84.6</td><td>09.2 / 24.0 / 35.8</td><td>10.3 / 30.1 / 40.5</td></tr><tr><td colspan=\"5\">ImageNet pre-trained visual encoder</td></tr><tr><td>Mean</td><td>77.5</td><td>86.5</td><td>09.6 / 26.7 / 39.5</td><td>09.5 / 27.5 / 40.9</td></tr><tr><td>Concat</td><td>78.0</td><td>87.0</td><td>10.4 / 30.5 / 42.0</td><td>10.6 / 30.8 / 42.9</td></tr><tr><td>VT</td><td>79.6</td><td>87.8</td><td>11.8 / 32.3 / 44.6</td><td>12.0 / 32.4 / 43.5</td></tr><tr><td colspan=\"5\">+ Video-text pre-training on WebVid</td></tr><tr><td>Mean</td><td>80.3</td><td>88.7</td><td>20.8 / 44.9 / 58.1</td><td>17.9 / 43.5 / 51.3</td></tr><tr><td>Concat</td><td>82.5</td><td>91.2</td><td>23.5 / 51.9 / 63.0</td><td>22.2 / 50.5 / 62.6</td></tr><tr><td>VT</td><td>85.8</td><td>92.1</td><td>27.0 / 56.5 / 68.8</td><td>26.1 / 56.9 / 68.9</td></tr></table>", "caption": "Table 3: Impact of different temporal modeling methods over video inputs under different settings: (i) random initialized visual encoder; (ii) ImageNet [84] pre-trained visual encoder and (iii) Adding video-text pre-training on WebVid [25].", "list_citation_info": ["[84] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In Conference on Neural Information Processing Systems (NeurIPS), 2012.", "[25] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tr><td>Pre-training</td><td>TGIF-</td><td>TGIF-</td><td>MSRVTT-</td><td>DiDeMo-</td></tr><tr><td>Task</td><td>Action</td><td>Transition</td><td>Retrieval</td><td>Retrieval</td></tr><tr><td>None</td><td>81.9</td><td>88.5</td><td>13.0 / 36.5 / 49.6</td><td>18.3 / 46.4 / 56.5</td></tr><tr><td>VTM+MLM</td><td>85.4</td><td>91.6</td><td>24.4 / 54.4 / 68.1</td><td>25.8 / 54.2 / 67.0</td></tr><tr><td>+ MCM</td><td>85.0</td><td>91.6</td><td>26.0 / 56.0 / 68.4</td><td>25.8 / 55.9 / 68.1</td></tr><tr><td>+ MFM</td><td>85.5</td><td>92.0</td><td>26.2 / 55.5 / 68.4</td><td>25.4 / 55.5 / 67.8</td></tr><tr><td>+ MPM</td><td>85.0</td><td>91.8</td><td>26.6 / 56.2 / 68.4</td><td>26.0 / 56.5 / 68.0</td></tr><tr><td>+ MVM</td><td>85.8</td><td>92.1</td><td>27.0 / 56.5 / 68.8</td><td>26.1 / 56.9 / 68.9</td></tr></table>", "caption": "Table 4: Impact of self-supervised pre-training on video inputs. All pre-training are conducted on WebVid [25].", "list_citation_info": ["[25] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td>TGIF-</td><td>TGIF-</td><td>MSRVTT-</td><td>DiDeMo-</td></tr><tr><td>Action</td><td>Transition</td><td>Retrieval</td><td>Retrieval</td></tr><tr><td colspan=\"4\">Without Pre-training</td><td></td></tr><tr><td>VIOLET</td><td>81.9</td><td>88.5</td><td>13.0 / 36.5 / 49.6</td><td>18.3 / 46.4 / 56.5</td></tr><tr><td colspan=\"4\">Pre-training on COCO+VG</td><td></td></tr><tr><td>ClipBERT [18]</td><td>82.8</td><td>87.8</td><td>22.0 / 46.8 / 59.9</td><td>20.4 / 48.0 / 60.8</td></tr><tr><td>VIOLET</td><td>84.8</td><td>90.2</td><td>23.5 / 50.5 / 63.9</td><td>22.8 / 51.2 / 62.0</td></tr><tr><td colspan=\"4\">Pre-training on WebVid+CC</td><td></td></tr><tr><td>Frozen [25]</td><td>-</td><td>-</td><td>31.0 / 59.5 / 70.5</td><td>31.0 / 59.8 / 72.4</td></tr><tr><td>VIOLET</td><td>87.1</td><td>93.6</td><td>34.2 / 63.5 / 73.6</td><td>32.9 / 63.0 / 74.5</td></tr><tr><td colspan=\"4\">Pre-training on YTTemporal</td><td></td></tr><tr><td>MERLOT [19]</td><td>94.0</td><td>96.2</td><td>-</td><td>-</td></tr><tr><td>VIOLET</td><td>91.0</td><td>94.7</td><td>25.4 / 54.3 / 64.6</td><td>26.7 / 56.4 / 64.6</td></tr></table>", "caption": "Table 5: Impact of using different pre-training data. We gray out MERLOT due to its excessive computational cost (e.g., 30K TPU hours vs. 2K GPU hours (ours) for pre-training and frame resolution 384x704 vs. 224x224 (ours) for downstream tasks).", "list_citation_info": ["[25] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In International Conference on Computer Vision (ICCV), 2021.", "[18] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[19] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. MERLOT: Multimodal Neural Script Knowledge Models. In Conference on Neural Information Processing Systems (NeurIPS), 2021."]}, {"table": "<table><tr><td>VideoQA</td><td>Task</td><td>#Option</td></tr><tr><td rowspan=\"4\">Multiple-Choice</td><td>TGIF-Action [6]</td><td>5</td></tr><tr><td>TGIF-Transition [6]</td><td>5</td></tr><tr><td>MSRVTT-MC [7]</td><td>5</td></tr><tr><td>LSMDC-MC [3]</td><td>5</td></tr><tr><td rowspan=\"4\">Open-Ended</td><td>TGIF-Frame [6]</td><td>1,540</td></tr><tr><td>MSRVTT-QA [7]</td><td>1,500</td></tr><tr><td>MSVD-QA [88]</td><td>1,000</td></tr><tr><td>LSMDC-FiB [81]</td><td>908</td></tr></table>", "caption": "Table 6: Summary of video question answering tasks.", "list_citation_info": ["[7] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video Question Answering via Gradually Refined Attention over Appearance and Motion. In ACM Multimedia (ACMMM), 2017.", "[88] David L. Chen and William B. Dolan. Collecting Highly Parallel Data for Paraphrase Evaluation. In Annual Meetings of the Association for Computational Linguistics (ACL), 2011.", "[81] Atousa Torabi, Niket Tandon, and Leonid Sigal. Learning Language-Visual Embedding for Movie Understanding with Natural-Language. In arXiv:1609.08124, 2016.", "[6] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[3] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A Dataset for Movie Description. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015."]}, {"table": "<table><tr><td>Masking</td><td>TGIF-</td><td>TGIF-</td><td>MSRVTT-</td><td>DiDeMo-</td></tr><tr><td>Strategy</td><td>Action</td><td>Transition</td><td>Retrieval</td><td>Retrieval</td></tr><tr><td colspan=\"5\">Without pre-training</td></tr><tr><td>None</td><td>81.9</td><td>88.5</td><td>13.0 / 36.5 / 49.6</td><td>18.3 / 46.4 / 56.5</td></tr><tr><td colspan=\"5\">Pre-train on WebVid [25] with VTM+MLM+MVM</td></tr><tr><td>Random</td><td>83.7</td><td>90.8</td><td>24.3 / 54.8 / 66.7</td><td>24.2 / 53.5 / 67.6</td></tr><tr><td>BM</td><td>85.4</td><td>91.8</td><td>27.0 / 56.2 / 68.6</td><td>25.8 / 56.8 / 68.8</td></tr><tr><td>AM</td><td>85.5</td><td>91.6</td><td>26.8 / 56.5 / 68.7</td><td>26.0 / 56.8 / 68.6</td></tr><tr><td>BM+AM</td><td>85.8</td><td>92.1</td><td>27.0 / 56.5 / 68.8</td><td>26.1 / 56.9 / 68.9</td></tr></table>", "caption": "Table 7: Impact of masking strategy in MVM and MLM.", "list_citation_info": ["[25] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tr><td>Method</td><td>Frame Size</td><td>VCR</td></tr><tr><td>MERLOT [19]</td><td>384x704</td><td>75.1</td></tr><tr><td>VIOLET</td><td>224x224</td><td>74.9</td></tr><tr><td>VIOLET</td><td>384x384</td><td>76.3</td></tr></table>", "caption": "Table 8: Comparison with MERLOT [19] under the same pre-training epoch on VCR [89]. The pre-training are conducted on YT-Temporal [19] for 5 epochs.", "list_citation_info": ["[89] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From Recognition to Cognition: Visual Commonsense Reasoning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[19] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. MERLOT: Multimodal Neural Script Knowledge Models. In Conference on Neural Information Processing Systems (NeurIPS), 2021."]}]}