{"title": "Group-wise correlation stereo network", "abstract": "Stereo matching estimates the disparity between a rectified image pair, which is of great importance to depth sensing, autonomous driving, and other related tasks. Previous works built cost volumes with cross-correlation or concatenation of left and right features across all disparity levels, and then a 2D or 3D convolutional neural network is utilized to regress the disparity maps. In this paper, we propose to construct the cost volume by group-wise correlation. The left features and the right features are divided into groups along the channel dimension, and correlation maps are computed among each group to obtain multiple matching cost proposals, which are then packed into a cost volume. Group-wise correlation provides efficient representations for measuring feature similarities and will not lose too much information like full correlation. It also preserves better performance when reducing parameters compared with previous methods. The 3D stacked hourglass network proposed in previous works is improved to boost the performance and decrease the inference computational cost. Experiment results show that our method outperforms previous methods on Scene Flow, KITTI 2012, and KITTI 2015 datasets. The code is available at https://github.com/xy-guo/GwcNet", "authors": ["Xiaoyang Guo", " Kai Yang", " Wukui Yang", " Xiaogang Wang", " Hongsheng Li"], "pdf_url": "https://arxiv.org/abs/1903.04025", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td><p>Concat Volume</p></td><td><p>Group Corr Volume</p></td><td><p>Stack Hourglass</p></td><td><p>Groups {\\times} Channels</p></td><td><p>Init Volume Channel</p></td><td><p>&gt;1px (%)</p></td><td><p>&gt;2px (%)</p></td><td><p>&gt;3px (%)</p></td><td><p>EPE (px)</p></td><td><p>Time (ms)</p></td></tr><tr><th>Cat64-Base</th><td><p>\u2713</p></td><td></td><td></td><td><p>-</p></td><td><p>64</p></td><td><p>12.78</p></td><td><p>8.05</p></td><td><p>6.33</p></td><td><p>1.308</p></td><td><p>117.1</p></td></tr><tr><th>Gwc1-Base</th><td></td><td><p>\u2713</p></td><td></td><td><p>1{\\times}320</p></td><td><p>1</p></td><td><p>13.32</p></td><td><p>8.37</p></td><td><p>6.62</p></td><td><p>1.369</p></td><td><p>104.0</p></td></tr><tr><th>Gwc10-Base</th><td></td><td><p>\u2713</p></td><td></td><td><p>10{\\times}32</p></td><td><p>10</p></td><td><p>11.82</p></td><td><p>7.31</p></td><td><p>5.70</p></td><td><p>1.230</p></td><td><p>112.8</p></td></tr><tr><th>Gwc20-Base</th><td></td><td><p>\u2713</p></td><td></td><td><p>20{\\times}16</p></td><td><p>20</p></td><td><p>11.84</p></td><td><p>7.29</p></td><td><p>5.67</p></td><td><p>1.216</p></td><td><p>116.3</p></td></tr><tr><th>Gwc40-Base</th><td></td><td><p>\u2713</p></td><td></td><td><p>40{\\times}8</p></td><td><p>40</p></td><td><p>11.68</p></td><td><p>7.18</p></td><td><p>5.58</p></td><td><p>1.212</p></td><td><p>122.2</p></td></tr><tr><th>Gwc80-Base</th><td></td><td><p>\u2713</p></td><td></td><td><p>80{\\times}4</p></td><td><p>80</p></td><td><p>11.69</p></td><td><p>7.17</p></td><td><p>5.57</p></td><td><p>1.214</p></td><td><p>133.3</p></td></tr><tr><th>Gwc160-Base</th><td></td><td><p>\u2713</p></td><td></td><td><p>160{\\times}2</p></td><td><p>160</p></td><td><p>11.58</p></td><td><p>7.08</p></td><td><p>5.49</p></td><td><p>1.188</p></td><td><p>157.3</p></td></tr><tr><th>Gwc40-Cat24-Base</th><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>40{\\times}8</p></td><td><p>40+24</p></td><td>11.26</td><td>6.87</td><td>5.31</td><td>1.127</td><td><p>135.1</p></td></tr><tr><th>PSMNet [2]</th><td><p>\u2713</p></td><td></td><td><p>[2]</p></td><td><p>-</p></td><td><p>64</p></td><td><p>9.46</p></td><td><p>5.19</p></td><td><p>3.80</p></td><td><p>0.887</p></td><td><p>246.1</p></td></tr><tr><th>Cat64-original-hg</th><td><p>\u2713</p></td><td></td><td><p>[2]</p></td><td><p>-</p></td><td><p>64</p></td><td><p>9.47</p></td><td><p>5.13</p></td><td><p>3.74</p></td><td><p>0.876</p></td><td><p>241.0</p></td></tr><tr><th>Cat64</th><td><p>\u2713</p></td><td></td><td><p>Ours</p></td><td><p>-</p></td><td><p>64</p></td><td><p>8.41</p></td><td><p>4.63</p></td><td><p>3.41</p></td><td><p>0.808</p></td><td><p>198.3</p></td></tr><tr><th>Gwc40 (GwcNet-g)</th><td></td><td><p>\u2713</p></td><td><p>Ours</p></td><td><p>40{\\times}8</p></td><td><p>40</p></td><td><p>8.18</p></td><td><p>4.57</p></td><td><p>3.39</p></td><td><p>0.792</p></td><td><p>200.3</p></td></tr><tr><th>Gwc40-Cat24 (GwcNet-gc)</th><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>Ours</p></td><td><p>40{\\times}8</p></td><td><p>40+24</p></td><td>8.03</td><td>4.47</td><td>3.30</td><td>0.765</td><td><p>210.7</p></td></tr></tbody></table>", "caption": "Table 2: Ablation study results of proposed networks on the Finalpass of Scene Flow datasets [14]. Cat, Gwc, Gwc-Cat represent only concatenation volume, only group-wise correlation volume, or the both. Base denotes the network variants without stacked hourglass networks. The time is the inference time for 480{\\times}640 inputs on a single Nvidia TITAN Xp GPU. The result of PSMNet [2] is trained with published code with our batch size, evaluation settings for fair comparison.", "list_citation_info": ["[2] J.-R. Chang and Y.-S. Chen. Pyramid stereo matching network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5410\u20135418, 2018.", "[14] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040\u20134048, 2016."]}, {"table": "<table><thead><tr><th>Model</th><th><p>KITTI 12 EPE (px)</p></th><th><p>KITTI 12 D1-all(%)</p></th><th><p>KITTI 15 EPE (px)</p></th><th><p>KITTI 15 D1-all (%)</p></th></tr></thead><tbody><tr><th>PSMNet [2]</th><td><p>0.713</p></td><td><p>2.53</p></td><td><p>0.639</p></td><td><p>1.50</p></td></tr><tr><th>Cat64-original-hg</th><td><p>0.740</p></td><td><p>2.72</p></td><td><p>0.652</p></td><td><p>1.76</p></td></tr><tr><th>Cat64</th><td><p>0.691</p></td><td><p>2.41</p></td><td><p>0.615</p></td><td><p>1.55</p></td></tr><tr><th>Gwc40</th><td><p>0.662</p></td><td><p>2.30</p></td><td>0.602</td><td>1.41</td></tr><tr><th>Gwc40-Cat24</th><td>0.659</td><td>2.10</td><td><p>0.613</p></td><td><p>1.49</p></td></tr></tbody></table>", "caption": "Table 3: Ablation study results of our networks on KITTI 2012 validation and KITTI 2015 validation sets. ", "list_citation_info": ["[2] J.-R. Chang and Y.-S. Chen. Pyramid stereo matching network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5410\u20135418, 2018."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">All (%)</th><th colspan=\"3\">Noc (%)</th><th><p>Time</p></th></tr><tr><th></th><th><p>D1-bg</p></th><th><p>D1-fg</p></th><th><p>D1-all</p></th><th><p>D1-bg</p></th><th><p>D1-fg</p></th><th><p>D1-all</p></th><th><p>(s)</p></th></tr></thead><tbody><tr><th>DispNetC [14]</th><td><p>4.32</p></td><td><p>4.41</p></td><td><p>4.34</p></td><td><p>4.11</p></td><td><p>3.72</p></td><td><p>4.05</p></td><td><p>0.06</p></td></tr><tr><th>GC-Net [6]</th><td><p>2.21</p></td><td><p>6.16</p></td><td><p>2.87</p></td><td><p>2.02</p></td><td><p>5.58</p></td><td><p>2.61</p></td><td><p>0.9</p></td></tr><tr><th>CRL [17]</th><td><p>2.48</p></td><td><p>3.59</p></td><td><p>2.67</p></td><td><p>2.32</p></td><td><p>3.12</p></td><td><p>2.45</p></td><td><p>0.47</p></td></tr><tr><th>iResNet-i2e2 [12]</th><td><p>2.14</p></td><td><p>3.45</p></td><td><p>2.36</p></td><td><p>1.94</p></td><td><p>3.20</p></td><td><p>2.15</p></td><td><p>0.22</p></td></tr><tr><th>PSMNet [6]</th><td><p>1.86</p></td><td><p>4.62</p></td><td><p>2.32</p></td><td><p>1.71</p></td><td><p>4.31</p></td><td><p>2.14</p></td><td><p>0.41</p></td></tr><tr><th>SegStereo [26]</th><td><p>1.88</p></td><td><p>4.07</p></td><td><p>2.25</p></td><td><p>1.76</p></td><td><p>3.70</p></td><td><p>2.08</p></td><td><p>0.6</p></td></tr><tr><th>GwcNet-g (Gwc40)</th><td>1.74</td><td>3.93</td><td>2.11</td><td>1.61</td><td>3.49</td><td>1.92</td><td><p>0.32</p></td></tr></tbody></table>", "caption": "Table 4: KITTI 2015 test set results. The dataset contains 200 images for training and 200 images for testing. ", "list_citation_info": ["[14] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040\u20134048, 2016.", "[12] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, and L. Q. L. Z. J. Zhang. Learning for disparity estimation through feature constancy. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2811\u20132820, 2018.", "[6] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach, and A. Bry. End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE International Conference on Computer Vision, pages 66\u201375, 2017.", "[26] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia. Segstereo: Exploiting semantic information for disparity estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 636\u2013651, 2018.", "[17] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan. Cascade residual learning: A two-stage convolutional neural network for stereo matching. In ICCV Workshops, volume 7, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">&gt;2px (%)</th><th colspan=\"2\">&gt;3px (%)</th><th colspan=\"2\">&gt;5px (%)</th><th colspan=\"2\">Mean Error (px)</th><th><p>Time</p></th></tr><tr><th></th><th><p>Noc</p></th><th><p>All</p></th><th><p>Noc</p></th><th><p>All</p></th><th><p>Noc</p></th><th><p>All</p></th><th><p>Noc</p></th><th><p>All</p></th><th><p>(s)</p></th></tr></thead><tbody><tr><th>DispNetC [14]</th><td><p>7.38</p></td><td><p>8.11</p></td><td><p>4.11</p></td><td><p>4.65</p></td><td><p>2.05</p></td><td><p>2.39</p></td><td><p>0.9</p></td><td><p>1.0</p></td><td><p>0.06</p></td></tr><tr><th>MC-CNN-acrt [29]</th><td><p>3.90</p></td><td><p>5.45</p></td><td><p>2.43</p></td><td><p>3.63</p></td><td><p>1.64</p></td><td><p>2.39</p></td><td><p>0.7</p></td><td><p>0.9</p></td><td><p>67</p></td></tr><tr><th>GC-Net [6]</th><td><p>2.71</p></td><td><p>3.46</p></td><td><p>1.77</p></td><td><p>2.30</p></td><td><p>1.12</p></td><td><p>1.46</p></td><td><p>0.6</p></td><td><p>0.7</p></td><td><p>0.9</p></td></tr><tr><th>iResNet-i2 [12]</th><td><p>2.69</p></td><td><p>3.34</p></td><td><p>1.71</p></td><td><p>2.16</p></td><td><p>1.06</p></td><td><p>1.32</p></td><td><p>0.5</p></td><td><p>0.6</p></td><td><p>0.12</p></td></tr><tr><th>SegStereo [26]</th><td><p>2.66</p></td><td><p>3.19</p></td><td><p>1.68</p></td><td><p>2.03</p></td><td><p>1.00</p></td><td><p>1.21</p></td><td><p>0.5</p></td><td><p>0.6</p></td><td><p>0.6</p></td></tr><tr><th>PSMNet [6]</th><td><p>2.44</p></td><td><p>3.01</p></td><td><p>1.49</p></td><td><p>1.89</p></td><td><p>0.90</p></td><td><p>1.15</p></td><td><p>0.5</p></td><td><p>0.6</p></td><td><p>0.41</p></td></tr><tr><th>GwcNet-gc (Gwc40-Cat24)</th><td>2.16</td><td>2.71</td><td>1.32</td><td>1.70</td><td>0.80</td><td>1.03</td><td>0.5</td><td>0.5</td><td><p>0.32</p></td></tr></tbody></table>", "caption": "Table 5: KITTI 2012 test set results. The dataset contains 194 images for training and 195 images for testing. ", "list_citation_info": ["[14] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040\u20134048, 2016.", "[12] Z. Liang, Y. Feng, Y. G. H. L. W. Chen, and L. Q. L. Z. J. Zhang. Learning for disparity estimation through feature constancy. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2811\u20132820, 2018.", "[6] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy, A. Bachrach, and A. Bry. End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE International Conference on Computer Vision, pages 66\u201375, 2017.", "[26] G. Yang, H. Zhao, J. Shi, Z. Deng, and J. Jia. Segstereo: Exploiting semantic information for disparity estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 636\u2013651, 2018.", "[29] J. Zbontar and Y. LeCun. Computing the stereo matching cost with a convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1592\u20131599, 2015."]}]}