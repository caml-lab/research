{"title": "Unsupervised 3D Shape Completion through GAN Inversion", "abstract": "Most 3D shape completion approaches rely heavily on partial-complete shape pairs and learn in a fully supervised manner. Despite their impressive performances on in-domain data, when generalizing to partial shapes in other forms or real-world partial scans, they often obtain unsatisfactory results due to domain gaps. In contrast to previous fully supervised approaches, in this paper we present ShapeInversion, which introduces Generative Adversarial Network (GAN) inversion to shape completion for the first time. ShapeInversion uses a GAN pre-trained on complete shapes by searching for a latent code that gives a complete shape that best reconstructs the given partial input. In this way, ShapeInversion no longer needs paired training data, and is capable of incorporating the rich prior captured in a well-trained generative model. On the ShapeNet benchmark, the proposed ShapeInversion outperforms the SOTA unsupervised method, and is comparable with supervised methods that are learned using paired data. It also demonstrates remarkable generalization ability, giving robust results for real-world scans and partial inputs of various forms and incompleteness levels. Importantly, ShapeInversion naturally enables a series of additional abilities thanks to the involvement of a pre-trained GAN, such as producing multiple valid complete shapes for an ambiguous partial input, as well as shape manipulation and interpolation.", "authors": ["Junzhe Zhang", " Xinyi Chen", " Zhongang Cai", " Liang Pan", " Haiyu Zhao", " Shuai Yi", " Chai Kiat Yeo", " Bo Dai", " Chen Change Loy"], "pdf_url": "https://arxiv.org/abs/2104.13366", "list_table_and_caption": [{"table": "<table><thead><tr><th>Methods</th><th>Plane</th><th>Cabinet</th><th>Car</th><th>Chair</th><th>Lamp</th><th>Sofa</th><th>Table</th><th>Boat</th><th>Average</th></tr></thead><tbody><tr><td>tree-GAN baseline</td><td>30.7</td><td>52.9</td><td>38.4</td><td>58.6</td><td>59.6</td><td>41.2</td><td>57.1</td><td>42.9</td><td>47.7</td></tr><tr><td>tree-GAN + expansion penality [21]</td><td>39.7</td><td>68.7</td><td>41.0</td><td>59.3</td><td>66.7</td><td>55.4</td><td>66.5</td><td>40.3</td><td>54.7</td></tr><tr><td>tree-GAN + repulsion loss [35]</td><td>29.8</td><td>54.5</td><td>36.9</td><td>53.2</td><td>61.3</td><td>44.9</td><td>56.1</td><td>40.7</td><td>47.2</td></tr><tr><td>tree-GAN + PatchVariance (ours)</td><td>28.1</td><td>35.0</td><td>30.9</td><td>45.9</td><td>52.1</td><td>35.5</td><td>47.7</td><td>36.9</td><td>39.0</td></tr></tbody></table>", "caption": "Table 1: Effectiveness of PatchVariance on the shape uniformity. PatchVariance achieves the lowest MMD-EMD \\downarrow (scaled by 10^{3}) across all the eight categories from ShapeNet, indicating the best uniformity and fidelity for the generated shapes", "list_citation_info": ["[21] Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi-Min Hu. Morphing and sampling network for dense point cloud completion. In AAAI, 2019.", "[35] Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. PU-Net: Point cloud upsampling network. In CVPR, 2018."]}, {"table": "<table><tbody><tr><th></th><th>Methods</th><td>Plane</td><td>Cabinet</td><td>Car</td><td>Chair</td><td>Lamp</td><td>Sofa</td><td>Table</td><td>Boat</td><td>Average</td></tr><tr><th>sup.</th><th>PCN [36]</th><td>3.5/96.5</td><td>11.3/86.4</td><td>6.4/94.0</td><td>11.0/86.0</td><td>11.6/84.6</td><td>11.5/85.2</td><td>10.4/89.4</td><td>7.4/91.7</td><td>9.1/89.2</td></tr><tr><th></th><th>TopNet [28]</th><td>4.1/96.0</td><td>12.9/84.1</td><td>7.8/91.3</td><td>13.4/82.3</td><td>14.8/79.4</td><td>16.0/80.8</td><td>12.9/85.7</td><td>8.9/89.3</td><td>11.4/86.1</td></tr><tr><th></th><th>MSN [21]</th><td>2.9/97.4</td><td>12.5/85.5</td><td>7.1/92.3</td><td>10.6/86.8</td><td>9.3/88.6</td><td>12.0/83.3</td><td>9.6/91.3</td><td>6.5/93.1</td><td>8.8/89.8</td></tr><tr><th></th><th>CRN [30]</th><td>2.3/98.3</td><td>11.4/86.2</td><td>6.2/93.8</td><td>8.8/89.7</td><td>8.5/90.2</td><td>11.3/85.1</td><td>9.3/92.9</td><td>6.1/94.2</td><td>8.0/91.3</td></tr><tr><th>unsup.</th><th>pcl2pcl [7]</th><td>9.8/89.1</td><td>27.1/68.4</td><td>15.8/80.0</td><td>26.9/70.4</td><td>25.7/70.4</td><td>34.1/58.4</td><td>23.6/79.0</td><td>15.7/77.8</td><td>22.4/74.2</td></tr><tr><th></th><th>Ours</th><td>5.6/94.3</td><td>16.1/77.2</td><td>13.0/85.8</td><td>15.4/81.2</td><td>18.0/81.7</td><td>24.6/78.4</td><td>16.2/85.5</td><td>10.1/87.0</td><td>14.9/83.9</td></tr></tbody></table>", "caption": "Table 3: Shape completion results on ShapeNet benchmark. The numbers shown are [CD \\downarrow / F1 \\uparrow], where CD is scaled by 10^{4}. ShapeInversion outperforms pcl2pcl by a large margin, and is comparable to the various supervised methods. sup.: supervised methods; unsup.: unsupervised methods ", "list_citation_info": ["[7] Xuelin Chen, Baoquan Chen, and Niloy J Mitra. Unpaired point cloud completion on real scans using adversarial training. In ICLR, 2020.", "[21] Minghua Liu, Lu Sheng, Sheng Yang, Jing Shao, and Shi-Min Hu. Morphing and sampling network for dense point cloud completion. In AAAI, 2019.", "[36] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert. PCN: Point completion network. In 3DV, 2018.", "[30] Xiaogang Wang, Marcelo H Ang Jr, and Gim Hee Lee. Cascaded refinement network for point cloud completion. In CVPR, 2020.", "[28] Lyne P Tchapmi, Vineet Kosaraju, Hamid Rezatofighi, Ian Reid, and Silvio Savarese. TopNet: Structural point cloud decoder. In CVPR, 2019."]}, {"table": "<table><tbody><tr><th>Target</th><td>Methods</td><td>Source</td><td>Chair</td><td>Table</td><td>Lamp</td></tr><tr><th rowspan=\"3\">Virtual scan</th><td>CRN</td><td>Virtual scan</td><td>8.8</td><td>9.3</td><td>8.5</td></tr><tr><td>MPC [33]</td><td>PartNet</td><td>45.9</td><td>88.9</td><td>63.0</td></tr><tr><td>Ours</td><td>-</td><td>15.4</td><td>16.2</td><td>18.0</td></tr><tr><th rowspan=\"6\">Ball-holed</th><td>PF-Net [15]</td><td>Ball-holed</td><td>11.9</td><td>9.9</td><td>23.1</td></tr><tr><td>MSN</td><td>Virtual scan</td><td>79.6</td><td>46.6</td><td>55.4</td></tr><tr><td>CRN</td><td>Virtual scan</td><td>44.7</td><td>52.9</td><td>52.1</td></tr><tr><td>pcl2pcl</td><td>Virtual scan</td><td>18.6</td><td>18.5</td><td>21.2</td></tr><tr><td>MPC</td><td>PartNet</td><td>44.7</td><td>28.9</td><td>69.5</td></tr><tr><td>Ours</td><td>-</td><td>10.1</td><td>16.0</td><td>17.3</td></tr><tr><th rowspan=\"5\">PartNet</th><td>MPC</td><td>PartNet</td><td>40.0</td><td>51.0</td><td>82.0</td></tr><tr><td>MSN</td><td>Virtual scan</td><td>198.0</td><td>143.2</td><td>229.9</td></tr><tr><td>CRN</td><td>Virtual scan</td><td>177.4</td><td>140.6</td><td>185.9</td></tr><tr><td>pcl2pcl</td><td>Virtual scan</td><td>51.0</td><td>76.6</td><td>111.2</td></tr><tr><td>Ours</td><td>-</td><td>36.8</td><td>77.8</td><td>100.8</td></tr></tbody></table>", "caption": "Table 4: Cross-domain validation. We follow the literature to train each method on a certain partial form (source) and cross-validate on other partial forms (targets). For each target domain, the SOTA in-domain results are listed at the first line for reference. Methods, especially supervised ones, usually perform well on the in-domain data but suffer large performance drops on the out-of-domain data, whereas ShapeInversion gives the best results for almost all the cross-domain tests, highlighting its robustness to partial form changes. The metric is CD\\downarrow (\\times 10^{4})", "list_citation_info": ["[33] Rundi Wu, Xuelin Chen, Yixin Zhuang, and Baoquan Chen. Multimodal shape completion via conditional generative adversarial networks. In ECCV, 2020.", "[15] Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le. PF-Net: Point fractal network for 3D point cloud completion. In CVPR, 2020."]}]}