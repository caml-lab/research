{"title": "Uncertainty-aware few-shot image classification", "abstract": "Few-shot image classification learns to recognize new categories from limited labelled data. Metric learning based approaches have been widely investigated, where a query sample is classified by finding the nearest prototype from the support set based on their feature similarities. A neural network has different uncertainties on its calculated similarities of different pairs. Understanding and modeling the uncertainty on the similarity could promote the exploitation of limited samples in few-shot optimization. In this work, we propose Uncertainty-Aware Few-Shot framework for image classification by modeling uncertainty of the similarities of query-support pairs and performing uncertainty-aware optimization. Particularly, we exploit such uncertainty by converting observed similarities to probabilistic representations and incorporate them to the loss for more effective optimization. In order to jointly consider the similarities between a query and the prototypes in a support set, a graph-based model is utilized to estimate the uncertainty of the pairs. Extensive experiments show our proposed method brings significant improvements on top of a strong baseline and achieves the state-of-the-art performance.", "authors": ["Zhizheng Zhang", " Cuiling Lan", " Wenjun Zeng", " Zhibo Chen", " Shih-Fu Chang"], "pdf_url": "https://arxiv.org/abs/2010.04525", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Backbone</td><td colspan=\"2\">mini-ImageNet</td><td colspan=\"2\">tiered-ImageNet</td><td colspan=\"2\">CIFAR-FS</td><td colspan=\"2\">FC-100  \\bigstrut[b]</td></tr><tr><td>1-shot</td><td>5-shot</td><td>1-shot</td><td>5-shot</td><td>1-shot</td><td>5-shot</td><td>1-shot</td><td>5-shot \\bigstrut[t]</td></tr><tr><td>MatchingNet Vinyals et al. (2016)</td><td>64-64-64-64</td><td>46.6</td><td>60.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>- \\bigstrut[t]</td></tr><tr><td>MAML Finn et al. (2017)</td><td>32-32-32-32</td><td>48.70 \\pm 1.84</td><td>63.11 \\pm 0.92</td><td>51.67 \\pm 1.81</td><td>70.30 \\pm 1.75</td><td>-</td><td>-</td><td>-</td><td>- \\bigstrut[t]</td></tr><tr><td>ProtoNet{}^{\\dagger} Snell et al. (2017)</td><td>64-64-64-64</td><td>49.42 \\pm 0.78</td><td>68.20 \\pm 0.66</td><td>53.31 \\pm 0.89</td><td>72.69 \\pm 0.74</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RelationNet Sung et al. (2018)</td><td>64-96-128-256</td><td>50.44 \\pm 0.82</td><td>65.32 \\pm 0.70</td><td>54.48 \\pm 0.93</td><td>71.32 \\pm 0.78</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TADAM Oreshkin et al. (2018)</td><td>ResNet-12</td><td>58.50 \\pm 0.30</td><td>76.70 \\pm 0.30</td><td>-</td><td>-</td><td>-</td><td>-</td><td>40.10 \\pm 0.40</td><td>56.10 \\pm 0.40</td></tr><tr><td>LEO{}^{\\dagger} Rusu et al. (2019)</td><td>WRN-28-10</td><td>61.76 \\pm 0.08</td><td>77.59 \\pm 0.10</td><td>66.33 \\pm 0.05</td><td>81.44 \\pm 0.09</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TapNet Yoon et al. (2019)</td><td>ResNet-12</td><td>61.65 \\pm 0.15</td><td>76.36 \\pm 0.10</td><td>63.08 \\pm 0.15</td><td>80.26 \\pm 0.12</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Shot-free Ravichandran et al. (2019)</td><td>ResNet-12</td><td>59.04 \\pm 0.43</td><td>77.64 \\pm 0:39</td><td>66.87 \\pm 0.43</td><td>82.64 \\pm 0.39</td><td>69.20 \\pm 0.40</td><td>84.70 \\pm 0.40</td><td>-</td><td>-</td></tr><tr><td>MetaOptNet Lee et al. (2019)</td><td>ResNet-12</td><td>62.64 \\pm 0.61</td><td>78.63 \\pm 0.46</td><td>65.99 \\pm 0.72</td><td>81.56 \\pm 0.53</td><td>72.00 \\pm 0.70</td><td>84.20 \\pm 0.50</td><td>41.10 \\pm 0.60</td><td>55.50 \\pm 0.60</td></tr><tr><td>CAN Hou et al. (2019)</td><td>ResNet-12</td><td>63.85 \\pm 0.48</td><td>79.44 \\pm 0.34</td><td>69.89 \\pm 0.51</td><td>84.23 \\pm 0.37</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Baseline2020 Dhillon et al. (2020)</td><td>WRN-28-10</td><td>56.17 \\pm 0.64</td><td>73.31 \\pm 0.53</td><td>67.45 \\pm 0.70</td><td>82.88 \\pm 0.53</td><td>70.26 \\pm 0.70</td><td>83.82 \\pm 0.49</td><td>36.82 \\pm 0.51</td><td>49.72 \\pm 0.55</td></tr><tr><td>MetaBaseline Chen et al. (2020){}^{1}</td><td>ResNet-12</td><td>63.17 \\pm 0.23</td><td>79.26 \\pm 0.17</td><td>68.62 \\pm 0.27</td><td>83.29 \\pm 0.18</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Meta-Base</td><td>ResNet-12</td><td>63.10 \\pm 0.85</td><td>79.44 \\pm 0.65</td><td>67.72 \\pm 0.80</td><td>83.61 \\pm 0.62</td><td>72.36 \\pm 0.67</td><td>84.43 \\pm 0.50</td><td>40.23 \\pm 0.22</td><td>56.16 \\pm 0.56 \\bigstrut[t]</td></tr><tr><td>Meta-UAFS</td><td>ResNet-12</td><td>64.22 \\pm 0.67</td><td>79.99 \\pm 0.49</td><td>69.13 \\pm 0.84</td><td>84.33 \\pm 0.59</td><td>74.08 \\pm 0.72</td><td>85.92 \\pm 0.42</td><td>41.99 \\pm 0.58</td><td>57.43 \\pm 0.38</td></tr></tbody></table>", "caption": "Table 3: Accuracy (%) comparison for 5-way few-shot classification of our schemes, our baselines and the state-of-the-arts on four benchmark datasets. For the backbone network, \u201cl_{1}-l_{2}-l_{3}-l_{4}\u201d denotes a 4-layer convolurional network with the number of convolutional filters in each layer, respectively. \u201cMeta-Base\u201d and \u201cMeta-UAFS\u201d refer to the strong baseline model and our final UAFS model trained with classification-based pre-training stage and meta-learning stage, respectively. The superscript {}^{\\dagger} refers to that the model is trained on the combination of training and validation sets while others only use the training set. Note that for fair comparisons, all the presented results are from inductive (non-transductive) setting. Bold numbers denotes the best performance while numbers with underlines denotes the second best performance.", "list_citation_info": ["Oreshkin et al. [2018] Boris Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. In NeurIPS, 2018.", "Snell et al. [2017] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS, 2017.", "Dhillon et al. [2020] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification. In ICLR, 2020.", "Sung et al. [2018] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, et al. Learning to compare: Relation network for few-shot learning. In CVPR, 2018.", "Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017.", "Yoon et al. [2019] Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-adaptive projection for few-shot learning. In ICML, 2019.", "Ravichandran et al. [2019] Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto. Few-shot learning with embedded class models and shot-free meta training. In ICCV, 2019.", "Chen et al. [2020] Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A new meta-baseline for few-shot learning. arXiv preprint arXiv:2003.04390, 2020.", "Vinyals et al. [2016] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In NeurIPS, 2016.", "Lee et al. [2019] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. In CVPR, 2019.", "Hou et al. [2019] Ruibing Hou, Hong Chang, MA Bingpeng, Shiguang Shan, and Xilin Chen. Cross attention network for few-shot classification. In NeurIPS, 2019.", "Rusu et al. [2019] Andrei A Rusu, Dushyant Rao, et al. Meta-learning with latent embedding optimization. In ICLR, 2019."]}]}