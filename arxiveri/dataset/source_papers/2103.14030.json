{"title": "Swin transformer: Hierarchical vision transformer using shifted windows", "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.", "authors": ["Ze Liu", " Yutong Lin", " Yue Cao", " Han Hu", " Yixuan Wei", " Zheng Zhang", " Stephen Lin", " Baining Guo"], "pdf_url": "https://arxiv.org/abs/2103.14030", "list_table_and_caption": [{"table": "<table><tr><td colspan=\"3\">   ADE20K</td><td>val</td><td>test</td><td rowspan=\"2\">#param.</td><td rowspan=\"2\">FLOPs</td><td rowspan=\"2\">FPS</td></tr><tr><td>Method</td><td>Backbone</td><td>mIoU</td><td>score</td><td></td></tr><tr><td>DANet [23]</td><td>ResNet-101</td><td>45.2</td><td>-</td><td>69M</td><td>1119G</td><td>15.2</td></tr><tr><td>DLab.v3+ [11]</td><td>ResNet-101</td><td>44.1</td><td>-</td><td>63M</td><td>1021G</td><td>16.0</td></tr><tr><td>ACNet [24]</td><td>ResNet-101</td><td>45.9</td><td>38.5</td><td>-</td><td></td><td></td></tr><tr><td>DNL [71]</td><td>ResNet-101</td><td>46.0</td><td>56.2</td><td>69M</td><td>1249G</td><td>14.8</td></tr><tr><td>OCRNet [73]</td><td>ResNet-101</td><td>45.3</td><td>56.0</td><td>56M</td><td>923G</td><td>19.3</td></tr><tr><td>UperNet [69]</td><td>ResNet-101</td><td>44.9</td><td>-</td><td>86M</td><td>1029G</td><td>20.1</td></tr><tr><td>OCRNet [73]</td><td>HRNet-w48</td><td>45.7</td><td>-</td><td>71M</td><td>664G</td><td>12.5</td></tr><tr><td>DLab.v3+ [11]</td><td>ResNeSt-101</td><td>46.9</td><td>55.1</td><td>66M</td><td>1051G</td><td>11.9</td></tr><tr><td>DLab.v3+ [11]</td><td>ResNeSt-200</td><td>48.4</td><td>-</td><td>88M</td><td>1381G</td><td>8.1</td></tr><tr><td>SETR [81]</td><td>T-Large{}^{\\ddagger}</td><td>50.3</td><td>61.7</td><td>308M</td><td>-</td><td>-</td></tr><tr><td>UperNet</td><td>DeiT-S{}^{\\dagger}</td><td>44.0</td><td>-</td><td>52M</td><td>1099G</td><td>16.2</td></tr><tr><td>UperNet</td><td>Swin-T</td><td>46.1</td><td>-</td><td>60M</td><td>945G</td><td>18.5</td></tr><tr><td>UperNet</td><td>Swin-S</td><td>49.3</td><td>-</td><td>81M</td><td>1038G</td><td>15.2</td></tr><tr><td>UperNet</td><td>Swin-B{}^{\\ddagger}</td><td>51.6</td><td>-</td><td>121M</td><td>1841G</td><td>8.7</td></tr><tr><td>UperNet</td><td>Swin-L{}^{\\ddagger}</td><td>53.5</td><td>62.8</td><td>234M</td><td>3230G</td><td>6.2</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 3: Results of semantic segmentation on the ADE20K val and test set. {}^{\\dagger} indicates additional deconvolution layers are used to produce hierarchical feature maps. {\\ddagger} indicates that the model is pre-trained on ImageNet-22K.", "list_citation_info": ["[81] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020.", "[73] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In 16th European Conference Computer Vision (ECCV 2020), August 2020.", "[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.", "[23] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3146\u20133154, 2019.", "[71] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In Proceedings of the European conference on computer vision (ECCV), 2020.", "[69] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418\u2013434, 2018.", "[24] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive context network for scene parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6748\u20136757, 2019."]}, {"table": "<table><tr><td rowspan=\"2\"> method</td><td colspan=\"4\">MSA in a stage (ms)</td><td colspan=\"3\">Arch. (FPS)</td></tr><tr><td>S1</td><td>S2</td><td>S3</td><td>S4</td><td>T</td><td>S</td><td>B</td></tr><tr><td>sliding window (naive)</td><td>122.5</td><td>38.3</td><td>12.1</td><td>7.6</td><td>183</td><td>109</td><td>77</td></tr><tr><td>sliding window (kernel)</td><td>7.6</td><td>4.7</td><td>2.7</td><td>1.8</td><td>488</td><td>283</td><td>187</td></tr><tr><td>Performer [14]</td><td>4.8</td><td>2.8</td><td>1.8</td><td>1.5</td><td>638</td><td>370</td><td>241</td></tr><tr><td>window (w/o shifting)</td><td>2.8</td><td>1.7</td><td>1.2</td><td>0.9</td><td>770</td><td>444</td><td>280</td></tr><tr><td>shifted window (padding)</td><td>3.3</td><td>2.3</td><td>1.9</td><td>2.2</td><td>670</td><td>371</td><td>236</td></tr><tr><td>shifted window (cyclic)</td><td>3.0</td><td>1.9</td><td>1.3</td><td>1.0</td><td>755</td><td>437</td><td>278</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 5: Real speed of different self-attention computation methods and implementations on a V100 GPU. ", "list_citation_info": ["[14] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021."]}, {"table": "<table><tr><td> </td><td></td><td colspan=\"2\">ImageNet</td><td colspan=\"2\">COCO</td><td>ADE20k</td></tr><tr><td></td><td>Backbone</td><td>top-1</td><td>top-5</td><td>AP{}^{\\text{box}}</td><td>AP{}^{\\text{mask}}</td><td>mIoU</td></tr><tr><td>sliding window</td><td>Swin-T</td><td>81.4</td><td>95.6</td><td>50.2</td><td>43.5</td><td>45.8</td></tr><tr><td>Performer [14]</td><td>Swin-T</td><td>79.0</td><td>94.2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>shifted window</td><td>Swin-T</td><td>81.3</td><td>95.6</td><td>50.5</td><td>43.7</td><td>46.1</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 6: Accuracy of Swin Transformer using different methods for self-attention computation on three benchmarks.", "list_citation_info": ["[14] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021."]}]}