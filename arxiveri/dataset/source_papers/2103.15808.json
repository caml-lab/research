{"title": "Cvt: Introducing convolutions to vision transformers", "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (\\ie shift, scale, and distortion invariance) while maintaining the merits of Transformers (\\ie dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (\\eg ImageNet-22k) and fine-tuned to downstream tasks. Pre-trained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\\% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks. Code will be released at \\url{https://github.com/leoxiaobin/CvT}.", "authors": ["Haiping Wu", " Bin Xiao", " Noel Codella", " Mengchen Liu", " Xiyang Dai", " Lu Yuan", " Lei Zhang"], "pdf_url": "https://arxiv.org/abs/2103.15808", "list_table_and_caption": [{"table": "<table><thead><tr><th>  Method</th><th>Needs Position Encoding (PE)</th><th>  Token Embedding</th><th>  Projection for Attention</th><th>Hierarchical Transformers</th></tr></thead><tbody><tr><td>ViT [11], DeiT [30]</td><td>yes</td><td>non-overlapping</td><td>linear</td><td>no</td></tr><tr><td>CPVT [6]</td><td>no (w/ PE Generator)</td><td>non-overlapping</td><td>linear</td><td>no</td></tr><tr><td>TNT [14]</td><td>yes</td><td>non-overlapping (patch+pixel)</td><td>linear</td><td>no</td></tr><tr><td>T2T [41]</td><td>yes</td><td>overlapping (concatenate)</td><td>linear</td><td>partial (tokenization)</td></tr><tr><td>PVT [34]</td><td>yes</td><td>non-overlapping</td><td>spatial reduction</td><td>yes</td></tr><tr><td>CvT (ours)</td><td>no</td><td>overlapping (convolution)</td><td>convolution</td><td>yes</td></tr></tbody></table>", "caption": "Table 1: Representative works of vision Transformers.", "list_citation_info": ["[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.", "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.", "[34] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.", "[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.", "[6] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit position encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.", "[41] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021."]}, {"table": "<table><tbody><tr><td></td><td></td><td>#Param.</td><td>image</td><td>FLOPs</td><td>ImageNet</td><td>Real</td><td>V2</td></tr><tr><td> Method Type</td><td>Network</td><td>(M)</td><td>size</td><td>(G)</td><td>top-1 (%)</td><td>top-1 (%)</td><td>top-1 (%)</td></tr><tr><td rowspan=\"3\"> Convolutional Networks</td><td>ResNet-50 [15]</td><td>25</td><td>224^{2}</td><td>4.1</td><td>76.2</td><td>82.5</td><td>63.3</td></tr><tr><td>ResNet-101 [15]</td><td>45</td><td>224^{2}</td><td>7.9</td><td>77.4</td><td>83.7</td><td>65.7</td></tr><tr><td>ResNet-152 [15]</td><td>60</td><td>224^{2}</td><td>11</td><td>78.3</td><td>84.1</td><td>67.0</td></tr><tr><td rowspan=\"12\"> Transformers</td><td>ViT-B/16 [11]</td><td>86</td><td>384^{2}</td><td>55.5</td><td>77.9</td><td>83.6</td><td>\u2013</td></tr><tr><td>ViT-L/16 [11]</td><td>307</td><td>384^{2}</td><td>191.1</td><td>76.5</td><td>82.2</td><td>\u2013</td></tr><tr><td>DeiT-S [30][arxiv 2020]</td><td>22</td><td>224^{2}</td><td>4.6</td><td>79.8</td><td>85.7</td><td>68.5</td></tr><tr><td>DeiT-B [30][arxiv 2020]</td><td>86</td><td>224^{2}</td><td>17.6</td><td>81.8</td><td>86.7</td><td>71.5</td></tr><tr><td>PVT-Small [34][arxiv 2021]</td><td>25</td><td>224^{2}</td><td>3.8</td><td>79.8</td><td>\u2013</td><td>\u2013</td></tr><tr><td>PVT-Medium [34][arxiv 2021]</td><td>44</td><td>224^{2}</td><td>6.7</td><td>81.2</td><td>\u2013</td><td>\u2013</td></tr><tr><td>PVT-Large [34][arxiv 2021]</td><td>61</td><td>224^{2}</td><td>9.8</td><td>81.7</td><td>\u2013</td><td>\u2013</td></tr><tr><td>T2T-ViT{}_{t}-14 [41][arxiv 2021]</td><td>22</td><td>224^{2}</td><td>6.1</td><td>80.7</td><td>\u2013</td><td>\u2013</td></tr><tr><td>T2T-ViT{}_{t}-19 [41][arxiv 2021]</td><td>39</td><td>224^{2}</td><td>9.8</td><td>81.4</td><td>\u2013</td><td>\u2013</td></tr><tr><td>T2T-ViT{}_{t}-24 [41][arxiv 2021]</td><td>64</td><td>224^{2}</td><td>15.0</td><td>82.2</td><td>\u2013</td><td>\u2013</td></tr><tr><td>TNT-S [14][arxiv 2021]</td><td>24</td><td>224^{2}</td><td>5.2</td><td>81.3</td><td>\u2013</td><td>\u2013</td></tr><tr><td>TNT-B [14][arxiv 2021]</td><td>66</td><td>224^{2}</td><td>14.1</td><td>82.8</td><td>\u2013</td><td>\u2013</td></tr><tr><td rowspan=\"4\"> Convolutional Transformers</td><td>Ours: CvT-13</td><td>20</td><td>224^{2}</td><td>4.5</td><td>81.6</td><td>86.7</td><td>70.4</td></tr><tr><td>Ours: CvT-21</td><td>32</td><td>224^{2}</td><td>7.1</td><td>82.5</td><td>87.2</td><td>71.3</td></tr><tr><td>Ours: CvT-13{}_{\\uparrow 384}</td><td>20</td><td>384^{2}</td><td>16.3</td><td>83.0</td><td>87.9</td><td>71.9</td></tr><tr><td>Ours: CvT-21{}_{\\uparrow 384}</td><td>32</td><td>384^{2}</td><td>24.9</td><td>83.3</td><td>87.7</td><td>71.9</td></tr><tr><td></td><td>Ours: CvT-13-NAS</td><td>18</td><td>224^{2}</td><td>4.1</td><td>82.2</td><td>87.5</td><td>71.3</td></tr><tr><td> \\emph{Convolution Networks}_{22k}</td><td>BiT-M{}_{\\uparrow 480} [18]</td><td>928</td><td>480^{2}</td><td>837</td><td>85.4</td><td>\u2013</td><td>\u2013</td></tr><tr><td rowspan=\"3\"> \\emph{Transformers}_{22k}</td><td>ViT-B/16{}_{\\uparrow 384} [11]</td><td>86</td><td>384^{2}</td><td>55.5</td><td>84.0</td><td>88.4</td><td>\u2013</td></tr><tr><td>ViT-L/16{}_{\\uparrow 384} [11]</td><td>307</td><td>384^{2}</td><td>191.1</td><td>85.2</td><td>88.4</td><td>\u2013</td></tr><tr><td>ViT-H/16{}_{\\uparrow 384} [11]</td><td>632</td><td>384^{2}</td><td>\u2013</td><td>85.1</td><td>88.7</td><td>\u2013</td></tr><tr><td rowspan=\"3\"> \\emph{Convolutional Transformers}_{22k}</td><td>Ours: CvT-13{}_{\\uparrow 384}</td><td>20</td><td>384^{2}</td><td>16</td><td>83.3</td><td>88.7</td><td>72.9</td></tr><tr><td>Ours: CvT-21{}_{\\uparrow 384}</td><td>32</td><td>384^{2}</td><td>25</td><td>84.9</td><td>89.8</td><td>75.6</td></tr><tr><td>Ours: CvT-W24{}_{\\uparrow 384}</td><td>277</td><td>384^{2}</td><td>193.2</td><td>87.7</td><td>90.6</td><td>78.8</td></tr></tbody></table>", "caption": "Table 3: Accuracy of manual designed architecture on ImageNet [9], ImageNet Real [2] and ImageNet V2 matched frequency [26]. Subscript {}_{22k} indicates the model pre-trained on ImageNet22k [9], and finetuned on ImageNet1k with the input size of 384\\times 384, except BiT-M [18] finetuned with input size of 480\\times 480.", "list_citation_info": ["[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.", "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.", "[26] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.", "[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.", "[2] Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.", "[34] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.", "[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.", "[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.", "[41] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.", "[18] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2):8, 2019."]}]}