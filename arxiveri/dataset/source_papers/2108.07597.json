{"title": "Light field image super-resolution with transformers", "abstract": "Light field (LF) image super-resolution (SR) aims at reconstructing high-resolution LF images from their low-resolution counterparts. Although CNN-based methods have achieved remarkable performance in LF image SR, these methods cannot fully model the non-local properties of the 4D LF data. In this paper, we propose a simple but effective Transformer-based method for LF image SR. In our method, an angular Transformer is designed to incorporate complementary information among different views, and a spatial Transformer is developed to capture both local and long-range dependencies within each sub-aperture image. With the proposed angular and spatial Transformers, the beneficial information in an LF can be fully exploited and the SR performance is boosted. We validate the effectiveness of our angular and spatial Transformers through extensive ablation studies, and compare our method to recent state-of-the-art methods on five public LF datasets. Our method achieves superior SR performance with a small model size and low computational cost. Code is available at https://github.com/ZhengyuLiang24/LFT.", "authors": ["Zhengyu Liang", " Yingqian Wang", " Longguang Wang", " Jungang Yang", " Shilin Zhou"], "pdf_url": "https://arxiv.org/abs/2108.07597", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"5\">2\\times</td><td colspan=\"5\">4\\times</td></tr><tr><td>EPFL</td><td>HCInew</td><td>HCIold</td><td>INRIA</td><td>STFgantry</td><td>EPFL</td><td>HCInew</td><td>HCIold</td><td>INRIA</td><td>STFgantry</td></tr><tr><td>Bicubic</td><td>29.74/0.941</td><td>31.89/0.939</td><td>37.69/0.979</td><td>31.33/0.959</td><td>31.06/0.954</td><td>25.14/0.833</td><td>27.61/0.853</td><td>32.42/0.931</td><td>26.82/0.886</td><td>25.93/0.847</td></tr><tr><td>VDSR [31]</td><td>32.50/0.960</td><td>34.37/0.956</td><td>40.61/0.987</td><td>34.43/0,974</td><td>35.54/0.979</td><td>27.25/0.878</td><td>29.31/0.883</td><td>34.81/0.952</td><td>29.19/0.921</td><td>28.51/0.901</td></tr><tr><td>EDSR [26]</td><td>33.09/0.963</td><td>34.83/0.959</td><td>41.01/0.988</td><td>34.97/0.977</td><td>36.29/0.982</td><td>27.84/0.886</td><td>29.60/0.887</td><td>35.18/0.954</td><td>29.66/0.926</td><td>28.70/0.908</td></tr><tr><td>RCAN [32]</td><td>33.16/0.964</td><td>34.98/0.960</td><td>41.05/0.988</td><td>35.01/0.977</td><td>36.33/0.983</td><td>27.88/0.886</td><td>29.63/0.888</td><td>35.20/0.954</td><td>29.76/0.927</td><td>28.90/0.911</td></tr><tr><td>resLF[11]</td><td>33.62/0.971</td><td>36.69/0.974</td><td>43.42/0.993</td><td>35.39/0.981</td><td>38.36/0.990</td><td>28.27/0.904</td><td>30.73/0.911</td><td>36.71/0.968</td><td>30.34/0.941</td><td>30.19/0.937</td></tr><tr><td>LFSSR [14]</td><td>33.68/0.974</td><td>36.81/0.975</td><td>43.81/0.994</td><td>35.28/0.983</td><td>37.95/0.990</td><td>28.27/0.908</td><td>30.72/0.912</td><td>36.70/0.969</td><td>30.31/0.945</td><td>30.15/0.939</td></tr><tr><td>LF-ATO [13]</td><td>34.27/0.976</td><td>37.24/0.977</td><td>44.20/0.994</td><td>36.15/0.984</td><td>39.64/0.993</td><td>28.52/0.912</td><td>30.88/0.914</td><td>37.00/0.970</td><td>30.71/0.949</td><td>30.61/0.943</td></tr><tr><td>LF-InterNet [15]</td><td>34.14/0.972</td><td>37.28/0.977</td><td>44.45/0.995</td><td>35.80/0.985</td><td>38.72/0.992</td><td>28.67/0.914</td><td>30.98/0.917</td><td>37.11/0.972</td><td>30.64/0.949</td><td>30.53/0.943</td></tr><tr><td>LF-DFnet [16]</td><td>34.44/0.977</td><td>37.44/0.979</td><td>44.23/0.994</td><td>36.36/0.984</td><td>39.61/0.993</td><td>28.77/0.917</td><td>31.23/0.920</td><td>37.32/0.972</td><td>30.83/0.950</td><td>31.15/0.949</td></tr><tr><td>MEG-Net [12]</td><td>34.30/0.977</td><td>37.42/0.978</td><td>44.08/0.994</td><td>36.09/0.985</td><td>38.77/0.991</td><td>28.74/0.916</td><td>31.10/0.918</td><td>37.28/0.972</td><td>30.66/0.949</td><td>30.77/0.945</td></tr><tr><td>LF-IINet [17]</td><td>34.68/0.977</td><td>37.74/0.979</td><td>44.84/0.995</td><td>36.57/0.985</td><td>39.86/0.994</td><td>29.11/0.920</td><td>31.36/0.921</td><td>37.62/0.974</td><td>31.08/0.952</td><td>31.21/0.950</td></tr><tr><td>DPT [33]</td><td>34.48/0.976</td><td>37.35/0.977</td><td>44.31/0.994</td><td>36.40/0.984</td><td>39.52/0.993</td><td>28.93/0.917</td><td>31.19/0.919</td><td>37.39/0.972</td><td>30.96/0.950</td><td>31.14/0.949</td></tr><tr><td>LFT(ours)</td><td>34.80/0.978</td><td>37.84/0.979</td><td>44.52/0.995</td><td>36.59/0.986</td><td>40.51/0.994</td><td>29.25/0.921</td><td>31.46/0.922</td><td>37.63/0.974</td><td>31.20/0.952</td><td>31.86/0.955</td></tr></table>", "caption": "TABLE I: PSNR/SSIM values achieved by different methods for 2\\times and 4\\timesSR. The best results are in bold faces.", "list_citation_info": ["[32] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, \u201cImage super-resolution using very deep residual channel attention networks,\u201d in European Conference on Computer Vision (ECCV), 2018, pp. 286\u2013301.", "[26] B. Lim, S. Son, H. Kim, S. Nah, and K. Lee, \u201cEnhanced deep residual networks for single image super-resolution,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2017, pp. 136\u2013144.", "[13] J. Jin, J. Hou, J. Chen, and S. Kwong, \u201cLight field spatial super-resolution via deep combinatorial geometry embedding and structural consistency regularization,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 2260\u20132269.", "[12] S. Zhang, S. Chang, and Y. Lin, \u201cEnd-to-end light field spatial super-resolution network using multiple epipolar geometry,\u201d IEEE Transactions on Image Processing, vol. 30, pp. 5956\u20135968, 2021.", "[14] H. Yeung, J. Hou, X. Chen, J. Chen, Z. Chen, and Y. Chung, \u201cLight field spatial super-resolution using deep efficient spatial-angular separable convolution,\u201d IEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2319\u20132330, 2018.", "[33] S. Wang, T. Zhou, Y. Lu, and H. Di, \u201cDetail preserving transformer for light field image super-resolution,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence,, 2022.", "[17] G. Liu, H. Yue, J. Wu, and J. Yang, \u201cIntra-inter view interaction network for light field image super-resolution,\u201d IEEE Transactions on Multimedia, pp. 1\u20131, 2021.", "[31] J. Kim, J. Lee, and K. Lee, \u201cAccurate image super-resolution using very deep convolutional networks,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 1646\u20131654.", "[11] S. Zhang, Y. Lin, and H. Sheng, \u201cResidual networks for light field image super-resolution,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 11\u2009046\u201311\u2009055.", "[16] Y. Wang, J. Yang, L. Wang, X. Ying, T. Wu, W. An, and Y. Guo, \u201cLight field image super-resolution using deformable convolution,\u201d IEEE Transactions on Image Processing, vol. 30, pp. 1057\u20131071, 2020.", "[15] Y. Wang, L. Wang, J. Yang, W. An, J. Yu, and Y. Guo, \u201cSpatial-angular interaction for light field image super-resolution,\u201d in European Conference on Computer Vision (ECCV). Springer, 2020, pp. 290\u2013308."]}, {"table": "<table><tr><td></td><td>AngTr</td><td>AngPos</td><td>SpaTr</td><td>SpaPos</td><td>#Param.</td><td>EPFL</td><td>HCIold</td><td>INRIA</td></tr><tr><td>1</td><td></td><td></td><td></td><td></td><td>1.49M</td><td>28.63</td><td>37.00</td><td>30.66</td></tr><tr><td>2</td><td>\u2713</td><td></td><td></td><td></td><td>1.42M</td><td>28.85</td><td>37.29</td><td>30.93</td></tr><tr><td>3</td><td>\u2713</td><td>\u2713</td><td></td><td></td><td>1.42M</td><td>28.98</td><td>37.38</td><td>30.93</td></tr><tr><td>4</td><td></td><td></td><td>\u2713</td><td></td><td>1.28M</td><td>28.93</td><td>37.30</td><td>30.97</td></tr><tr><td>5</td><td></td><td></td><td>\u2713</td><td>\u2713</td><td>1.28M</td><td>28.95</td><td>37.41</td><td>30.98</td></tr><tr><td>6</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>1.16M</td><td>29.25</td><td>37.46</td><td>31.20</td></tr></table>", "caption": "TABLE III: PSNR results achieved on the EPFL [34], HCIold [36] and INRIA [37] datasets by several variants of LFT for 4\\timesSR.Note that, AngTr and SpaTr represent models using angular Transformer and spatial Transformer, respectively. AngPos and SpaPos denote models using positional encoding in AngTr and SpaTr, respectively. #Param. represents the number of parameters of different variants.", "list_citation_info": ["[36] S. Wanner, S. Meister, and B. Goldluecke, \u201cDatasets and benchmarks for densely sampled 4d light fields.\u201d in Vision, Modelling and Visualization (VMV), vol. 13. Citeseer, 2013, pp. 225\u2013226.", "[37] M. Pendu, X. Jiang, and C. Guillemot, \u201cLight field inpainting propagation via low rank matrix completion,\u201d IEEE Transactions on Image Processing, vol. 27, no. 4, pp. 1981\u20131993, 2018.", "[34] M. Rerabek and T. Ebrahimi, \u201cNew light field image dataset,\u201d in International Conference on Quality of Multimedia Experience (QoMEX), 2016."]}]}