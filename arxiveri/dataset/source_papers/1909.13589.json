{"title": "Domain Adaptation for Semantic Segmentation with Maximum Squares Loss", "abstract": "Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss.", "authors": ["Minghao Chen", " Hongyang Xue", " Deng Cai"], "pdf_url": "https://arxiv.org/abs/1909.13589", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>A \\to W</th><th>D \\to W</th><th>W \\to D</th><th>A \\to D</th><th>D \\to A</th><th>W \\to A</th><th>Avg</th></tr></thead><tbody><tr><td>ResNet-50 [12]</td><td>68.4\\pm0.2</td><td>96.7\\pm0.1</td><td>99.3\\pm0.1</td><td>68.9\\pm0.2</td><td>62.5\\pm0.3</td><td>60.7\\pm0.3</td><td>76.1</td></tr><tr><td>DANN [10]</td><td>82.0\\pm0.4</td><td>96.9\\pm0.2</td><td>99.1\\pm0.1</td><td>79.7\\pm0.4</td><td>68.2\\pm0.4</td><td>67.4\\pm0.5</td><td>82.2</td></tr><tr><td>EntMin</td><td>89.0\\pm0.1</td><td>99.0\\pm0.1</td><td>100.0\\pm.0</td><td>86.3\\pm0.3</td><td>67.5\\pm0.2</td><td>63.0\\pm0.1</td><td>84.1</td></tr><tr><td>MaxSquare</td><td>92.4\\pm0.5</td><td>99.1\\pm0.1</td><td>100.0\\pm.0</td><td>90.0\\pm0.2</td><td>68.1\\pm0.4</td><td>64.2\\pm0.2</td><td>85.6</td></tr></tbody></table>", "caption": "Table 1: Comparison between the entropy minimization and maximum square loss on Office-31.", "list_citation_info": ["[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.", "[10] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17, 2016."]}, {"table": "<table><thead><tr><th colspan=\"22\">GTA5\\toCityscapes</th></tr><tr><th>Method</th><th>Backbone</th><th><p>road</p></th><th><p>sidewalk</p></th><th><p>building</p></th><th><p>wall</p></th><th><p>fence</p></th><th><p>pole</p></th><th><p>light</p></th><th><p>sign</p></th><th><p>veg.</p></th><th><p>terrain</p></th><th><p>sky</p></th><th><p>person</p></th><th><p>rider</p></th><th><p>car</p></th><th><p>truck</p></th><th><p>bus</p></th><th><p>train</p></th><th><p>motor</p></th><th><p>bike</p></th><th>mIoU (%)</th></tr></thead><tbody><tr><th>Source only [36]</th><th>Wider</th><td>70.0</td><td>23.7</td><td>67.8</td><td>15.4</td><td>18.1</td><td>40.2</td><td>41.9</td><td>25.3</td><td>78.8</td><td>11.7</td><td>31.4</td><td>62.9</td><td>29.8</td><td>60.1</td><td>21.5</td><td>26.8</td><td>7.7</td><td>28.1</td><td>12.0</td><td>35.4</td></tr><tr><th>CBST [36]</th><th>ResNet-38</th><td>86.8</td><td>46.7</td><td>76.9</td><td>26.3</td><td>24.8</td><td>42.0</td><td>46.0</td><td>38.6</td><td>80.7</td><td>15.7</td><td>48.0</td><td>57.3</td><td>27.9</td><td>78.2</td><td>24.5</td><td>49.6</td><td>17.7</td><td>25.5</td><td>45.1</td><td>45.2</td></tr><tr><th>CBST-SP [36]</th><th>[32]</th><td>88.0</td><td>56.2</td><td>77.0</td><td>27.4</td><td>22.4</td><td>40.7</td><td>47.3</td><td>40.9</td><td>82.4</td><td>21.6</td><td>60.3</td><td>50.2</td><td>20.4</td><td>83.8</td><td>35.0</td><td>51.0</td><td>15.2</td><td>20.6</td><td>37.0</td><td>46.2</td></tr><tr><th>AdaptSegNet [28]</th><th rowspan=\"3\">ResNet101</th><td>86.5</td><td>36.0</td><td>79.9</td><td>23.4</td><td>23.3</td><td>23.9</td><td>35.2</td><td>14.8</td><td>83.4</td><td>33.3</td><td>75.6</td><td>58.5</td><td>27.6</td><td>73.7</td><td>32.5</td><td>35.4</td><td>3.9</td><td>30.1</td><td>28.1</td><td>42.4</td></tr><tr><th>MinEnt [31]</th><td>86.2</td><td>18.6</td><td>80.3</td><td>27.2</td><td>24.0</td><td>23.4</td><td>33.5</td><td>24.7</td><td>83.3</td><td>31.0</td><td>75.6</td><td>54.6</td><td>25.6</td><td>85.2</td><td>30.0</td><td>10.9</td><td>0.1</td><td>21.9</td><td>37.1</td><td>42.3</td></tr><tr><th>AdvEnt+MinEnt [31]</th><td>87.6</td><td>21.4</td><td>82.0</td><td>34.8</td><td>26.2</td><td>28.5</td><td>35.6</td><td>23.0</td><td>84.5</td><td>35.1</td><td>76.2</td><td>58.6</td><td>30.7</td><td>84.8</td><td>34.2</td><td>43.4</td><td>0.4</td><td>28.4</td><td>35.3</td><td>44.8</td></tr><tr><th>Source only</th><th rowspan=\"5\">ResNet101</th><td>71.4</td><td>15.3</td><td>74.0</td><td>21.1</td><td>14.4</td><td>22.8</td><td>33.9</td><td>18.6</td><td>80.7</td><td>20.9</td><td>68.5</td><td>56.6</td><td>27.1</td><td>67.4</td><td>32.8</td><td>5.6</td><td>7.7</td><td>28.4</td><td>33.8</td><td>36.9</td></tr><tr><th>MinEnt{}^{\\dagger}</th><td>84.2</td><td>34.4</td><td>80.7</td><td>27.0</td><td>15.7</td><td>25.8</td><td>32.6</td><td>18.0</td><td>83.4</td><td>29.4</td><td>76.9</td><td>58.7</td><td>24.0</td><td>78.7</td><td>35.9</td><td>29.9</td><td>6.5</td><td>28.3</td><td>31.4</td><td>42.2</td></tr><tr><th>MaxSquare</th><td>88.1</td><td>27.7</td><td>80.8</td><td>28.7</td><td>19.8</td><td>24.9</td><td>34.0</td><td>17.8</td><td>83.6</td><td>34.7</td><td>76.0</td><td>58.6</td><td>28.6</td><td>84.1</td><td>37.8</td><td>43.1</td><td>7.2</td><td>32.2</td><td>34.2</td><td>44.3</td></tr><tr><th>MaxSquare+IW</th><td>89.3</td><td>40.5</td><td>81.2</td><td>29.0</td><td>20.4</td><td>25.6</td><td>34.4</td><td>19.0</td><td>83.6</td><td>34.4</td><td>76.5</td><td>59.2</td><td>27.4</td><td>83.8</td><td>38.4</td><td>43.6</td><td>7.1</td><td>32.2</td><td>32.5</td><td>45.2</td></tr><tr><th>MaxSquare+IW+Multi</th><td>89.4</td><td>43.0</td><td>82.1</td><td>30.5</td><td>21.3</td><td>30.3</td><td>34.7</td><td>24.0</td><td>85.3</td><td>39.4</td><td>78.2</td><td>63.0</td><td>22.9</td><td>84.6</td><td>36.4</td><td>43.0</td><td>5.5</td><td>34.7</td><td>33.5</td><td>46.4</td></tr></tbody></table>", "caption": "Table 2: Results for GTA5-to-Cityscapes experiments. \u201cMaxSquare\u201d denotes our maximum squares loss method and \u201cMaxSquare+IW\u201d is the maximum squares loss combined with our image-wise weighting factor (Eq. 13). \u201c Multi\u201d denotes combining the multi-level self-guided method in Section 3.4. For comparison, we reproduce the result of entropy minimization method [31], which is denoted as \u201cMinEnt{}^{\\dagger}\u201d. CBST [36] adopts a wider ResNet model [32], which is more powerful than the original ResNet [12] that we adopt.", "list_citation_info": ["[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.", "[28] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.", "[36] Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In ECCV, 2018.", "[32] Zifeng Wu, Chunhua Shen, and Anton van den Hengel. Wider or deeper: Revisiting the resnet model for visual recognition. CoRR, abs/1611.10080, 2016.", "[31] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00e9rez. ADVENT: adversarial entropy minimization for domain adaptation in semantic segmentation. CoRR, abs/1811.12833, 2018."]}, {"table": "<table><thead><tr><th colspan=\"20\">SYNTHIA\\toCityscapes</th></tr></thead><tbody><tr><th>Method</th><th>Backbone</th><td><p>road</p></td><td><p>sidewalk</p></td><td><p>building</p></td><td><p>wall*</p></td><td><p>fence*</p></td><td><p>pole*</p></td><td><p>light</p></td><td><p>sign</p></td><td><p>veg.</p></td><td><p>sky</p></td><td><p>person</p></td><td><p>rider</p></td><td><p>car</p></td><td><p>bus</p></td><td><p>motor</p></td><td><p>bike</p></td><td>mIoU (%)</td><td>mIoU* (%)</td></tr><tr><th>Source only [36]</th><th>Wider</th><td>32.6</td><td>21.5</td><td>46.5</td><td>4.8</td><td>0.1</td><td>26.5</td><td>14.8</td><td>13.1</td><td>70.8</td><td>60.3</td><td>56.6</td><td>3.5</td><td>74.1</td><td>20.4</td><td>8.9</td><td>13.1</td><td>29.2</td><td>33.6</td></tr><tr><th>CBST [36]</th><th>ResNet-38</th><td>53.6</td><td>23.7</td><td>75.0</td><td>12.5</td><td>0.3</td><td>36.4</td><td>23.5</td><td>26.3</td><td>84.8</td><td>74.7</td><td>67.2</td><td>17.5</td><td>84.5</td><td>28.4</td><td>15.2</td><td>55.8</td><td>42.5</td><td>48.4</td></tr><tr><th>AdaptSegNet [28]</th><th rowspan=\"3\">ResNet101</th><td>84.3</td><td>42.7</td><td>77.5</td><td>-</td><td>-</td><td>-</td><td>4.7</td><td>7.0</td><td>77.9</td><td>82.5</td><td>54.3</td><td>21.0</td><td>72.3</td><td>32.2</td><td>18.9</td><td>32.3</td><td>-</td><td>46.7</td></tr><tr><th>MinEnt [31]</th><td>73.5</td><td>29.2</td><td>77.1</td><td>7.7</td><td>0.2</td><td>27.0</td><td>7.1</td><td>11.4</td><td>76.7</td><td>82.1</td><td>57.2</td><td>21.3</td><td>69.4</td><td>29.2</td><td>12.9</td><td>27.9</td><td>38.1</td><td>44.2</td></tr><tr><th>AdvEnt+MinEnt [31]</th><td>85.6</td><td>42.2</td><td>79.7</td><td>8.7</td><td>0.4</td><td>25.9</td><td>5.4</td><td>8.1</td><td>80.4</td><td>84.1</td><td>57.9</td><td>23.8</td><td>73.3</td><td>36.4</td><td>14.2</td><td>33.0</td><td>41.2</td><td>48.0</td></tr><tr><th>Source only</th><th rowspan=\"5\">ResNet101</th><td>17.7</td><td>15.0</td><td>74.3</td><td>10.1</td><td>0.1</td><td>25.5</td><td>6.3</td><td>10.2</td><td>75.5</td><td>77.9</td><td>57.1</td><td>19.2</td><td>31.2</td><td>31.2</td><td>10.0</td><td>20.1</td><td>30.1</td><td>34.3</td></tr><tr><th>MinEnt{}^{\\dagger}</th><td>67.8</td><td>28.3</td><td>79.0</td><td>4.8</td><td>0.1</td><td>24.7</td><td>4.0</td><td>7.3</td><td>81.7</td><td>84.1</td><td>58.9</td><td>19.4</td><td>75.9</td><td>36.2</td><td>10.4</td><td>26.1</td><td>38.0</td><td>44.5</td></tr><tr><th>MaxSquare</th><td>77.4</td><td>34.0</td><td>78.7</td><td>5.6</td><td>0.2</td><td>27.7</td><td>5.8</td><td>9.8</td><td>80.7</td><td>83.2</td><td>58.5</td><td>20.5</td><td>74.1</td><td>32.1</td><td>11.0</td><td>29.9</td><td>39.3</td><td>45.8</td></tr><tr><th>MaxSquare+IW</th><td>78.5</td><td>34.7</td><td>76.3</td><td>6.5</td><td>0.1</td><td>30.4</td><td>12.4</td><td>12.2</td><td>82.2</td><td>84.3</td><td>59.9</td><td>17.9</td><td>80.6</td><td>24.1</td><td>15.2</td><td>31.2</td><td>40.4</td><td>46.9</td></tr><tr><th>MaxSquare+IW+Multi</th><td>82.9</td><td>40.7</td><td>80.3</td><td>10.2</td><td>0.8</td><td>25.8</td><td>12.8</td><td>18.2</td><td>82.5</td><td>82.2</td><td>53.1</td><td>18.0</td><td>79.0</td><td>31.4</td><td>10.4</td><td>35.6</td><td>41.4</td><td>48.2</td></tr></tbody></table>", "caption": "Table 3: Results for SYNTHIA-to-Cityscapes experiments.", "list_citation_info": ["[31] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00e9rez. ADVENT: adversarial entropy minimization for domain adaptation in semantic segmentation. CoRR, abs/1811.12833, 2018.", "[28] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.", "[36] Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In ECCV, 2018."]}, {"table": "<table><thead><tr><th colspan=\"16\">Cross-City Adaptation</th></tr></thead><tbody><tr><th>City</th><th>Method</th><td><p>road</p></td><td><p>sidewalk</p></td><td><p>building</p></td><td><p>light</p></td><td><p>sign</p></td><td><p>veg.</p></td><td><p>sky</p></td><td><p>person</p></td><td><p>rider</p></td><td><p>car</p></td><td><p>bus</p></td><td><p>motor</p></td><td><p>bike</p></td><td>mIoU (%)</td></tr><tr><th rowspan=\"6\">Rome</th><th>Cross city [5]</th><td>79.5</td><td>29.3</td><td>84.5</td><td>0.0</td><td>22.2</td><td>80.6</td><td>82.8</td><td>29.5</td><td>13.0</td><td>71.7</td><td>37.5</td><td>25.9</td><td>1.0</td><td>42.9</td></tr><tr><th>CBST [36]</th><td>87.1</td><td>43.9</td><td>89.7</td><td>14.8</td><td>47.7</td><td>85.4</td><td>90.3</td><td>45.4</td><td>26.6</td><td>85.4</td><td>20.5</td><td>49.8</td><td>10.3</td><td>53.6</td></tr><tr><th>AdaptSegNet [28]</th><td>83.9</td><td>34.2</td><td>88.3</td><td>18.8</td><td>40.2</td><td>86.2</td><td>93.1</td><td>47.8</td><td>21.7</td><td>80.9</td><td>47.8</td><td>48.3</td><td>8.6</td><td>53.8</td></tr><tr><th>Source only</th><td>85.0</td><td>34.7</td><td>86.4</td><td>17.5</td><td>39.0</td><td>84.9</td><td>85.4</td><td>43.8</td><td>15.5</td><td>81.8</td><td>46.3</td><td>38.4</td><td>4.8</td><td>51.0</td></tr><tr><th>MaxSquare</th><td>80.0</td><td>27.6</td><td>87.0</td><td>20.8</td><td>42.5</td><td>85.1</td><td>92.4</td><td>46.7</td><td>22.9</td><td>82.1</td><td>53.5</td><td>50.8</td><td>8.8</td><td>53.9</td></tr><tr><th>MaxSquare+IW</th><td>82.9</td><td>32.6</td><td>86.7</td><td>20.7</td><td>41.6</td><td>85.0</td><td>93.0</td><td>47.2</td><td>22.5</td><td>82.2</td><td>53.8</td><td>50.5</td><td>9.9</td><td>54.5</td></tr><tr><th rowspan=\"6\">Rio</th><th>Cross city [5]</th><td>74.2</td><td>43.9</td><td>79.0</td><td>2.4</td><td>7.5</td><td>77.8</td><td>69.5</td><td>39.3</td><td>10.3</td><td>67.9</td><td>41.2</td><td>27.9</td><td>10.9</td><td>42.5</td></tr><tr><th>CBST [36]</th><td>84.3</td><td>55.2</td><td>85.4</td><td>19.6</td><td>30.1</td><td>80.5</td><td>77.9</td><td>55.2</td><td>28.6</td><td>79.7</td><td>33.2</td><td>37.6</td><td>11.5</td><td>52.2</td></tr><tr><th>AdaptSegNet [28]</th><td>76.2</td><td>44.7</td><td>84.6</td><td>9.3</td><td>25.5</td><td>81.8</td><td>87.3</td><td>55.3</td><td>32.7</td><td>74.3</td><td>28.9</td><td>43.0</td><td>27.6</td><td>51.6</td></tr><tr><th>Source only</th><td>74.2</td><td>42.2</td><td>84.0</td><td>12.1</td><td>20.4</td><td>78.3</td><td>87.9</td><td>50.1</td><td>25.6</td><td>76.6</td><td>40.0</td><td>27.6</td><td>17.0</td><td>48.9</td></tr><tr><th>MaxSquare</th><td>70.9</td><td>39.2</td><td>85.6</td><td>14.5</td><td>19.7</td><td>81.8</td><td>88.1</td><td>55.2</td><td>31.5</td><td>77.2</td><td>39.3</td><td>43.1</td><td>30.1</td><td>52.0</td></tr><tr><th>MaxSquare+IW</th><td>76.9</td><td>48.8</td><td>85.2</td><td>13.8</td><td>18.9</td><td>81.7</td><td>88.1</td><td>54.9</td><td>34.0</td><td>76.8</td><td>39.8</td><td>44.1</td><td>29.7</td><td>53.3</td></tr><tr><th rowspan=\"6\">Tokyo</th><th>Cross city [5]</th><td>83.4</td><td>35.4</td><td>72.8</td><td>12.3</td><td>12.7</td><td>77.4</td><td>64.3</td><td>42.7</td><td>21.5</td><td>64.1</td><td>20.8</td><td>8.9</td><td>40.3</td><td>42.8</td></tr><tr><th>CBST [36]</th><td>85.2</td><td>33.6</td><td>80.4</td><td>8.3</td><td>31.1</td><td>83.9</td><td>78.2</td><td>53.2</td><td>28.9</td><td>72.7</td><td>4.4</td><td>27.0</td><td>47.0</td><td>48.8</td></tr><tr><th>AdaptSegNet [28]</th><td>81.5</td><td>26.0</td><td>77.8</td><td>17.8</td><td>26.8</td><td>82.7</td><td>90.9</td><td>55.8</td><td>38.0</td><td>72.1</td><td>4.2</td><td>24.5</td><td>50.8</td><td>49.9</td></tr><tr><th>Source only</th><td>81.4</td><td>28.4</td><td>78.1</td><td>14.5</td><td>19.6</td><td>81.4</td><td>86.5</td><td>51.9</td><td>22.0</td><td>70.4</td><td>18.2</td><td>22.3</td><td>46.4</td><td>47.8</td></tr><tr><th>MaxSquare</th><td>79.3</td><td>28.5</td><td>78.3</td><td>14.5</td><td>27.9</td><td>82.8</td><td>89.6</td><td>57.3</td><td>31.9</td><td>71.9</td><td>6.0</td><td>29.1</td><td>49.2</td><td>49.7</td></tr><tr><th>MaxSquare+IW</th><td>81.2</td><td>30.1</td><td>77.0</td><td>12.3</td><td>27.3</td><td>82.8</td><td>89.5</td><td>58.2</td><td>32.7</td><td>71.5</td><td>5.5</td><td>37.4</td><td>48.9</td><td>50.5</td></tr><tr><th rowspan=\"6\">Taipei</th><th>Cross city [5]</th><td>78.6</td><td>28.6</td><td>80.0</td><td>13.1</td><td>7.6</td><td>68.2</td><td>82.1</td><td>16.8</td><td>9.4</td><td>60.4</td><td>34.0</td><td>26.5</td><td>9.9</td><td>39.6</td></tr><tr><th>CBST [36]</th><td>86.1</td><td>35.2</td><td>84.2</td><td>15.0</td><td>22.2</td><td>75.6</td><td>74.9</td><td>22.7</td><td>33.1</td><td>78.0</td><td>37.6</td><td>58.0</td><td>30.9</td><td>50.3</td></tr><tr><th>AdaptSegNet [28]</th><td>81.7</td><td>29.5</td><td>85.2</td><td>26.4</td><td>15.6</td><td>76.7</td><td>91.7</td><td>31.0</td><td>12.5</td><td>71.5</td><td>41.1</td><td>47.3</td><td>27.7</td><td>49.1</td></tr><tr><th>Source only</th><td>82.6</td><td>33.0</td><td>86.3</td><td>16.0</td><td>16.5</td><td>78.3</td><td>83.3</td><td>26.5</td><td>8.4</td><td>70.7</td><td>36.1</td><td>47.9</td><td>15.7</td><td>46.3</td></tr><tr><th>MaxSquare</th><td>81.2</td><td>32.8</td><td>85.4</td><td>31.9</td><td>14.7</td><td>78.3</td><td>92.7</td><td>28.3</td><td>8.6</td><td>68.2</td><td>42.2</td><td>51.3</td><td>32.4</td><td>49.8</td></tr><tr><th>MaxSquare+IW</th><td>80.7</td><td>32.5</td><td>85.5</td><td>32.7</td><td>15.1</td><td>78.1</td><td>91.3</td><td>32.9</td><td>7.6</td><td>69.5</td><td>44.8</td><td>52.4</td><td>34.9</td><td>50.6</td></tr></tbody></table>", "caption": "Table 6: Results for Cross-City experiments.", "list_citation_info": ["[5] Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai, Yu-Chiang Frank Wang, and Min Sun. No more discrimination: Cross city adaptation of road scene segmenters. In ICCV, 2017.", "[28] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.", "[36] Yang Zou, Zhiding Yu, B. V. K. Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In ECCV, 2018."]}]}