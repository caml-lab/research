{"title": "Asmnet: A lightweight deep neural network for face alignment and pose estimation", "abstract": "Active Shape Model (ASM) is a statistical model of object shapes that represents a target structure. ASM can guide machine learning algorithms to fit a set of points representing an object (e.g., face) onto an image. This paper presents a lightweight Convolutional Neural Network (CNN) architecture with a loss function being assisted by ASM for face alignment and estimating head pose in the wild. We use ASM to first guide the network towards learning a smoother distribution of the facial landmark points. Inspired by transfer learning, during the training process, we gradually harden the regression problem and guide the network towards learning the original landmark points distribution. We define multi-tasks in our loss function that are responsible for detecting facial landmark points as well as estimating the face pose. Learning multiple correlated tasks simultaneously builds synergy and improves the performance of individual tasks. We compare the performance of our proposed model called ASMNet with MobileNetV2 (which is about 2 times bigger than ASMNet) in both the face alignment and pose estimation tasks. Experimental results on challenging datasets show that by using the proposed ASM assisted loss function, the ASMNet performance is comparable with MobileNetV2 in the face alignment task. In addition, for face pose estimation, ASMNet performs much better than MobileNetV2. ASMNet achieves an acceptable performance for facial landmark points detection and pose estimation while having a significantly smaller number of parameters and floating-point operations compared to many CNN-based models.", "authors": ["Ali Pourramezan Fard", " Hojjat Abdollahi", " Mohammad Mahoor"], "pdf_url": "https://arxiv.org/abs/2103.00119", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">NME</th><th rowspan=\"2\">Params (M)</th><th rowspan=\"2\">FLOPs (B)</th></tr><tr><th>300W</th><th>WFLW</th></tr></thead><tbody><tr><th>mnv2</th><td>4.70</td><td>9.57</td><td rowspan=\"2\">2.42</td><td rowspan=\"2\">0.60</td></tr><tr><th>mnv2_r</th><td>4.59</td><td>9.41</td></tr><tr><th>ASMNet_nr</th><td>6.49</td><td>11.96</td><td rowspan=\"2\">1.43</td><td rowspan=\"2\">0.51</td></tr><tr><th>ASMNet</th><td>5.50</td><td>10.77</td></tr></tbody></table>", "caption": "Table 1: Number of parameters in million (M) and FLOPs in billion (B), as well as Normalized Mean Error (NME in %) of landmarks localization on 300W [31], and WFLW [44] datasets.", "list_citation_info": ["[31] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 397\u2013403, 2013.", "[44] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou. Look at boundary: A boundary-aware face alignment algorithm. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2129\u20132138, 2018."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">Normalized Mean Error</td></tr><tr><td>Common</td><td>Challenging</td><td>Fullset</td></tr><tr><th>RCN [16]</th><td>4.67</td><td>8.44</td><td>5.41</td></tr><tr><th>DAN [21]</th><td>3.19</td><td>5.24</td><td>3.59</td></tr><tr><th>PCD-CNN [22]</th><td>3.67</td><td>7.62</td><td>4.44</td></tr><tr><th>CPM [13]</th><td>3.39</td><td>8.14</td><td>4.36</td></tr><tr><th>DSRN [26]</th><td>4.12</td><td>9.68</td><td>5.21</td></tr><tr><th>SAN [12]</th><td>3.34</td><td>6.60</td><td>3.98</td></tr><tr><th>LAB [44]</th><td>2.98</td><td>5.19</td><td>3.49</td></tr><tr><th>DCFE [40]</th><td>2.76</td><td>5.22</td><td>3.24</td></tr><tr><th>mnv2</th><td>3.93</td><td>7.52</td><td>4.70</td></tr><tr><th>mnv2_r</th><td>3.88</td><td>7.35</td><td>4.59</td></tr><tr><th>ASMNet_nr</th><td>5.86</td><td>8.80</td><td>6.46</td></tr><tr><th>ASMNet</th><td>4.82</td><td>8.2</td><td>5.50</td></tr></tbody></table>", "caption": "Table 2: Normalized Mean Error (in %) of 68-point landmarks localization on 300W [31] dataset.", "list_citation_info": ["[40] R. Valle, J. M. Buenaposada, A. Vald\u00e9s, and L. Baumela. A deeply-initialized coarse-to-fine ensemble of regression trees for face alignment. In Proceedings of the European Conference on Computer Vision (ECCV), pages 585\u2013601, 2018.", "[31] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 397\u2013403, 2013.", "[16] S. Honari, J. Yosinski, P. Vincent, and C. Pal. Recombinator networks: Learning coarse-to-fine feature aggregation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5743\u20135752, 2016.", "[26] X. Miao, X. Zhen, X. Liu, C. Deng, V. Athitsos, and H. Huang. Direct shape regression networks for end-to-end face alignment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5040\u20135049, 2018.", "[44] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou. Look at boundary: A boundary-aware face alignment algorithm. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2129\u20132138, 2018.", "[21] M. Kowalski, J. Naruniec, and T. Trzcinski. Deep alignment network: A convolutional neural network for robust face alignment. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 88\u201397, 2017.", "[12] X. Dong, Y. Yan, W. Ouyang, and Y. Yang. Style aggregated network for facial landmark detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 379\u2013388, 2018.", "[13] X. Dong, S.-I. Yu, X. Weng, S.-E. Wei, Y. Yang, and Y. Sheikh. Supervision-by-registration: An unsupervised approach to improve the precision of facial landmark detectors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 360\u2013368, 2018.", "[22] A. Kumar and R. Chellappa. Disentangling 3d pose in a dendritic cnn for unconstrained 2d face alignment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 430\u2013439, 2018."]}, {"table": "<table><thead><tr><th colspan=\"2\">Method</th><th>ASMNet_nr</th><th>ASMNet</th><th>mnv2</th><th>mnv2_r</th></tr></thead><tbody><tr><th rowspan=\"3\">300W [31]</th><td>yaw</td><td>2.41</td><td>1.62</td><td>1.75</td><td>1.71</td></tr><tr><td>pitch</td><td>1.87</td><td>1.80</td><td>1.93</td><td>1.89</td></tr><tr><td>roll</td><td>2.115</td><td>1.24</td><td>1.32</td><td>1.30</td></tr><tr><th rowspan=\"3\">WFLW [44]</th><td>yaw</td><td>3.14</td><td>2.97</td><td>3.06</td><td>3.08</td></tr><tr><td>pitch</td><td>2.99</td><td>2.93</td><td>3.03</td><td>2.94</td></tr><tr><td>roll</td><td>2.23</td><td>2.21</td><td>2.26</td><td>2.22</td></tr></tbody></table>", "caption": "Table 4: Mean Absolute Error of pose estimation on 300W [31], WFLW [44] datasets compared to HopeNet[30].", "list_citation_info": ["[31] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 397\u2013403, 2013.", "[30] N. Ruiz, E. Chong, and J. M. Rehg. Fine-grained head pose estimation without keypoints. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2018.", "[44] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou. Look at boundary: A boundary-aware face alignment algorithm. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2129\u20132138, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Pitch</th><th>Yaw</th><th>Roll</th></tr></thead><tbody><tr><th>Yanget. al [50]</th><td>5.1</td><td>4.2</td><td>2.4</td></tr><tr><th>JFA [48]</th><td>3.0</td><td>2.5</td><td>2.6</td></tr><tr><th>ASMNet</th><td>1.80</td><td>1.62</td><td>1.24</td></tr></tbody></table>", "caption": "Table 5: Mean Absolute Error of pose estimation on using ASMNet, JFA [48], and Yanget. al [50] on 300W [31].", "list_citation_info": ["[48] X. Xu and I. A. Kakadiaris. Joint head pose estimation and face alignment framework using global and local cnn features. In 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017), pages 642\u2013649. IEEE, 2017.", "[50] H. Yang, W. Mou, Y. Zhang, I. Patras, H. Gunes, and P. Robinson. Face alignment assisted by head pose estimation. arXiv preprint arXiv:1507.03148, 2015.", "[31] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 397\u2013403, 2013."]}, {"table": "<table><thead><tr><th>Method</th><th>Backbone</th><th>#Params (M)</th><th>FLOPs (B)</th></tr></thead><tbody><tr><th>DVLN [45]</th><td>VGG-16</td><td>132.0</td><td>14.4</td></tr><tr><th>SAN [12]</th><td>ResNet-152</td><td>57.4</td><td>10.7</td></tr><tr><th>LAB [44]</th><td>Hourglass</td><td>25.1</td><td>19.1</td></tr><tr><th>ResNet50 (Wing + PDB) [15]</th><td>ResNet-50</td><td>25</td><td>3.8</td></tr><tr><th>ASMNet</th><td>MobileNetV2 [33]</td><td>1.4</td><td>0.5</td></tr><tr><th>MobileNetV2 [33]</th><td>-</td><td>2.4</td><td>0.6</td></tr></tbody></table>", "caption": "Table 6:  Model size (the number of model parameters) and computational cost (FLOPs) analysis of different networks.", "list_citation_info": ["[44] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou. Look at boundary: A boundary-aware face alignment algorithm. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2129\u20132138, 2018.", "[15] Z.-H. Feng, J. Kittler, M. Awais, P. Huber, and X.-J. Wu. Wing loss for robust facial landmark localisation with convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2235\u20132245, 2018.", "[33] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4510\u20134520, 2018.", "[12] X. Dong, Y. Yan, W. Ouyang, and Y. Yang. Style aggregated network for facial landmark detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 379\u2013388, 2018.", "[45] W. Wu and S. Yang. Leveraging intra and inter-dataset variations for robust face alignment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 150\u2013159, 2017."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">NME reduction (in %)</td></tr><tr><td>ASMNet</td><td>mnv2</td></tr><tr><th rowspan=\"3\">300W [31]</th><td>Full</td><td>0.96</td><td>0.11</td></tr><tr><td>Common</td><td>1.58</td><td>0.05</td></tr><tr><td>Challenging</td><td>0.60</td><td>0.17</td></tr><tr><th rowspan=\"7\">WFLW [44]</th><td>Full</td><td>1.19</td><td>0.16</td></tr><tr><td>Large pose</td><td>0.84</td><td>0.32</td></tr><tr><td>Expression</td><td>1.06</td><td>0.15</td></tr><tr><td>Illumination</td><td>1.09</td><td>0.08</td></tr><tr><td>Makeup</td><td>1.29</td><td>0.25</td></tr><tr><td>Occlusion</td><td>0.13</td><td>0.90</td></tr><tr><td>Blur</td><td>0.98</td><td>0.13</td></tr></tbody></table>", "caption": "Table 7: Investigating the effect of using ASM assisted loss function both on MobileNetV2 [33] and ASMNet.", "list_citation_info": ["[31] C. Sagonas, G. Tzimiropoulos, S. Zafeiriou, and M. Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 397\u2013403, 2013.", "[44] W. Wu, C. Qian, S. Yang, Q. Wang, Y. Cai, and Q. Zhou. Look at boundary: A boundary-aware face alignment algorithm. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2129\u20132138, 2018.", "[33] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4510\u20134520, 2018."]}]}