{"title": "Der: Dynamically expandable representation for class incremental learning", "abstract": "We address the problem of class incremental learning, which is a core step towards achieving adaptive vision intelligence. In particular, we consider the task setting of incremental learning with limited memory and aim to achieve better stability-plasticity trade-off. To this end, we propose a novel two-stage learning approach that utilizes a dynamically expandable representation for more effective incremental concept modeling. Specifically, at each incremental step, we freeze the previously learned representation and augment it with additional feature dimensions from a new learnable feature extractor. This enables us to integrate new visual concepts with retaining learned knowledge. We dynamically expand the representation according to the complexity of novel concepts by introducing a channel-level mask-based pruning strategy. Moreover, we introduce an auxiliary loss to encourage the model to learn diverse and discriminate features for novel concepts. We conduct extensive experiments on the three class incremental learning benchmarks and our method consistently outperforms other methods with a large margin.", "authors": ["Shipeng Yan", " Jiangwei Xie", " Xuming He"], "pdf_url": "https://arxiv.org/abs/2103.16788", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td colspan=\"2\">5 steps</td><td colspan=\"2\">10 steps</td><td colspan=\"2\">20 steps</td><td colspan=\"2\">50 steps</td></tr><tr><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td></tr><tr><th>Bound</th><td>11.2</td><td>80.40</td><td>11.2</td><td>80.41</td><td>11.2</td><td>81.49</td><td>11.2</td><td>81.74</td></tr><tr><th>iCaRL[27]</th><td>11.2</td><td>71.14_{\\footnotesize{\\pm 0.34}}</td><td>11.2</td><td>65.27_{\\footnotesize{\\pm 1.02}}</td><td>11.2</td><td>61.20_{\\footnotesize{\\pm 0.83}}</td><td>11.2</td><td>56.08_{\\footnotesize{\\pm 0.83}}</td></tr><tr><th>UCIR[12]</th><td>11.2</td><td>62.77_{\\footnotesize{\\pm 0.82}}</td><td>11.2</td><td>58.66_{\\footnotesize{\\pm 0.71}}</td><td>11.2</td><td>58.17_{\\footnotesize{\\pm 0.30}}</td><td>11.2</td><td>56.86_{\\footnotesize{\\pm 3.74}}</td></tr><tr><th>BiC[12]</th><td>11.2</td><td>73.10_{\\footnotesize{\\pm 0.55}}</td><td>11.2</td><td>68.80_{\\footnotesize{\\pm 1.20}}</td><td>11.2</td><td>66.48_{\\footnotesize{\\pm 0.32}}</td><td>11.2</td><td>62.09_{\\footnotesize{\\pm 0.85}}</td></tr><tr><th>WA[39]</th><td>11.2</td><td>72.81_{\\footnotesize{\\pm 0.28}}</td><td>11.2</td><td>69.46_{\\footnotesize{\\pm 0.29}}</td><td>11.2</td><td>67.33_{\\footnotesize{\\pm 0.15}}</td><td>11.2</td><td>64.32_{\\footnotesize{\\pm 0.28}}</td></tr><tr><th>PODNet[6]</th><td>11.2</td><td>66.70_{\\footnotesize{\\pm 0.64}}</td><td>11.2</td><td>58.03_{\\footnotesize{\\pm 1.27}}</td><td>11.2</td><td>53.97_{\\footnotesize{\\pm 0.85}}</td><td>11.2</td><td>51.19_{\\footnotesize{\\pm 1.02}}</td></tr><tr><th>RPSNet[26]</th><td>60.6</td><td>70.5</td><td>56.5</td><td>68.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours(w/o P)</th><td>33.6</td><td>\\textbf{76.80}_{\\footnotesize{\\pm 0.79}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+3.7)}}</td><td>61.6</td><td>\\textbf{75.36}_{\\footnotesize{\\pm 0.36}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+5.9)}}</td><td>117.6</td><td>\\textbf{74.09}_{\\footnotesize{\\pm 0.33}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+6.76)}}</td><td>285.6</td><td>\\textbf{72.41}_{\\footnotesize{\\pm 0.36}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+8.09)}}</td></tr><tr><th>Ours</th><td>2.89</td><td>\\textbf{75.55}_{\\footnotesize{\\pm 0.65}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+2.45)}}</td><td>4.96</td><td>\\textbf{74.64}_{\\footnotesize{\\pm 0.28}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+5.18)}}</td><td>7.21</td><td>\\textbf{73.98}_{\\footnotesize{\\pm 0.36}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+6.65)}}</td><td>10.15</td><td>\\textbf{72.05}_{\\footnotesize{\\pm 0.55}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+7.73)}}</td></tr></tbody></table>", "caption": "Table 1: Results on CIFAR100-B0 benchmark which is averaged over three runs. \\#Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (\\%) over steps. Ours(w/o P) means our method without pruning.", "list_citation_info": ["[26] Jathushan Rajasegaran, Munawar Hayat, Salman H Khan, Fahad Shahbaz Khan, and Ling Shao. Random path selection for continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2019.", "[6] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[39] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2020.", "[27] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td colspan=\"2\">2Steps</td><td colspan=\"2\">5Steps</td><td colspan=\"2\">10Steps</td></tr><tr><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td></tr><tr><th>Bound</th><td>11.2</td><td>77.22</td><td>11.2</td><td>79.89</td><td>11.2</td><td>79.91</td></tr><tr><th>iCaRL[27]</th><td>11.2</td><td>71.33_{\\footnotesize{\\pm 0.35}}</td><td>11.2</td><td>65.06_{\\footnotesize{\\pm 0.53}}</td><td>11.2</td><td>58.59_{\\footnotesize{\\pm 0.95}}</td></tr><tr><th>UCIR[12]</th><td>11.2</td><td>67.21_{\\footnotesize{\\pm 0.35}}</td><td>11.2</td><td>64.28_{\\footnotesize{\\pm 0.19}}</td><td>11.2</td><td>59.92_{\\footnotesize{\\pm 2.4}}</td></tr><tr><th>BiC[12]</th><td>11.2</td><td>72.47_{\\footnotesize{\\pm 0.99}}</td><td>11.2</td><td>66.62_{\\footnotesize{\\pm 0.45}}</td><td>11.2</td><td>60.25_{\\footnotesize{\\pm 0.34}}</td></tr><tr><th>WA[39]</th><td>11.2</td><td>71.43_{\\footnotesize{\\pm 0.65}}</td><td>11.2</td><td>64.01_{\\footnotesize{\\pm 1.62}}</td><td>11.2</td><td>57.86_{\\footnotesize{\\pm 0.81}}</td></tr><tr><th>PODNet[6]</th><td>11.2</td><td>71.30_{\\footnotesize{\\pm 0.46}}</td><td>11.2</td><td>67.25_{\\footnotesize{\\pm 0.27}}</td><td>11.2</td><td>64.04_{\\footnotesize{\\pm 0.43}}</td></tr><tr><th>Ours(w/o P)</th><td>22.4</td><td>\\textbf{74.61}_{\\footnotesize{\\pm 0.52}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+2.14)}}</td><td>39.2</td><td>\\textbf{73.21}_{\\footnotesize{\\pm 0.78}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+5.96)}}</td><td>67.2</td><td>\\textbf{72.81}_{\\footnotesize{\\pm 0.88}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+8.77)}}</td></tr><tr><th>Ours</th><td>3.90</td><td>\\textbf{74.57}_{\\footnotesize{\\pm 0.42}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+2.10)}}</td><td>6.13</td><td>\\textbf{72.60}_{\\footnotesize{\\pm 0.78}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+5.35)}}</td><td>8.79</td><td>\\textbf{72.45}_{\\footnotesize{\\pm 0.76}}\\textbf{\\color[rgb]{0.224,0.710,0.290}{(+8.41)}}</td></tr></tbody></table>", "caption": "Table 2: Results on CIFAR100-B50 (average over 3 runs). \\#Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (\\%) over steps. Ours(w/o P) means our method without pruning.", "list_citation_info": ["[39] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2020.", "[27] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017.", "[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2019.", "[6] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"3\">Methods</th><td colspan=\"5\">ImageNet100-B0</td><td colspan=\"5\">ImageNet1000-B0</td></tr><tr><td rowspan=\"2\">\\#Paras</td><td colspan=\"2\">top-1</td><td colspan=\"2\">top-5</td><td rowspan=\"2\">\\#Paras</td><td colspan=\"2\">top-1</td><td colspan=\"2\">top-5</td></tr><tr><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td></tr><tr><th>Bound</th><td>11.2</td><td>-</td><td>-</td><td>-</td><td>95.1</td><td>11.2</td><td>89.27</td><td>-</td><td>-</td><td>-</td></tr><tr><th>iCaRL[27]</th><td>11.2</td><td>-</td><td>-</td><td>83.6</td><td>63.8</td><td>11.2</td><td>38.4</td><td>22.7</td><td>63.7</td><td>44.0</td></tr><tr><th>BiC[12]</th><td>11.2</td><td>-</td><td>-</td><td>90.6</td><td>84.4</td><td>11.2</td><td>-</td><td>-</td><td>84.0</td><td>73.2</td></tr><tr><th>WA[39]</th><td>11.2</td><td>-</td><td>-</td><td>91.0</td><td>84.1</td><td>11.2</td><td>65.67</td><td>55.6</td><td>86.6</td><td>81.1</td></tr><tr><th>RPSNet[26]</th><td>-</td><td>-</td><td>-</td><td>87.9</td><td>74.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours(w/o P)</th><td>61.6</td><td>77.18</td><td>66.70</td><td>93.23</td><td>87.52</td><td>61.6</td><td>68.84</td><td>60.16</td><td>88.17</td><td>82.86</td></tr><tr><th>Ours</th><td>7.67</td><td>76.12</td><td>66.06</td><td>92.79</td><td>88.38</td><td>14.52</td><td>66.73</td><td>58.62</td><td>87.08</td><td>81.89</td></tr></tbody></table>MethodsImageNet100-B50\\#Parastop-1top-5AvgLastAvgLastBound11.281.2081.5--UCIR[12]11.268.0957.3--PODNet[6]11.274.33---TPCIL[34]11.274.8166.91--Ours(w/o P)67.2078.2074.9294.2091.30Ours8.8777.7372.0694.0191.64", "caption": "Table 3: Results on ImageNet-100 and ImageNet-1000 datasets.Left: The results on ImageNet100-B0 and ImageNet1000-B0 benchmark. Right: The results on ImageNet100-B50 benchmark. \\#Paras means the average number of parameters during inference over steps, which is counted by million. Avg means the average accuracy (\\%) over steps. Last is the accuracy (\\%) of the last step. Ours(w/o P) means our method without pruning.", "list_citation_info": ["[26] Jathushan Rajasegaran, Munawar Hayat, Salman H Khan, Fahad Shahbaz Khan, and Ling Shao. Random path selection for continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2019.", "[6] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[34] Tao Xiaoyu, Chang Xinyuan, Hong Xiaopeng, Wei Xing, and Gong Yihong. Topology-preserving class-incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[39] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2020.", "[27] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td colspan=\"2\">5 steps</td><td colspan=\"2\">10 steps</td><td colspan=\"2\">20 steps</td><td colspan=\"2\">50 steps</td></tr><tr><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td></tr><tr><th>iCaRL [27]</th><td>0.46</td><td>67.20</td><td>0.46</td><td>64.04</td><td>0.46</td><td>61.16</td><td>0.46</td><td>57.00</td></tr><tr><th>BiC [12]</th><td>0.46</td><td>68.92</td><td>0.46</td><td>66.15</td><td>0.46</td><td>63.80</td><td>0.46</td><td>-</td></tr><tr><th>WA [39]</th><td>0.46</td><td>70.00</td><td>0.46</td><td>67.25</td><td>0.46</td><td>64.33</td><td>0.46</td><td>-</td></tr><tr><th>Ours(w/o P)</th><td>1.38</td><td>73.00(+3.00)</td><td>2.53</td><td>71.29(+4.04)</td><td>4.83</td><td>71.07(+6.74)</td><td>11.73</td><td>70.58(+13.58)</td></tr><tr><th>Ours</th><td>0.42</td><td>72.31(+2.31)</td><td>0.52</td><td>69.41(+2.16)</td><td>0.45</td><td>68.82(+4.49)</td><td>0.70</td><td>67.29(+10.29)</td></tr></tbody></table>", "caption": "Table 7: Results on CIFAR100-B0 benchmark using modified 32-layer ResNet. \\#Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (\\%) over steps. Ours(w/o P) means our method without pruning.", "list_citation_info": ["[39] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2020.", "[27] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2017.", "[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2019."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td colspan=\"2\">2Steps</td><td colspan=\"2\">5Steps</td><td colspan=\"2\">10Steps</td></tr><tr><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td><td>\\#Paras</td><td>Avg</td></tr><tr><th>UCIR [12]</th><td>0.46</td><td>66.76</td><td>0.46</td><td>63.42</td><td>0.46</td><td>60.18</td></tr><tr><th>PODNet [6]</th><td>-</td><td>-</td><td>0.46</td><td>64.83</td><td>0.46</td><td>64.03</td></tr><tr><th>TPCIL [34]</th><td>-</td><td>-</td><td>0.46</td><td>65.34</td><td>0.46</td><td>63.58</td></tr><tr><th>Ours(w/o P)</th><td>0.92</td><td>70.18(+3.42)</td><td>1.61</td><td>68.52(+3.18)</td><td>2.76</td><td>67.09(+3.06)</td></tr><tr><th>Ours</th><td>0.32</td><td>69.52(+2.76)</td><td>0.59</td><td>67.60(+2.26)</td><td>0.61</td><td>66.36(+2.33)</td></tr></tbody></table>", "caption": "Table 8: Results on CIFAR100-B50 using modified 32-layer ResNet. \\#Paras means the average number of parameters used during inference over steps, which is counted by million. Avg means the average accuracy (\\%) over steps. Ours(w/o P) means our method without pruning.", "list_citation_info": ["[12] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), 2019.", "[6] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[34] Tao Xiaoyu, Chang Xinyuan, Hong Xiaopeng, Wei Xing, and Gong Yihong. Topology-preserving class-incremental learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020."]}]}