{"title": "Once-for-all: Train one network and specialize it for efficient deployment", "abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all.", "authors": ["Han Cai", " Chuang Gan", " Tianzhe Wang", " Zhekai Zhang", " Song Han"], "pdf_url": "https://arxiv.org/abs/1908.09791", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Model</td><td>ImageNet</td><td rowspan=\"2\">MACs</td><td>Mobile</td><td>Search cost</td><td>Training cost</td><td colspan=\"3\">Total cost (N=40)</td></tr><tr><td>Top1 (%)</td><td>latency</td><td>(GPU hours)</td><td>(GPU hours)</td><td>GPU hours</td><td>CO_{2}e (lbs)</td><td>AWS cost</td></tr><tr><td>MobileNetV2 [31]</td><td>72.0</td><td>300M</td><td>66ms</td><td>0</td><td>150N</td><td>6k</td><td>1.7k</td><td>$18.4k</td></tr><tr><td>MobileNetV2 #1200</td><td>73.5</td><td>300M</td><td>66ms</td><td>0</td><td>1200N</td><td>48k</td><td>13.6k</td><td>$146.9k</td></tr><tr><td>NASNet-A [44]</td><td>74.0</td><td>564M</td><td>-</td><td>48,000N</td><td>-</td><td>1,920k</td><td>544.5k</td><td>$5875.2k</td></tr><tr><td>DARTS [25]</td><td>73.1</td><td>595M</td><td>-</td><td>96N</td><td>250N</td><td>14k</td><td>4.0k</td><td>$42.8k</td></tr><tr><td>MnasNet [33]</td><td>74.0</td><td>317M</td><td>70ms</td><td>40,000N</td><td>-</td><td>1,600k</td><td>453.8k</td><td>$4896.0k</td></tr><tr><td>FBNet-C [36]</td><td>74.9</td><td>375M</td><td>-</td><td>216N</td><td>360N</td><td>23k</td><td>6.5k</td><td>$70.4k</td></tr><tr><td>ProxylessNAS [4]</td><td>74.6</td><td>320M</td><td>71ms</td><td>200N</td><td>300N</td><td>20k</td><td>5.7k</td><td>$61.2k</td></tr><tr><td>SinglePathNAS [8]</td><td>74.7</td><td>328M</td><td>-</td><td>288 + 24N</td><td>384N</td><td>17k</td><td>4.8k</td><td>$52.0k</td></tr><tr><td>AutoSlim [38]</td><td>74.2</td><td>305M</td><td>63ms</td><td>180</td><td>300N</td><td>12k</td><td>3.4k</td><td>$36.7k</td></tr><tr><td>MobileNetV3-Large [15]</td><td>75.2</td><td>219M</td><td>58ms</td><td>-</td><td>180N</td><td>7.2k</td><td>1.8k</td><td>$22.2k</td></tr><tr><td>OFA w/o PS</td><td>72.4</td><td>235M</td><td>59ms</td><td>40</td><td>1200</td><td>1.2k</td><td>0.34k</td><td>$3.7k</td></tr><tr><td>OFA w/ PS</td><td>76.0</td><td>230M</td><td>58ms</td><td>40</td><td>1200</td><td>1.2k</td><td>0.34k</td><td>$3.7k</td></tr><tr><td>OFA w/ PS #25</td><td>76.4</td><td>230M</td><td>58ms</td><td>40</td><td>1200 + 25N</td><td>2.2k</td><td>0.62k</td><td>$6.7k</td></tr><tr><td>OFA w/ PS #75</td><td>76.9</td><td>230M</td><td>58ms</td><td>40</td><td>1200 + 75N</td><td>4.2k</td><td>1.2k</td><td>$13.0k</td></tr><tr><td>OFA{}_{\\text{Large}} w/ PS #75</td><td>80.0</td><td>595M</td><td>-</td><td>40</td><td>1200 + 75N</td><td>4.2k</td><td>1.2k</td><td>$13.0k</td></tr></tbody></table>", "caption": "Table 1: Comparison with SOTA hardware-aware NAS methods on Pixel1 phone.OFA decouples model training from neural architecture search. The search cost and training cost both stay constant as the number of deployment scenarios grows. \u201c#25\u201d denotes the specialized sub-networks are fine-tuned for 25 epochs after grabbing weights from the once-for-all network. \u201cCO_{2}e\u201d denotes CO_{2} emission which is calculated based on Strubell et al. (2019). AWS cost is calculated based on the price of on-demand P3.16xlarge instances.", "list_citation_info": ["Yu & Huang (2019a) Jiahui Yu and Thomas Huang. Autoslim: Towards one-shot architecture search for channel numbers. arXiv preprint arXiv:1903.11728, 2019a.", "Howard et al. (2019) Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV 2019, 2019.", "Strubell et al. (2019) Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In ACL, 2019.", "Wu et al. (2019) Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR, 2019.", "Guo et al. (2019) Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. arXiv preprint arXiv:1904.00420, 2019.", "Sandler et al. (2018) Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.", "Liu et al. (2019) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In ICLR, 2019.", "Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In CVPR, 2018.", "Cai et al. (2019) Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In ICLR, 2019. URL https://arxiv.org/pdf/1812.00332.pdf.", "Tan et al. (2019) Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2820\u20132828, 2019."]}]}