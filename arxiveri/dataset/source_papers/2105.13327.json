{"title": "Encoders and ensembles for task-free continual learning", "abstract": "We present an architecture that is effective for continual learning in an especially demanding setting, where task boundaries do not exist or are unknown, and where classes have to be learned online (with each example presented only once). To obtain good performance under these constraints, while mitigating catastrophic forgetting, we exploit recent advances in contrastive, self-supervised learning, allowing us to use a pre-trained, general purpose image encoder whose weights can be frozen, which precludes forgetting. The pre-trained encoder also greatly simplifies the downstream task of classification, which we solve with an ensemble of very simple classifiers. Collectively, the ensemble exhibits much better performance than any individual classifier, an effect which is amplified through specialisation and competitive selection. We assess the performance of the encoders-and-ensembles architecture on standard continual learning benchmarks, where it outperforms prior state-of-the-art by a large margin on the hardest problems, as well as in less familiar settings where the data distribution changes gradually or the classes are presented one at a time.", "authors": ["Murray Shanahan", " Christos Kaplanis", " Jovana Mitrovi\u0107"], "pdf_url": "https://arxiv.org/abs/2105.13327", "list_table_and_caption": [{"table": "<img/>", "caption": "Table 1: Accuracy (%) (higher is better). Results for GEN-MIR and ER-MIR are taken from [1]. Results for CN-DPM are taken from [29]. Results for GDumb are taken from [35]. For our models, means and standard deviations for 20 runs are shown.", "list_citation_info": ["[35] Ameya Prabhu, Puneet Kumar Dokania, and Philip H.S. Torr. GDumb: A simple approach that questions our progress in continual learning. In European Conference on Computer Vision (ECCV), 2020.", "[29] Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture model for task-free continual learning. In International Conference on Learning Representations (ICLR), 2020.", "[1] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems, volume 32, pages 11849\u201311860, 2019."]}, {"table": "<img/>", "caption": "Table 2: Forgetting (lower is better). Results for GEN-MIR and ER-MIR are taken from [1]. Means and standard deviations for 20 runs are shown.", "list_citation_info": ["[1] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, and Lucas Page-Caccia. Online continual learning with maximal interfered retrieval. In Advances in Neural Information Processing Systems, volume 32, pages 11849\u201311860, 2019."]}]}