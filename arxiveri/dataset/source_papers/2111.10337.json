{"title": "Advancing high-resolution video-language representation with large-scale video transcriptions", "abstract": "We study joint video and language (VL) pre-training to enable cross-modality learning and benefit plentiful downstream VL tasks. Existing works either extract low-quality video features or learn limited text embedding, while neglecting that high-resolution videos and diversified semantics can significantly improve cross-modality learning. In this paper, we propose a novel High-resolution and Diversified VIdeo-LAnguage pre-training model (HD-VILA) for many visual tasks. In particular, we collect a large dataset with two distinct properties: 1) the first high-resolution dataset including 371.5k hours of 720p videos, and 2) the most diversified dataset covering 15 popular YouTube categories. To enable VL pre-training, we jointly optimize the HD-VILA model by a hybrid Transformer that learns rich spatiotemporal features, and a multimodal Transformer that enforces interactions of the learned video features with diversified texts. Our pre-training model achieves new state-of-the-art results in 10 VL understanding tasks and 2 more novel text-to-visual generation tasks. For example, we outperform SOTA models with relative increases of 40.4% R@1 in zero-shot MSR-VTT text-to-video retrieval task and 55.4% in high-resolution dataset LSMDC. The learned VL embedding is also effective in generating visually pleasing and semantically relevant results in text-to-visual editing and super-resolution tasks.", "authors": ["Hongwei Xue", " Tiankai Hang", " Yanhong Zeng", " Yuchong Sun", " Bei Liu", " Huan Yang", " Jianlong Fu", " Baining Guo"], "pdf_url": "https://arxiv.org/abs/2111.10337", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>Domain</th><th>#Video clips</th><th>#Sentence</th><th>Avg len(sec)</th><th>Sent len</th><th>Duration(h)</th><th>Resolution</th></tr></thead><tbody><tr><td>MSR-VTT [59]</td><td>open</td><td>10K</td><td>200K</td><td>15.0</td><td>9.3</td><td>40</td><td>240p</td></tr><tr><td>DideMo [2]</td><td>Flickr</td><td>27K</td><td>41K</td><td>6.9</td><td>8.0</td><td>87</td><td>-</td></tr><tr><td>LSMDC [45]</td><td>movie</td><td>118K</td><td>118K</td><td>4.8</td><td>7.0</td><td>158</td><td>1080p</td></tr><tr><td>YouCook II [69]</td><td>cooking</td><td>14K</td><td>14K</td><td>19.6</td><td>8.8</td><td>176</td><td>-</td></tr><tr><td>How2 [47]</td><td>instructional</td><td>80K</td><td>80K</td><td>90.0</td><td>20.0</td><td>2K</td><td>-</td></tr><tr><td>ActivityNet Caption [28]</td><td>action</td><td>100K</td><td>100K</td><td>36.0</td><td>13.5</td><td>849</td><td>-</td></tr><tr><td>WebVid-2M [3]</td><td>open</td><td>2.5M</td><td>2.5M</td><td>18.0</td><td>12.0</td><td>13K</td><td>360p</td></tr><tr><td>HowTo100M [41]</td><td>instructional</td><td>136M</td><td>136M</td><td>3.6</td><td>4.0</td><td>134.5K</td><td>240p</td></tr><tr><td>HD-VILA-100M (Ours)</td><td>open</td><td>103M</td><td>103M</td><td>13.4</td><td>32.5</td><td>371.5K</td><td>720p</td></tr></tbody></table>", "caption": "Table 1: Statistics of HD-VILA-100M and its comparison with existing video-language datasets.", "list_citation_info": ["[2] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, pages 5803\u20135812, 2017.", "[45] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, H. Larochelle, Aaron C. Courville, and Bernt Schiele. Movie description. IJCV, pages 94\u2013120, 2016.", "[41] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, pages 2630\u20132640, 2019.", "[3] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.", "[69] Luowei Zhou, Chenliang Xu, and Jason J. Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018.", "[59] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, pages 5288\u20135296, 2016.", "[47] Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo\u00efc Barrault, Lucia Specia, and Florian Metze. How2: A large-scale dataset for multimodal language understanding. In NeurIPS, 2018.", "[28] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706\u2013715, 2017."]}, {"table": "<table><thead><tr><th>Method</th><th>R@1 \\uparrow</th><th>R@5 \\uparrow</th><th>R@10 \\uparrow</th><th>MedR \\downarrow</th></tr></thead><tbody><tr><th>HowTo100M [41]</th><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td></tr><tr><th>CE [35]</th><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td></tr><tr><th>DECEMBERT [50]</th><td>17.5</td><td>44.3</td><td>58.6</td><td>9.0</td></tr><tr><th>HERO [32]</th><td>16.8</td><td>43.4</td><td>57.7</td><td>-</td></tr><tr><th>ClipBERT [30]</th><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><th>VLM [57]</th><td>28.1</td><td>55.5</td><td>67.4</td><td>4.0</td></tr><tr><th>MMT [15]</th><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><th>Support Set [43]</th><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><th>VideoCLIP [58]</th><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><th>Ours</th><td>35.6</td><td>65.3</td><td>78.0</td><td>3.0</td></tr><tr><th>Zero-shot</th><td></td><td></td><td></td><td></td></tr><tr><th>HT MIL-NCE [39]</th><td>9.9</td><td>24.0</td><td>32.4</td><td>29.5</td></tr><tr><th>Support Set [43]</th><td>8.7</td><td>23.0</td><td>31.1</td><td>31.0</td></tr><tr><th>VideoCLIP [58]</th><td>10.4</td><td>22.2</td><td>30.0</td><td>-</td></tr><tr><th>Ours</th><td>14.6</td><td>34.4</td><td>44.1</td><td>15.0</td></tr></tbody></table>", "caption": "Table 3: Comparison of text-to-video retrieval in MSR-VTT [59]. We gray out some lines to highlight fair comparisons with traditional retrieval models and general pre-training models. This mark is also applicable to Table 5, 6.", "list_citation_info": ["[50] Zineng Tang, Jie Lei, and Mohit Bansal. Decembert: Learning from noisy instructional videos via dense captions and entropy minimization. In NAACL, pages 2415\u20132426, 2021.", "[58] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In EMNLP, pages 6787\u20136800, 2021.", "[32] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. In EMNLP, pages 2046\u20132065, 2020.", "[30] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 7331\u20137341, 2021.", "[57] Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. Vlm: Task-agnostic video-language model pre-training for video understanding. arXiv preprint arXiv:2105.09996, 2021.", "[43] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.", "[15] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214\u2013229, 2020.", "[39] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, pages 9879\u20139889, 2020.", "[59] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, pages 5288\u20135296, 2016.", "[41] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, pages 2630\u20132640, 2019.", "[35] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In BMVC, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>R@1 \\uparrow</th><th>R@5 \\uparrow</th><th>R@10 \\uparrow</th><th>MedR \\downarrow</th></tr></thead><tbody><tr><th>HERO [32]</th><td>2.1</td><td>-</td><td>11.4</td><td>-</td></tr><tr><th>S2VT [53]</th><td>11.9</td><td>33.6</td><td>-</td><td>13.0</td></tr><tr><th>FSE [65]</th><td>13.9</td><td>36.0</td><td>-</td><td>11.0</td></tr><tr><th>CE [35]</th><td>16.1</td><td>41.1</td><td>-</td><td>8.3</td></tr><tr><th>ClipBERT [30]</th><td>20.4</td><td>48.0</td><td>60.8</td><td>6.0</td></tr><tr><th>Ours</th><td>28.8</td><td>57.4</td><td>69.1</td><td>4.0</td></tr></tbody></table>", "caption": "Table 4: Comparison of text-to-video retrieval on DiDeMo [2]. ", "list_citation_info": ["[65] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In ECCV, pages 374\u2013390, 2018.", "[2] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, pages 5803\u20135812, 2017.", "[32] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+ language omni-representation pre-training. In EMNLP, pages 2046\u20132065, 2020.", "[30] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 7331\u20137341, 2021.", "[53] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond J Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. In HLT-NAACL, 2015.", "[35] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In BMVC, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>R@1 \\uparrow</th><th>R@5 \\uparrow</th><th>R@10 \\uparrow</th><th>MedR \\downarrow</th></tr></thead><tbody><tr><th>JSFusion [61]</th><td>9.1</td><td>21.2</td><td>34.1</td><td>36.0</td></tr><tr><th>MEE [40]</th><td>9.3</td><td>25.1</td><td>33.4</td><td>27.0</td></tr><tr><th>CE [35]</th><td>11.2</td><td>26.9</td><td>34.8</td><td>25.3</td></tr><tr><th>MMT [15]</th><td>12.9</td><td>29.9</td><td>40.1</td><td>19.3</td></tr><tr><th>Ours</th><td>17.4</td><td>34.1</td><td>44.1</td><td>15.0</td></tr></tbody></table>", "caption": "Table 5: Comparison of text-to-video retrieval on LSMDC [45].", "list_citation_info": ["[45] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, H. Larochelle, Aaron C. Courville, and Bernt Schiele. Movie description. IJCV, pages 94\u2013120, 2016.", "[61] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In ECCV, pages 471\u2013487, 2018.", "[40] Antoine Miech, Ivan Laptev, and Josef Sivic. Learning a text-video embedding from incomplete and heterogeneous data. arXiv preprint arXiv:1804.02516, 2018.", "[15] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214\u2013229, 2020.", "[35] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In BMVC, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>R@1 \\uparrow</th><th>R@5 \\uparrow</th><th>R@50 \\uparrow</th><th>MedR \\downarrow</th></tr></thead><tbody><tr><th>FSE [65]</th><td>18.2</td><td>44.8</td><td>89.1</td><td>7.0</td></tr><tr><th>CE [35]</th><td>18.2</td><td>47.7</td><td>91.4</td><td>6.0</td></tr><tr><th>HSE [65]</th><td>20.5</td><td>49.3</td><td>-</td><td>-</td></tr><tr><th>ClipBERT [30]</th><td>21.3</td><td>49.0</td><td>-</td><td>6.0</td></tr><tr><th>MMT [15]</th><td>28.7</td><td>61.4</td><td>94.5</td><td>3.3</td></tr><tr><th>Support Set [43]</th><td>29.2</td><td>61.6</td><td>94.7</td><td>3.0</td></tr><tr><th>Ours</th><td>28.5</td><td>57.4</td><td>94.0</td><td>4.0</td></tr></tbody></table>", "caption": "Table 6: Comparison of text-to-video retrieval on ActivityNet [28].", "list_citation_info": ["[65] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In ECCV, pages 374\u2013390, 2018.", "[30] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 7331\u20137341, 2021.", "[43] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.", "[15] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214\u2013229, 2020.", "[35] Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman. Use what you have: Video retrieval using representations from collaborative experts. In BMVC, 2019.", "[28] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706\u2013715, 2017."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Dataset</th><th colspan=\"3\"># avg unique n-grams</th><th colspan=\"4\"># avg POS tags</th></tr><tr><th>2-gram</th><th>3-gram</th><th>4-gram</th><th>noun</th><th>adj</th><th>adv</th><th>verb</th></tr></thead><tbody><tr><th>HowTo100M[41]</th><td>1.77</td><td>2.08</td><td>1.46</td><td>2.25</td><td>0.85</td><td>0.68</td><td>0.20</td></tr><tr><th>HD-VILA-100M</th><td>4.18</td><td>13.08</td><td>20.89</td><td>6.63</td><td>1.88</td><td>2.07</td><td>5.09</td></tr></tbody></table>", "caption": "Table 10: Statistics of average unique n-grams and POS tags. Our dataset has more unique n-grams and POS tags than HowTo100M[41]. The result indicates the transcriptions in HD-VILA-100M have richer and more diverse semantics.", "list_citation_info": ["[41] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, pages 2630\u20132640, 2019."]}]}