{"title": "Implicit Transformer Network for Screen Content Image Continuous Super-Resolution", "abstract": "Nowadays, there is an explosive growth of screen contents due to the wide application of screen sharing, remote cooperation, and online education. To match the limited terminal bandwidth, high-resolution (HR) screen contents may be downsampled and compressed. At the receiver side, the super-resolution (SR) of low-resolution (LR) screen content images (SCIs) is highly demanded by the HR display or by the users to zoom in for detail observation. However, image SR methods mostly designed for natural images do not generalize well for SCIs due to the very different image characteristics as well as the requirement of SCI browsing at arbitrary scales. To this end, we propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at arbitrary ratios, pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. We construct benchmark SCI1K and SCI1K-compression datasets with LR and HR SCI pairs. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs.", "authors": ["Jingyu Yang", " Sheng Shen", " Huanjing Yue", " Kun Li"], "pdf_url": "https://arxiv.org/abs/2112.06174", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><td colspan=\"6\">Dataset: SCI1K</td><td colspan=\"6\">Dataset: SCI1K-compression</td></tr><tr><td colspan=\"3\">In-training-scale</td><td colspan=\"3\">Out-of-training-scale</td><td colspan=\"3\">In-training-scale</td><td colspan=\"3\">Out-of-training-scale</td></tr><tr><td>\\times2</td><td>\\times3</td><td>\\times4</td><td>\\times5</td><td>\\times7</td><td>\\times9</td><td>\\times2</td><td>\\times3</td><td>\\times4</td><td>\\times5</td><td>\\times7</td><td>\\times9</td></tr><tr><th>Bicubic [18]</th><td>28.81</td><td>25.15</td><td>23.18</td><td>22.02</td><td>20.72</td><td>19.96</td><td>28.28</td><td>24.87</td><td>22.99</td><td>21.84</td><td>20.58</td><td>19.84</td></tr><tr><th>RDN [18]</th><td>38.45</td><td>33.59</td><td>29.81</td><td>-</td><td>-</td><td>-</td><td>35.16</td><td>30.60</td><td>27.17</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RCAN [4]</th><td>38.61</td><td>33.91</td><td>30.80</td><td>-</td><td>-</td><td>-</td><td>35.25</td><td>31.15</td><td>27.78</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MetaSR-RDN [5]</th><td>38.57</td><td>33.67</td><td>30.12</td><td>27.52</td><td>23.91</td><td>22.02</td><td>35.20</td><td>30.96</td><td>27.63</td><td>25.31</td><td>22.57</td><td>21.30</td></tr><tr><th>LIIF-RDN [6]</th><td>38.65</td><td>33.97</td><td>30.55</td><td>27.77</td><td>23.99</td><td>22.18</td><td>35.43</td><td>31.07</td><td>27.69</td><td>25.27</td><td>22.59</td><td>21.36</td></tr><tr><th>ITSRN-RDN(Ours)</th><td>38.74</td><td>34.32</td><td>30.82</td><td>28.15</td><td>24.36</td><td>22.36</td><td>35.53</td><td>31.31</td><td>28.02</td><td>25.62</td><td>22.79</td><td>21.45</td></tr></tbody></table>", "caption": "Table 1: Quantitative comparison on SCI1K and SCI1K-compression test sets in terms of PSNR (dB). The best (second best) results are in red (blue). RDN [18] and RCAN [4] use different models for different upsampling scales. MetaSR [5], LIIF [6] and ITSRN(ours) use one model for all the upsampling scales, and the three models are trained with continuous random scales uniformly sampled from \\times1 \\sim \\times4.", "list_citation_info": ["[6] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. arXiv preprint arXiv:2012.09161, 2020.", "[18] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2472\u20132481, 2018.", "[4] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 294\u2013310, 2018.", "[5] Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, and Jian Sun. Meta-sr: A magnification-arbitrary network for super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1575\u20131584, 2019."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Method</th><th colspan=\"3\">In-training-scale</th><th colspan=\"6\">Out-of-training-scale</th></tr><tr><th>\\times2</th><th>\\times3</th><th>\\times4</th><th>\\times5</th><th>\\times6</th><th>\\times7</th><th>\\times8</th><th>\\times9</th><th>\\times10</th></tr></thead><tbody><tr><th rowspan=\"5\">SCID [12]</th><th>RDN [18]</th><td>34.00</td><td>28.34</td><td>25.74</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RCAN [4]</th><td>33.90</td><td>28.98</td><td>26.02</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MetaSR-RDN [5]</th><td>33.84</td><td>29.08</td><td>25.76</td><td>23.62</td><td>22.38</td><td>21.59</td><td>21.07</td><td>20.71</td><td>20.41</td></tr><tr><th>LIIF-RDN [6]</th><td>34.24</td><td>29.10</td><td>25.89</td><td>23.77</td><td>22.53</td><td>21.73</td><td>21.21</td><td>20.84</td><td>20.54</td></tr><tr><th>ITSRN-RDN</th><td>34.19</td><td>29.46</td><td>26.22</td><td>23.96</td><td>22.64</td><td>21.80</td><td>21.26</td><td>20.87</td><td>20.56</td></tr><tr><th rowspan=\"5\">SIQAD [13]</th><th>RDN [18]</th><td>33.53</td><td>26.89</td><td>23.38</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RCAN [4]</th><td>32.87</td><td>27.27</td><td>23.69</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MetaSR-RDN [5]</th><td>34.12</td><td>28.40</td><td>23.55</td><td>21.18</td><td>20.18</td><td>19.63</td><td>19.25</td><td>18.94</td><td>18.65</td></tr><tr><th>LIIF-RDN [6]</th><td>34.31</td><td>28.27</td><td>23.44</td><td>21.16</td><td>20.25</td><td>19.70</td><td>19.36</td><td>19.02</td><td>18.70</td></tr><tr><th>ITSRN-RDN</th><td>34.68</td><td>29.07</td><td>24.03</td><td>21.44</td><td>20.38</td><td>19.77</td><td>19.40</td><td>19.09</td><td>18.79</td></tr></tbody></table>", "caption": "Table 2: Quantitative evaluation on SCI quality assessment datasets in terms of PSNR (dB). The best (second best) results are in red (blue). RDN [18] and RCAN [4] train different models for different upsampling scales. The rest methods train one model for all the upsampling scales. All the models are trained on the SCI1K training set.", "list_citation_info": ["[13] Huan Yang, Yuming Fang, and Weisi Lin. Perceptual quality assessment of screen content images. IEEE Transactions on Image Processing, 24(11):4408\u20134421, 2015.", "[18] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2472\u20132481, 2018.", "[12] Zhangkai Ni, Lin Ma, Huanqiang Zeng, Jing Chen, Canhui Cai, and Kai-Kuang Ma. Esim: Edge similarity for screen content image quality assessment. IEEE Transactions on Image Processing, 26(10):4818\u20134831, 2017.", "[6] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. arXiv preprint arXiv:2012.09161, 2020.", "[5] Xuecai Hu, Haoyuan Mu, Xiangyu Zhang, Zilei Wang, Tieniu Tan, and Jian Sun. Meta-sr: A magnification-arbitrary network for super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1575\u20131584, 2019.", "[4] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 294\u2013310, 2018."]}]}