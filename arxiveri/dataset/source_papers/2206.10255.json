{"title": "GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles", "abstract": "Multi-object tracking (MOT) is among crucial applications in modern advanced driver assistance systems (ADAS) and autonomous driving (AD) systems. The global nearest neighbor (GNN) filter, as the earliest random vector-based Bayesian tracking framework, has been adopted in most of state-of-the-arts trackers in the automotive industry. The development of random finite set (RFS) theory facilitates a mathematically rigorous treatment of the MOT problem, and different variants of RFS-based Bayesian filters have then been proposed. However, their effectiveness in the real ADAS and AD application is still an open problem. In this paper, it is demonstrated that the latest RFS-based Bayesian tracking framework could be superior to typical random vector-based Bayesian tracking framework via a systematic comparative study of both traditional random vector-based Bayesian filters with rule-based heuristic track maintenance and RFS-based Bayesian filters on the nuScenes validation dataset. An RFS-based tracker, namely Poisson multi-Bernoulli filter using the global nearest neighbor (GNN-PMB), is proposed to LiDAR-based MOT tasks. This GNN-PMB tracker is simple to use, and it achieves competitive results on the nuScenes dataset. Specifically, the proposed GNN-PMB tracker outperforms most state-of-the-art LiDAR-only trackers and LiDAR and camera fusion-based trackers, ranking the $3^{rd}$ among all LiDAR-only trackers on nuScenes 3D tracking challenge leader board at the time of submission.", "authors": ["Jianan Liu", " Liping Bai", " Yuxuan Xia", " Tao Huang", " Bing Zhu", " Qing-Long Han"], "pdf_url": "https://arxiv.org/abs/2206.10255", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>AMOTA\\uparrow</td><td>AMOTP\\downarrow</td><td>MT\\uparrow</td><td>ML\\downarrow</td><td>TP\\uparrow</td><td>FP\\downarrow</td><td>FN\\downarrow</td><td>IDS\\downarrow</td><td>FRAG\\downarrow</td></tr><tr><td>AB3DMOT (IROS 2020)* [11]</td><td>0.151</td><td>1.501</td><td>1006</td><td>4428</td><td>34808</td><td>15088</td><td>75730</td><td>9027</td><td>2557</td></tr><tr><td>StanfordIPRL-TRI (NeurIPS Workshop 2019)* [12]</td><td>0.550</td><td>0.798</td><td>4294</td><td>2184</td><td>85399</td><td>17533</td><td>33216</td><td>950</td><td>776</td></tr><tr><td>RFS-M3 (ICRA 2021)* [27]</td><td>0.619</td><td>0.752</td><td>5107</td><td>1878</td><td>90872</td><td>16728</td><td>27168</td><td>1525</td><td>856</td></tr><tr><td>CBMOT-LiDAR (IROS 2021)* [26]</td><td>0.649</td><td>0.592</td><td>5319</td><td>1966</td><td>94916</td><td>16469</td><td>24092</td><td>557</td><td>450</td></tr><tr><td>SimpleTrack (Arxiv 2021)* [13]</td><td>0.668</td><td>0.550</td><td>5476</td><td>1780</td><td>95539</td><td>17514</td><td>23451</td><td>575</td><td>591</td></tr><tr><td>BPTracker (Proceedings of theIEEE 2018)* [28]</td><td>0.646</td><td>0.606</td><td>5186</td><td>2259</td><td>95053</td><td>18581</td><td>24358</td><td>154</td><td>221</td></tr><tr><td>ImmortalTracker (Arxiv 2021)* [25]</td><td>0.677</td><td>0.599</td><td>5565</td><td>1669</td><td>96584</td><td>18012</td><td>21661</td><td>320</td><td>477</td></tr><tr><td>GNN-PMB (Our)*</td><td>0.678</td><td>0.560</td><td>5698</td><td>1622</td><td>97274</td><td>17071</td><td>21521</td><td>770</td><td>431</td></tr><tr><td>PF-MOT tracker (ICRA 2022)** [53]</td><td>0.682</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td><td>N/A</td></tr><tr><td>GNN-PMB (Ours)**</td><td>0.707</td><td>0.560</td><td>4608</td><td>1347</td><td>83134</td><td>12362</td><td>18113</td><td>650</td><td>345</td></tr></table><ul><li>*<p>The metrics are reported on the nuScenes test set.</p></li><li>**<p>The metrics are reported on the nuScenes validation set.</p></li></ul>", "caption": "TABLE III: Tracking results of proposed method and different model-based trackers using LiDAR on nuScenes", "list_citation_info": ["[11] X. Weng, J. Wang, D. Held, and K. Kitani, \u201c3D multi-object tracking: A baseline and new evaluation metrics,\u201d in Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 10359-10366.", "[28] F. Meyer, T. Kropfreiter, J. L. Williams, R. Lau, F. Hlawatsch, P. Braca, and M. Z. Win, \u201cMessage passing algorithms for scalable multitarget tracking,\u201d in Proceedings of the IEEE, vol. 106, no. 2, pp. 221-259, Feb. 2018.", "[27] S. Pang, D. Morris, and H. Radha, \u201c3D multi-object tracking using random finite set-based multiple measurement models filtering (RFS-M3) for autonomous vehicles,\u201d in Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 13701-13707.", "[53] T. Wen, Y. Zhang, and N. M. Freris, \u201cPF-MOT: Probability fusion based 3D multi-object tracking for autonomous vehicles,\u201d in Proceedings of the 2022 International Conference on Robotics and Automation (ICRA), 2022, pp. 700-706.", "[13] Z. Pang, Z. Li, and N. Wang, \u201cSimpleTrack: Understanding and rethinking 3D multi-object tracking,\u201d 2021, arXiv:2111.09621.", "[26] N. Benbarka, J. Schr\u00f6der, and A. Zell, \u201cScore refinement for confidence-based 3D multi-object tracking,\u201d in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 8083-8090.", "[25] Q. Wang, Y. Chen, Z. Pang, N. Wang, and Z. Zhang, \u201cImmortal tracker: Tracklet never dies,\u201d 2021, arXiv:2111.13672.", "[12] H.-K. Chiu, A. Prioletti, J. Li, and J. Bohg, \u201cProbabilistic 3d multi-object tracking for autonomous driving,\u201d 2020, arXiv:2001.05673."]}, {"table": "<table><tr><td>Method</td><td>AMOTA\\uparrow</td><td>AMOTP\\downarrow</td><td>MT\\uparrow</td><td>ML\\downarrow</td><td>TP\\uparrow</td><td>FP\\downarrow</td><td>FN\\downarrow</td><td>IDS\\downarrow</td><td>FRAG\\downarrow</td></tr><tr><td>SimTrack (ICCV 2021)* [29]</td><td>0.645</td><td>0.681</td><td>5063</td><td>1986</td><td>92093</td><td>17443</td><td>26430</td><td>1042</td><td>472</td></tr><tr><td>OGR3MOT (IEEE RAL 2022)* [30]</td><td>0.656</td><td>0.620</td><td>5278</td><td>2094</td><td>95264</td><td>17877</td><td>24013</td><td>288</td><td>371</td></tr><tr><td>NEBP (Arxiv 2022)* [31]</td><td>0.673</td><td>0.586</td><td>5380</td><td>2126</td><td>97023</td><td>19535</td><td>22380</td><td>162</td><td>256</td></tr><tr><td>GNN-PMB (Our)*</td><td>0.678</td><td>0.560</td><td>5698</td><td>1622</td><td>97274</td><td>17071</td><td>21521</td><td>770</td><td>431</td></tr><tr><td>TransMOT (IEEE IV 2022)** [54]</td><td>0.674</td><td>0.754</td><td>2096</td><td>N/A</td><td>N/A</td><td>9449</td><td>14071</td><td>1403</td><td>N/A</td></tr><tr><td>GNN-PMB (Ours)**</td><td>0.849</td><td>0.387</td><td>2762</td><td>668</td><td>49182</td><td>6140</td><td>8791</td><td>344</td><td>170</td></tr></table><ul><li>*<p>The metrics are reported on the nuScenes test set.</p></li><li>**<p>The metrics are reported on the nuScenes validation set for car.</p></li></ul>", "caption": "TABLE IV: Tracking results of proposed method and different learning-based trackers using LiDAR on nuScenes", "list_citation_info": ["[31] M. Liang and F. Meyer, \u201cNeural enhanced belief propagation for data association in multiobject tracking,\u201d 2022, arXiv:2203.09948.", "[29] C. Luo, X. Yang, and A. Yuille, \u201cExploring simple 3d multi-object tracking for autonomous driving,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 10488-10497.", "[54] F. Ruppel, F. Faion, C. Gl\u00e4ser, and K. Dietmayer. \u201cTransformers for multi-object tracking on point clouds,\u201d 2022, arXiv:2205.15730. Accepted by the 2022 IEEE Intelligent Vehicles Symposium (IV).", "[30] J.-N. Zaech, A. Liniger, D. Dai, M. Danelljan, and L. Van Gool, \u201cLearnable online graph representations for 3D multi-object tracking,\u201d IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 5103-5110, April 2022."]}, {"table": "<table><tr><td>Method</td><td>AMOTA\\uparrow</td><td>AMOTP\\downarrow</td><td>MT\\uparrow</td><td>ML\\downarrow</td><td>TP\\uparrow</td><td>FP\\downarrow</td><td>FN\\downarrow</td><td>IDS\\downarrow</td><td>FRAG\\downarrow</td></tr><tr><td>Probabilistic3DMM (ICRA 2021)* [14]</td><td>0.655</td><td>0.617</td><td>5494</td><td>1557</td><td>95199</td><td>18061</td><td>23323</td><td>1043</td><td>717</td></tr><tr><td>EagerMOT (ICRA 2021)* [15]</td><td>0.677</td><td>0.550</td><td>5303</td><td>1842</td><td>93484</td><td>17705</td><td>24925</td><td>1156</td><td>601</td></tr><tr><td>CBMOT (IROS 2021)* [26]</td><td>0.676</td><td>0.518</td><td>5420</td><td>1654</td><td>96028</td><td>21604</td><td>22828</td><td>709</td><td>1015</td></tr><tr><td>AlphaTrack (IROS 2021)* [19]</td><td>0.693</td><td>0.585</td><td>5560</td><td>1744</td><td>95851</td><td>18421</td><td>22996</td><td>718</td><td>480</td></tr><tr><td>GNN-PMB (Ours)**</td><td>0.678</td><td>0.560</td><td>5698</td><td>1622</td><td>97274</td><td>17071</td><td>21521</td><td>770</td><td>431</td></tr></table><ul><li>*<p>All trackers are based fusion of LiDAR and camera.</p></li><li>**<p>Our tracker is based on LiDAR only.</p></li></ul>", "caption": "TABLE V: Tracking results of proposed method and different trackers using LiDAR and camera fusion on nuScenes test set", "list_citation_info": ["[15] A. Kim, A. O\u0161ep, and L. Leal-Taix\u00e9, \u201cEagerMOT: 3D multi-object tracking via sensor fusion,\u201d in Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 11315-11321.", "[14] H.-K. Chiu, J. Li, R. Ambru\u015f, and J. Bohg, \u201cProbabilistic 3D multi-modal, multi-object tracking for autonomous driving,\u201d in Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA), 2021, pp. 14227-14233.", "[19] Y. Zeng, C. Ma, M. Zhu, Z. Fan, and X. Yang, \u201cCross-modal 3D object detection and tracking for auto-driving,\u201d in Proceedings of the 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 3850-3857.", "[26] N. Benbarka, J. Schr\u00f6der, and A. Zell, \u201cScore refinement for confidence-based 3D multi-object tracking,\u201d in Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021, pp. 8083-8090."]}]}