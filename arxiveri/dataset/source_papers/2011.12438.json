{"title": "Continuous surface embeddings", "abstract": "In this work, we focus on the task of learning and representing dense correspondences in deformable object categories. While this problem has been considered before, solutions so far have been rather ad-hoc for specific object types (i.e., humans), often with significant manual work involved. However, scaling the geometry understanding to all objects in nature requires more automated approaches that can also express correspondences between related, but geometrically different objects. To this end, we propose a new, learnable image-based representation of dense correspondences. Our model predicts, for each pixel in a 2D image, an embedding vector of the corresponding vertex in the object mesh, therefore establishing dense correspondences between image pixels and 3D object geometry. We demonstrate that the proposed approach performs on par or better than the state-of-the-art methods for dense pose estimation for humans, while being conceptually simpler. We also collect a new in-the-wild dataset of dense correspondences for animal classes and demonstrate that our framework scales naturally to the new deformable object categories.", "authors": ["Natalia Neverova", " David Novotny", " Vasil Khalidov", " Marc Szafraniec", " Patrick Labatut", " Andrea Vedaldi"], "pdf_url": "https://arxiv.org/abs/2011.12438", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th>architecture</th><th>AP</th><th>\\textbf{AP}_{50}</th><th>\\textbf{AP}_{75}</th><th>\\textbf{AP}_{M}</th><th>\\textbf{AP}_{L}</th><th>AR</th><th>\\textbf{AR}_{50}</th><th>\\textbf{AR}_{75}</th><th>\\textbf{AR}_{M}</th><th>\\textbf{AR}_{L}</th></tr></thead><tbody><tr><td rowspan=\"8\"><p>IUV (baselines)</p></td><td>DP-RCNN (R50) guler18densepose </td><td>54.9</td><td>89.8</td><td>62.2</td><td>47.8</td><td>56.3</td><td>61.9</td><td>93.9</td><td>70.8</td><td>49.1</td><td>62.8</td></tr><tr><td>DP-RCNN (R101) guler18densepose </td><td>56.1</td><td>90.4</td><td>64.4</td><td>49.2</td><td>57.4</td><td>62.8</td><td>93.7</td><td>72.4</td><td>50.1</td><td>63.6</td></tr><tr><td>Parsing-RCNN yang2019cvpr </td><td>65</td><td>93</td><td>78</td><td>56</td><td>67</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>AMA-net guo2019aamanet </td><td>64.1</td><td>91.4</td><td>72.9</td><td>59.3</td><td>65.3</td><td>71.6</td><td>94.7</td><td>79.8</td><td>61.3</td><td>72.3</td></tr><tr><td>DP-RCNN* (R50)</td><td>65.3</td><td>92.5</td><td>77.1</td><td>58.6</td><td>66.6</td><td>71.1</td><td>95.3</td><td>82.0</td><td>60.1</td><td>71.9</td></tr><tr><td>DP-RCNN* (R101)</td><td>66.4</td><td>92.9</td><td>77.9</td><td>60.6</td><td>67.5</td><td>71.9</td><td>95.5</td><td>82.6</td><td>62.1</td><td>72.6</td></tr><tr><td>DP-RCNN-DeepLab* (R50)</td><td>66.8</td><td>92.8</td><td>79.7</td><td>60.7</td><td>68.0</td><td>72.1</td><td>95.8</td><td>82.9</td><td>62.2</td><td>72.4</td></tr><tr><td>DP-RCNN-DeepLab* (R101)</td><td>67.7</td><td>93.5</td><td>79.7</td><td>62.6</td><td>69.1</td><td>73.6</td><td>96.5</td><td>84.7</td><td>64.2</td><td>74.2</td></tr><tr><td rowspan=\"4\"><p>CSE</p></td><td>DP-RCNN* (R50)</td><td>66.1</td><td>92.5</td><td>78.2</td><td>58.7</td><td>67.4</td><td>71.7</td><td>95.5</td><td>82.4</td><td>60.3</td><td>72.5</td></tr><tr><td>DP-RCNN* (R101)</td><td>67.0</td><td>93.8</td><td>78.6</td><td>60.1</td><td>68.3</td><td>72.8</td><td>96.4</td><td>83.7</td><td>61.5</td><td>73.6</td></tr><tr><td>DP-RCNN-DeepLab* (R50)</td><td>66.6</td><td>93.8</td><td>77.6</td><td>60.8</td><td>67.7</td><td>72.8</td><td>96.5</td><td>83.1</td><td>62.1</td><td>73.5</td></tr><tr><td>DP-RCNN-DeepLab* (R101)</td><td>68.0</td><td>94.1</td><td>80.0</td><td>61.9</td><td>69.4</td><td>74.3</td><td>97.1</td><td>85.5</td><td>63.8</td><td>75.0</td></tr></tbody></table>", "caption": "Table 2: Performance on DensePose-COCO, with IUV (top) and CSE (bottom) training (GPSm scores, minival). First block: published SOTA DensePose methods, second block: our optimized architectures + IUV training, third block: our optimized architectures + CSE training. All CSE models are trained with loss \\mathcal{L}_{\\sigma} (eq. 4), LBO size M=256, embedding size D=16. ", "list_citation_info": ["[20] Yuyu Guo, Lianli Gao, Jingkuan Song, Peng Wang, Wuyuan Xie, and Heng Tao Shen. Adaptive Multi-Path Aggregation for Human DensePose Estimation in the Wild. In ACM International Conference on Multimedia, pages 356\u2013364, 2019.", "[17] R\u0131za Alp G\u00fcler, Natalia Neverova, and Iasonas Kokkinos. DensePose: Dense Human Pose Estimation in the Wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 7297\u20137306, 2018.", "[56] Lu Yang, Qing Song, Zhihui Wang, and Ming Jiang. Parsing R-CNN for Instance-Level Human Analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 364\u2013373, 2019."]}, {"table": "<table><thead><tr><th>model</th><th>\\mathcal{S}_{\\textrm{chimp}}</th><th>\\mathcal{S}_{\\textrm{smpl}}</th></tr></thead><tbody><tr><th>human model</th><td>\u2013</td><td>2.0</td></tr><tr><th>loss \\mathcal{L}</th><td>8.5</td><td>3.3</td></tr><tr><th>loss \\mathcal{L}_{\\sigma}</th><td>21.1</td><td>3.2</td></tr><tr><th>cla + pretrain \\Phi</th><td>36.7</td><td>34.5</td></tr><tr><th>cla + align E</th><td>37.2</td><td>35.7</td></tr></tbody></table><img/>", "caption": "Table 4: Performance on the DensePose-Chimps dataset with CSE training (AP, GPSm scores, measured on both chimp and SMPL meshes wrt the GT mapping \\mathcal{S}_{\\textrm{chimp}}\\to\\mathcal{S}_{\\textrm{smpl}} from sanakoyeu2018pr ). ", "list_citation_info": ["[43] Artsiom Sanakoyeu, Miguel \u00c1ngel Bautista, and Bj\u00f6rn Ommer. Deep unsupervised learning of visual similarities. Pattern Recognition, 78:331\u2013343, 2018."]}]}