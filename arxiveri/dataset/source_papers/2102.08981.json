{"title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "abstract": "The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pre-training. However, these datasets are often collected with overrestrictive requirements inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pre-training data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training. We perform an analysis of this dataset and benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.", "authors": ["Soravit Changpinyo", " Piyush Sharma", " Nan Ding", " Radu Soricut"], "pdf_url": "https://arxiv.org/abs/2102.08981", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"8\">nocaps val</td></tr><tr><th>Method</th><td colspan=\"2\">in-domain</td><td colspan=\"2\">near-domain</td><td colspan=\"2\">out-of-domain</td><td colspan=\"2\">overall</td></tr><tr><th></th><td>CIDEr</td><td>SPICE</td><td>CIDEr</td><td>SPICE</td><td>CIDEr</td><td>SPICE</td><td>CIDEr</td><td>SPICE</td></tr><tr><th>UpDown [2]</th><td>78.1</td><td>11.6</td><td>57.7</td><td>10.3</td><td>31.3</td><td>8.3</td><td>55.3</td><td>10.1</td></tr><tr><th>UpDown + CBS [2]</th><td>80.0</td><td>12.0</td><td>73.6</td><td>11.3</td><td>66.4</td><td>9.7</td><td>73.1</td><td>11.1</td></tr><tr><th>UpDown + ELMo + CBS [2]</th><td>79.3</td><td>12.4</td><td>73.8</td><td>11.4</td><td>71.7</td><td>9.9</td><td>74.3</td><td>11.2</td></tr><tr><th>Oscar{}_{L} [50]</th><td>79.9</td><td>12.4</td><td>68.2</td><td>11.8</td><td>45.1</td><td>9.4</td><td>65.2</td><td>11.4</td></tr><tr><th>Oscar{}_{L} + CBS [50]</th><td>78.8</td><td>12.2</td><td>78.9</td><td>12.1</td><td>77.4</td><td>10.5</td><td>78.6</td><td>11.8</td></tr><tr><th>Oscar{}_{L} + SCST + CBS [50]</th><td>85.4</td><td>11.9</td><td>84.0</td><td>11.7</td><td>80.3</td><td>10.0</td><td>83.4</td><td>11.4</td></tr><tr><th>VIVO [32]</th><td>88.8</td><td>12.9</td><td>83.2</td><td>12.6</td><td>71.1</td><td>10.6</td><td>81.5</td><td>12.2</td></tr><tr><th>VIVO + CBS [32]</th><td>90.4</td><td>13.0</td><td>84.9</td><td>12.5</td><td>83.0</td><td>10.7</td><td>85.3</td><td>12.2</td></tr><tr><th>VIVO + SCST + CBS [32]</th><td>92.2</td><td>12.9</td><td>87.8</td><td>12.6</td><td>87.5</td><td>11.5</td><td>88.3</td><td>12.4</td></tr><tr><th>pretrain ic on CC12M</th><td>88.3</td><td>12.3</td><td>86.0</td><td>11.8</td><td>91.3</td><td>11.2</td><td>87.4</td><td>11.8</td></tr><tr><th>pretrain ic on CC3M+CC12M</th><td>92.6</td><td>12.5</td><td>88.3</td><td>12.1</td><td>94.5</td><td>11.9</td><td>90.2</td><td>12.1</td></tr><tr><th>Human</th><td>84.4</td><td>14.3</td><td>85.0</td><td>14.3</td><td>95.7</td><td>14.0</td><td>87.1</td><td>14.2</td></tr><tr><th></th><td colspan=\"8\">nocaps test</td></tr><tr><th>UpDown [2]</th><td>74.3</td><td>11.5</td><td>56.9</td><td>10.3</td><td>30.1</td><td>8.1</td><td>54.3</td><td>10.1</td></tr><tr><th>UpDown + ELMo + CBS [2]</th><td>76.0</td><td>11.8</td><td>74.2</td><td>11.5</td><td>66.7</td><td>9.7</td><td>73.1</td><td>11.2</td></tr><tr><th>VIVO + SCST + CBS [32]</th><td>89.0</td><td>12.9</td><td>87.8</td><td>12.6</td><td>80.1</td><td>11.1</td><td>86.6</td><td>12.4</td></tr><tr><th>pretrain ic on CC12M</th><td>82.9</td><td>11.9</td><td>85.7</td><td>12.0</td><td>85.3</td><td>11.3</td><td>85.3</td><td>11.8</td></tr><tr><th>pretrain ic on CC3M+CC12M</th><td>87.2</td><td>12.3</td><td>87.4</td><td>12.1</td><td>87.2</td><td>11.4</td><td>87.3</td><td>12.0</td></tr><tr><th>Human</th><td>80.6</td><td>15.0</td><td>84.6</td><td>14.7</td><td>91.6</td><td>14.2</td><td>85.3</td><td>14.7</td></tr></tbody></table>", "caption": "Table 4: Comparison between our best model (in italics, pre-trained on CC12M with ic and fine-tuned on COCO Captions) and existing models, on the nocaps val (top) and test (bottom) splits. Bold indicates best-to-date, underline indicates second-best.", "list_citation_info": ["[2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, 2019.", "[50] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, 2020.", "[32] Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. VIVO: Surpassing human performance in novel object captioning with visual vocabulary pre-training. arXiv preprint arXiv:2009.13682, 2020."]}, {"table": "<table><tbody><tr><th></th><td>CC3M</td><td>CC3M</td></tr><tr><th>Method</th><td>dev</td><td>test</td></tr><tr><th></th><td>CIDEr</td><td>CIDEr</td></tr><tr><th>FRCNN [19]</th><td>89.2</td><td>94.4</td></tr><tr><th>TTIC+BIU (single model)</th><td>-</td><td>98.0</td></tr><tr><th>Ultra [19]</th><td>93.7</td><td>98.4</td></tr><tr><th>no pretrain</th><td>100.9</td><td>-</td></tr><tr><th>pretrain ic on CC12M (no ft)</th><td>39.3</td><td>-</td></tr><tr><th>pretrain ic on CC12M</th><td>105.4</td><td>-</td></tr></tbody></table>", "caption": "Table 7: Performance on the Conceptual Captions (CC3M) benchmark. Our methods are in italics. \u201cft\u201d stands for fine-tuning. The top two CC3M test CIDEr baseline scores are from the Conceptual Captions Leaderboard as of Nov 15, 2020.", "list_citation_info": ["[19] Soravit Changpinyo, Bo Pang, Piyush Sharma, and Radu Soricut. Decoupled box proposal and featurization with ultrafine-grained semantic labels improve image captioning and visual question answering. In EMNLP-IJCNLP, 2019."]}]}