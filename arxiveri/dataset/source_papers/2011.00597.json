{"title": "Coot: Cooperative Hierarchical Transformer for Video-Text Representation Learning", "abstract": "Many real-world video-text tasks involve different levels of granularity, such as frames and words, clip and sentences or videos and paragraphs, each with distinct semantics. In this paper, we propose a Cooperative hierarchical Transformer (COOT) to leverage this hierarchy information and model the interactions between different levels of granularity and different modalities. The method consists of three major components: an attention-aware feature aggregation layer, which leverages the local temporal context (intra-level, e.g., within a clip), a contextual transformer to learn the interactions between low-level and high-level semantics (inter-level, e.g. clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss to connect video and text. The resulting method compares favorably to the state of the art on several benchmarks while having few parameters. All code is available open-source at https://github.com/gingsi/coot-videotext", "authors": ["Simon Ging", " Mohammadreza Zolfaghari", " Hamed Pirsiavash", " Thomas Brox"], "pdf_url": "https://arxiv.org/abs/2011.00597", "list_table_and_caption": [{"table": "<table><tr><td></td><td colspan=\"4\">Paragraph\\impliesVideo</td><td colspan=\"4\">Video\\impliesParagraph</td></tr><tr><td>Method</td><td>R@1</td><td>R@5</td><td>R@50</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@50</td><td>MR</td></tr><tr><td>LSTM-YT venugopalan2015sequence </td><td>0.0</td><td>4.0</td><td>24.0</td><td>102.0</td><td>0.0</td><td>7.0</td><td>38.0</td><td>98.0</td></tr><tr><td>No Context venugopalan2014translating </td><td>5.0</td><td>14.0</td><td>32.0</td><td>78.0</td><td>7.0</td><td>18.0</td><td>45.0</td><td>56.0</td></tr><tr><td>DENSE krishna2017dense </td><td>14.0</td><td>32.0</td><td>65.0</td><td>34.0</td><td>18.0</td><td>36.0</td><td>74.0</td><td>32.0</td></tr><tr><td>VSE NIPS2013_5204 ( Shao_2018_ECCV )</td><td>11.7</td><td>34.7</td><td>85.7</td><td>10</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>FSE cmhse </td><td>18.2</td><td>44.8</td><td>89.1</td><td>7</td><td>16.7</td><td>43.1</td><td>88.4</td><td>7</td></tr><tr><td>HSE cmhse </td><td>44.4\\pm0.5</td><td>76.7\\pm0.3</td><td>97.1\\pm0.1</td><td>2</td><td>44.2\\pm0.6</td><td>76.7\\pm0.3</td><td>97.0\\pm0.3</td><td>2</td></tr><tr><td>COOT</td><td>60.8\\pm0.6</td><td>86.6\\pm0.4</td><td>98.6\\pm0.1</td><td>1</td><td>60.9\\pm0.3</td><td>87.4\\pm0.5</td><td>98.6\\pm0.0</td><td>1</td></tr></table>", "caption": "Table 2: Video-paragraph retrieval results on AcitvityNet-captions dataset (val1).", "list_citation_info": ["[39] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In The IEEE International Conference on Computer Vision (ICCV), Oct 2017.", "[5] Dian Shao, Yu Xiong, Yue Zhao, Qingqiu Huang, Yu Qiao, and Dahua Lin. Find and focus: Retrieve and localize video events with natural language queries. In The European Conference on Computer Vision (ECCV), September 2018.", "[52] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision, pages 4534\u20134542, 2015.", "[54] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc' Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2121\u20132129. Curran Associates, Inc., 2013.", "[21] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, pages 385\u2013401, 2018.", "[53] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, 2014."]}, {"table": "<table><tr><td></td><td></td><td colspan=\"4\">Paragraph\\impliesVideo</td><td colspan=\"4\">Sentence\\impliesClip</td></tr><tr><td>Method</td><td>TrainSet</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MR</td></tr><tr><td>Random</td><td>-</td><td>0.21</td><td>1.09</td><td>2.19</td><td>229</td><td>0.03</td><td>0.15</td><td>0.3</td><td>1675</td></tr><tr><td>Miech et al. miech19howto100m </td><td>HowTo100M</td><td>43.1*</td><td>68.6*</td><td>79.1*</td><td>2*</td><td>6.1</td><td>17.3</td><td>24.8</td><td>46</td></tr><tr><td>ActBERT actbert20 </td><td>HowTo100M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>9.6</td><td>26.7</td><td>38.0</td><td>19</td></tr><tr><td>MIL-NCE miech20endtoend </td><td>HowTo100M</td><td>61.9*</td><td>89.4*</td><td>98.9*</td><td>1*</td><td>15.1</td><td>38.0</td><td>51.2</td><td>10</td></tr><tr><td>HGLMM klein15 </td><td>YouCook2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.6</td><td>14.3</td><td>21.6</td><td>75</td></tr><tr><td>Miech et al. miech19howto100m </td><td>YouCook2</td><td>32.3*</td><td>59.2*</td><td>70.9*</td><td>4*</td><td>4.2</td><td>13.7</td><td>21.5</td><td>65</td></tr><tr><td>COOT</td><td>YouCook2</td><td>50.4\\pm2.6</td><td>79.4\\pm0.6</td><td>87.4\\pm0.8</td><td>1.3\\pm0.6</td><td>5.9\\pm0.7</td><td>16.7\\pm0.6</td><td>24.8\\pm0.8</td><td>49.7\\pm2.9</td></tr><tr><td rowspan=\"2\">Miech et al. miech19howto100m </td><td>HowTo100M+</td><td rowspan=\"2\">59.6*</td><td rowspan=\"2\">86.0*</td><td rowspan=\"2\">93.6*</td><td rowspan=\"2\">1*</td><td rowspan=\"2\">8.2</td><td rowspan=\"2\">24.5</td><td rowspan=\"2\">35.3</td><td rowspan=\"2\">24</td></tr><tr><td>YouCook2</td></tr><tr><td rowspan=\"2\">COOT</td><td>HowTo100M{}^{\\bigtriangleup}+</td><td rowspan=\"2\">77.2\\pm1.0</td><td rowspan=\"2\">95.8\\pm0.8</td><td rowspan=\"2\">97.5\\pm0.3</td><td rowspan=\"2\">1.0\\pm0.0</td><td rowspan=\"2\">16.7\\pm0.4</td><td rowspan=\"2\">40.2\\pm0.3</td><td rowspan=\"2\">52.3\\pm0.5</td><td rowspan=\"2\">9.0\\pm0.0</td></tr><tr><td>YouCook2</td></tr></table>", "caption": "Table 3: Retrieval results on YouCook2 dataset.Results with * are computed by us. {}^{\\bigtriangleup} we use features of a video-textmodel miech20endtoend  pretrained on the HowTo100m dataset.", "list_citation_info": ["[16] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, 2019.", "[8] Yi Yang Linchao Zhu. Actbert: Learning global-local video-text representations. In CVPR, 2020.", "[17] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.", "[44] B. Klein, G. Lev, G. Sadeh, and L. Wolf. Associating neural word embeddings with deep image representations using fisher vectors. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4437\u20134446, 2015."]}, {"table": "<table><tr><td>Features</td><td>Method</td><td>TrainSet</td><td>B@3</td><td>B@4</td><td>RougeL</td><td>METEOR</td><td>CIDEr-D</td><td>R@4\\downarrow</td></tr><tr><td>RGB+Flow</td><td>VTransformer zhou2018endtoend </td><td>YouCook2</td><td>13.08*</td><td>7.62</td><td>32.18*</td><td>15.65</td><td>32.26</td><td>7.83</td></tr><tr><td>RGB+Flow</td><td>TransformerXL dai2019transformerxl </td><td>YouCook2</td><td>11.46*</td><td>6.56</td><td>30.78*</td><td>14.76</td><td>26.35</td><td>6.30</td></tr><tr><td>RGB+Flow</td><td>MART mart </td><td>YouCook2</td><td>12.83*</td><td>8.00</td><td>31.97*</td><td>15.90</td><td>35.74</td><td>4.39</td></tr><tr><td>COOT clip</td><td>MART</td><td>YouCook2</td><td>14.17</td><td>8.69</td><td>33.01</td><td>16.11</td><td>38.28</td><td>8.07</td></tr><tr><td>COOT video+clip</td><td>MART</td><td>YouCook2</td><td>15.75</td><td>9.44</td><td>34.32</td><td>18.17</td><td>46.06</td><td>6.30</td></tr><tr><td>COOT clip</td><td>MART</td><td>H100M{}^{\\bigtriangleup}+YC2</td><td>17.12</td><td>10.91</td><td>37.59</td><td>18.85</td><td>54.07</td><td>5.11</td></tr><tr><td>COOT clip</td><td>MART w/o re.</td><td>H100M{}^{\\bigtriangleup}+YC2</td><td>17.16</td><td>10.69</td><td>37.43</td><td>19.18</td><td>54.85</td><td>5.45</td></tr><tr><td>COOT clip</td><td>VTransformer</td><td>H100M{}^{\\bigtriangleup}+YC2</td><td>17.62</td><td>11.09</td><td>37.63</td><td>19.34</td><td>54.67</td><td>4.57</td></tr><tr><td>COOT video+clip</td><td>VTransformer zhou2018endtoend </td><td>H100M{}^{\\bigtriangleup}+YC2</td><td>17.79</td><td>11.05</td><td>37.51</td><td>19.79</td><td>55.57</td><td>5.69</td></tr><tr><td>COOT video+clip</td><td>MART</td><td>H100M{}^{\\bigtriangleup}+YC2</td><td>17.97</td><td>11.30</td><td>37.94</td><td>19.85</td><td>57.24</td><td>6.69</td></tr></table>", "caption": "Table 4: Captioning results on the YouCook2 dataset (val split).Results with * are computed by us. {}^{\\bigtriangleup} we use features of a video-textmodel miech20endtoend  pretrained on the HowTo100m dataset.\"MART w/o re\" denotes a MART variant without recurrence.", "list_citation_info": ["[56] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019.", "[55] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, and Caiming Xiong. End-to-end dense video captioning with masked transformer, 2018.", "[17] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020.", "[45] Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L Berg, and Mohit Bansal. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning. In ACL, 2020."]}, {"table": "<table><tr><td>Features</td><td>Method</td><td>TrainSet</td><td>B@3</td><td>B@4</td><td>RougeL</td><td>METEOR</td><td>CIDEr-D</td><td>R@4\\downarrow</td></tr><tr><td>RGB+Flow</td><td>VTransformer zhou2018endtoend </td><td>ActivityNet</td><td>16.27*</td><td>9.31</td><td>29.18*</td><td>15.54</td><td>21.33</td><td>7.45</td></tr><tr><td>RGB+Flow</td><td>TransformerXL dai2019transformerxl </td><td>ActivityNet</td><td>16.71*</td><td>10.25</td><td>30.53*</td><td>14.91</td><td>21.71</td><td>8.79</td></tr><tr><td>RGB+Flow</td><td>MART</td><td>ActivityNet</td><td>16.43*</td><td>9.78</td><td>30.63*</td><td>15.57</td><td>22.16</td><td>5.44</td></tr><tr><td>COOT video+clip</td><td>TransformerXL dai2019transformerxl </td><td>ActivityNet</td><td>16.94</td><td>10.57</td><td>30.93</td><td>14.76</td><td>22.04</td><td>15.85</td></tr><tr><td>COOT video+clip</td><td>VTransformer zhou2018endtoend </td><td>ActivityNet</td><td>16.80</td><td>10.47</td><td>30.37</td><td>15.76</td><td>25.90</td><td>19.14</td></tr><tr><td>COOT clip</td><td>MART w/o re.</td><td>ActivityNet</td><td>15.41</td><td>9.37</td><td>28.66</td><td>15.61</td><td>22.05</td><td>12.03</td></tr><tr><td>COOT video+clip</td><td>MART w/o re.</td><td>ActivityNet</td><td>16.59</td><td>10.33</td><td>29.93</td><td>15.64</td><td>25.41</td><td>17.03</td></tr><tr><td>COOT clip</td><td>MART</td><td>ActivityNet</td><td>16.53</td><td>10.22</td><td>30.68</td><td>15.91</td><td>23.98</td><td>5.35</td></tr><tr><td>COOT video+clip</td><td>MART</td><td>ActivityNet</td><td>17.43</td><td>10.85</td><td>31.45</td><td>15.99</td><td>28.19</td><td>6.64</td></tr></table>", "caption": "Table 5: Captioning results on the ActivityNet-Captions dataset (ae-test split of MART mart ).Results with * are computed by us.\"MART w/o re\" denotes a MART variant without recurrence.", "list_citation_info": ["[55] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, and Caiming Xiong. End-to-end dense video captioning with masked transformer, 2018.", "[56] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context, 2019.", "[45] Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L Berg, and Mohit Bansal. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning. In ACL, 2020."]}, {"table": "<table><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Text</td><td colspan=\"3\">Paragraph\\impliesVideo</td><td colspan=\"3\">Video\\impliesParagraph</td></tr><tr><td>R@1</td><td>R@5</td><td>R@50</td><td>R@1</td><td>R@5</td><td>R@50</td></tr><tr><td>HSE</td><td>GloVe</td><td>45.7\\pm0.3</td><td>76.1\\pm0.7</td><td>96.0\\pm0.3</td><td>44.9\\pm0.5</td><td>75.8\\pm1.2</td><td>95.8\\pm0.4</td></tr><tr><td>HSE</td><td>Bert</td><td>47.0\\pm1.1</td><td>77.0\\pm1.5</td><td>96.1\\pm0.4</td><td>46.9\\pm0.8</td><td>77.2\\pm1.1</td><td>95.9\\pm0.6</td></tr><tr><td>COOT</td><td>GloVe</td><td>56.5\\pm1.1</td><td>84.1\\pm1.3</td><td>98.0\\pm0.3</td><td>57.3\\pm1.8</td><td>84.5\\pm1.4</td><td>98.2\\pm0.2</td></tr><tr><td>COOT</td><td>Bert</td><td>60.8\\pm0.6</td><td>86.6\\pm0.4</td><td>98.6\\pm0.1</td><td>60.9\\pm0.3</td><td>87.4\\pm0.5</td><td>98.6\\pm0.0</td></tr></table>", "caption": "Table 8: Text feature ablation study on ActivityNet-captions (val1).We evaluate our choice of text encoding and show that Bert devlin2018bert  outperformsGloVe pennington2014glove  on both models and all metrics.", "list_citation_info": ["[90] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, 2014.", "[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, Jun 2019. Association for Computational Linguistics."]}, {"table": "<table><tr><td colspan=\"2\">Query:</td><td colspan=\"2\">Query:</td></tr><tr><td> RankScore </td><td><p>Retrieved Text</p></td><td> RankScore </td><td><p>Retrieved Text</p></td></tr><tr><td> 10.523 </td><td><p>place the potato wedges into a pan of hot oil</p></td><td> 10.705 </td><td><p>add oil and herbs to a pan</p></td></tr><tr><td> 20.514 </td><td>cook the apple slices in the pan</td><td> 20.622 </td><td><p>heat oil to 365 in a pan</p></td></tr><tr><td> 30.510 </td><td><p>remove the potatoes from the oil and place on paper towel</p></td><td> 30.603 </td><td><p>heat some oil in a pan</p></td></tr><tr><td> 40.497 </td><td><p>add oil to the pan and fry the hash browns</p></td><td> 40.579 </td><td><p>heat some oil in a pan</p></td></tr><tr><td> 50.495 </td><td><p>fry the potatos in oil</p></td><td> 50.575 </td><td><p>add oil to a pan</p></td></tr><tr><td> 60.480 </td><td><p>add the potatoes to the pan</p></td><td> 60.570 </td><td><p>heat some olive oil in a pan</p></td></tr><tr><td> 70.477 </td><td><p>heat the apple in a pan with some oil</p></td><td> 70.567 </td><td><p>heat some oil in a pan</p></td></tr><tr><td> 80.475 </td><td><p>pierce the knife inside the potatoes and find if the potatoes are cooked properly</p></td><td> 80.564 </td><td><p>heat oil in a pan</p></td></tr><tr><td> 90.474 </td><td><p>melt little butter and olive oil in a pan</p></td><td> 90.564 </td><td><p>heat some oil cumin seeds and coriander seeds in a pan</p></td></tr><tr><td> 100.470 </td><td><p>fry the potatoes in a deep fryer</p></td><td> 850.385 </td><td>add white wine onions a bay leaf and thyme to the pot</td></tr></table>", "caption": "Table 16: Clip-to-Sentence Retrieval on Youcook2 val set.Left: The model gives high relative score to the relevant text but has problems visuallydistinguishing apples from potatoes. Right:: Wine is confusedwith oil and the herbs cannot be identified precisely to be bay leaves andthyme. Identical sentences can produce different results, since the Bert devlin2018bert text encoder takes paragraph context into account and therefore the model inputs differ.", "list_citation_info": ["[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, Jun 2019. Association for Computational Linguistics."]}]}