{"title": "Clip-mesh: Generating Textured Meshes from Text Using Pretrained Image-Text Models", "abstract": "We present a technique for zero-shot generation of a 3D model using only a target text prompt. Without any 3D supervision our method deforms the control shape of a limit subdivided surface along with its texture map and normal map to obtain a 3D asset that corresponds to the input text prompt and can be easily deployed into games or modeling applications. We rely only on a pre-trained CLIP model that compares the input text prompt with differentiably rendered images of our 3D model. While previous works have focused on stylization or required training of generative models we perform optimization on mesh parameters directly to generate shape, texture or both. To constrain the optimization to produce plausible meshes and textures we introduce a number of techniques using image augmentations and the use of a pretrained prior that generates CLIP image embeddings given a text embedding.", "authors": ["Nasir Mohammad Khalid", " Tianhao Xie", " Eugene Belilovsky", " Tiberiu Popa"], "pdf_url": "https://arxiv.org/abs/2203.13333", "list_table_and_caption": [{"table": "<table><thead><tr><th>Generation Model</th><th colspan=\"2\">CLIP ViT-B/16</th><th colspan=\"2\">CLIP ViT-B/32</th></tr><tr><th>Evaluation Model</th><th>ViT-B/16</th><th>ViT-B/32</th><th>ViT-B/16</th><th>ViT-B/32</th></tr></thead><tbody><tr><th>Dreamfields</th><td>93.5</td><td>59.8</td><td>74.2</td><td>86.6</td></tr><tr><th>(Jain et al., 2021)</th><td></td><td></td><td></td><td></td></tr><tr><th>CLIP-Mesh [Ours]</th><td>96.7</td><td>67.8</td><td>75.8</td><td>91.4</td></tr></tbody></table>", "caption": "Table 1. Quantative comparision of our work with dreamfields on COCO caption object generation", "list_citation_info": ["Jain et al. (2021) Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. 2021. Zero-Shot Text-Guided Object Generation with Dream Fields. arXiv preprint arXiv:2112.01455 (2021)."]}]}