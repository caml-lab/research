{"title": "Video Swin Transformer", "abstract": "The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less pre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). The code and models will be made publicly available at https://github.com/SwinTransformer/Video-Swin-Transformer.", "authors": ["Ze Liu", " Jia Ning", " Yue Cao", " Yixuan Wei", " Zheng Zhang", " Stephen Lin", " Han Hu"], "pdf_url": "https://arxiv.org/abs/2106.13230", "list_table_and_caption": [{"table": "<table><tr><td> Method</td><td>Pretrain</td><td>Top-1</td><td>Top-5</td><td>Views</td><td>FLOPs</td><td>Param</td></tr><tr><td>R(2+1)D Tran et al., (2018)</td><td>-</td><td>72.0</td><td>90.0</td><td>10 \u00d7 1</td><td>75</td><td>61.8</td></tr><tr><td>I3D Carreira and Zisserman, 2017b </td><td>ImageNet-1K</td><td>72.1</td><td>90.3</td><td>-</td><td>108</td><td>25.0</td></tr><tr><td>NL I3D-101 Wang et al., (2018)</td><td>ImageNet-1K</td><td>77.7</td><td>93.3</td><td>10 \u00d7 3</td><td>359</td><td>61.8</td></tr><tr><td>ip-CSN-152 Tran et al., (2019)</td><td>-</td><td>77.8</td><td>92.8</td><td>10 \u00d7 3</td><td>109</td><td>32.8</td></tr><tr><td>CorrNet-101 Wang et al., (2020)</td><td>-</td><td>79.2</td><td>-</td><td>10 \u00d7 3</td><td>224</td><td>-</td></tr><tr><td>SlowFast R101+NL Feichtenhofer et al., (2019)</td><td>-</td><td>79.8</td><td>93.9</td><td>10 \u00d7 3</td><td>234</td><td>59.9</td></tr><tr><td>X3D-XXL Feichtenhofer, (2020)</td><td>-</td><td>80.4</td><td>94.6</td><td>10 \u00d7 3</td><td>144</td><td>20.3</td></tr><tr><td>MViT-B, 32\u00d73 Fan et al., 2021b </td><td>-</td><td>80.2</td><td>94.4</td><td>1 \u00d7 5</td><td>170</td><td>36.6</td></tr><tr><td>MViT-B, 64\u00d73 Fan et al., 2021b </td><td>-</td><td>81.2</td><td>95.1</td><td>3 \u00d7 3</td><td>455</td><td>36.6</td></tr><tr><td>TimeSformer-L Bertasius et al., (2021)</td><td>ImageNet-21K</td><td>80.7</td><td>94.7</td><td>1 \u00d7 3</td><td>2380</td><td>121.4</td></tr><tr><td>ViT-B-VTN Neimark et al., (2021)</td><td>ImageNet-21K</td><td>78.6</td><td>93.7</td><td>1 \u00d7 1</td><td>4218</td><td>11.04</td></tr><tr><td>ViViT-L/16x2 Arnab et al., (2021)</td><td>ImageNet-21K</td><td>80.6</td><td>94.7</td><td>4 \u00d7 3</td><td>1446</td><td>310.8</td></tr><tr><td>ViViT-L/16x2 320 Arnab et al., (2021)</td><td>ImageNet-21K</td><td>81.3</td><td>94.7</td><td>4 \u00d7 3</td><td>3992</td><td>310.8</td></tr><tr><td>ip-CSN-152 Tran et al., (2019)</td><td>IG-65M</td><td>82.5</td><td>95.3</td><td>10 \u00d7 3</td><td>109</td><td>32.8</td></tr><tr><td>ViViT-L/16x2 Arnab et al., (2021)</td><td>JFT-300M</td><td>82.8</td><td>95.5</td><td>4 \u00d7 3</td><td>1446</td><td>310.8</td></tr><tr><td>ViViT-L/16x2 320 Arnab et al., (2021)</td><td>JFT-300M</td><td>83.5</td><td>95.5</td><td>4 \u00d7 3</td><td>3992</td><td>310.8</td></tr><tr><td>ViViT-H/16x2 Arnab et al., (2021)</td><td>JFT-300M</td><td>84.8</td><td>95.8</td><td>4 \u00d7 3</td><td>8316</td><td>647.5</td></tr><tr><td>Swin-T</td><td>ImageNet-1K</td><td>78.8</td><td>93.6</td><td>4 \u00d7 3</td><td>88</td><td>28.2</td></tr><tr><td>Swin-S</td><td>ImageNet-1K</td><td>80.6</td><td>94.5</td><td>4 \u00d7 3</td><td>166</td><td>49.8</td></tr><tr><td>Swin-B</td><td>ImageNet-1K</td><td>80.6</td><td>94.6</td><td>4 \u00d7 3</td><td>282</td><td>88.1</td></tr><tr><td>Swin-B</td><td>ImageNet-21K</td><td>82.7</td><td>95.5</td><td>4 \u00d7 3</td><td>282</td><td>88.1</td></tr><tr><td>Swin-L</td><td>ImageNet-21K</td><td>83.1</td><td>95.9</td><td>4 \u00d7 3</td><td>604</td><td>197.0</td></tr><tr><td>Swin-L (384\\uparrow)</td><td>ImageNet-21K</td><td>84.6</td><td>96.5</td><td>4 \u00d7 3</td><td>2107</td><td>200.0</td></tr><tr><td>Swin-L (384\\uparrow)</td><td>ImageNet-21K</td><td>84.9</td><td>96.7</td><td>10 \u00d7 5</td><td>2107</td><td>200.0</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Comparison to state-of-the-art on Kinetics-400. \"384\\uparrow\" signifies that the model uses a larger spatial resolution of 384\\times384. \u201cViews\u201d indicates # temporal clip \\times # spatial crop. The magnitudes are Giga (10^{9}) and Mega (10^{6}) for FLOPs and Param respectively.", "list_citation_info": ["Bertasius et al., (2021) Bertasius, G., Wang, H., and Torresani, L. (2021). Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095.", "Arnab et al., (2021) Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lu\u010di\u0107, M., and Schmid, C. (2021). Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691.", "Tran et al., (2019) Tran, D., Wang, H., Torresani, L., and Feiszli, M. (2019). Video classification with channel-separated convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5552\u20135561.", "Wang et al., (2018) Wang, X., Girshick, R., Gupta, A., and He, K. (2018). Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794\u20137803.", "Feichtenhofer, (2020) Feichtenhofer, C. (2020). X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213.", "(6) Carreira, J. and Zisserman, A. (2017b). Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308.", "Neimark et al., (2021) Neimark, D., Bar, O., Zohar, M., and Asselmann, D. (2021). Video transformer network. arXiv preprint arXiv:2102.00719.", "Tran et al., (2018) Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., and Paluri, M. (2018). A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450\u20136459.", "Feichtenhofer et al., (2019) Feichtenhofer, C., Fan, H., Malik, J., and He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6202\u20136211.", "Wang et al., (2020) Wang, H., Tran, D., Torresani, L., and Feiszli, M. (2020). Video modeling with correlation networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 352\u2013361.", "(10) Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and Feichtenhofer, C. (2021b). Multiscale vision transformers. arXiv preprint arXiv:2104.11227."]}, {"table": "<table><tr><td> Method</td><td>Pretrain</td><td>Top-1</td><td>Top-5</td><td>Views</td><td>FLOPs</td><td>Param</td></tr><tr><td>SlowFast R101+NL Feichtenhofer et al., (2019)</td><td>-</td><td>81.8</td><td>95.1</td><td>10 \u00d7 3</td><td>234</td><td>59.9</td></tr><tr><td>X3D-XL Feichtenhofer, (2020)</td><td>-</td><td>81.9</td><td>95.5</td><td>10 \u00d7 3</td><td>48</td><td>11.0</td></tr><tr><td>MViT-B-24, 32\u00d73 Fan et al., 2021a </td><td>-</td><td>83.8</td><td>96.3</td><td>5 \u00d7 1</td><td>236</td><td>52.9</td></tr><tr><td>TimeSformer-HR Bertasius et al., (2021)</td><td>ImageNet-21K</td><td>82.4</td><td>96</td><td>1 \u00d7 3</td><td>1703</td><td>121.4</td></tr><tr><td>ViViT-L/16x2 320 Arnab et al., (2021)</td><td>ImageNet-21K</td><td>83.0</td><td>95.7</td><td>4 \u00d7 3</td><td>3992</td><td>310.8</td></tr><tr><td>ViViT-H/16x2 Fan et al., 2021a </td><td>JFT-300M</td><td>85.8</td><td>96.5</td><td>4 \u00d7 3</td><td>8316</td><td>647.5</td></tr><tr><td>Swin-B</td><td>ImageNet-21K</td><td>84.0</td><td>96.5</td><td>4 \u00d7 3</td><td>282</td><td>88.1</td></tr><tr><td>Swin-L (384\\uparrow)</td><td>ImageNet-21K</td><td>85.9</td><td>97.1</td><td>4 \u00d7 3</td><td>2107</td><td>200.0</td></tr><tr><td>Swin-L (384\\uparrow)</td><td>ImageNet-21K</td><td>86.1</td><td>97.3</td><td>10 \u00d7 5</td><td>2107</td><td>200.0</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 2: Comparison to state-of-the-art on Kinetics-600.", "list_citation_info": ["Bertasius et al., (2021) Bertasius, G., Wang, H., and Torresani, L. (2021). Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095.", "Arnab et al., (2021) Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lu\u010di\u0107, M., and Schmid, C. (2021). Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691.", "Feichtenhofer, (2020) Feichtenhofer, C. (2020). X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213.", "Feichtenhofer et al., (2019) Feichtenhofer, C., Fan, H., Malik, J., and He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6202\u20136211.", "(9) Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and Feichtenhofer, C. (2021a). Multiscale vision transformers. arXiv:2104.11227."]}, {"table": "<table><tr><td> Method</td><td>Pretrain</td><td>Top-1</td><td>Top-5</td><td>Views</td><td>FLOPs</td><td>Param</td></tr><tr><td>TimeSformer-HR Bertasius et al., (2021)</td><td>ImageNet-21K</td><td>62.5</td><td>-</td><td>1 \u00d7 3</td><td>1703</td><td>121.4</td></tr><tr><td>SlowFast R101, 8\u00d78 Feichtenhofer et al., (2019)</td><td>Kinetics-400</td><td>63.1</td><td>87.6</td><td>1 \u00d7 3</td><td>106</td><td>53.3</td></tr><tr><td>TSM-RGB Lin et al., (2019)</td><td>Kinetics-400</td><td>63.3</td><td>88.2</td><td>2 \u00d7 3</td><td>62</td><td>42.9</td></tr><tr><td>MSNet Kwon et al., (2020)</td><td>ImageNet-21K</td><td>64.7</td><td>89.4</td><td>1 \u00d7 1</td><td>67</td><td>24.6</td></tr><tr><td>TEA Li et al., (2020)</td><td>ImageNet-21K</td><td>65.1</td><td>89.9</td><td>10 \u00d7 3</td><td>70</td><td>-</td></tr><tr><td>blVNet Fan et al., (2019)</td><td>SSv2</td><td>65.2</td><td>90.3</td><td>1 \u00d7 1</td><td>129</td><td>40.2</td></tr><tr><td>ViViT-L/16x2 Arnab et al., (2021)</td><td>-</td><td>65.4</td><td>89.8</td><td>-</td><td>903</td><td>352.1</td></tr><tr><td>MViT-B, 64\u00d73 Fan et al., 2021b </td><td>Kinetics-400</td><td>67.7</td><td>90.9</td><td>1 \u00d7 3</td><td>455</td><td>36.6</td></tr><tr><td>MViT-B-24, 32\u00d73 Fan et al., 2021b </td><td>Kinetics-600</td><td>68.7</td><td>91.5</td><td>1 \u00d7 3</td><td>236</td><td>53.2</td></tr><tr><td>Swin-B</td><td>Kinetics-400</td><td>69.6</td><td>92.7</td><td>1 \u00d7 3</td><td>321</td><td>88.8</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 3: Comparison to state-of-the-art on Something-Something v2.", "list_citation_info": ["Li et al., (2020) Li, Y., Ji, B., Shi, X., Zhang, J., Kang, B., and Wang, L. (2020). Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 909\u2013918.", "Lin et al., (2019) Lin, J., Gan, C., and Han, S. (2019). Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093.", "Fan et al., (2019) Fan, Q., Chen, C.-F., Kuehne, H., Pistoia, M., and Cox, D. (2019). More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation. arXiv preprint arXiv:1912.00869.", "Bertasius et al., (2021) Bertasius, G., Wang, H., and Torresani, L. (2021). Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095.", "Arnab et al., (2021) Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lu\u010di\u0107, M., and Schmid, C. (2021). Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691.", "Feichtenhofer et al., (2019) Feichtenhofer, C., Fan, H., Malik, J., and He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6202\u20136211.", "Kwon et al., (2020) Kwon, H., Kim, M., Kwak, S., and Cho, M. (2020). Motionsqueeze: Neural motion feature learning for video understanding. In European Conference on Computer Vision, pages 345\u2013362. Springer.", "(10) Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., and Feichtenhofer, C. (2021b). Multiscale vision transformers. arXiv preprint arXiv:2104.11227."]}]}