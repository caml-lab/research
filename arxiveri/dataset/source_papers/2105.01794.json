{"title": "Real-time deep dynamic characters", "abstract": "We propose a deep videorealistic 3D human character model displaying highly realistic shape, motion, and dynamic appearance learned in a new weakly supervised way from multi-view imagery. In contrast to previous work, our controllable 3D character displays dynamics, e.g., the swing of the skirt, dependent on skeletal body motion in an efficient data-driven way, without requiring complex physics simulation. Our character model also features a learned dynamic texture model that accounts for photo-realistic motion-dependent appearance details, as well as view-dependent lighting effects. During training, we do not need to resort to difficult dynamic 3D capture of the human; instead we can train our model entirely from multi-view video in a weakly supervised manner. To this end, we propose a parametric and differentiable character representation which allows us to model coarse and fine dynamic deformations, e.g., garment wrinkles, as explicit space-time coherent mesh geometry that is augmented with high-quality dynamic textures dependent on motion and view point. As input to the model, only an arbitrary 3D skeleton motion is required, making it directly compatible with the established 3D animation pipeline. We use a novel graph convolutional network architecture to enable motion-dependent deformation learning of body and clothing, including dynamics, and a neural generative dynamic texture model creates corresponding dynamic texture maps. We show that by merely providing new skeletal motions, our model creates motion-dependent surface deformations, physically plausible dynamic clothing deformations, as well as video-realistic surface textures at a much higher level of detail than previous state of the art approaches, and even in real-time.", "authors": ["Marc Habermann", " Lingjie Liu", " Weipeng Xu", " Michael Zollhoefer", " Gerard Pons-Moll", " Christian Theobalt"], "pdf_url": "https://arxiv.org/abs/2105.01794", "list_table_and_caption": [{"table": "<table><thead><tr><th colspan=\"8\">Comparison to Previous Multi-view Based Methods</th></tr></thead><tbody><tr><th></th><td>Dyn. Geo.</td><td>Dyn. Tex.</td><td>View Dep. Effects</td><td>Controllable</td><td>Real-time</td><td>Unseen Motions</td><td>Loose Clothing</td></tr><tr><th>(Xu et al., 2011)</th><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2717</td></tr><tr><th>(Casaset al., 2014)</th><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2717</td></tr><tr><th>(Shysheya et al., 2019)</th><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2717</td></tr><tr><th>Ours</th><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody></table>", "caption": "Table 1. Conceptual comparison to previous multi-view based approaches for controllable character animation / synthesis (Xu et al., 2011; Casaset al., 2014; Shysheya et al., 2019).Note that all previous works fall short in multiple desirable categories while our proposed approach fulfills all these requirements.", "list_citation_info": ["Casas et al. (2014) Dan Casas, Marco Volino, John Collomosse, and Adrian Hilton. 2014. 4D Video Textures for Interactive Character Appearance. Comput. Graph. Forum 33, 2 (May 2014), 371\u2013380. https://doi.org/10.1111/cgf.12296", "Xu et al. (2011) Feng Xu, Yebin Liu, Carsten Stoll, James Tompkin, Gaurav Bharaj, Qionghai Dai, Hans-Peter Seidel, Jan Kautz, and Christian Theobalt. 2011. Video-based Characters: Creating New Human Performances from a Multi-view Video Database. In ACM SIGGRAPH 2011 Papers (Vancouver, British Columbia, Canada) (SIGGRAPH \u201911). ACM, New York, NY, USA, Article 32, 10 pages. https://doi.org/10.1145/1964921.1964927", "Shysheya et al. (2019) Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. 2019. Textured Neural Avatars. arXiv:1905.08776 [cs.CV]"]}, {"table": "<table><thead><tr><th colspan=\"2\">AMVIoU (in %) on S4 sequence</th></tr></thead><tbody><tr><th>Method</th><td>AMVIoU\\uparrow</td></tr><tr><th>MVBL (Habermann et al., 2020)</th><td>88.14</td></tr><tr><th>(Kavan et al., 2007)</th><td>79.45</td></tr><tr><th>Ours</th><td>90.70</td></tr><tr><th>Ours (Train)</th><td>94.07</td></tr></tbody></table>", "caption": "Table 2. Accuracy of the surface deformation.Note that we outperform the pure skinning based approach (Kavan et al., 2007) as they cannot account for dynamic cloth deformations.Our method further improves over MVBL even though this optimization based approach sees the multi-view test images.Finally, our approach performs similarly on training and testing data showing that the geometry networks generalize to unseen motions.", "list_citation_info": ["Kavan et al. (2007) Ladislav Kavan, Steven Collins, Ji\u0159\u00ed \u017d\u00e1ra, and Carol O\u2019Sullivan. 2007. Skinning with dual quaternions. In Proceedings of the 2007 symposium on Interactive 3D graphics and games. ACM, 39\u201346.", "Habermann et al. (2020) Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. 2020. DeepCap: Monocular Human Performance Capture Using Weak Supervision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)."]}]}