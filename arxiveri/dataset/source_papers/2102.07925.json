{"title": "Focal Inverse Distance Transform Maps for Crowd Localization", "abstract": "In this paper, we focus on the crowd localization task, a crucial topic of crowd analysis. Most regression-based methods utilize convolution neural networks (CNN) to regress a density map, which can not accurately locate the instance in the extremely dense scene, attributed to two crucial reasons: 1) the density map consists of a series of blurry Gaussian blobs, 2) severe overlaps exist in the dense region of the density map. To tackle this issue, we propose a novel Focal Inverse Distance Transform (FIDT) map for the crowd localization task. Compared with the density maps, the FIDT maps accurately describe the persons' locations without overlapping in dense regions. Based on the FIDT maps, a Local-Maxima-Detection-Strategy (LMDS) is derived to effectively extract the center point for each individual. Furthermore, we introduce an Independent SSIM (I-SSIM) loss to make the model tend to learn the local structural information, better recognizing local maxima. Extensive experiments demonstrate that the proposed method reports state-of-the-art localization performance on six crowd datasets and one vehicle dataset. Additionally, we find that the proposed method shows superior robustness on the negative and extremely dense scenes, which further verifies the effectiveness of the FIDT maps. The code and model will be available at https://github.com/dk-liang/FIDTM.", "authors": ["Dingkang Liang", " Wei Xu", " Yingying Zhu", " Yu Zhou"], "pdf_url": "https://arxiv.org/abs/2102.07925", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">TrainingLabels</td><td colspan=\"6\">Validation set</td><td colspan=\"6\">Test set</td></tr><tr><td colspan=\"3\">\\sigma_{l}</td><td colspan=\"3\">\\sigma_{s}</td><td colspan=\"3\">\\sigma_{l}</td><td colspan=\"3\">\\sigma_{s}</td></tr><tr><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td></tr><tr><td>Faster RCNN [38]</td><td>Box</td><td>7.3</td><td>96.4</td><td>3.8</td><td>6.8</td><td>90.0</td><td>3.5</td><td>6.7</td><td>95.8</td><td>3.5</td><td>6.3</td><td>89.4</td><td>3.3</td></tr><tr><td>TinyFaces [13]</td><td>Box</td><td>59.8</td><td>54.3</td><td>66.6</td><td>55.3</td><td>50.2</td><td>61.7</td><td>56.7</td><td>52.9</td><td>61.1</td><td>52.6</td><td>49.1</td><td>56.6</td></tr><tr><td>TopoCount [1]</td><td>Box</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>69.1</td><td>69.5</td><td>68.7</td><td>60.1</td><td>60.5</td><td>59.8</td></tr><tr><td>VGG+GPR [6]</td><td>Point</td><td>56.3</td><td>61.0</td><td>52.2</td><td>46.0</td><td>49.9</td><td>42.7</td><td>52.5</td><td>55.8</td><td>49.6</td><td>42.6</td><td>45.3</td><td>40.2</td></tr><tr><td>RAZ_Loc [25]</td><td>Point</td><td>62.5</td><td>69.2</td><td>56.9</td><td>54.5</td><td>60.5</td><td>49.6</td><td>59.8</td><td>66.6</td><td>54.3</td><td>51.7</td><td>57.6</td><td>47.0</td></tr><tr><td>Crowd-SDNet [53]</td><td>Point</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>63.7</td><td>65.1</td><td>62.4</td><td>-</td><td>-</td><td>-</td></tr><tr><td>AutoScale{}^{*} [57]</td><td>Point</td><td>66.8</td><td>70.1</td><td>63.8</td><td>60.0</td><td>62.9</td><td>57.3</td><td>62.0</td><td>67.3</td><td>57.4</td><td>54.4</td><td>59.1</td><td>50.4</td></tr><tr><td>GL [47]</td><td>Point</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>66.0</td><td>80.0</td><td>56.2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SCALNet [54]</td><td>Point</td><td>72.4</td><td>73.5</td><td>71.4</td><td>66.9</td><td>67.9</td><td>65.9</td><td>69.1</td><td>69.2</td><td>63.6</td><td>63.6</td><td>63.7</td><td>63.6</td></tr><tr><td>Ours</td><td>Point</td><td>78.9</td><td>82.2</td><td>75.9</td><td>73.7</td><td>76.7</td><td>70.9</td><td>75.5</td><td>79.7</td><td>71.7</td><td>70.5</td><td>74.4</td><td>66.9</td></tr></table>", "caption": "TABLE I: Quantitative comparison of the localization performance on the NWPU-Crowd dataset. The results of other methods are from the online benchmark website [51]. F, P, and R refer to the F-measure, precision and recall, respectively.", "list_citation_info": ["[47] Jia Wan, Ziquan Liu, and Antoni B Chan. A generalized loss function for crowd counting and localization. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1974\u20131983, 2021.", "[53] Yi Wang, Junhui Hou, Xinyu Hou, and Lap-Pui Chau. A self-training approach for point-supervised object detection and counting in crowds. IEEE Transactions on Image Processing, 30:2876\u20132887, 2021.", "[54] Yi Wang, Xinyu Hou, and Lap-Pui Chau. Dense point prediction: A simple baseline for crowd counting and localization. In 2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pages 1\u20136. IEEE, 2021.", "[38] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proc. of Advances in Neural Information Processing Systems, volume 28, pages 91\u201399, 2015.", "[25] Chenchen Liu, Xinyu Weng, and Yadong Mu. Recurrent attentive zooming for joint crowd counting and precise localization. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1217\u20131226, 2019.", "[57] Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka. Autoscale: Learning to scale for crowd counting. International Journal of Computer Vision, pages 1\u201330, 2022.", "[13] Peiyun Hu and Deva Ramanan. Finding tiny faces. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 951\u2013959, 2017.", "[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021.", "[6] Junyu Gao, Tao Han, Qi Wang, and Yuan Yuan. Domain-adaptive crowd counting via inter-domain features segregation and gaussian-prior reconstruction. arXiv preprint arXiv:1912.03677, 2019.", "[51] Qi Wang, Junyu Gao, Wei Lin, and Xuelong Li. Nwpu-crowd: A large-scale benchmark for crowd counting and localization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(6):2141\u20132149, 2020."]}, {"table": "<table><tr><td>Method</td><td>Av.Precision</td><td>Av.Recall</td><td>F-measure</td></tr><tr><td>MCNN [66]</td><td>59.93%</td><td>63.50%</td><td>61.66%</td></tr><tr><td>CL [16]</td><td>75.80%</td><td>59.75%</td><td>66.82%</td></tr><tr><td>LCFCN [20]</td><td>77.89%</td><td>52.40%</td><td>62.65%</td></tr><tr><td>Method in [39]</td><td>75.46%</td><td>49.87%</td><td>60.05%</td></tr><tr><td>LSC-CNN[41]</td><td>74.62%</td><td>73.50%</td><td>74.06%</td></tr><tr><td>GL [47]</td><td>78.20%</td><td>74.80%</td><td>76.30%</td></tr><tr><td>TopoCount[1]</td><td>81.77%</td><td>78.96%</td><td>80.34%</td></tr><tr><td>Ours</td><td>84.49%</td><td>80.10%</td><td>82.23%</td></tr></table>", "caption": "TABLE II: Quantitative evaluation of localization-based methods on the UCF-QNRF dataset. We report the average precision, average Recall, and average F-measure at different distance thresholds (1, 2, 3, \u2026, 100).", "list_citation_info": ["[41] Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2739\u20132751, 2020.", "[47] Jia Wan, Ziquan Liu, and Antoni B Chan. A generalized loss function for crowd counting and localization. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1974\u20131983, 2021.", "[39] Javier Ribera, David G\u00fcera, Yuhao Chen, and Edward J. Delp. Locating objects without bounding boxes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 6479\u20136489, 2019.", "[20] Issam H Laradji, Negar Rostamzadeh, Pedro O Pinheiro, David Vazquez, and Mark Schmidt. Where are the blobs: Counting by localization with point supervision. In Proc. of European Conference on Computer Vision, pages 547\u2013562, 2018.", "[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 589\u2013597, 2016.", "[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021.", "[16] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map estimation and localization in dense crowds. In Proc. of European Conference on Computer Vision, pages 532\u2013546, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">\\sigma = 4</td><td colspan=\"3\">\\sigma = 8</td></tr><tr><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td></tr><tr><td>TopoCount [1]</td><td>31.5%</td><td>28.8%</td><td>30.1%</td><td>63.6%</td><td>58.3%</td><td>60.8%</td></tr><tr><td>Ours</td><td>38.9%</td><td>38.7%</td><td>38.8%</td><td>62.5%</td><td>62.4%</td><td>62.4%</td></tr></table>", "caption": "TABLE III: Quantitative evaluation of localization-based methods on the JHU-Crowd++ dataset using Precision (P), Recall (R), and F-measure (F).", "list_citation_info": ["[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021."]}, {"table": "<table><tr><td rowspan=\"3\">Method</td><td colspan=\"6\">Part A</td><td colspan=\"6\">Part B</td></tr><tr><td colspan=\"3\">\\sigma = 4</td><td colspan=\"3\">\\sigma = 8</td><td colspan=\"3\">\\sigma = 4</td><td colspan=\"3\">\\sigma = 8</td></tr><tr><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td></tr><tr><td>LCFCN[20]</td><td>43.3%</td><td>26.0%</td><td>32.5%</td><td>75.1%</td><td>45.1%</td><td>56.3%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Method in [39]</td><td>34.9%</td><td>20.7%</td><td>25.9%</td><td>67.7%</td><td>44.8%</td><td>53.9%</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LSC-CNN [41]</td><td>33.4%</td><td>31.9%</td><td>32.6%</td><td>63.9%</td><td>61.0%</td><td>62.4%</td><td>29.7%</td><td>29.2%</td><td>29.5%</td><td>57.5%</td><td>56.7%</td><td>57.0%</td></tr><tr><td>TopoCount [1]</td><td>41.7%</td><td>40.6%</td><td>41.1%</td><td>74.6%</td><td>72.7%</td><td>73.6%</td><td>63.4%</td><td>63.1%</td><td>63.2%</td><td>82.3%</td><td>81.8%</td><td>82.0%</td></tr><tr><td>Ours</td><td>59.1%</td><td>58.2%</td><td>58.6%</td><td>78.2%</td><td>77.0%</td><td>77.6%</td><td>64.9%</td><td>64.5%</td><td>64.7%</td><td>83.9%</td><td>83.2%</td><td>83.5%</td></tr></table>", "caption": "TABLE IV: Comparison of the localization performance on the ShanghaiTech Part A [66] and ShanghaiTech Part B [66] datasets using Precision (P), Recall (R), and F-measure (F).", "list_citation_info": ["[41] Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2739\u20132751, 2020.", "[39] Javier Ribera, David G\u00fcera, Yuhao Chen, and Edward J. Delp. Locating objects without bounding boxes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 6479\u20136489, 2019.", "[20] Issam H Laradji, Negar Rostamzadeh, Pedro O Pinheiro, David Vazquez, and Mark Schmidt. Where are the blobs: Counting by localization with point supervision. In Proc. of European Conference on Computer Vision, pages 547\u2013562, 2018.", "[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 589\u2013597, 2016.", "[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">\\sigma = 4</td><td colspan=\"3\">\\sigma = 8</td></tr><tr><td>P (%)</td><td>R (%)</td><td>F (%)</td><td>P (%)</td><td>R (%)</td><td>F (%)</td></tr><tr><td>LSC-CNN\u2020 [41]</td><td>37.7%</td><td>39.5%</td><td>38.6%</td><td>57.8%</td><td>61.1%</td><td>59.4%</td></tr><tr><td>AutoScale\u2020 [57]</td><td>37.8%</td><td>40.5%</td><td>39.1%</td><td>59.0%</td><td>62.3%</td><td>60.6%</td></tr><tr><td>TopoCount\u2020 [1]</td><td>39.5%</td><td>42.0%</td><td>40.7%</td><td>62.5%</td><td>66.9%</td><td>64.6%</td></tr><tr><td>Ours</td><td>46.5%</td><td>49.0%</td><td>47.7%</td><td>67.0%</td><td>70.6%</td><td>68.7%</td></tr></table>", "caption": "TABLE V: Quantitative evaluation of localization-based methods on the UCF_CC_50 dataset using Precision (P), Recall (R), and F-measure (F). {\\dagger} represents that the networks are trained by ourselves.", "list_citation_info": ["[41] Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2739\u20132751, 2020.", "[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021.", "[57] Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka. Autoscale: Learning to scale for crowd counting. International Journal of Computer Vision, pages 1\u201330, 2022."]}, {"table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">OutputPositionInformation</td><td colspan=\"2\">validation set</td><td colspan=\"4\">Test set</td></tr><tr><td colspan=\"2\">Overall</td><td colspan=\"2\">Overall</td><td colspan=\"2\">Scene Level (only MAE)</td></tr><tr><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>Avg.</td><td>S0\\sim S4</td></tr><tr><td>C3F-VGG [8]</td><td>\u2717</td><td>105.79</td><td>504.39</td><td>127.0</td><td>439.6</td><td>666.9</td><td>140.9/26.5/58.0/307.1/2801.8</td></tr><tr><td>CSRNet [23]</td><td>\u2717</td><td>104.89</td><td>433.48</td><td>121.3</td><td>387.8</td><td>522.7</td><td>176.0/35.8/59.8/285.8/2055.8</td></tr><tr><td>CAN [30]</td><td>\u2717</td><td>93.58</td><td>489.90</td><td>106.3</td><td>386.5</td><td>612.2</td><td>82.6/14.7/46.6/269.7/2647.0</td></tr><tr><td>SCAR [9]</td><td>\u2717</td><td>81.57</td><td>397.92</td><td>110.0</td><td>495.3</td><td>718.3</td><td>122.9/16.7/46.0/241.7/3164.3</td></tr><tr><td>BL [33]</td><td>\u2717</td><td>93.64</td><td>470.38</td><td>105.4</td><td>454.2</td><td>750.5</td><td>66.5/8.7/41.2/249.9/3386.4</td></tr><tr><td>SFCN [52]</td><td>\u2717</td><td>95.46</td><td>608.32</td><td>105.7</td><td>424.1</td><td>712.7</td><td>54.2/14.8/44.4/249.6/3200.5</td></tr><tr><td>KDMG [48]</td><td>\u2717</td><td>-</td><td>-</td><td>100.5</td><td>415.5</td><td>632.7</td><td>77.3/10.3/38.5/259.4/2777.9</td></tr><tr><td>NoisyCC [46]</td><td>\u2717</td><td>-</td><td>-</td><td>96.9</td><td>534.2</td><td>608.1</td><td>218.7/10.7/35.2/203.2/2572.8</td></tr><tr><td>DM-Count  [49]</td><td>\u2717</td><td>70.5</td><td>357.6</td><td>88.4</td><td>388.6</td><td>498.0</td><td>146.6/7.6/31.2/228.7/2075.8</td></tr><tr><td>RAZ_loc* [25]</td><td>\u2714</td><td>128.7</td><td>665.4</td><td>151.4</td><td>634.6</td><td>1166.0</td><td>60.6/17.1/48.3/364.7/5339.0</td></tr><tr><td>AutoScale* [57]</td><td>\u2714</td><td>97.3</td><td>571.2</td><td>123.9</td><td>515.5</td><td>871.0</td><td>42.3/18.8/46.1/301.7/3947.0</td></tr><tr><td>TopoCount* [1]</td><td>\u2714</td><td>-</td><td>-</td><td>107.8</td><td>438.5</td><td>-</td><td>-</td></tr><tr><td>SCALNet* [54]</td><td>\u2714</td><td>64.4</td><td>251.1</td><td>86.8</td><td>339.9</td><td>429.5</td><td>92.0/11.2/41.1/227.7/1775.3</td></tr><tr><td>Ours*</td><td>\u2714</td><td>51.4</td><td>107.6</td><td>86.0</td><td>312.5</td><td>390.6</td><td>21.6/13.7/55.6/217.1/1645.4</td></tr></table>", "caption": "TABLE VI: Comparison of the counting performance on the NWPU-Crowd. S0\\!\\sim\\!S4 respectively indicate five categories according to the different number range: 0, (0,100], (100,500], (500,5000], \\textgreater 5000. * means the localization-based methods, which can provide the position information.", "list_citation_info": ["[23] Yuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1091\u20131100, 2018.", "[49] Boyu Wang, Huidong Liu, Dimitris Samaras, and Minh Hoai. Distribution matching for crowd counting. In Proc. of Advances in Neural Information Processing Systems, 2020.", "[54] Yi Wang, Xinyu Hou, and Lap-Pui Chau. Dense point prediction: A simple baseline for crowd counting and localization. In 2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pages 1\u20136. IEEE, 2021.", "[46] Jia Wan and Antoni Chan. Modeling noisy annotations for crowd counting. In Proc. of Advances in Neural Information Processing Systems, pages 3386\u20133396, 2020.", "[9] Junyu Gao, Qi Wang, and Yuan Yuan. Scar: Spatial-/channel-wise attention regression networks for crowd counting. Neurocomputing, 363:1\u20138, 2019.", "[25] Chenchen Liu, Xinyu Weng, and Yadong Mu. Recurrent attentive zooming for joint crowd counting and precise localization. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1217\u20131226, 2019.", "[57] Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka. Autoscale: Learning to scale for crowd counting. International Journal of Computer Vision, pages 1\u201330, 2022.", "[33] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 6142\u20136151, 2019.", "[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021.", "[52] Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learning from synthetic data for crowd counting in the wild. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 8198\u20138207, 2019.", "[48] Jia Wan, Qingzhong Wang, and Antoni B Chan. Kernel-based density map generation for dense object counting. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20131, 2020.", "[8] Junyu Gao, Wei Lin, Bin Zhao, Dong Wang, Chenyu Gao, and Jun Wen. C^ 3 framework: An open-source pytorch code for crowd counting. arXiv preprint arXiv:1907.02724, 2019.", "[30] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 5099\u20135108, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">OutputPositionInformation</td><td colspan=\"2\">JHU</td><td colspan=\"2\">QNRF</td><td colspan=\"2\">Part A</td><td colspan=\"2\">Part B</td><td colspan=\"2\">UCF_CC_50</td></tr><tr><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>CSRNet [23]</td><td>\u2717</td><td>85.9</td><td>309.2</td><td>-</td><td>-</td><td>68.2</td><td>115.0</td><td>10.6</td><td>16.0</td><td>266.1</td><td>397.5</td></tr><tr><td>SFCN [52]</td><td>\u2717</td><td>77.5</td><td>297.6</td><td>102.0</td><td>171.4</td><td>64.8</td><td>107.5</td><td>7.6</td><td>13.0</td><td>214.2</td><td>318.2</td></tr><tr><td>L2SM [58]</td><td>\u2717</td><td>-</td><td>-</td><td>104.7</td><td>173.6</td><td>64.2</td><td>98.4</td><td>7.2</td><td>11.1</td><td>188.4</td><td>315.3</td></tr><tr><td>CG-DRCN [44]</td><td>\u2717</td><td>82.3</td><td>328.0</td><td>112.2</td><td>176.3</td><td>64.0</td><td>98.4</td><td>8.5</td><td>14.4</td><td>-</td><td>-</td></tr><tr><td>MUD-iKNN [36]</td><td>\u2717</td><td>-</td><td>-</td><td>104.0</td><td>172.0</td><td>68.0</td><td>117.7</td><td>13.4</td><td>21.4</td><td>237.7</td><td>305.7</td></tr><tr><td>DSSI-Net [27]</td><td>\u2717</td><td>133.5</td><td>416.5</td><td>99.1</td><td>159.2</td><td>60.6</td><td>96.0</td><td>6.9</td><td>10.3</td><td>216.9</td><td>302.4</td></tr><tr><td>MBTTBF [43]</td><td>\u2717</td><td>81.8</td><td>299.1</td><td>97.5</td><td>165.2</td><td>60.2</td><td>94.1</td><td>8.0</td><td>15.5</td><td>233.1</td><td>300.9</td></tr><tr><td>BL [33]</td><td>\u2717</td><td>75.0</td><td>299.9</td><td>88.7</td><td>154.8</td><td>62.8</td><td>101.8</td><td>7.7</td><td>12.7</td><td>229.3</td><td>308.2</td></tr><tr><td>RPNet [60]</td><td>\u2717</td><td>-</td><td>-</td><td>-</td><td>-</td><td>61.2</td><td>96.9</td><td>8.1</td><td>11.6</td><td>-</td><td>-</td></tr><tr><td>ASNet [18]</td><td>\u2717</td><td>-</td><td>-</td><td>91.6</td><td>159.7</td><td>57.8</td><td>90.1</td><td>-</td><td>-</td><td>174.8</td><td>251.6</td></tr><tr><td>AMSNet [14]</td><td>\u2717</td><td>-</td><td>-</td><td>101.8</td><td>163.2</td><td>56.7</td><td>93.4</td><td>6.7</td><td>10.2</td><td>208.6</td><td>296.3</td></tr><tr><td>LibraNet [26]</td><td>\u2717</td><td>-</td><td>-</td><td>88.1</td><td>143.7</td><td>55.9</td><td>97.1</td><td>7.3</td><td>11.3</td><td>181.2</td><td>262.2</td></tr><tr><td>KDMG [48]</td><td>\u2717</td><td>69.7</td><td>268.3</td><td>99.5</td><td>173.0</td><td>63.8</td><td>99.2</td><td>7.8</td><td>12.7</td><td>-</td><td>-</td></tr><tr><td>NoisyCC [46]</td><td>\u2717</td><td>67.7</td><td>258.5</td><td>85.8</td><td>150.6</td><td>61.9</td><td>99.6</td><td>7.4</td><td>11.3</td><td>-</td><td>-</td></tr><tr><td>DM-Count  [49]</td><td>\u2717</td><td>-</td><td>-</td><td>85.6</td><td>148.3</td><td>59.7</td><td>95.7</td><td>7.4</td><td>11.8</td><td>211.0</td><td>291.5</td></tr><tr><td>RAZ_loc+* [25]</td><td>\u2714</td><td>-</td><td>-</td><td>118.0</td><td>198.0</td><td>71.6</td><td>120.1</td><td>9.9</td><td>15.6</td><td>-</td><td>-</td></tr><tr><td>PSDDN* [32]</td><td>\u2714</td><td>-</td><td>-</td><td>-</td><td>-</td><td>65.9</td><td>112.3</td><td>9.1</td><td>14.2</td><td>359.4</td><td>514.8</td></tr><tr><td>LSC-CNN* [41]</td><td>\u2714</td><td>112.7</td><td>454.4</td><td>120.5</td><td>218.2</td><td>66.4</td><td>117.0</td><td>8.1</td><td>12.7</td><td>225.6</td><td>302.7</td></tr><tr><td>Crowd-SDNet* [53]</td><td>\u2714</td><td>-</td><td>-</td><td>-</td><td>-</td><td>65.1</td><td>104.4</td><td>7.8</td><td>12.6</td><td>-</td><td>-</td></tr><tr><td>AutoScale* [57]</td><td>\u2714</td><td>85.6</td><td>356.1</td><td>104.4</td><td>174.2</td><td>65.8</td><td>112.1</td><td>8.6</td><td>13.9</td><td>210.5</td><td>287.4</td></tr><tr><td>TopoCount* [1]</td><td>\u2714</td><td>60.9</td><td>267.4</td><td>89.0</td><td>159.0</td><td>61.2</td><td>104.6</td><td>7.8</td><td>13.7</td><td>184.1</td><td>258.3</td></tr><tr><td>Ours*</td><td>\u2714</td><td>66.6</td><td>253.6</td><td>89.0</td><td>153.5</td><td>57.0</td><td>103.4</td><td>6.9</td><td> 11.8</td><td>171.4</td><td>233.1</td></tr></table>", "caption": "TABLE VII: Comparison of the counting performance on the JHU-Crowd++, UCF-QNRF, ShanghaiTech Part A, Part B and UCF_CC_50 datasets. * means the localization-based methods, which can provide the position information.", "list_citation_info": ["[27] Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, and Liang Lin. Crowd counting with deep structured scale integration network. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 1774\u20131783, 2019.", "[58] Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, and Xiang Bai. Learn to scale: Generating multipolar normalized density maps for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 8382\u20138390, 2019.", "[46] Jia Wan and Antoni Chan. Modeling noisy annotations for crowd counting. In Proc. of Advances in Neural Information Processing Systems, pages 3386\u20133396, 2020.", "[25] Chenchen Liu, Xinyu Weng, and Yadong Mu. Recurrent attentive zooming for joint crowd counting and precise localization. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1217\u20131226, 2019.", "[36] Greg Olmschenk, Hao Tang, and Zhigang Zhu. Improving dense crowd counting convolutional neural networks using inverse k-nearest neighbor maps and multiscale upsampling. In 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, volume 5, 2020.", "[32] Yuting Liu, Miaojing Shi, Qijun Zhao, and Xiaofang Wang. Point in, box out: Beyond counting persons in crowds. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 6469\u20136478, 2019.", "[57] Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka. Autoscale: Learning to scale for crowd counting. International Journal of Computer Vision, pages 1\u201330, 2022.", "[33] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 6142\u20136151, 2019.", "[60] Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, and Nicu Sebe. Reverse perspective network for perspective-aware object counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 4374\u20134383, 2020.", "[26] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua Shen. Weighing counts: Sequential crowd counting by reinforcement learning. In Proc. of European Conference on Computer Vision, pages 164\u2013181, 2020.", "[23] Yuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1091\u20131100, 2018.", "[18] Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, and Yanwei Pang. Attention scaling for crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 4706\u20134715, 2020.", "[48] Jia Wan, Qingzhong Wang, and Antoni B Chan. Kernel-based density map generation for dense object counting. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20131, 2020.", "[44] Vishwanath A Sindagi, Rajeev Yasarla, and Vishal M Patel. Pushing the frontiers of unconstrained crowd counting: New dataset and benchmark method. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 1221\u20131231, 2019.", "[41] Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2739\u20132751, 2020.", "[53] Yi Wang, Junhui Hou, Xinyu Hou, and Lap-Pui Chau. A self-training approach for point-supervised object detection and counting in crowds. IEEE Transactions on Image Processing, 30:2876\u20132887, 2021.", "[43] Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom feature fusion for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 1002\u20131012, 2019.", "[52] Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learning from synthetic data for crowd counting in the wild. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 8198\u20138207, 2019.", "[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021.", "[49] Boyu Wang, Huidong Liu, Dimitris Samaras, and Minh Hoai. Distribution matching for crowd counting. In Proc. of Advances in Neural Information Processing Systems, 2020.", "[14] Yutao Hu, Xiaolong Jiang, Xuhui Liu, Baochang Zhang, Jungong Han, Xianbin Cao, and David Doermann. Nas-count: Counting-by-density with neural architecture search. In Proc. of European Conference on Computer Vision, pages 747\u2013766, 2020."]}, {"table": "<table><tr><td>Method</td><td>GAME(0)</td><td>GAME(1)</td><td>GAME(2)</td><td>GAME(3)</td></tr><tr><td>PSDDN [32]</td><td>4.79</td><td>5.43</td><td>6.68</td><td>8.40</td></tr><tr><td>LSC-CNN [41]</td><td>4.60</td><td>5.40</td><td>6.90</td><td>8.30</td></tr><tr><td>AutoScale [57]</td><td>2.88</td><td>4.97</td><td>6.64</td><td>9.73</td></tr><tr><td>Crowd-SDNet\u2020 [53]</td><td>3.82</td><td>5.27</td><td>7.72</td><td>10.11</td></tr><tr><td>TopoCount\u2020 [1]</td><td>3.42</td><td>4.76</td><td>6.51</td><td>8.55</td></tr><tr><td>Ours</td><td>2.25</td><td>3.91</td><td>5.66</td><td>8.36</td></tr></table>", "caption": "TABLE VIII: Quantitative comparison of vehicles counting on the TRANCOS [10] dataset. {\\dagger} represents that the networks are trained by ourselves.", "list_citation_info": ["[41] Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2739\u20132751, 2020.", "[53] Yi Wang, Junhui Hou, Xinyu Hou, and Lap-Pui Chau. A self-training approach for point-supervised object detection and counting in crowds. IEEE Transactions on Image Processing, 30:2876\u20132887, 2021.", "[57] Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka. Autoscale: Learning to scale for crowd counting. International Journal of Computer Vision, pages 1\u201330, 2022.", "[32] Yuting Liu, Miaojing Shi, Qijun Zhao, and Xiaofang Wang. Point in, box out: Beyond counting persons in crowds. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 6469\u20136478, 2019.", "[10] Ricardo Guerrero-G\u00f3mez-Olmedo, Beatriz Torre-Jim\u00e9nez, Roberto L\u00f3pez-Sastre, Saturnino Maldonado-Basc\u00f3n, and Daniel Onoro-Rubio. Extremely overlapping vehicle counting. In IbPRIA, pages 423\u2013431, 2015.", "[1] Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proc. of the AAAI Conf. on Artificial Intelligence, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">S0-level</td><td colspan=\"2\">S4-level</td></tr><tr><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>KDMG [48]</td><td>77.3</td><td>303.0</td><td>2777.9</td><td>3521.8</td></tr><tr><td>DM-Count [49]</td><td>146.7</td><td>736.1</td><td>2075.8</td><td>2895.2</td></tr><tr><td>NoisyCC [46]</td><td>218.7</td><td>1415.6</td><td>2572.5</td><td>3414.9</td></tr><tr><td>SCALNet [54]</td><td>92.0</td><td>479.3</td><td>1775.3</td><td>2676.4</td></tr><tr><td>Ours</td><td>21.6</td><td>129.3</td><td>1645.4</td><td>2288.2</td></tr></table>", "caption": "TABLE XIII: The results of S0 and S4 on NWPU-Crowd test set.", "list_citation_info": ["[49] Boyu Wang, Huidong Liu, Dimitris Samaras, and Minh Hoai. Distribution matching for crowd counting. In Proc. of Advances in Neural Information Processing Systems, 2020.", "[48] Jia Wan, Qingzhong Wang, and Antoni B Chan. Kernel-based density map generation for dense object counting. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u20131, 2020.", "[46] Jia Wan and Antoni Chan. Modeling noisy annotations for crowd counting. In Proc. of Advances in Neural Information Processing Systems, pages 3386\u20133396, 2020.", "[54] Yi Wang, Xinyu Hou, and Lap-Pui Chau. Dense point prediction: A simple baseline for crowd counting and localization. In 2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pages 1\u20136. IEEE, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Density map(counting by integration)</td><td colspan=\"5\">FIDT map(counting by localization)</td></tr><tr><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>P(%)</td><td>R(%)</td><td>F(%)</td></tr><tr><td>CSRNET [23]</td><td>104.9</td><td>433.5</td><td>100.6</td><td>464.3</td><td>68.8%</td><td>66.2%</td><td>67.5%</td></tr><tr><td>CSRNET [23] + FPN</td><td>95.5</td><td>450.8</td><td>70.6</td><td>369.7</td><td>73.9%</td><td>69.8%</td><td>71.8%</td></tr><tr><td>BL [33]</td><td>93.6</td><td>470.4</td><td>89.7</td><td>446.6</td><td>69.9%</td><td>68.9%</td><td>69.4%</td></tr><tr><td>BL [33] + FPN</td><td>85.4</td><td>412.9</td><td>65.7</td><td>259.2</td><td>77.8%</td><td>70.0%</td><td>73.7%</td></tr></table>", "caption": "TABLE XIV: The results of different regressors on the NWPU-Crowd [51] (validation set) dataset.", "list_citation_info": ["[33] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 6142\u20136151, 2019.", "[51] Qi Wang, Junyu Gao, Wei Lin, and Xuelong Li. Nwpu-crowd: A large-scale benchmark for crowd counting and localization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(6):2141\u20132149, 2020.", "[23] Yuhong Li, Xiaofan Zhang, and Deming Chen. Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1091\u20131100, 2018."]}, {"table": "<table><tr><td>Method</td><td>MACs (G)</td><td>Inference speed</td><td>F-measure</td></tr><tr><td>LSC-CNN [41]</td><td>1244.3</td><td>2.6 FPS</td><td>-</td></tr><tr><td>AutoScale [57]</td><td>1074.6</td><td>5.7 FPS</td><td>62.0%</td></tr><tr><td>Crowd-SDNet<sup>1</sup><sup>1</sup>1We try our best to calculate the MACs of Crowd-SDNet, but the official code relies on the old version Keras, which is hard to obtain the MACs. [53]</td><td>-</td><td>0.8 FPS</td><td>63.7%</td></tr><tr><td>GL [47]</td><td>324.6</td><td>20.3 FPS</td><td>66.0%</td></tr><tr><td>TopoCount [49]</td><td>797.2</td><td>9.4 FPS</td><td>69.1%</td></tr><tr><td>Ours</td><td>426.7</td><td>7.1 FPS</td><td>75.5%</td></tr></table>", "caption": "TABLE XV: The comparisons of complexity. The F-measure is from the NWPU-Crowd benchmark (test set).", "list_citation_info": ["[41] Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 2739\u20132751, 2020.", "[47] Jia Wan, Ziquan Liu, and Antoni B Chan. A generalized loss function for crowd counting and localization. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 1974\u20131983, 2021.", "[53] Yi Wang, Junhui Hou, Xinyu Hou, and Lap-Pui Chau. A self-training approach for point-supervised object detection and counting in crowds. IEEE Transactions on Image Processing, 30:2876\u20132887, 2021.", "[49] Boyu Wang, Huidong Liu, Dimitris Samaras, and Minh Hoai. Distribution matching for crowd counting. In Proc. of Advances in Neural Information Processing Systems, 2020.", "[57] Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka. Autoscale: Learning to scale for crowd counting. International Journal of Computer Vision, pages 1\u201330, 2022."]}]}