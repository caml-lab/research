{"title": "Monocular Multi-Layer Layout Estimation for Warehouse Racks", "abstract": "Given a monocular colour image of a warehouse rack, we aim to predict the bird's-eye view layout for each shelf in the rack, which we term as multi-layer layout prediction. To this end, we present RackLay, a deep neural network for real-time shelf layout estimation from a single image. Unlike previous layout estimation methods, which provide a single layout for the dominant ground plane alone, RackLay estimates the top-view and front-view layout for each shelf in the considered rack populated with objects. RackLay's architecture and its variants are versatile and estimate accurate layouts for diverse scenes characterized by varying number of visible shelves in an image, large range in shelf occupancy factor and varied background clutter. Given the extreme paucity of datasets in this space and the difficulty involved in acquiring real data from warehouses, we additionally release a flexible synthetic dataset generation pipeline WareSynth which allows users to control the generation process and tailor the dataset according to contingent application. The ablations across architectural variants and comparison with strong prior baselines vindicate the efficacy of RackLay as an apt architecture for the novel problem of multi-layered layout estimation. We also show that fusing the top-view and front-view enables 3D reasoning applications such as metric free space estimation for the considered rack.", "authors": ["Meher Shashwat Nigam", " Avinash Prabhu", " Anurag Sahu", " Puru Gupta", " Tanvi Karandikar", " N. Sai Shankar", " Ravi Kiran Sarvadevabhatla", " K. Madhava Krishna"], "pdf_url": "https://arxiv.org/abs/2103.09174", "list_table_and_caption": [{"table": "<p>Top ViewFront ViewRackBoxRackBoxMethodmIoUmAPmIoUmAPmIoUmAPmIoUmAPRackLay-D-disc93.1598.7395.0797.9090.7598.5494.2997.95RackLay-D95.0398.3792.9497.6395.2198.4895.1797.94RackLay-S-disc92.3498.2893.7197.8591.9698.1392.6597.51RackLay-S93.0298.6194.6198.0794.3098.0992.1197.56PseudoLidar-PointRCNN(Shi et al., 2019)73.2877.4055.7781.26--63.0589.45MaskRCNN-GTdepth(Heet al., 2017)36.4842.4835.5747.44----</p>", "caption": "Table 1. Quantitative results: We benchmark the 4 different versions of our network- RackLay-S, RackLay-S-disc, RackLay-D and RackLay-D-disc, along with two baselines- PseudoLidar-PointRCNN(Wang et al., 2019a; Shi et al., 2019) and MaskRCNN-GTdepth(Heet al., 2017) (as described in Sec. 5.2). Note that RackLay-S and RackLay-S-disc are single decoder models and hence cannot predict top view and front view simultaneously. The top view and front view results displayed for each of these two models were trained separately (scaled results out of 100).", "list_citation_info": ["He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. 2017. MaskRCNN. In Proceedings of the IEEE international conference on computer vision. 2961\u20132969.", "Shi et al. (2019) Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. 2019. Pointrcnn: 3d object proposal generation and detection from point cloud. In CVPR.", "Wang et al. (2019a) Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. 2019a. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In CVPR."]}]}