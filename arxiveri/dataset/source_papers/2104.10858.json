{"title": "All tokens matter: Token labeling for training better vision transformers", "abstract": "In this paper, we present token labeling -- a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classification loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous models (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pre-trained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and all the training details will be made publicly available at https://github.com/zihangJiang/TokenLabeling.", "authors": ["Zihang Jiang", " Qibin Hou", " Li Yuan", " Daquan Zhou", " Yujun Shi", " Xiaojie Jin", " Anran Wang", " Jiashi Feng"], "pdf_url": "https://arxiv.org/abs/2104.10858", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td>Network</td><td>Params</td><td>FLOPs</td><td>Train size</td><td>Test size</td><td>Top-1(%)</td><td>Real Top-1 (%)</td></tr><tr><td rowspan=\"6\"> CNNs</td><td>EfficientNet-B5 [34]</td><td>030M</td><td>009.9B</td><td>456</td><td>456</td><td>83.6</td><td>88.3</td></tr><tr><td>EfficientNet-B7 [34]</td><td>066M</td><td>037.0B</td><td>600</td><td>600</td><td>84.3</td><td>_</td></tr><tr><td>Fix-EfficientNet-B8 [34, 38]</td><td>087M</td><td>089.5B</td><td>672</td><td>800</td><td>85.7</td><td>90.0</td></tr><tr><td>NFNet-F3 [3]</td><td>255M</td><td>114.8B</td><td>320</td><td>416</td><td>85.7</td><td>89.4</td></tr><tr><td>NFNet-F4 [3]</td><td>316M</td><td>215.3B</td><td>384</td><td>512</td><td>85.9</td><td>89.4</td></tr><tr><td>NFNet-F5 [3]</td><td>377M</td><td>289.8B</td><td>416</td><td>544</td><td>86.0</td><td>89.2</td></tr><tr><td rowspan=\"16\"> Transformers</td><td>ViT-B/16 [16]</td><td>086M</td><td>055.4B</td><td>224</td><td>384</td><td>77.9</td><td>83.6</td></tr><tr><td>ViT-L/16 [16]</td><td>307M</td><td>190.7B</td><td>224</td><td>384</td><td>76.5</td><td>82.2</td></tr><tr><td>T2T-ViT-14 [46]</td><td>022M</td><td>005.2B</td><td>224</td><td>224</td><td>81.5</td><td>_</td></tr><tr><td>T2T-ViT-14\\uparrow384 [46]</td><td>022M</td><td>017.1B</td><td>224</td><td>384</td><td>83.3</td><td>_</td></tr><tr><td>CrossViT [7]</td><td>045M</td><td>056.6B</td><td>224</td><td>480</td><td>84.1</td><td>_</td></tr><tr><td>Swin-B [26]</td><td>088M</td><td>047.0B</td><td>224</td><td>384</td><td>84.2</td><td>_</td></tr><tr><td>TNT-B [17]</td><td>066M</td><td>014.1B</td><td>224</td><td>224</td><td>82.8</td><td>_</td></tr><tr><td>DeepViT-S [59]</td><td>027M</td><td>006.2B</td><td>224</td><td>224</td><td>82.3</td><td>_</td></tr><tr><td>DeepViT-L [59]</td><td>055M</td><td>012.5B</td><td>224</td><td>224</td><td>83.1</td><td>_</td></tr><tr><td>DeiT-S [36]</td><td>022M</td><td>004.6B</td><td>224</td><td>224</td><td>79.9</td><td>85.7</td></tr><tr><td>DeiT-B [36]</td><td>086M</td><td>017.5B</td><td>224</td><td>224</td><td>81.8</td><td>86.7</td></tr><tr><td>DeiT-B\\uparrow384 [36]</td><td>086M</td><td>055.4B</td><td>224</td><td>384</td><td>83.1</td><td>87.7</td></tr><tr><td>BoTNet-S1-128 [31]</td><td>79.1M</td><td>019.3B</td><td>256</td><td>256</td><td>84.2</td><td>-</td></tr><tr><td>BoTNet-S1-128\\uparrow384 [31]</td><td>79.1M</td><td>045.8B</td><td>256</td><td>384</td><td>84.7</td><td>-</td></tr><tr><td>CaiT-S36\\uparrow384 [37]</td><td>068M</td><td>048.0B</td><td>224</td><td>384</td><td>85.4</td><td>89.8</td></tr><tr><td>CaiT-M36 [37]</td><td>271M</td><td>053.7B</td><td>224</td><td>224</td><td>85.1</td><td>89.3</td></tr><tr><td></td><td>CaiT-M36\\uparrow448 [37]</td><td>271M</td><td>247.8B</td><td>224</td><td>448</td><td>86.3</td><td>90.2</td></tr><tr><td rowspan=\"7\"> Ours LV-ViT</td><td>LV-ViT-S</td><td>026M</td><td>006.6B</td><td>224</td><td>224</td><td>83.3</td><td>88.1</td></tr><tr><td>LV-ViT-S\\uparrow384</td><td>026M</td><td>022.2B</td><td>224</td><td>384</td><td>84.4</td><td>88.9</td></tr><tr><td>LV-ViT-M</td><td>056M</td><td>016.0B</td><td>224</td><td>224</td><td>84.1</td><td>88.4</td></tr><tr><td>LV-ViT-M\\uparrow384</td><td>056M</td><td>042.2B</td><td>224</td><td>384</td><td>85.4</td><td>89.5</td></tr><tr><td>LV-ViT-L</td><td>150M</td><td>059.0B</td><td>288</td><td>288</td><td>85.3</td><td>89.3</td></tr><tr><td>LV-ViT-L\\uparrow448</td><td>150M</td><td>157.2B</td><td>288</td><td>448</td><td>85.9</td><td>89.7</td></tr><tr><td>LV-ViT-L\\uparrow448</td><td>150M</td><td>157.2B</td><td>448</td><td>448</td><td>86.2</td><td>89.9</td></tr><tr><td></td><td>LV-ViT-L\\uparrow512</td><td>151M</td><td>214.8B</td><td>448</td><td>512</td><td>86.4</td><td>90.1</td></tr></tbody></table>", "caption": "Table 4: Top-1 accuracy comparison with other methods on ImageNet [14]and ImageNet Real [2]. All models are trained without external data.With the same computation and parameter constraint, our model consistently outperformsother CNN-based and transformer-based counterparts. The results of CNNs and ViT are referenced from [37].", "list_citation_info": ["[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.", "[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.", "[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[2] Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.", "[59] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.", "[31] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition. arXiv preprint arXiv:2101.11605, 2021.", "[17] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.", "[3] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021.", "[34] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.", "[46] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.", "[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.", "[7] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. arXiv preprint arXiv:2103.14899, 2021.", "[37] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021."]}, {"table": "<table><tbody><tr><th></th><th>Backbone</th><td>Segmentation Architecture</td><td>Model Size</td><td>mIoU (MS)</td><td>Pixel Acc. (MS)</td></tr><tr><th rowspan=\"4\">CNNs</th><th>ResNet-269</th><td>PSPNet [54]</td><td>-</td><td>44.9</td><td>81.7</td></tr><tr><th>ResNet-101</th><td>UperNet [44]</td><td>86M</td><td>44.9</td><td>-</td></tr><tr><th>ResNet-101</th><td>Strip Pooling [23]</td><td>-</td><td>45.6</td><td>82.1</td></tr><tr><th>ResNeSt200</th><td>DeepLabV3+ [9]</td><td>88M</td><td>48.4</td><td>-</td></tr><tr><th rowspan=\"6\">Transformers</th><th>DeiT-S</th><td>UperNet</td><td>52M</td><td>44.0</td><td>-</td></tr><tr><th>ViT-Large{}^{\\dagger}</th><td>SETR [56]</td><td>308M</td><td>50.3</td><td>83.5</td></tr><tr><th>Swin-T [26]</th><td>UperNet</td><td>60M</td><td>46.1</td><td>-</td></tr><tr><th>Swin-S [26]</th><td>UperNet</td><td>81M</td><td>49.3</td><td>-</td></tr><tr><th>Swin-B [26]</th><td>UperNet</td><td>121M</td><td>49.7</td><td>-</td></tr><tr><th>Swin-B{}^{\\dagger} [26]</th><td>UperNet</td><td>121M</td><td>51.6</td><td>-</td></tr><tr><th rowspan=\"4\">LV-ViT</th><th>LV-ViT-S</th><td>FCN</td><td>30M</td><td>48.4</td><td>83.0</td></tr><tr><th>LV-ViT-S</th><td>UperNet</td><td>44M</td><td>48.6</td><td>83.1</td></tr><tr><th>LV-ViT-M</th><td>UperNet</td><td>77M</td><td>50.6</td><td>83.5</td></tr><tr><th>LV-ViT-L</th><td>UperNet</td><td>209M</td><td>51.8</td><td>84.1</td></tr></tbody></table>", "caption": "Table 6: Comparison with previous work on ADE20K validation set.As far as we know,our LV-ViT-L + UperNet achieves the best result on ADE20K with only ImageNet-1Kas training data in pretraining. {}^{\\dagger}Pretrained on ImageNet-22K.", "list_citation_info": ["[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pooling: Rethinking spatial pooling for scene parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4003\u20134012, 2020.", "[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[44] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418\u2013434, 2018.", "[54] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881\u20132890, 2017.", "[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.", "[56] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020."]}, {"table": "<table><thead><tr><th>Training techniques</th><th>#Param.</th><th>Top-1 Acc. (%)</th></tr></thead><tbody><tr><td>Baseline (DeiT-Small [36])</td><td>22M</td><td>79.9</td></tr><tr><td>+ More transformers (12\\rightarrow 16)</td><td>28M</td><td>81.2 (+1.2)</td></tr><tr><td>+ Less MLP expansion ratio (4\\rightarrow 3)</td><td>25M</td><td>81.1 (+1.1)</td></tr><tr><td>+ More convs for patch embedding</td><td>26M</td><td>82.2 (+2.3)</td></tr><tr><td>+ Enhanced residual connection</td><td>26M</td><td>82.4(+2.5)</td></tr><tr><td>+ Token labeling with MixToken</td><td>26M</td><td>83.3(+3.4)</td></tr><tr><td>+ Input resolution (224\\rightarrow 384)</td><td>26M</td><td>84.4 (+4.5)</td></tr></tbody></table>", "caption": "Table 8: Ablation path from the DeiT-Small [36] baseline to our LV-ViT-S.All experiments expect for larger input resolution can be finished within 3 days using a single server nodewith 8 V100 GPUs. Clearly, with only 26M learnable parameters, the performance can be boosted from 79.9to 84.4 (+4.5) using the proposed Token Labeling and other proposed training techniques.", "list_citation_info": ["[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020."]}, {"table": "<table><thead><tr><th>Model</th><th colspan=\"3\">Mixer-S/16 [35]</th><th colspan=\"3\">Mixer-B/16 [35]</th><th colspan=\"3\">Mixer-L/16 [35]</th><th colspan=\"3\">ResNeSt-50 [51]</th></tr></thead><tbody><tr><th>Token Labeling</th><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2713</td></tr><tr><th>Parameters</th><td>18M</td><td>18M</td><td>18M</td><td>59M</td><td>59M</td><td>59M</td><td>207M</td><td>207M</td><td>207M</td><td>27M</td><td>27M</td><td>27M</td></tr><tr><th>Top-1 Acc. (%)</th><td>73.8{}^{\\dagger}</td><td>75.6</td><td>76.1</td><td>76.4{}^{\\dagger}</td><td>78.3</td><td>79.5</td><td>71.6{}^{\\dagger}</td><td>77.7</td><td>80.1</td><td>81.1{}^{\\dagger}</td><td>80.9</td><td>81.5</td></tr></tbody></table>", "caption": "Table 11: Performance of the proposed token labeling objective on representative CNN-based (ResNeSt) and MLP-based (Mixer-MLP) models. Our method has a consistent improvement on all different models. Here {}^{\\dagger} indicates results reported in original papers.", "list_citation_info": ["[35] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.", "[51] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020."]}, {"table": "<table><tbody><tr><td>Settings</td><td>LV-ViT (Ours)</td><td>CaiT [37]</td></tr><tr><td>Transformer Blocks</td><td>20</td><td>36</td></tr><tr><td>#Head in Self-attention</td><td>8</td><td>12</td></tr><tr><td>MLP Expansion Ratio</td><td>3</td><td>4</td></tr><tr><td>Embedding Dimension</td><td>512</td><td>384</td></tr><tr><td>Stochastic Depth [24]</td><td>0.2 (Linear)</td><td>0.2 (Fixed)</td></tr><tr><td>Rand Augmentation [11]</td><td>\u2713</td><td>\u2713</td></tr><tr><td>CutMix Augmentation [48]</td><td></td><td>\u2713</td></tr><tr><td>MixUp Augmentation [52]</td><td></td><td>\u2713</td></tr><tr><td>LayerScaling [37]</td><td></td><td>\u2713</td></tr><tr><td>Class Attention [37]</td><td></td><td>\u2713</td></tr><tr><td>Knowledge Distillation</td><td></td><td>\u2713</td></tr><tr><td>Enhanced Residuals (Ours)</td><td>\u2713</td><td></td></tr><tr><td>MixToken (Ours)</td><td>\u2713</td><td></td></tr><tr><td>Token Labeling (Ours)</td><td>\u2713</td><td></td></tr><tr><td>Test Resolution</td><td>384\\times 384</td><td>384\\times 384</td></tr><tr><td>Model Size</td><td>56M</td><td>69M</td></tr><tr><td>Computations</td><td>42B</td><td>48B</td></tr><tr><td>Training Epoch</td><td>300</td><td>400</td></tr><tr><td>ImageNet Top-1 Acc.</td><td>85.4</td><td>85.4</td></tr></tbody></table>", "caption": "Table 12: Comparison with CaiT [37]. Our model exploits less training techniques,model size, and computations but achieve identical result to CaiT.", "list_citation_info": ["[11] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702\u2013703, 2020.", "[48] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6023\u20136032, 2019.", "[24] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646\u2013661. Springer, 2016.", "[52] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.", "[37] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021."]}]}