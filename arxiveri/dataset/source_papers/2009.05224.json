{"title": "Haa500: Human-centric atomic action dataset with curated videos", "abstract": "We contribute HAA500, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591K labeled frames. To minimize ambiguities in action classification, HAA500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g., \"Baseball Pitching\" vs \"Free Throw in Basketball\". Thus HAA500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as \"Throw\". HAA500 has been carefully curated to capture the precise movement of human figures with little class-irrelevant motions or spatio-temporal label noises. The advantages of HAA500 are fourfold: 1) human-centric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20-60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic action classes. Our extensive experiments including cross-data validation using datasets collected in the wild demonstrate the clear benefits of human-centric and atomic characteristics of HAA500, which enable training even a baseline deep learning model to improve prediction by attending to atomic human poses. We detail the HAA500 dataset statistics and collection methodology and compare quantitatively with existing action recognition datasets.", "authors": ["Jihoon Chung", " Cheng-hsin Wuu", " Hsuan-ru Yang", " Yu-Wing Tai", " Chi-Keung Tang"], "pdf_url": "https://arxiv.org/abs/2009.05224", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>Videos</th><th>Actions</th><th>Atomic</th></tr></thead><tbody><tr><td>KTH [37]</td><td>600</td><td>6</td><td>\u2713</td></tr><tr><td>Weizmann [2]</td><td>90</td><td>10</td><td>\u2713</td></tr><tr><td>UCF Sports [34]</td><td>150</td><td>10</td><td></td></tr><tr><td>Hollywood-2 [29]</td><td>1,707</td><td>12</td><td></td></tr><tr><td>HMDB51 [25]</td><td>7,000</td><td>51</td><td></td></tr><tr><td>UCF101 [42]</td><td>13,320</td><td>101</td><td></td></tr><tr><td>DALY [44]</td><td>510</td><td>10</td><td></td></tr><tr><td>AVA [17]</td><td>387,000</td><td>80</td><td>\u2713</td></tr><tr><td>Kinetics 700 [3]</td><td>650,317</td><td>700</td><td></td></tr><tr><td>HACS [48]</td><td>1,550,000</td><td>200</td><td>\u2713</td></tr><tr><td>Moments in Time [32]</td><td>1,000,000</td><td>339</td><td>\u2713</td></tr><tr><td>FineGym [39]</td><td>32,687</td><td>530</td><td>\u2713</td></tr><tr><td>HAA500</td><td>10,000</td><td>500</td><td>\u2713</td></tr></tbody></table>", "caption": "Table 1: Summary of representative action recognition datasets.", "list_citation_info": ["[34] Mikel D. Rodriguez, Javed Ahmed, and Mubarak Shah. Action MACH a spatio-temporal maximum average correlation height filter for action recognition. In CVPR 2008.", "[17] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik. AVA: A video dataset of spatio-temporally localized atomic visual actions. In CVPR 2018.", "[44] Philippe Weinzaepfel, Xavier Martin, and Cordelia Schmid. Towards weakly-supervised action localization. arXiv preprint arXiv:1605.05197, 2, 2016.", "[48] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In ICCV 2019.", "[2] Moshe Blank, Lena Gorelick, Eli Shechtman, Michal Irani, and Ronen Basri. Actions as space-time shapes. In ICCV 2005.", "[25] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso A. Poggio, and Thomas Serre. HMDB: A large video database for human motion recognition. In ICCV 2011.", "[39] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In CVPR 2020.", "[42] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.", "[3] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019.", "[29] Marcin Marszalek, Ivan Laptev, and Cordelia Schmid. Actions in context. In CVPR 2009.", "[37] Christian Sch\u00fcldt, Ivan Laptev, and Barbara Caputo. Recognizing human actions: A local SVM approach. In CVPR 2004.", "[32] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. TPAMI 2019."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"2\">Kinetics 400 [21]</td><td colspan=\"2\">Something V1 [16]</td></tr><tr><th>Models</th><td>Top-1</td><td>Top-5</td><td>Top-1</td><td>Top-5</td></tr><tr><th>TSN (R-50) [43]</th><td>70.6%</td><td>89.2%</td><td>20.5%</td><td>47.5%</td></tr><tr><th>2-Stream I3D [4]</th><td>71.6%</td><td>90.0%</td><td>41.6%</td><td>72.2%</td></tr><tr><th>TSM (R-50) [27]</th><td>74.1%</td><td>91.2%</td><td>47.3%</td><td>76.2%</td></tr><tr><th>TPN (TSM) [46]</th><td>78.9%</td><td>93.9%</td><td>50.2%</td><td>75.8%</td></tr><tr><th>Skeleton-based</th><td colspan=\"2\">Kinetics 400 [21]</td><td colspan=\"2\">NTU-RGB+D [38]</td></tr><tr><th>Models</th><td>Top-1</td><td>Top-5</td><td>X-Sub</td><td>X-View</td></tr><tr><th>Deep LSTM [38]</th><td>16.4%</td><td>35.3%</td><td>62.9%</td><td>70.3%</td></tr><tr><th>ST-GCN [45]</th><td>30.7%</td><td>52.8%</td><td>81.5%</td><td>88.3%</td></tr></tbody></table>", "caption": "Table 2: Performance of previous works on Kinetics 400 [21], Something-Something [16], and NTU-RGB+D [38] dataset.We evaluate on both cross-subject (X-Sub) and cross-view (X-View) benchmarks for NTU-RGB+D.For a fair comparison, in this paper we use [21] rather than [3] as representative action recognition model still use [21] for pre-training or benchmarking at the time of writing.", "list_citation_info": ["[4] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR 2017.", "[38] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. NTU RGB+D: A large scale dataset for 3d human activity analysis. In CVPR 2016.", "[46] Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, and Bolei Zhou. Temporal pyramid network for action recognition. In CVPR 2020.", "[27] Ji Lin, Chuang Gan, and Song Han. TSM: Temporal shift module for efficient video understanding. In ICCV 2019.", "[43] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks for action recognition in videos. TPAMI 2018.", "[21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.", "[3] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019.", "[16] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \u201csomething something\" video database for learning and evaluating visual common sense. In ICCV 2017.", "[45] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In AAAI 2018."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Clip Length</th><th>Irr. Actions</th><th>Camera Cuts</th></tr></thead><tbody><tr><td>UCF101 [42]</td><td>Varies</td><td></td><td></td></tr><tr><td>HMDB51 [25]</td><td>Varies</td><td></td><td>\u2713</td></tr><tr><td>AVA [17]</td><td>1 second</td><td>\u2713</td><td>\u2713</td></tr><tr><td>HACS [48]</td><td>2 second</td><td>\u2713</td><td></td></tr><tr><td>Kinetics [21]</td><td>10 second</td><td>\u2713</td><td>\u2713</td></tr><tr><td>M.i.T. [32]</td><td>3 second</td><td></td><td></td></tr><tr><td>HAA500</td><td>Just Right</td><td></td><td></td></tr></tbody></table>", "caption": "Table 4: Clip length and irrelevant frames of video action datasets.", "list_citation_info": ["[48] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In ICCV 2019.", "[17] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik. AVA: A video dataset of spatio-temporally localized atomic visual actions. In CVPR 2018.", "[25] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso A. Poggio, and Thomas Serre. HMDB: A large video database for human motion recognition. In ICCV 2011.", "[21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.", "[42] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.", "[32] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. TPAMI 2019."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Detectable Joints</th></tr></thead><tbody><tr><th>Kinetics 400 [21]</th><td>41.0%</td></tr><tr><th>UCF101 [42]</th><td>37.8%</td></tr><tr><th>HMDB51 [25]</th><td>41.8%</td></tr><tr><th>FineGym [39]</th><td>44.7%</td></tr><tr><th>HAA500</th><td>69.7%</td></tr></tbody></table>", "caption": "Table 5: Detectable joints of video action datasets. We use AlphaPose [10] to detect the largest person in the frame, and count the number of joints with a score higher than 0.5. ", "list_citation_info": ["[25] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso A. Poggio, and Thomas Serre. HMDB: A large video database for human motion recognition. In ICCV 2011.", "[39] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In CVPR 2020.", "[42] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.", "[21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.", "[10] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. RMPE: Regional multi-person pose estimation. In ICCV 2017."]}, {"table": "<table><thead><tr><th></th><th>RGB</th><th>Pose</th><th>RGB + Pose</th></tr></thead><tbody><tr><td>HAA500</td><td>33.53%</td><td>35.73%</td><td>42.80%</td></tr><tr><td>\u2005\u2005\u2005 Sport</td><td>38.52%</td><td>47.33%</td><td>50.94%</td></tr><tr><td>\u2005\u2005\u2005 Instrument</td><td>30.72%</td><td>24.18%</td><td>32.03%</td></tr><tr><td>\u2005\u2005\u2005 Hobbies</td><td>31.30%</td><td>26.42%</td><td>35.37%</td></tr><tr><td>\u2005\u2005\u2005 Daily</td><td>28.82%</td><td>28.60%</td><td>39.14%</td></tr><tr><td>Gym288 [39]</td><td>76.11%</td><td>65.16%</td><td>77.31%</td></tr></tbody></table>", "caption": "Table 7: Atomic action classification accuracy when both RGB image and pose estimation are given as an input. We also show performance when they are trained separately for comparison.", "list_citation_info": ["[39] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In CVPR 2020."]}, {"table": "<table><tbody><tr><th></th><td>UCF101 [42]</td><td>ActNet 100 [9]</td><td>HMDB51 [25]</td></tr><tr><th>Pre-trained</th><td>Top-1</td><td>Top-1</td><td>Top-1</td></tr><tr><th>None</th><td>58.87%</td><td>43.54%</td><td>28.56%</td></tr><tr><th>AVA [17]</th><td>48.54%</td><td>30.51%</td><td>25.28%</td></tr><tr><th>Gym288 [39]</th><td>69.94%</td><td>43.79%</td><td>36.24%</td></tr><tr><th>UCF101 [42]</th><td>-</td><td>42.94%</td><td>32.37%</td></tr><tr><th>ActNet 100 [9]</th><td>57.52%</td><td>-</td><td>28.63%</td></tr><tr><th>HMDB51 [25]</th><td>53.36%</td><td>39.33%</td><td>-</td></tr><tr><th>HAA500</th><td>68.70%</td><td>47.75%</td><td>40.45%</td></tr><tr><th>\u2005\u2005\u2005Relaxed</th><td>62.24%</td><td>38.30%</td><td>33.29%</td></tr></tbody></table>", "caption": "Table 8: Fine-tuning performance on I3D.", "list_citation_info": ["[17] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik. AVA: A video dataset of spatio-temporally localized atomic visual actions. In CVPR 2018.", "[25] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso A. Poggio, and Thomas Serre. HMDB: A large video database for human motion recognition. In ICCV 2011.", "[9] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In CVPR 2015.", "[39] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In CVPR 2020.", "[42] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012."]}, {"table": "<table><tbody><tr><th># of frames</th><td>HAA500</td><td>UCF101 [42]</td><td>AVA [17]</td><td>Gym288 [39]</td></tr><tr><th>1</th><td>19.93%</td><td>45.57%</td><td>33.57%</td><td>39.77%</td></tr><tr><th>2</th><td>23.27%</td><td>47.26%</td><td>39.42%</td><td>44.68%</td></tr><tr><th>4</th><td>24.40%</td><td>49.30%</td><td>39.48%</td><td>51.22%</td></tr><tr><th>8</th><td>24.07%</td><td>49.80%</td><td>42.38%</td><td>59.64%</td></tr><tr><th>16</th><td>28.20%</td><td>52.31%</td><td>43.11%</td><td>69.25%</td></tr><tr><th>32</th><td>33.53%</td><td>57.65%</td><td>29.88%</td><td>76.11%</td></tr><tr><th>stride 2</th><td>27.47%</td><td>57.23%</td><td>41.49%</td><td>68.68%</td></tr><tr><th>stride 4</th><td>23.87%</td><td>52.29%</td><td>40.52%</td><td>60.76%</td></tr><tr><th>stride 8</th><td>18.47%</td><td>47.95%</td><td>38.45%</td><td>39.31%</td></tr></tbody></table>", "caption": "Table 11: Performance comparison on I3D-RGB over the number of frames and strides, wherein the latter a window size of 32 frames is used except AVA which we test with 16 frames.", "list_citation_info": ["[17] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, and Jitendra Malik. AVA: A video dataset of spatio-temporally localized atomic visual actions. In CVPR 2018.", "[39] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In CVPR 2020.", "[42] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012."]}]}