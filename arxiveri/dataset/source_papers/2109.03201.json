{"title": "nnformer: Interleaved transformer for volumetric segmentation", "abstract": "Transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to overcome their inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. To address this issue, we introduce nnFormer, a 3D transformer for volumetric medical image segmentation. nnFormer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. Moreover, nnFormer proposes to use skip attention to replace the traditional concatenation/summation operations in skip connections in U-Net like architecture. Experiments show that nnFormer significantly outperforms previous transformer-based counterparts by large margins on three public datasets. Compared to nnUNet, nnFormer produces significantly lower HD95 and comparable DSC results. Furthermore, we show that nnFormer and nnUNet are highly complementary to each other in model ensembling.", "authors": ["Hong-Yu Zhou", " Jiansen Guo", " Yinghao Zhang", " Lequan Yu", " Liansheng Wang", " Yizhou Yu"], "pdf_url": "https://arxiv.org/abs/2109.03201", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Average</td><td colspan=\"2\">WT</td><td colspan=\"2\">ET</td><td colspan=\"2\">TC</td></tr><tr><td>HD95 \\downarrow</td><td>DSC \\uparrow</td><td>HD95 \\downarrow</td><td>DSC \\uparrow</td><td>HD95 \\downarrow</td><td>DSC \\uparrow</td><td>HD95 \\downarrow</td><td>DSC \\uparrow</td></tr><tr><td>SETR NUP [33]</td><td>13.78</td><td>63.7</td><td>14.419</td><td>69.7</td><td>11.72</td><td>54.4</td><td>15.19</td><td>66.9</td></tr><tr><td>SETR PUP [33]</td><td>14.01</td><td>63.8</td><td>15.245</td><td>69.6</td><td>11.76</td><td>54.9</td><td>15.023</td><td>67.0</td></tr><tr><td>SETR MLA [33]</td><td>13.49</td><td>63.9</td><td>15.503</td><td>69.8</td><td>10.24</td><td>55.4</td><td>14.72</td><td>66.5</td></tr><tr><td>TransUNet [11]</td><td>12.98</td><td>64.4</td><td>14.03</td><td>70.6</td><td>10.42</td><td>54.2</td><td>14.5</td><td>68.4</td></tr><tr><td>TransBTS [25]</td><td>9.65</td><td>69.6</td><td>10.03</td><td>77.9</td><td>9.97</td><td>57.4</td><td>8.95</td><td>73.5</td></tr><tr><td>CoTr w/o CNN encoder [24]</td><td>11.22</td><td>64.4</td><td>11.49</td><td>71.2</td><td>9.59</td><td>52.3</td><td>12.58</td><td>69.8</td></tr><tr><td>CoTr [24]</td><td>9.70</td><td>68.3</td><td>9.20</td><td>74.6</td><td>9.45</td><td>55.7</td><td>10.45</td><td>74.8</td></tr><tr><td>UNETR [34]</td><td>8.82</td><td>71.1</td><td>8.27</td><td>78.9</td><td>9.35</td><td>58.5</td><td>8.85</td><td>76.1</td></tr><tr><td>Our nnFormer</td><td>4.05</td><td>86.4</td><td>3.80</td><td>91.3</td><td>3.87</td><td>81.8</td><td>4.49</td><td>86.0</td></tr><tr><td>P-values</td><td colspan=\"8\">&lt; 1e-2 (HD95), &lt; 1e-2 (DSC)</td></tr></table>", "caption": "Table 2: Comparison with transformer-based models on brain tumor segmentation. The evaluation metrics are HD95 (mm) and DSC in (%). Best results are bolded while second best are underlined. Experimental results of baselines are from [34]. We calculate the p-values between the average performance of our nnFormer and the best performing baseline in both metrics.", "list_citation_info": ["[24] Y. Xie, J. Zhang, C. Shen, and Y. Xia, \u201cCoTr: Efficiently bridging cnn and transformer for 3d medical image segmentation,\u201d arXiv preprint arXiv:2103.03024, 2021.", "[11] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, et al., \u201cTransUNet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.", "[34] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, \u201cUNETR: Transformers for 3d medical image segmentation,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 574\u2013584, January 2022.", "[25] W. Wang, C. Chen, M. Ding, J. Li, H. Yu, and S. Zha, \u201cTransBTS: Multimodal brain tumor segmentation using transformer,\u201d arXiv preprint arXiv:2103.04430, 2021.", "[33] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr, et al., \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6881\u20136890, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">Average</td><td rowspan=\"2\">Aotra</td><td rowspan=\"2\">Gallbladder</td><td rowspan=\"2\">Kidney (Left)</td><td rowspan=\"2\">Kidney (Right)</td><td rowspan=\"2\">Liver</td><td rowspan=\"2\">Pancreas</td><td rowspan=\"2\">Spleen</td><td rowspan=\"2\">Stomach</td></tr><tr><td>HD95 \\downarrow</td><td>DSC \\uparrow</td></tr><tr><td>ViT [2] + CUP [11]</td><td>36.11</td><td>67.86</td><td>70.19</td><td>45.10</td><td>74.70</td><td>67.40</td><td>91.32</td><td>42.00</td><td>81.75</td><td>70.44</td></tr><tr><td>R50-ViT [2] + CUP [11]</td><td>32.87</td><td>71.29</td><td>73.73</td><td>55.13</td><td>75.80</td><td>72.20</td><td>91.51</td><td>45.99</td><td>81.99</td><td>73.95</td></tr><tr><td>TransUNet [11]</td><td>31.69</td><td>77.48</td><td>87.23</td><td>63.16</td><td>81.87</td><td>77.02</td><td>94.08</td><td>55.86</td><td>85.08</td><td>75.62</td></tr><tr><td>TransUNet{}^{\\bigtriangledown} [11]</td><td>-</td><td>84.36</td><td>90.68</td><td>71.99</td><td>86.04</td><td>83.71</td><td>95.54</td><td>73.96</td><td>88.80</td><td>84.20</td></tr><tr><td>SwinUNet [18]</td><td>21.55</td><td>79.13</td><td>85.47</td><td>66.53</td><td>83.28</td><td>79.61</td><td>94.29</td><td>56.58</td><td>90.66</td><td>76.60</td></tr><tr><td>TransClaw U-Net [15]</td><td>26.38</td><td>78.09</td><td>85.87</td><td>61.38</td><td>84.83</td><td>79.36</td><td>94.28</td><td>57.65</td><td>87.74</td><td>73.55</td></tr><tr><td>LeVit-UNet-384s [22]</td><td>16.84</td><td>78.53</td><td>87.33</td><td>62.23</td><td>84.61</td><td>80.25</td><td>93.11</td><td>59.07</td><td>88.86</td><td>72.76</td></tr><tr><td>MISSFormer [35]</td><td>18.20</td><td>81.96</td><td>86.99</td><td>68.65</td><td>85.21</td><td>82.00</td><td>94.41</td><td>65.67</td><td>91.92</td><td>80.81</td></tr><tr><td>UNETR [34]</td><td>22.97</td><td>79.56</td><td>89.99</td><td>60.56</td><td>85.66</td><td>84.80</td><td>94.46</td><td>59.25</td><td>87.81</td><td>73.99</td></tr><tr><td>Our nnFormer</td><td>10.63</td><td>86.57</td><td>92.04</td><td>70.17</td><td>86.57</td><td>86.25</td><td>96.84</td><td>83.35</td><td>90.51</td><td>86.83</td></tr><tr><td>P-values</td><td colspan=\"10\">&lt; 1e-2 (HD95), &lt; 1e-2 (DSC)</td></tr></table>", "caption": "Table 3: Comparison with transformer-based models on multi-organ segmentation (Synapse). The evaluation metrics are HD95 (mm) and DSC in (%). Best results are bolded while second best are underlined. \\bigtriangledown denotes TransUNet uses larger inputs, whose size is 512\\times512. The p-values are calculated based on the average performance of our nnFormer and the best performing baseline in both metrics.", "list_citation_info": ["[15] Y. Chang, H. Menghan, Z. Guangtao, and Z. Xiao-Ping, \u201cTransClaw U-Net: Claw u-net with transformers for medical image segmentation,\u201d arXiv preprint arXiv:2107.05188, 2021.", "[22] G. Xu, X. Wu, X. Zhang, and X. He, \u201cLeViT-UNet: Make faster encoders with transformer for medical image segmentation,\u201d arXiv preprint arXiv:2107.08623, 2021.", "[11] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, et al., \u201cTransUNet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.", "[35] X. Huang, Z. Deng, D. Li, and X. Yuan, \u201cMISSFormer: An effective medical image segmentation transformer,\u201d arXiv preprint arXiv:2109.07162, 2021.", "[18] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, \u201cSwin-Unet: Unet-like pure transformer for medical image segmentation,\u201d arXiv preprint arXiv:2105.05537, 2021.", "[34] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, \u201cUNETR: Transformers for 3d medical image segmentation,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 574\u2013584, January 2022.", "[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020."]}, {"table": "<table><tr><td>Methods</td><td>Average</td><td>RV</td><td>Myo</td><td>LV</td></tr><tr><td>VIT-CUP [2]</td><td>81.45</td><td>81.46</td><td>70.71</td><td>92.18</td></tr><tr><td>R50-VIT-CUP [2]</td><td>87.57</td><td>86.07</td><td>81.88</td><td>94.75</td></tr><tr><td>TransUNet [11]</td><td>89.71</td><td>88.86</td><td>84.54</td><td>95.73</td></tr><tr><td>SwinUNet [18]</td><td>90.00</td><td>88.55</td><td>85.62</td><td>95.83</td></tr><tr><td>LeViT-UNet-384s [22]</td><td>90.32</td><td>89.55</td><td>87.64</td><td>93.76</td></tr><tr><td>UNETR [34]</td><td>88.61</td><td>85.29</td><td>86.52</td><td>94.02</td></tr><tr><td>nnFormer</td><td>92.06</td><td>90.94</td><td>89.58</td><td>95.65</td></tr><tr><td>P-value</td><td colspan=\"4\">&lt; 1e-2 (DSC)</td></tr></table>", "caption": "Table 4: Comparison with transformer-based models on automatic cardiac diagnosis (ACDC). The evaluation metric is DSC (%). Best results are bolded while second best are underlined. The default evaluation metric is DSC, based on which we calculate the p-value.", "list_citation_info": ["[22] G. Xu, X. Wu, X. Zhang, and X. He, \u201cLeViT-UNet: Make faster encoders with transformer for medical image segmentation,\u201d arXiv preprint arXiv:2107.08623, 2021.", "[11] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, et al., \u201cTransUNet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.", "[18] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang, \u201cSwin-Unet: Unet-like pure transformer for medical image segmentation,\u201d arXiv preprint arXiv:2105.05537, 2021.", "[34] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, \u201cUNETR: Transformers for 3d medical image segmentation,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 574\u2013584, January 2022.", "[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020."]}, {"table": "<table><tr><td>#</td><td>Models</td><td>Average</td><td>RV</td><td>Myo</td><td>LV</td></tr><tr><td>0</td><td>1\\timesLV-MSA + PM [3] + PE [3]</td><td>90.55</td><td>88.59</td><td>88.47</td><td>94.60</td></tr><tr><td>1</td><td>1\\timesLV-MSA + PM [3] + Conv. Embed.</td><td>90.97</td><td>88.94</td><td>88.84</td><td>95.13</td></tr><tr><td>2</td><td>1\\timesLV-MSA + Conv. Down. + Conv. Embed.</td><td>91.26</td><td>89.70</td><td>89.04</td><td>95.04</td></tr><tr><td>3</td><td>1\\timesLV-MSA + 1\\timesGV-MSA + Conv. Down. + Conv. Embed.</td><td>91.46</td><td>89.82</td><td>89.17</td><td>95.39</td></tr><tr><td>4</td><td>1\\timesLV-MSA + 1\\timesGV-MSA + Conv. Down. + Conv. Embed. + Skip Att.</td><td>91.85</td><td>90.41</td><td>89.50</td><td>95.63</td></tr><tr><td>5</td><td>1\\timesLV-MSA + 1\\timesSLV-MSA + 2\\timesGV-MSA + Conv. Down. + Conv. Embed. + Skip Att.</td><td>92.06</td><td>90.94</td><td>89.58</td><td>95.65</td></tr></table>", "caption": "Table 6: Investigation of the impact of different modules used in nnFormer. PM and PE denote the patch merging and patch embedding strategies used in swin transformer [3]. Conv. Embed. and Conv. Down. represent our convolutional embedding and down-sampling layers, respectively. Skip Att. refers to the proposed skip attention mechanism. 1\\timesLV-MSA in lines 0-2 means that each transformer block contains one transformer layer and each layer consists of one LV-MSA. 1\\timesGV-MSA in lines 3-4 denotes that we replace LV-MSA in the bottleneck with GV-MSA. 1\\timesSLV-MSA and 2\\timesGV-MSA in line 5 mean that we increase the number of transformer layers in each transformer block from one to two. To be specific, in the encoder/decoder, each transformer block contains 1\\timesLV-MSA and 1\\timesSLV-MSA while in the bottleneck, there are 2\\timesGV-MSA in each block.", "list_citation_info": ["[3] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d arXiv preprint arXiv:2103.14030, 2021."]}]}