{"title": "Patching open-vocabulary models by interpolating weights", "abstract": "Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.", "authors": ["Gabriel Ilharco", " Mitchell Wortsman", " Samir Yitzhak Gadre", " Shuran Song", " Hannaneh Hajishirzi", " Simon Kornblith", " Ali Farhadi", " Ludwig Schmidt"], "pdf_url": "https://arxiv.org/abs/2208.05592", "list_table_and_caption": [{"table": "<table><tr><td></td><td colspan=\"3\">Size of the set used for</td><td></td></tr><tr><td>Dataset</td><td>Training</td><td>Validation</td><td>Testing</td><td>Number of classes</td></tr><tr><td>Cars Krause et al. (2013)</td><td>7,330</td><td>814</td><td>8041</td><td>196</td></tr><tr><td>DTD Cimpoi et al. (2014)</td><td>3,384</td><td>376</td><td>1,880</td><td>47</td></tr><tr><td>EuroSAT Helber et al. (2019)</td><td>21,600</td><td>2,700</td><td>2,700</td><td>10</td></tr><tr><td>GTSRB Stallkamp et al. (2011)</td><td>23,976</td><td>2,664</td><td>12,630</td><td>43</td></tr><tr><td>KITTI Geiger et al. (2012)</td><td>6,347</td><td>423</td><td>711</td><td>4</td></tr><tr><td>MNIST LeCun (1998)</td><td>55,000</td><td>5,000</td><td>10,000</td><td>10</td></tr><tr><td>RESISC45 Cheng et al. (2017)</td><td>17,010</td><td>1,890</td><td>6,300</td><td>45</td></tr><tr><td>SUN397 Xiao et al. (2016)</td><td>17,865</td><td>1,985</td><td>19,850</td><td>397</td></tr><tr><td>SVHN Netzer et al. (2011)</td><td>68,257</td><td>5,000</td><td>26,032</td><td>10</td></tr><tr><td>FashionMNIST Xiao et al. (2017)</td><td>55,000</td><td>5,000</td><td>10,000</td><td>10</td></tr><tr><td>MTSD Ertler et al. (2020)</td><td>55,078</td><td>5,000</td><td>8,737</td><td>235</td></tr><tr><td>CIFAR10 Krizhevsky et al. (2009)</td><td>45,000</td><td>5,000</td><td>10,000</td><td>10</td></tr><tr><td>CIFAR100 Krizhevsky et al. (2009)</td><td>45,000</td><td>5,000</td><td>10,000</td><td>100</td></tr><tr><td>Food101 Bossard et al. (2014)</td><td>70,750</td><td>5,000</td><td>25,250</td><td>101</td></tr><tr><td>STL10 Coates et al. (2011)</td><td>4,500</td><td>500</td><td>8,000</td><td>10</td></tr><tr><td>ImageNet Deng et al. (2009)</td><td>1,255,167</td><td>26,000</td><td>50,000</td><td>1,000</td></tr></table>", "caption": "Table 3: Dataset statistics for patching and supported tasks.When a set for validation is not available we use held-out data from the official training set for validation purposes.In cases like ImageNet we use the official validation set for testing.", "list_citation_info": ["Xiao et al. [2016] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 2016. https://link.springer.com/article/10.1007/s11263-014-0748-y.", "Cimpoi et al. [2014] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014. https://openaccess.thecvf.com/content_cvpr_2014/html/Cimpoi_Describing_Textures_in_2014_CVPR_paper.html.", "Cheng et al. [2017] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the Institute of Electrical and Electronics Engineers (IEEE), 2017. https://ieeexplore.ieee.org/abstract/document/7891544.", "Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In Advances in Neural Information Processing Systems (NeurIPS) Workshops, 2011. https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37648.pdf.", "Helber et al. [2019] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. https://arxiv.org/abs/1709.00029.", "Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009. https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.", "LeCun [1998] Yann LeCun. The mnist database of handwritten digits, 1998. http://yann.lecun.com/exdb/mnist/.", "Bossard et al. [2014] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In European Conference on Computer Vision (ECCV), 2014. https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29.", "Geiger et al. [2012] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012. https://ieeexplore.ieee.org/abstract/document/6248074.", "Ertler et al. [2020] Christian Ertler, Jerneja Mislej, Tobias Ollmann, Lorenzo Porzi, Gerhard Neuhold, and Yubin Kuang. The mapillary traffic sign dataset for detection and classification on a global scale. In European Conference on Computer Vision (ECCV), 2020. https://arxiv.org/abs/1909.04422.", "Xiao et al. [2017] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017. https://arxiv.org/abs/1708.07747.", "Stallkamp et al. [2011] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In International Joint Conference on Neural Networks (IJCNN), 2011. https://ieeexplore.ieee.org/document/6033395.", "Krause et al. [2013] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In International Conference on Computer Vision Workshops (ICML), 2013. https://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W19/html/Krause_3D_Object_Representations_2013_ICCV_paper.html.", "Coates et al. [2011] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2011. https://proceedings.mlr.press/v15/coates11a.html.", "Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), 2009. https://ieeexplore.ieee.org/abstract/document/5206848."]}]}