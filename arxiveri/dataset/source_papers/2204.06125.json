{"title": "Hierarchical text-conditional image generation with clip latents", "abstract": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.", "authors": ["Aditya Ramesh", " Prafulla Dhariwal", " Alex Nichol", " Casey Chu", " Mark Chen"], "pdf_url": "https://arxiv.org/abs/2204.06125", "list_table_and_caption": [{"table": "<table><tr><td>Model</td><td>FID</td><td>Zero-shot FID</td><td>Zero-shot FID (filt)</td></tr><tr><td>AttnGAN (Xu et al., 2017)</td><td>35.49</td><td></td><td></td></tr><tr><td>DM-GAN (Zhu et al., 2019)</td><td>32.64</td><td></td><td></td></tr><tr><td>DF-GAN (Tao et al., 2020)</td><td>21.42</td><td></td><td></td></tr><tr><td>DM-GAN + CL (Ye et al., 2021)</td><td>20.79</td><td></td><td></td></tr><tr><td>XMC-GAN (Zhang et al., 2021a)</td><td>9.33</td><td></td><td></td></tr><tr><td>LAFITE (Zhou et al., 2021)</td><td>8.12</td><td></td><td></td></tr><tr><td>Make-A-Scene (Gafni et al., 2022)</td><td>7.55</td><td></td><td></td></tr><tr><td>DALL-E (Ramesh et al., 2021)</td><td></td><td>\\sim 28</td><td></td></tr><tr><td>LAFITE (Zhou et al., 2021)</td><td></td><td>26.94</td><td></td></tr><tr><td>GLIDE (Nichol et al., 2021)</td><td></td><td>12.24</td><td>12.89</td></tr><tr><td>Make-A-Scene (Gafni et al., 2022)</td><td></td><td></td><td>11.84</td></tr><tr><td>unCLIP (AR prior)</td><td></td><td>10.63</td><td>11.08</td></tr><tr><td>unCLIP (Diffusion prior)</td><td></td><td>10.39</td><td>10.87</td></tr></table>", "caption": "Table 2:  Comparison of FID on MS-COCO 256\\times 256. We use guidance scale 1.25 for the decoder for both the AR and diffusion prior, and achieve the best results using the diffusion prior.", "list_citation_info": ["Zhang et al. [2021a] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-Modal Contrastive Learning for Text-to-Image Generation. arXiv:2101.04702, 2021a.", "Tao et al. [2020] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis. arXiv:2008.05865, 2020.", "Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation. arXiv:2102.12092, 2021.", "Xu et al. [2017] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. arXiv:1711.10485, 2017.", "Zhu et al. [2019] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis. arXiv:1904.01310, 2019.", "Zhou et al. [2021] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: Towards Language-Free Training for Text-to-Image Generation. arXiv:2111.13792, 2021.", "Nichol et al. [2021] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. arXiv:2112.10741, 2021.", "Gafni et al. [2022] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. arXiv:2203.13131, 2022.", "Ye et al. [2021] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving Text-to-Image Synthesis Using Contrastive Learning. arXiv:2107.02423, 2021."]}, {"table": "<table><tr><td></td><td>AR prior</td><td>Diffusion prior</td><td>64</td><td>64\\rightarrow 256</td><td>256\\rightarrow 1024</td></tr><tr><td>Diffusion steps</td><td>-</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Noise schedule</td><td>-</td><td>cosine</td><td>cosine</td><td>cosine</td><td>linear</td></tr><tr><td>Sampling steps</td><td>-</td><td>64</td><td>250</td><td>27</td><td>15</td></tr><tr><td>Sampling variance method</td><td>-</td><td>analytic Bao et al. [2022]</td><td>learned Nichol and Dhariwal [2021]</td><td>DDIM Song et al. [2020]</td><td>DDIM Song et al. [2020]</td></tr><tr><td>Crop fraction</td><td>-</td><td>-</td><td>-</td><td>0.25</td><td>0.25</td></tr><tr><td>Model size</td><td>1B</td><td>1B</td><td>3.5B</td><td>700M</td><td>300M</td></tr><tr><td>Channels</td><td>-</td><td>-</td><td>512</td><td>320</td><td>192</td></tr><tr><td>Depth</td><td>-</td><td>-</td><td>3</td><td>3</td><td>2</td></tr><tr><td>Channels multiple</td><td>-</td><td>-</td><td>1,2,3,4</td><td>1,2,3,4</td><td>1,1,2,2,4,4</td></tr><tr><td>Heads channels</td><td>-</td><td>-</td><td>64</td><td>-</td><td>-</td></tr><tr><td>Attention resolution</td><td>-</td><td>-</td><td>32,16,8</td><td>-</td><td>-</td></tr><tr><td>Text encoder context</td><td>256</td><td>256</td><td>256</td><td>-</td><td>-</td></tr><tr><td>Text encoder width</td><td>2048</td><td>2048</td><td>2048</td><td>-</td><td>-</td></tr><tr><td>Text encoder depth</td><td>24</td><td>24</td><td>24</td><td>-</td><td>-</td></tr><tr><td>Text encoder heads</td><td>32</td><td>32</td><td>32</td><td>-</td><td>-</td></tr><tr><td>Latent decoder context</td><td>384</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Latent decoder width</td><td>1664</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Latent decoder depth</td><td>24</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Latent decoder heads</td><td>26</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Dropout</td><td>-</td><td>-</td><td>0.1</td><td>0.1</td><td>-</td></tr><tr><td>Weight decay</td><td>4.0e-2</td><td>6.0e-2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Batch size</td><td>4096</td><td>4096</td><td>2048</td><td>1024</td><td>512</td></tr><tr><td>Iterations</td><td>1M</td><td>600K</td><td>800K</td><td>1M</td><td>1M</td></tr><tr><td>Learning rate</td><td>1.6e-4</td><td>1.1e-4</td><td>1.2e-4</td><td>1.2e-4</td><td>1.0e-4</td></tr><tr><td>Adam \\beta_{2}</td><td>0.91</td><td>0.96</td><td>0.999</td><td>0.999</td><td>0.999</td></tr><tr><td>Adam \\epsilon</td><td>1.0e-10</td><td>1.0e-6</td><td>1.0e-8</td><td>1.0e-8</td><td>1.0e-8</td></tr><tr><td>EMA decay</td><td>0.999</td><td>0.9999</td><td>0.9999</td><td>0.9999</td><td>0.9999</td></tr></table>", "caption": "Table 3: Hyperparameters for the models", "list_citation_info": ["Song et al. [2020] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. arXiv:2010.02502, 2020.", "Bao et al. [2022] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models. CoRR, abs/2201.06503, 2022. URL https://arxiv.org/abs/2201.06503.", "Nichol and Dhariwal [2021] Alex Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models. arXiv:2102.09672, 2021."]}]}