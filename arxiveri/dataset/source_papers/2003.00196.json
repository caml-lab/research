{"title": "First Order Motion Model for Image Animation", "abstract": "Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly available.", "authors": ["Aliaksandr Siarohin", " St\u00e9phane Lathuili\u00e8re", " Sergey Tulyakov", " Elisa Ricci", " Nicu Sebe"], "pdf_url": "https://arxiv.org/abs/2003.00196", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"3\">Tai-Chi-HD</th><th colspan=\"3\">VoxCeleb</th><th colspan=\"3\">Nemo</th><th>Bair</th></tr><tr><th></th><th>\\mathcal{L}_{1}</th><th>(AKD, MKR)</th><th>AED</th><th>\\mathcal{L}_{1}</th><th>AKD</th><th>AED</th><th>\\mathcal{L}_{1}</th><th>AKD</th><th>AED</th><th>\\mathcal{L}_{1}</th></tr></thead><tbody><tr><td>X2Face  [41]</td><td>0.080</td><td>(17.654, 0.109)</td><td>0.272</td><td>0.078</td><td>7.687</td><td>0.405</td><td>0.031</td><td>3.539</td><td>0.221</td><td>0.065</td></tr><tr><td>Monkey-Net  [29]</td><td>0.077</td><td>(10.798, 0.059)</td><td>0.228</td><td>0.049</td><td>1.878</td><td>0.199</td><td>0.018</td><td>1.285</td><td>0.077</td><td>0.034</td></tr><tr><td>Ours</td><td>0.063</td><td>(6.862, 0.036)</td><td>0.179</td><td>0.043</td><td>1.294</td><td>0.140</td><td>0.016</td><td>1.119</td><td>0.048</td><td>0.027</td></tr></tbody></table>", "caption": "Table 3: Video reconstruction: comparison with the state of the art on four different datasets.", "list_citation_info": ["[41] Olivia Wiles, A Sophia Koepke, and Andrew Zisserman. X2face: A network for controlling face generation using images, audio, and pose codes. In ECCV, 2018.", "[29] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In CVPR, 2019."]}]}