{"title": "Generative Dual Adversarial Network for Generalized Zero-Shot Learning", "abstract": "This paper studies the problem of generalized zero-shot learning which requires the model to train on image-label pairs from some seen classes and test on the task of classifying new images from both seen and unseen classes. Most previous models try to learn a fixed one-directional mapping between visual and semantic space, while some recently proposed generative methods try to generate image features for unseen classes so that the zero-shot learning problem becomes a traditional fully-supervised classification problem. In this paper, we propose a novel model that provides a unified framework for three different approaches: visual-> semantic mapping, semantic->visual mapping, and metric learning. Specifically, our proposed model consists of a feature generator that can generate various visual features given class embeddings as input, a regressor that maps each visual feature back to its corresponding class embedding, and a discriminator that learns to evaluate the closeness of an image feature and a class embedding. All three components are trained under the combination of cyclic consistency loss and dual adversarial loss. Experimental results show that our model not only preserves higher accuracy in classifying images from seen classes, but also performs better than existing state-of-the-art models in in classifying images from unseen classes.", "authors": ["He Huang", " Changhu Wang", " Philip S. Yu", " Chang-Dong Wang"], "pdf_url": "https://arxiv.org/abs/1811.04857", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Dataset</th><td colspan=\"3\">SUN</td><td colspan=\"3\">CUB</td><td colspan=\"3\">AwA2</td><td colspan=\"3\">aPY</td></tr><tr><th>Methods</th><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td><td>U</td><td>S</td><td>H</td></tr><tr><th>SSE [39]</th><td>2.1</td><td>36.4</td><td>4.0</td><td>8.5</td><td>46.9</td><td>14.4</td><td>8.1</td><td>82.6</td><td>14.8</td><td>0.2</td><td>78.9</td><td>0.4</td></tr><tr><th>LATEM [33]</th><td>14.7</td><td>28.8</td><td>19.5</td><td>15.2</td><td>57.3</td><td>24.0</td><td>11.5</td><td>77.3</td><td>20.0</td><td>0.1</td><td>73.0</td><td>0.2</td></tr><tr><th>ALE [1]</th><td>21.8</td><td>33.1</td><td>26.3</td><td>27.3</td><td>62.8</td><td>34.4</td><td>14.0</td><td>81.8</td><td>23.9</td><td>4.6</td><td>73.7</td><td>8.7</td></tr><tr><th>DEVISE [11]</th><td>16.9</td><td>27.4</td><td>20.9</td><td>23.8</td><td>53.0</td><td>32.8</td><td>17.1</td><td>74.7</td><td>27.8</td><td>4.9</td><td>76.9</td><td>9.2</td></tr><tr><th>SJE [2]</th><td>14.7</td><td>30.5</td><td>19.8</td><td>23.5</td><td>52.9</td><td>33.6</td><td>8.0</td><td>73.9</td><td>14.4</td><td>3.7</td><td>55.7</td><td>6.9</td></tr><tr><th>ESZSL [27]</th><td>11.0</td><td>27.9</td><td>15.8</td><td>12.6</td><td>63.8</td><td>21.0</td><td>5.9</td><td>77.8</td><td>11.0</td><td>2.4</td><td>70.1</td><td>4.6</td></tr><tr><th>SYNC [5]</th><td>7.9</td><td>43.3</td><td>13.4</td><td>11.5</td><td>70.9</td><td>19.8</td><td>10.0</td><td>90.5</td><td>18.0</td><td>7.4</td><td>66.3</td><td>13.3</td></tr><tr><th>SAE [17]</th><td>8.8</td><td>18.0</td><td>11.8</td><td>7.8</td><td>54.0</td><td>13.6</td><td>1.1</td><td>82.2</td><td>2.2</td><td>0.4</td><td>80.9</td><td>0.9</td></tr><tr><th>DEM [38]</th><td>34.3</td><td>20.5</td><td>25.6</td><td>19.6</td><td>57.9</td><td>29.2</td><td>30.5</td><td>86.4</td><td>45.1</td><td>11.1</td><td>79.4</td><td>19.4</td></tr><tr><th>RelationNet [36]</th><td>-</td><td>-</td><td>-</td><td>38.1</td><td>61.1</td><td>47.0</td><td>30</td><td>93.4</td><td>45.3</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PSR-ZSL [3]</th><td>20.8</td><td>37.2</td><td>26.7</td><td>24.6</td><td>54.3</td><td>33.9</td><td>20.7</td><td>73.8</td><td>32.3</td><td>13.5</td><td>51.4</td><td>21.4</td></tr><tr><th>SP-AEN [7]</th><td>24.9</td><td>38.2</td><td>30.3</td><td>34.7</td><td>70.6</td><td>46.6</td><td>23.3</td><td>90.9</td><td>31.1</td><td>13.7</td><td>63.4</td><td>22.6</td></tr><tr><th>CVAE-ZSL [23]</th><td>-</td><td>-</td><td>26.7</td><td>-</td><td>-</td><td>34.5</td><td>-</td><td>-</td><td>51.2</td><td>-</td><td>-</td><td>-</td></tr><tr><th>GDAN</th><td>38.1</td><td>89.9</td><td>53.4</td><td>39.3</td><td>66.7</td><td>49.5</td><td>32.1</td><td>67.5</td><td>43.5</td><td>30.4</td><td>75.0</td><td>43.4</td></tr><tr><th>SE-GZSL* [31]</th><td>40.9</td><td>30.5</td><td>34.9</td><td>41.5</td><td>53.3</td><td>46.7</td><td>58.3</td><td>68.1</td><td>62.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>f-CLSWGAN* [35]</th><td>42.6</td><td>36.6</td><td>39.4</td><td>43.7</td><td>57.7</td><td>49.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Results of generalized zero-shot learning evaluated on four benchmark datasets. *Note that SE-GZSL [31] trains an additional LinearSVC for testing, and that f-CLSWGAN [35] trains additional embedding models for testing, so their results may not be directly comparable with others.", "list_citation_info": ["[27] B. Romera-Paredes and P. Torr. An embarrassingly simple approach to zero-shot learning. In International Conference on Machine Learning, pages 2152\u20132161, 2015.", "[3] Y. Annadani and S. Biswas. Preserving semantic relations for zero-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7603\u20137612, 2018.", "[35] Y. Xian, T. Lorenz, B. Schiele, and Z. Akata. Feature generating networks for zero-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.", "[36] F. S. Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation network for few-shot learning. 2018.", "[23] A. Mishra, M. Reddy, A. Mittal, and H. A. Murthy. A generative model for zero shot learning using conditional variational autoencoders. arXiv preprint arXiv:1709.00663, 2017.", "[39] Z. Zhang and V. Saligrama. Zero-shot learning via semantic similarity embedding. In Proceedings of the IEEE international conference on computer vision, pages 4166\u20134174, 2015.", "[17] E. Kodirov, T. Xiang, and S. Gong. Semantic autoencoder for zero-shot learning. arXiv preprint arXiv:1704.08345, 2017.", "[7] L. Chen, H. Zhang, J. Xiao, W. Liu, and S.-F. Chang. Zero-shot visual recognition using semantics-preserving adversarial embedding network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 2, 2018.", "[38] L. Zhang, T. Xiang, S. Gong, et al. Learning a deep embedding model for zero-shot learning. 2017.", "[11] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems, pages 2121\u20132129, 2013.", "[1] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-embedding for attribute-based classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 819\u2013826, 2013.", "[31] V. K. Verma, G. Arora, A. Mishra, and P. Rai. Generalized zero-shot learning via synthesized examples. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.", "[5] S. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Synthesized classifiers for zero-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5327\u20135336, 2016.", "[2] Z. Akata, S. Reed, D. Walter, H. Lee, and B. Schiele. Evaluation of output embeddings for fine-grained image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2927\u20132936, 2015.", "[33] Y. Xian, Z. Akata, G. Sharma, Q. Nguyen, M. Hein, and B. Schiele. Latent embeddings for zero-shot classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 69\u201377, 2016."]}]}