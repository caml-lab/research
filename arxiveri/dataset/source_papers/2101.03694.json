{"title": "Learning to Segment Rigid Motions from Two Frames", "abstract": "Appearance-based detectors achieve remarkable performance on common scenes, but tend to fail for scenarios lack of training data. Geometric motion segmentation algorithms, however, generalize to novel scenes, but have yet to achieve comparable performance to appearance-based ones, due to noisy motion estimations and degenerate motion configurations. To combine the best of both worlds, we propose a modular network, whose architecture is motivated by a geometric analysis of what independent object motions can be recovered from an egomotion field. It takes two consecutive frames as input and predicts segmentation masks for the background and multiple rigidly moving objects, which are then parameterized by 3D rigid transformations. Our method achieves state-of-the-art performance for rigid motion segmentation on KITTI and Sintel. The inferred rigid motions lead to a significant improvement for depth and scene flow estimation. At the time of submission, our method ranked 1st on KITTI scene flow leaderboard, out-performing the best published method (scene flow error: 4.89% vs 6.31%).", "authors": ["Gengshan Yang", " Deva Ramanan"], "pdf_url": "https://arxiv.org/abs/2101.03694", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th>Method</th><td>K: obj \\uparrow</td><td>K: bg \\uparrow</td><td>S: bg \\uparrow</td></tr><tr><th rowspan=\"4\">(1)</th><th>Mask R-CNN [59]</th><td>88.20</td><td>96.42</td><td>81.98</td></tr><tr><th>U{}^{2} (Saliency) [38]</th><td>64.80{}^{*}</td><td>93.34</td><td>82.01</td></tr><tr><th>MR-Flow-S (K)  [60]</th><td>75.59{}^{*}</td><td>94.70{}^{\\ddagger}</td><td>76.11</td></tr><tr><th>MR-Flow-S (S)  [60]</th><td>11.11{}^{*}</td><td>84.72</td><td>92.64{}^{\\ddagger}</td></tr><tr><th rowspan=\"5\">(2)</th><th>FSEG [25]</th><td>85.08{}^{*}</td><td>96.27</td><td>80.22</td></tr><tr><th>MAT-Net [70]</th><td>68.40{}^{*}</td><td>93.08</td><td>77.95</td></tr><tr><th>COSNet [31]</th><td>66.67{}^{*}</td><td>93.03</td><td>80.86</td></tr><tr><th>CC [40]</th><td>50.87{}^{*}</td><td>85.50</td><td>\u2717</td></tr><tr><th>RTN [32]</th><td>34.29{}^{*}</td><td>84.44</td><td>64.86</td></tr><tr><th rowspan=\"4\">(3)</th><th>FSEG-Motion [25]</th><td>61.29</td><td>89.41</td><td>78.25</td></tr><tr><th>CC-Motion [40]</th><td>42.99</td><td>74.06</td><td>\u2717</td></tr><tr><th>Flow angle [5, 60]</th><td>25.83</td><td>85.52</td><td>74.23</td></tr><tr><th>Ours</th><td>90.71</td><td>97.05</td><td>86.72</td></tr></tbody></table>", "caption": "Table 1: Rigidity estimation on KITTI (K) and Sintel (S) without fine-tuning. {}^{(1)}Single frame. {}^{(2)}Multi-frame with appearance features. {}^{(3)}Multi-frame without appearance. The best result under each metric (IoU in %) is bolded. {}^{*}:For methods only estimating background masks, we use connected components to obtain object masks. {}^{\\ddagger}:Methods trained on target dataset. MR-Flow-S (K) is trained on KITTI, and MR-Flow-S (S) is trained on Sintel.", "list_citation_info": ["[70] Tianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, and Ling Shao. Motion-attentive transition for zero-shot video object segmentation. In AAAI, 2020.", "[38] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. Pattern Recognition, 2020.", "[25] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos. In CVPR, 2017.", "[31] Xiankai Lu, Wenguan Wang, Chao Ma, Jianbing Shen, Ling Shao, and Fatih Porikli. See more, know more: Unsupervised video object segmentation with co-attention siamese networks. In CVPR, 2019.", "[32] Zhaoyang Lv, Kihwan Kim, Alejandro Troccoli, Deqing Sun, James Rehg, and Jan Kautz. Learning rigidity in dynamic scenes with a moving camera for 3d motion field estimation. In ECCV, 2018.", "[59] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.", "[60] Jonas Wulff, Laura Sevilla-Lara, and Michael J. Black. Optical flow in mostly rigid scenes. In CVPR, 2017.", "[40] Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, and Michael J Black. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. In CVPR, 2019.", "[5] Pia Bideau and Erik Learned-Miller. It\u2019s moving! a probabilistic model for causal motion segmentation in moving camera videos. In ECCV, 2016."]}, {"table": "<table><tbody><tr><th>Method</th><td>K: D1 \\downarrow</td><td>K:SF \\downarrow</td><td>S: D1 \\downarrow</td><td>S:SF \\downarrow</td></tr><tr><th>CC [40]</th><td>36.20</td><td>51.80</td><td>\u2717</td><td>\u2717</td></tr><tr><th>SSM [22]</th><td>31.25</td><td>47.05</td><td>\u2717</td><td>\u2717</td></tr><tr><th>Mono-SF [10]</th><td>16.72</td><td>21.60</td><td>\u2717</td><td>\u2717</td></tr><tr><th>MiDaS+OE [65]</th><td>37.27</td><td>44.87</td><td>49.89</td><td>55.43</td></tr><tr><th>MiDaS+Mask</th><td>17.33</td><td>22.47</td><td>39.60</td><td>47.40</td></tr><tr><th>MiDaS+Ours</th><td>16.98</td><td>22.19</td><td>38.29</td><td>46.05</td></tr><tr><th></th><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 2: Monocular depth and scene flow results on KITTI (K) and Sintel (S). D1: first frame disparity (inverse depth) error. SF: scene flow error (%). The best result is underlined, and further bolded if not trained on the target domain data. On Sintel, we evaluate on 330 frame pairs with average flow magnitude greater than 5 pixel.", "list_citation_info": ["[22] Junhwa Hur and Stefan Roth. Self-supervised monocular scene flow estimation. In CVPR, 2020.", "[65] Gengshan Yang and Deva Ramanan. Upgrading optical flow to 3d scene flow through optical expansion. In CVPR, 2020.", "[10] Fabian Brickwedde, Steffen Abraham, and Rudolf Mester. Mono-SF: Multi-view geometry meets single-view depth for monocular scene flow estimation of dynamic traffic scenes. In CVPR, 2019.", "[40] Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim, Deqing Sun, Jonas Wulff, and Michael J Black. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. In CVPR, 2019."]}, {"table": "<table><tbody><tr><th>Method</th><td>D1{}^{*} \\downarrow</td><td>D2 \\downarrow</td><td>Fl \\downarrow</td><td>SF \\downarrow</td></tr><tr><th>PRSM [56]</th><td>4.27</td><td>6.79</td><td>6.68</td><td>8.97</td></tr><tr><th>OpticalExp [65]</th><td>1.81</td><td>4.25</td><td>6.30</td><td>8.12</td></tr><tr><th>DRISF [33]</th><td>2.55</td><td>4.04</td><td>4.73</td><td>6.31</td></tr><tr><th>Ours Mask R-CNN</th><td>1.89</td><td>3.42</td><td>4.26</td><td>5.61</td></tr><tr><th>Ours Rigid Mask</th><td>1.89</td><td>3.23</td><td>3.50</td><td>4.89</td></tr><tr><th></th><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 3: Stereo scene flow results on KITTI benchmark. D1 and D2: first and second frame disparity error. Fl: optical flow error. SF: scene flow error. Metrics are errors in percentage and top results are bolded. {}^{*}First frame disparity is not refined by our method.", "list_citation_info": ["[65] Gengshan Yang and Deva Ramanan. Upgrading optical flow to 3d scene flow through optical expansion. In CVPR, 2020.", "[56] Christoph Vogel, Konrad Schindler, and Stefan Roth. 3D scene flow estimation with a piecewise rigid scene model. IJCV, 2015.", "[33] Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, and Raquel Urtasun. Deep rigid instance scene flow. In CVPR, 2019."]}, {"table": "<table><tbody><tr><th>Method</th><td>K: obj \\uparrow</td><td>K: bg \\uparrow</td><td>S: bg \\uparrow</td></tr><tr><th>Reference</th><td>89.53</td><td>97.22</td><td>84.63</td></tr><tr><th>{}^{(1)}w/o cost maps</th><td>88.66</td><td>96.59</td><td>76.81</td></tr><tr><th>{}^{(2)}w/o uncertainty</th><td>85.09</td><td>95.72</td><td>77.25</td></tr><tr><th>{}^{(3)}w/o monocular depth</th><td>84.46</td><td>94.84</td><td>76.14</td></tr><tr><th>{}^{(4)}w/o expansion (MoA [6])</th><td>81.28</td><td>95.50</td><td>77.00</td></tr><tr><th>{}^{(5)}w/o learning [5, 60]</th><td>25.83</td><td>85.52</td><td>74.23</td></tr><tr><th></th><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 4: Diagnostics of rigid body motion segmentation on KITTI-SF. Dignostics in the second group are sequential.", "list_citation_info": ["[6] Pia Bideau, Rakesh R Menon, and Erik Learned-Miller. Moa-net: self-supervised motion segmentation. In ECCVW, 2018.", "[5] Pia Bideau and Erik Learned-Miller. It\u2019s moving! a probabilistic model for causal motion segmentation in moving camera videos. In ECCV, 2016."]}, {"table": "<table><tbody><tr><th>Parameter</th><td>Value</td></tr><tr><th colspan=\"2\">Optical flow</th></tr><tr><th>Network architecture</th><td>VCN [64]</td></tr><tr><th>Optimizer</th><td>Adam [26]</td></tr><tr><th>Learning rate</th><td>1\\times 10^{-3}</td></tr><tr><th>Batch size / iterations on C</th><td>16 image pairs / 70k</td></tr><tr><th>Batch size / iterations on T</th><td>16 image pairs / 70k</td></tr><tr><th>Batch size / iterations on C+SF+V</th><td>12 image pairs / 70k</td></tr><tr><th colspan=\"2\">Optical expansion</th></tr><tr><th>Network backbone</th><td>U-Net [42, 65]</td></tr><tr><th>Optimizer</th><td>Adam [26]</td></tr><tr><th>Learning rate</th><td>1\\times 10^{-3}</td></tr><tr><th>Batch size / iterations on SF</th><td>12 image pairs / 70k</td></tr><tr><th colspan=\"2\">Rigid motion segmentation</th></tr><tr><th>Network backbone</th><td>U-Net+DLA-34 [42, 65, 67, 71]</td></tr><tr><th>Optimizer</th><td>Adam [26]</td></tr><tr><th>Learning rate</th><td>5\\times 10^{-4}</td></tr><tr><th>Batch size / iterations on SF</th><td>12 image pairs / 70k</td></tr><tr><th></th><td></td></tr></tbody></table>", "caption": "Table 5: Details for network training. C: FlythingChairs [14]. T: FlythingThings [34]. SF: SceneFlow [34]. V: VIPER [41]. The optical flow network is trained sequentially on C, T, and C+SF+V.", "list_citation_info": ["[14] A. Dosovitskiy, P. Fischer, E. Ilg, P. H\u00e4usser, C. Haz\u0131rba\u015f, V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In ICCV, 2015.", "[34] N. Mayer, E. Ilg, P. H\u00e4usser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In CVPR, 2016.", "[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.", "[64] Gengshan Yang and Deva Ramanan. Volumetric correspondence networks for optical flow. In NeurIPS, 2019.", "[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.", "[41] Stephan R Richter, Zeeshan Hayder, and Vladlen Koltun. Playing for benchmarks. In ICCV, pages 2213\u20132222, 2017."]}, {"table": "<table><tbody><tr><td></td><td colspan=\"2\">{}^{*}D1 (%)</td><td colspan=\"2\">D2 (%)</td><td colspan=\"4\">Fl (%)</td><td colspan=\"4\">SF (%)</td></tr><tr><td>Method</td><td>all \\downarrow</td><td>fg \\downarrow</td><td>all \\downarrow</td><td>fg \\downarrow</td><td>all \\downarrow</td><td>\\Delta-all \\uparrow</td><td>fg \\downarrow</td><td>\\Delta-fg \\uparrow</td><td>all \\downarrow</td><td>\\Delta-all \\uparrow</td><td>fg \\downarrow</td><td>\\Delta-fg \\uparrow</td></tr><tr><td>Baseline OE [65]</td><td>1.41</td><td>0.76</td><td>2.45</td><td>0.91</td><td>4.02</td><td>0</td><td>2.50</td><td>0</td><td>5.12</td><td>0</td><td>3.07</td><td>0</td></tr><tr><td>Ours Mask R-CNN</td><td>1.41</td><td>0.76</td><td>2.11</td><td>1.99</td><td>3.53</td><td>12.1</td><td>4.34</td><td>-73.6</td><td>4.02</td><td>21.5</td><td>4.86</td><td>-36.8</td></tr><tr><td>Ours Rigid Mask</td><td>1.41</td><td>0.76</td><td>2.04</td><td>1.05</td><td>3.32</td><td>17.4</td><td>2.16</td><td>15.7</td><td>3.86</td><td>24.6</td><td>2.78</td><td>10.4</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 6: Ablation study of stereo scene flow on KITTI-SF images. D1 and D2: first and second frame disparity error. Fl: optical flow error. all: evaluated on all pixels. fg: evaluated on foreground pixels only. SF: scene flow error. \\Delta: percentage of error reduction after refinement. {}^{*}First frame disparity does not change during refinement.", "list_citation_info": ["[65] Gengshan Yang and Deva Ramanan. Upgrading optical flow to 3d scene flow through optical expansion. In CVPR, 2020."]}]}