{"title": "End-to-end semi-supervised object detection with soft teacher", "abstract": "This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detection training. We also propose two simple yet effective techniques within this framework: a soft teacher mechanism where the classification loss of each unlabeled bounding box is weighed by the classification score produced by the teacher network; a box jittering approach to select reliable pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1\\%, 5\\% and 10\\%. Moreover, our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still significantly improve the detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pre-trained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art.", "authors": ["Mengde Xu", " Zheng Zhang", " Han Hu", " Jianfeng Wang", " Lijuan Wang", " Fangyun Wei", " Xiang Bai", " Zicheng Liu"], "pdf_url": "https://arxiv.org/abs/2106.09018", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">detector</th><th rowspan=\"2\">method</th><th colspan=\"2\">val2017</th><th colspan=\"2\">test-dev2017</th></tr><tr><th>\\text{mAP}^{\\text{det}}</th><th>\\text{mAP}^{\\text{mask}}</th><th>\\text{mAP}^{\\text{det}}</th><th>\\text{mAP}^{\\text{mask}}</th></tr></thead><tbody><tr><td rowspan=\"3\">HTC++(Swin-L) w/ single-scale</td><td>supervised</td><td>57.1</td><td>49.6</td><td>-</td><td>-</td></tr><tr><td>ours</td><td>59.1</td><td>51.0</td><td>-</td><td>-</td></tr><tr><td>ours{}^{*}</td><td>60.1</td><td>51.9</td><td>-</td><td>-</td></tr><tr><td rowspan=\"3\">HTC++(Swin-L) w/ multi-scale</td><td>supervised</td><td>58.2</td><td>50.5</td><td>58.9</td><td>51.2</td></tr><tr><td>ours</td><td>59.9</td><td>51.9</td><td>60.4</td><td>52.4</td></tr><tr><td>ours{}^{*}</td><td>60.7</td><td>52.5</td><td>61.3</td><td>53.0</td></tr></tbody></table>", "caption": "Table 1: On the state-of-the-art detector HTC++(Swin-L), our method surpasses the supervised learning on both val2017 and test-dev2017. * indicates that models are pre-trained with Object365 [24] dataset.", "list_citation_info": ["[24] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In CVPR, 2019."]}, {"table": "<table><thead><tr><th>Augmentation</th><th>Labeled image training</th><th>Unlabeled image training</th><th>Pseudo-label generation</th></tr></thead><tbody><tr><td>Scale jitter</td><td>short edge \\in(0.5,1.5)</td><td>short edge \\in(0.5,1.5)</td><td>short edge \\in(0.5,1.5)</td></tr><tr><td>Solarize jitter</td><td>p=0.25, ratio \\in(0,1)</td><td>p=0.25, ratio \\in(0,1)</td><td>-</td></tr><tr><td>Brightness jitter</td><td>p=0.25, ratio \\in(0,1)</td><td>p=0.25, ratio \\in(0,1)</td><td>-</td></tr><tr><td>Constrast jitter</td><td>p=0.25, ratio \\in(0,1)</td><td>p=0.25, ratio \\in(0,1)</td><td>-</td></tr><tr><td>Sharpness jitter</td><td>p=0.25, ratio \\in(0,1)</td><td>p=0.25, ratio \\in(0,1)</td><td>-</td></tr><tr><td>Translation</td><td>-</td><td>p=0.3, translation ratio \\in(0,0.1)</td><td>-</td></tr><tr><td>Rotate</td><td>-</td><td>p=0.3, angle \\in(0,30^{\\circ})</td><td>-</td></tr><tr><td>Shift</td><td>-</td><td>p=0.3, angle \\in(0,30^{\\circ})</td><td>-</td></tr><tr><td>Cutout</td><td>num \\in(1,5), ratio \\in(0.05,0.2)</td><td>num \\in(1,5), ratio \\in(0.05,0.2)</td><td>-</td></tr></tbody></table>", "caption": "Table 2: The summary of the data augmentation used in our approach. We follow the practice of STAC [27] and FixMatch [26] to provide different data augmentation for pseudo-label generation, labeled image training and unlabeled image training. \u201c-\u201d indicates the augmentation is not used.", "list_citation_info": ["[26] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. NIPS, 2020.", "[27] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><td>1%</td><td>5%</td><td>10%</td></tr><tr><th>Supervised baseline (Ours)</th><td>10.0\\pm{0.26}</td><td>20.92\\pm{0.15}</td><td>26.94\\pm{0.111}</td></tr><tr><th>Supervised baseline (STAC) [27]</th><td>9.83\\pm{0.23}</td><td>21.18\\pm{0.20}</td><td>26.18\\pm{0.12}</td></tr><tr><th>STAC [27]</th><td>13.97\\pm{0.35}</td><td>24.38\\pm{0.12}</td><td>28.64\\pm{0.21}</td></tr><tr><th>Ours</th><td>20.46\\pm{0.39}</td><td>30.74\\pm{0.08}</td><td>34.04\\pm{0.14}</td></tr></tbody></table>", "caption": "Table 3: System level comparison with STAC on val2017 under the Partially Labeled Data setting. All the results are the average of all 5 folds. For benchmarking, we also compare the supervised benchmark performance between our method and STAC, and their performance is similar.", "list_citation_info": ["[27] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Extra dataset</th><th>mAP</th></tr></thead><tbody><tr><td>Proposal learning [28]</td><td>unlabeled2017</td><td>37.4\\xrightarrow{\\text{+1.0}}38.4</td></tr><tr><td>STAC [27]</td><td>unlabeled2017</td><td>39.5\\xrightarrow{\\text{-0.3}}39.2</td></tr><tr><td>Self-training [36]</td><td>ImageNet+OpenImages</td><td>41.1\\xrightarrow{\\text{+0.8}}41.9</td></tr><tr><td>Ours</td><td>unlabeled2017</td><td>40.9\\xrightarrow{\\textbf{+3.6}}44.5</td></tr></tbody></table>", "caption": "Table 4: Comparison with other state-of-the-arts under the setting of using all data of train2017 set. Particularly, Self-training uses ImageNet (1.2M images) and OpenImages (1.7M images) as additional unlabeled images, which is 20\\times larger than unlabeled2017 (123k images).", "list_citation_info": ["[36] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training and self-training. NIPS, 2020.", "[28] Peng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, and Caiming Xiong. Proposal learning for semi-supervised object detection. In WACV, 2021.", "[27] Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-supervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020."]}]}