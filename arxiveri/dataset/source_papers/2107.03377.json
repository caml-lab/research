{"title": "Long short-term transformer for online action detection", "abstract": "We present Long Short-term TRansformer (LSTR), a temporal modeling algorithm for online action detection, which employs a long- and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the fine-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efficient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS Segment. Code has been made available at: https://xumingze0308.github.io/projects/lstr", "authors": ["Mingze Xu", " Yuanjun Xiong", " Hao Chen", " Xinyu Li", " Wei Xia", " Zhuowen Tu", " Stefano Soatto"], "pdf_url": "https://arxiv.org/abs/2107.03377", "list_table_and_caption": [{"table": "<table><tr><td></td><td rowspan=\"2\"> Features </td><td colspan=\"10\">Portion of Video</td></tr><tr><td></td><td> 0-10% </td><td> 10-20% </td><td> 20-30% </td><td> 30-40% </td><td> 40-50% </td><td> 50-60% </td><td> 60-70% </td><td> 70-80% </td><td> 80-90% </td><td> 90-100% </td></tr><tr><td>TRN Xu et al. (2019)</td><td rowspan=\"4\"> ActivityNet </td><td>78.8</td><td>79.6</td><td>80.4</td><td>81.0</td><td>81.6</td><td>81.9</td><td>82.3</td><td>82.7</td><td>82.9</td><td>83.3</td></tr><tr><td>IDN Eun et al. (2020)</td><td>80.6</td><td>81.1</td><td>81.9</td><td>82.3</td><td>82.6</td><td>82.8</td><td>82.6</td><td>82.9</td><td>83.0</td><td>83.9</td></tr><tr><td>TFN Eun et al. (2021)</td><td>83.1</td><td>84.4</td><td>85.4</td><td>85.8</td><td>87.1</td><td>88.4</td><td>87.6</td><td>87.0</td><td>86.7</td><td>85.6</td></tr><tr><td>LSTR (ours)</td><td>83.6</td><td>85.0</td><td>86.3</td><td>87.0</td><td>87.8</td><td>88.5</td><td>88.6</td><td>88.9</td><td>89.0</td><td>88.9</td></tr><tr><td>IDN Eun et al. (2020)</td><td rowspan=\"3\">Kinetics</td><td>81.7</td><td>81.9</td><td>83.1</td><td>82.9</td><td>83.2</td><td>83.2</td><td>83.2</td><td>83.0</td><td>83.3</td><td>86.6</td></tr><tr><td>PKD Zhao et al. (2020)</td><td>82.1</td><td>83.5</td><td>86.1</td><td>87.2</td><td>88.3</td><td>88.4</td><td>89.0</td><td>88.7</td><td>88.9</td><td>87.7</td></tr><tr><td>LSTR (ours)</td><td>84.4</td><td>85.6</td><td>87.2</td><td>87.8</td><td>88.8</td><td>89.4</td><td>89.6</td><td>89.9</td><td>90.0</td><td>90.1</td></tr></table>", "caption": "Table 2: Online action detection results when only portions of videos are consideredin cAP (%) on TVSeries (e.g., 80%-90% means only frames of this range of action instances were evaluated).LSTR outperforms existing methods at every time stage, especially on boundary locations.", "list_citation_info": ["Zhao et al. [2020] Peisen Zhao, Jiajie Wang, Lingxi Xie, Ya Zhang, Yanfeng Wang, and Qi Tian. Privileged knowledge distillation for online action detection. arXiv:2011.09158, 2020.", "Xu et al. [2019] Mingze Xu, Mingfei Gao, Yi-Ting Chen, Larry S Davis, and David J Crandall. Temporal recurrent networks for online action detection. In ICCV, 2019.", "Eun et al. [2021] Hyunjun Eun, Jinyoung Moon, Jongyoul Park, Chanho Jung, and Changick Kim. Temporal filtering networks for online action detection. Pattern Recognition, 2021.", "Eun et al. [2020] Hyunjun Eun, Jinyoung Moon, Jongyoul Park, Chanho Jung, and Changick Kim. Learning to discriminate information for online action detection. In CVPR, 2020."]}]}