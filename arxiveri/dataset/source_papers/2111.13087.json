{"title": "Boxer: Box-attention for 2d and 3d transformers", "abstract": "In this paper, we propose a simple attention mechanism, we call box-attention. It enables spatial interaction between grid features, as sampled from boxes of interest, and improves the learning capability of transformers for several vision tasks. Specifically, we present BoxeR, short for Box Transformer, which attends to a set of boxes by predicting their transformation from a reference window on an input feature map. The BoxeR computes attention weights on these boxes by considering its grid structure. Notably, BoxeR-2D naturally reasons about box information within its attention module, making it suitable for end-to-end instance detection and segmentation tasks. By learning invariance to rotation in the box-attention module, BoxeR-3D is capable of generating discriminative information from a bird's-eye view plane for 3D end-to-end object detection. Our experiments demonstrate that the proposed BoxeR-2D achieves state-of-the-art results on COCO detection and instance segmentation. Besides, BoxeR-3D improves over the end-to-end 3D object detection baseline and already obtains a compelling performance for the vehicle category of Waymo Open, without any class-specific optimization. Code is available at https://github.com/kienduynguyen/BoxeR.", "authors": ["Duy-Kien Nguyen", " Jihong Ju", " Olaf Booij", " Martin R. Oswald", " Cees G. M. Snoek"], "pdf_url": "https://arxiv.org/abs/2111.13087", "list_table_and_caption": [{"table": "<table><tr><td></td><td>FLOPs\\downarrow</td><td>AP\\uparrow</td><td>\\text{AP}_{\\text{S}}\\!\\uparrow</td><td>\\text{AP}_{\\text{M}}\\!\\uparrow</td><td>\\text{AP}_{\\text{L}}\\!\\uparrow</td></tr><tr><td>Self-Attention [40]</td><td>187G</td><td>36.2</td><td>16.3</td><td>39.2</td><td>53.9</td></tr><tr><td>Deformable-Attention{}^{\\dagger} [48]</td><td>173G</td><td>46.9</td><td>29.6</td><td>50.1</td><td>61.6</td></tr><tr><td>Dynamic-Attention [4]</td><td>-</td><td>47.2</td><td>28.6</td><td>49.3</td><td>59.1</td></tr><tr><td>Box-Attention (Ours)</td><td>167G</td><td>48.7</td><td>31.6</td><td>52.3</td><td>63.2</td></tr><tr><td>w/o (\\mathcal{F}_{t} and \\mathcal{F}_{s})</td><td>164G</td><td>46.4</td><td>29.6</td><td>49.8</td><td>59.7</td></tr></table><p>{}^{\\dagger} Based on author-provided github, which is higher than in their original paper. </p>", "caption": "Table 1: Box-Attention vs. alternatives in end-to-end object detection on the COCO val set using a R-50 backbone pretrained on ImageNet. Box-Attention performs best with the least FLOPs.", "list_citation_info": ["[48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In ICLR, 2021.", "[4] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, and Lei Zhang. Dynamic DETR: End-to-end object detection with dynamic attention. In ICCV, 2021.", "[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017."]}, {"table": "<table><tr><td>Method</td><td>Backbone</td><td>Epochs</td><td>end-to-end</td><td>AP\\uparrow</td><td>\\text{AP}_{\\text{50}}\\!\\!\\uparrow</td><td>\\text{AP}_{\\text{75}}\\!\\!\\uparrow</td><td>\\text{AP}_{\\text{S}}\\!\\!\\uparrow</td><td>\\text{AP}_{\\text{M}}\\!\\!\\uparrow</td><td>\\text{AP}_{\\text{L}}\\!\\!\\uparrow</td></tr><tr><td>Faster RCNN-FPN [31]</td><td>R-101</td><td>36</td><td>\u2717</td><td>36.2</td><td>59.1</td><td>39.0</td><td>18.2</td><td>39.0</td><td>48.2</td></tr><tr><td>ATSS [47]</td><td>R-101</td><td>24</td><td>\u2717</td><td>43.6</td><td>62.1</td><td>47.4</td><td>26.1</td><td>47.0</td><td>53.6</td></tr><tr><td>Sparse RCNN [37]</td><td>X-101</td><td>36</td><td>\u2713</td><td>46.9</td><td>66.3</td><td>51.2</td><td>28.6</td><td>49.2</td><td>58.7</td></tr><tr><td>VFNet [46]</td><td>R-101</td><td>24</td><td>\u2717</td><td>46.7</td><td>64.9</td><td>50.8</td><td>28.4</td><td>50.2</td><td>57.6</td></tr><tr><td>Deformable DETR [48]</td><td>R-50</td><td>50</td><td>\u2713</td><td>46.9</td><td>66.4</td><td>50.8</td><td>27.7</td><td>49.7</td><td>59.9</td></tr><tr><td>Deformable DETR [48]</td><td>R-101</td><td>50</td><td>\u2713</td><td>48.7</td><td>68.1</td><td>52.9</td><td>29.1</td><td>51.5</td><td>62.0</td></tr><tr><td>Dynamic DETR [4]</td><td>R-50</td><td>50</td><td>\u2713</td><td>47.2</td><td>65.9</td><td>51.1</td><td>28.6</td><td>49.3</td><td>59.1</td></tr><tr><td>TSP-RCNN [38]</td><td>R-101</td><td>96</td><td>\u2713</td><td>46.6</td><td>66.2</td><td>51.3</td><td>28.4</td><td>49.0</td><td>58.5</td></tr><tr><td>BoxeR-2D</td><td>R-50</td><td>50</td><td>\u2713</td><td>50.0</td><td>67.9</td><td>54.7</td><td>30.9</td><td>52.8</td><td>62.6</td></tr><tr><td>BoxeR-2D (3\\times schedule)</td><td>R-50</td><td>36</td><td>\u2713</td><td>49.9</td><td>68.0</td><td>54.4</td><td>30.9</td><td>52.6</td><td>62.5</td></tr><tr><td>BoxeR-2D (3\\times schedule)</td><td>R-101</td><td>36</td><td>\u2713</td><td>51.1</td><td>68.5</td><td>55.8</td><td>31.5</td><td>54.1</td><td>64.6</td></tr></table>", "caption": "Table 4: Comparison of BoxeR-2D in object detection on the COCO 2017 test-dev set with various backbone networks. BoxeR-2D outperforms other methods including transformer-based object detectors with a faster training schedule.", "list_citation_info": ["[47] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In CVPR, 2020.", "[38] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set prediction for object detection. In ICCV, 2021.", "[4] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, and Lei Zhang. Dynamic DETR: End-to-end object detection with dynamic attention. In ICCV, 2021.", "[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.", "[48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In ICLR, 2021.", "[46] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko S\u00fcnderhauf. Varifocalnet: An iou-aware dense object detector. In CVPR, 2021.", "[37] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse r-cnn: End-to-end object detection with learnable proposals. In CVPR, 2021."]}, {"table": "<table><tr><td></td><td>Epoch</td><td>end-to-end</td><td>AP\\uparrow</td><td>\\text{AP}_{\\text{S}}\\!\\!\\uparrow</td><td>\\text{AP}_{\\text{M}}\\!\\!\\uparrow</td><td>\\text{AP}_{\\text{L}}\\!\\!\\uparrow</td><td>AP{}^{\\text{m}}\\!\\!\\uparrow</td><td>\\text{AP}^{\\text{m}}_{\\text{S}}\\!\\!\\uparrow</td><td>\\text{AP}^{\\text{m}}_{\\text{M}}\\!\\!\\uparrow</td><td>\\text{AP}^{\\text{m}}_{\\text{L}}\\!\\!\\uparrow</td></tr><tr><td>Mask R-CNN [11]</td><td>36</td><td>\u2717</td><td>43.1</td><td>25.1</td><td>46.0</td><td>54.3</td><td>38.8</td><td>21.8</td><td>41.4</td><td>50.5</td></tr><tr><td>QueryInst [9]</td><td>36</td><td>\u2717</td><td>48.1</td><td>-</td><td>-</td><td>-</td><td>42.8</td><td>24.6</td><td>45.0</td><td>55.5</td></tr><tr><td>SOLQ [6]</td><td>50</td><td>\u2713</td><td>48.7</td><td>28.6</td><td>51.7</td><td>63.1</td><td>40.9</td><td>22.5</td><td>43.8</td><td>54.6</td></tr><tr><td>BoxeR-2D (3\\times schedule)</td><td>36</td><td>\u2713</td><td>51.1</td><td>31.5</td><td>54.1</td><td>64.6</td><td>43.8</td><td>25.0</td><td>46.5</td><td>57.9</td></tr></table>", "caption": "Table 5: Comparison of BoxeR-2D in instance segmentation on the COCO 2017 test-dev set using a R-101 backbone. BoxeR-2D shows better results in both detection and instance segmentation.", "list_citation_info": ["[6] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. SOLQ: Segmenting objects by learning queries. In NeurIPS, 2021.", "[11] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. In ICCV, 2017.", "[9] Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, and Wenyu Liu. Instances as queries. In ICCV, 2021."]}, {"table": "<table><tr><td></td><td rowspan=\"2\">end-to-end</td><td colspan=\"2\">Vehicle</td><td colspan=\"2\">Pedestrian</td></tr><tr><td></td><td>AP\\uparrow</td><td>APH\\uparrow</td><td>AP\\uparrow</td><td>APH\\uparrow</td></tr><tr><td>PointPillar [18]</td><td>\u2717</td><td>55.2</td><td>54.7</td><td>60.0</td><td>49.1</td></tr><tr><td>PV-RCNN [33]</td><td>\u2717</td><td>65.4</td><td>64.8</td><td>-</td><td>-</td></tr><tr><td>RSN S_1f [36]</td><td>\u2717</td><td>63.0</td><td>62.6</td><td>65.4</td><td>60.7</td></tr><tr><td>Deformable DETR [48]</td><td>\u2713</td><td>59.6</td><td>59.2</td><td>45.8</td><td>36.2</td></tr><tr><td>BoxeR-3D</td><td>\u2713</td><td>63.9</td><td>63.7</td><td>61.5</td><td>53.7</td></tr></table>", "caption": "Table 6: Comparison of BoxeR-3D in 3D object detection on the Waymo Open val set (LEVEL_2 difficulty). Despite the lack of any class-specific optimization, BoxeR-3D is surprisingly effective and even competitive on the Vehicle category.", "list_citation_info": ["[48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In ICLR, 2021.", "[33] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: point-voxel feature set abstraction for 3d object detection. In CVPR, 2020.", "[36] Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang, Cristian Sminchisescu, and Dragomir Anguelov. RSN: range sparse net for efficient, accurate lidar 3d object detection. In CVPR, 2021.", "[18] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019."]}]}