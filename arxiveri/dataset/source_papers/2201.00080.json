{"title": "Patchtrack: Multiple Object Tracking Using Frame Patches", "abstract": "Object motion and object appearance are commonly used information in multiple object tracking (MOT) applications, either for associating detections across frames in tracking-by-detection methods or direct track predictions for joint-detection-and-tracking methods. However, not only are these two types of information often considered separately, but also they do not help optimize the usage of visual information from the current frame of interest directly. In this paper, we present PatchTrack, a Transformer-based joint-detection-and-tracking system that predicts tracks using patches of the current frame of interest. We use the Kalman filter to predict the locations of existing tracks in the current frame from the previous frame. Patches cropped from the predicted bounding boxes are sent to the Transformer decoder to infer new tracks. By utilizing both object motion and object appearance information encoded in patches, the proposed method pays more attention to where new tracks are more likely to occur. We show the effectiveness of PatchTrack on recent MOT benchmarks, including MOT16 (MOTA 73.71%, IDF1 65.77%) and MOT17 (MOTA 73.59%, IDF1 65.23%). The results are published on https://motchallenge.net/method/MOT=4725&chl=10.", "authors": ["Xiaotong Chen", " Seyed Mehdi Iranmanesh", " Kuo-Chin Lien"], "pdf_url": "https://arxiv.org/abs/2201.00080", "list_table_and_caption": [{"table": "<table><thead><tr><th><p>Dataset</p></th><th><p>Method</p></th><th><p>MOTA\\uparrow</p></th><th><p>IDF1\\uparrow</p></th><th><p>MT\\uparrow</p></th><th><p>ML\\downarrow</p></th><th><p>FP\\downarrow</p></th><th><p>FN\\downarrow</p></th><th><p>IDsw\\downarrow</p></th></tr></thead><tbody><tr><th rowspan=\"16\"><p>MOT16</p></th><th><p>DeepSORT [44]</p></th><td><p>61.4</p></td><td><p>62.2</p></td><td><p>32.8</p></td><td><p>18.2</p></td><td><p>12,852</p></td><td><p>56,668</p></td><td><p>781</p></td></tr><tr><th><p>HTA [21]</p></th><td><p>62.4</p></td><td><p>64.2</p></td><td><p>37.5</p></td><td><p>12.1</p></td><td><p>19,071</p></td><td><p>47,839</p></td><td><p>1,619</p></td></tr><tr><th><p>VMaxx [41]</p></th><td><p>62.6</p></td><td><p>49.2</p></td><td><p>32.7</p></td><td><p>21.1</p></td><td><p>10,604</p></td><td><p>56,182</p></td><td><p>1,389</p></td></tr><tr><th><p>RAR16 [9]</p></th><td><p>63.0</p></td><td><p>63.8</p></td><td><p>39.9</p></td><td><p>22.1</p></td><td><p>13,663</p></td><td><p>53,248</p></td><td><p>482</p></td></tr><tr><th><p>TAP [55]</p></th><td><p>64.8</p></td><td>73.5</td><td><p>40.6</p></td><td><p>22.0</p></td><td><p>12,980</p></td><td><p>50,635</p></td><td><p>794</p></td></tr><tr><th><p>CNNMTT [23]</p></th><td><p>65.2</p></td><td><p>62.2</p></td><td><p>32.4</p></td><td><p>21.3</p></td><td><p>6,578</p></td><td><p>55,896</p></td><td><p>946</p></td></tr><tr><th><p>POI [49]</p></th><td><p>66.1</p></td><td><p>65.1</p></td><td><p>34.0</p></td><td><p>21.3</p></td><td>5,061</td><td><p>55,914</p></td><td><p>805</p></td></tr><tr><th><p>GSDT [42]</p></th><td><p>66.7</p></td><td><p>69.2</p></td><td><p>38.6</p></td><td><p>19.0</p></td><td><p>14,754</p></td><td><p>45,057</p></td><td><p>959</p></td></tr><tr><th><p>TubeTK [27]</p></th><td><p>66.9</p></td><td><p>62.2</p></td><td><p>39.0</p></td><td><p>18.1</p></td><td><p>11,544</p></td><td><p>47,502</p></td><td><p>1,236</p></td></tr><tr><th><p>LM_CNN [1]</p></th><td><p>67.4</p></td><td><p>61.2</p></td><td><p>38.2</p></td><td><p>19.2</p></td><td><p>10,109</p></td><td><p>48,435</p></td><td><p>931</p></td></tr><tr><th><p>Chain-Tracker [29]</p></th><td><p>67.6</p></td><td><p>57.2</p></td><td><p>32.9</p></td><td><p>23.1</p></td><td><p>8,934</p></td><td><p>48,305</p></td><td><p>1,897</p></td></tr><tr><th><p>KDNT(POI) [49]</p></th><td><p>68.2</p></td><td><p>60.0</p></td><td><p>41.0</p></td><td><p>19.0</p></td><td><p>11,479</p></td><td><p>45,605</p></td><td><p>933</p></td></tr><tr><th><p>FairMOT [51]</p></th><td><p>69.3</p></td><td><p>72.3</p></td><td><p>40.3</p></td><td><p>16.7</p></td><td><p>13,501</p></td><td><p>41,653</p></td><td><p>815</p></td></tr><tr><th><p>QuasiDense [28]</p></th><td><p>69.8</p></td><td><p>67.1</p></td><td><p>41.6</p></td><td><p>19.8</p></td><td><p>9,861</p></td><td><p>44,050</p></td><td><p>1,097</p></td></tr><tr><th><p>TraDeS [45]</p></th><td><p>70.1</p></td><td><p>64.7</p></td><td><p>37.3</p></td><td><p>20.0</p></td><td><p>8,091</p></td><td><p>45,210</p></td><td><p>1,144</p></td></tr><tr><th><p>LMP_p [37]</p></th><td><p>71.0</p></td><td><p>70.1</p></td><td>46.9</td><td><p>21.9</p></td><td><p>7,880</p></td><td><p>44,564</p></td><td>434</td></tr><tr><th></th><th>PatchTrack (Ours)</th><td>73.3</td><td><p>65.8</p></td><td><p>45.7</p></td><td>11.3</td><td><p>10,660</p></td><td>36,824</td><td><p>1,179</p></td></tr></tbody></table>", "caption": "Table 1: Evaluation on the MOT16 test set. We evaluate recent MOT systems on the MOT16 test set in the private detection protocol. The method names are taken directly from the leaderboard of motchallenge, where the names in parentheses are associated with their respective literatures. Metrics with \\uparrow means higher numbers are preferable, while the ones with \\downarrow means lower numbers are preferable. Numbers are marked in bold if they are the best in their respective metric columns. Our proposed PatchTrack achieves best results in MOTA, ML, and FN.", "list_citation_info": ["[29] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In European Conference on Computer Vision, pages 145\u2013161. Springer, 2020.", "[55] Zongwei Zhou, Junliang Xing, Mengdan Zhang, and Weiming Hu. Online multi-target tracking with tensor-based high-order graph matching. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 1809\u20131814. IEEE, 2018.", "[45] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12352\u201312361, 2021.", "[27] Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, and Cewu Lu. Tubetk: Adopting tubes to track multi-object in a one-step training model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6308\u20136318, 2020.", "[1] Maryam Babaee, Zimu Li, and Gerhard Rigoll. A dual cnn\u2013rnn for multiple people tracking. Neurocomputing, 368:69\u201383, 2019.", "[49] Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi, and Junjie Yan. Poi: Multiple object tracking with high performance detection and appearance feature. In European Conference on Computer Vision, pages 36\u201342. Springer, 2016.", "[42] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13708\u201313715. IEEE, 2021.", "[51] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. arXiv preprint arXiv:2004.01888, 2020.", "[21] Xufeng Lin, Chang-Tsun Li, Victor Sanchez, and Carsten Maple. On the detection-to-track association for online multi-object tracking. Pattern Recognition Letters, 146:200\u2013207, 2021.", "[9] Kuan Fang, Yu Xiang, Xiaocheng Li, and Silvio Savarese. Recurrent autoregressive networks for online multi-object tracking. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 466\u2013475. IEEE, 2018.", "[41] Xingyu Wan, Jinjun Wang, Zhifeng Kong, Qing Zhao, and Shunming Deng. Multi-object tracking using online metric learning with long short-term memory. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 788\u2013792. IEEE, 2018.", "[28] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 164\u2013173, 2021.", "[23] Nima Mahmoudi, Seyed Mohammad Ahadi, and Mohammad Rahmati. Multi-target tracking using cnn-based features: Cnnmtt. Multimedia Tools and Applications, 78(6):7077\u20137096, 2019.", "[37] Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt Schiele. Multiple people tracking by lifted multicut and person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3539\u20133548, 2017.", "[44] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In 2017 IEEE International Conference on Image Processing (ICIP), pages 3645\u20133649. IEEE, 2017."]}, {"table": "<table><tbody><tr><th><p>Dataset</p></th><th><p>(CNN-based) method</p></th><td><p>MOTA\\uparrow</p></td><td><p>IDF1\\uparrow</p></td><td><p>MT\\uparrow</p></td><td><p>ML\\downarrow</p></td><td><p>FP\\downarrow</p></td><td><p>FN\\downarrow</p></td><td><p>IDsw\\downarrow</p></td></tr><tr><th rowspan=\"23\"><p>MOT17</p></th><th><p>DAN [36]</p></th><td><p>52.4</p></td><td><p>49.5</p></td><td><p>21.4</p></td><td><p>30.7</p></td><td><p>25,423</p></td><td><p>234,592</p></td><td><p>8,431</p></td></tr><tr><th><p>TubeTK [27]</p></th><td><p>63.0</p></td><td><p>58.6</p></td><td><p>31.2</p></td><td><p>19.9</p></td><td><p>27,060</p></td><td><p>177,483</p></td><td><p>4,137</p></td></tr><tr><th><p>GSDT [42]</p></th><td><p>66.2</p></td><td><p>63.4</p></td><td><p>36.9</p></td><td><p>21.7</p></td><td><p>25,800</p></td><td><p>164,120</p></td><td><p>2,711</p></td></tr><tr><th><p>Chained-Tracker [29]</p></th><td><p>66.6</p></td><td><p>57.4</p></td><td><p>37.8</p></td><td><p>18.5</p></td><td><p>22,284</p></td><td><p>160,491</p></td><td><p>5,529</p></td></tr><tr><th><p>CenterTrack [53]</p></th><td><p>67.8</p></td><td><p>64.7</p></td><td><p>34.6</p></td><td><p>24.6</p></td><td>18,498</td><td><p>160,332</p></td><td><p>3,039</p></td></tr><tr><th><p>QuasiDense [28]</p></th><td><p>68.7</p></td><td><p>66.3</p></td><td><p>40.6</p></td><td><p>21.9</p></td><td><p>26,589</p></td><td><p>146,643</p></td><td><p>3,378</p></td></tr><tr><th><p>TraDes [45]</p></th><td><p>69.1</p></td><td><p>63.9</p></td><td><p>36.4</p></td><td><p>21.5</p></td><td><p>20,892</p></td><td><p>150,060</p></td><td><p>3,555</p></td></tr><tr><th><p>MAT [14]</p></th><td><p>69.5</p></td><td><p>63.1</p></td><td><p>43.8</p></td><td><p>18.9</p></td><td><p>30,660</p></td><td><p>138,741</p></td><td><p>2,844</p></td></tr><tr><th><p>SOTMOT [52]</p></th><td><p>71.0</p></td><td><p>71.9</p></td><td><p>42.7</p></td><td><p>15.3</p></td><td><p>39,537</p></td><td><p>118,983</p></td><td><p>5,184</p></td></tr><tr><th><p>RADTrack (RelationTrack) [48]</p></th><td><p>73.1</p></td><td><p>73.7</p></td><td><p>39.9</p></td><td><p>20.0</p></td><td><p>25,935</p></td><td><p>122,700</p></td><td><p>3,021</p></td></tr><tr><th><p>GSDT [42]</p></th><td><p>73.2</p></td><td><p>66.5</p></td><td><p>41.7</p></td><td><p>17.5</p></td><td><p>26,397</p></td><td><p>120,666</p></td><td><p>3,891</p></td></tr><tr><th><p>Semi-TCL [18]</p></th><td><p>73.3</p></td><td><p>73.2</p></td><td><p>41.8</p></td><td><p>18.7</p></td><td><p>22,944</p></td><td><p>124,980</p></td><td><p>2,790</p></td></tr><tr><th><p>FairMOT [51]</p></th><td><p>73.7</p></td><td><p>72.3</p></td><td><p>43.2</p></td><td><p>17.3</p></td><td><p>27,507</p></td><td><p>117,477</p></td><td><p>3,303</p></td></tr><tr><th><p>RelationTrack [48]</p></th><td><p>73.8</p></td><td>74.7</td><td><p>41.7</p></td><td><p>23.2</p></td><td><p>27,999</p></td><td><p>118,623</p></td><td>1,374</td></tr><tr><th><p>PermaTrackPr [38]</p></th><td><p>73.8</p></td><td><p>68.9</p></td><td><p>43.8</p></td><td><p>17.2</p></td><td><p>28,998</p></td><td><p>115,104</p></td><td><p>3,699</p></td></tr><tr><th><p>CSTrack [19]</p></th><td>74.9</td><td><p>72.6</p></td><td><p>41.5</p></td><td><p>17.5</p></td><td><p>23,847</p></td><td>114,303</td><td><p>3,567</p></td></tr><tr><th>PatchTrack (ours)</th><td><p>73.6</p></td><td><p>65.2</p></td><td>44.6</td><td>12.5</td><td><p>23,976</p></td><td><p>121,230</p></td><td><p>3,795</p></td></tr><tr><th><p>Transformer-based method</p></th><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th><p>MOTR [50]</p></th><td><p>65.1</p></td><td>66.4</td><td><p>33.0</p></td><td><p>25.2</p></td><td><p>45,486</p></td><td><p>149,307</p></td><td>2,049</td></tr><tr><th><p>TrackFormer [24]</p></th><td><p>65.0</p></td><td><p>63.9</p></td><td><p>45.6</p></td><td><p>13.8</p></td><td><p>70,443</p></td><td><p>123,552</p></td><td>3,528</td></tr><tr><th><p>MOTPrivate (TransCenter) [46]</p></th><td><p>70.0</p></td><td><p>62.1</p></td><td><p>38.9</p></td><td><p>20.4</p></td><td><p>28,119</p></td><td><p>136,722</p></td><td><p>4,647</p></td></tr><tr><th><p>TransCenter [46]</p></th><td><p>73.2</p></td><td><p>62.2</p></td><td><p>40.8</p></td><td><p>18.5</p></td><td>23,112</td><td><p>123,738</p></td><td><p>4,614</p></td></tr><tr><th><p>TrTrack (TransTrack) [35]</p></th><td>75.2</td><td><p>63.5</p></td><td>55.3</td><td>10.2</td><td><p>50,157</p></td><td>86,442</td><td><p>3,603</p></td></tr><tr><th></th><th>PatchTrack (ours)</th><td>73.6</td><td>65.2</td><td>44.6</td><td>12.5</td><td>23,976</td><td>121,230</td><td><p>3,795</p></td></tr></tbody></table>", "caption": "Table 2: Evaluation on MOT17 test set. We evaluate recent MOT systems on the MOT17 test set in a private detection protocol. Compared to CNN-based (non Transformer-based) methods, PatchTrack outperforms in MT and ML. We also compare our proposed method with MOT systems that are also Transformer based. Numbers are in bold if they are the best in their respective metric columns, and in blue if they are the second-to-best.", "list_citation_info": ["[29] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In European Conference on Computer Vision, pages 145\u2013161. Springer, 2020.", "[45] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12352\u201312361, 2021.", "[14] Shoudong Han, Piao Huang, Hongwei Wang, En Yu, Donghaisheng Liu, Xiaofeng Pan, and Jun Zhao. Mat: Motion-aware multi-object tracking. arXiv preprint arXiv:2009.04794, 2020.", "[19] Chao Liang, Zhipeng Zhang, Yi Lu, Xue Zhou, Bing Li, Xiyong Ye, and Jianxiao Zou. Rethinking the competition between detection and reid in multi-object tracking. arXiv preprint arXiv:2010.12138, 2020.", "[36] ShiJie Sun, Naveed Akhtar, HuanSheng Song, Ajmal Mian, and Mubarak Shah. Deep affinity network for multiple object tracking. IEEE transactions on pattern analysis and machine intelligence, 43(1):104\u2013119, 2019.", "[53] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In European Conference on Computer Vision, pages 474\u2013490. Springer, 2020.", "[27] Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, and Cewu Lu. Tubetk: Adopting tubes to track multi-object in a one-step training model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6308\u20136318, 2020.", "[52] Linyu Zheng, Ming Tang, Yingying Chen, Guibo Zhu, Jinqiao Wang, and Hanqing Lu. Improving multiple object tracking with single object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2453\u20132462, 2021.", "[48] En Yu, Zhuoling Li, Shoudong Han, and Hongwei Wang. Relationtrack: Relation-aware multiple object tracking with decoupled representation. arXiv preprint arXiv:2105.04322, 2021.", "[18] Wei Li, Yuanjun Xiong, Shuo Yang, Mingze Xu, Yongxin Wang, and Wei Xia. Semi-tcl: Semi-supervised track contrastive representation learning. arXiv preprint arXiv:2107.02396, 2021.", "[42] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13708\u201313715. IEEE, 2021.", "[51] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. arXiv preprint arXiv:2004.01888, 2020.", "[38] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. arXiv preprint arXiv:2103.14258, 2021.", "[50] Fangao Zeng, Bin Dong, Tiancai Wang, Cheng Chen, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. arXiv preprint arXiv:2105.03247, 2021.", "[28] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 164\u2013173, 2021.", "[46] Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, and Xavier Alameda-Pineda. Transcenter: Transformers with dense queries for multiple-object tracking. arXiv preprint arXiv:2103.15145, 2021.", "[24] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking with transformers. arXiv preprint arXiv:2101.02702, 2021.", "[35] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and Ping Luo. Transtrack: Multiple-object tracking with transformer. arXiv preprint arXiv:2012.15460, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>MOTA</th><th>MT</th><th>ML</th><th>IDsw</th></tr></thead><tbody><tr><td>w/o patch queries</td><td>71.4</td><td>165</td><td>42</td><td>214</td></tr><tr><td>w/o track queries</td><td>66.3</td><td>141</td><td>61</td><td>248</td></tr><tr><td>w/o patch-track queries</td><td>72.0</td><td>176</td><td>40</td><td>200</td></tr><tr><td>PatchTrack</td><td>72.1</td><td>176</td><td>40</td><td>192</td></tr></tbody></table>", "caption": "Table 3: Ablation study on type of query inputs. We send different types of query inputs to our system and evaluate their effects. The results suggest the positive effect of patch queries and track queries. When the system doesn\u2019t use patch-track queries and behave as an object detector, where we use Kalman filter [43] and Hungarian algorithm [16] to associate predicted detections, the system produces more ID switches.", "list_citation_info": ["[16] Istv\u00e1n Kenesei, Robert M Vago, and Anna Fenyvesi. Hungarian. Routledge, 2002.", "[43] Greg Welch, Gary Bishop, et al. An introduction to the kalman filter. 1995."]}]}