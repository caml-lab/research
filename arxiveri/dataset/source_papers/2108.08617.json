{"title": "Spatially-adaptive image restoration using distortion-guided networks", "abstract": "We present a general learning-based solution for restoring images suffering from spatially-varying degradations. Prior approaches are typically degradation-specific and employ the same processing across different images and different pixels within. However, we hypothesize that such spatially rigid processing is suboptimal for simultaneously restoring the degraded pixels as well as reconstructing the clean regions of the image. To overcome this limitation, we propose SPAIR, a network design that harnesses distortion-localization information and dynamically adjusts computation to difficult regions in the image. SPAIR comprises of two components, (1) a localization network that identifies degraded pixels, and (2) a restoration network that exploits knowledge from the localization network in filter and feature domain to selectively and adaptively restore degraded pixels. Our key idea is to exploit the non-uniformity of heavy degradations in spatial-domain and suitably embed this knowledge within distortion-guided modules performing sparse normalization, feature extraction and attention. Our architecture is agnostic to physical formation model and generalizes across several types of spatially-varying degradations. We demonstrate the efficacy of SPAIR individually on four restoration tasks-removal of rain-streaks, raindrops, shadows and motion blur. Extensive qualitative and quantitative comparisons with prior art on 11 benchmark datasets demonstrate that our degradation-agnostic network design offers significant performance gains over state-of-the-art degradation-specific architectures. Code available at https://github.com/human-analysis/spatially-adaptive-image-restoration.", "authors": ["Kuldeep Purohit", " Maitreya Suin", " A. N. Rajagopalan", " Vishnu Naresh Boddeti"], "pdf_url": "https://arxiv.org/abs/2108.08617", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"2\">Test100 [72]</th><th colspan=\"2\">Rain100H [66]</th><th colspan=\"2\">Rain100L [66]</th><th colspan=\"2\">Test2800 [8]</th><th colspan=\"2\">Test1200 [71]</th><th colspan=\"2\">Average</th></tr><tr><th>Methods</th><th>PSNR {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>SSIM {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>PSNR {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>SSIM {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>PSNR {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>SSIM {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>PSNR {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>SSIM {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>PSNR {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>SSIM {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>PSNR {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th><th>SSIM {\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\uparrow}</th></tr></thead><tbody><tr><td>DerainNet [7]</td><td>22.77</td><td>0.810</td><td>14.92</td><td>0.592</td><td>27.03</td><td>0.884</td><td>24.31</td><td>0.861</td><td>23.38</td><td>0.835</td><td>22.48 (69.3%)</td><td>0.796 (61.3%)</td></tr><tr><td>SEMI [61]</td><td>22.35</td><td>0.788</td><td>16.56</td><td>0.486</td><td>25.03</td><td>0.842</td><td>24.43</td><td>0.782</td><td>26.05</td><td>0.822</td><td>22.88 (67.8%)</td><td>0.744 (69.1%)</td></tr><tr><td>DIDMDN [71]</td><td>22.56</td><td>0.818</td><td>17.35</td><td>0.524</td><td>25.23</td><td>0.741</td><td>28.13</td><td>0.867</td><td>29.65</td><td>0.901</td><td>24.58 (60.9%)</td><td>0.770 (65.7%)</td></tr><tr><td>UMRL [67]</td><td>24.41</td><td>0.829</td><td>26.01</td><td>0.832</td><td>29.18</td><td>0.923</td><td>29.97</td><td>0.905</td><td>30.55</td><td>0.910</td><td>28.02 (41.9%)</td><td>0.880 (34.2%)</td></tr><tr><td>RESCAN [28]</td><td>25.00</td><td>0.835</td><td>26.36</td><td>0.786</td><td>29.80</td><td>0.881</td><td>31.29</td><td>0.904</td><td>30.51</td><td>0.882</td><td>28.59 (37.9%)</td><td>0.857 (44.8%)</td></tr><tr><td>PreNet [46]</td><td>24.81</td><td>0.851</td><td>26.77</td><td>0.858</td><td>32.44</td><td>0.950</td><td>31.75</td><td>0.916</td><td>31.36</td><td>0.911</td><td>29.42 (31.7%)</td><td>0.897 (23.3%)</td></tr><tr><td>MSPFN [20]</td><td>27.50</td><td>0.876</td><td>28.66</td><td>0.860</td><td>32.40</td><td>0.933</td><td>32.82</td><td>0.930</td><td>32.39</td><td>0.916</td><td>30.75 (21.9%)</td><td>0.903 (18.6%)</td></tr><tr><td>SPAIR (Ours)</td><td>30.35</td><td>0.909</td><td>30.95</td><td>0.892</td><td>36.93</td><td>0.969</td><td>33.34</td><td>0.936</td><td>33.04</td><td>0.922</td><td>32.91 (0.0%)</td><td>0.926 (0.0%)</td></tr></tbody></table>", "caption": "Table 1: Image deraining results. Best and second best scores are highlighted and underlined. For each method, relative MSE reduction achieved by SPAIR is reported in parenthesis (see Section 4 for calculation). SPAIR achieves \\sim22\\% improvement over MSPFN [20].", "list_citation_info": ["[67] Rajeev Yasarla and Vishal M Patel. Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining. In CVPR, 2019.", "[61] Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu, and Ying Wu. Semi-supervised transfer learning for image rain removal. In CVPR, 2019.", "[72] He Zhang, Vishwanath Sindagi, and Vishal M Patel. Image de-raining using a conditional generative adversarial network. TCSVT, 2019.", "[28] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hongbin Zha. Recurrent squeeze-and-excitation context aggregation net for single image deraining. In ECCV, 2018.", "[20] Kui Jiang, Zhongyuan Wang, Peng Yi, Baojin Huang, Yimin Luo, Jiayi Ma, and Junjun Jiang. Multi-scale progressive fusion network for single image deraining. In CVPR, 2020.", "[71] He Zhang and Vishal M Patel. Density-aware single image de-raining using a multi-stream dense network. In CVPR, 2018.", "[7] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, and John Paisley. Clearing the skies: A deep network architecture for single-image rain removal. TIP, 2017.", "[46] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. Progressive image deraining networks: A better and simpler baseline. In CVPR, 2019.", "[66] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from a single image. In CVPR, 2017.", "[8] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, and John Paisley. Removing rain from single images via a deep detail network. In CVPR, 2017."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Metric</th><th>DSC</th><th>GMM</th><th>Clear</th><th>DDN</th><th>RESCAN</th><th>PReNet</th><th>SPANet</th><th>JORDER{}_{E}</th><th>RCDNet{}_{1}</th><th>RCDNet</th><th>SPAIR</th></tr></thead><tbody><tr><td rowspan=\"2\">SpaNet [58]</td><td>PSNR</td><td>34.95</td><td>34.30</td><td>34.39</td><td>36.16</td><td>38.11</td><td>40.16</td><td>40.24</td><td>40.78</td><td>40.99</td><td>41.47</td><td>44.10</td></tr><tr><td>SSIM</td><td>0.9416</td><td>0.9428</td><td>0.9509</td><td>0.9463</td><td>0.9707</td><td>0.9816</td><td>0.9811</td><td>**</td><td>0.9816</td><td>0.9834</td><td>0.9872</td></tr><tr><td rowspan=\"2\">Rain100H [66]</td><td>PSNR</td><td>13.77</td><td>15.23</td><td>15.33</td><td>22.85</td><td>29.62</td><td>30.11</td><td>25.11</td><td>30.50</td><td>30.91</td><td>31.28</td><td>31.69</td></tr><tr><td>SSIM</td><td>0.3199</td><td>0.4498</td><td>0.7421</td><td>0.7250</td><td>0.8720</td><td>0.9053</td><td>0.8332</td><td>0.8967</td><td>0.9037</td><td>0.9093</td><td>0.9201</td></tr></tbody></table>", "caption": "Table 2: Quantitative comparisons of models trained and tested on the SPANet [58] and the Rain100H [66] benchmarks.", "list_citation_info": ["[66] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from a single image. In CVPR, 2017.", "[58] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang, and Rynson WH Lau. Spatial attentive single-image deraining with a high quality real rain dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12270\u201312279, 2019."]}, {"table": "<table><thead><tr><th>Method</th><th>Eigen [4]</th><th>Pix2pix [19]</th><th>AGAN[42]</th><th>DuRN[32]</th><th>Quan[44]</th><th>SPAIR</th></tr></thead><tbody><tr><td>PSNR</td><td>28.59</td><td>30.59</td><td>31.51</td><td>31.24</td><td>31.44</td><td>32.73</td></tr><tr><td>SSIM</td><td>0.6726</td><td>0.8075</td><td>0.9213</td><td>0.9259</td><td>0.9263</td><td>0.9410</td></tr></tbody></table>", "caption": "Table 3: Raindrop removal results on testset from Qian et al. [42].", "list_citation_info": ["[44] Yuhui Quan, Shijie Deng, Yixin Chen, and Hui Ji. Deep learning for seeing through window with raindrops. In Proceedings of the IEEE International Conference on Computer Vision, pages 2463\u20132471, 2019.", "[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125\u20131134, 2017.", "[42] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative adversarial network for raindrop removal from a single image. In CVPR, 2018.", "[4] David Eigen, Dilip Krishnan, and Rob Fergus. Restoring an image taken through a window covered with dirt or rain. In Proceedings of the IEEE International Conference on Computer Vision, pages 633\u2013640, 2013.", "[32] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki Okatani. Dual residual networks leveraging the potential of paired operations for image restoration. In Proc. Conference on Computer Vision and Pattern Recognition, pages 7007\u20137016, 2019."]}, {"table": "<table><thead><tr><th>Metric</th><th>Input</th><th>[64]</th><th>[14]</th><th>[11]</th><th>[57]</th><th>[16]</th><th>[75]</th><th>[1]</th><th>SPAIR</th></tr></thead><tbody><tr><td>RMSE{}_{S}</td><td>32.12</td><td>19.82</td><td>18.95</td><td>14.98</td><td>10.33</td><td>9.48</td><td>8.99</td><td>8.14</td><td>8.05</td></tr><tr><td>RMSE{}_{NS}</td><td>7.19</td><td>14.83</td><td>7.46</td><td>7.29</td><td>6.93</td><td>6.14</td><td>6.33</td><td>6.04</td><td>5.47</td></tr><tr><td>RMSE</td><td>10.97</td><td>15.63</td><td>9.30</td><td>8.53</td><td>7.47</td><td>6.67</td><td>6.95</td><td>6.37</td><td>5.88</td></tr></tbody></table>", "caption": "Table 4: Shadow removal results on ISTD Dataset [57]. Subscripts S and NS indicate shadow and non-shadow regions, respectively.", "list_citation_info": ["[14] Ruiqi Guo, Qieyun Dai, and Derek Hoiem. Paired regions for shadow detection and removal. IEEE transactions on pattern analysis and machine intelligence, 35(12):2956\u20132967, 2012.", "[1] Xiaodong Cun, Chi-Man Pun, and Cheng Shi. Towards ghost-free shadow removal via dual hierarchical aggregation network and shadow matting gan. In AAAI, pages 10680\u201310687, 2020.", "[57] Jifeng Wang, Xiang Li, and Jian Yang. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1788\u20131797, 2018.", "[11] Han Gong and Darren Cosker. Interactive shadow removal and ground truth for variable scene categories. In BMVC, pages 1\u201311. Citeseer, 2014.", "[64] Qingxiong Yang, Kar-Han Tan, and Narendra Ahuja. Shadow removal using bilateral filtering. IEEE Transactions on Image processing, 21(10):4361\u20134368, 2012.", "[16] Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng-Ann Heng. Direction-aware spatial context features for shadow detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7454\u20137462, 2018.", "[75] Ling Zhang, Chengjiang Long, Xiaolong Zhang, and Chunxia Xiao. Ris-gan: Explore residual and illumination with generative adversarial networks for shadow removal. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12829\u201312836, 2020."]}, {"table": "<table><tbody><tr><td></td><td colspan=\"2\">GoPro [35]</td><td colspan=\"2\">HIDE [49]</td><td colspan=\"3\">RealBlur-J [47]</td></tr><tr><td>Method</td><td>PSNR</td><td>SSIM</td><td>PSNR</td><td>SSIM</td><td>PSNR</td><td>SSIM</td><td>PSNR{}^{\\ddagger}</td></tr><tr><td>Xu et al. [63]</td><td>21.00</td><td>0.741</td><td>-</td><td>-</td><td>27.14</td><td>0.830</td><td></td></tr><tr><td>DeblurGAN [24]</td><td>28.70</td><td>0.858</td><td>24.51</td><td>0.871</td><td>27.97</td><td>0.834</td><td></td></tr><tr><td>Nah et al. [35]</td><td>29.08</td><td>0.914</td><td>25.73</td><td>0.874</td><td>27.87</td><td>0.827</td><td></td></tr><tr><td>Zhang et al. [73]</td><td>29.19</td><td>0.931</td><td>-</td><td>-</td><td>27.80</td><td>0.847</td><td></td></tr><tr><td>DeblurGAN-v2 [25]</td><td>29.55</td><td>0.934</td><td>26.61</td><td>0.875</td><td>28.70</td><td>0.866</td><td>29.69</td></tr><tr><td>SRN [53]</td><td>30.26</td><td>0.934</td><td>28.36</td><td>0.915</td><td>28.56</td><td>0.867</td><td>31.38</td></tr><tr><td>Shen et al. [49]</td><td>30.26</td><td>0.940</td><td>28.89</td><td>0.930</td><td>-</td><td>-</td><td></td></tr><tr><td>Purohit et al. [40]</td><td>30.58</td><td>0.941</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>Purohit et al. [38]</td><td>30.73</td><td>0.942</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>DBGAN [74]</td><td>31.10</td><td>0.942</td><td>28.94</td><td>0.915</td><td>-</td><td>-</td><td></td></tr><tr><td>MT-RNN [37]</td><td>31.15</td><td>0.945</td><td>29.15</td><td>0.918</td><td>-</td><td>-</td><td></td></tr><tr><td>DMPHN [70]</td><td>31.20</td><td>0.940</td><td>29.09</td><td>0.924</td><td>28.42</td><td>0.860</td><td></td></tr><tr><td>RADN [39]</td><td>31.76</td><td>0.952</td><td>29.68</td><td>0.927</td><td>-</td><td>-</td><td></td></tr><tr><td>Suin et al. [51]</td><td>31.85</td><td>0.948</td><td>29.98</td><td>0.930</td><td>-</td><td>-</td><td></td></tr><tr><td>SPAIR</td><td>32.06</td><td>0.953</td><td>30.29</td><td>0.931</td><td>28.81</td><td>0.875</td><td>31.82</td></tr></tbody></table>", "caption": "Table 5: Deblurring results. Our method is trained only on the GoPro dataset [35] and directly applied to the test images of HIDE [49] and RealBlur-J [47] datasets. PSNR\\ddagger scores were obtained after training and testing on RealBlur-J dataset.", "list_citation_info": ["[35] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR, 2017.", "[47] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho. Real-world blur dataset for learning and benchmarking deblurring algorithms. In ECCV, 2020.", "[24] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji\u0159\u00ed Matas. DeblurgGAN: Blind motion deblurring using conditional adversarial networks. In CVPR, 2018.", "[53] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. Scale-recurrent network for deep image deblurring. In CVPR, 2018.", "[38] Kuldeep Purohit and AN Rajagopalan. Efficient motion deblurring with feature transformation and spatial attention. In 2019 IEEE International Conference on Image Processing (ICIP), pages 4674\u20134678. IEEE, 2019.", "[49] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao. Human-aware motion deblurring. In ICCV, 2019.", "[73] Jiawei Zhang, Jinshan Pan, Jimmy Ren, Yibing Song, Linchao Bao, Rynson WH Lau, and Ming-Hsuan Yang. Dynamic scene deblurring using spatially variant recurrent neural networks. In CVPR, 2018.", "[51] Maitreya Suin, Kuldeep Purohit, and A. N. Rajagopalan. Spatially-attentive patch-hierarchical network for adaptive motion deblurring. In CVPR, 2020.", "[37] Dongwon Park, Dong Un Kang, Jisoo Kim, and Se Young Chun. Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training. In ECCV, 2020.", "[74] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, and Hongdong Li. Deblurring by realistic blurring. In CVPR, 2020.", "[63] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse representation for natural image deblurring. In CVPR, 2013.", "[70] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr Koniusz. Deep stacked hierarchical multi-patch network for image deblurring. In CVPR, 2019.", "[25] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. DeblurGAN-v2: Deblurring (orders-of-magnitude) faster and better. In ICCV, 2019.", "[39] Kuldeep Purohit and AN Rajagopalan. Region-adaptive dense network for efficient motion deblurring. In AAAI, 2020.", "[40] Kuldeep Purohit, Anshul Shah, and AN Rajagopalan. Bringing alive blurred moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6830\u20136839, 2019."]}]}