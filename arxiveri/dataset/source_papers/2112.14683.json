{"title": "StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2", "abstract": "Videos show continuous events, yet most $-$ if not all $-$ video synthesis frameworks treat them discretely in time. In this work, we think of videos of what they should be $-$ time-continuous signals, and extend the paradigm of neural representations to build a continuous-time video generator. For this, we first design continuous motion representations through the lens of positional embeddings. Then, we explore the question of training on very sparse videos and demonstrate that a good generator can be learned by using as few as 2 frames per clip. After that, we rethink the traditional image + video discriminators pair and design a holistic discriminator that aggregates temporal information by simply concatenating frames' features. This decreases the training cost and provides richer learning signal to the generator, making it possible to train directly on 1024$^2$ videos for the first time. We build our model on top of StyleGAN2 and it is just ${\\approx}5\\%$ more expensive to train at the same resolution while achieving almost the same image quality. Moreover, our latent space features similar properties, enabling spatial manipulations that our method can propagate in time. We can generate arbitrarily long videos at arbitrary high frame rate, while prior work struggles to generate even 64 frames at a fixed rate. Our model is tested on four modern 256$^2$ and one 1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it performs on average ${\\approx}30\\%$ better than the closest runner-up. Project website: https://universome.github.io.", "authors": ["Ivan Skorokhodov", " Sergey Tulyakov", " Mohamed Elhoseiny"], "pdf_url": "https://arxiv.org/abs/2112.14683", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">FaceForensics</td><td colspan=\"2\">SkyTimelapse</td><td colspan=\"2\">UCF101</td><td colspan=\"2\">RainbowJelly</td><td rowspan=\"2\"> Training cost(GPU-days) </td></tr><tr><td>FVD{}_{16}</td><td>FVD{}_{128}</td><td>FVD{}_{16}</td><td>FVD{}_{128}</td><td>FVD{}_{16}</td><td>FVD{}_{128}</td><td>FVD{}_{16}</td><td>FVD{}_{128}</td></tr><tr><td>MoCoGAN [67]</td><td>124.7</td><td>257.3</td><td>206.6</td><td>575.9</td><td>2886.9</td><td>3679.0</td><td>1572.9</td><td>549.7</td><td>5</td></tr><tr><td> + StyleGAN2 backbone</td><td>55.62</td><td>309.3</td><td>85.88</td><td>272.8</td><td>1821.4</td><td>2311.3</td><td>638.5</td><td>463.0</td><td>8</td></tr><tr><td>MoCoGAN-HD [65]</td><td>111.8</td><td>653.0</td><td>164.1</td><td>878.1</td><td>1729.6</td><td>2606.5</td><td>579.1</td><td>628.2</td><td>7.5 + 9</td></tr><tr><td>VideoGPT [79]{}^{\\dagger}</td><td>185.9</td><td>N/A</td><td>222.7</td><td>N/A</td><td>2880.6</td><td>N/A</td><td>136.0</td><td>N/A</td><td>16 + 16</td></tr><tr><td>DIGAN [80]</td><td>62.5</td><td>1824.7</td><td>83.11</td><td>196.7</td><td>1630.2</td><td>2293.7</td><td>436.6</td><td>369.0</td><td>16</td></tr><tr><td>StyleGAN-V (ours)</td><td>47.41</td><td>89.34</td><td>79.52</td><td>197.0</td><td>1431.0</td><td>1773.4</td><td>195.4</td><td>262.5</td><td>8</td></tr></table>", "caption": "Table 1: Quantitative performance and training cost of different methods.We trained all the methods from scratch on 256^{2} resolution datasets using the official codebases and evaluated them under the unified evaluation protocol (see \u00a74). Training was done on \\times 4 32 GB NVidia V100 GPUs for all the methods except VideoGPT, which was trained on \\times 4 NVidia A6000 GPUs (with 48.5 GB of memory each) due to its high memory consumption. For 2-stage methods, we report their training cost in the \u201cX+Y\u201d format. {}^{\\dagger}VideoGPT was trained for our maximum resource constraint of 32 GPU-days which was detrimental to its performance on 256^{2} resolution. Vanilla StyleGAN2 training time on 256^{2} resolution (with mixed precision and optimizations [28]) is 7.72 GPU-days in our environment.", "list_citation_info": ["[67] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526\u20131535, 2018.", "[65] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In International Conference on Learning Representations, 2021.", "[80] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2022.", "[28] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020.", "[79] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021."]}, {"table": "<table><tr><td>Method</td><td>Inception Score [54]</td></tr><tr><td>MoCoGAN [67]</td><td>10.09\\pm0.30</td></tr><tr><td>MoCoGAN+SG2 (ours)</td><td>15.26\\pm0.95</td></tr><tr><td>VideoGPT [79]</td><td>12.61\\pm0.33</td></tr><tr><td>MoCoGAN-HD [65]</td><td>23.39\\pm1.48</td></tr><tr><td>DIGAN [80]</td><td>23.16\\pm1.13</td></tr><tr><td>StyleGAN-V (ours)</td><td>23.94\\pm0.73</td></tr><tr><td>Real videos</td><td>97.23\\pm0.38</td></tr></table>", "caption": "Table 5: Inception Score [54] on UCF101 256^{2} (note that the underlying C3D model resizes the 256^{2} videos into 112^{2} resolution under the hood, eliminating high-quality details).", "list_citation_info": ["[67] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526\u20131535, 2018.", "[65] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In International Conference on Learning Representations, 2021.", "[54] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 2830\u20132839, 2017.", "[80] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2022.", "[79] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021."]}, {"table": "<table><tr><td>Method</td><td>FVD{}_{16}</td><td>FID</td><td>Training cost</td></tr><tr><td>MoCoGAN [67]</td><td>124.7</td><td>23.97</td><td>5</td></tr><tr><td>MoCoGAN+SG2 (ours)</td><td>55.62</td><td>10.82</td><td>8</td></tr><tr><td>VideoGPT [79]</td><td>185.9</td><td>22.7</td><td>32</td></tr><tr><td>MoCoGAN-HD [65]</td><td>111.8</td><td>7.12</td><td>16.5</td></tr><tr><td>DIGAN [80]</td><td>62.5</td><td>19.1</td><td>16</td></tr><tr><td>StyleGAN-V (ours)</td><td>47.41</td><td>9.445</td><td>8</td></tr><tr><td>StyleGAN2 [28]</td><td>N/A</td><td>8.42</td><td>7.72</td></tr></table>", "caption": "Table 6: FVD{}_{16}, FID and training costs of modern video generators on FaceForensics 256^{2}. Training cost is measured in terms of GPU-days.", "list_citation_info": ["[67] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526\u20131535, 2018.", "[65] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good image generator is what you need for high-resolution video synthesis. In International Conference on Learning Representations, 2021.", "[80] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2022.", "[28] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020.", "[79] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021."]}, {"table": "<table><tr><td>Dataset</td><td>#hours</td><td>avg len</td><td>FPS</td><td>#speakers</td></tr><tr><td>FaceForensics [53]</td><td>4.04</td><td>20.7s</td><td>25</td><td>704</td></tr><tr><td>SkyTimelapse [78]</td><td>12.99</td><td>22.1s</td><td>25</td><td>N/A</td></tr><tr><td>UCF-101 [62]</td><td>0.51</td><td>6.8s</td><td>25</td><td>N/A</td></tr><tr><td>RainbowJelly</td><td>7.99</td><td>17.1s</td><td>30</td><td>N/A</td></tr><tr><td>MEAD [72]</td><td>36.11</td><td>4.3s</td><td>30</td><td>48</td></tr></table>", "caption": "Table 7: Additional datasets information in terms of total lengths (in the total number of hours), average video length (in seconds), frame rate and the amount of speakers (for FaceForensics and MEAD).", "list_citation_info": ["[53] Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforensics: A large-scale video dataset for forgery detection in human faces. arXiv, 2018.", "[78] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "[72] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: A large-scale audio-visual dataset for emotional talking-face generation. In European Conference on Computer Vision, pages 700\u2013717. Springer, 2020.", "[62] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012."]}]}