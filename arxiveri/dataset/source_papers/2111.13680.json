{"title": "Gmflow: Learning optical flow via global matching", "abstract": "Learning-based optical flow estimation has been dominated with the pipeline of cost volume with convolutions for flow regression, which is inherently limited to local correlations and thus is hard to address the long-standing challenge of large displacements. To alleviate this, the state-of-the-art framework RAFT gradually improves its prediction quality by using a large number of iterative refinements, achieving remarkable performance but introducing linearly increasing inference time. To enable both high accuracy and efficiency, we completely revamp the dominant flow regression pipeline by reformulating optical flow as a global matching problem, which identifies the correspondences by directly comparing feature similarities. Specifically, we propose a GMFlow framework, which consists of three main components: a customized Transformer for feature enhancement, a correlation and softmax layer for global feature matching, and a self-attention layer for flow propagation. We further introduce a refinement step that reuses GMFlow at higher feature resolution for residual flow prediction. Our new framework outperforms 31-refinements RAFT on the challenging Sintel benchmark, while using only one refinement and running faster, suggesting a new paradigm for accurate and efficient optical flow estimation. Code is available at https://github.com/haofeixu/gmflow.", "authors": ["Haofei Xu", " Jing Zhang", " Jianfei Cai", " Hamid Rezatofighi", " Dacheng Tao"], "pdf_url": "https://arxiv.org/abs/2111.13680", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>#refine.</td><td colspan=\"4\">Things (val, clean)</td><td colspan=\"4\">Sintel (train, clean)</td><td colspan=\"4\">Sintel (train, final)</td><td>Param(M)</td><td>Time(ms)</td></tr><tr><td></td><td></td><td>EPE</td><td>s_{0-10}</td><td>s_{10-40}</td><td>s_{40+}</td><td>EPE</td><td>s_{0-10}</td><td>s_{10-40}</td><td>s_{40+}</td><td>EPE</td><td>s_{0-10}</td><td>s_{10-40}</td><td>s_{40+}</td><td></td><td></td></tr><tr><td rowspan=\"6\">RAFT [41]</td><td>0</td><td>14.28</td><td>1.47</td><td>3.62</td><td>40.48</td><td>4.04</td><td>0.77</td><td>4.30</td><td>26.66</td><td>5.45</td><td>0.99</td><td>6.30</td><td>35.19</td><td rowspan=\"6\">5.3</td><td>25 (14)</td></tr><tr><td>3</td><td>6.27</td><td>0.69</td><td>1.67</td><td>17.63</td><td>1.92</td><td>0.47</td><td>2.32</td><td>11.37</td><td>3.25</td><td>0.65</td><td>4.00</td><td>20.04</td><td>39 (21)</td></tr><tr><td>7</td><td>4.66</td><td>0.55</td><td>1.38</td><td>12.87</td><td>1.61</td><td>0.39</td><td>1.90</td><td>9.61</td><td>2.80</td><td>0.53</td><td>3.30</td><td>17.76</td><td>58 (31)</td></tr><tr><td>11</td><td>4.31</td><td>0.53</td><td>1.33</td><td>11.79</td><td>1.55</td><td>0.41</td><td>1.73</td><td>9.19</td><td>2.72</td><td>0.52</td><td>3.12</td><td>17.43</td><td>78 (41)</td></tr><tr><td>23</td><td>4.22</td><td>0.53</td><td>1.32</td><td>11.52</td><td>1.47</td><td>0.36</td><td>1.63</td><td>9.00</td><td>2.69</td><td>0.52</td><td>3.05</td><td>17.28</td><td>133 (71)</td></tr><tr><td>31</td><td>4.25</td><td>0.53</td><td>1.31</td><td>11.63</td><td>1.41</td><td>0.32</td><td>1.55</td><td>8.83</td><td>2.69</td><td>0.52</td><td>3.00</td><td>17.45</td><td>170 (91)</td></tr><tr><td rowspan=\"2\">GMFlow</td><td>0</td><td>3.48</td><td>0.67</td><td>1.31</td><td>8.97</td><td>1.50</td><td>0.46</td><td>1.77</td><td>8.26</td><td>2.96</td><td>0.72</td><td>3.45</td><td>17.70</td><td>4.7</td><td>57 (26)</td></tr><tr><td>1</td><td>2.80</td><td>0.53</td><td>1.01</td><td>7.31</td><td>1.08</td><td>0.30</td><td>1.25</td><td>6.26</td><td>2.48</td><td>0.51</td><td>2.81</td><td>15.67</td><td>4.7</td><td>151 (66)</td></tr></table>", "caption": "Table 3: RAFT\u2019s iterative refinement framework vs. our GMFlow framework. The models are trained on Chairs and Things training sets. We use RAFT\u2019s officially released model for evaluation. The inference time is measured on a single V100 and A100 (in parentheses) GPU at Sintel resolution (436\\times 1024). Our framework gains more speedup than RAFT (2.29\\times vs. 1.87\\times, i.e., ours: 151\\to 66, RAFT: 170\\to 91) on the high-end A100 GPU since our method doesn\u2019t require a large number of sequential computation.", "list_citation_info": ["[41] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402\u2013419. Springer, 2020."]}, {"table": "<table><tr><td>Method</td><td colspan=\"3\">Sintel (clean)</td><td colspan=\"3\">Sintel (final)</td></tr><tr><td></td><td>all</td><td>matched</td><td>unmatched</td><td>all</td><td>matched</td><td>unmatched</td></tr><tr><td>FlowNet2 [17]</td><td>4.16</td><td>1.56</td><td>25.40</td><td>5.74</td><td>2.75</td><td>30.11</td></tr><tr><td>PWC-Net+ [39]</td><td>3.45</td><td>1.41</td><td>20.12</td><td>4.60</td><td>2.25</td><td>23.70</td></tr><tr><td>HD{}^{3} [52]</td><td>4.79</td><td>1.62</td><td>30.63</td><td>4.67</td><td>2.17</td><td>24.99</td></tr><tr><td>VCN [51]</td><td>2.81</td><td>1.11</td><td>16.68</td><td>4.40</td><td>2.22</td><td>22.24</td></tr><tr><td>DICL [44]</td><td>2.63</td><td>0.97</td><td>16.24</td><td>3.60</td><td>1.66</td><td>19.44</td></tr><tr><td>RAFT [41]</td><td>1.94</td><td>-</td><td>-</td><td>3.18</td><td>-</td><td>-</td></tr><tr><td>GMFlow</td><td>1.74</td><td>0.65</td><td>10.56</td><td>2.90</td><td>1.32</td><td>15.80</td></tr><tr><td>RAFT{}^{\\dagger} [41]</td><td>1.61</td><td>0.62</td><td>9.65</td><td>2.86</td><td>1.41</td><td>14.68</td></tr><tr><td>GMA{}^{\\dagger} [19]</td><td>1.39</td><td>0.58</td><td>7.96</td><td>2.47</td><td>1.24</td><td>12.50</td></tr></table>", "caption": "Table 5: Comparisons on Sintel test test. {}^{\\dagger} represents the method uses last frame\u2019s flow prediction as initialization for subsequent refinement, while other methods all use two frames only.", "list_citation_info": ["[39] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Models matter, so does training: An empirical study of cnns for optical flow estimation. TPAMI, 42(6):1408\u20131423, 2019.", "[44] Jianyuan Wang, Yiran Zhong, Yuchao Dai, Kaihao Zhang, Pan Ji, and Hongdong Li. Displacement-invariant matching cost learning for accurate optical flow estimation. NeurIPS, 33, 2020.", "[51] Gengshan Yang and Deva Ramanan. Volumetric correspondence networks for optical flow. NeurIPS, 32:794\u2013805, 2019.", "[19] Shihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, and Richard Hartley. Learning to estimate hidden motions with global motion aggregation. In ICCV, pages 9772\u20139781, October 2021.", "[52] Zhichao Yin, Trevor Darrell, and Fisher Yu. Hierarchical discrete distribution decomposition for match density estimation. In CVPR, pages 6044\u20136053, 2019.", "[41] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402\u2013419. Springer, 2020.", "[17] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In CVPR, pages 2462\u20132470, 2017."]}, {"table": "<table><tr><td>Method</td><td>FlowNet2 [17]</td><td>PWC-Net+ [39]</td><td>RAFT [41]</td><td>GMFlow</td></tr><tr><td>All</td><td>11.48</td><td>7.72</td><td>5.10</td><td>9.32</td></tr><tr><td>Noc</td><td>6.94</td><td>4.91</td><td>3.07</td><td>3.80</td></tr></table>", "caption": "Table 6: Comparisons on KITTI test set. The metric is F1-all. \u201cAll\u201d denotes the evaluation results on all pixels with ground truth, and \u201cNoc\u201d denotes non-occluded pixels only.", "list_citation_info": ["[39] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Models matter, so does training: An empirical study of cnns for optical flow estimation. TPAMI, 42(6):1408\u20131423, 2019.", "[41] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402\u2013419. Springer, 2020.", "[17] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In CVPR, pages 2462\u20132470, 2017."]}]}