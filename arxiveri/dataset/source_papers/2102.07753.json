{"title": "Learning intra-batch connections for deep metric learning", "abstract": "The goal of metric learning is to learn a function that maps samples to a lower-dimensional space where similar samples lie closer than dissimilar ones. Particularly, deep metric learning utilizes neural networks to learn such a mapping. Most approaches rely on losses that only take the relations between pairs or triplets of samples into account, which either belong to the same class or two different classes. However, these methods do not explore the embedding space in its entirety. To this end, we propose an approach based on message passing networks that takes all the relations in a mini-batch into account. We refine embedding vectors by exchanging messages among all samples in a given batch allowing the training process to be aware of its overall structure. Since not all samples are equally important to predict a decision boundary, we use an attention mechanism during message passing to allow samples to weigh the importance of each neighbor accordingly. We achieve state-of-the-art results on clustering and image retrieval on the CUB-200-2011, Cars196, Stanford Online Products, and In-Shop Clothes datasets. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/intra_batch_connections.", "authors": ["Jenny Seidenschwarz", " Ismail Elezi", " Laura Leal-Taix\u00e9"], "pdf_url": "https://arxiv.org/abs/2102.07753", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th></th><th colspan=\"5\">CUB-200-2011</th><th colspan=\"5\">CARS196</th><th colspan=\"4\">Stanford Online Products</th></tr><tr><th>Method</th><th>BB</th><th>R@1</th><th>R@2</th><th>R@4</th><th>R@8</th><th>NMI</th><th>R@1</th><th>R@2</th><th>R@4</th><th>R@8</th><th>NMI</th><th>R@1</th><th>R@10</th><th>R@100</th><th>NMI</th></tr></thead><tbody><tr><td>Triplet{}^{64} (Schroff et al., 2015) CVPR15</td><td>G</td><td>42.5</td><td>55</td><td>66.4</td><td>77.2</td><td>55.3</td><td>51.5</td><td>63.8</td><td>73.5</td><td>82.4</td><td>53.4</td><td>66.7</td><td>82.4</td><td>91.9</td><td>89.5</td></tr><tr><td>Npairs{}^{64} (Sohn, 2016) NeurIPS16</td><td>G</td><td>51.9</td><td>64.3</td><td>74.9</td><td>83.2</td><td>60.2</td><td>68.9</td><td>78.9</td><td>85.8</td><td>90.9</td><td>62.7</td><td>66.4</td><td>82.9</td><td>92.1</td><td>87.9</td></tr><tr><td>Deep Spectral{}^{512} (Law et al., 2017) ICML17</td><td>BNI</td><td>53.2</td><td>66.1</td><td>76.7</td><td>85.2</td><td>59.2</td><td>73.1</td><td>82.2</td><td>89.0</td><td>93.0</td><td>64.3</td><td>67.6</td><td>83.7</td><td>93.3</td><td>89.4</td></tr><tr><td>Angular Loss{}^{512} (Wang et al., 2017) ICCV17</td><td>G</td><td>54.7</td><td>66.3</td><td>76</td><td>83.9</td><td>61.1</td><td>71.4</td><td>81.4</td><td>87.5</td><td>92.1</td><td>63.2</td><td>70.9</td><td>85.0</td><td>93.5</td><td>88.6</td></tr><tr><td>Proxy-NCA{}^{64} (Movshovitz-Attias et al., 2017) ICCV17</td><td>BNI</td><td>49.2</td><td>61.9</td><td>67.9</td><td>72.4</td><td>59.5</td><td>73.2</td><td>82.4</td><td>86.4</td><td>88.7</td><td>64.9</td><td>73.7</td><td>-</td><td>-</td><td>90.6</td></tr><tr><td>Margin Loss{}^{128} (Manmatha et al., 2017) ICCV17</td><td>R50</td><td>63.6</td><td>74.4</td><td>83.1</td><td>90.0</td><td>69.0</td><td>79.6</td><td>86.5</td><td>91.9</td><td>95.1</td><td>69.1</td><td>72.7</td><td>86.2</td><td>93.8</td><td>90.7</td></tr><tr><td>Hierarchical triplet{}^{512} (Ge et al., 2018) ECCV18</td><td>BNI</td><td>57.1</td><td>68.8</td><td>78.7</td><td>86.5</td><td>-</td><td>81.4</td><td>88.0</td><td>92.7</td><td>95.7</td><td>-</td><td>74.8</td><td>88.3</td><td>94.8</td><td>-</td></tr><tr><td>ABE{}^{512} (Kim et al., 2018) ECCV18</td><td>G</td><td>60.6</td><td>71.5</td><td>79.8</td><td>87.4</td><td>-</td><td>85.2</td><td>90.5</td><td>94.0</td><td>96.1</td><td>-</td><td>76.3</td><td>88.4</td><td>94.8</td><td>-</td></tr><tr><td>Normalized Softmax{}^{512} (Zhai &amp; Wu, 2019) BMVC19</td><td>R50</td><td>61.3</td><td>73.9</td><td>83.5</td><td>90.0</td><td>69.7</td><td>84.2</td><td>90.4</td><td>94.4</td><td>96.9</td><td>74.0</td><td>78.2</td><td>90.6</td><td>96.2</td><td>91.0</td></tr><tr><td>RLL-H{}^{512} (Wang et al., 2019b) CVPR19</td><td>BNI</td><td>57.4</td><td>69.7</td><td>79.2</td><td>86.9</td><td>63.6</td><td>74</td><td>83.6</td><td>90.1</td><td>94.1</td><td>65.4</td><td>76.1</td><td>89.1</td><td>95.4</td><td>89.7</td></tr><tr><td>Multi-similarity{}^{512} (Wang et al., 2019a) CVPR19</td><td>BNI</td><td>65.7</td><td>77.0</td><td>86.3</td><td>91.2</td><td>-</td><td>84.1</td><td>90.4</td><td>94.0</td><td>96.5</td><td>-</td><td>78.2</td><td>90.5</td><td>96.0</td><td>-</td></tr><tr><td>Relational Knowledge{}^{512} (Park et al., 2019a) CVPR19</td><td>G</td><td>61.4</td><td>73.0</td><td>81.9</td><td>89.0</td><td>-</td><td>82.3</td><td>89.8</td><td>94.2</td><td>96.6</td><td>-</td><td>75.1</td><td>88.3</td><td>95.2</td><td>-</td></tr><tr><td>Divide and Conquer{}^{1028} (Sanakoyeu et al., 2019) CVPR19</td><td>R50</td><td>65.9</td><td>76.6</td><td>84.4</td><td>90.6</td><td>69.6</td><td>84.6</td><td>90.7</td><td>94.1</td><td>96.5</td><td>70.3</td><td>75.9</td><td>88.4</td><td>94.9</td><td>90.2</td></tr><tr><td>SoftTriple Loss{}^{512} (Qian et al., 2019) ICCV19</td><td>BNI</td><td>65.4</td><td>76.4</td><td>84.5</td><td>90.4</td><td>69.3</td><td>84.5</td><td>90.7</td><td>94.5</td><td>96.9</td><td>70.1</td><td>78.3</td><td>90.3</td><td>95.9</td><td>92.0</td></tr><tr><td>HORDE{}^{512} (Jacob et al., 2019) ICCV19</td><td>BNI</td><td>66.3</td><td>76.7</td><td>84.7</td><td>90.6</td><td>-</td><td>83.9</td><td>90.3</td><td>94.1</td><td>96.3</td><td>-</td><td>80.1</td><td>91.3</td><td>96.2</td><td>-</td></tr><tr><td>MIC{}^{128} (Brattoli et al., 2019) ICCV19</td><td>R50</td><td>66.1</td><td>76.8</td><td>85.6</td><td>-</td><td>69.7</td><td>82.6</td><td>89.1</td><td>93.2</td><td>-</td><td>68.4</td><td>77.2</td><td>89.4</td><td>95.6</td><td>90.0</td></tr><tr><td>Easy triplet mining{}^{512} (Xuan et al., 2020b) WACV20</td><td>R50</td><td>64.9</td><td>75.3</td><td>83.5</td><td>-</td><td>-</td><td>82.7</td><td>89.3</td><td>93.0</td><td>-</td><td>-</td><td>78.3</td><td>90.7</td><td>96.3</td><td>-</td></tr><tr><td>Group Loss{}^{1024} (Elezi et al., 2020) ECCV20</td><td>BNI</td><td>65.5</td><td>77.0</td><td>85.0</td><td>91.3</td><td>69.0</td><td>85.6</td><td>91.2</td><td>94.9</td><td>97.0</td><td>72.7</td><td>75.1</td><td>87.5</td><td>94.2</td><td>90.8</td></tr><tr><td>Proxy NCA++{}^{512} (Teh et al., 2020) ECCV20</td><td>R50</td><td>66.3</td><td>77.8</td><td>87.7</td><td>91.3</td><td>71.3</td><td>84.9</td><td>90.6</td><td>94.9</td><td>97.2</td><td>71.5</td><td>79.8</td><td>91.4</td><td>96.4</td><td>-</td></tr><tr><td>DiVA{}^{512} (Milbich et al., 2020) ECCV20</td><td>R50</td><td>69.2</td><td>79.3</td><td>-</td><td>-</td><td>71.4</td><td>87.6</td><td>92.9</td><td>-</td><td>-</td><td>72.2</td><td>79.6</td><td>-</td><td>-</td><td>90.6</td></tr><tr><td>PADS{}^{128} (Roth et al., 2020) CVPR20</td><td>R50</td><td>67.3</td><td>78.0</td><td>85.9</td><td>-</td><td>69.9</td><td>83.5</td><td>89.7</td><td>93.8</td><td>-</td><td>68.8</td><td>76.5</td><td>89.0</td><td>95.4</td><td>89.9</td></tr><tr><td>Proxy Anchor{}^{512} (Kim et al., 2020) CVPR20</td><td>BNI</td><td>68.4</td><td>79.2</td><td>86.8</td><td>91.6</td><td>-</td><td>86.1</td><td>91.7</td><td>95.0</td><td>97.3</td><td>-</td><td>79.1</td><td>90.8</td><td>96.2</td><td>-</td></tr><tr><td>Proxy Anchor{}^{512} (Kim et al., 2020) CVPR20</td><td>R50</td><td>69.7</td><td>80.0</td><td>87.0</td><td>92.4</td><td>-</td><td>87.7</td><td>92.9</td><td>95.8</td><td>97.9</td><td>-</td><td>80.0</td><td>91.7</td><td>96.6</td><td>-</td></tr><tr><td>Proxy Few{}^{512} (Zhu et al., 2020) NeurIPS20</td><td>BNI</td><td>66.6</td><td>77.6</td><td>86.4</td><td>-</td><td>69.8</td><td>85.5</td><td>91.8</td><td>95.3</td><td>-</td><td>72.4</td><td>78.0</td><td>90.6</td><td>96.2</td><td>90.2</td></tr><tr><td>Ours{}^{512}</td><td>R50</td><td>70.3</td><td>80.3</td><td>87.6</td><td>92.7</td><td>74.0</td><td>88.1</td><td>93.3</td><td>96.2</td><td>98.2</td><td>74.8</td><td>81.4</td><td>91.3</td><td>95.9</td><td>92.6</td></tr></tbody></table>", "caption": "Table 1: Retrieval and Clustering performance on CUB-200-2011, CARS196 and Stanford Online Products datasets. Bold indicates best, red second best, and blue third best results. The exponents attached to the method name indicates the embedding dimension. BB=backbone, G=GoogLeNet, BNI=BN-Inception and R50=ResNet50.", "list_citation_info": ["Zhu et al. (2020) Zhu, Y., Yang, M., Deng, C., and Liu, W. Fewer is more: A deep graph metric learning perspective using fewer proxies. In Advances in Neural Information Processing Systems (NeurIPS), 2020.", "Jacob et al. (2019) Jacob, P., Picard, D., Histace, A., and Klein, E. Metric learning with HORDE: high-order regularizer for deep embeddings. In International Conference on Computer Vision (ICCV), 2019.", "Xuan et al. (2020b) Xuan, H., Stylianou, A., and Pless, R. Improved embeddings with easy positive triplet mining. In Winter Conference on Applications of Computer Vision (WACV), 2020b.", "Sohn (2016) Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in Neural Information Processing Systems (NIPS), 2016.", "Qian et al. (2019) Qian, Q., Shang, L., Sun, B., Hu, J., Tacoma, T., Li, H., and Jin, R. Softtriple loss: Deep metric learning without triplet sampling. In International Conference on Computer Vision (ICCV), 2019.", "Milbich et al. (2020) Milbich, T., Roth, K., Bharadhwaj, H., Sinha, S., Bengio, Y., Ommer, B., and Cohen, J. P. Diva: Diverse visual feature aggregation for deep metric learning. In European Conference in Computer Vision (ECCV), 2020.", "Teh et al. (2020) Teh, E. W., DeVries, T., and Taylor, G. W. Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. In European Conference on Computer Vision (ECCV), 2020.", "Sanakoyeu et al. (2019) Sanakoyeu, A., Tschernezki, V., B\u00fcchler, U., and Ommer, B. Divide and conquer the embedding space for metric learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "Wang et al. (2017) Wang, J., Zhou, F., Wen, S., Liu, X., and Lin, Y. Deep metric learning with angular loss. In International Conference on Computer Vision (ICCV), 2017.", "Zhai & Wu (2019) Zhai, A. and Wu, H. Classification is a strong baseline for deep metric learning. In British Machine Vision Conference (BMVC), 2019.", "Manmatha et al. (2017) Manmatha, R., Wu, C., Smola, A. J., and Kr\u00e4henb\u00fchl, P. Sampling matters in deep embedding learning. In International Conference on Computer Vision (ICCV), 2017.", "Wang et al. (2019a) Wang, X., Han, X., Huang, W., Dong, D., and Scott, M. R. Multi-similarity loss with general pair weighting for deep metric learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019a.", "Kim et al. (2018) Kim, W., Goyal, B., Chawla, K., Lee, J., and Kwon, K. Attention-based ensemble for deep metric learning. In European Conference on Computer Vision (ECCV), 2018.", "Wang et al. (2019b) Wang, X., Hua, Y., Kodirov, E., Hu, G., Garnier, R., and Robertson, N. M. Ranked list loss for deep metric learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019b.", "Elezi et al. (2020) Elezi, I., Vascon, S., Torchinovich, A., Pelillo, M., and Leal-Taix\u00e9, L. The group loss for deep metric learning. In European Conference in Computer Vision (ECCV), 2020.", "Movshovitz-Attias et al. (2017) Movshovitz-Attias, Y., Toshev, A., Leung, T. K., Ioffe, S., and Singh, S. No fuss distance metric learning using proxies. In International Conference on Computer Vision (ICCV), 2017.", "Kim et al. (2020) Kim, S., Kim, D., Cho, M., and Kwak, S. Proxy anchor loss for deep metric learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "Schroff et al. (2015) Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A unified embedding for face recognition and clustering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "Park et al. (2019a) Park, W., Kim, D., Lu, Y., and Cho, M. Relational knowledge distillation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019a.", "Brattoli et al. (2019) Brattoli, B., Roth, K., and Ommer, B. MIC: mining interclass characteristics for improved metric learning. In International Conference on Computer Vision (ICCV), 2019.", "Roth et al. (2020) Roth, K., Milbich, T., and Ommer, B. PADS: policy-adapted sampling for visual similarity learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "Law et al. (2017) Law, M. T., Urtasun, R., and Zemel, R. S. Deep spectral clustering learning. In Proceedings of the 34th International Conference on Machine Learning (ICML), 2017.", "Ge et al. (2018) Ge, W., Huang, W., Dong, D., and Scott, M. R. Deep metric learning with hierarchical triplet loss. In European Conference in Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>BB</th><th>R@1</th><th>R@10</th><th>R@20</th><th>R@40</th></tr></thead><tbody><tr><td>FashionNet{}^{4096} (Liu et al., 2016) CVPR16</td><td>V</td><td>53.0</td><td>73.0</td><td>76.0</td><td>79.0</td></tr><tr><td>A-BIER{}^{512} (Opitz et al., 2020) PAMI20</td><td>G</td><td>83.1</td><td>95.1</td><td>96.9</td><td>97.8</td></tr><tr><td>ABE{}^{512} (Kim et al., 2018) ECCV18</td><td>G</td><td>87.3</td><td>96.7</td><td>97.9</td><td>98.5</td></tr><tr><td>Multi-similarity{}^{512} (Wang et al., 2019a) CVPR19</td><td>BNI</td><td>89.7</td><td>97.9</td><td>98.5</td><td>99.1</td></tr><tr><td>Learning to Rank{}^{512} (\u00c7akir et al., 2019)</td><td>R50</td><td>90.9</td><td>97.7</td><td>98.5</td><td>98.9</td></tr><tr><td>HORDE{}^{512} (Jacob et al., 2019) ICCV19</td><td>BNI</td><td>90.4</td><td>97.8</td><td>98.4</td><td>98.9</td></tr><tr><td>MIC{}^{128} (Brattoli et al., 2019) ICCV19</td><td>R50</td><td>88.2</td><td>97.0</td><td>98.0</td><td>98.8</td></tr><tr><td>Proxy NCA++{}^{512} (Teh et al., 2020) ECCV20</td><td>R50</td><td>90.4</td><td>98.1</td><td>98.8</td><td>99.2</td></tr><tr><td>Proxy Anchor{}^{512} (Kim et al., 2020) CVPR20</td><td>BNI</td><td>91.5</td><td>98.1</td><td>98.8</td><td>99.1</td></tr><tr><td>Proxy Anchor{}^{512} (Kim et al., 2020) CVPR20</td><td>R50</td><td>92.1</td><td>98.1</td><td>98.7</td><td>99.2</td></tr><tr><td>Ours{}^{512}</td><td>R50</td><td>92.8</td><td>98.5</td><td>99.1</td><td>99.2</td></tr></tbody></table>", "caption": "Table 2: Retrieval performance on In Shop Clothes. ", "list_citation_info": ["\u00c7akir et al. (2019) \u00c7akir, F., He, K., Xia, X., Kulis, B., and Sclaroff, S. Deep metric learning to rank. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "Kim et al. (2020) Kim, S., Kim, D., Cho, M., and Kwak, S. Proxy anchor loss for deep metric learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "Teh et al. (2020) Teh, E. W., DeVries, T., and Taylor, G. W. Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. In European Conference on Computer Vision (ECCV), 2020.", "Jacob et al. (2019) Jacob, P., Picard, D., Histace, A., and Klein, E. Metric learning with HORDE: high-order regularizer for deep embeddings. In International Conference on Computer Vision (ICCV), 2019.", "Opitz et al. (2020) Opitz, M., Waltner, G., Possegger, H., and Bischof, H. Deep metric learning with BIER: boosting independent embeddings robustly. IEEE Trans. Pattern Anal. Mach. Intell. (tPAMI), 42(2):276\u2013290, 2020.", "Brattoli et al. (2019) Brattoli, B., Roth, K., and Ommer, B. MIC: mining interclass characteristics for improved metric learning. In International Conference on Computer Vision (ICCV), 2019.", "Liu et al. (2016) Liu, Z., Luo, P., Qiu, S., Wang, X., and Tang, X. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Conference on Computer Vision and Pattern Recognition, (CVPR), 2016.", "Wang et al. (2019a) Wang, X., Han, X., Huang, W., Dong, D., and Scott, M. R. Multi-similarity loss with general pair weighting for deep metric learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019a.", "Kim et al. (2018) Kim, W., Goyal, B., Chawla, K., Lee, J., and Kwon, K. Attention-based ensemble for deep metric learning. In European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">CUB-200-2011</th><th colspan=\"2\">Cars196</th><th colspan=\"2\">Stanford Online Products</th><th>In-Shop Clothes</th></tr><tr><th></th><th>R@1</th><th>NMI</th><th>R@1</th><th>NMI</th><th>R@1</th><th>NMI</th><th>R@1</th></tr></thead><tbody><tr><td>GL</td><td>65.5</td><td>69.0</td><td>85.6</td><td>72.7</td><td>75.7</td><td>91.1</td><td>-</td></tr><tr><td>Ours</td><td>70.3</td><td>74.0</td><td>88.1</td><td>74.8</td><td>81.4</td><td>92.6</td><td>92.8</td></tr><tr><td>GL 2</td><td>65.8</td><td>68.5</td><td>86.2</td><td>72.6</td><td>75.9</td><td>91.1</td><td>-</td></tr><tr><td>Ours 2</td><td>72.2</td><td>74.3</td><td>90.9</td><td>74.9</td><td>81.8</td><td>92.7</td><td>92.9</td></tr><tr><td>GL 5</td><td>66.9</td><td>70.0</td><td>88.0</td><td>74.2</td><td>76.3</td><td>91.1</td><td>-</td></tr><tr><td>Ours 5</td><td>73.1</td><td>74.4</td><td>91.5</td><td>75.4</td><td>82.1</td><td>92.8</td><td>93.4</td></tr></tbody></table>", "caption": "Table 4: Performance of our ensembles and comparisons with the ensemble models of (Elezi et al., 2020).", "list_citation_info": ["Elezi et al. (2020) Elezi, I., Vascon, S., Torchinovich, A., Pelillo, M., and Leal-Taix\u00e9, L. The group loss for deep metric learning. In European Conference in Computer Vision (ECCV), 2020."]}, {"table": "<table><thead><tr><th></th><th>CUB-200-2011</th><th>Cars196</th><th>Stanford Online Products</th><th>In-Shop Clothes</th></tr></thead><tbody><tr><th>Our results on (Teh et al., 2020)</th><td>66.3</td><td>84.9</td><td>79.8</td><td>90.4</td></tr><tr><th>Results in (Teh et al., 2020)</th><td>64.7 \\pm 1.6</td><td>85.1 \\pm 0.3</td><td>79.6 \\pm 0.6</td><td>87.6 \\pm 1.0</td></tr></tbody></table>", "caption": "Table 6: Comparison of Recall@1 on images of size 227\\times 227 using ProxyNCA ++ (Teh et al., 2020) of the results reported in (Teh et al., 2020) and our results obtained by running their code.", "list_citation_info": ["Teh et al. (2020) Teh, E. W., DeVries, T., and Taylor, G. W. Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. In European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><tbody><tr><td></td><td colspan=\"2\">CUB-200-2011</td><td colspan=\"2\">Cars196</td><td colspan=\"2\">Stanford Online Products</td><td>In-Shop Clothes</td></tr><tr><td></td><td>R@1</td><td>NMI</td><td>R@1</td><td>NMI</td><td>R@1</td><td>NMI</td><td>R@1</td></tr><tr><td>Horde{}^{512\\dagger} (Jacob et al., 2019)</td><td>66.3</td><td>-</td><td>83.9</td><td>-</td><td>80.1</td><td>-</td><td>90.4</td></tr><tr><td>Proxy NCA++{}^{512\\dagger} (Teh et al., 2020)</td><td>69.0</td><td>73.9</td><td>86.5</td><td>73.8</td><td>80.7</td><td>-</td><td>90.4</td></tr><tr><td>Proxy Anchor{}^{512\\dagger} (Kim et al., 2020)</td><td>71.1</td><td>-</td><td>88.3</td><td>-</td><td>80.3</td><td>-</td><td>92.6</td></tr><tr><td>Ours{}^{512}</td><td>70.3</td><td>74.0</td><td>88.1</td><td>74.8</td><td>81.4</td><td>92.6</td><td>92.8</td></tr><tr><td>Ours{}^{512\\dagger}</td><td>71.7</td><td>74.3</td><td>90.2</td><td>75.4</td><td>81.7</td><td>92.3</td><td>92.9</td></tr></tbody></table>", "caption": "Table 7: Performance of our approach using larger images compared to the approaches that only report their results on larger images. \\dagger indicates results on larger images. Proxy Anchor (Kim et al., 2020) presents the results both in regular (shown in the tables in the main paper) and large size images.", "list_citation_info": ["Kim et al. (2020) Kim, S., Kim, D., Cho, M., and Kwak, S. Proxy anchor loss for deep metric learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "Jacob et al. (2019) Jacob, P., Picard, D., Histace, A., and Klein, E. Metric learning with HORDE: high-order regularizer for deep embeddings. In International Conference on Computer Vision (ICCV), 2019.", "Teh et al. (2020) Teh, E. W., DeVries, T., and Taylor, G. W. Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. In European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><tbody><tr><td></td><td>CUB</td><td>CARS</td><td>SOP</td><td>In-Shop</td></tr><tr><td>Ours</td><td>70.3</td><td>88.1</td><td>81.4</td><td>92.8</td></tr><tr><td>Ours reality check</td><td>67.1\\pm0.69</td><td>86.7\\pm0.48</td><td>81.1\\pm0.13</td><td>92.5\\pm0.11</td></tr></tbody></table>", "caption": "Table 9: Performance of our approach following (Musgrave et al., 2020) to find the hyperparameters (e.g., number of epochs) without feedback from the test set.", "list_citation_info": ["Musgrave et al. (2020) Musgrave, K., Belongie, S. J., and Lim, S. A metric learning reality check. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.), Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXV, volume 12370 of Lecture Notes in Computer Science, pp. 681\u2013699, 2020."]}]}