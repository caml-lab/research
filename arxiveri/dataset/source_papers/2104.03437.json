{"title": "Captra: Category-level pose tracking for rigid and articulated objects from point clouds", "abstract": "In this work, we tackle the problem of category-level online pose tracking of objects from point cloud sequences. For the first time, we propose a unified framework that can handle 9DoF pose tracking for novel rigid object instances as well as per-part pose tracking for articulated objects from known categories. Here the 9DoF pose, comprising 6D pose and 3D size, is equivalent to a 3D amodal bounding box representation with free 6D pose. Given the depth point cloud at the current frame and the estimated pose from the last frame, our novel end-to-end pipeline learns to accurately update the pose. Our pipeline is composed of three modules: 1) a pose canonicalization module that normalizes the pose of the input depth point cloud; 2) RotationNet, a module that directly regresses small interframe delta rotations; and 3) CoordinateNet, a module that predicts the normalized coordinates and segmentation, enabling analytical computation of the 3D size and translation. Leveraging the small pose regime in the pose-canonicalized point clouds, our method integrates the best of both worlds by combining dense coordinate prediction and direct rotation regression, thus yielding an end-to-end differentiable pipeline optimized for 9DoF pose accuracy (without using non-differentiable RANSAC). Our extensive experiments demonstrate that our method achieves new state-of-the-art performance on category-level rigid object pose (NOCS-REAL275) and articulated object pose benchmarks (SAPIEN, BMVC) at the fastest FPS ~12.", "authors": ["Yijia Weng", " He Wang", " Qiang Zhou", " Yuzhe Qin", " Yueqi Duan", " Qingnan Fan", " Baoquan Chen", " Hao Su", " Leonidas J. Guibas"], "pdf_url": "https://arxiv.org/abs/2104.03437", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>NOCS[29]</th><th>CASS[6]</th><th>CPS++[17]</th><th>Oracle ICP</th><th>6-PACK[28]</th><th>6-PACK [28]</th><th>Ours</th><th>Ours+RGB seg.</th></tr><tr><th>Input</th><th>RGBD</th><th>RGBD</th><th>RGB</th><th>Depth</th><th>RGBD</th><th>RGBD</th><th>Depth</th><th>RGBD</th></tr><tr><th>Setting</th><th colspan=\"3\">Single frame</th><th colspan=\"5\">Tracking</th></tr><tr><th>Initialization</th><th>N/A</th><th>N/A</th><th>N/A</th><th>GT.</th><th>GT.</th><th>Pert.</th><th>Pert.</th><th>Pert.</th></tr></thead><tbody><tr><th>5{}^{\\circ}5cm \\uparrow</th><td>16.97</td><td>29.44</td><td>2.24</td><td>0.65</td><td>28.92</td><td>22.13</td><td>62.16</td><td>63.60</td></tr><tr><th>mIoU\\uparrow</th><td>55.15</td><td>55.98</td><td>30.02</td><td>14.69</td><td>55.42</td><td>53.58</td><td>64.10</td><td>69.19</td></tr><tr><th>R_{err}\\downarrow</th><td>20.18</td><td>14.17</td><td>25.32</td><td>40.28</td><td>19.33</td><td>19.66</td><td>5.94</td><td>6.43</td></tr><tr><th>T_{err}\\downarrow</th><td>4.85</td><td>12.07</td><td>21.62</td><td>7.71</td><td>3.31</td><td>3.62</td><td>7.92</td><td>4.18</td></tr></tbody></table>", "caption": "Table 1: Results of category-level rigid object pose tracking on NOCS-REAL275. The results are averaged over all 6 categories.", "list_citation_info": ["[29] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6D object pose and size estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2642\u20132651, 2019.", "[17] Fabian Manhardt, Gu Wang, Benjamin Busam, Manuel Nickel, Sven Meier, Luca Minciullo, Xiangyang Ji, and Nassir Navab. Cps++: Improving class-level 6d pose and shape estimation from monocular images with self-supervised learning, 2020.", "[6] Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu. Learning canonical shape space for category-level 6D object pose and size estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11973\u201311982, 2020.", "[28] C. Wang, R. Mart\u00edn-Mart\u00edn, D. Xu, J. Lv, C. Lu, L. Fei-Fei, S. Savarese, and Y. Zhu. 6-pack: Category-level 6d pose tracker with anchor-based keypoints. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 10059\u201310066, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><td>5{}^{\\circ} 5cm\\uparrow</td><td>mIoU\\uparrow</td><td>R_{err}\\downarrow</td><td>T_{err}\\downarrow</td><td>\\theta_{err}\\downarrow</td><td>d_{err}\\downarrow</td></tr><tr><th>ANCSH* [14]</th><td>92.55</td><td>68.69</td><td>2.18</td><td>0.48</td><td>1.62</td><td>0.64</td></tr><tr><th>Oracle ICP</th><td>62.87</td><td>56.61</td><td>8.95</td><td>3.04</td><td>7.21</td><td>1.05</td></tr><tr><th>Ours</th><td>98.35</td><td>74.00</td><td>1.03</td><td>0.29</td><td>1.38</td><td>0.34</td></tr><tr><th>C-sRT regression</th><td>21.69</td><td>34.21</td><td>20.48</td><td>11.46</td><td>6.08</td><td>7.57</td></tr><tr><th>C-CoordinateNet</th><td>95.06</td><td>71.99</td><td>2.09</td><td>0.40</td><td>1.52</td><td>0.75</td></tr><tr><th>C-Crd. + DSAC++ [3]</th><td>95.68</td><td>68.21</td><td>1.80</td><td>0.47</td><td>1.61</td><td>0.56</td></tr><tr><th>Ours w/o L_{c},L_{s},L_{t}</th><td>97.63</td><td>72.09</td><td>1.24</td><td>0.35</td><td>1.43</td><td>0.36</td></tr><tr><th>Ours + Rot. Proj.</th><td>98.74</td><td>74.17</td><td>0.97</td><td>0.29</td><td>1.37</td><td>0.34</td></tr></tbody></table>", "caption": "Table 2: Experiment results and ablation studies of articulated object pose tracking on the held-out instances from SAPIEN. \\theta_{err} is averaged over all revolute joints, while d_{err} is averaged over all prismatic joints. Other results are averaged over parts and categories. See appendix I.2 for per-part, per-category results. Ours + Rot. Proj leverages kinematic constraints, see Section 5.8.", "list_citation_info": ["[14] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn Abbott, and Shuran Song. Category-level articulated object pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3706\u20133715, 2020.", "[3] Eric Brachmann and Carsten Rother. Learning less is more-6D camera localization via 3D surface regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4654\u20134662, 2018."]}, {"table": "<table><tbody><tr><td colspan=\"2\">Method</td><td>Michel et al.</td><td>ANCSH</td><td>ANCSH*</td><td>Ours</td></tr><tr><td colspan=\"2\">Setting</td><td colspan=\"2\">Known instance</td><td colspan=\"2\">Category-level</td></tr><tr><td>1</td><td>all / parts</td><td>64.8 / 65.5 66.9</td><td>94.1 / 97.5 94.7</td><td>74.7 / 89.1 78.5</td><td>95.5 / 99.8 95.7</td></tr><tr><td>2</td><td>all / parts</td><td>65.7 / 66.3 66.6</td><td>98.4 / 98.9 99.0</td><td>97.0 / 98.0 97.6</td><td>98.9 / 100.0 98.9</td></tr></tbody></table>", "caption": "Table 3: Results on two real sequences of an unseen laptop are measured in pose tolerance (the higher, the better, see [18]). The left two columns reported by [18, 14] are directly trained on the instance, whereas ANCSH*(with GT part mask) and ours are only trained on SAPIEN and have never seen the instance.", "list_citation_info": ["[18] Frank Michel, Alexander Krull, Eric Brachmann, Michael Ying Yang, Stefan Gumhold, and Carsten Rother. Pose estimation of kinematic chain instances via object coordinate regression. In BMVC, pages 181\u20131, 2015."]}]}