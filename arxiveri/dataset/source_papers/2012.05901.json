{"title": "Robust consistent video depth estimation", "abstract": "We present an algorithm for estimating consistent dense depth maps and camera poses from a monocular video. We integrate a learning-based depth prior, in the form of a convolutional neural network trained for single-image depth estimation, with geometric optimization, to estimate a smooth camera trajectory as well as detailed and stable depth reconstruction. Our algorithm combines two complementary techniques: (1) flexible deformation-splines for low-frequency large-scale alignment and (2) geometry-aware depth filtering for high-frequency alignment of fine depth details. In contrast to prior approaches, our method does not require camera poses as input and achieves robust reconstruction for challenging hand-held cell phone captures containing a significant amount of noise, shake, motion blur, and rolling shutter deformations. Our method quantitatively outperforms state-of-the-arts on the Sintel benchmark for both depth and pose estimations and attains favorable qualitative results across diverse wild datasets.", "authors": ["Johannes Kopf", " Xuejian Rong", " Jia-Bin Huang"], "pdf_url": "https://arxiv.org/abs/2012.05901", "list_table_and_caption": [{"table": "<table><tr><td></td><td colspan=\"4\">Depth - Error metric \\downarrow</td><td colspan=\"3\">Depth - Accuracy metric \\uparrow</td><td colspan=\"3\">Pose - Error metric \\downarrow</td></tr><tr><td>Method</td><td>Abs Rel</td><td>Sq Rel</td><td>RMSE</td><td>log RMSE</td><td>\\mathbf{\\delta&lt;1.25}</td><td>\\mathbf{\\delta&lt;1.25^{2}}</td><td>\\mathbf{\\delta&lt;1.25^{3}}</td><td>ATE (m)\\downarrow</td><td>RPE Trans (m)\\downarrow</td><td>RPE Rot (deg)\\downarrow</td></tr><tr><td>DeepV2D [56]</td><td>0.526</td><td>3.629</td><td>6.493</td><td>0.683</td><td>0.487</td><td>0.671</td><td>0.761</td><td>0.9526</td><td>0.3819</td><td>0.1869</td></tr><tr><td>Ours - Single-scale pose (aligned MiDaS)</td><td>0.380</td><td>2.617</td><td>5.773</td><td>0.533</td><td>0.562</td><td>0.736</td><td>0.832</td><td>0.1883</td><td>0.0806</td><td>0.0262</td></tr><tr><td>Ours - Single-scale pose + depth fine-tuning</td><td>0.472</td><td>3.444</td><td>6.340</td><td>0.635</td><td>0.534</td><td>0.694</td><td>0.790</td><td>0.1686</td><td>0.0724</td><td>0.0139</td></tr><tr><td>Ours - Single-scale pose + depth filter</td><td>0.375</td><td>2.546</td><td>5.763</td><td>0.530</td><td>0.569</td><td>0.738</td><td>0.835</td><td>0.1882</td><td>0.0806</td><td>0.0262</td></tr><tr><td>Ours - Flexible pose</td><td>0.379</td><td>2.702</td><td>5.795</td><td>0.533</td><td>0.565</td><td>0.744</td><td>0.836</td><td>0.1843</td><td>0.0723</td><td>0.0095</td></tr><tr><td>Ours - Flexible pose + depth fine-tuning</td><td>0.439</td><td>3.100</td><td>6.213</td><td>0.614</td><td>0.524</td><td>0.698</td><td>0.796</td><td>0.1656</td><td>0.0651</td><td>0.0070</td></tr><tr><td>Ours - Flexible pose + depth filter</td><td>0.377</td><td>2.657</td><td>5.786</td><td>0.531</td><td>0.568</td><td>0.745</td><td>0.837</td><td>0.1843</td><td>0.0723</td><td>0.0095</td></tr><tr><td>DeepV2D</td><td>0.526</td><td>3.620</td><td>6.470</td><td>0.670</td><td>0.486</td><td>0.674</td><td>0.760</td><td>0.9192</td><td>0.5834</td><td>0.2506</td></tr><tr><td>Ours - Single-scale pose (aligned MiDaS)</td><td>0.425</td><td>2.640</td><td>5.858</td><td>0.559</td><td>0.529</td><td>0.726</td><td>0.828</td><td>0.2210</td><td>0.0827</td><td>0.0258</td></tr><tr><td>Ours - Single-scale pose + depth fine-tuning</td><td>0.473</td><td>3.215</td><td>6.298</td><td>0.639</td><td>0.527</td><td>0.684</td><td>0.782</td><td>0.1620</td><td>0.0727</td><td>0.0116</td></tr><tr><td>Ours - Single-scale pose + depth filter</td><td>0.421</td><td>2.616</td><td>5.850</td><td>0.556</td><td>0.533</td><td>0.728</td><td>0.830</td><td>0.1803</td><td>0.0827</td><td>0.0258</td></tr><tr><td>Ours - Flexible pose</td><td>0.421</td><td>2.660</td><td>5.906</td><td>0.559</td><td>0.523</td><td>0.730</td><td>0.832</td><td>0.1831</td><td>0.0713</td><td>0.0088</td></tr><tr><td>Ours - Flexible pose + depth fine-tuning</td><td>0.438</td><td>3.053</td><td>6.300</td><td>0.605</td><td>0.525</td><td>0.705</td><td>0.807</td><td>0.1594</td><td>0.0652</td><td>0.0073</td></tr><tr><td>Ours - Flexible pose + depth filter</td><td>0.419</td><td>2.628</td><td>5.896</td><td>0.558</td><td>0.526</td><td>0.730</td><td>0.833</td><td>0.1831</td><td>0.0714</td><td>0.0088</td></tr></table>", "caption": "Table 1: Quantitative evaluations of depth and pose on the MPI Sintel benchmark (Top: Sintel Clean, Bottom: Sintel Final). For depth evaluation, we present per-frame evaluations on standard error and accuracy metrics. For pose evaluation, we present per-sequence evaluations on translational and rotational error metrics.", "list_citation_info": ["[56] Zachary Teed and Jia Deng. DeepV2D: Video to depth with differentiable structure from motion. In ICLR, 2020."]}]}