{"title": "Anabranch network for camouflaged object segmentation", "abstract": "Camouflaged objects attempt to conceal their texture into the background and discriminating them from the background is hard even for human beings. The main objective of this paper is to explore the camouflaged object segmentation problem, namely, segmenting the camouflaged object(s) for a given image. This problem has not been well studied in spite of a wide range of potential applications including the preservation of wild animals and the discovery of new species, surveillance systems, search-and-rescue missions in the event of natural disasters such as earthquakes, floods or hurricanes. This paper addresses a new challenging problem of camouflaged object segmentation. To address this problem, we provide a new image dataset of camouflaged objects for benchmarking purposes. In addition, we propose a general end-to-end network, called the Anabranch Network, that leverages both classification and segmentation tasks. Different from existing networks for segmentation, our proposed network possesses the second branch for classification to predict the probability of containing camouflaged object(s) in an image, which is then fused into the main branch for segmentation to boost up the segmentation accuracy. Extensive experiments conducted on the newly built dataset demonstrate the effectiveness of our network using various fully convolutional networks. \\url{https://sites.google.com/view/ltnghia/research/camo}", "authors": ["Trung-Nghia Le", " Tam V. Nguyen", " Zhongliang Nie", " Minh-Triet Tran", " Akihiro Sugimoto"], "pdf_url": "https://arxiv.org/abs/2105.09451", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset in test</th><th colspan=\"5\">CAMO</th><th colspan=\"5\">CAMO-COCO</th></tr></thead><tbody><tr><td>Method</td><td></td><td colspan=\"2\">Adaptive Threshold</td><td colspan=\"2\">Fixed Threshold</td><td></td><td colspan=\"2\">Adaptive Threshold</td><td colspan=\"2\">Fixed Threshold</td></tr><tr><td></td><td>MAE \\Downarrow</td><td>F{}_{\\beta} \\Uparrow</td><td>IOU \\Uparrow</td><td>F{}_{\\beta} \\Uparrow</td><td>IOU \\Uparrow</td><td>MAE \\Downarrow</td><td>F{}_{\\beta} \\Uparrow</td><td>IOU \\Uparrow</td><td>F{}_{\\beta} \\Uparrow</td><td>IOU \\Uparrow</td></tr><tr><td>DHS (Liu and Han, 2016) (pre-trained)</td><td>0.173</td><td>0.548</td><td>0.351</td><td>0.562</td><td>0.332</td><td>0.204</td><td>0.746</td><td>0.571</td><td>0.750</td><td>0.549</td></tr><tr><td>DHS (fine-tuned with CAMO)</td><td>0.129</td><td>0.640</td><td>0.459</td><td>0.643</td><td>0.444</td><td>0.169</td><td>0.794</td><td>0.633</td><td>0.793</td><td>0.618</td></tr><tr><td>DHS (fine-tuned with CAMO-COCO)</td><td>0.138</td><td>0.596</td><td>0.388</td><td>0.614</td><td>0.367</td><td>0.072</td><td>0.796</td><td>0.679</td><td>0.808</td><td>0.681</td></tr><tr><td>ANet-DHS (baseline)</td><td>0.130</td><td>0.626</td><td>0.437</td><td>0.631</td><td>0.423</td><td>0.072</td><td>0.812</td><td>0.712</td><td>0.814</td><td>0.705</td></tr><tr><td>DSS (Hou et al., 2017) (pre-trained)</td><td>0.157</td><td>0.564</td><td>0.320</td><td>0.563</td><td>0.333</td><td>0.176</td><td>0.757</td><td>0.559</td><td>0.756</td><td>0.570</td></tr><tr><td>DSS (fine-tuned with CAMO)</td><td>0.141</td><td>0.622</td><td>0.425</td><td>0.631</td><td>0.420</td><td>0.152</td><td>0.791</td><td>0.633</td><td>0.795</td><td>0.630</td></tr><tr><td>DSS (fine-tuned with CAMO-COCO)</td><td>0.145</td><td>0.582</td><td>0.385</td><td>0.584</td><td>0.381</td><td>0.076</td><td>0.790</td><td>0.686</td><td>0.792</td><td>0.687</td></tr><tr><td>ANet-DSS (baseline)</td><td>0.132</td><td>0.587</td><td>0.404</td><td>0.607</td><td>0.390</td><td>0.067</td><td>0.795</td><td>0.701</td><td>0.804</td><td>0.694</td></tr><tr><td>SRM (Wang et al., 2017b) (pre-trained)</td><td>0.171</td><td>0.448</td><td>0.258</td><td>0.425</td><td>0.213</td><td>0.191</td><td>0.699</td><td>0.535</td><td>0.685</td><td>0.502</td></tr><tr><td>SRM (fine-tuned with CAMO)</td><td>0.120</td><td>0.683</td><td>0.507</td><td>0.688</td><td>0.498</td><td>0.176</td><td>0.815</td><td>0.651</td><td>0.812</td><td>0.634</td></tr><tr><td>SRM (fine-tuned with CAMO-COCO)</td><td>0.127</td><td>0.663</td><td>0.454</td><td>0.656</td><td>0.421</td><td>0.067</td><td>0.830</td><td>0.717</td><td>0.831</td><td>0.708</td></tr><tr><td>ANet-SRM (baseline)</td><td>0.126</td><td>0.654</td><td>0.475</td><td>0.662</td><td>0.466</td><td>0.069</td><td>0.826</td><td>0.732</td><td>0.830</td><td>0.727</td></tr><tr><td>WSS (Wang et al., 2017a) (pre-trained)</td><td>0.178</td><td>0.559</td><td>0.323</td><td>0.531</td><td>0.265</td><td>0.197</td><td>0.754</td><td>0.567</td><td>0.740</td><td>0.528</td></tr><tr><td>WSS (fine-tuned with CAMO)</td><td>0.145</td><td>0.658</td><td>0.477</td><td>0.661</td><td>0.465</td><td>0.174</td><td>0.807</td><td>0.649</td><td>0.810</td><td>0.637</td></tr><tr><td>WSS (fine-tuned with CAMO-COCO)</td><td>0.149</td><td>0.642</td><td>0.439</td><td>0.638</td><td>0.382</td><td>0.085</td><td>0.811</td><td>0.678</td><td>0.820</td><td>0.687</td></tr><tr><td>ANet-WSS (baseline)</td><td>0.140</td><td>0.661</td><td>0.459</td><td>0.643</td><td>0.407</td><td>0.078</td><td>0.826</td><td>0.710</td><td>0.820</td><td>0.697</td></tr></tbody></table>", "caption": "Table 3: Experimental results on two datasets: CAMO dataset (the left part), and CAMO-COCO dataset (the right part). The evaluation is based on F-measure (Achanta et al., 2009) (the higher the better), IOU (Long et al., 2015) (the higher the better), and MAE (the smaller the better). The 1st and 2nd places are shown in blue and red, respectively.", "list_citation_info": ["Achanta et al. (2009) Achanta, R., Hemami, S., Estrada, F., Susstrunk, S., 2009. Frequency-tuned salient region detection, in: Conference on Computer Vision and Pattern Recognition, pp. 1597\u20131604.", "Long et al. (2015) Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic segmentation, in: Conference on Computer Vision and Pattern Recognition, pp. 3431\u20133440.", "Wang et al. (2017b) Wang, T., Borji, A., Zhang, L., Zhang, P., Lu, H., 2017b. A stagewise refinement model for detecting salient objects in images, in: International Conference on Computer Vision.", "Liu and Han (2016) Liu, N., Han, J., 2016. Dhsnet: Deep hierarchical saliency network for salient object detection, in: Conference on Computer Vision and Pattern Recognition, pp. 678\u2013686.", "Wang et al. (2017a) Wang, L., Lu, H., Wang, Y., Feng, M., Wang, D., Yin, B., Ruan, X., 2017a. Learning to detect salient objects with image-level supervision, in: Conference on Computer Vision and Pattern Recognition.", "Hou et al. (2017) Hou, Q., Cheng, M.M., Hu, X., Borji, A., Tu, Z., Torr, P., 2017. Deeply supervised salient object detection with short connections, in: Conference on Computer Vision and Pattern Recognition."]}, {"table": "<table><thead><tr><th>Method</th><th>Accuracy</th></tr></thead><tbody><tr><th>SVM-BoW (Fei-Fei and Perona, 2005)</th><td>54.0</td></tr><tr><th>AlexNet (Krizhevsky et al., 2012)</th><td>76.0</td></tr><tr><th>VGG-16 (Simonyan and Zisserman, 2014b)</th><td>88.6</td></tr><tr><th>ANet-DHS</th><td>91.4</td></tr><tr><th>ANet-DSS</th><td>89.2</td></tr><tr><th>ANet-SRM</th><td>90.6</td></tr><tr><th>ANet-WSS</th><td>89.6</td></tr></tbody></table>", "caption": "Table 4: The accuracy (%) of camouflaged object classification on the CAMO-COCO dataset.", "list_citation_info": ["Simonyan and Zisserman (2014b) Simonyan, K., Zisserman, A., 2014b. Very deep convolutional networks for large-scale image recognition. ILSVRC .", "Krizhevsky et al. (2012) Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks, in: Advances in Neural Information Processing Systems, pp. 1097\u20131105.", "Fei-Fei and Perona (2005) Fei-Fei, L., Perona, P., 2005. A bayesian hierarchical model for learning natural scene categories, in: Conference on Computer Vision and Pattern Recognition, pp. 524\u2013531 vol. 2."]}, {"table": "<table><thead><tr><th>Method</th><th>FCN</th><th>ANet (baseline)</th></tr></thead><tbody><tr><th>DHS (Liu and Han, 2016)</th><td>63</td><td>73</td></tr><tr><th>DSS (Hou et al., 2017)</th><td>76</td><td>79</td></tr><tr><th>SRM (Wang et al., 2017b)</th><td>81</td><td>86</td></tr><tr><th>WSS (Wang et al., 2017a)</th><td>61</td><td>65</td></tr></tbody></table>", "caption": "Table 5: The average wall-clock time in millisecond for each image.", "list_citation_info": ["Wang et al. (2017b) Wang, T., Borji, A., Zhang, L., Zhang, P., Lu, H., 2017b. A stagewise refinement model for detecting salient objects in images, in: International Conference on Computer Vision.", "Liu and Han (2016) Liu, N., Han, J., 2016. Dhsnet: Deep hierarchical saliency network for salient object detection, in: Conference on Computer Vision and Pattern Recognition, pp. 678\u2013686.", "Wang et al. (2017a) Wang, L., Lu, H., Wang, Y., Feng, M., Wang, D., Yin, B., Ruan, X., 2017a. Learning to detect salient objects with image-level supervision, in: Conference on Computer Vision and Pattern Recognition.", "Hou et al. (2017) Hou, Q., Cheng, M.M., Hu, X., Borji, A., Tu, Z., Torr, P., 2017. Deeply supervised salient object detection with short connections, in: Conference on Computer Vision and Pattern Recognition."]}]}