{"title": "Using Latent Space Regression to Analyze and Leverage Compositionality in GANs", "abstract": "In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs. We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and because it uses only a single forward pass, it can operate in real-time. Code is available on our project page: https://chail.github.io/latent-composition/.", "authors": ["Lucy Chai", " Jonas Wulff", " Phillip Isola"], "pdf_url": "https://arxiv.org/abs/2103.10426", "list_table_and_caption": [{"table": "<table><thead><tr><th>Model</th><th>Metric</th><th>GANSamples</th><th>GANReconstructions</th><th>Collage</th><th>CollageFilled</th><th>PoissonBlended</th><th>RGBInverted</th><th>RGB FilledInverted</th><th>RGBMInverted</th></tr></thead><tbody><tr><th rowspan=\"4\">ProGANChurch</th><th>FID</th><th>8.01</th><th>6.72</th><td>21.20</td><td>27.35</td><td>24.12</td><td>10.85</td><td>12.90</td><td>9.40</td></tr><tr><th>Density</th><th>0.94</th><th>1.04</th><td>0.15</td><td>0.15</td><td>0.45</td><td>1.02</td><td>0.92</td><td>1.23</td></tr><tr><th>Coverage</th><th>0.78</th><th>0.82</th><td>0.22</td><td>0.22</td><td>0.41</td><td>0.71</td><td>0.74</td><td>0.78</td></tr><tr><th>Masked L1</th><th>\u2013</th><th>\u2013</th><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>0.26</td><td>0.28</td><td>0.26</td></tr><tr><th rowspan=\"4\">ProGANLiving Room</th><th>FID</th><th>13.17</th><th>10.46</th><td>72.82</td><td>69.95</td><td>47.84</td><td>19.50</td><td>17.70</td><td>14.90</td></tr><tr><th>Density</th><th>0.69</th><th>0.86</th><td>0.01</td><td>0.03</td><td>0.28</td><td>0.98</td><td>0.81</td><td>1.05</td></tr><tr><th>Coverage</th><th>0.63</th><th>0.69</th><td>0.02</td><td>0.02</td><td>0.26</td><td>0.60</td><td>0.59</td><td>0.64</td></tr><tr><th>Masked L1</th><th>\u2013</th><th>\u2013</th><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>0.33</td><td>0.34</td><td>0.30</td></tr><tr><th rowspan=\"4\">ProGANCelebA-HQ</th><th>FID</th><th>10.43</th><th>12.38</th><td>81.82</td><td>82.09</td><td>53.76</td><td>16.24</td><td>15.35</td><td>17.41</td></tr><tr><th>Density</th><th>1.27</th><th>1.55</th><td>0.11</td><td>0.14</td><td>0.28</td><td>1.14</td><td>1.19</td><td>1.69</td></tr><tr><th>Coverage</th><th>0.83</th><th>0.84</th><td>0.03</td><td>0.03</td><td>0.15</td><td>0.72</td><td>0.74</td><td>0.78</td></tr><tr><th>Masked L1</th><th>\u2013</th><th>\u2013</th><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>0.26</td><td>0.26</td><td>0.24</td></tr><tr><th rowspan=\"4\">StyleGANChurch</th><th>FID</th><th>3.90</th><th>4.27</th><td>15.66</td><td>22.89</td><td>16.92</td><td>6.13</td><td>6.75</td><td>6.09</td></tr><tr><th>Density</th><th>0.88</th><th>1.06</th><td>0.15</td><td>0.14</td><td>0.41</td><td>1.31</td><td>1.31</td><td>1.50</td></tr><tr><th>Coverage</th><th>0.85</th><th>0.87</th><td>0.26</td><td>0.25</td><td>0.50</td><td>0.85</td><td>0.86</td><td>0.87</td></tr><tr><th>Masked L1</th><th>\u2013</th><th>\u2013</th><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>0.32</td><td>0.32</td><td>0.30</td></tr><tr><th rowspan=\"4\">StyleGANCar</th><th>FID</th><th>2.31</th><th>2.98</th><td>17.44</td><td>141.21</td><td>15.38</td><td>6.23</td><td>5.87</td><td>5.38</td></tr><tr><th>Density</th><th>1.07</th><th>1.32</th><td>0.37</td><td>0.42</td><td>0.61</td><td>1.69</td><td>1.53</td><td>1.51</td></tr><tr><th>Coverage</th><th>0.94</th><th>0.94</th><td>0.44</td><td>0.36</td><td>0.54</td><td>0.92</td><td>0.91</td><td>0.91</td></tr><tr><th>Masked L1</th><th>\u2013</th><th>\u2013</th><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>0.32</td><td>0.34</td><td>0.30</td></tr><tr><th rowspan=\"4\">StyleGANFFHQ</th><th>FID</th><th>2.86</th><th>6.27</th><td>92.15</td><td>92.00</td><td>36.54</td><td>26.25</td><td>25.63</td><td>24.09</td></tr><tr><th>Density</th><th>1.17</th><th>1.61</th><td>0.16</td><td>0.17</td><td>0.26</td><td>1.67</td><td>1.58</td><td>2.01</td></tr><tr><th>Coverage</th><th>0.90</th><th>0.89</th><td>0.02</td><td>0.02</td><td>0.24</td><td>0.67</td><td>0.67</td><td>0.74</td></tr><tr><th>Masked L1</th><th>\u2013</th><th>\u2013</th><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>0.30</td><td>0.30</td><td>0.28</td></tr></tbody></table>", "caption": "Table 3: Quantitative comparison of automated collaging and latent regression variations. The latent regressor and generator pulls the unrealistic composite images closer to the real manifold, yielding high density and coverage and lower FID on output, compared to the collaged inputs and Poisson blending. For ProGAN models, we use PyTorch models from Bau et al. (2019); for StyleGAN models, we use a Pytorch conversion of the Tensorflow models from  Karras et al. (2019a). ", "list_citation_info": ["Bau et al. (2019) David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba. Seeing what a gan cannot generate. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.", "Karras et al. (2019a) Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., 2019a."]}]}