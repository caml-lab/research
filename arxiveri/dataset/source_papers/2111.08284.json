{"title": "Few-shot self-rationalization with natural language prompts", "abstract": "Self-rationalization models that predict task labels and generate free-text elaborations for their predictions could enable more intuitive interaction with NLP systems. These models are, however, currently trained with a large amount of human-written free-text explanations for each task which hinders their broader usage. We propose to study a more realistic setting of self-rationalization using few training examples. We present FEB -- a standardized collection of four existing English-language datasets and associated metrics. We identify the right prompting approach by extensively exploring natural language prompts on FEB. Then, by using this prompt and scaling the model size, we demonstrate that making progress on few-shot self-rationalization is possible. We show there is still ample room for improvement in this task: the average plausibility of generated explanations assessed by human annotators is at most 51% (with GPT-3), while plausibility of human explanations is 76%. We hope that FEB and our proposed approach will spur the community to take on the few-shot self-rationalization challenge.", "authors": ["Ana Marasovi\u0107", " Iz Beltagy", " Doug Downey", " Matthew E. Peters"], "pdf_url": "https://arxiv.org/abs/2111.08284", "list_table_and_caption": [{"table": "<table><tr><td colspan=\"2\">FEB Tasks</td><td># Shots</td></tr><tr><td>E-SNLI</td><td><p>Classify the entailment relation between two sequences</p></td><td>16</td></tr><tr><td>ECQA</td><td><p>Select the correct answer to a given question from five answer choices</p></td><td>48</td></tr><tr><td>ComVE</td><td><p>Select one of two sequences as more nonsensical</p></td><td>24</td></tr><tr><td>SBIC</td><td><p>Classify a post as offensive or not</p></td><td>24</td></tr></table>", "caption": "Table 1: Tasks that we have included in FEB. The number of shots is the number of training instances per label. Training sets for all classification tasks are balanced and contain 48 instances. Sources: E-SNLI Camburu et al. (2018), ECQA Aggarwal et al. (2021), ComVE Wang et al. (2019), SBIC Sap et al. (2020).", "list_citation_info": ["Aggarwal et al. (2021) Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. 2021. Explanations for CommonsenseQA: New Dataset and Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3050\u20133065, Online. Association for Computational Linguistics.", "Wang et al. (2019) Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? a pilot study for sense making and explanation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4020\u20134026, Florence, Italy. Association for Computational Linguistics.", "Sap et al. (2020) Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5477\u20135490, Online. Association for Computational Linguistics.", "Camburu et al. (2018) Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)."]}, {"table": "<table><tr><td>FEB Task</td><td colspan=\"2\">Similar T5 Pretraining Tasks</td></tr><tr><td>E-SNLI </td><td>MNLIWilliams et al. (2018) </td><td><p>Classify the entailment relation between two sequences</p></td></tr><tr><td>ECQA </td><td>RECORDZhang et al. (2018) </td><td><p>Answer a cloze-style query about a passage given entities in it</p></td></tr><tr><td>ComVE </td><td>COPARoemmele et al. (2011) </td><td><p>Select one of two sequences as the cause/effect of a premise</p></td></tr><tr><td>SBIC </td><td>COLAWarstadt et al. (2019) </td><td><p>Classify a sentence as acceptable or not</p></td></tr></table>", "caption": "Table 6: The first column shows tasks that we have included in FEB. Tasks on the right are included in T5\u2019s pretraining and they are similar to FEB\u2019s tasks. We explore self-rationalization prompts for FEB\u2019s tasks based on the tasks on the right, and compare them to prompts designed as span infilling and QA (\u00a73).", "list_citation_info": ["Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.", "Zhang et al. (2018) Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv:1810.12885.", "Roemmele et al. (2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium Series.", "Warstadt et al. (2019) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641."]}, {"table": "<table><tr><td></td><td></td><td>Accuracy</td><td>BERTscore</td></tr><tr><td rowspan=\"4\">CoS-E</td><td>Infilling (b)</td><td>34.3{}_{0.4}</td><td>29.6{}_{0.3}</td></tr><tr><td>Infilling (n)</td><td>40.1{}_{0.4}</td><td>34.7{}_{0.3}</td></tr><tr><td>\\approxT5</td><td>51.7{}_{0.4}</td><td>44.6{}_{0.4}</td></tr><tr><td>SQuAD{}_{\\textnormal{T5}}</td><td>51.1{}_{0.3}</td><td>44.1{}_{0.3}</td></tr><tr><td></td><td>QA{}_{\\textnormal{SIMPLE}}</td><td>60.0{}_{0.3}</td><td>48.6{}_{0.3}</td></tr></table>", "caption": "Table 8: A comparison of all prompt types introduced in \u00a73 on CoS-E. We do not support using CoS-E in the future given the reported issues with it Narang et al. (2020); Wiegreffe and Marasovi\u0107 (2021), especially since ECQA is introduced.", "list_citation_info": ["Narang et al. (2020) Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training Text-to-Text Models to Explain their Predictions. arXiv:2004.14546."]}, {"table": "<table><tr><td></td><td>Size</td><td>Accuracy</td><td>BERTscore</td></tr><tr><td rowspan=\"5\">CoS-E</td><td>Base</td><td>58.3{}_{0.3}</td><td>50.4{}_{0.2}</td></tr><tr><td>Large</td><td>69.4{}_{0.3}</td><td>60.1{}_{0.3}</td></tr><tr><td>3B</td><td>75.4{}_{0.3}</td><td>65.3{}_{0.3}</td></tr><tr><td>GPT-3</td><td>68.4{}_{1.3}</td><td>59.5{}_{1.2}</td></tr></table>", "caption": "Table 9: The effect of scaling the UnifiedQA model size on self-rationalization of CoS-E. We do not support using CoS-E in the future given the reported issues with it Narang et al. (2020); Wiegreffe and Marasovi\u0107 (2021), especially since ECQA is introduced.", "list_citation_info": ["Narang et al. (2020) Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training Text-to-Text Models to Explain their Predictions. arXiv:2004.14546."]}, {"table": "<table><tr><td><p>Sentence1: The stove was cleaned with a cleaner. Sentence2: The stove was cleaned with a mop.</p></td></tr><tr><td><p>Nonsensical Sentence: Sentence2 Explanation: A mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: Infilling \\times Basic</p></td></tr><tr><td><p>Input: explain sensemaking choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop. &lt;extra_id_0&gt; because &lt;extra_id_1&gt;</p></td></tr><tr><td><p>Output: &lt;extra_id_0&gt; choice2 &lt;extra_id_1&gt; A mop is too large to clean the stove. &lt;extra_id_2&gt;</p></td></tr><tr><td><p>Prompt: Infilling \\times Natural Sounding</p></td></tr><tr><td><p>Input: explain sensemaking choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop. It is &lt;extra_id_0&gt; that choice2 is less common because &lt;extra_id_1&gt;</p></td></tr><tr><td><p>Output: &lt;extra_id_0&gt; True &lt;extra_id_1&gt; A mop is too large to clean the stove. &lt;extra_id_2&gt;</p></td></tr><tr><td><p>Prompt: \\approxT5 \\times COPA</p></td></tr><tr><td><p>Input: explain sensemaking choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop. Less common is choice2</p></td></tr><tr><td><p>Output: True because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: SQuAD{}_{\\textnormal{T5}} \\times Yes/No + Tags</p></td></tr><tr><td><p>Input: explain sensemaking question: Is choice2 more nonsensical? context: choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop.</p></td></tr><tr><td><p>Output: Yes because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: SQuAD{}_{\\textnormal{T5}} \\times What is\u2026? + Tags</p></td></tr><tr><td><p>Input: explain sensemaking question: What is more nonsensical? context: choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop.</p></td></tr><tr><td><p>Output: choice2 because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: QA{}_{\\textnormal{SIMPLE}} \\times Yes/No</p></td></tr><tr><td><p>Input: explain is choice2 more nonsensical? \\\\n The stove was cleaned with a cleaner. The stove was cleaned with a mop.&lt;/s&gt;</p></td></tr><tr><td><p>Output: yes because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: QA{}_{\\textnormal{SIMPLE}} \\times Yes/No + Tags</p></td></tr><tr><td><p>Input: explain is choice2 more nonsensical? \\\\n choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop.&lt;/s&gt;</p></td></tr><tr><td><p>Output: yes because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: QA{}_{\\textnormal{SIMPLE}} \\times Yes/No + Tags + Choices</p></td></tr><tr><td><p>Input: explain is choice2 more nonsensical? \\\\n (A) yes (B) no \\\\n choice1:  The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop.&lt;/s&gt;</p></td></tr><tr><td><p>Output: yes because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: QA{}_{\\textnormal{SIMPLE}} \\times What is\u2026?</p></td></tr><tr><td><p>Input: explain what is more nonsensical? \\\\n The stove was cleaned with a cleaner. The stove was cleaned with a mop.&lt;/s&gt;</p></td></tr><tr><td><p>Output: choice2 because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: QA{}_{\\textnormal{SIMPLE}} \\times What is\u2026? + Tags</p></td></tr><tr><td><p>Input: explain what is more nonsensical? \\\\n choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop.&lt;/s&gt;</p></td></tr><tr><td><p>Output: choice2 because a mop is too large to clean the stove.</p></td></tr><tr><td><p>Prompt: QA{}_{\\textnormal{SIMPLE}} \\times What is\u2026? + Tags + Choices</p></td></tr><tr><td><p>Input: explain what is more nonsensical? \\\\n (A) choice1 (B) choice2 \\\\n choice1: The stove was cleaned with a cleaner. choice2: The stove was cleaned with a mop.&lt;/s&gt;</p></td></tr><tr><td><p>Output: choice2 because a mop is too large to clean the stove.</p></td></tr></table>", "caption": "Table 11: ComVE self-rationalization prompts that we design and test. Infilling marks span-filling prompts; \\approxT5 prompts made by following the most similar T5 pretraining task (Table 1); SQuAD{}_{\\textnormal{T5}} prompts designed following SQuAD\u2019s formatting in T5 pretraining; and QA{}_{\\textnormal{SIMPLE}} prompts made following UnifiedQA. This table shows variations of these prompt types. We refer to spans \u201cchoice1:\u201d/\u201cchoice2:\u201d as Tags, and to \u201c(A) yes (B) no\u201d/\u201c(A) choice1 (B) choice2\u201d as Choices. Yes/No and What is\u2026? refer to a question type. Following Hendrycks et al. (2021), we add &lt;/s&gt; to the end of our QA{}_{\\textnormal{SIMPLE}} prompts. More info in \u00a73.", "list_citation_info": ["Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In The International Conference on Learning Representations (ICLR)."]}]}