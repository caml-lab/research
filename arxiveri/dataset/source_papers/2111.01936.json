{"title": "Revisiting spatio-temporal layouts for compositional action recognition", "abstract": "Recognizing human actions is fundamentally a spatio-temporal reasoning problem, and should be, at least to some extent, invariant to the appearance of the human and the objects involved. Motivated by this hypothesis, in this work, we take an object-centric approach to action recognition. Multiple works have studied this setting before, yet it remains unclear (i) how well a carefully crafted, spatio-temporal layout-based method can recognize human actions, and (ii) how, and when, to fuse the information from layout and appearance-based models. The main focus of this paper is compositional/few-shot action recognition, where we advocate the usage of multi-head attention (proven to be effective for spatial reasoning) over spatio-temporal layouts, i.e., configurations of object bounding boxes. We evaluate different schemes to inject video appearance information to the system, and benchmark our approach on background cluttered action recognition. On the Something-Else and Action Genome datasets, we demonstrate (i) how to extend multi-head attention for spatio-temporal layout-based action recognition, (ii) how to improve the performance of appearance-based models by fusion with layout-based models, (iii) that even on non-compositional background-cluttered video datasets, a fusion between layout- and appearance-based models improves the performance.", "authors": ["Gorjan Radevski", " Marie-Francine Moens", " Tinne Tuytelaars"], "pdf_url": "https://arxiv.org/abs/2111.01936", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td colspan=\"4\">Compositional setting</td><td colspan=\"4\">Few-shot setting</td></tr><tr><td></td><td colspan=\"2\">Obj. predictions</td><td colspan=\"2\">Oracle</td><td colspan=\"2\">Obj. predictions</td><td colspan=\"2\">Oracle</td></tr><tr><td>Method</td><td>Top 1 acc.</td><td>Top 5 acc.</td><td>Top 1. acc.</td><td>Top 5 acc.</td><td>Top 1 acc. (5-shot)</td><td>Top 1. acc. (10-shot)</td><td>Top 1 acc. (5-shot)</td><td>Top 1 acc. (10-shot)</td></tr><tr><td>STIN [Materzynska et al.(2020)Materzynska, Xiao, Herzig, Xu, Wang, andDarrell]</td><td>37.2</td><td>62.4</td><td>51.4</td><td>79.3</td><td>17.7</td><td>20.8</td><td>27.7</td><td>33.5</td></tr><tr><td>SFI [Yan et al.(2020)Yan, Xie, Shu, and Tang]</td><td>\u2014</td><td>\u2014</td><td>44.1</td><td>74.0</td><td>\u2014</td><td>\u2014</td><td>24.3</td><td>29.8</td></tr><tr><td>STLT (Ours)</td><td>41.6</td><td>67.9</td><td>59.0</td><td>86.0</td><td>18.8</td><td>24.8</td><td>31.4</td><td>38.6</td></tr><tr><td>I3D [Carreira and Zisserman(2017)]</td><td>46.8</td><td>72.2</td><td>46.8</td><td>72.2</td><td>21.8</td><td>26.7</td><td>21.8</td><td>26.7</td></tr><tr><td>STIN [Materzynska et al.(2020)Materzynska, Xiao, Herzig, Xu, Wang, andDarrell] + I3D [Carreira and Zisserman(2017)]</td><td>48.2</td><td>72.6</td><td>54.6</td><td>79.4</td><td>23.7</td><td>27.0</td><td>28.1</td><td>33.6</td></tr><tr><td>STRG [Wang and Gupta(2018)]</td><td>52.3</td><td>78.3</td><td>\u2014</td><td>\u2014</td><td>24.8</td><td>29.9</td><td>\u2014</td><td>\u2014</td></tr><tr><td>SFI [Yan et al.(2020)Yan, Xie, Shu, and Tang]</td><td>\u2014</td><td>\u2014</td><td>59.6</td><td>85.8</td><td>\u2014</td><td>\u2014</td><td>30.7</td><td>36.2</td></tr><tr><td>CACNF (Ours)</td><td>56.9</td><td>82.5</td><td>67.1</td><td>90.4</td><td>27.1</td><td>33.9</td><td>37.1</td><td>45.5</td></tr></tbody></table>", "caption": "Table 2: Something-Else SOTA comparisons. Left: Compositional setting, Right: Few-shot setting. Top: Layout-based methods, Bottom: I3D and Multimodal methods.", "list_citation_info": ["[Wang and Gupta(2018)] Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In Proceedings of the European Conference on Computer Vision (ECCV), pages 399\u2013417, 2018.", "[Yan et al.(2020)Yan, Xie, Shu, and Tang] Rui Yan, Lingxi Xie, Xiangbo Shu, and Jinhui Tang. Interactive fusion of multi-level features for compositional activity recognition. arXiv preprint arXiv:2012.05689, 2020.", "[Carreira and Zisserman(2017)] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.", "[Materzynska et al.(2020)Materzynska, Xiao, Herzig, Xu, Wang, and Darrell] Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, and Trevor Darrell. Something-else: Compositional action recognition with spatial-temporal interaction networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1049\u20131059, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Input modalities</th><th>Num. categories</th><th>Obj. predictions mAP</th><th>Oracle mAP</th></tr></thead><tbody><tr><td>LFB [Wu et al.(2019)Wu, Feichtenhofer, Fan, He, Krahenbuhl, andGirshick]</td><td>Video Appearance</td><td>\u2014</td><td>42.5</td><td>42.5</td></tr><tr><td>SGFB [Ji et al.(2020)Ji, Krishna, Fei-Fei, and Niebles]</td><td>Scene Graphs &amp; Video Appearance</td><td>38</td><td>44.3 (1.8)</td><td>60.3 (17.8)</td></tr><tr><td>I3D (Ours) [Carreira and Zisserman(2017)]</td><td>Video Appearance</td><td>\u2014</td><td>33.5</td><td>33.5</td></tr><tr><td>STLT (Ours)</td><td>Obj. detections</td><td>2</td><td>16.1</td><td>19.9</td></tr><tr><td>STLT + I3D (Ours)</td><td>Obj. detections &amp; Video Appearance</td><td>2</td><td>33.8 (0.3)</td><td>36.5 (3.0)</td></tr><tr><td>STLT (Ours)</td><td>Obj. detections</td><td>38</td><td>30.2</td><td>60.6</td></tr><tr><td>STLT + I3D (Ours)</td><td>Obj. detections &amp; Video Appearance</td><td>38</td><td>38.5 (5.0)</td><td>61.63 (28.1)</td></tr></tbody></table>", "caption": "Table 3: Action Genome results. From top to bottom: Baselines, I3D, STLT and STLT + I3D ensemble with 2 generic obj. categories (person, obj.), STLT and STLT + I3D ensemble with all 38 obj. categories (person, book, phone, etc.). Relative mAP improvement over appearance method in parenthesis.", "list_citation_info": ["[Wu et al.(2019)Wu, Feichtenhofer, Fan, He, Krahenbuhl, and Girshick] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 284\u2013293, 2019.", "[Ji et al.(2020)Ji, Krishna, Fei-Fei, and Niebles] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10236\u201310247, 2020.", "[Carreira and Zisserman(2017)] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017."]}]}