{"title": "Learning Salient Boundary Feature for Anchor-Free Temporal Action Localization", "abstract": "Temporal action localization is an important yet challenging task in video understanding. Typically, such a task aims at inferring both the action category and localization of the start and end frame for each action instance in a long, untrimmed video.While most current models achieve good results by using pre-defined anchors and numerous actionness, such methods could be bothered with both large number of outputs and heavy tuning of locations and sizes corresponding to different anchors. Instead, anchor-free methods is lighter, getting rid of redundant hyper-parameters, but gains few attention. In this paper, we propose the first purely anchor-free temporal localization method, which is both efficient and effective. Our model includes (i) an end-to-end trainable basic predictor, (ii) a saliency-based refinement module to gather more valuable boundary features for each proposal with a novel boundary pooling, and (iii) several consistency constraints to make sure our model can find the accurate boundary given arbitrary proposals. Extensive experiments show that our method beats all anchor-based and actionness-guided methods with a remarkable margin on THUMOS14, achieving state-of-the-art results, and comparable ones on ActivityNet v1.3. Code is available at https://github.com/TencentYoutuResearch/ActionDetection-AFSD.", "authors": ["Chuming Lin", " Chengming Xu", " Donghao Luo", " Yabiao Wang", " Ying Tai", " Chengjie Wang", " Jilin Li", " Feiyue Huang", " Yanwei Fu"], "pdf_url": "https://arxiv.org/abs/2103.13137", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Type</td><td rowspan=\"2\">Model</td><td rowspan=\"2\">Backbone</td><td colspan=\"6\">THUMOS14</td><td colspan=\"4\">ActivityNet1.3</td></tr><tr><td>0.3</td><td>0.4</td><td>0.5</td><td>0.6</td><td>0.7</td><td>Avg.</td><td>0.5</td><td>0.75</td><td>0.95</td><td>Avg.</td></tr><tr><td rowspan=\"6\">Anchor-based</td><td>SSAD [19]</td><td>TS</td><td>43.0</td><td>35.0</td><td>24.6</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><td>TURN [10]</td><td>C3D</td><td>44.1</td><td>34.9</td><td>25.6</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><td>R-C3D [38]</td><td>C3D</td><td>44.8</td><td>35.6</td><td>28.9</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>26.8</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><td>CBR [11]</td><td>TS</td><td>50.1</td><td>41.3</td><td>31.0</td><td>19.1</td><td>9.9</td><td>30.3</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><td>TAL [7]</td><td>I3D</td><td>53.2</td><td>48.5</td><td>42.8</td><td>33.8</td><td>20.8</td><td>39.8</td><td>38.2</td><td>18.3</td><td>1.3</td><td>20.2</td></tr><tr><td>GTAN [23]</td><td>P3D</td><td>57.8</td><td>47.2</td><td>38.8</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>52.6</td><td>34.1</td><td>8.9</td><td>34.3</td></tr><tr><td rowspan=\"7\">Actionness</td><td>CDC [32]</td><td>\u2014</td><td>40.1</td><td>29.4</td><td>23.3</td><td>13.1</td><td>7.9</td><td>22.8</td><td>45.3</td><td>26.0</td><td>0.2</td><td>23.8</td></tr><tr><td>SSN [44]</td><td>TS</td><td>51.0</td><td>41.0</td><td>29.8</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>43.2</td><td>28.7</td><td>5.6</td><td>28.3</td></tr><tr><td>BSN [20]</td><td>TS</td><td>53.5</td><td>45.0</td><td>36.9</td><td>28.4</td><td>20.0</td><td>36.8</td><td>46.5</td><td>30.0</td><td>8.0</td><td>30.0</td></tr><tr><td>BMN [18]</td><td>TS</td><td>56.0</td><td>47.4</td><td>38.8</td><td>29.7</td><td>20.5</td><td>38.5</td><td>50.1</td><td>34.8</td><td>8.3</td><td>33.9</td></tr><tr><td>DBG [17]</td><td>TS</td><td>57.8</td><td>49.4</td><td>42.8</td><td>33.8</td><td>21.7</td><td>41.1</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><td>G-TAD [39]</td><td>TS</td><td>54.5</td><td>47.6</td><td>40.2</td><td>30.8</td><td>23.4</td><td>39.3</td><td>50.4</td><td>34.6</td><td>9.0</td><td>34.1</td></tr><tr><td>BU-TAL [43]</td><td>I3D</td><td>53.9</td><td>50.7</td><td>45.4</td><td>38.0</td><td>28.5</td><td>43.3</td><td>43.5</td><td>33.9</td><td>9.2</td><td>30.1</td></tr><tr><td></td><td>BC-GNN [2]</td><td>TS</td><td>57.1</td><td>49.1</td><td>40.4</td><td>31.2</td><td>23.1</td><td>40.2</td><td>50.6</td><td>34.8</td><td>9.4</td><td>34.3</td></tr><tr><td rowspan=\"2\">Other</td><td>A2Net [40]</td><td>I3D</td><td>58.6</td><td>54.1</td><td>45.5</td><td>32.5</td><td>17.2</td><td>41.6</td><td>43.6</td><td>28.7</td><td>3.7</td><td>27.8</td></tr><tr><td>G-TAD+PGCN [41]</td><td>I3D</td><td>66.4</td><td>60.4</td><td>51.6</td><td>37.6</td><td>22.9</td><td>47.8</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><td>Anchor-free</td><td>Ours</td><td>I3D</td><td>67.3</td><td>62.4</td><td>55.5</td><td>43.7</td><td>31.1</td><td>52.0</td><td>52.4</td><td>35.3</td><td>6.5</td><td>34.4</td></tr></tbody></table>", "caption": "Table 1:  Performance comparison with state-of-the-art methods on THUMOS14 and ActivityNet1.3, measured by mAP at different IoU thresholds, and average mAP in [0.3:0.1:0.7] on THUMOS14 and [0.5:0.05:0.95] on ActivityNet1.3.", "list_citation_info": ["[2] Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang, Qiyue Liu, and Junhui Liu. Boundary content graph neural network for temporal action proposal generation. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.", "[39] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and Bernard Ghanem. G-tad: Sub-graph localization for temporal action detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10156\u201310165, 2020.", "[40] Le Yang, Houwen Peng, Dingwen Zhang, Jianlong Fu, and Junwei Han. Revisiting anchor mechanisms for temporal action localization. IEEE Transactions on Image Processing, 29:8535\u20138548, 2020.", "[20] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. Bsn: Boundary sensitive network for temporal action proposal generation. In ECCV, pages 3\u201319, 2018.", "[43] Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang, and Qi Tian. Bottom-up temporal action localization with mutual regularization. In European Conference on Computer Vision, pages 539\u2013555. Springer, 2020.", "[10] Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and Ram Nevatia. Turn tap: Temporal unit regression network for temporal action proposals. In Proceedings of the IEEE international conference on computer vision, pages 3628\u20133636, 2017.", "[32] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5734\u20135743, 2017.", "[17] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang, and Rongrong Ji. Fast learning of temporal action proposal via dense boundary generator. In AAAI, pages 11499\u201311506, 2020.", "[18] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for temporal action proposal generation. In Proceedings of the IEEE International Conference on Computer Vision, pages 3889\u20133898, 2019.", "[11] Jiyang Gao, Zhenheng Yang, and Ram Nevatia. Cascaded boundary regression for temporal action detection. arXiv preprint arXiv:1705.01180, 2017.", "[41] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, and Chuang Gan. Graph convolutional networks for temporal action localization. In Proceedings of the IEEE International Conference on Computer Vision, pages 7094\u20137103, 2019.", "[19] Tianwei Lin, Xu Zhao, and Zheng Shou. Single shot temporal action detection. In Proceedings of the 25th ACM international conference on Multimedia, pages 988\u2013996, 2017.", "[44] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin. Temporal action detection with structured segment networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 2914\u20132923, 2017.", "[23] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo Luo, and Tao Mei. Gaussian temporal awareness networks for action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 344\u2013353, 2019.", "[38] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region convolutional 3d network for temporal activity detection. In Proceedings of the IEEE international conference on computer vision, pages 5783\u20135792, 2017.", "[7] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia Deng, and Rahul Sukthankar. Rethinking the faster r-cnn architecture for temporal action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1130\u20131139, 2018."]}, {"table": "<table><thead><tr><th>Model</th><th>GPU</th><th>FPS</th></tr></thead><tbody><tr><td>S-CNN [33]</td><td>\u2014</td><td>60</td></tr><tr><td>DAP [9]</td><td>\u2014</td><td>134</td></tr><tr><td>CDC [32]</td><td>TITAN Xm</td><td>500</td></tr><tr><td>SS-TAD [4]</td><td>TITAN Xm</td><td>701</td></tr><tr><td>R-C3D [38]</td><td>TITAN Xm</td><td>569</td></tr><tr><td>R-C3D [38]</td><td>TITAN Xp</td><td>1030</td></tr><tr><td>Ours</td><td>1080 Ti</td><td>3259</td></tr><tr><td>Ours</td><td>V100</td><td>4057</td></tr></tbody></table>", "caption": "Table 3:  Comparison of inference speed between our method and other methods on THUMOS14.", "list_citation_info": ["[32] Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5734\u20135743, 2017.", "[33] Zheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1049\u20131058, 2016.", "[38] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region convolutional 3d network for temporal activity detection. In Proceedings of the IEEE international conference on computer vision, pages 5783\u20135792, 2017.", "[4] Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, and Juan Carlos Niebles. End-to-end, single-stream temporal action detection in untrimmed videos. 2019.", "[9] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Daps: Deep action proposals for action understanding. In European Conference on Computer Vision, pages 768\u2013784. Springer, 2016."]}]}