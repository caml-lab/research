{"title": "Self-slimmed Vision Transformer", "abstract": "Vision transformers (ViTs) have become the popular structures and outperformed convolutional neural networks (CNNs) on various vision tasks. However, such powerful transformers bring a huge computation burden, because of the exhausting token-to-token comparison. The previous works focus on dropping insignificant tokens to reduce the computational cost of ViTs. But when the dropping ratio increases, this hard manner will inevitably discard the vital tokens, which limits its efficiency. To solve the issue, we propose a generic self-slimmed learning approach for vanilla ViTs, namely SiT. Specifically, we first design a novel Token Slimming Module (TSM), which can boost the inference efficiency of ViTs by dynamic token aggregation. As a general method of token hard dropping, our TSM softly integrates redundant tokens into fewer informative ones. It can dynamically zoom visual attention without cutting off discriminative token relations in the images, even with a high slimming ratio. Furthermore, we introduce a concise Feature Recalibration Distillation (FRD) framework, wherein we design a reverse version of TSM (RTSM) to recalibrate the unstructured token in a flexible auto-encoder manner. Due to the similar structure between teacher and student, our FRD can effectively leverage structure knowledge for better convergence. Finally, we conduct extensive experiments to evaluate our SiT. It demonstrates that our method can speed up ViTs by 1.7x with negligible accuracy drop, and even speed up ViTs by 3.6x while maintaining 97% of their performance. Surprisingly, by simply arming LV-ViT with our SiT, we achieve new state-of-the-art performance on ImageNet. Code is available at https://github.com/Sense-X/SiT.", "authors": ["Zhuofan Zong", " Kunchang Li", " Guanglu Song", " Yali Wang", " Yu Qiao", " Biao Leng", " Yu Liu"], "pdf_url": "https://arxiv.org/abs/2111.12624", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\"> Type</td><td rowspan=\"2\">Method</td><td rowspan=\"2\">Reference</td><td>ImageNet</td><td colspan=\"2\">Throughput</td></tr><tr><td>Top-1(%)</td><td>(image/s)</td><td>(%)</td></tr><tr><td>Baseline</td><td>DeiT[29]</td><td>ICML21</td><td>79.8</td><td>1637</td><td>0</td></tr><tr><td>Structure Pruning</td><td>S{}^{2}ViTE[6]</td><td>NeurIPS21</td><td>79.2(-0.6)</td><td>2117</td><td>29.3</td></tr><tr><td rowspan=\"5\">Token Hard Dropping</td><td>PS-ViT[28]</td><td>CVPR22</td><td>79.4(-0.5)</td><td>2351</td><td>43.6</td></tr><tr><td>IA-RED{}^{2}[22]</td><td>NeurIPS21</td><td>79.1(-0.7)</td><td>2369</td><td>44.7</td></tr><tr><td>Dynamic-ViT[24]</td><td>NeurIPS21</td><td>79.3(-0.5)</td><td>2575</td><td>57.3</td></tr><tr><td>Evo-ViT[37]</td><td>AAAI22</td><td>79.4(-0.4)</td><td>2629</td><td>60.6</td></tr><tr><td>EViT[20]</td><td>ICLR22</td><td>79.1(-0.7)</td><td>2783</td><td>70.0</td></tr><tr><td rowspan=\"2\">Token Soft Slimming</td><td rowspan=\"2\">Our SiT</td><td>ECCV22</td><td>79.8(-0.0)</td><td>2344</td><td>43.2</td></tr><tr><td>ECCV22</td><td>79.4(-0.4)</td><td>3308</td><td>102.1</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Comparison to recent model pruning methods for ViT. Our SiT surpasses all the other methods based on structure pruning or hard dropping.", "list_citation_info": ["[28] Tang, Y., Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., Tao, D.: Patch slimming for efficient vision transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12165\u201312174 (2022)", "[37] Xu, Y., Zhang, Z., Zhang, M., Sheng, K., Li, K., Dong, W., Zhang, L., Xu, C., Sun, X.: Evo-vit: Slow-fast token evolution for dynamic vision transformer. Proceedings of the AAAI Conference on Artificial Intelligence (2022)", "[20] Liang, Y., GE, C., Tong, Z., Song, Y., Wang, J., Xie, P.: EVit: Expediting vision transformers via token reorganizations. In: International Conference on Learning Representations (2022)", "[6] Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., Wang, Z.: Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems (2021)", "[22] Pan, B., Panda, R., Jiang, Y., Wang, Z., Feris, R., Oliva, A.: Ia-red2: Interpretability-aware redundancy reduction for vision transformers. Advances in Neural Information Processing Systems (2021)", "[29] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning (2021)", "[24] Rao, Y., Zhao, W., Liu, B., Lu, J., Zhou, J., Hsieh, C.J.: Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems (2021)"]}, {"table": "<table><tr><td rowspan=\"2\"> Model</td><td rowspan=\"2\">Depth</td><td rowspan=\"2\">Stage</td><td rowspan=\"2\"> EmbedDim </td><td rowspan=\"2\">Heads</td><td rowspan=\"2\">Resolution</td><td>#Params</td><td>FLOPs</td></tr><tr><td>(M)</td><td>(G)</td></tr><tr><td>SiT-Ti</td><td>14</td><td>{1,1,1,11}</td><td>320</td><td>5</td><td>224{}^{2}</td><td>15.9</td><td>1.0</td></tr><tr><td>SiT-XS</td><td>16</td><td>{1,1,1,13}</td><td>384</td><td>6</td><td>224{}^{2}</td><td>25.6</td><td>1.5</td></tr><tr><td>SiT-S</td><td>16</td><td>{9,3,2,2}</td><td>384</td><td>6</td><td>224{}^{2}</td><td>25.6</td><td>4.0</td></tr><tr><td>SiT-M</td><td>20</td><td>{10,4,3,3}</td><td>512</td><td>8</td><td>224{}^{2}</td><td>55.6</td><td>8.1</td></tr><tr><td>SiT-L</td><td>24</td><td>{10,4,3,7}</td><td>768</td><td>12</td><td>288{}^{2}</td><td>148.2</td><td>34.4</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table> ModelStudentTeacherThroughputTop-1Top-1\\ddaggerThroughputTop-1(image/s)(%)(%)(image/s)(%)SiT-Ti5896 (\\mathbf{3.2}\\times)80.1 (-2.0)80.6 (-1.5)182782.1SiT-XS4839 (\\mathbf{3.6}\\times)81.1 (-2.2)81.8 (-1.5)136083.3SiT-S1892 (\\mathbf{1.4}\\times)83.2 (-0.1)83.4 (+0.1)136083.3SiT-M1197 (\\mathbf{1.5}\\times)84.1 (-0.1)84.3 (+0.1)80484.2SiT-L346 (\\mathbf{1.7}\\times)85.6 (-0.1)-20485.7 ", "caption": "Table 2: Main results on ImageNet.We apply our self-slimming learning on the state-of-the-art vanilla vision transformer LV-ViT [16].\\ddagger means we adopt an extra CNN teacher.Our SiT can speed up LV-ViT \\mathbf{1.7}\\times with a slight accuracy drop.For fast inference,our SiT can maintain 97% of the performance while speeding up the original transformers by \\mathbf{3.6}\\times.", "list_citation_info": ["[16] Jiang, Z.H., Hou, Q., Yuan, L., Zhou, D., Shi, Y., Jin, X., Wang, A., Feng, J.: All tokens matter: Token labeling for training better vision transformers. Advances in Neural Information Processing Systems (2021)"]}, {"table": "<table><tr><td rowspan=\"2\"> Model</td><td rowspan=\"2\">Resolution</td><td>#Params</td><td>FLOPs</td><td>Throughput</td><td>ImageNet</td></tr><tr><td>(M)</td><td>(G)</td><td>(image/s)</td><td>Top-1(%)</td></tr><tr><td>EfficientNet-B1 [26]</td><td>240^{2}</td><td>7.8</td><td>0.7</td><td>2559</td><td>79.1</td></tr><tr><td>EfficientNet-B2 [26]</td><td>260^{2}</td><td>9.1</td><td>1.1</td><td>1808</td><td>80.1</td></tr><tr><td>DeiT-T [29]</td><td>224^{2}</td><td>5.9</td><td>1.3</td><td>3346</td><td>74.5</td></tr><tr><td>LeViT-256 [12]</td><td>224^{2}</td><td>18.9</td><td>1.1</td><td>5802</td><td>80.1</td></tr><tr><td>SiT-Ti</td><td>224^{2}</td><td>15.9</td><td>1.0</td><td>5896</td><td>80.1</td></tr><tr><td>SiT-Ti</td><td>224^{2}</td><td>16.2</td><td>1.0</td><td>5833</td><td>80.6</td></tr><tr><td> EfficientNet-B3 [26]</td><td>300^{2}</td><td>12.2</td><td>1.9</td><td>1062</td><td>81.6</td></tr><tr><td>Swin-T [21]</td><td>224^{2}</td><td>28.3</td><td>4.5</td><td>1023</td><td>81.3</td></tr><tr><td>DeiT-S [29]</td><td>224^{2}</td><td>22.4</td><td>4.6</td><td>1598</td><td>81.2</td></tr><tr><td>LeViT-384 [12]</td><td>224^{2}</td><td>39.1</td><td>2.4</td><td>3876</td><td>81.6</td></tr><tr><td>SiT-XS</td><td>224^{2}</td><td>25.6</td><td>1.5</td><td>4839</td><td>81.1</td></tr><tr><td>SiT-XS</td><td>224^{2}</td><td>26.0</td><td>1.5</td><td>4798</td><td>81.8</td></tr><tr><td> EfficientNet-B4 [26]</td><td>380^{2}</td><td>19.3</td><td>4.6</td><td>545</td><td>82.9</td></tr><tr><td>Swin-B [21]</td><td>224^{2}</td><td>87.8</td><td>15.5</td><td>474</td><td>83.3</td></tr><tr><td>DeiT-B [29]</td><td>224^{2}</td><td>87.3</td><td>17.7</td><td>718</td><td>83.4</td></tr><tr><td>LV-ViT-S [16]</td><td>224^{2}</td><td>26.2</td><td>6.6</td><td>1270</td><td>83.3</td></tr><tr><td>SiT-S</td><td>224^{2}</td><td>25.6</td><td>4.0</td><td>1892</td><td>83.2</td></tr><tr><td>SiT-S</td><td>224^{2}</td><td>26.0</td><td>4.0</td><td>1873</td><td>83.4</td></tr><tr><td> EfficientNet-B6 [26]</td><td>528^{2}</td><td>43.0</td><td>19.9</td><td>153</td><td>84.0</td></tr><tr><td>EfficientNetV2-S [27]</td><td>384^{2}</td><td>21.5</td><td>8.5</td><td>742</td><td>83.9</td></tr><tr><td>CaiT-S36 [30]</td><td>224^{2}</td><td>68.2</td><td>13.9</td><td>233</td><td>83.9</td></tr><tr><td>LV-ViT-M [16]</td><td>224^{2}</td><td>55.8</td><td>12.7</td><td>768</td><td>84.1</td></tr><tr><td>SiT-M</td><td>224^{2}</td><td>55.6</td><td>8.1</td><td>1197</td><td>84.1</td></tr><tr><td>SiT-M</td><td>224^{2}</td><td>56.2</td><td>8.1</td><td>1185</td><td>84.3</td></tr><tr><td> EfficientNetV2-M [27]</td><td>480^{2}</td><td>54.1</td><td>25.0</td><td>271</td><td>85.1</td></tr><tr><td>NFNet-F1 [2]</td><td>320^{2}</td><td>132.6</td><td>36.0</td><td>128</td><td>84.7</td></tr><tr><td>CaiT-M36 [30]</td><td>224^{2}</td><td>270.1</td><td>53.4</td><td>130</td><td>85.1</td></tr><tr><td>LV-ViT-L [16]</td><td>288^{2}</td><td>150.1</td><td>58.8</td><td>208</td><td>85.3</td></tr><tr><td>SiT-L</td><td>288^{2}</td><td>148.2</td><td>34.4</td><td>346</td><td>85.6</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 3: Comparison to the state-of-the-art on ImageNet.The models marked in gray color are trained with distillation supervision from a powerful CNN for 300 epochs.Our SiT achieves the best performance trade-off.", "list_citation_info": ["[30] Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., J\u00e9gou, H.: Going deeper with image transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2021)", "[2] Brock, A., De, S., Smith, S.L., Simonyan, K.: High-performance large-scale image recognition without normalization. In: International Conference on Machine Learning (2021)", "[27] Tan, M., Le, Q.: Efficientnetv2: Smaller models and faster training. In: International Conference on Machine Learning (2021)", "[21] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2021)", "[16] Jiang, Z.H., Hou, Q., Yuan, L., Zhou, D., Shi, Y., Jin, X., Wang, A., Feng, J.: All tokens matter: Token labeling for training better vision transformers. Advances in Neural Information Processing Systems (2021)", "[12] Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J\u00e9gou, H., Douze, M.: Levit: a vision transformer in convnet\u2019s clothing for faster inference. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2021)", "[29] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning (2021)", "[26] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International conference on machine learning (2019)"]}]}