{"title": "Mdetr - Modulated Detection for End-to-End Multi-Modal Understanding", "abstract": "Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.", "authors": ["Aishwarya Kamath", " Mannat Singh", " Yann LeCun", " Gabriel Synnaeve", " Ishan Misra", " Nicolas Carion"], "pdf_url": "https://arxiv.org/abs/2104.12763", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>CLEVR</th><th colspan=\"2\">CLEVR-Hu</th><th colspan=\"2\">CoGenT</th><th>CLEVR-Ref+</th></tr></thead><tbody><tr><td></td><th>Overall</th><th>- FT</th><th>+ FT</th><th>TestA</th><th>TestB</th><th>Acc</th></tr><tr><td>MAttNet[69]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>60.9</td></tr><tr><td>MGA-Net[73]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>80.1</td></tr><tr><td>FiLM[42]</td><td>97.7</td><td>56.6</td><td>75.9</td><td>98.3</td><td>78.8</td><td>-</td></tr><tr><td>MAC [17]</td><td>98.9</td><td>57.4</td><td>81.5</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NS-VQA[67]{}^{*}</td><td>99.8</td><td>-</td><td>67.8</td><td>99.8</td><td>63.9</td><td>-</td></tr><tr><td>OCCAM [60]</td><td>99.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MDETR</td><td>99.7</td><td>59.9</td><td>81.7</td><td>99.8</td><td>76.7</td><td>100</td></tr></tbody></table>", "caption": "Table 1: Results on CLEVR-based datasets. We report accuracies on the test set of CLEVR. On CLEVR-Humans, we report accuracy on the test set before and after fine-tuning. On CoGenT, we report performance when the model is trained in condition A, without finetuning on condition B. On CLEVR-Ref+, we report the accuracy on the subset where the referred object is unique. *indicates method uses external program annotations. Further details in Appendix B.", "list_citation_info": ["[60] Zhonghao Wang, Mo Yu, Kai Wang, Jinjun Xiong, Wen-mei Hwu, Mark Hasegawa-Johnson, and Humphrey Shi. Interpretable Visual Reasoning via Induced Symbolic Space.", "[69] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L. Berg. Mattnet: Modular attention network for referring expression comprehension. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1307\u20131315. IEEE Computer Society, 2018.", "[42] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 3942\u20133951. AAAI Press, 2018.", "[67] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic VQA: disentangling reasoning from vision and language understanding. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 1039\u20131050, 2018.", "[17] Drew A. Hudson and Christopher D. Manning. Compositional attention networks for machine reasoning. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.", "[73] Yihan Zheng, Zhiquan Wen, Mingkui Tan, Runhao Zeng, Qi Chen, Yaowei Wang, and Qi Wu. Modular graph attention network for complex visual relational reasoning. In Proceedings of the Asian Conference on Computer Vision (ACCV), 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Detection</th><th>Pre-training</th><th colspan=\"3\">RefCOCO</th><th colspan=\"3\">RefCOCO+</th><th colspan=\"2\">RefCOCOg</th></tr></thead><tbody><tr><td></td><th>backbone</th><th>image data</th><th>val</th><th>testA</th><th>testB</th><th>val</th><th>testA</th><th>testB</th><th>val</th><th>test</th></tr><tr><td>MAttNet[69]</td><td>R101</td><td>None</td><td>76.65</td><td>81.14</td><td>69.99</td><td>65.33</td><td>71.62</td><td>56.02</td><td>66.58</td><td>67.27</td></tr><tr><td>ViLBERT[34]</td><td>R101</td><td>CC (3.3M)</td><td>-</td><td>-</td><td>-</td><td>72.34</td><td>78.52</td><td>62.61</td><td>-</td><td>-</td></tr><tr><td>VL-BERT_L [54]</td><td>R101</td><td>CC (3.3M)</td><td>-</td><td>-</td><td>-</td><td>72.59</td><td>78.57</td><td>62.30</td><td>-</td><td>-</td></tr><tr><td>UNITER_L[6]{}^{*}</td><td>R101</td><td>CC, SBU, COCO, VG (4.6M)</td><td>81.41</td><td>87.04</td><td>74.17</td><td>75.90</td><td>81.45</td><td>66.70</td><td>74.86</td><td>75.77</td></tr><tr><td>VILLA_L[9]{}^{*}</td><td>R101</td><td>CC, SBU, COCO, VG (4.6M)</td><td>82.39</td><td>87.48</td><td>74.84</td><td>76.17</td><td>81.54</td><td>66.84</td><td>76.18</td><td>76.71</td></tr><tr><td>ERNIE-ViL_L[68]</td><td>R101</td><td>CC, SBU (4.3M)</td><td>-</td><td>-</td><td>-</td><td>75.95</td><td>82.07</td><td>66.88</td><td>-</td><td>-</td></tr><tr><td>MDETR</td><td>R101</td><td>COCO, VG, Flickr30k (200k)</td><td>86.75</td><td>89.58</td><td>81.41</td><td>79.52</td><td>84.09</td><td>70.62</td><td>81.64</td><td>80.89</td></tr><tr><td>MDETR</td><td>ENB3</td><td>COCO, VG, Flickr30k (200k)</td><td>87.51</td><td>90.40</td><td>82.67</td><td>81.13</td><td>85.52</td><td>72.96</td><td>83.35</td><td>83.31</td></tr></tbody></table>", "caption": "Table 2: Accuracy results on referring expression comprehension. *As mentioned in UNITER [6], methods using box proposals from the BUTD detector [1] suffer from a test set leak, since the detector was trained on images including the validation and test set of the RE comprehension datasets. We report numbers for these methods from their papers using these \u201ccontaminated features\" but we would like to stress that all of our pre-training excluded the images used in the val/test of any of the downstream datasets including for RE comprehension. CC refers to Conceptual Captions [53], VG to Visual Genome [24], SBU refers to the SBU Captions[41] and COCO to Micosoft COCO [30].", "list_citation_info": ["[54] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: pre-training of generic visual-linguistic representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.", "[34] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13\u201323, 2019.", "[41] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned photographs. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 1143\u20131151, 2011.", "[69] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L. Berg. Mattnet: Modular attention network for referring expression comprehension. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1307\u20131315. IEEE Computer Society, 2018.", "[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Learning universal image-text representations. 2019.", "[9] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.", "[68] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. ArXiv preprint, abs/2006.16934, 2020.", "[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6077\u20136086. IEEE Computer Society, 2018.", "[53] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, Melbourne, Australia, 2018. Association for Computational Linguistics.", "[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. ArXiv preprint, abs/1602.07332, 2016.", "[30] Tsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays, P. Perona, D. Ramanan, Piotr Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014."]}, {"table": "<table><tbody><tr><td>Method</td><td colspan=\"3\">Val</td><td colspan=\"3\">Test</td></tr><tr><td></td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><td></td><td colspan=\"6\">Any-Box-Protocol</td></tr><tr><td>BAN [22]</td><td>-</td><td>-</td><td>-</td><td>69.7</td><td>84.2</td><td>86.4</td></tr><tr><td>VisualBert[26]</td><td>68.1</td><td>84.0</td><td>86.2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VisualBert\\dagger[26]</td><td>70.4</td><td>84.5</td><td>86.3</td><td>71.3</td><td>85.0</td><td>86.5</td></tr><tr><td>MDETR-R101</td><td>78.9</td><td>88.8</td><td>90.8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MDETR-R101\\dagger*</td><td>82.5</td><td>92.9</td><td>94.9</td><td>83.4</td><td>93.5</td><td>95.3</td></tr><tr><td>MDETR-ENB3\\dagger*</td><td>82.9</td><td>93.2</td><td>95.2</td><td>84.0</td><td>93.8</td><td>95.6</td></tr><tr><td>MDETR-ENB5\\dagger*</td><td>83.6</td><td>93.4</td><td>95.1</td><td>84.3</td><td>93.9</td><td>95.8</td></tr><tr><td></td><td colspan=\"6\">Merged-Boxes-Protocol</td></tr><tr><td>CITE [43]</td><td>-</td><td>-</td><td>-</td><td>61.9</td><td>-</td><td>-</td></tr><tr><td>FAOG [66]</td><td>-</td><td>-</td><td>-</td><td>68.7</td><td>-</td><td>-</td></tr><tr><td>SimNet-CCA [45]</td><td>-</td><td>-</td><td>-</td><td>71.9</td><td>-</td><td>-</td></tr><tr><td>DDPN [71]</td><td>72.8</td><td>-</td><td>-</td><td>73.5</td><td>-</td><td>-</td></tr><tr><td>MDETR-R101</td><td>79.0</td><td>86.7</td><td>88.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MDETR-R101\\dagger*</td><td>82.3</td><td>91.8</td><td>93.7</td><td>83.8</td><td>92.7</td><td>94.4</td></tr></tbody></table>", "caption": "Table 3: Results on the phrase grounding task on Flickr30k entities dataset [46]. Models with \\dagger are pre-trained on COCO, models with * are also pre-trained on VG and Flickr 30k. Our models (MDETR) use a RoBERTa text encoder while other models use RNNs, word2vec-based features, or BERT (comparable to RoBERTa) text encoders. All models use a ResNet101 backbone, except MDETR-ENB3 which uses EfficientNet-B3 and MDETR-ENB5 with an EfficientNet-B5.", "list_citation_info": ["[46] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 2641\u20132649. IEEE Computer Society, 2015.", "[43] Bryan A Plummer, Paige Kordas, M Hadi Kiapour, Shuai Zheng, Robinson Piramuthu, and Svetlana Lazebnik. Conditional image-text embedding networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 249\u2013264, 2018.", "[66] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. A fast and accurate one-stage approach to visual grounding. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 4682\u20134692. IEEE, 2019.", "[71] Zhou Yu, Jun Yu, Chenchao Xiang, Zhou Zhao, Qi Tian, and Dacheng Tao. Rethinking diversified and discriminative proposal generation for visual grounding. In J\u00e9r\u00f4me Lang, editor, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 1114\u20131120. ijcai.org, 2018.", "[26] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. ArXiv preprint, abs/1908.03557, 2019.", "[45] Bryan Allen Plummer, Kevin Shih, Yichen Li, Ke Xu, Svetlana Lazebnik, Stan Sclaroff, and Kate Saenko. Revisiting image-language networks for open-ended phrase detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[22] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 1571\u20131581, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Backbone</th><th colspan=\"4\">PhraseCut</th></tr></thead><tbody><tr><td></td><th></th><th>M-IoU</th><th>Pr@0.5</th><th>Pr@0.7</th><th>Pr@0.9</th></tr><tr><td>RMI[3]</td><td>R101</td><td>21.1</td><td>22.0</td><td>11.6</td><td>1.5</td></tr><tr><td>HULANet[62]</td><td>R101</td><td>41.3</td><td>42.4</td><td>27.0</td><td>5.7</td></tr><tr><td>MDETR</td><td>R101</td><td>53.1</td><td>56.1</td><td>38.9</td><td>11.9</td></tr><tr><td>MDETR</td><td>ENB3</td><td>53.7</td><td>57.5</td><td>39.9</td><td>11.9</td></tr></tbody></table>", "caption": "Table 4: Following [62], we report the mean intersection-over-union (IoU) of our masks with the ground-truth masks. We also report the precision Pr@I of our model, where success is marked when our proposed mask has an IoU with the ground-truth higher than the threshold I. With a comparable ResNet backbone, we observe consistent gains across all metrics over HULANet [62], the current state-of-the-art. The EfficientNet backbone further improves on those results.", "list_citation_info": ["[62] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and Subhransu Maji. Phrasecut: Language-based image segmentation in the wild. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10213\u201310222. IEEE, 2020.", "[3] Ding-Jie Chen, Songhao Jia, Yi-Chen Lo, Hwann-Tzong Chen, and Tyng-Luh Liu. See-through-text grouping for referring image segmentation. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 7453\u20137462. IEEE, 2019."]}, {"table": "<table><tbody><tr><td>Method</td><td>Pre-training img data</td><td>Test-dev</td><td>Test-std</td></tr><tr><td>MoVie [39]</td><td>-</td><td>-</td><td>57.10</td></tr><tr><td>LXMERT[55]</td><td>VG, COCO (180k)</td><td>60.0</td><td>60.33</td></tr><tr><td>VL-T5 [7]</td><td>VG, COCO (180k)</td><td>-</td><td>60.80</td></tr><tr><td>MMN [5]</td><td>-</td><td>-</td><td>60.83</td></tr><tr><td rowspan=\"2\">OSCAR [28]</td><td>VG, COCO,</td><td rowspan=\"2\">61.58</td><td rowspan=\"2\">61.62</td></tr><tr><td>Flickr, SBU (4.3M)</td></tr><tr><td>NSM [19]</td><td>-</td><td>-</td><td>63.17</td></tr><tr><td rowspan=\"3\">VinVL [72]</td><td>VG, COCO, Objects365, SBU</td><td rowspan=\"2\">65.05</td><td rowspan=\"2\">64.65</td></tr><tr><td>Flickr30k, CC, VQA,</td></tr><tr><td>OpenImagesV5 (5.65M)</td><td></td><td></td></tr><tr><td>MDETR-R101</td><td>VG, COCO, Flickr30k (200k)</td><td>62.48</td><td>61.99</td></tr><tr><td>MDETR-ENB5</td><td>VG, COCO, Flickr30k (200k)</td><td>62.95</td><td>62.45</td></tr></tbody></table>", "caption": "Table 5: Visual question answering on the GQA dataset.", "list_citation_info": ["[39] Duy-Kien Nguyen, Vedanuj Goswami, and Xinlei Chen. MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond. arXiv:2004.11883 [cs], 2020. arXiv: 2004.11883.", "[7] Jaemin Cho, Jie Lei, H. Tan, and M. Bansal. Unifying vision-and-language tasks via text generation. ArXiv preprint, abs/2102.02779, 2021.", "[55] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, Hong Kong, China, 2019. Association for Computational Linguistics.", "[5] Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta module network for compositional visual reasoning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 655\u2013664, 2021.", "[72] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Making visual representations matter in vision-language models. ArXiv preprint, abs/2101.00529, 2021.", "[19] Drew A. Hudson and Christopher D. Manning. Learning by abstraction: The neural state machine. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d\u2019Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5901\u20135914, 2019.", "[28] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th colspan=\"6\">CLEVR</th><th colspan=\"2\">CLEVR-Humans</th><th colspan=\"2\">CoGenT</th><th>CLEVR-Ref+</th></tr></thead><tbody><tr><td></td><th>Overall</th><th>Count</th><th>Exist</th><th>Comp. Num</th><th>Query</th><th>Comp. Att</th><th>Before FT</th><th>After FT</th><th>TestA</th><th>TestB</th><th>Acc</th></tr><tr><td>MAttNet[69]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>60.9</td></tr><tr><td>MGA-Net[73]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>80.1</td></tr><tr><td>FiLM[42]</td><td>97.7</td><td>94.3</td><td>99.1</td><td>96.8</td><td>99.1</td><td>99.1</td><td>56.6</td><td>75.9</td><td>98.3</td><td>78.8</td><td>-</td></tr><tr><td>MAC [17]</td><td>98.9</td><td>97.1</td><td>99.5</td><td>99.1</td><td>99.5</td><td>99.5</td><td>57.4</td><td>81.5</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NS-VQA[67]{}^{*}</td><td>99.8</td><td>99.7</td><td>99.9</td><td>99.8</td><td>99.8</td><td>99.8</td><td>-</td><td>67.8</td><td>99.8</td><td>63.9</td><td>-</td></tr><tr><td>OCCAM [60]</td><td>99.4</td><td>98.1</td><td>99.8</td><td>99.0</td><td>99.9</td><td>99.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MDETR</td><td>99.7</td><td>99.3</td><td>99.9</td><td>99.4</td><td>99.9</td><td>99.9</td><td>59.9</td><td>81.7</td><td>99.8</td><td>76.7</td><td>100</td></tr></tbody></table>", "caption": "Table 7: Results on CLEVR-based datasets. We report accuracies on the test set of CLEVR, including the detail by question type. On CLEVR-Humans, we report accuracy on the test set before and after fine-tuning. On CoGenT, we report performance when the model is trained in condition A, without finetuning on condition B. On CLEVR-Ref+, we report the accuracy on the subset where the referred object is unique. *indicates method uses external program annotations", "list_citation_info": ["[60] Zhonghao Wang, Mo Yu, Kai Wang, Jinjun Xiong, Wen-mei Hwu, Mark Hasegawa-Johnson, and Humphrey Shi. Interpretable Visual Reasoning via Induced Symbolic Space.", "[69] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L. Berg. Mattnet: Modular attention network for referring expression comprehension. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1307\u20131315. IEEE Computer Society, 2018.", "[42] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with a general conditioning layer. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 3942\u20133951. AAAI Press, 2018.", "[67] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic VQA: disentangling reasoning from vision and language understanding. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 1039\u20131050, 2018.", "[17] Drew A. Hudson and Christopher D. Manning. Compositional attention networks for machine reasoning. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.", "[73] Yihan Zheng, Zhiquan Wen, Mingkui Tan, Runhao Zeng, Qi Chen, Yaowei Wang, and Qi Wu. Modular graph attention network for complex visual relational reasoning. In Proceedings of the Asian Conference on Computer Vision (ACCV), 2020."]}]}