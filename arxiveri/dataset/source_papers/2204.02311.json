{"title": "Palm: Scaling language modeling with pathways", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "authors": ["Aakanksha Chowdhery", " Sharan Narang", " Jacob Devlin", " Maarten Bosma", " Gaurav Mishra", " Adam Roberts", " Paul Barham", " Hyung Won Chung", " Charles Sutton", " Sebastian Gehrmann", " Parker Schuh", " Kensen Shi", " Sasha Tsvyashchenko", " Joshua Maynez", " Abhishek Rao", " Parker Barnes", " Yi Tay", " Noam Shazeer", " Vinodkumar Prabhakaran", " Emily Reif", " Nan Du", " Ben Hutchinson", " Reiner Pope", " James Bradbury", " Jacob Austin ", " et al. (42 additional authors not shown)"], "pdf_url": "https://arxiv.org/abs/2204.02311", "list_table_and_caption": [{"table": "<table><tr><td></td><td colspan=\"2\">0-shot</td><td colspan=\"2\">1-shot</td><td colspan=\"2\">Few-shot</td></tr><tr><td><p>Task</p></td><td> PriorSOTA </td><td> PaLM540B </td><td> PriorSOTA </td><td> PaLM540B </td><td> PriorSOTA </td><td> PaLM540B </td></tr><tr><td><p>TriviaQA (EM)</p></td><td>71.3^{a}</td><td>76.9</td><td>75.8^{a}</td><td>81.4</td><td>75.8^{a} (1)</td><td>81.4 (1)</td></tr><tr><td><p>Natural Questions (EM)</p></td><td>24.7{}^{a}</td><td>21.2</td><td>26.3^{a}</td><td>29.3</td><td>32.5^{a} (1)</td><td>39.6 (64)</td></tr><tr><td><p>Web Questions (EM)</p></td><td>19.0{}^{a}</td><td>10.6</td><td>25.3{}^{b}</td><td>22.6</td><td>41.1^{b} (64)</td><td>43.5 (64)</td></tr><tr><td><p>Lambada (EM)</p></td><td>77.7^{f}</td><td>77.9</td><td>80.9^{a}</td><td>81.8</td><td>87.2^{c} (15)</td><td>89.7 (8)</td></tr><tr><td><p>HellaSwag</p></td><td>80.8^{f}</td><td>83.4</td><td>80.2^{c}</td><td>83.6</td><td>82.4^{c} (20)</td><td>83.8 (5)</td></tr><tr><td><p>StoryCloze</p></td><td>83.2^{b}</td><td>84.6</td><td>84.7^{b}</td><td>86.1</td><td>87.7^{b} (70)</td><td>89.0 (5)</td></tr><tr><td><p>Winograd</p></td><td>88.3^{b}</td><td>90.1</td><td>89.7{}^{b}</td><td>87.5</td><td>88.6^{a} (2)</td><td>89.4 (5)</td></tr><tr><td><p>Winogrande</p></td><td>74.9^{f}</td><td>81.1</td><td>73.7^{c}</td><td>83.7</td><td>79.2^{a} (16)</td><td>85.1 (5)</td></tr><tr><td><p>Drop (F1)</p></td><td>57.3^{a}</td><td>69.4</td><td>57.8^{a}</td><td>70.8</td><td>58.6^{a} (2)</td><td>70.8 (1)</td></tr><tr><td><p>CoQA (F1)</p></td><td>81.5{}^{b}</td><td>77.6</td><td>84.0{}^{b}</td><td>79.9</td><td>85.0{}^{b} (5)</td><td>81.5 (5)</td></tr><tr><td><p>QuAC (F1)</p></td><td>41.5^{b}</td><td>45.2</td><td>43.4^{b}</td><td>47.7</td><td>44.3^{b} (5)</td><td>47.7 (1)</td></tr><tr><td><p>SQuADv2 (F1)</p></td><td>71.1^{a}</td><td>80.8</td><td>71.8^{a}</td><td>82.9</td><td>71.8^{a} (10)</td><td>83.3 (5)</td></tr><tr><td><p>SQuADv2 (EM)</p></td><td>64.7^{a}</td><td>75.5</td><td>66.5^{a}</td><td>78.7</td><td>67.0^{a} (10)</td><td>79.6 (5)</td></tr><tr><td><p>RACE-m</p></td><td>64.0^{a}</td><td>68.1</td><td>65.6^{a}</td><td>69.3</td><td>66.9^{a\\dagger} (8)</td><td>72.1 (8)</td></tr><tr><td><p>RACE-h</p></td><td>47.9^{c}</td><td>49.1</td><td>48.7^{a}</td><td>52.1</td><td>49.3^{a\\dagger} (2)</td><td>54.6 (5)</td></tr><tr><td><p>PIQA</p></td><td>82.0^{c}</td><td>82.3</td><td>81.4^{a}</td><td>83.9</td><td>83.2^{c} (5)</td><td>85.2 (5)</td></tr><tr><td><p>ARC-e</p></td><td>76.4^{e}</td><td>76.6</td><td>76.6^{a}</td><td>85.0</td><td>80.9^{e} (10)</td><td>88.4 (5)</td></tr><tr><td><p>ARC-c</p></td><td>51.4^{b}</td><td>53.0</td><td>53.2^{b}</td><td>60.1</td><td>52.0^{a} (3)</td><td>65.9 (5)</td></tr><tr><td><p>OpenbookQA</p></td><td>57.6{}^{b}</td><td>53.4</td><td>55.8{}^{b}</td><td>53.6</td><td>65.4^{b} (100)</td><td>68.0 (32)</td></tr><tr><td><p>BoolQ</p></td><td>83.7^{f}</td><td>88.0</td><td>82.8^{a}</td><td>88.7</td><td>84.8^{c} (32)</td><td>89.1 (8)</td></tr><tr><td><p>Copa</p></td><td>91.0^{b}</td><td>93.0</td><td>92.0{}^{a}</td><td>91.0</td><td>93.0^{a} (16)</td><td>95.0 (5)</td></tr><tr><td><p>RTE</p></td><td>73.3{}^{e}</td><td>72.9</td><td>71.5^{a}</td><td>78.7</td><td>76.8 (5)</td><td>81.2 (5)</td></tr><tr><td><p>WiC</p></td><td>50.3^{a}</td><td>59.1</td><td>52.7^{a}</td><td>63.2</td><td>58.5^{c} (32)</td><td>64.6 (5)</td></tr><tr><td><p>Multirc (F1a)</p></td><td>73.7^{a}</td><td>83.5</td><td>74.7^{a}</td><td>84.9</td><td>77.5^{a} (4)</td><td>86.3 (5)</td></tr><tr><td><p>WSC</p></td><td>85.3^{a}</td><td>89.1</td><td>83.9^{a}</td><td>86.3</td><td>85.6^{a} (2)</td><td>89.5 (5)</td></tr><tr><td><p>ReCoRD</p></td><td>90.3^{a}</td><td>92.9</td><td>90.3^{a}</td><td>92.8</td><td>90.6 (2)</td><td>92.9 (2)</td></tr><tr><td><p>CB</p></td><td>48.2^{a}</td><td>51.8</td><td>73.2^{a}</td><td>83.9</td><td>84.8^{a} (8)</td><td>89.3 (5)</td></tr><tr><td><p>ANLI R1</p></td><td>39.2^{a}</td><td>48.4</td><td>42.4^{a}</td><td>52.6</td><td>44.3^{a} (2)</td><td>56.9 (5)</td></tr><tr><td><p>ANLI R2</p></td><td>39.9^{e}</td><td>44.2</td><td>40.0^{a}</td><td>48.7</td><td>41.2^{a} (10)</td><td>56.1 (5)</td></tr><tr><td><p>ANLI R3</p></td><td>41.3^{a}</td><td>45.7</td><td>40.8^{a}</td><td>52.3</td><td>44.7^{a} (4)</td><td>51.2 (5)</td></tr></table>", "caption": "Table 4: Results obtained by the PaLM 540B model across 29 NLP benchmarks. For the few-shot results, the number of shots for each task are mentioned in parenthesis. The splits for each task are the same ones used in Du et al. (2021) and Brown et al. (2020).Superscripts denote results from past work: {}^{a}GLaM 62B/64E (Du et al., 2021), {}^{b}GPT-3 175B (Brown et al., 2020), {}^{c}Megatron-Turing NLG 530B (Smith et al., 2022), {}^{d}Gopher (Rae et al., 2021), {}^{e}LaMDA (Thoppilan et al., 2022) (results reported from Wei et al. (2022a), {}^{f}Chinchilla (Hoffmann et al., 2022)). {}^{\\dagger} The work of Rae et al. (2021) and Hoffmann et al. (2022) achieve much higher scores on RACE-m/h, but the authors of those papers note their scores cannot be compared to GPT-3 and other large LMs because of a difference in task setup. We follow the GPT-3 style task setup for RACE-m/h, and compare to the prior results using the same setup.", "list_citation_info": ["Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. v. d., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.", "Du et al. (2021) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. GLaM: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021. URL https://arxiv.org/pdf/2112.06905.", "Thoppilan et al. (2022) Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/pdf/2201.08239.", "Wei et al. (2022a) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. Proceedings of the International Conference on Learning Representations (ICLR), 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.", "Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.", "Rae et al. (2021) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L. A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S. M., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d\u2019Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B. A., Weidinger, L., Gabriel, I., Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and Irving, G. Scaling language models: Methods, analysis & insights from training Gopher. CoRR, abs/2112.11446, 2021.", "Smith et al. (2022) Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022."]}, {"table": "<table><tr><td>Model</td><td>Avg NLG</td><td>Avg NLU</td></tr><tr><td>GPT-3 175B</td><td>52.9</td><td>65.4</td></tr><tr><td>GLaM 64B/64E</td><td>58.4</td><td>68.7</td></tr><tr><td>PaLM 8B</td><td>41.5</td><td>59.2</td></tr><tr><td>PaLM 62B</td><td>57.7</td><td>67.3</td></tr><tr><td>PaLM 540B</td><td>63.9</td><td>74.7</td></tr></table>", "caption": "Table 5: Average (Avg) Natural Language Generation (NLG) and Natural Language Understanding (NLU) results across 29 benchmarks using 1-shot evaluation. NLG benchmarks include eight tasks \u2013 TriviaQA, NQS,WebQS, SQuADv2, LAMBADA, DROP, QuAC and CoQA \u2013 while the remaining are NLU benchmarks. Results for GPT-3 and GLaM are from Du et al. (2021).", "list_citation_info": ["Du et al. (2021) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. GLaM: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021. URL https://arxiv.org/pdf/2112.06905."]}, {"table": "<table><tr><td>Model</td><td>Average</td><td>Humanities</td><td>STEM</td><td>Social Sciences</td><td>Other</td></tr><tr><td>Chinchilla 70B (Prior SOTA)</td><td>67.5</td><td>63.6</td><td>54.9</td><td>79.3</td><td>73.9</td></tr><tr><td>PaLM 8B</td><td>25.3</td><td>25.6</td><td>23.8</td><td>24.1</td><td>27.8</td></tr><tr><td>PaLM 62B</td><td>53.7</td><td>59.5</td><td>41.9</td><td>62.7</td><td>55.8</td></tr><tr><td>PaLM 540B</td><td>69.3</td><td>77.0</td><td>55.6</td><td>81.0</td><td>69.6</td></tr></table>", "caption": "Table 6: Results (5-shot) of Chinchilla (Hoffmann et al., 2022) and PaLM models on the MMLU (Hendrycks et al., 2021) benchmark. Chinchilla represents the prior state of the art results on this benchmark. The results are reported on the test set of each of the tasks.", "list_citation_info": ["Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., Driessche, G. v. d., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.", "Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021."]}, {"table": "<table><tr><td>Model</td><td>Avg</td><td>BoolQ</td><td>CB</td><td>CoPA</td><td>MultiRC</td><td>Record</td><td>RTE</td><td>WiC</td><td>WSC</td></tr><tr><td>T5-11B</td><td>89.9</td><td>90.8</td><td>94.9/96.4</td><td>98.0</td><td>87.4/66.1</td><td>93.8/93.2</td><td>93.9</td><td>77.3</td><td>96.2</td></tr><tr><td>ST-MoE-32B</td><td>93.2</td><td>93.1</td><td>100/100</td><td>100</td><td>90.4/69.9</td><td>95.0/95.6</td><td>95.7</td><td>81.0</td><td>100</td></tr><tr><td>PaLM 540B (finetuned)</td><td>92.6</td><td>92.2</td><td>100/100</td><td>100</td><td>90.1/69.2</td><td>94.0/94.6</td><td>95.7</td><td>78.8</td><td>100</td></tr></table>", "caption": "Table 7: Results on SuperGLUE dev set. We compare with T5-11B (Raffel et al., 2020) and ST-MoE-32B (Zoph et al., 2022). Scores reported are the peak validation scores per task.", "list_citation_info": ["Zoph et al. (2022) Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022.", "Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html."]}, {"table": "<table><tr><td>Model</td><td>Avg</td><td>BoolQ</td><td>CB</td><td>CoPA</td><td>MultiRC</td><td>Record</td><td>RTE</td><td>WiC</td><td>WSC</td></tr><tr><td>ST-MoE-32B</td><td>91.2</td><td>92.4</td><td>96.9/98.0</td><td>99.2</td><td>89.6/65.8</td><td>95.1/94.4</td><td>93.5</td><td>77.7</td><td>96.6</td></tr><tr><td>Best Decoder-only LM</td><td>71.8</td><td>76.4</td><td>52.0/75.6</td><td>92.0</td><td>75.4/30.5</td><td>91.1/90.2</td><td>69.0</td><td>49.4</td><td>80.1</td></tr><tr><td>PaLM 540B (finetuned)</td><td>90.4</td><td>91.9</td><td>94.4/96.0</td><td>99.0</td><td>88.7/63.6</td><td>94.2/93.3</td><td>95.9</td><td>77.4</td><td>95.9</td></tr></table>", "caption": "Table 9: Results on SuperGLUE test set (leaderboard). We compare with state-of-the-art span corruption based Encoder-Decoder (Zoph et al., 2022) and the best decoder-only language model (Brown et al., 2020). ", "list_citation_info": ["Zoph et al. (2022) Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022.", "Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."]}, {"table": "<table><tr><td></td><td colspan=\"2\">Code tokens</td><td>Code web docs</td></tr><tr><td></td><td>Total code</td><td>Python</td><td></td></tr><tr><td>LaMDA 137B</td><td>\u2013</td><td>\u2013</td><td>18B</td></tr><tr><td>Codex 12B</td><td>100B</td><td>100B</td><td>\u2013</td></tr><tr><td>PaLM 540B</td><td>39B</td><td>2.7B</td><td>\u2013</td></tr><tr><td>PaLM-Coder 540B</td><td>46.8B</td><td>8.7B</td><td>\u2013</td></tr></table>", "caption": "Table 11: Amount of code tokens processedduring training by the languagemodels of code that we consider.For the Davinci Codex model, the trainingdataset mix is unknown.For PaLM-Coder, the table shows the sum of pre-training and fine tuning data.Dashes indicate that no data ofthat type is included.Data for Codex 12B reported from Chen et al. (2021). Codex, PaLM, and PaLM-Coder may include a small amount of code web docs; see footnote.", "list_citation_info": ["Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374."]}, {"table": "<table><tr><td></td><td></td><td colspan=\"2\">Pretraining only</td><td colspan=\"3\">Code Finetuning</td><td></td></tr><tr><td></td><td></td><td> LaMDA137B </td><td> PaLM540B </td><td> Codex12B{}^{a} </td><td> DavinciCodex{}^{*} </td><td> PaLMCoder 540B </td><td> OtherWork </td></tr><tr><td>HumanEval (0)</td><td>pass@100</td><td>47.3</td><td>76.2</td><td>72.3</td><td>81.7</td><td>88.4</td><td>\u2013</td></tr><tr><td>MBPP (3)</td><td>pass@80</td><td>62.4{}^{\\emph{b}}</td><td>75.0</td><td>\u2013</td><td>84.4</td><td>80.8</td><td>\u2013</td></tr><tr><td>TransCoder (3)</td><td>pass@25</td><td>\u2013</td><td>79.8</td><td>\u2013</td><td>71.7</td><td>82.5</td><td>67.2{}^{\\emph{c}}</td></tr><tr><td>HumanEval (0)</td><td>pass@1</td><td>14.0</td><td>26.2</td><td>28.8</td><td>36.0</td><td>36.0</td><td>\u2013</td></tr><tr><td>MBPP (3)</td><td>pass@1</td><td>14.8{}^{\\emph{b}}</td><td>36.8</td><td>\u2013</td><td>50.4</td><td>47.0</td><td>\u2013</td></tr><tr><td>GSM8K-Python (4)</td><td>pass@1</td><td>7.6</td><td>51.3</td><td>\u2013</td><td>32.1</td><td>50.9</td><td>\u2013</td></tr><tr><td>TransCoder (3)</td><td>pass@1</td><td>30.2</td><td>51.8</td><td>\u2013</td><td>54.4</td><td>55.1</td><td>44.5{}^{\\emph{c}}</td></tr><tr><td>DeepFix (2)</td><td>pass@1</td><td>4.3</td><td>73.7</td><td>\u2013</td><td>81.1</td><td>82.1</td><td>71.7{}^{\\it d}</td></tr></table>", "caption": "Table 12: Results obtained by the PaLM 540B and PaLM-Coder 540B models across code synthesis and software engineering tasks. For the few-shot results, the number of shots for each task are mentioned in parenthesis.Superscripts denote results that are quoted from past work: {}^{a} Chen et al. (2021); {}^{b} Austin et al. (2021); {}^{c} Lachaux et al. (2020); {}^{d}Yasunaga &amp; Liang (2021).{}^{*}Davinci Codex results are our own calculations obtained using the OpenAI Codex API and recommended settings for Codex as outlined in Chen et al. (2021).", "list_citation_info": ["Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.", "Yasunaga & Liang (2021) Yasunaga, M. and Liang, P. Break-it-fix-it: Unsupervised learning for program repair. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 11941\u201311952. PMLR, 2021. URL http://proceedings.mlr.press/v139/yasunaga21a.html.", "Lachaux et al. (2020) Lachaux, M., Rozi\u00e8re, B., Chanussot, L., and Lample, G. Unsupervised translation of programming languages. CoRR, abs/2006.03511, 2020. URL https://arxiv.org/abs/2006.03511.", "Austin et al. (2021) Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/2108.07732."]}, {"table": "<table><tr><td></td><td></td><td colspan=\"2\">0-shot</td><td colspan=\"2\">1-shot</td><td colspan=\"2\">Few-shot</td><td>Supervised</td></tr><tr><td>Src</td><td>Tgt</td><td> PriorSOTA </td><td> PaLM540B </td><td> PriorSOTA </td><td> PaLM540B </td><td> PriorSOTA </td><td> PaLM540B </td><td> FinetunedSOTA </td></tr><tr><td>en</td><td>fr</td><td>32.9^{a}</td><td>38.5</td><td>28.3^{b}</td><td>37.5</td><td>33.9^{a} (9)</td><td>44.0</td><td>\\underline{45.6}^{c}</td></tr><tr><td>en</td><td>de</td><td>25.4^{a}</td><td>31.8</td><td>26.2^{b}</td><td>31.8</td><td>26.8^{a} (11)</td><td>37.4</td><td>\\underline{41.2}^{d}</td></tr><tr><td>en</td><td>ro</td><td>16.7^{a}</td><td>24.2</td><td>20.6^{b}</td><td>28.2</td><td>20.5^{a} (9)</td><td>28.7</td><td>\\underline{33.4}^{e}</td></tr><tr><td>fr</td><td>en</td><td>35.5^{a}</td><td>41.1</td><td>33.7^{b}</td><td>37.4</td><td>38.0^{a} (9)</td><td>42.8</td><td>\\underline{45.4}^{f}</td></tr><tr><td>de</td><td>en</td><td>38.9^{a}</td><td>43.8</td><td>30.4^{b}</td><td>43.9</td><td>40.6^{a} (11)</td><td>47.5</td><td>41.2^{g}</td></tr><tr><td>ro</td><td>en</td><td>36.8^{a}</td><td>39.9</td><td>38.6^{b}</td><td>42.1</td><td>37.3^{a} (9)</td><td>43.8</td><td>39.1^{h}</td></tr></table>", "caption": "Table 14: Translation BLEU scores on traditional WMT language pairs. Superscripts denote results from past work: {}^{a}FLAN(Wei et al., 2022a); {}^{b}GPT-3 175B (Brown et al., 2020); {}^{c}(Edunov et al., 2018); {}^{d}(Wang et al., 2019b); {}^{e}(Caswell et al., 2019); {}^{f}Lin et al. (2020); {}^{g}(Wang et al., 2019b); {}^{h}(Song et al., 2019). For PaLM, we use 5 shots in the few-shot setting. We bold the best zero/few-shot results, and underline the best results overall. Few-shot evaluation corresponds to 5 shots for PaLM. Note that 0-shot prompt includes the source and target language names in the prompt, while 1-shot and few-shot don\u2019t (languages must be inferred from the exemplars), which may explain the strong 0-shot performance in some language pairs.", "list_citation_info": ["Caswell et al. (2019) Caswell, I., Chelba, C., and Grangier, D. Tagged back-translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pp. 53\u201363, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5206. URL https://aclanthology.org/W19-5206.", "Lin et al. (2020) Lin, Z., Pan, X., Wang, M., Qiu, X., Feng, J., Zhou, H., and Li, L. Pre-training multilingual neural machine translation by leveraging alignment information. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2649\u20132663, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.210. URL https://aclanthology.org/2020.emnlp-main.210.", "Edunov et al. (2018) Edunov, S., Ott, M., Auli, M., and Grangier, D. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 489\u2013500, 2018. URL https://aclanthology.org/D18-1045.", "Wei et al. (2022a) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. Proceedings of the International Conference on Learning Representations (ICLR), 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.", "Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.", "Song et al. (2019) Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. MASS: Masked sequence to sequence pre-training for language generation. Proceedings of the International Conference on Machine Learning, 2019. URL https://arxiv.org/abs/1905.02450.", "Wang et al. (2019b) Wang, Y., Xia, Y., He, T., Tian, F., Qin, T., Zhai, C., and Liu, T.-Y. Multi-agent dual learning. In Proceedings of the International Conference on Learning Representations (ICLR) 2019, 2019b. URL https://openreview.net/forum?id=HyGhN2A5tm."]}, {"table": "<table><tr><td></td><td></td><td>0-shot</td><td>1-shot</td><td>Few-shot</td><td>Supervised</td></tr><tr><td>Src</td><td>Tgt</td><td> PaLM540B </td><td> PaLM540B </td><td> PaLM540B </td><td> FinetunedSOTA </td></tr><tr><td>en</td><td>kk</td><td>\u2009 \u2009 \u2009 1.8</td><td>\u2009 \u2009 4.2</td><td>\u2009 \u2009 5.1</td><td>15.5{}^{a}</td></tr><tr><td>de</td><td>fr</td><td>\u2009 \u2009 \u2009 28.6</td><td>\u2009 \u2009 20.9</td><td>\u2009 \u2009 25.7</td><td>31.5{}^{b}</td></tr><tr><td>kk</td><td>en</td><td>\u2009 \u2009 \u2009 18.0</td><td>\u2009 \u2009 20.3</td><td>\u2009 \u2009 20.8</td><td>30.5{}^{c}</td></tr><tr><td>fr</td><td>de</td><td>\u2009 \u2009 \u2009 25.2</td><td>\u2009 \u2009 9.5</td><td>\u2009 \u2009 17.4</td><td>24.9{}^{b}</td></tr></table>", "caption": "Table 15: Translation BLEU scores on non-English centric and extremely-low resource language pairs. {}^{a}(Toral et al., 2019); {}^{b}(Xia et al., 2019);{}^{c}(Li et al., 2019). Note that 0-shot prompt includes the source and target language names in the prompt, while 1-shot and few-shot don\u2019t (languages must be inferred from the exemplars), which may explain the strong 0-shot performance in some language pairs.", "list_citation_info": ["Xia et al. (2019) Xia, Y., Tan, X., Tian, F., Gao, F., He, D., Chen, W., Fan, Y., Gong, L., Leng, Y., Luo, R., Wang, Y., Wu, L., Zhu, J., Qin, T., and Liu, T.-Y. Microsoft Research Asia\u2019s systems for WMT19. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 424\u2013433, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5348. URL https://aclanthology.org/W19-5348.", "Li et al. (2019) Li, B., Li, Y., Xu, C., Lin, Y., Liu, J., Liu, H., Wang, Z., Zhang, Y., Xu, N., Wang, Z., Feng, K., Chen, H., Liu, T., Li, Y., Wang, Q., Xiao, T., and Zhu, J. The NiuTrans machine translation systems for WMT19. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 257\u2013266, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5325. URL https://aclanthology.org/W19-5325.", "Toral et al. (2019) Toral, A., Edman, L., Yeshmagambetova, G., and Spenader, J. Neural machine translation for English\u2013Kazakh with morphological segmentation and synthetic data. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pp. 386\u2013392, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5343. URL https://aclanthology.org/W19-5343."]}, {"table": "<table><tr><td></td><td colspan=\"4\">1-shot</td><td colspan=\"5\">Finetuning</td></tr><tr><td><p>Task</p></td><td> LaMDA137B </td><td> PaLM8B </td><td> PaLM62B </td><td> PaLM540B </td><td> PriorSOTA </td><td> T5XXL </td><td> PaLM8B </td><td> PaLM62B </td><td> PaLM540B </td></tr><tr><td colspan=\"10\">Data-To-Text</td></tr><tr><td><p>Czech Restaurant (cs)</p></td><td>6.6</td><td>8.2</td><td>12.2</td><td>16.1</td><td>30.2^{a}</td><td>28.8</td><td>30.2</td><td>30.3</td><td>30.6</td></tr><tr><td><p>E2E (en)</p></td><td>29.2</td><td>27.7</td><td>33.5</td><td>35.2</td><td>\\textbf{45.8}^{b}</td><td>45.3</td><td>45.7</td><td>45.2</td><td>45.3</td></tr><tr><td><p>WebNLG (en)</p></td><td>30.5</td><td>29.1</td><td>38.6</td><td>44.4</td><td>\\textbf{53.5}^{c}</td><td>39.6</td><td>47.6</td><td>48.6</td><td>49.3</td></tr><tr><td><p>WebNLG (ru)</p></td><td>5.4</td><td>4.5</td><td>8.5</td><td>14.9</td><td>\\textbf{25.5}^{b}</td><td>23.2</td><td>22.4</td><td>23.3</td><td>23.4</td></tr><tr><td colspan=\"10\">Summarization</td></tr><tr><td><p>MLSum (de)</p></td><td>0.9</td><td>4.6</td><td>10.5</td><td>12.8</td><td>\\textbf{36.4}^{d}</td><td>35.9</td><td>26.5</td><td>30.0</td><td>33.1</td></tr><tr><td><p>MLSum (es)</p></td><td>0.5</td><td>2.3</td><td>3.2</td><td>3.6</td><td>\\textbf{13.8}^{b}</td><td>12.0</td><td>10.6</td><td>11.2</td><td>12.0</td></tr><tr><td><p>WikiLingua (en \\rightarrow en)</p></td><td>5.4</td><td>5.6</td><td>8.9</td><td>9.9</td><td>-</td><td>23.8</td><td>19.3</td><td>22.1</td><td>23.2</td></tr><tr><td><p>WikiLingua (es \\rightarrow en)</p></td><td>2.2</td><td>3.4</td><td>5.8</td><td>7.7</td><td>18.3^{d}</td><td>17.9</td><td>16.1</td><td>18.2</td><td>20.9</td></tr><tr><td><p>WikiLingua (ru \\rightarrow en)</p></td><td>0.1</td><td>2.3</td><td>5.2</td><td>6.6</td><td>14.6^{b}</td><td>12.5</td><td>13.9</td><td>16.6</td><td>18.6</td></tr><tr><td><p>WikiLingua (tr \\rightarrow en)</p></td><td>1.8</td><td>1.8</td><td>5.6</td><td>8.5</td><td>18.3^{b}</td><td>13.8</td><td>16.7</td><td>21.4</td><td>23.1</td></tr><tr><td><p>WikiLingua (vi \\rightarrow en)</p></td><td>0.3</td><td>1.5</td><td>4.0</td><td>5.5</td><td>14.9^{b}</td><td>9.7</td><td>13.4</td><td>16.3</td><td>19.1</td></tr><tr><td><p>XSum (en)</p></td><td>5.4</td><td>7.9</td><td>11.2</td><td>12.2</td><td>\\textbf{23.2}^{e}</td><td>21.0</td><td>16.3</td><td>18.5</td><td>21.2</td></tr></table>", "caption": "Table 16: ROUGE-2 results in GEM data-to-text and summarization datasets. We present finetuning results in comparison with prior reported SOTA, T5 XXL finetuned baselines. We also present few-shot results comparing them to LaMDA baselines. {}^{a}(Dusek &amp; Jurvc\u2019ivcek, 2019), {}^{b}(Xue et al., 2021b), {}^{c}(Bakshi et al., 2021), {}^{d}(Gehrmann et al., 2021), {}^{e}(Zhang et al., 2020). ", "list_citation_info": ["Bakshi et al. (2021) Bakshi, S., Batra, S., Heidari, P., Arun, A., Jain, S., and White, M. Structure-to-text generation with self-training, acceptability classifiers and context-conditioning for the GEM shared task. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pp. 136\u2013147, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.gem-1.12. URL https://aclanthology.org/2021.gem-1.12.", "Dusek & Jurvc\u2019ivcek (2019) Dusek, O. and Jurvc\u2019ivcek, F. Neural generation for czech: Data and baselines. 2019.", "Xue et al. (2021b) Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. mT5: A massively multilingual pre-trained text-to-text transformer. In NAACL, 2021b.", "Zhang et al. (2020) Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 11328\u201311339. PMLR, 2020. URL http://proceedings.mlr.press/v119/zhang20ae.html.", "Gehrmann et al. (2021) Gehrmann, S., Adewumi, T., Aggarwal, K., Ammanamanchi, P. S., Aremu, A., Bosselut, A., Chandu, K. R., Clinciu, M.-A., Das, D., Dhole, K., Du, W., Durmus, E., Du\u0161ek, O., Emezue, C. C., Gangal, V., Garbacea, C., Hashimoto, T., Hou, Y., Jernite, Y., Jhamtani, H., Ji, Y., Jolly, S., Kale, M., Kumar, D., Ladhak, F., Madaan, A., Maddela, M., Mahajan, K., Mahamood, S., Majumder, B. P., Martins, P. H., McMillan-Major, A., Mille, S., van Miltenburg, E., Nadeem, M., Narayan, S., Nikolaev, V., Niyongabo Rubungo, A., Osei, S., Parikh, A., Perez-Beltrachini, L., Rao, N. R., Raunak, V., Rodriguez, J. D., Santhanam, S., Sedoc, J., Sellam, T., Shaikh, S., Shimorina, A., Sobrevilla Cabezudo, M. A., Strobelt, H., Subramani, N., Xu, W., Yang, D., Yerukola, A., and Zhou, J. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pp. 96\u2013120, 2021. URL https://aclanthology.org/2021.gem-1.10."]}, {"table": "<table><tr><td></td><td colspan=\"2\"> First-sentence </td><td colspan=\"2\"> 128-decode steps </td></tr><tr><td>Model</td><td>Toxic</td><td>Non-toxic</td><td>Toxic</td><td>Non-toxic</td></tr><tr><td>PaLM 8B</td><td>0.78</td><td>0.44</td><td>0.90</td><td>0.53</td></tr><tr><td>PaLM 62B</td><td>0.81</td><td>0.46</td><td>0.91</td><td>0.58</td></tr><tr><td>PaLM 540B</td><td>0.80</td><td>0.46</td><td>0.91</td><td>0.56</td></tr></table>", "caption": "Table 20:  Probability of generating a comment that could be perceived as toxic (i.e. toxicity score &gt; 0.5) at least once in 25 continuations for different model sizes. We compute the toxicity probability for \u201cToxic\u201d and \u201cNon-Toxic\u201d prompts and report the results as such. We report the metric both in the first full-sentence completion and in the full 128 decoding steps similar to Rae et al. (2021). Note that the toxicity score assigned by the Perspective API tends to increase with the number of tokens generated, given that the results are not normalized to the text length.", "list_citation_info": ["Rae et al. (2021) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L. A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S. M., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d\u2019Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B. A., Weidinger, L., Gabriel, I., Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and Irving, G. Scaling language models: Methods, analysis & insights from training Gopher. CoRR, abs/2112.11446, 2021."]}, {"table": "<table><tr><td>Islam</td><td>Christianity</td><td>Hinduism</td><td>Buddhism</td><td>Judaism</td></tr><tr><td>muslim</td><td>christians</td><td>hindu</td><td>all</td><td>jewish</td></tr><tr><td>muslims</td><td>christian</td><td>hindus</td><td>rights</td><td>jews</td></tr><tr><td>islam</td><td>atheist</td><td>muslim</td><td>reserved</td><td>torah</td></tr><tr><td>islamic</td><td>bible</td><td>india</td><td>ja</td><td>judaism</td></tr><tr><td>moslem</td><td>christianity</td><td>muslims</td><td>buddha</td><td>synagogue</td></tr><tr><td>allah</td><td>atheists</td><td>bjp</td><td>buddhist</td><td>rabbis</td></tr><tr><td>mosque</td><td>jesus</td><td>buddhist</td><td>buddhism</td><td>rabbi</td></tr><tr><td>religion</td><td>muslims</td><td>indian</td><td>#</td><td>jew</td></tr><tr><td>terrorist</td><td>religion</td><td>sikh</td><td>buddhists</td><td>christians</td></tr><tr><td>islamist</td><td>christendom</td><td>temple</td><td></td><td>talmud</td></tr><tr><td>christians</td><td>christ</td><td>pradesh</td><td></td><td>anti-semitism</td></tr></table>", "caption": "Table 25: TF-IDF tokens co-occurring most frequently with religions (identities chosen to match the analysis in Brown et al. (2020)).", "list_citation_info": ["Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."]}, {"table": "<table><tr><td>asian_people</td><td>black_people</td><td>white_people</td><td>hispanic_people</td><td>indian_people</td><td>middle_eastern</td></tr><tr><td>asians</td><td>blacks</td><td>non-white</td><td>hispanic</td><td>india</td><td>persians</td></tr><tr><td>asian</td><td>negros</td><td>supremacist</td><td>hispanics</td><td>indian</td><td>ethnically</td></tr><tr><td>3gp</td><td>hispanics</td><td>racists</td><td>latino</td><td>narendra</td><td>iranians</td></tr><tr><td>milfs</td><td>racists</td><td>supremacists</td><td>latinos</td><td>delhi</td><td>caucasian</td></tr><tr><td>femdom</td><td>racially</td><td>hispanics</td><td>islander</td><td>mumbai</td><td>kurds</td></tr><tr><td>webcams</td><td>non-white</td><td>racially</td><td>demographics</td><td>gujarat</td><td>lebanese</td></tr><tr><td>pantyhose</td><td>african-americans</td><td>caucasian</td><td>asians</td><td>maharashtra</td><td>eastern</td></tr><tr><td></td><td>supremacist</td><td>asians</td><td>caucasian</td><td>hyderabad</td><td>semitic</td></tr></table>", "caption": "Table 26: TF-IDF tokens co-occurring most frequently with racial and ethnic identities (identities chosen to match the analysis in Brown et al. (2020)).", "list_citation_info": ["Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf."]}, {"table": "<table><tr><td colspan=\"2\">Motivation</td></tr><tr><td><p>For what purpose was the dataset created? Who created the dataset? Who funded the creation of the dataset?</p></td><td><p>The dataset was created for pre-training language models by a team of researchers at Google.</p></td></tr><tr><td><p>Any other comments?</p></td><td><p>To train the model, we started with the dataset described in  Du et al. (2021). The dataset is representative of a wide range of natural language use cases, and contains a high-quality filtered subset of webpages combined with books, Wikipedia pages, and data from public domain social media conversations used by  Adiwardana et al. (2020). We made several modifications to the dataset:\u2022Adjusted the mixing proportions of the components of the dataset to avoid repeating training examples and minimize the risk of unstable training or overfitting. The mixing proportions are given in Table 2.\u2022Used multilingual versions of Wikipedia and conversations data to improve the multilingual capabilities of the model and increase the number of tokens. The training mixture includes 124 languages, with English accounting for approximately 78% of the training tokens. The language distribution is shown in Figure 29.\u2022Included deduplicated code from GitHub, filtered by license so as to exclude repositories with a copyleft license.\u2022Included date markers in the conversation data, to allow for conditional generation on dates (in particular conditioning on a recent date to avoid generating outdated facts).</p></td></tr><tr><td colspan=\"2\">Composition</td></tr><tr><td><p>What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?</p></td><td><p>All instances of the dataset are text-only documents. Depending on the source, these are web pages, Wikipedia articles, news articles, books or source code files.</p></td></tr><tr><td><p>How many instances are there in total (of each type, if appropriate)?</p></td><td><p>The data makeup is given in Table 2.</p></td></tr><tr><td><p>Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?</p></td><td><p>The dataset is a (random) sample from a larger set. The sampling methodology is described in  Du et al. (2021). The different components of the dataset have different weights, as specified in Table 2.</p></td></tr><tr><td><p>What data does each instance consist of?</p></td><td><p>Each instance is a SentencePiece (Kudo &amp; Richardson, 2018b) encoded sequence of text.</p></td></tr><tr><td><p>Is there a label or target associated with each instance?</p></td><td><p>No, there are no labels associated with each instance.</p></td></tr><tr><td><p>Is any information missing from individual instances?</p></td><td><p>No.</p></td></tr><tr><td><p>Are relationships between individual instances made explicit?</p></td><td><p>There are no relationships between the different documents in each subset.</p></td></tr><tr><td><p>Are there recommended data splits?</p></td><td><p>We use random splits for the training and development sets.</p></td></tr><tr><td><p>Are there any errors, sources of noise, or redundancies in the dataset?</p></td><td><p>Despite removing duplicates at the document level, there is a lot of redundancy at the sub-document (paragraph, sentence) level. There is also redundancy coming from different instantiations of the same textual pattern, and from general low quality text from the Web, e.g., SEO spam.</p></td></tr><tr><td><p>Is the dataset self-contained, or does it link to or otherwise rely on external resources?</p></td><td><p>The dataset is self-contained.</p></td></tr><tr><td><p>Does the dataset contain data that might be considered confidential?</p></td><td><p>No.</p></td></tr><tr><td><p>Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?</p></td><td><p>The dataset likely contains data that might be considered offensive, insulting or threatening as such data is prevalent on the web and potentially in old books.</p></td></tr><tr><td colspan=\"2\">Collection Process</td></tr><tr><td><p>How was the data associated with each instance acquired?</p></td><td><p>The data was collected from publicly available sources.</p></td></tr><tr><td><p>What mechanisms or procedures were used to collect the data?</p></td><td><p>The data was collected using a variety of software programs to extract and clean raw text.</p></td></tr><tr><td><p>If the dataset is a sample from a larger set, what was the sampling strategy?</p></td><td><p>The sampling methodology is described in  Du et al. (2021). For Web documents, we use two methods of sampling:\u2022Random sampling based on a classifier that gives higher probability to high quality documents.\u2022Selecting documents that are also in the Colossal Clean Crawled Corpus (C4) (Raffel et al., 2020).</p></td></tr><tr><td><p>Who was involved in the data collection process?</p></td><td><p>A team of researchers at Google.</p></td></tr><tr><td><p>Over what timeframe was the data collected?</p></td><td><p>2019-2021</p></td></tr><tr><td><p>Were any ethical review processes conducted?</p></td><td><p>No.</p></td></tr><tr><td colspan=\"2\">Preprocessing, cleaning, and labeling</td></tr><tr><td><p>Was any preprocessing, cleaning, or labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?</p></td><td><p>We remove boilerplate from web pages using proprietary software. We also remove HTML markup. We extract conversations using a special-purpose algorithm.</p></td></tr><tr><td><p>Is the software used to preprocess, clean, or label the instances available?</p></td><td><p>No.</p></td></tr><tr><td colspan=\"2\">Uses</td></tr><tr><td><p>Has the dataset been used for any tasks already?</p></td><td><p>Yes, we use the dataset for pre-training language models.</p></td></tr><tr><td><p>Is there a repository that links to any or all papers or systems that use the dataset?</p></td><td><p>No.</p></td></tr><tr><td><p>What (other) tasks could the dataset be used for?</p></td><td><p>The large-scale task-agnostic nature of the dataset makes it suitable for many NLP tasks such as language model pretraining or question answering.</p></td></tr><tr><td><p>Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?</p></td><td><p>The dataset is static in nature and thus will become progressively more \u201cstale\u201d. It will for example not reflect new language and norms that evolve over time. However, due to the nature of the dataset it is relatively cheap to collect an up-to-date version of the same dataset.</p></td></tr><tr><td><p>Are there tasks for which the dataset should not be used?</p></td><td><p>This model should not be used for any of the unacceptable language model use cases, e.g., generation of toxic speech.</p></td></tr><tr><td colspan=\"2\">Distribution</td></tr><tr><td><p>Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?</p></td><td><p>No.</p></td></tr></table>", "caption": "Table 27: PaLM Datasheet", "list_citation_info": ["Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.", "Adiwardana et al. (2020) Adiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., et al. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020.", "Du et al. (2021) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. GLaM: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021. URL https://arxiv.org/pdf/2112.06905.", "Kudo & Richardson (2018b) Kudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Blanco, E. and Lu, W. (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pp. 66\u201371. Association for Computational Linguistics, 2018b. doi: 10.18653/v1/d18-2012. URL https://doi.org/10.18653/v1/d18-2012."]}, {"table": "<table><tr><td colspan=\"2\">Model Summary</td></tr><tr><td>Model Architecture</td><td><p>Dense decoder-only model with 540 billion parameters. Transformer model architecture with variants to speed up training and inference. For details, see Model Architecture (Section 2).</p></td></tr><tr><td>Input(s)</td><td><p>The model takes text as input.</p></td></tr><tr><td>Output(s)</td><td><p>The model generates text as output.</p></td></tr><tr><td colspan=\"2\">Usage</td></tr><tr><td>Application</td><td><p>The primary use is research on language models, including: research on NLP applications like machine translation and question answering, advancing fairness and safety research, and understanding limitations of current LLMs.</p><p>Within Google, PaLM is being used for research on a variety of open-ended text and code generation tasks, including reasoning (Section 6.3) and code synthesis and understanding (Section 6.4).</p></td></tr><tr><td>Known Caveats</td><td><p>Gopher (Rae et al., 2021) describes safety benefits and safety risks associated with large language models, including PaLM. These risks include uses of language models for language generation in harmful or deceitful settings.</p><p>PaLM should not be used for downstream applications without a prior assessment and mitigation of the safety and fairness concerns specific to the downstream application. In particular, we recommend focusing mitigation efforts at the downstream application level rather than at the pretrained level.</p></td></tr><tr><td colspan=\"2\">System Type</td></tr><tr><td>System Description</td><td><p>This is a standalone model.</p></td></tr><tr><td>Upstream Dependencies</td><td><p>None.</p></td></tr><tr><td>Downstream Dependencies</td><td><p>None.</p></td></tr><tr><td colspan=\"2\">Implementation Frameworks</td></tr><tr><td>Hardware &amp; Software: Training</td><td><p>Hardware: TPU v4 (Jouppi et al., 2020).</p><p>Software: T5X (t5x, 2021), JAX (Bradbury et al., 2018), Pathways (Barham et al., 2022).</p><p>For details, see Training Infrastructure (Section 4).</p></td></tr><tr><td>Hardware &amp; Software: Deployment</td><td><p>Hardware: TPU v4 (Jouppi et al., 2020).</p><p>Software: T5X (t5x, 2021).</p></td></tr><tr><td>Compute Requirements</td><td><p>Reported in Compute Usage (Section B).</p></td></tr><tr><td colspan=\"2\">Model Characteristics</td></tr><tr><td>Model Initialization</td><td><p>The model is trained from a random initialization.</p></td></tr><tr><td>Model Status</td><td><p>This is a static model trained on an offline dataset.</p></td></tr><tr><td>Model Stats</td><td><p>The largest PaLM model has 540 billion dense parameters. We have also trained 8 billion and 62 billion parameter models.</p></td></tr><tr><td colspan=\"2\">Data Overview</td></tr><tr><td>Training Dataset</td><td><p>See Datasheet (Appendix D) for the description of datasets used to train PaLM.</p></td></tr><tr><td>Evaluation Dataset</td><td><p>We evaluate the PaLM family of models on a wide variety of tasks. Specifically, we evaluate the models on English Natural Language Processing (NLP) tasks (Section 6.1), tasks from BIG-bench  (BIG-bench collaboration, 2021), reasoning tasks (Section 6.3), code completion tasks (Section 6.4), multilingual generation and question answering tasks (Section 6.6), translation tasks (Section 6.5), and bias and toxicity benchmarks (Rudinger et al., 2018; Gehman et al., 2020).</p></td></tr><tr><td>Fine-tuning Dataset</td><td><p>We include finetuning results on SuperGLUE (wang2019superglue), tasks from GEM (Gehrmann et al., 2021), and TyDiQA (Clark et al., 2020). We also finetune on a code dataset and share results on the finetuned model on code synthesis tasks.</p></td></tr><tr><td colspan=\"2\">Evaluation Results</td></tr><tr><td>Benchmark Information</td><td><p>\u2022Fewshot: English Natural Language Processing (NLP) tasks (Section 6.1), BIG-bench (Section 6.2), Reasoning (Section 6.3), Code (Section 6.4), GEM (Section 6.6), Translation (Section 6.5), Multi-lingual Question Answering (Section 6.7)\u2022Finetuning: SuperGLUE (Section 6.1.2), GEM (Section 6.6), TyDiQA (Section 6.7).\u2022Responsible AI: Co-occurrence, Winogender (Section 10.1.1), RealToxicity (Section 10.2).\u2022Data contamination (Section 8)</p></td></tr><tr><td>Evaluation Results</td><td><p>Reported in Evaluation (Section 6).</p></td></tr><tr><td colspan=\"2\">Model Usage &amp; Limitations</td></tr><tr><td>Sensitive Use</td><td><p>PaLM is capable of open-ended text generation. This model should not be used for any of the unacceptable language model use cases, e.g., generation of toxic speech.</p></td></tr><tr><td>Known Limitations</td><td><p>PaLM is designed for research. The model has not been tested in settings outside of research that can affect performance, and it should not be used for downstream applications without further analysis on factors in the proposed downstream application.</p></td></tr><tr><td>Ethical Considerations &amp; Risks</td><td><p>Reported in Ethical Considerations (Section 11).</p></td></tr></table>", "caption": "Table 30: PaLM Model Card", "list_citation_info": ["Barham et al. (2022) Barham, P., Chowdhery, A., Dean, J., Ghemawat, S., Hand, S., Hurt, D., Isard, M., Lim, H., Pang, R., Roy, S., Saeta, B., Schuh, P., Sepassi, R., Shafey, L. E., Thekkath, C. A., and Wu, Y. Pathways: Asynchronous distributed dataflow for ML. To appear in MLSys 2022, 2022. URL https://arxiv.org/abs/2203.12533.", "Jouppi et al. (2020) Jouppi, N. P., Yoon, D. H., Kurian, G., Li, S., Patil, N., Laudon, J., Young, C., and Patterson, D. A domain-specific supercomputer for training deep neural networks. Communications of the ACM, 63(7):67\u201378, 2020.", "Bradbury et al. (2018) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: Composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.", "Rae et al. (2021) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L. A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S. M., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d\u2019Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B. A., Weidinger, L., Gabriel, I., Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and Irving, G. Scaling language models: Methods, analysis & insights from training Gopher. CoRR, abs/2112.11446, 2021.", "BIG-bench collaboration (2021) BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. In preparation, 2021. URL https://github.com/google/BIG-bench/.", "t5x (2021) T5x, 2021. URL https://github.com/google-research/t5x.", "Clark et al. (2020) Clark, J., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J. TydiQA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 2020. URL https://storage.googleapis.com/tydiqa/tydiqa.pdf.", "Gehrmann et al. (2021) Gehrmann, S., Adewumi, T., Aggarwal, K., Ammanamanchi, P. S., Aremu, A., Bosselut, A., Chandu, K. R., Clinciu, M.-A., Das, D., Dhole, K., Du, W., Durmus, E., Du\u0161ek, O., Emezue, C. C., Gangal, V., Garbacea, C., Hashimoto, T., Hou, Y., Jernite, Y., Jhamtani, H., Ji, Y., Jolly, S., Kale, M., Kumar, D., Ladhak, F., Madaan, A., Maddela, M., Mahajan, K., Mahamood, S., Majumder, B. P., Martins, P. H., McMillan-Major, A., Mille, S., van Miltenburg, E., Nadeem, M., Narayan, S., Nikolaev, V., Niyongabo Rubungo, A., Osei, S., Parikh, A., Perez-Beltrachini, L., Rao, N. R., Raunak, V., Rodriguez, J. D., Santhanam, S., Sedoc, J., Sellam, T., Shaikh, S., Shimorina, A., Sobrevilla Cabezudo, M. A., Strobelt, H., Subramani, N., Xu, W., Yang, D., Yerukola, A., and Zhou, J. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pp. 96\u2013120, 2021. URL https://aclanthology.org/2021.gem-1.10.", "Rudinger et al. (2018) Rudinger, R., Naradowsky, J., Leonard, B., and Van Durme, B. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 8\u201314, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2002. URL https://aclanthology.org/N18-2002."]}, {"table": "<table><tr><td></td><td colspan=\"3\">0-shot</td><td colspan=\"4\">1-shot</td><td colspan=\"3\">Few-shot</td></tr><tr><td><p>Task</p></td><td> PaLM8B </td><td> PaLM62B </td><td> PaLM540B </td><td> PaLM8B </td><td> PaLM62B </td><td> PaLM62B </td><td> PaLM540B </td><td> PaLM8B </td><td> PaLM62B </td><td> PaLM540B </td></tr><tr><td><p>Training tokens</p></td><td> 780B </td><td> 795B </td><td> 780B </td><td> 780B </td><td> 795B </td><td> 1325B </td><td> 780B </td><td> 780B </td><td> 795B </td><td> 780B </td></tr><tr><td><p>TriviaQA (EM)</p></td><td>39.5</td><td>67.3</td><td>76.9</td><td>48.5</td><td>72.7</td><td>74.8</td><td>81.4</td><td>47.2</td><td>70.1</td><td>81.4 (1)</td></tr><tr><td><p>Natural Questions (EM)</p></td><td>8.4</td><td>18.1</td><td>21.2</td><td>10.6</td><td>23.1</td><td>26.5</td><td>29.3</td><td>14.6</td><td>27.6</td><td>39.6 (64)</td></tr><tr><td><p>Web Questions (EM)</p></td><td>6.7</td><td>11.5</td><td>10.6</td><td>12.6</td><td>19.8</td><td>24.9</td><td>22.6</td><td>19.9</td><td>29.7</td><td>43.5 (64)</td></tr><tr><td><p>Lambada (EM)</p></td><td>69.5</td><td>75.4</td><td>77.9</td><td>57.8</td><td>75.5</td><td>81.0</td><td>81.8</td><td>75.5</td><td>83.3</td><td>89.7 (8)</td></tr><tr><td><p>HellaSwag</p></td><td>68.7</td><td>79.7</td><td>83.4</td><td>68.2</td><td>79.7</td><td>81.0</td><td>83.6</td><td>68.6</td><td>80.0</td><td>83.8</td></tr><tr><td><p>StoryCloze</p></td><td>77.5</td><td>83.5</td><td>84.6</td><td>78.7</td><td>83.8</td><td>85.2</td><td>86.1</td><td>81.5</td><td>86.7</td><td>89.0</td></tr><tr><td><p>Winograd</p></td><td>80.6</td><td>87.5</td><td>90.1</td><td>85.3</td><td>82.4</td><td>88.6</td><td>87.5</td><td>83.2</td><td>89.7</td><td>89.4</td></tr><tr><td><p>Winogrande</p></td><td>66.3</td><td>77.0</td><td>81.1</td><td>68.3</td><td>76.8</td><td>77.7</td><td>83.7</td><td>70.1</td><td>79.8</td><td>85.1</td></tr><tr><td><p>Drop (F1)</p></td><td>45.1</td><td>57.9</td><td>69.4</td><td>40.7</td><td>59.0</td><td>62.9</td><td>70.8</td><td>43.7</td><td>58.4</td><td>70.8 (1)</td></tr><tr><td><p>CoQA (F1)</p></td><td>66.0</td><td>75.7</td><td>77.6</td><td>67.4</td><td>77.9</td><td>78.5</td><td>79.9</td><td>69.2</td><td>79.7</td><td>81.5</td></tr><tr><td><p>QuAC (F1)</p></td><td>33.1</td><td>40.0</td><td>45.2</td><td>29.4</td><td>41.3</td><td>42.5</td><td>47.7</td><td>29.4 (1)</td><td>41.3 (1)</td><td>47.7 (1)</td></tr><tr><td><p>SQuADv2 (F1)</p></td><td>56.4</td><td>75.6</td><td>80.8</td><td>56.0</td><td>77.6</td><td>78.1</td><td>82.9</td><td>57.5</td><td>78.6</td><td>83.3</td></tr><tr><td><p>SQuADv2 (EM)</p></td><td>49.6</td><td>69.9</td><td>75.5</td><td>50.1</td><td>72.9</td><td>73.4</td><td>78.7</td><td>51.7</td><td>74.2</td><td>79.6</td></tr><tr><td><p>RACE-m</p></td><td>57.9</td><td>64.3</td><td>68.1</td><td>57.7</td><td>64.1</td><td>66.6</td><td>69.3</td><td>58.2</td><td>64.9</td><td>72.1 (8)</td></tr><tr><td><p>RACE-h</p></td><td>42.3</td><td>47.5</td><td>49.1</td><td>41.6</td><td>48.7</td><td>49.5</td><td>52.1</td><td>41.9</td><td>48.1</td><td>54.6</td></tr><tr><td><p>PIQA</p></td><td>77.1</td><td>80.5</td><td>82.3</td><td>76.1</td><td>80.9</td><td>81.6</td><td>83.9</td><td>77.5</td><td>82.3</td><td>85.2</td></tr><tr><td><p>ARC-e</p></td><td>69.2</td><td>75.2</td><td>76.6</td><td>71.3</td><td>78.9</td><td>82.0</td><td>85.0</td><td>71.3</td><td>80.6</td><td>88.4</td></tr><tr><td><p>ARC-c</p></td><td>39.2</td><td>52.5</td><td>53.0</td><td>42.3</td><td>51.8</td><td>56.3</td><td>60.1</td><td>40.2</td><td>53.7</td><td>65.9</td></tr><tr><td><p>OpenbookQA</p></td><td>48.4</td><td>50.4</td><td>53.4</td><td>47.4</td><td>51.2</td><td>53.6</td><td>53.6</td><td>48.4</td><td>57.2</td><td>68.0 (32)</td></tr><tr><td><p>BoolQ</p></td><td>68.3</td><td>84.8</td><td>88.0</td><td>64.7</td><td>83.1</td><td>86.1</td><td>88.7</td><td>68.9</td><td>82.7</td><td>89.1 (8)</td></tr><tr><td><p>Copa</p></td><td>86.0</td><td>93.0</td><td>93.0</td><td>82.0</td><td>93.0</td><td>89.0</td><td>91.0</td><td>86.0</td><td>93.0</td><td>95.0</td></tr><tr><td><p>RTE</p></td><td>54.2</td><td>67.9</td><td>72.9</td><td>57.8</td><td>71.5</td><td>78.3</td><td>78.7</td><td>56.7</td><td>76.5</td><td>81.2</td></tr><tr><td><p>WiC</p></td><td>47.0</td><td>49.4</td><td>59.1</td><td>47.3</td><td>48.6</td><td>55.5</td><td>63.2</td><td>52.4</td><td>57.5</td><td>64.6</td></tr><tr><td><p>Multirc (F1a)</p></td><td>47.5</td><td>72.1</td><td>83.5</td><td>50.6</td><td>72.7</td><td>79.6</td><td>84.9</td><td>41.1</td><td>78.6</td><td>86.3</td></tr><tr><td><p>WSC</p></td><td>78.9</td><td>86.3</td><td>89.1</td><td>81.4</td><td>84.9</td><td>88.1</td><td>86.3</td><td>83.2</td><td>88.8</td><td>89.5</td></tr><tr><td><p>ReCoRD</p></td><td>87.8</td><td>91.4</td><td>92.9</td><td>87.8</td><td>91.0</td><td>91.6</td><td>92.8</td><td>88.0</td><td>91.2</td><td>92.9 (2)</td></tr><tr><td><p>CB</p></td><td>41.1^{a}</td><td>57.1</td><td>51.8</td><td>41.1</td><td>55.4</td><td>71.4</td><td>83.9</td><td>57.1</td><td>78.6</td><td>89.3</td></tr><tr><td><p>ANLI R1</p></td><td>34.9</td><td>36.4</td><td>48.4</td><td>32.4</td><td>36.3</td><td>45.4</td><td>52.6</td><td>29.8</td><td>38.2</td><td>56.9</td></tr><tr><td><p>ANLI R2</p></td><td>35.8</td><td>37.2</td><td>44.2</td><td>31.4</td><td>34.5</td><td>39.4</td><td>48.7</td><td>32.5</td><td>35.3</td><td>56.1</td></tr><tr><td><p>ANLI R3</p></td><td>34.5</td><td>36.7</td><td>45.7</td><td>34.5</td><td>40.1</td><td>45.9</td><td>52.3</td><td>32.7</td><td>40.8</td><td>51.2</td></tr></table>", "caption": "Table 40: Results obtained by the PaLM 8B and 62B model in comparison to PaLM 540B across 29 NLP benchmarks. For the few-shot results, we report results using 5 shot evaluation unless otherwise specified. The splits for each task are the same ones used in Du et al. (2021) and Brown et al. (2020). We report accuracy for all tasks unless otherwise specified. For the 62B model, we report 1-shot results at 795B tokens and with additional training at 1325B tokens.", "list_citation_info": ["Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901, 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.", "Du et al. (2021) Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A. W., Firat, O., et al. GLaM: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021. URL https://arxiv.org/pdf/2112.06905."]}]}