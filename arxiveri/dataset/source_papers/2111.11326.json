{"title": "Dytox: Transformers for continual learning with dynamic token expansion", "abstract": "Deep network architectures struggle to continually learn new tasks without forgetting the previous tasks. A recent trend indicates that dynamic architectures based on an expansion of the parameters can reduce catastrophic forgetting efficiently in continual learning. However, existing approaches often require a task identifier at test-time, need complex tuning to balance the growing number of parameters, and barely share any information across tasks. As a result, they struggle to scale to a large number of tasks without significant overhead. In this paper, we propose a transformer architecture based on a dedicated encoder/decoder framework. Critically, the encoder and decoder are shared among all tasks. Through a dynamic expansion of special tokens, we specialize each forward of our decoder network on a task distribution. Our strategy scales to a large number of tasks while having negligible memory and time overheads due to strict control of the parameters expansion. Moreover, this efficient strategy doesn't need any hyperparameter tuning to control the network's expansion. Our model reaches excellent results on CIFAR100 and state-of-the-art performances on the large-scale ImageNet100 and ImageNet1000 while having less parameters than concurrent dynamic frameworks.", "authors": ["Arthur Douillard", " Alexandre Ram\u00e9", " Guillaume Couairon", " Matthieu Cord"], "pdf_url": "https://arxiv.org/abs/2111.11326", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"5\">ImageNet100 10 steps</td><td colspan=\"5\">ImageNet1000 10 steps</td></tr><tr><th></th><td rowspan=\"2\">\\#P</td><td colspan=\"2\">top-1</td><td colspan=\"2\">top-5</td><td rowspan=\"2\">\\#P</td><td colspan=\"2\">top-1</td><td colspan=\"2\">top-5</td></tr><tr><th>Methods</th><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td></tr><tr><th>ResNet18 joint</th><td>11.22</td><td>-</td><td>-</td><td>-</td><td>95.10</td><td>11.68</td><td>-</td><td>-</td><td>-</td><td>89.27</td></tr><tr><th>Transf. joint</th><td>11.00</td><td>-</td><td>79.12</td><td>-</td><td>93.48</td><td>11.35</td><td>-</td><td>73.58</td><td>-</td><td>90.60</td></tr><tr><th>E2E [5]</th><td>11.22</td><td>-</td><td>-</td><td>89.92</td><td>80.29</td><td>11.68</td><td>-</td><td>-</td><td>72.09</td><td>52.29</td></tr><tr><th>Simple-DER [48]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>28.00</td><td>66.63</td><td>59.24</td><td>85.62</td><td>80.76</td></tr><tr><th>iCaRL [59]</th><td>11.22</td><td>-</td><td>-</td><td>83.60</td><td>63.80</td><td>11.68</td><td>38.40</td><td>22.70</td><td>63.70</td><td>44.00</td></tr><tr><th>BiC [32]</th><td>11.22</td><td>-</td><td>-</td><td>90.60</td><td>84.40</td><td>11.68</td><td>-</td><td>-</td><td>84.00</td><td>73.20</td></tr><tr><th>WA [81]</th><td>11.22</td><td>-</td><td>-</td><td>91.00</td><td>84.10</td><td>11.68</td><td>65.67</td><td>55.60</td><td>86.60</td><td>81.10</td></tr><tr><th>RPSNet [56]</th><td></td><td>-</td><td>-</td><td>87.90</td><td>74.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DER w/o P [76]</th><td>112.27</td><td>77.18</td><td>66.70</td><td>93.23</td><td>87.52</td><td>116.89</td><td>68.84</td><td>60.16</td><td>88.17</td><td>82.86</td></tr><tr><th>\\text{DER}^{\\dagger} [76]</th><td>-</td><td>76.12</td><td>66.06</td><td>92.79</td><td>88.38</td><td>-</td><td>66.73</td><td>58.62</td><td>87.08</td><td>81.89</td></tr><tr><th>DyTox</th><td>11.01</td><td>77.15</td><td>69.10</td><td>92.04</td><td>87.98</td><td>11.36</td><td>71.29</td><td>63.34</td><td>88.59</td><td>84.49</td></tr></tbody></table>", "caption": "Table 2: Results on ImageNet-100 and ImageNet-1000 datasets, learned with 10 steps of respectively 10 and 100 new classes. E2E [5] and Simple-DER [48] results come from their respective papers, and used a different class ordering. Other results come from [76]. The \\dagger symbol means that [76] needed setting-sensitive hyperparameters. Moreover, its reported parameters count was an average over all steps ([76] reported 14.52M on ImageNet1000): the final parameters count (necessarily higher) was not available.", "list_citation_info": ["[32] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[48] Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang, and Wei-Shi Zheng. Preserving earlier knowledge in continual learning with the help of all previous feature extractors. In arXiv preprint library, 2021.", "[5] Francisco M. Castro, Manuel J Mar\u00edn-Jim\u00e9nez, Nicol\u00e1s Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2018.", "[59] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[56] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An adaptive random path selection approach for incremental learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "[76] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[81] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shutao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"3\">10 steps</td><td colspan=\"3\">20 steps</td><td colspan=\"3\">50 steps</td></tr><tr><th>Methods</th><td>#P</td><td>Avg</td><td>Last</td><td>#P</td><td>Avg</td><td>Last</td><td>#P</td><td>Avg</td><td>Last</td></tr><tr><th>ResNet18 Joint</th><td>11.22</td><td>-</td><td>80.41</td><td>11.22</td><td>-</td><td>81.49</td><td>11.22</td><td>-</td><td>81.74</td></tr><tr><th>Transf. Joint</th><td>10.72</td><td>-</td><td>76.12</td><td>10.72</td><td>-</td><td>76.12</td><td>10.72</td><td>-</td><td>76.12</td></tr><tr><th>iCaRL [59]</th><td>11.22</td><td>65.27\u2009\\pm\u20091.02</td><td>50.74</td><td>11.22</td><td>61.20\u2009\\pm\u20090.83</td><td>43.75</td><td>11.22</td><td>56.08\u2009\\pm\u20090.83</td><td>36.62</td></tr><tr><th>UCIR [32]</th><td>11.22</td><td>58.66\u2009\\pm\u20090.71</td><td>43.39</td><td>11.22</td><td>58.17\u2009\\pm\u20090.30</td><td>40.63</td><td>11.22</td><td>56.86\u2009\\pm\u20090.83</td><td>37.09</td></tr><tr><th>BiC [75]</th><td>11.22</td><td>68.80\u2009\\pm\u20091.20</td><td>53.54</td><td>11.22</td><td>66.48\u2009\\pm\u20090.32</td><td>47.02</td><td>11.22</td><td>62.09\u2009\\pm\u20090.85</td><td>41.04</td></tr><tr><th>WA [81]</th><td>11.22</td><td>69.46\u2009\\pm\u20090.29</td><td>53.78</td><td>11.22</td><td>67.33\u2009\\pm\u20090.15</td><td>47.31</td><td>11.22</td><td>64.32\u2009\\pm\u20090.28</td><td>42.14</td></tr><tr><th>PODNet [19]</th><td>11.22</td><td>58.03\u2009\\pm\u20091.27</td><td>41.05</td><td>11.22</td><td>53.97\u2009\\pm\u20090.85</td><td>35.02</td><td>11.22</td><td>51.19\u2009\\pm\u20091.02</td><td>32.99</td></tr><tr><th>RPSNet [56]</th><td>56.5</td><td>68.60</td><td>57.05</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DER w/o P [76]</th><td>112.27</td><td>75.36\u2009\\pm\u20090.36</td><td>65.22</td><td>224.55</td><td>74.09\u2009\\pm\u20090.33</td><td>62.48</td><td>561.39</td><td>72.41\u2009\\pm\u20090.36</td><td>59.08</td></tr><tr><th>\\text{DER}^{\\dagger} [76]</th><td>-</td><td>74.64\u2009\\pm\u20090.28</td><td>64.35</td><td>-</td><td>73.98\u2009\\pm\u20090.36</td><td>62.55</td><td>-</td><td>72.05\u2009\\pm\u20090.55</td><td>59.76</td></tr><tr><th>DyTox</th><td>10.73</td><td>73.66\u2009\\pm\u20090.02</td><td>60.67\u2009\\pm\u20090.34</td><td>10.74</td><td>72.27\u2009\\pm\u20090.18</td><td>56.32\u2009\\pm\u20090.61</td><td>10.77</td><td>70.20\u2009\\pm\u20090.16</td><td>52.34\u2009\\pm\u20090.26</td></tr><tr><th>DyTox+</th><td>10.73</td><td>75.54\u2009\\pm\u20090.10</td><td>62.06\u2009\\pm\u20090.25</td><td>10.74</td><td>75.04\u2009\\pm\u20090.11</td><td>60.03\u2009\\pm\u20090.45</td><td>10.77</td><td>74.35\u2009\\pm\u20090.05</td><td>57.09\u2009\\pm\u20090.13</td></tr></tbody></table>", "caption": "Table 3: Results on CIFAR100 averaged over three different class orders. Baselines results are come from [76]. The \\dagger symbol means that [76] needed setting-sensitive hyperparameters. Moreover, its reported parameters count was an average over all steps: the final parameters count (necessarily higher) was not available.", "list_citation_info": ["[32] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[75] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[59] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[19] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2020.", "[56] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An adaptive random path selection approach for incremental learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "[76] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[81] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shutao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><thead><tr><th></th><th>1 step</th><th colspan=\"2\">50 steps</th></tr><tr><th>Training</th><th>Last (\\uparrow)</th><th>Last (\\uparrow)</th><th>Forgetting (\\downarrow)</th></tr></thead><tbody><tr><th>DyTox</th><th>76.12</th><td>52.34</td><td>33.15 </td></tr><tr><th>DyTox+</th><th>77.51+1.39</th><td>57.09+4.75</td><td>31.50-1.65</td></tr></tbody></table>", "caption": "Table 4: \u201cLast\u201d accuracy and forgetting [8] on CIFAR100 for the joint (1 step, no continual) and 50 steps settings.", "list_citation_info": ["[8] Arslan Chaudhry, Puneet Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>Hyperparameter</th><th>Range</th><th>Chosen value</th></tr></thead><tbody><tr><th>Learning rate</th><td>1e^{-3}, 5e^{-4}, 1e^{-4}</td><td>5e^{-4}</td></tr><tr><th>Epochs</th><td>300, 500, 700</td><td>500</td></tr><tr><th>\\lambda</th><td>0.05, 0.1, 0.5</td><td>0.1</td></tr><tr><th>CIFAR\u2019s patch size</th><td>4, 8, 16</td><td>4</td></tr><tr><th>ImageNet\u2019s patch size</th><td>14, 16</td><td>16</td></tr></tbody></table>", "caption": "Table 7: Hyperparameters that were tuned from the codebase of [69]. We ran a gridsearch on CIFAR100 10 steps on a validation set made of 10% of the training set, and kept fixed the chosen hyperparameters for all experiments (any number of steps and any datasets).", "list_citation_info": ["[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers and distillation through attention. In International Conference on Machine Learning (ICML), 2021."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"3\">10 steps</td><td colspan=\"3\">20 steps</td><td colspan=\"3\">50 steps</td></tr><tr><th>Methods</th><td>#P</td><td>Avg</td><td>Last</td><td>#P</td><td>Avg</td><td>Last</td><td>#P</td><td>Avg</td><td>Last</td></tr><tr><th>ResNet18 Joint</th><td>11.22</td><td>-</td><td>80.41</td><td>11.22</td><td>-</td><td>81.49</td><td>11.22</td><td>-</td><td>81.74</td></tr><tr><th>Transf. Joint</th><td>10.72</td><td>-</td><td>76.12</td><td>10.72</td><td>-</td><td>76.12</td><td>10.72</td><td>-</td><td>76.12</td></tr><tr><th>WA [81]</th><td>11.22</td><td>69.46\u2009\\pm\u20090.29</td><td>53.78</td><td>11.22</td><td>67.33\u2009\\pm\u20090.15</td><td>47.31</td><td>11.22</td><td>64.32\u2009\\pm\u20090.28</td><td>42.14</td></tr><tr><th>DER w/o P [76]</th><td>112.27</td><td>75.36\u2009\\pm\u20090.36</td><td>65.22</td><td>224.55</td><td>74.09\u2009\\pm\u20090.33</td><td>62.48</td><td>561.39</td><td>72.41\u2009\\pm\u20090.36</td><td>59.08</td></tr><tr><th>DyTox</th><td>10.73</td><td>73.66\u2009\\pm\u20090.02</td><td>60.67\u2009\\pm\u20090.34</td><td>10.74</td><td>72.27\u2009\\pm\u20090.18</td><td>56.32\u2009\\pm\u20090.61</td><td>10.77</td><td>70.20\u2009\\pm\u20090.16</td><td>52.34\u2009\\pm\u20090.26</td></tr><tr><th>DyTox+</th><td>10.73</td><td>75.54\u2009\\pm\u20090.10</td><td>62.06\u2009\\pm\u20090.25</td><td>10.74</td><td>75.04\u2009\\pm\u20090.11</td><td>60.03\u2009\\pm\u20090.45</td><td>10.77</td><td>74.35\u2009\\pm\u20090.05</td><td>57.09\u2009\\pm\u20090.13</td></tr><tr><th>DyTox++</th><td>10.73</td><td>77.10\u2009\\pm\u20090.08</td><td>64.53\u2009\\pm\u20090.08</td><td>10.74</td><td>76.57\u2009\\pm\u20090.18</td><td>62.44\u2009\\pm\u20090.22</td><td>10.77</td><td>75.45\u2009\\pm\u20090.19</td><td>58.76\u2009\\pm\u20090.28</td></tr><tr><th>DyTox+</th><td>10.73</td><td>76.74\u2009\\pm\u20091.08</td><td>67.04\u2009\\pm\u20090.10</td><td>10.74</td><td>76.25\u2009\\pm\u20090.30</td><td>62.85\u2009\\pm\u20090.16</td><td>10.77</td><td>74.16\u2009\\pm\u20091.89</td><td>59.10\u2009\\pm\u20090.99</td></tr><tr><th>DyTox++</th><td>10.73</td><td>77.01\u2009\\pm\u20091.21</td><td>67.53\u2009\\pm\u20090.37</td><td>10.74</td><td>76.81\u2009\\pm\u20090.43</td><td>64.27\u2009\\pm\u20090.81</td><td>10.77</td><td>75.53\u2009\\pm\u20092.79</td><td>59.51\u2009\\pm\u20091.61</td></tr></tbody></table>", "caption": "Table 9: Results on CIFAR100 averaged over three different class orders. WA and DER w/o P results are reported from [76]. DyTox+ uses MixUp in addition of the DyTox strategy, DyTox++ further adds a sharpness-aware minimization [41].", "list_citation_info": ["[76] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[81] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shutao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[41] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In International Conference on Machine Learning (ICML), 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td rowspan=\"2\">\\#P</td><td colspan=\"2\">top-1</td><td colspan=\"2\">top-5</td></tr><tr><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td></tr><tr><th>ResNet18 joint</th><td>11.22</td><td>-</td><td>-</td><td>-</td><td>95.1</td></tr><tr><th>Transf. joint</th><td>11.00</td><td>-</td><td>79.12</td><td>-</td><td>93.48</td></tr><tr><th>WA [81]</th><td>11.22</td><td>-</td><td>-</td><td>91.00</td><td>84.10</td></tr><tr><th>DER w/o P [76]</th><td>112.27</td><td>77.18</td><td>66.70</td><td>93.23</td><td>87.52</td></tr><tr><th>DyTox</th><td>11.01</td><td>77.15</td><td>69.10</td><td>92.04</td><td>87.98</td></tr><tr><th>DyTox+</th><td>11.01</td><td>79.22</td><td>69.06</td><td>93.72</td><td>88.82</td></tr><tr><th>DyTox++</th><td>11.01</td><td>80.76</td><td>72.46</td><td>94.40</td><td>90.10</td></tr></tbody></table>", "caption": "Table 10: Results on ImageNet-100 with 10 steps of 10 new classes each. WA and DER w/o P results are reported from [76]. DyTox+ uses MixUp in addition of the DyTox strategy, DyTox++ further adds a sharpness-aware minimizer.", "list_citation_info": ["[76] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[81] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shutao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><thead><tr><th></th><th>Joint (1 step)</th><th colspan=\"2\">50 steps</th></tr><tr><th>Training</th><th>Last (\\uparrow)</th><th>Last (\\uparrow)</th><th>Forgetting (\\downarrow)</th></tr></thead><tbody><tr><th>DyTox</th><th>76.12</th><td>52.34</td><td>33.15 </td></tr><tr><th>DyTox+</th><th>78.86+1.39</th><td>59.10+4.75</td><td>24.81-1.65</td></tr><tr><th>DyTox++</th><th>78.70+0.40</th><td>59.51+1.67</td><td>26.70-1.03</td></tr></tbody></table>", "caption": "Table 11: \u201cLast\u201d accuracy and forgetting [8] on CIFAR100 for the joint (1 step, no continual) and 50 steps settings.", "list_citation_info": ["[8] Arslan Chaudhry, Puneet Dokania, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">CIFAR100</th><th colspan=\"2\">ImageNet100</th></tr><tr><th></th><th colspan=\"2\">Top-1</th><th colspan=\"2\">Top-5</th></tr><tr><th>Task decoder</th><th>Avg</th><th>Last</th><th>Avg</th><th>Last</th></tr></thead><tbody><tr><th>Residual Adapters [58]</th><td>70.00</td><td>52.38</td><td>91.25</td><td>85.00</td></tr><tr><th>FiLM [54]</th><td>69.42</td><td>54.05</td><td>89.49</td><td>81.40</td></tr><tr><th>TAB (ours)</th><td>70.20</td><td>52.34</td><td>92.04</td><td>87.98</td></tr></tbody></table>", "caption": "Table 14: Alternative task conditioner on CIFAR100 50 steps and ImageNet100 10 steps. While the simpler Residual Adapters and FiLM perform similarly to our TAB on CIFAR100, they forget significantly more on the complex ImageNet100.", "list_citation_info": ["[54] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2018.", "[58] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems (NeurIPS), 2017."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"3\">10 steps</td><td colspan=\"3\">20 steps</td><td colspan=\"3\">50 steps</td></tr><tr><th>Methods</th><td>#P</td><td>Avg</td><td>Last</td><td>#P</td><td>Avg</td><td>Last</td><td>#P</td><td>Avg</td><td>Last</td></tr><tr><th>ResNet18 Joint</th><td>11.22</td><td>-</td><td>80.41</td><td>11.22</td><td>-</td><td>81.49</td><td>11.22</td><td>-</td><td>81.74</td></tr><tr><th>Transf. Joint</th><td>10.72</td><td>-</td><td>76.12</td><td>10.72</td><td>-</td><td>76.12</td><td>10.72</td><td>-</td><td>76.12</td></tr><tr><th>iCaRL [59]</th><td>11.22</td><td>65.27\u2009\\pm\u20091.02</td><td>50.74</td><td>11.22</td><td>61.20\u2009\\pm\u20090.83</td><td>43.75</td><td>11.22</td><td>56.08\u2009\\pm\u20090.83</td><td>36.62</td></tr><tr><th>UCIR [32]</th><td>11.22</td><td>58.66\u2009\\pm\u20090.71</td><td>43.39</td><td>11.22</td><td>58.17\u2009\\pm\u20090.30</td><td>40.63</td><td>11.22</td><td>56.86\u2009\\pm\u20090.83</td><td>37.09</td></tr><tr><th>BiC [75]</th><td>11.22</td><td>68.80\u2009\\pm\u20091.20</td><td>53.54</td><td>11.22</td><td>66.48\u2009\\pm\u20090.32</td><td>47.02</td><td>11.22</td><td>62.09\u2009\\pm\u20090.85</td><td>41.04</td></tr><tr><th>WA [81]</th><td>11.22</td><td>69.46\u2009\\pm\u20090.29</td><td>53.78</td><td>11.22</td><td>67.33\u2009\\pm\u20090.15</td><td>47.31</td><td>11.22</td><td>64.32\u2009\\pm\u20090.28</td><td>42.14</td></tr><tr><th>PODNet [19]</th><td>11.22</td><td>58.03\u2009\\pm\u20091.27</td><td>41.05</td><td>11.22</td><td>53.97\u2009\\pm\u20090.85</td><td>35.02</td><td>11.22</td><td>51.19\u2009\\pm\u20091.02</td><td>32.99</td></tr><tr><th>RPSNet [56]</th><td>56.5</td><td>68.60</td><td>57.05</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DER w/o P [76]</th><td>112.27</td><td>75.36\u2009\\pm\u20090.36</td><td>65.22</td><td>224.55</td><td>74.09\u2009\\pm\u20090.33</td><td>62.48</td><td>561.39</td><td>72.41\u2009\\pm\u20090.36</td><td>59.08</td></tr><tr><th>\\text{DER}^{\\dagger} [76]</th><td>-</td><td>74.64\u2009\\pm\u20090.28</td><td>64.35</td><td>-</td><td>73.98\u2009\\pm\u20090.36</td><td>62.55</td><td>-</td><td>72.05\u2009\\pm\u20090.55</td><td>59.76</td></tr><tr><th>DyTox</th><td>10.73</td><td>73.66\u2009\\pm\u20090.02</td><td>60.67\u2009\\pm\u20090.34</td><td>10.74</td><td>72.27\u2009\\pm\u20090.18</td><td>56.32\u2009\\pm\u20090.61</td><td>10.77</td><td>70.20\u2009\\pm\u20090.16</td><td>52.34\u2009\\pm\u20090.26</td></tr><tr><th>DyTox+</th><td>10.73</td><td>75.54\u2009\\pm\u20090.10</td><td>62.06\u2009\\pm\u20090.25</td><td>10.74</td><td>75.04\u2009\\pm\u20090.11</td><td>60.03\u2009\\pm\u20090.45</td><td>10.77</td><td>74.35\u2009\\pm\u20090.05</td><td>57.09\u2009\\pm\u20090.13</td></tr><tr><th>DyTox distMem</th><td>10.73</td><td>71.50</td><td>57.76</td><td>10.74</td><td>68.86</td><td>51.47</td><td>10.77</td><td>64.82</td><td>45.61</td></tr><tr><th>DyTox+ distMem</th><td>10.73</td><td>74.10</td><td>62.34</td><td>10.74</td><td>71.62</td><td>57.43</td><td>10.77</td><td>68.90</td><td>51.09</td></tr></tbody></table>", "caption": "Table 16: Results on CIFAR100. Updated version with an erratum for Global vs. Distributed memory of the table Table 3. Gray color symbolizes results presented in the original paper version where up to 2 times the amount of rehearsal samples was used.", "list_citation_info": ["[32] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[75] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[59] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[19] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2020.", "[56] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An adaptive random path selection approach for incremental learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "[76] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[81] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shutao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"5\">ImageNet100 10 steps</td><td colspan=\"5\">ImageNet1000 10 steps</td></tr><tr><th></th><td rowspan=\"2\">\\#P</td><td colspan=\"2\">top-1</td><td colspan=\"2\">top-5</td><td rowspan=\"2\">\\#P</td><td colspan=\"2\">top-1</td><td colspan=\"2\">top-5</td></tr><tr><th>Methods</th><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td><td>Avg</td><td>Last</td></tr><tr><th>ResNet18 joint</th><td>11.22</td><td>-</td><td>-</td><td>-</td><td>95.10</td><td>11.68</td><td>-</td><td>-</td><td>-</td><td>89.27</td></tr><tr><th>Transf. joint</th><td>11.00</td><td>-</td><td>79.12</td><td>-</td><td>93.48</td><td>11.35</td><td>-</td><td>73.58</td><td>-</td><td>90.60</td></tr><tr><th>E2E [5]</th><td>11.22</td><td>-</td><td>-</td><td>89.92</td><td>80.29</td><td>11.68</td><td>-</td><td>-</td><td>72.09</td><td>52.29</td></tr><tr><th>Simple-DER [48]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>28.00</td><td>66.63</td><td>59.24</td><td>85.62</td><td>80.76</td></tr><tr><th>iCaRL [59]</th><td>11.22</td><td>-</td><td>-</td><td>83.60</td><td>63.80</td><td>11.68</td><td>38.40</td><td>22.70</td><td>63.70</td><td>44.00</td></tr><tr><th>BiC [32]</th><td>11.22</td><td>-</td><td>-</td><td>90.60</td><td>84.40</td><td>11.68</td><td>-</td><td>-</td><td>84.00</td><td>73.20</td></tr><tr><th>WA [81]</th><td>11.22</td><td>-</td><td>-</td><td>91.00</td><td>84.10</td><td>11.68</td><td>65.67</td><td>55.60</td><td>86.60</td><td>81.10</td></tr><tr><th>RPSNet [56]</th><td></td><td>-</td><td>-</td><td>87.90</td><td>74.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DER w/o P [76]</th><td>112.27</td><td>77.18</td><td>66.70</td><td>93.23</td><td>87.52</td><td>116.89</td><td>68.84</td><td>60.16</td><td>88.17</td><td>82.86</td></tr><tr><th>\\text{DER}^{\\dagger} [76]</th><td>-</td><td>76.12</td><td>66.06</td><td>92.79</td><td>88.38</td><td>-</td><td>66.73</td><td>58.62</td><td>87.08</td><td>81.89</td></tr><tr><th>DyTox</th><td>11.01</td><td>77.15</td><td>69.10</td><td>92.04</td><td>87.98</td><td>11.36</td><td>71.29</td><td>63.34</td><td>88.59</td><td>84.49</td></tr><tr><th>DyTox+</th><td>11.01</td><td>79.22</td><td>69.06</td><td>93.72</td><td>88.82</td><td>11.01</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><th>DyTox globalMem</th><td>11.01</td><td>71.85</td><td>57.94</td><td>90.72</td><td>83.52</td><td>11.36</td><td>68.14</td><td>59.75</td><td>87.03</td><td>82.93</td></tr><tr><th>DyTox+ globalMem</th><td>11.01</td><td>77.62</td><td>65.94</td><td>93.15</td><td>88.78</td><td>11.36</td><td>73.21</td><td>64.56</td><td>91.09</td><td>87.07</td></tr><tr><th>DyTox distMem</th><td>11.01</td><td>73.96</td><td>62.20</td><td>91.29</td><td>85.60</td><td>11.36</td><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>\u2014</td></tr><tr><th>DyTox+ distMem</th><td>11.01</td><td>77.15</td><td>67.70</td><td>93.17</td><td>89.42</td><td>11.36</td><td>70.88</td><td>60.00</td><td>90.53</td><td>85.25</td></tr></tbody></table>", "caption": "Table 17: Results on ImageNet-100 and ImageNet-1000 datasets. Updated version with an erratum for Global vs. Distributed memory of the table Table 2. Gray color symbolizes results presented in the original paper version where up to 4 times the amount of rehearsal samples was used.", "list_citation_info": ["[32] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[48] Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang, and Wei-Shi Zheng. Preserving earlier knowledge in continual learning with the help of all previous feature extractors. In arXiv preprint library, 2021.", "[5] Francisco M. Castro, Manuel J Mar\u00edn-Jim\u00e9nez, Nicol\u00e1s Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the IEEE European Conference on Computer Vision (ECCV), 2018.", "[59] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[56] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An adaptive random path selection approach for incremental learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.", "[76] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[81] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shutao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}]}