{"title": "Composition and style attributes guided image aesthetic assessment", "abstract": "The aesthetic quality of an image is defined as the measure or appreciation of the beauty of an image. Aesthetics is inherently a subjective property but there are certain factors that influence it such as, the semantic content of the image, the attributes describing the artistic aspect, the photographic setup used for the shot, etc. In this paper we propose a method for the automatic prediction of the aesthetics of an image that is based on the analysis of the semantic content, the artistic style and the composition of the image. The proposed network includes: a pre-trained network for semantic features extraction (the Backbone); a Multi Layer Perceptron (MLP) network that relies on the Backbone features for the prediction of image attributes (the AttributeNet); a self-adaptive Hypernetwork that exploits the attributes prior encoded into the embedding generated by the AttributeNet to predict the parameters of the target network dedicated to aesthetic estimation (the AestheticNet). Given an image, the proposed multi-network is able to predict: style and composition attributes, and aesthetic score distribution. Results on three benchmark datasets demonstrate the effectiveness of the proposed method, while the ablation study gives a better understanding of the proposed network.", "authors": ["Luigi Celona", " Marco Leonardi", " Paolo Napoletano", " Alessandro Rozza"], "pdf_url": "https://arxiv.org/abs/2111.04647", "list_table_and_caption": [{"table": "<table><tr><td>Attribute Name</td><td>AADB [41]</td><td>AVA</td><td>FlickrStyle + KU-PCP</td></tr><tr><td>Balancing elements</td><td>\u2713</td><td></td><td></td></tr><tr><td>Bright</td><td></td><td></td><td>\u2713</td></tr><tr><td>Bokeh</td><td></td><td></td><td>\u2713</td></tr><tr><td>Center</td><td></td><td></td><td>\u2713</td></tr><tr><td>Color harmony</td><td>\u2713</td><td></td><td></td></tr><tr><td>Complementary</td><td></td><td>\u2713</td><td></td></tr><tr><td>Content</td><td>\u2713</td><td></td><td></td></tr><tr><td>Curved</td><td></td><td></td><td>\u2713</td></tr><tr><td>Depth-of-Field</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td>Detailed</td><td></td><td></td><td>\u2713</td></tr><tr><td>Diagonal</td><td></td><td></td><td>\u2713</td></tr><tr><td>Duotones</td><td></td><td>\u2713</td><td></td></tr><tr><td>Ethereal</td><td></td><td></td><td>\u2713</td></tr><tr><td>Geometric</td><td></td><td></td><td>\u2713</td></tr><tr><td>Hazy</td><td></td><td></td><td>\u2713</td></tr><tr><td>HDR</td><td></td><td>\u2713</td><td>\u2713</td></tr><tr><td>Horizontal</td><td></td><td></td><td>\u2713</td></tr><tr><td>Horror</td><td></td><td></td><td>\u2713</td></tr><tr><td>Light</td><td>\u2713</td><td></td><td></td></tr><tr><td>Light on White</td><td></td><td>\u2713</td><td></td></tr><tr><td>Long Exposure</td><td></td><td>\u2713</td><td>\u2713</td></tr><tr><td>Macro</td><td></td><td>\u2713</td><td>\u2713</td></tr><tr><td>Melancholy</td><td></td><td></td><td>\u2713</td></tr><tr><td>Minimal</td><td></td><td></td><td>\u2713</td></tr><tr><td>Motion Blur</td><td>\u2713</td><td>\u2713</td><td></td></tr><tr><td>Negative Photo</td><td></td><td>\u2713</td><td></td></tr><tr><td>Noir</td><td></td><td></td><td>\u2713</td></tr><tr><td>Object</td><td>\u2713</td><td></td><td></td></tr><tr><td>Pastel</td><td></td><td></td><td>\u2713</td></tr><tr><td>Photo Grain</td><td></td><td>\u2713</td><td></td></tr><tr><td>Pattern/Repetition</td><td>\u2713</td><td></td><td>\u2713</td></tr><tr><td>Romantic</td><td></td><td></td><td>\u2713</td></tr><tr><td>Rule of Thirds</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><td>Serene</td><td></td><td></td><td>\u2713</td></tr><tr><td>Silhouettes</td><td></td><td>\u2713</td><td></td></tr><tr><td>Soft Focus</td><td></td><td>\u2713</td><td></td></tr><tr><td>Sunny</td><td></td><td></td><td>\u2713</td></tr><tr><td>Symmetry</td><td>\u2713</td><td></td><td>\u2713</td></tr><tr><td>Texture</td><td></td><td></td><td>\u2713</td></tr><tr><td>Triangle</td><td></td><td></td><td>\u2713</td></tr><tr><td>Vanishing Point</td><td></td><td>\u2713</td><td></td></tr><tr><td>Vertical</td><td></td><td></td><td>\u2713</td></tr><tr><td>Vintage</td><td></td><td></td><td>\u2713</td></tr><tr><td>Vivid Color</td><td>\u2713</td><td></td><td></td></tr></table>", "caption": "TABLE I: Image attributes available for AADB, AVA, and in our selection.", "list_citation_info": ["[41] J.-T. Lee, H.-U. Kim, C. Lee, and C.-S. Kim, \u201cPhotographic composition classification and dominant geometric element detection for outdoor scenes,\u201d Elsevier Journal of Visual Communication and Image Representation, vol. 55, pp. 91\u2013105, 2018."]}, {"table": "<table><tr><td>Method</td><td>Network architecture</td><td>MTL</td><td>Accuracy (%) \\uparrow</td><td>SROCC \\uparrow</td><td>PLCC \\uparrow</td><td>MAE \\downarrow</td><td>RMSE \\downarrow</td><td>EMD \\downarrow</td></tr><tr><td>Baseline</td><td></td><td></td><td>61.58</td><td>-0.0744</td><td>-0.0543</td><td>0.1449</td><td>0.1799</td><td>0.1407</td></tr><tr><td>Reg-Net [11]</td><td>AlexNet</td><td></td><td>\u2013</td><td>0.6782</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Malu et al.  [44]</td><td>ResNet-50</td><td>\u2713</td><td>\u2013</td><td>0.6890</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>PI-DCNN [17]</td><td>ResNet-50</td><td>\u2713</td><td>\u2013</td><td>0.7051</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Chen et al.  [45]</td><td>ResNet-50</td><td></td><td>\u2013</td><td>0.7080</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Pan et al.  [18]</td><td>ResNet-50</td><td>\u2713</td><td>\u2013</td><td>0.7041</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Reddy et al.  [38]</td><td>EfficientNet-B4</td><td>\u2713</td><td>\u2013</td><td>0.7059</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>RGNet [4]</td><td>DenseNet-121</td><td></td><td>\u2013</td><td>0.7104</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Leonardi et al.  [23]</td><td>EfficientNet-B4</td><td></td><td>79.51</td><td>0.7454</td><td>0.7479</td><td>0.1062</td><td>0.1351</td><td>\u2013</td></tr><tr><td>Proposed</td><td>EfficientNet-B4</td><td>\u2713</td><td>81.64</td><td>0.7567</td><td>0.7616</td><td>0.0832</td><td>0.1059</td><td>0.0951</td></tr></table>", "caption": "TABLE II: Comparison of the proposed method with state-of-the-art methods on the AADB dataset. The \u201c\u2013\u201d means that the result is not available. The network architecture and whether it uses Multi-Task Learning (MTL) is indicated for each method.", "list_citation_info": ["[18] B. Pan, S. Wang, and Q. Jiang, \u201cImage aesthetic assessment assisted by attributes through adversarial learning,\u201d in AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 679\u2013686.", "[45] Z. Chen, \u201cData covariance learning in aesthetic attributes assessment,\u201d Journal of Applied Mathematics and Physics, vol. 8, no. 12, pp. 2869\u20132879, 2020.", "[38] G. V. Reddy, S. Mukherjee, and M. Thakur, \u201cMeasuring photography aesthetics with deep cnns,\u201d IET Image Processing, vol. 14, no. 8, pp. 1561\u20131570, 2020.", "[17] Y. Shu, Q. Li, S. Liu, and G. Xu, \u201cLearning with privileged information for photo aesthetic assessment,\u201d Elsevier Neurocomputing, vol. 404, pp. 304\u2013316, 2020.", "[4] D. Liu, R. Puri, N. Kamath, and S. Bhattacharya, \u201cComposition-aware image aesthetics assessment,\u201d in Winter Conference on Applications of Computer Vision, 2020, pp. 3569\u20133578.", "[23] M. Leonardi, P. Napoletano, A. Rozza, and R. Schettini, \u201cModeling image aesthetics through aesthetic-related attributes,\u201d in London Imaging Meeting, vol. 2021, no. 1. Society for Imaging Science and Technology, 2021, pp. \u2013.", "[11] S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes, \u201cPhoto aesthetics ranking network with attributes and content adaptation,\u201d in European Conference on Computer Vision (ECCV), 2016.", "[44] G. Malu, R. S. Bapi, and B. Indurkhya, \u201cLearning photography aesthetics with deep cnns,\u201d arXiv preprint arXiv:1707.03981, 2017."]}, {"table": "<table><tr><td>Method</td><td>Network architecture</td><td>MTL</td><td>Accuracy (%) \\uparrow</td><td>SROCC \\uparrow</td><td>PLCC \\uparrow</td><td>MAE \\downarrow</td><td>RMSE \\downarrow</td><td>EMD \\downarrow</td></tr><tr><td>Baseline</td><td></td><td></td><td>71.28</td><td>-0.0003</td><td>-0.0021</td><td>0.6230</td><td>0.7550</td><td>0.0743</td></tr><tr><td>RAPID [12]</td><td>AlexNet</td><td></td><td>74.20</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>DMA-Net [29]</td><td>AlexNet</td><td></td><td>75.42</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>MNA-CNN [30]</td><td>VGG16</td><td></td><td>76.10</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Reg-Net [11]</td><td>AlexNet</td><td></td><td>77.33</td><td>0.5581</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>MTCNN [13]</td><td>VGG16</td><td>\u2713</td><td>78.56</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Multimodal DBM Model [28]</td><td>VGG16</td><td></td><td>78.88</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>NIMA [19]</td><td>VGG16</td><td></td><td>80.60</td><td>0.5920</td><td>0.6100</td><td>\u2013</td><td>\u2013</td><td>0.0520</td></tr><tr><td>GPF-CNN [20]</td><td>VGG16</td><td></td><td>80.70</td><td>0.6762</td><td>0.6868</td><td>0.4144</td><td>0.5347</td><td>0.0460</td></tr><tr><td>NIMA [19]</td><td>InceptionNet</td><td></td><td>81.51</td><td>0.6120</td><td>0.6360</td><td>\u2013</td><td>\u2013</td><td>0.0500</td></tr><tr><td>MLSP [16]</td><td>InceptionNet</td><td></td><td>81.68</td><td>0.7524</td><td>0.7545</td><td>0.3831</td><td>0.4943</td><td>\u2013</td></tr><tr><td>GPF-CNN [20]</td><td>InceptionNet</td><td></td><td>81.81</td><td>0.6900</td><td>0.7042</td><td>0.4072</td><td>0.5246</td><td>0.0450</td></tr><tr><td>MULTIGAP [27]</td><td>InceptionNet</td><td></td><td>82.27</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>A-Lamp [14]</td><td>VGG16</td><td></td><td>82.50</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>AFDC+SPP [21]</td><td>ResNet-50</td><td></td><td>83.24</td><td>0.6489</td><td>0.6711</td><td>\u2013</td><td>\u2013</td><td>0.0447</td></tr><tr><td>PI-DCNN [17]</td><td>ResNet-50</td><td>\u2713</td><td>\u2013</td><td>0.6578</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Pan et al.  [18]</td><td>ResNet-50</td><td>\u2713</td><td>\u2013</td><td>0.7041</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>RGNet [4]</td><td>DenseNet-121</td><td></td><td>83.59</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Proposed</td><td>EfficentNet-B4</td><td>\u2713</td><td>80.75</td><td>0.7318</td><td>0.7329</td><td>0.4011</td><td>0.5128</td><td>0.0439</td></tr></table>", "caption": "TABLE III: Comparison of the proposed method with state-of-the-art methods on the AVA dataset. In each column, the best and second-best results are marked in boldface and underlined, respectively. The \u201c\u2013\u201d means that the result is not available. The network architecture and whether it uses Multi-Task Learning (MTL) is indicated for each method.", "list_citation_info": ["[27] Y.-L. Hii, J. See, M. Kairanbay, and L.-K. Wong, \u201cMultigap: Multi-pooled inception network with text augmentation for aesthetic prediction of photographs,\u201d in International Conference on Image Processing (ICIP). IEEE, 2017, pp. 1722\u20131726.", "[19] H. Talebi and P. Milanfar, \u201cNima: Neural image assessment,\u201d IEEE Transactions on Image Processing, vol. 27, no. 8, pp. 3998\u20134011, 2018.", "[30] L. Mai, H. Jin, and F. Liu, \u201cComposition-preserving deep photo aesthetics assessment,\u201d in Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2016, pp. 497\u2013506.", "[12] X. Lu, Z. Lin, H. Jin, J. Yang, and J. Z. Wang, \u201cRapid: Rating pictorial aesthetics using deep learning,\u201d in International Conference on Multimedia. ACM, 2014, pp. 457\u2013466.", "[18] B. Pan, S. Wang, and Q. Jiang, \u201cImage aesthetic assessment assisted by attributes through adversarial learning,\u201d in AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 679\u2013686.", "[13] Y. Kao, R. He, and K. Huang, \u201cDeep aesthetic quality assessment with semantic information,\u201d IEEE Transactions on Image Processing, vol. 26, no. 3, pp. 1482\u20131495, 2017.", "[20] X. Zhang, X. Gao, W. Lu, and L. He, \u201cA gated peripheral-foveal convolutional neural network for unified image aesthetic prediction,\u201d IEEE Transactions on Multimedia, vol. 21, no. 11, pp. 2815\u20132826, 2019.", "[28] Y. Zhou, X. Lu, J. Zhang, and J. Z. Wang, \u201cJoint image and text representation for aesthetics analysis,\u201d in International Conference on Multimedia. ACM, 2016, pp. 262\u2013266.", "[21] Q. Chen, W. Zhang, N. Zhou, P. Lei, Y. Xu, Y. Zheng, and J. Fan, \u201cAdaptive fractional dilated convolution network for image aesthetics assessment,\u201d in Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 14\u2009114\u201314\u2009123.", "[16] V. Hosu, B. Goldlucke, and D. Saupe, \u201cEffective aesthetics prediction with multi-level spatially pooled features,\u201d in Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019, pp. 9375\u20139383.", "[17] Y. Shu, Q. Li, S. Liu, and G. Xu, \u201cLearning with privileged information for photo aesthetic assessment,\u201d Elsevier Neurocomputing, vol. 404, pp. 304\u2013316, 2020.", "[4] D. Liu, R. Puri, N. Kamath, and S. Bhattacharya, \u201cComposition-aware image aesthetics assessment,\u201d in Winter Conference on Applications of Computer Vision, 2020, pp. 3569\u20133578.", "[11] S. Kong, X. Shen, Z. Lin, R. Mech, and C. Fowlkes, \u201cPhoto aesthetics ranking network with attributes and content adaptation,\u201d in European Conference on Computer Vision (ECCV), 2016.", "[14] S. Ma, J. Liu, and C. Wen Chen, \u201cA-lamp: Adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment,\u201d in Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017, pp. 4535\u20134544.", "[29] X. Lu, Z. Lin, X. Shen, R. Mech, and J. Z. Wang, \u201cDeep multi-patch aggregation network for image style, aesthetics, and quality estimation,\u201d in International Conference on Computer Vision (ICCV). IEEE, 2015, pp. 990\u2013998."]}, {"table": "<table><tr><td>Method</td><td>Net. arch.</td><td>MTL</td><td>Accuracy (%) \\uparrow</td><td>SROCC \\uparrow</td><td>PLCC \\uparrow</td><td>MAE \\downarrow</td><td>RMSE \\downarrow</td><td>EMD \\downarrow</td><td>Evaluation protocol</td></tr><tr><td>Baseline</td><td></td><td></td><td>66.58 \\pm 0.30</td><td>0.0060 \\pm 0.02629</td><td>0.0053 \\pm 0.0285</td><td>0.4481 \\pm 0.0023</td><td>0.5652 \\pm 0.0023</td><td>0.0789 \\pm 0.0004</td><td>15K train, 1000 val, 1200 test*</td></tr><tr><td>GIST_SVM [22]</td><td></td><td></td><td>59.90</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>5-fold cross-validation</td></tr><tr><td>FV_SIFT_SVM [22]</td><td></td><td></td><td>60.80</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>5-fold cross-validation</td></tr><tr><td>MTCNN [13]</td><td>VGG16</td><td>\u2713</td><td>65.20</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>about 15K train, 3000 test</td></tr><tr><td>GPF-CNN [20]</td><td>VGG16</td><td></td><td>75.60</td><td>0.5217</td><td>0.5464</td><td>0.4242</td><td>0.5211</td><td>0.0700</td><td>15K train, 1000 val, 1200 test</td></tr><tr><td>Proposed</td><td>EfficinetNet-B4</td><td>\u2713</td><td>70.05\\pm0.89</td><td>0.5650\\pm0.0153</td><td>0.5698\\pm0.0141</td><td>0.3714\\pm0.0065</td><td>0.4700\\pm0.0071</td><td>0.0689\\pm0.0009</td><td>15K train, 1000 val, 1200 test*</td></tr><tr><td colspan=\"7\">* Average and standard deviation on the 10 iterations of train-val-test splits.</td><td></td><td></td><td></td></tr></table>", "caption": "TABLE IV: Comparison of the proposed method with state-of-the-art methods on the Photo.net dataset. In each column, the best and second-best results are marked in boldface and underlined, respectively. The \u201c\u2013\u201d means that the result is not available. The network architecture and whether it uses Multi-Task Learning (MTL) is indicated for each method.", "list_citation_info": ["[13] Y. Kao, R. He, and K. Huang, \u201cDeep aesthetic quality assessment with semantic information,\u201d IEEE Transactions on Image Processing, vol. 26, no. 3, pp. 1482\u20131495, 2017.", "[22] L. Marchesotti, F. Perronnin, D. Larlus, and G. Csurka, \u201cAssessing the aesthetic quality of photographs using generic image descriptors,\u201d in International Conference on Computer Vision (ICCV). IEEE, 2011, pp. 1784\u20131791.", "[20] X. Zhang, X. Gao, W. Lu, and L. He, \u201cA gated peripheral-foveal convolutional neural network for unified image aesthetic prediction,\u201d IEEE Transactions on Multimedia, vol. 21, no. 11, pp. 2815\u20132826, 2019."]}]}