{"title": "Clip4caption: Clip for video caption", "abstract": "This report describes our solution to the VALUE Challenge 2021 in the captioning task. Our solution, named CLIP4Caption++, is built on X-Linear/X-Transformer, which is an advanced model with encoder-decoder architecture. We make the following improvements on the proposed CLIP4Caption++: We employ an advanced encoder-decoder model architecture X-Transformer as our main framework and make the following improvements: 1) we utilize three strong pre-trained CLIP models to extract the text-related appearance visual features. 2) we adopt the TSN sampling strategy for data enhancement. 3) we involve the video subtitle information to provide richer semantic information. 3) we introduce the subtitle information, which fuses with the visual features as guidance. 4) we design word-level and sentence-level ensemble strategies. Our proposed method achieves 86.5, 148.4, 64.5 CIDEr scores on VATEX, YC2C, and TVC datasets, respectively, which shows the superior performance of our proposed CLIP4Caption++ on all three datasets.", "authors": ["Mingkang Tang", " Zhanyu Wang", " Zhaoyang Zeng", " Fengyun Rao", " Dian Li"], "pdf_url": "https://arxiv.org/abs/2110.05204", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Datasets</th><th rowspan=\"2\">Model</th><th colspan=\"3\">Public Test</th><th>Private Test</th></tr><tr><th>BLEU-4</th><th>ROUGE-L</th><th>CIDEr</th><th>CIDEr</th></tr></thead><tbody><tr><td rowspan=\"2\">TVC [4]</td><td>single model</td><td>13.43</td><td>35.73</td><td>61.20</td><td>59.41</td></tr><tr><td>ensemble</td><td>15.03</td><td>36.86</td><td>66.02</td><td>64.49</td></tr><tr><td rowspan=\"2\">YC2C [14]</td><td>single model</td><td>13.03</td><td>39.78</td><td>136.01</td><td>133.98</td></tr><tr><td>ensemble</td><td>13.60</td><td>41.43</td><td>148.91</td><td>148.39</td></tr><tr><td rowspan=\"2\">VATEX [13]</td><td>single model</td><td>38.66</td><td>53.61</td><td>81.95</td><td>83.67</td></tr><tr><td>ensemble</td><td>40.59</td><td>54.45</td><td>85.71</td><td>86.53</td></tr></tbody></table>", "caption": "Table 1: Best single model and multiple model ensemble results on the TVC, YC2C and VATEX test set of public and private.", "list_citation_info": ["[13] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4581\u20134591, 2019.", "[14] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.", "[4] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr: A large-scale dataset for video-subtitle moment retrieval. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXI 16, pages 447\u2013463. Springer, 2020."]}]}