{"title": "Simple copy-paste is a strong data augmentation method for instance segmentation", "abstract": "Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation ([13, 12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.", "authors": ["Golnaz Ghiasi", " Yin Cui", " Aravind Srinivas", " Rui Qian", " Tsung-Yi Lin", " Ekin D. Cubuk", " Quoc V. Le", " Barret Zoph"], "pdf_url": "https://arxiv.org/abs/2012.07177", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td>FLOPs</td><td># Params</td><td>AP{}_{\\text{val}}</td><td>AP{}_{\\text{test-dev}}</td><td>Mask AP{}_{\\text{val}}</td><td>Mask AP{}_{\\text{test-dev}}</td></tr><tr><th>SpineNet-190 (1536) [11]</th><td>2076B</td><td>176M</td><td>52.2</td><td>52.5</td><td>46.1</td><td>46.3</td></tr><tr><th>DetectoRS ResNeXt-101-64x4d  [43]</th><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>55.7{}^{\\dagger}</td><td>\u2014</td><td>48.5 {}^{\\dagger}</td></tr><tr><th>SpineNet-190 (1280) [11]</th><td>1885B</td><td>164M</td><td>52.6</td><td>52.8</td><td>\u2014</td><td>\u2014</td></tr><tr><th>SpineNet-190 (1280) w/ self-training [72]</th><td>1885B</td><td>164M</td><td>54.2</td><td>54.3</td><td>\u2014</td><td>\u2014</td></tr><tr><th>EfficientDet-D7x (1536) [57]</th><td>410B</td><td>77M</td><td>54.4</td><td>55.1</td><td>\u2014</td><td>\u2014</td></tr><tr><th>YOLOv4-P7 (1536) [61]</th><td>\u2014</td><td>\u2014</td><td>\u2014</td><td>55.8{}^{\\dagger}</td><td>\u2014</td><td>\u2014</td></tr><tr><th>Cascade Eff-B7 NAS-FPN (1280)</th><td>1440B</td><td>185M</td><td>54.5</td><td>54.8</td><td>46.8</td><td>46.9</td></tr><tr><th>w/ Copy-Paste</th><td>1440B</td><td>185M</td><td>(+1.4) 55.9</td><td>(+1.2) 56.0</td><td>(+0.4) 47.2</td><td>(+0.5) 47.4</td></tr><tr><th>w/ self-training Copy-Paste</th><td>1440B</td><td>185M</td><td>(+2.5) 57.0</td><td>(+2.5) 57.3</td><td>(+2.1) 48.9</td><td>(+2.2) 49.1</td></tr></tbody></table>", "caption": "Table 4: Comparison with the state-of-the-art models on COCO object detection and instance segmentation. Parentheses next to the model name denote the input image size. {}^{\\dagger} indicates results with test time augmentation.", "list_citation_info": ["[61] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Scaled-yolov4: Scaling cross stage partial network. arXiv preprint arXiv:2011.08036, 2020.", "[72] Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning data augmentation strategies for object detection. In ECCV, 2020.", "[11] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song. Spinenet: Learning scale-permuted backbone for recognition and localization. In CVPR, 2020.", "[43] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution. arXiv preprint arXiv:2006.02334, 2020.", "[57] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In CVPR, 2020."]}, {"table": "<table><thead><tr><th>Model</th><th>AP50</th><th>AP</th></tr></thead><tbody><tr><th>RefineDet512+ [68]</th><td>83.8</td><td>-</td></tr><tr><th>SNIPER [52]</th><td>86.9</td><td>-</td></tr><tr><th>Cascade Eff-B7 NAS-FPN</th><td>88.6</td><td>75.0</td></tr><tr><th>w/ Copy-Paste pre-training</th><td>(+0.7) 89.3</td><td>(+1.5) 76.5</td></tr></tbody></table>", "caption": "Table 5: PASCAL VOC 2007 detection result on test set. We present results of our EfficientNet-B7 NAS-FPN model pre-trained with and without Copy-Paste on COCO.", "list_citation_info": ["[68] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and Stan Z Li. Single-shot refinement neural network for object detection. In CVPR, 2018.", "[52] Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper: Efficient multi-scale training. In NeurIPS, 2018."]}, {"table": "<table><thead><tr><th>Model</th><th>mIOU</th></tr></thead><tbody><tr><th>DeepLabv3+ {}^{\\dagger} [4]</th><td>84.6</td></tr><tr><th>ExFuse {}^{\\dagger} [70]</th><td>85.8</td></tr><tr><th>Eff-B7 [73]</th><td>85.2</td></tr><tr><th>Eff-L2 [73]</th><td>88.7</td></tr><tr><th>Eff-B7 NAS-FPN</th><td>83.9</td></tr><tr><th>w/ Copy-Paste pre-training</th><td>(+2.7) 86.6</td></tr></tbody></table>", "caption": "Table 6: PASCAL VOC 2012 semantic segmentation results on val set. We present results of our EfficientNet-B7 NAS-FPN model pre-trained with and without Copy-Paste on COCO. {}^{\\dagger} indicates multi-scale/flip ensembling inference.", "list_citation_info": ["[4] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.", "[73] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, and Quoc V. Le. Rethinking pre-training and self-training. In NeurIPS, 2020.", "[70] Zhenli Zhang, Xiangyu Zhang, Chao Peng, Xiangyang Xue, and Jian Sun. Exfuse: Enhancing feature fusion for semantic segmentation. In ECCV, 2018."]}, {"table": "<table><tbody><tr><th></th><td>Mask AP</td><td>Mask AP{}_{\\text{r}}</td><td>Mask AP{}_{\\text{c}}</td><td>Mask AP{}_{\\text{f}}</td><td>Box AP</td></tr><tr><th>cRT (ResNeXt-101-32\\times8d) [33]</th><td>27.2</td><td>19.6</td><td>26.0</td><td>31.9</td><td>\u2014</td></tr><tr><th>LVIS Challenge 2020 Winner{}^{\\dagger} [55]</th><td>38.8</td><td>28.5</td><td>39.5</td><td>42.7</td><td>41.1</td></tr><tr><th>ResNet-50 FPN (1024)</th><td>30.3</td><td>22.2</td><td>29.5</td><td>34.7</td><td>31.5</td></tr><tr><th>w/ Copy-Paste</th><td>(+2.0) 32.3</td><td>(+4.3) 26.5</td><td>(+2.3) 31.8</td><td>(+0.6) 35.3</td><td>(+2.8) 34.3</td></tr><tr><th>ResNet-101 FPN (1024)</th><td>31.9</td><td>24.7</td><td>30.5</td><td>36.3</td><td>33.3</td></tr><tr><th>w/ Copy-Paste</th><td>(+2.1) 34.0</td><td>(+2.7) 27.4</td><td>(+3.4) 33.9</td><td>(+0.9) 37.2</td><td>(+3.1) 36.4</td></tr><tr><th>EfficientNet-B7 FPN (1024)</th><td>33.7</td><td>26.4</td><td>33.1</td><td>37.6</td><td>35.5</td></tr><tr><th>w/ Copy-Paste</th><td>(+2.3) 36.0</td><td>(+3.3) 29.7</td><td>(+2.7) 35.8</td><td>(+1.3) 38.9</td><td>(+3.7) 39.2</td></tr><tr><th>EfficientNet-B7 NAS-FPN (1280)</th><td>34.7</td><td>26.0</td><td>33.4</td><td>39.8</td><td>37.2</td></tr><tr><th>w/ Copy-Paste</th><td>(+3.4) 38.1</td><td>(+6.1) 32.1</td><td>(+3.7) 37.1</td><td>(+2.1) 41.9</td><td>(+4.4) 41.6</td></tr></tbody></table>", "caption": "Table 7: Comparison with the state-of-the-art models on LVIS v1.0 object detection and instance segmentation. Parentheses next to our models denote the input image size. {}^{\\dagger} We report the 2020 winning entry\u2019s result without test-time augmentation.", "list_citation_info": ["[33] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. In ICLR, 2020.", "[55] Jingru Tan, Gang Zhang, Hanming Deng, Changbao Wang, Lewei Lu, Quanquan Li, and Jifeng Dai. 1st place solution of lvis challenge 2020: A good box is not a guarantee of a good mask. arXiv preprint arXiv:2009.01559, 2020."]}]}