{"title": "Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification", "abstract": "We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28 (3D) with the corresponding classification labels so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various dataset scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression, and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision, and machine learning. We benchmark several baseline methods on MedMNIST v2, including 2D / 3D neural networks and open-source / commercial AutoML tools. The data and code are publicly available at https://medmnist.com/.", "authors": ["Jiancheng Yang", " Rui Shi", " Donglai Wei", " Zequan Liu", " Lin Zhao", " Bilian Ke", " Hanspeter Pfister", " Bingbing Ni"], "pdf_url": "https://arxiv.org/abs/2110.14795", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td>Visual Domain Decathlon [4]</td><td>Medical Segmentation Decathlon [5]</td><td>MedMNIST v1 [9]</td><td>MedMNIST v2</td></tr><tr><th>Domain</th><td>Natural</td><td>Medical</td><td>Medical</td><td>Medical</td></tr><tr><th>Task</th><td>Classification</td><td>Segmentation</td><td>Classification</td><td>Classification</td></tr><tr><th>Datasets</th><td>10</td><td>10</td><td>10</td><td>18</td></tr><tr><th>2D / 3D</th><td>2D</td><td>3D</td><td>2D</td><td>2D &amp; 3D</td></tr><tr><th>Image Size</th><td>Variable (\\approx 72^{2})</td><td>Variable (\\approx(30-300)^{3})</td><td>Fixed (28^{2})</td><td>Fixed (28^{2} &amp; 28^{3})</td></tr></tbody></table>", "caption": "Table 1: A comparison of MedMNIST v2 and other \u201cdecathlon\u201d datasets.", "list_citation_info": ["[4] Rebuffi, S.-A., Bilen, H. & Vedaldi, A. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, 506\u2013516 (2017).", "[9] Yang, J., Shi, R. & Ni, B. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In International Symposium on Biomedical Imaging, 191\u2013195 (2021).", "[5] Simpson, A. L. et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms. Preprint at https://arxiv.org/abs/1902.09063 (2019)."]}, {"table": "<table><tbody><tr><th>Name</th><td>Source</td><td>Data Modality</td><td>Task (# Classes / Labels)</td><td># Samples</td><td># Training / Validation / Test</td></tr><tr><th>MedMNIST2D</th><td></td><td></td><td></td><td></td><td></td></tr><tr><th>PathMNIST</th><td>Kather et al. [16, 17]</td><td>Colon Pathology</td><td>MC (9)</td><td>107,180</td><td>89,996 / 10,004 / 7,180</td></tr><tr><th>ChestMNIST</th><td>Wang et al. [18]</td><td>Chest X-Ray</td><td>ML (14) BC (2)</td><td>112,120</td><td>78,468 / 11,219 / 22,433</td></tr><tr><th>DermaMNIST</th><td>Tschandl et al. [19, 20], Codella et al. [21]</td><td>Dermatoscope</td><td>MC (7)</td><td>10,015</td><td>7,007 / 1,003 / 2,005</td></tr><tr><th>OCTMNIST</th><td>Kermany et al. [22, 23]</td><td>Retinal OCT</td><td>MC (4)</td><td>109,309</td><td>97,477 / 10,832 / 1,000</td></tr><tr><th>PneumoniaMNIST</th><td>Kermany et al. [22, 23]</td><td>Chest X-Ray</td><td>BC (2)</td><td>5,856</td><td>4,708 / 524 / 624</td></tr><tr><th>RetinaMNIST</th><td>DeepDRiD Team[24]</td><td>Fundus Camera</td><td>OR (5)</td><td>1,600</td><td>1,080 / 120 / 400</td></tr><tr><th>BreastMNIST</th><td>Al-Dhabyani et al. [25]</td><td>Breast Ultrasound</td><td>BC (2)</td><td>780</td><td>546 / 78 / 156</td></tr><tr><th>BloodMNIST</th><td>Acevedo et al. [26, 27]</td><td>Blood Cell Microscope</td><td>MC (8)</td><td>17,092</td><td>11,959 / 1,712 / 3,421</td></tr><tr><th>TissueMNIST</th><td>Ljosa et al. [28]</td><td>Kidney Cortex Microscope</td><td>MC (8)</td><td>236,386</td><td>165,466 / 23,640 / 47,280</td></tr><tr><th>OrganAMNIST</th><td>Bilic et al. [29], Xu et al. [30]</td><td>Abdominal CT</td><td>MC (11)</td><td>58,850</td><td>34,581 / 6,491 / 17,778</td></tr><tr><th>OrganCMNIST</th><td>Bilic et al. [29], Xu et al. [30]</td><td>Abdominal CT</td><td>MC (11)</td><td>23,660</td><td>13,000 / 2,392 / 8,268</td></tr><tr><th>OrganSMNIST</th><td>Bilic et al. [29], Xu et al. [30]</td><td>Abdominal CT</td><td>MC (11)</td><td>25,221</td><td>13,940 / 2,452 / 8,829</td></tr><tr><th>MedMNIST3D</th><td></td><td></td><td></td><td></td><td></td></tr><tr><th>OrganMNIST3D</th><td>Bilic et al. [29], Xu et al. [30]</td><td>Abdominal CT</td><td>MC (11)</td><td>1,743</td><td>972 / 161 / 610</td></tr><tr><th>NoduleMNIST3D</th><td>Armato et al. [31]</td><td>Chest CT</td><td>BC (2)</td><td>1,633</td><td>1,158 / 165 / 310</td></tr><tr><th>AdrenalMNIST3D</th><td>New</td><td>Shape from Abdominal CT</td><td>BC (2)</td><td>1,584</td><td>1,188 / 98 / 298</td></tr><tr><th>FractureMNIST3D</th><td>Jin et al. [32]</td><td>Chest CT</td><td>MC (3)</td><td>1,370</td><td>1,027 / 103 / 240</td></tr><tr><th>VesselMNIST3D</th><td>Yang et al. [33]</td><td>Shape from Brain MRA</td><td>BC (2)</td><td>1,909</td><td>1,335 / 192 / 382</td></tr><tr><th>SynapseMNIST3D</th><td>New</td><td>Electron Microscope</td><td>BC (2)</td><td>1,759</td><td>1,230 / 177 / 352</td></tr></tbody></table>", "caption": "Table 2: Data summary of MedMNIST v2 dataset, including data source, data modality, type of the classification task together with the number of classes for multi-class or that of labels for multi-label, number of samples in total and in each data split (training/validation/test). Upper: MedMNIST2D, 12 datasets of 2D images. Lower: MedMNIST3D, 6 datasets of 3D images. MC: Multi-Class. BC: Binary-Class. ML: Multi-Label. OR: Ordinal Regression.", "list_citation_info": ["[30] Xu, X., Zhou, F. et al. Efficient multiple organ localization in ct image using 3d region proposal network. \\JournalTitleIEEE Transactions on Medical Imaging 38, 1885\u20131898 (2019).", "[16] Kather, J. N. et al. Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study. \\JournalTitlePLOS Medicine 16, 1\u201322, https://doi.org/10.1371/journal.pmed.1002730 (2019).", "[18] Wang, X. et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Conference on Computer Vision and Pattern Recognition, 3462\u20133471 (2017).", "[25] Al-Dhabyani, W., Gomaa, M., Khaled, H. & Fahmy, A. Dataset of breast ultrasound images. \\JournalTitleData in Brief 28, 104863, https://doi.org/10.1016/j.dib.2019.104863 (2020).", "[32] Jin, L. et al. Deep-learning-assisted detection and segmentation of rib fractures from ct scans: Development and validation of fracnet. \\JournalTitleEBioMedicine 62, 103106, https://doi.org/10.1016/j.ebiom.2020.103106 (2020).", "[26] Acevedo, A. et al. A dataset of microscopic peripheral blood cell images for development of automatic recognition systems. \\JournalTitleData in Brief 30, 105474, https://doi.org/10.1016/j.dib.2020.105474 (2020).", "[31] Armato III, S. G. et al. The lung image database consortium (lidc) and image database resource initiative (idri): A completed reference database of lung nodules on ct scans. \\JournalTitleMedical Physics 38, 915\u2013931, https://doi.org/10.1118/1.3528204 (2011). https://aapm.onlinelibrary.wiley.com/doi/pdf/10.1118/1.3528204.", "[24] DeepDRiD. The 2nd diabetic retinopathy \u2013 grading and image quality estimation challenge. https://isbi.deepdr.org/data.html (2020).", "[28] Ljosa, V., Sokolnicki, K. L. & Carpenter, A. E. Annotated high-throughput microscopy image sets for validation. \\JournalTitleNature methods 9, 637\u2013637 (2012).", "[19] Tschandl, P., Rosendahl, C. & Kittler, H. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. \\JournalTitleScientific data 5, 180161 (2018).", "[21] Codella, N. et al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic). Preprint at https://arxiv.org/abs/1902.03368v2 (2019).", "[22] Kermany, D. S. et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. \\JournalTitleCell 172, 1122 \u2013 1131.e9, https://doi.org/10.1016/j.cell.2018.02.010 (2018).", "[29] Bilic, P. et al. The liver tumor segmentation benchmark (lits). Preprint at https://arxiv.org/abs/1901.04056 (2019).", "[33] Yang, X., Xia, D., Kin, T. & Igarashi, T. Intra: 3d intracranial aneurysm dataset for deep learning. In Conference on Computer Vision and Pattern Recognition (2020)."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Methods</th><th colspan=\"2\">PathMNIST</th><th colspan=\"2\">ChestMNIST</th><th colspan=\"2\">DermaMNIST</th><th colspan=\"2\">OCTMNIST</th><th colspan=\"2\">PneumoniaMNIST</th><th colspan=\"2\">RetinaMNIST</th></tr><tr><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th></tr></thead><tbody><tr><th>ResNet-18 (28) [10]</th><td>0.983</td><td>0.907</td><td>0.768</td><td>0.947</td><td>0.917</td><td>0.735</td><td>0.943</td><td>0.743</td><td>0.944</td><td>0.854</td><td>0.717</td><td>0.524</td></tr><tr><th>ResNet-18 (224) [10]</th><td>0.989</td><td>0.909</td><td>0.773</td><td>0.947</td><td>0.920</td><td>0.754</td><td>0.958</td><td>0.763</td><td>0.956</td><td>0.864</td><td>0.710</td><td>0.493</td></tr><tr><th>ResNet-50 (28) [10]</th><td>0.990</td><td>0.911</td><td>0.769</td><td>0.947</td><td>0.913</td><td>0.735</td><td>0.952</td><td>0.762</td><td>0.948</td><td>0.854</td><td>0.726</td><td>0.528</td></tr><tr><th>ResNet-50 (224) [10]</th><td>0.989</td><td>0.892</td><td>0.773</td><td>0.948</td><td>0.912</td><td>0.731</td><td>0.958</td><td>0.776</td><td>0.962</td><td>0.884</td><td>0.716</td><td>0.511</td></tr><tr><th>auto-sklearn [11]</th><td>0.934</td><td>0.716</td><td>0.649</td><td>0.779</td><td>0.902</td><td>0.719</td><td>0.887</td><td>0.601</td><td>0.942</td><td>0.855</td><td>0.690</td><td>0.515</td></tr><tr><th>AutoKeras [12]</th><td>0.959</td><td>0.834</td><td>0.742</td><td>0.937</td><td>0.915</td><td>0.749</td><td>0.955</td><td>0.763</td><td>0.947</td><td>0.878</td><td>0.719</td><td>0.503</td></tr><tr><th>Google AutoML Vision</th><td>0.944</td><td>0.728</td><td>0.778</td><td>0.948</td><td>0.914</td><td>0.768</td><td>0.963</td><td>0.771</td><td>0.991</td><td>0.946</td><td>0.750</td><td>0.531</td></tr><tr><th rowspan=\"2\">Methods</th><th colspan=\"2\">BreastMNIST</th><th colspan=\"2\">BloodMNIST</th><th colspan=\"2\">TissueMNIST</th><th colspan=\"2\">OrganAMNIST</th><th colspan=\"2\">OrganCMNIST</th><th colspan=\"2\">OrganSMNIST</th></tr><tr><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th></tr><tr><th>ResNet-18 (28) [10]</th><td>0.901</td><td>0.863</td><td>0.998</td><td>0.958</td><td>0.930</td><td>0.676</td><td>0.997</td><td>0.935</td><td>0.992</td><td>0.900</td><td>0.972</td><td>0.782</td></tr><tr><th>ResNet-18 (224) [10]</th><td>0.891</td><td>0.833</td><td>0.998</td><td>0.963</td><td>0.933</td><td>0.681</td><td>0.998</td><td>0.951</td><td>0.994</td><td>0.920</td><td>0.974</td><td>0.778</td></tr><tr><th>ResNet-50 (28) [10]</th><td>0.857</td><td>0.812</td><td>0.997</td><td>0.956</td><td>0.931</td><td>0.680</td><td>0.997</td><td>0.935</td><td>0.992</td><td>0.905</td><td>0.972</td><td>0.770</td></tr><tr><th>ResNet-50 (224) [10]</th><td>0.866</td><td>0.842</td><td>0.997</td><td>0.950</td><td>0.932</td><td>0.680</td><td>0.998</td><td>0.947</td><td>0.993</td><td>0.911</td><td>0.975</td><td>0.785</td></tr><tr><th>auto-sklearn [11]</th><td>0.836</td><td>0.803</td><td>0.984</td><td>0.878</td><td>0.828</td><td>0.532</td><td>0.963</td><td>0.762</td><td>0.976</td><td>0.829</td><td>0.945</td><td>0.672</td></tr><tr><th>AutoKeras [12]</th><td>0.871</td><td>0.831</td><td>0.998</td><td>0.961</td><td>0.941</td><td>0.703</td><td>0.994</td><td>0.905</td><td>0.990</td><td>0.879</td><td>0.974</td><td>0.813</td></tr><tr><th>Google AutoML Vision</th><td>0.919</td><td>0.861</td><td>0.998</td><td>0.966</td><td>0.924</td><td>0.673</td><td>0.990</td><td>0.886</td><td>0.988</td><td>0.877</td><td>0.964</td><td>0.749</td></tr></tbody></table>", "caption": "Table 3: Benchmark on each dataset of MedMNIST2D in metrics of AUC and ACC.", "list_citation_info": ["[11] Feurer, M. et al. Auto-sklearn: efficient and robust automated machine learning. In Automated Machine Learning, 113\u2013134 (Springer, Cham, 2019).", "[12] Jin, H., Song, Q. & Hu, X. Auto-keras: An efficient neural architecture search system. In Conference on Knowledge Discovery and Data Mining, 1946\u20131956 (ACM, 2019).", "[10] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 770\u2013778 (2016)."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Methods</th><th colspan=\"2\">OrganMNIST3D</th><th colspan=\"2\">NoduleMNIST3D</th><th colspan=\"2\">FractureMNIST3D</th><th colspan=\"2\">AdrenalMNIST3D</th><th colspan=\"2\">VesselMNIST3D</th><th colspan=\"2\">SynapseMNIST3D</th></tr><tr><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th><th>AUC</th><th>ACC</th></tr></thead><tbody><tr><th>ResNet-18 [10]+2.5D</th><td>0.977</td><td>0.788</td><td>0.838</td><td>0.835</td><td>0.587</td><td>0.451</td><td>0.718</td><td>0.772</td><td>0.748</td><td>0.846</td><td>0.634</td><td>0.696</td></tr><tr><th>ResNet-18 [10]+3D</th><td>0.996</td><td>0.907</td><td>0.863</td><td>0.844</td><td>0.712</td><td>0.508</td><td>0.827</td><td>0.721</td><td>0.874</td><td>0.877</td><td>0.820</td><td>0.745</td></tr><tr><th>ResNet-18 [10]+ACS [41]</th><td>0.994</td><td>0.900</td><td>0.873</td><td>0.847</td><td>0.714</td><td>0.497</td><td>0.839</td><td>0.754</td><td>0.930</td><td>0.928</td><td>0.705</td><td>0.722</td></tr><tr><th>ResNet-50 [10]+2.5D</th><td>0.974</td><td>0.769</td><td>0.835</td><td>0.848</td><td>0.552</td><td>0.397</td><td>0.732</td><td>0.763</td><td>0.751</td><td>0.877</td><td>0.669</td><td>0.735</td></tr><tr><th>ResNet-50 [10]+3D</th><td>0.994</td><td>0.883</td><td>0.875</td><td>0.847</td><td>0.725</td><td>0.494</td><td>0.828</td><td>0.745</td><td>0.907</td><td>0.918</td><td>0.851</td><td>0.795</td></tr><tr><th>ResNet-50 [10]+ACS [41]</th><td>0.994</td><td>0.889</td><td>0.886</td><td>0.841</td><td>0.750</td><td>0.517</td><td>0.828</td><td>0.758</td><td>0.912</td><td>0.858</td><td>0.719</td><td>0.709</td></tr><tr><th>auto-sklearn [11]</th><td>0.977</td><td>0.814</td><td>0.914</td><td>0.874</td><td>0.628</td><td>0.453</td><td>0.828</td><td>0.802</td><td>0.910</td><td>0.915</td><td>0.631</td><td>0.730</td></tr><tr><th>AutoKeras [12]</th><td>0.979</td><td>0.804</td><td>0.844</td><td>0.834</td><td>0.642</td><td>0.458</td><td>0.804</td><td>0.705</td><td>0.773</td><td>0.894</td><td>0.538</td><td>0.724</td></tr></tbody></table>", "caption": "Table 4: Benchmark on each dataset of MedMNIST3D in metrics of AUC and ACC.", "list_citation_info": ["[11] Feurer, M. et al. Auto-sklearn: efficient and robust automated machine learning. In Automated Machine Learning, 113\u2013134 (Springer, Cham, 2019).", "[41] Yang, J. et al. Reinventing 2d convolutions for 3d images. \\JournalTitleIEEE Journal of Biomedical and Health Informatics 1\u20131, https://doi.org/10.1109/JBHI.2021.3049452 (2021).", "[12] Jin, H., Song, Q. & Hu, X. Auto-keras: An efficient neural architecture search system. In Conference on Knowledge Discovery and Data Mining, 1946\u20131956 (ACM, 2019).", "[10] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 770\u2013778 (2016)."]}, {"table": "<table><thead><tr><th>Methods</th><th>AVG AUC</th><th>AVG ACC</th></tr></thead><tbody><tr><th>ResNet-18 (28) [10]</th><td>0.922</td><td>0.819</td></tr><tr><th>ResNet-18 (224) [10]</th><td>0.925</td><td>0.821</td></tr><tr><th>ResNet-50 (28) [10]</th><td>0.920</td><td>0.816</td></tr><tr><th>ResNet-50 (224) [10]</th><td>0.923</td><td>0.821</td></tr><tr><th>auto-sklearn [11]</th><td>0.878</td><td>0.722</td></tr><tr><th>AutoKeras [12]</th><td>0.917</td><td>0.813</td></tr><tr><th>Google AutoML Vision</th><td>0.927</td><td>0.809</td></tr></tbody></table>", "caption": "Table 5: Average performance of MedMNIST2D in metrics of average AUC and average ACC over all 2D datasets.", "list_citation_info": ["[11] Feurer, M. et al. Auto-sklearn: efficient and robust automated machine learning. In Automated Machine Learning, 113\u2013134 (Springer, Cham, 2019).", "[12] Jin, H., Song, Q. & Hu, X. Auto-keras: An efficient neural architecture search system. In Conference on Knowledge Discovery and Data Mining, 1946\u20131956 (ACM, 2019).", "[10] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 770\u2013778 (2016)."]}, {"table": "<table><thead><tr><th>Methods</th><th>AVG AUC</th><th>AVG ACC</th></tr></thead><tbody><tr><th>ResNet-18 [10]+2.5D</th><td>0.750</td><td>0.731</td></tr><tr><th>ResNet-18 [10]+3D</th><td>0.849</td><td>0.767</td></tr><tr><th>ResNet-18 [10]+ACS [41]</th><td>0.842</td><td>0.775</td></tr><tr><th>ResNet-50 [10]+2.5D</th><td>0.752</td><td>0.732</td></tr><tr><th>ResNet-50 [10]+3D</th><td>0.863</td><td>0.780</td></tr><tr><th>ResNet-50 [10]+ACS [41]</th><td>0.848</td><td>0.762</td></tr><tr><th>auto-sklearn [11]</th><td>0.815</td><td>0.765</td></tr><tr><th>AutoKeras [12]</th><td>0.763</td><td>0.737</td></tr></tbody></table>", "caption": "Table 6: Average performance of MedMNIST3D in metrics of average AUC and average ACC over all 3D datasets.", "list_citation_info": ["[11] Feurer, M. et al. Auto-sklearn: efficient and robust automated machine learning. In Automated Machine Learning, 113\u2013134 (Springer, Cham, 2019).", "[41] Yang, J. et al. Reinventing 2d convolutions for 3d images. \\JournalTitleIEEE Journal of Biomedical and Health Informatics 1\u20131, https://doi.org/10.1109/JBHI.2021.3049452 (2021).", "[12] Jin, H., Song, Q. & Hu, X. Auto-keras: An efficient neural architecture search system. In Conference on Knowledge Discovery and Data Mining, 1946\u20131956 (ACM, 2019).", "[10] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 770\u2013778 (2016)."]}, {"table": "<table><tbody><tr><td>Methods</td><td>AUC</td><td>ACC</td></tr><tr><td>2D-Input ResNet-18</td><td></td><td></td></tr><tr><td>Trained with OrganAMNIST</td><td>0.995</td><td>0.907</td></tr><tr><td>Trained with axial central slices of OrganMNIST3D</td><td>0.995</td><td>0.916</td></tr><tr><td>Trained with OrganCMNIST</td><td>0.991</td><td>0.877</td></tr><tr><td>Trained with coronal central slices of OrganMNIST3D</td><td>0.992</td><td>0.890</td></tr><tr><td>Trained with OrganSMNIST</td><td>0.959</td><td>0.697</td></tr><tr><td>Trained with sagittal central slices of OrganMNIST3D</td><td>0.963</td><td>0.701</td></tr><tr><td>3D-Input ResNet-18</td><td></td><td></td></tr><tr><td>2.5D trained with OrganMNIST3D</td><td>0.977</td><td>0.788</td></tr><tr><td>3D trained with OrganMNIST3D</td><td>0.996</td><td>0.907</td></tr><tr><td>ACS trained with OrganMNIST3D</td><td>0.994</td><td>0.900</td></tr></tbody></table>", "caption": "Table 7: Model performance on OrganMNIST3D test set in various settings, including (upper) 2D-input ResNet-18 [10] trained with Organ{A,C,S}MNIST and axial / coronal / sagittal central slices of OrganMNIST3D, and (lower) 3D-input ResNet-18 with 2.5D / 3D / ACS [41] convolutions, trained with OrganMNIST3D (same as Table 4).", "list_citation_info": ["[41] Yang, J. et al. Reinventing 2d convolutions for 3d images. \\JournalTitleIEEE Journal of Biomedical and Health Informatics 1\u20131, https://doi.org/10.1109/JBHI.2021.3049452 (2021).", "[10] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, 770\u2013778 (2016)."]}]}