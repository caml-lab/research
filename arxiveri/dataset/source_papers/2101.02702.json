{"title": "Trackformer: Multi-object tracking with transformers", "abstract": "The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17 and MOT20) and segmentation (MOTS20). The code is available at https://github.com/timmeinhardt/trackformer .", "authors": ["Tim Meinhardt", " Alexander Kirillov", " Laura Leal-Taixe", " Christoph Feichtenhofer"], "pdf_url": "https://arxiv.org/abs/2101.02702", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td>Method</td><td>Data</td><td>FPS \\uparrow</td><td>MOTA \\uparrow</td><td>IDF1 \\uparrow</td><td>MT \\uparrow</td><td>ML \\downarrow</td><td>FP \\downarrow</td><td>FN \\downarrow</td><td>ID Sw. \\downarrow</td></tr><tr><td colspan=\"11\">MOT17 [30] - Public</td></tr><tr><td rowspan=\"6\"><p>Offline</p></td><td>jCC [22]</td><td>\u2013</td><td>\u2013</td><td>51.2</td><td>54.5</td><td>493</td><td>872</td><td>25937</td><td>247822</td><td>1802</td></tr><tr><td>FWT [19]</td><td>\u2013</td><td>\u2013</td><td>51.3</td><td>47.6</td><td>505</td><td>830</td><td>24101</td><td>247921</td><td>2648</td></tr><tr><td>eHAF [46]</td><td>\u2013</td><td>\u2013</td><td>51.8</td><td>54.7</td><td>551</td><td>893</td><td>33212</td><td>236772</td><td>1834</td></tr><tr><td>TT [65]</td><td>\u2013</td><td>\u2013</td><td>54.9</td><td>63.1</td><td>575</td><td>897</td><td>20236</td><td>233295</td><td>1088</td></tr><tr><td>MPNTrack [6]</td><td>M+C</td><td>\u2013</td><td>58.8</td><td>61.7</td><td>679</td><td>788</td><td>17413</td><td>213594</td><td>1185</td></tr><tr><td>Lif_T [20]</td><td>M+C</td><td>\u2013</td><td>60.5</td><td>65.6</td><td>637</td><td>791</td><td>14966</td><td>206619</td><td>1189</td></tr><tr><td rowspan=\"6\"><p>Online</p></td><td>FAMNet [10]</td><td>\u2013</td><td>\u2013</td><td>52.0</td><td>48.7</td><td>450</td><td>787</td><td>14138</td><td>253616</td><td>3072</td></tr><tr><td>Tracktor++ [4]</td><td>M+C</td><td>1.3</td><td>56.3</td><td>55.1</td><td>498</td><td>831</td><td>8866</td><td>235449</td><td>1987</td></tr><tr><td>GSM [29]</td><td>M+C</td><td>\u2013</td><td>56.4</td><td>57.8</td><td>523</td><td>813</td><td>14379</td><td>230174</td><td>1485</td></tr><tr><td>CenterTrack [69]</td><td>\u2013</td><td>17.7</td><td>60.5</td><td>55.7</td><td>580</td><td>777</td><td>11599</td><td>208577</td><td>2540</td></tr><tr><td>TMOH [47]</td><td>\u2013</td><td>\u2013</td><td>62.1</td><td>62.8</td><td>633</td><td>739</td><td>10951</td><td>201195</td><td>1897</td></tr><tr><td>TrackFormer</td><td>\u2013</td><td>7.4</td><td>62.3</td><td>57.6</td><td>688</td><td>638</td><td>16591</td><td>192123</td><td>4018</td></tr><tr><td colspan=\"11\">MOT17 [30] - Private</td></tr><tr><td rowspan=\"11\"><p>Online</p></td><td>TubeTK [32]</td><td>JTA</td><td>\u2013</td><td>63.0</td><td>58.6</td><td>735</td><td>468</td><td>27060</td><td>177483</td><td>4137</td></tr><tr><td>GSDT [55]</td><td>6M</td><td>\u2013</td><td>73.2</td><td>66.5</td><td>981</td><td>411</td><td>26397</td><td>120666</td><td>3891</td></tr><tr><td>FairMOT [66]</td><td>CH+6M</td><td>\u2013</td><td>73.7</td><td>72.3</td><td>1017</td><td>408</td><td>27507</td><td>117477</td><td>3303</td></tr><tr><td>PermaTrack [50]</td><td>CH+PD</td><td>\u2013</td><td>73.8</td><td>68.9</td><td>1032</td><td>405</td><td>28998</td><td>115104</td><td>3699</td></tr><tr><td>GRTU [54]</td><td>CH+6M</td><td>\u2013</td><td>75.5</td><td>76.9</td><td>1158</td><td>495</td><td>27813</td><td>108690</td><td>1572</td></tr><tr><td>TLR [53]</td><td>CH+6M</td><td>\u2013</td><td>76.5</td><td>73.6</td><td>1122</td><td>300</td><td>29808</td><td>99510</td><td>3369</td></tr><tr><td>CTracker [36]</td><td>\u2013</td><td>\u2013</td><td>66.6</td><td>57.4</td><td>759</td><td>570</td><td>22284</td><td>160491</td><td>5529</td></tr><tr><td>CenterTrack [69]</td><td>CH</td><td>17.7</td><td>67.8</td><td>64.7</td><td>816</td><td>579</td><td>18498</td><td>160332</td><td>3039</td></tr><tr><td>QuasiDense [33]</td><td>\u2013</td><td>\u2013</td><td>68.7</td><td>66.3</td><td>957</td><td>516</td><td>26589</td><td>146643</td><td>3378</td></tr><tr><td>TraDeS [57]</td><td>CH</td><td>\u2013</td><td>69.1</td><td>63.9</td><td>858</td><td>507</td><td>20892</td><td>150060</td><td>3555</td></tr><tr><td>TrackFormer</td><td>CH</td><td>7.4</td><td>74.1</td><td>68.0</td><td>1113</td><td>246</td><td>34602</td><td>108777</td><td>2829</td></tr><tr><td colspan=\"11\">MOT20 [13] - Private</td></tr><tr><td></td><td>FairMOT [66]</td><td>CH+6M</td><td>\u2013</td><td>61.8</td><td>67.3</td><td>855</td><td>94</td><td>103440</td><td>88901</td><td>5243</td></tr><tr><td></td><td>GSDT [55]</td><td>6M</td><td>\u2013</td><td>67.1</td><td>67.5</td><td>660</td><td>164</td><td>31913</td><td>135409</td><td>3131</td></tr><tr><td></td><td>SOTMOT [68]</td><td>CH+6M</td><td>\u2013</td><td>68.6</td><td>71.4</td><td>806</td><td>120</td><td>57064</td><td>101154</td><td>4209</td></tr><tr><td></td><td>ReMOT [60]</td><td>CH+6M</td><td>\u2013</td><td>77.4</td><td>73.1</td><td>846</td><td>123</td><td>28351</td><td>86659</td><td>1789</td></tr><tr><td></td><td>TrackFormer</td><td>CH</td><td>7.4</td><td>68.6</td><td>65.7</td><td>666</td><td>181</td><td>20348</td><td>140373</td><td>1532</td></tr></tbody></table>", "caption": "Table 1: Comparison of multi-object tracking methods on the MOT17 [30] and  MOT20 [13] test sets.We report private as well as public detection results and separate between online and offline approaches.Both TrackFormer and CenterTrack filter tracks by requiring a minimum IoU with public detections.For a detailed discussion on the fairness of such a filtering, we refer to the appendix.We indicated additional training Data: CH=CrowdHuman [45], PD=Parallel Domain [50], 6M=6 tracking datasets as in [66], JTA [14], M=Market1501 [67] and C=CUHK03 [27].Runtimes (FPS) are self-measured.", "list_citation_info": ["[13] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian D. Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix\u00e9. MOT20: A benchmark for multi object tracking in crowded scenes. CoRR, 2020.", "[66] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, pages 1\u201319, 2021.", "[65] Y. Zhang, H. Sheng, Y. Wu, S. Wang, W. Lyu, W. Ke, and Z. Xiong. Long-term tracking with deep tracklet association. IEEE Trans. Image Process., 2020.", "[14] Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea Palazzi, Roberto Vezzani, and Rita Cucchiara. Learning to detect and track visible and occluded body joints in a virtual world. In European Conference on Computer Vision (ECCV), 2018.", "[32] Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, and Cewu Lu. Tubetk: Adopting tubes to track multi-object in a one-step training model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[20] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swoboda. Lifted disjoint paths with application in multiple object tracking. In Int. Conf. Mach. Learn., 2020.", "[19] Roberto Henschel, Laura Leal-Taix\u00e9, Daniel Cremers, and Bodo Rosenhahn. Improvements to frank-wolfe optimization for multi-detector multi-object tracking. In IEEE Conf. Comput. Vis. Pattern Recog., 2017.", "[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taix\u00e9. Tracking without bells and whistles. In Int. Conf. Comput. Vis., 2019.", "[68] Linyu Zheng, Ming Tang, Yingying Chen, Guibo Zhu, Jinqiao Wang, and Hanqing Lu. Improving multiple object tracking with single object tracking. In IEEE Conf. Comput. Vis. Pattern Recog., pages 2453\u20132462, 2021.", "[60] Fan Yang, Xin Chang, Sakriani Sakti, Yang Wu, and Satoshi Nakamura. Remot: A model-agnostic refinement for multiple object tracking. Image and Vision Computing, 106:104091, 2021.", "[29] Qiankun Liu, Qi Chu, Bin Liu, and Nenghai Yu. Gsm: Graph similarity model for multi-object tracking. In Int. Joint Conf. Art. Int., 2020.", "[10] Peng Chu and Haibin Ling. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking. In Int. Conf. Comput. Vis., 2019.", "[6] Guillem Bras\u00f3 and Laura Leal-Taix\u00e9. Learning a neural solver for multiple object tracking. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.", "[33] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, June 2021.", "[55] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In IEEE Int. Conf. Rob. Aut., May 2021.", "[50] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In Int. Conf. Comput. Vis., 2021.", "[54] Shuai Wang, Hao Sheng, Yang Zhang, Yubin Wu, and Zhang Xiong. A general recurrent tracking framework without real data. In Int. Conf. Comput. Vis., 2021.", "[36] Jinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In Proceedings of the European Conference on Computer Vision, 2020.", "[53] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object tracking with correlation learning. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.", "[45] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: A benchmark for detecting human in a crowd. arXiv:1805.00123, 2018.", "[67] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In Int. Conf. Comput. Vis., 2015.", "[46] H. Sheng, Y. Zhang, J. Chen, Z. Xiong, and J. Zhang. Heterogeneous association graph fusion for target association in multiple object tracking. IEEE Transactions on Circuits and Systems for Video Technology, 2019.", "[47] Daniel Stadler and Jurgen Beyerer. Improving multiple pedestrian tracking by track management and occlusion handling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10958\u201310967, June 2021.", "[27] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing neural network for person re-identification. In IEEE Conf. Comput. Vis. Pattern Recog., 2014.", "[57] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021.", "[22] Margret Keuper, Siyu Tang, Bjoern Andres, Thomas Brox, and Bernt Schiele. Motion segmentation & multiple object tracking by correlation co-clustering. In IEEE Trans. Pattern Anal. Mach. Intell., 2018.", "[69] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. ECCV, 2020.", "[30] Anton Milan, Laura Leal-Taix\u00e9, Ian D. Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016."]}, {"table": "<table><tbody><tr><th>Method</th><th>TbD</th><td>sMOTSA \\uparrow</td><td>IDF1 \\uparrow</td><td>FP \\downarrow</td><td>FN \\downarrow</td><td>ID Sw. \\downarrow</td></tr><tr><th colspan=\"7\">Train set (4-fold cross-validation)</th></tr><tr><th>MHT_DAM [23]</th><th>\\times</th><td>48.0</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>FWT [19]</th><th>\\times</th><td>49.3</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>MOTDT [8]</th><th>\\times</th><td>47.8</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>jCC [22]</th><th>\\times</th><td>48.3</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>TrackRCNN [52]</th><th></th><td>52.7</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>MOTSNet [38]</th><th></th><td>56.8</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>PointTrack [58]</th><th></th><td>58.1</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th>TrackFormer</th><th></th><td>58.7</td><td>\u2013</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><th colspan=\"7\">Test set</th></tr><tr><th>Track R-CNN [52]</th><th></th><td>40.6</td><td>42.4</td><td>1261</td><td>12641</td><td>567</td></tr><tr><th>TrackFormer</th><th></th><td>54.9</td><td>63.6</td><td>2233</td><td>7195</td><td>278</td></tr></tbody></table>", "caption": "Table 2: Comparison of multi-object tracking and segmentation methods evaluated on the MOTS20 [52] train and test sets.We indicate methods which first perform tracking-by-detection (TbD) on SDP [62] detections and then apply a Mask R-CNN [17].", "list_citation_info": ["[38] Lorenzo Porzi, Markus Hofinger, Idoia Ruiz, Joan Serrat, Samuel Rota Bulo, and Peter Kontschieder. Learning multi-object tracking and segmentation from automatic annotations. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.", "[58] Zhenbo Xu, Wei Zhang, Xiao Tan, Wei Yang, Huan Huang, Shilei Wen, Errui Ding, and Liusheng Huang. Segment as points for efficient online multi-object tracking and segmentation. In Eur. Conf. Comput. Vis., 2020.", "[23] Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M. Rehg. Multiple hypothesis tracking revisited. In Int. Conf. Comput. Vis., 2015.", "[52] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.", "[19] Roberto Henschel, Laura Leal-Taix\u00e9, Daniel Cremers, and Bodo Rosenhahn. Improvements to frank-wolfe optimization for multi-detector multi-object tracking. In IEEE Conf. Comput. Vis. Pattern Recog., 2017.", "[8] Long Chen, Haizhou Ai, Zijie Zhuang, and Chong Shang. Real-time multiple people tracking with deeply learned candidate selection and person re-identification. In Int. Conf. Multimedia and Expo, 2018.", "[17] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In IEEE Conf. Comput. Vis. Pattern Recog., 2017.", "[62] Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. IEEE Conf. Comput. Vis. Pattern Recog., pages 2129\u20132137, 2016.", "[22] Margret Keuper, Siyu Tang, Bjoern Andres, Thomas Brox, and Bernt Schiele. Motion segmentation & multiple object tracking by correlation co-clustering. In IEEE Trans. Pattern Anal. Mach. Intell., 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>MOTA \\uparrow</th><th>\\Delta</th><th>IDF1 \\uparrow</th><th>\\Delta</th></tr></thead><tbody><tr><th>TrackFormer</th><td>71.3</td><td></td><td>73.4</td><td></td></tr><tr><th>\u2014\u2014\u2014\u2014\u2014 w\\o \u2014\u2014\u2014\u2014\u2014</th><td colspan=\"4\">\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013</td></tr><tr><th>Pretraining on CrowdHuman</th><td>69.3</td><td>-2.0</td><td>71.8</td><td>-1.6</td></tr><tr><th>Track query re-identification</th><td>69.2</td><td>-0.1</td><td>70.4</td><td>-1.4</td></tr><tr><th>Track augmentations (FP)</th><td>68.4</td><td>-0.8</td><td>70.0</td><td>-0.4</td></tr><tr><th>Track augmentations (Range)</th><td>64.0</td><td>-4.4</td><td>59.2</td><td>-10.8</td></tr><tr><th>Track queries</th><td>61.0</td><td>-3.0</td><td>45.1</td><td>-14.1</td></tr></tbody></table>", "caption": "Table 3: Ablation study on TrackFormer components.We report MOT17 [30] training set private results on a 50-50 frame split.The last row without (w\\o) all components is only trained for object detection and associates tracks via greedy matching as in [69].", "list_citation_info": ["[69] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. ECCV, 2020.", "[30] Anton Milan, Laura Leal-Taix\u00e9, Ian D. Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016."]}, {"table": "<table><thead><tr><th>Method</th><th>Mask training</th><th>MOTA \\uparrow</th><th>IDF1 \\uparrow</th></tr></thead><tbody><tr><th rowspan=\"2\">TrackFormer</th><th>\\times</th><td>61.9</td><td>56.0</td></tr><tr><th></th><td>61.9</td><td>54.8</td></tr></tbody></table>", "caption": "Table 4: We demonstrate the effect of jointly training for tracking and segmentation on a 4-fold split on the MOTS20 [52] train set.We evaluate with regular MOT metrics, i.e., matching to ground truth with bounding boxes instead of masks.", "list_citation_info": ["[52] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019."]}, {"table": "<table><tbody><tr><th>Method</th><td>IN</td><td>IoU</td><td>CD</td><td>MOTA \\uparrow</td><td>IDF1 \\uparrow</td></tr><tr><th colspan=\"6\">Offline</th></tr><tr><th>MHT_DAM [23]</th><td>\\times</td><td></td><td></td><td>50.7</td><td>47.2</td></tr><tr><th>jCC [22]</th><td>\\times</td><td></td><td></td><td>51.2</td><td>54.5</td></tr><tr><th>FWT [19]</th><td>\\times</td><td></td><td></td><td>51.3</td><td>47.6</td></tr><tr><th>eHAF [46]</th><td>\\times</td><td></td><td></td><td>51.8</td><td>54.7</td></tr><tr><th>TT [65]</th><td>\\times</td><td></td><td></td><td>54.9</td><td>63.1</td></tr><tr><th>MPNTrack [6]</th><td>\\times</td><td></td><td></td><td>58.8</td><td>61.7</td></tr><tr><th>Lif_T [20]</th><td>\\times</td><td></td><td></td><td>60.5</td><td>65.6</td></tr><tr><th colspan=\"6\">Online</th></tr><tr><th>MOTDT [8]</th><td>\\times</td><td></td><td></td><td>50.9</td><td>52.7</td></tr><tr><th>FAMNet [10]</th><td>\\times</td><td></td><td></td><td>52.0</td><td>48.7</td></tr><tr><th>Tracktor++ [4]</th><td>\\times</td><td></td><td></td><td>56.3</td><td>55.1</td></tr><tr><th>GSM_Tracktor [29]</th><td>\\times</td><td></td><td></td><td>56.4</td><td>57.8</td></tr><tr><th>TMOH [47]</th><td>\\times</td><td></td><td></td><td>62.1</td><td>62.8</td></tr><tr><th>CenterTrack [69]</th><td></td><td>\\times</td><td></td><td>60.5</td><td>55.7</td></tr><tr><th>TrackFormer</th><td></td><td>\\times</td><td></td><td>62.3</td><td>57.6</td></tr><tr><th>CenterTrack [69]</th><td></td><td></td><td>\\times</td><td>61.5</td><td>59.6</td></tr><tr><th>TrackFormer</th><td></td><td></td><td>\\times</td><td>63.4</td><td>60.0</td></tr></tbody></table>", "caption": "Table A.1: Comparison of modern multi-object tracking methods evaluated on the MOT17 [30] test set for different public detection processing.Public detections are either directly processed as input (IN) or applied for filtering of track initializations by center distance (CD) or intersection over union (IoU).We report mean results over the three sets of public detections provided by [30] and separate between online and offline approaches.The arrows indicate low or high optimal metric values.", "list_citation_info": ["[23] Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M. Rehg. Multiple hypothesis tracking revisited. In Int. Conf. Comput. Vis., 2015.", "[20] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swoboda. Lifted disjoint paths with application in multiple object tracking. In Int. Conf. Mach. Learn., 2020.", "[19] Roberto Henschel, Laura Leal-Taix\u00e9, Daniel Cremers, and Bodo Rosenhahn. Improvements to frank-wolfe optimization for multi-detector multi-object tracking. In IEEE Conf. Comput. Vis. Pattern Recog., 2017.", "[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taix\u00e9. Tracking without bells and whistles. In Int. Conf. Comput. Vis., 2019.", "[65] Y. Zhang, H. Sheng, Y. Wu, S. Wang, W. Lyu, W. Ke, and Z. Xiong. Long-term tracking with deep tracklet association. IEEE Trans. Image Process., 2020.", "[46] H. Sheng, Y. Zhang, J. Chen, Z. Xiong, and J. Zhang. Heterogeneous association graph fusion for target association in multiple object tracking. IEEE Transactions on Circuits and Systems for Video Technology, 2019.", "[8] Long Chen, Haizhou Ai, Zijie Zhuang, and Chong Shang. Real-time multiple people tracking with deeply learned candidate selection and person re-identification. In Int. Conf. Multimedia and Expo, 2018.", "[29] Qiankun Liu, Qi Chu, Bin Liu, and Nenghai Yu. Gsm: Graph similarity model for multi-object tracking. In Int. Joint Conf. Art. Int., 2020.", "[10] Peng Chu and Haibin Ling. Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking. In Int. Conf. Comput. Vis., 2019.", "[47] Daniel Stadler and Jurgen Beyerer. Improving multiple pedestrian tracking by track management and occlusion handling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10958\u201310967, June 2021.", "[22] Margret Keuper, Siyu Tang, Bjoern Andres, Thomas Brox, and Bernt Schiele. Motion segmentation & multiple object tracking by correlation co-clustering. In IEEE Trans. Pattern Anal. Mach. Intell., 2018.", "[69] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. ECCV, 2020.", "[30] Anton Milan, Laura Leal-Taix\u00e9, Ian D. Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016.", "[6] Guillem Bras\u00f3 and Laura Leal-Taix\u00e9. Learning a neural solver for multiple object tracking. In IEEE Conf. Comput. Vis. Pattern Recog., 2020."]}, {"table": "<table><tbody><tr><td>Sequence</td><td>Public detection</td><td>MOTA \\uparrow</td><td>IDF1 \\uparrow</td><td>MT \\uparrow</td><td>ML \\downarrow</td><td>FP \\downarrow</td><td>FN \\downarrow</td><td>ID Sw. \\downarrow</td></tr><tr><td>MOT17-01</td><td>DPM [16]</td><td>57.9</td><td>49.7</td><td>11</td><td>4</td><td>477</td><td>2191</td><td>45</td></tr><tr><td>MOT17-03</td><td>DPM</td><td>88.6</td><td>79.6</td><td>124</td><td>3</td><td>2469</td><td>9365</td><td>122</td></tr><tr><td>MOT17-06</td><td>DPM</td><td>59.8</td><td>60.8</td><td>104</td><td>27</td><td>1791</td><td>2775</td><td>173</td></tr><tr><td>MOT17-07</td><td>DPM</td><td>65.5</td><td>49.5</td><td>24</td><td>5</td><td>1030</td><td>4671</td><td>118</td></tr><tr><td>MOT17-08</td><td>DPM</td><td>54.5</td><td>42.5</td><td>24</td><td>9</td><td>1461</td><td>7861</td><td>279</td></tr><tr><td>MOT17-12</td><td>DPM</td><td>51.8</td><td>63.0</td><td>43</td><td>14</td><td>1880</td><td>2258</td><td>42</td></tr><tr><td>MOT17-14</td><td>DPM</td><td>47.4</td><td>54.9</td><td>41</td><td>20</td><td>2426</td><td>7138</td><td>164</td></tr><tr><td>MOT17-01</td><td>FRCNN [39]</td><td>57.9</td><td>49.7</td><td>11</td><td>4</td><td>477</td><td>2191</td><td>45</td></tr><tr><td>MOT17-03</td><td>FRCNN</td><td>88.6</td><td>79.6</td><td>124</td><td>3</td><td>2469</td><td>9365</td><td>122</td></tr><tr><td>MOT17-06</td><td>FRCNN</td><td>59.8</td><td>60.8</td><td>104</td><td>27</td><td>1791</td><td>2775</td><td>173</td></tr><tr><td>MOT17-07</td><td>FRCNN</td><td>65.5</td><td>49.5</td><td>24</td><td>5</td><td>1030</td><td>4671</td><td>118</td></tr><tr><td>MOT17-08</td><td>FRCNN</td><td>54.5</td><td>42.5</td><td>24</td><td>9</td><td>1461</td><td>7861</td><td>279</td></tr><tr><td>MOT17-12</td><td>FRCNN</td><td>51.8</td><td>63.0</td><td>43</td><td>14</td><td>1880</td><td>2258</td><td>42</td></tr><tr><td>MOT17-14</td><td>FRCNN</td><td>47.4</td><td>54.9</td><td>41</td><td>20</td><td>2426</td><td>7138</td><td>164</td></tr><tr><td>MOT17-01</td><td>SDP [61]</td><td>57.9</td><td>49.7</td><td>11</td><td>4</td><td>477</td><td>2191</td><td>45</td></tr><tr><td>MOT17-03</td><td>SDP</td><td>88.6</td><td>79.6</td><td>124</td><td>3</td><td>2469</td><td>9365</td><td>122</td></tr><tr><td>MOT17-06</td><td>SDP</td><td>59.8</td><td>60.8</td><td>104</td><td>27</td><td>1791</td><td>2775</td><td>173</td></tr><tr><td>MOT17-07</td><td>SDP</td><td>65.5</td><td>49.5</td><td>24</td><td>5</td><td>1030</td><td>4671</td><td>118</td></tr><tr><td>MOT17-08</td><td>SDP</td><td>54.5</td><td>42.5</td><td>24</td><td>9</td><td>1461</td><td>7861</td><td>279</td></tr><tr><td>MOT17-12</td><td>SDP</td><td>51.8</td><td>63.0</td><td>43</td><td>14</td><td>1880</td><td>2258</td><td>42</td></tr><tr><td>MOT17-14</td><td>SDP</td><td>47.4</td><td>54.9</td><td>41</td><td>20</td><td>2426</td><td>7138</td><td>164</td></tr><tr><td>All</td><td>All</td><td>74.1</td><td>68.0</td><td>1113</td><td>246</td><td>34602</td><td>108777</td><td>2829</td></tr></tbody></table>", "caption": "Table A.2: We report private TrackFormer results on each individual sequence evaluated on the MOT17 [30] test set.To follow the official MOT17 format, we display the same results per public detection set.The arrows indicate low or high optimal metric values.", "list_citation_info": ["[61] Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. IEEE Conf. Comput. Vis. Pattern Recog., 2016.", "[16] Pedro F. Felzenszwalb, Ross B. Girshick, David A. McAllester, and Deva Ramanan. Object detection with discriminatively trained part based models. IEEE Trans. Pattern Anal. Mach. Intell., 2009.", "[30] Anton Milan, Laura Leal-Taix\u00e9, Ian D. Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016.", "[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Adv. Neural Inform. Process. Syst., 2015."]}, {"table": "<table><tbody><tr><td>Sequence</td><td>Public detection</td><td>MOTA \\uparrow</td><td>IDF1 \\uparrow</td><td>MT \\uparrow</td><td>ML \\downarrow</td><td>FP \\downarrow</td><td>FN \\downarrow</td><td>ID Sw. \\downarrow</td></tr><tr><td>MOT17-01</td><td>DPM [16]</td><td>49.9</td><td>43.0</td><td>5</td><td>8</td><td>258</td><td>2932</td><td>40</td></tr><tr><td>MOT17-03</td><td>DPM</td><td>74.0</td><td>66.5</td><td>85</td><td>18</td><td>1389</td><td>25396</td><td>374</td></tr><tr><td>MOT17-06</td><td>DPM</td><td>53.6</td><td>51.8</td><td>63</td><td>75</td><td>711</td><td>4575</td><td>180</td></tr><tr><td>MOT17-07</td><td>DPM</td><td>52.6</td><td>48.1</td><td>12</td><td>16</td><td>258</td><td>7663</td><td>88</td></tr><tr><td>MOT17-08</td><td>DPM</td><td>32.5</td><td>31.9</td><td>10</td><td>32</td><td>288</td><td>13838</td><td>128</td></tr><tr><td>MOT17-12</td><td>DPM</td><td>51.3</td><td>57.7</td><td>21</td><td>31</td><td>606</td><td>3565</td><td>53</td></tr><tr><td>MOT17-14</td><td>DPM</td><td>38.1</td><td>42.0</td><td>15</td><td>63</td><td>627</td><td>10505</td><td>314</td></tr><tr><td>MOT17-01</td><td>FRCNN [39]</td><td>50.9</td><td>42.3</td><td>8</td><td>6</td><td>308</td><td>2813</td><td>48</td></tr><tr><td>MOT17-03</td><td>FRCNN</td><td>75.3</td><td>67.0</td><td>84</td><td>16</td><td>1434</td><td>24040</td><td>335</td></tr><tr><td>MOT17-06</td><td>FRCNN</td><td>57.2</td><td>54.8</td><td>73</td><td>48</td><td>960</td><td>3856</td><td>226</td></tr><tr><td>MOT17-07</td><td>FRCNN</td><td>52.4</td><td>47.9</td><td>12</td><td>11</td><td>499</td><td>7437</td><td>106</td></tr><tr><td>MOT17-08</td><td>FRCNN</td><td>31.1</td><td>31.7</td><td>10</td><td>36</td><td>285</td><td>14166</td><td>102</td></tr><tr><td>MOT17-12</td><td>FRCNN</td><td>47.7</td><td>56.7</td><td>19</td><td>32</td><td>702</td><td>3785</td><td>45</td></tr><tr><td>MOT17-14</td><td>FRCNN</td><td>37.8</td><td>41.8</td><td>17</td><td>56</td><td>1300</td><td>9795</td><td>406</td></tr><tr><td>MOT17-01</td><td>SDP [61]</td><td>53.7</td><td>45.3</td><td>10</td><td>5</td><td>556</td><td>2386</td><td>47</td></tr><tr><td>MOT17-03</td><td>SDP</td><td>79.6</td><td>65.8</td><td>95</td><td>13</td><td>2134</td><td>18632</td><td>545</td></tr><tr><td>MOT17-06</td><td>SDP</td><td>56.4</td><td>54.0</td><td>82</td><td>57</td><td>1017</td><td>3889</td><td>228</td></tr><tr><td>MOT17-07</td><td>SDP</td><td>54.6</td><td>47.8</td><td>16</td><td>11</td><td>590</td><td>6965</td><td>121</td></tr><tr><td>MOT17-08</td><td>SDP</td><td>35.0</td><td>33.0</td><td>12</td><td>27</td><td>443</td><td>13152</td><td>144</td></tr><tr><td>MOT17-12</td><td>SDP</td><td>48.9</td><td>57.5</td><td>22</td><td>28</td><td>850</td><td>3527</td><td>54</td></tr><tr><td>MOT17-14</td><td>SDP</td><td>40.4</td><td>42.4</td><td>17</td><td>49</td><td>1376</td><td>9206</td><td>434</td></tr><tr><td>ALL</td><td>ALL</td><td>62.3</td><td>57.6</td><td>688</td><td>638</td><td>16591</td><td>192123</td><td>4018</td></tr></tbody></table>", "caption": "Table A.3: We report TrackFormer results on each individual sequence and set of public detections evaluated on the MOT17 [30] test set.We apply our minimum Intersection over Union (IoU) public detection filtering.The arrows indicate low or high optimal metric values.", "list_citation_info": ["[61] Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. IEEE Conf. Comput. Vis. Pattern Recog., 2016.", "[16] Pedro F. Felzenszwalb, Ross B. Girshick, David A. McAllester, and Deva Ramanan. Object detection with discriminatively trained part based models. IEEE Trans. Pattern Anal. Mach. Intell., 2009.", "[30] Anton Milan, Laura Leal-Taix\u00e9, Ian D. Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016.", "[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Adv. Neural Inform. Process. Syst., 2015."]}, {"table": "<table><thead><tr><th>Sequence</th><th>MOTA \\uparrow</th><th>IDF1 \\uparrow</th><th>MT \\uparrow</th><th>ML \\downarrow</th><th>FP \\downarrow</th><th>FN \\downarrow</th><th>ID Sw. \\downarrow</th></tr></thead><tbody><tr><td>MOT20-04</td><td>82.7</td><td>75.6</td><td>490</td><td>28</td><td>9639</td><td>37165</td><td>566</td></tr><tr><td>MOT20-06</td><td>55.9</td><td>53.5</td><td>96</td><td>72</td><td>5582</td><td>52440</td><td>545</td></tr><tr><td>MOT20-07</td><td>56.2</td><td>59.0</td><td>41</td><td>20</td><td>547</td><td>13856</td><td>92</td></tr><tr><td>MOT20-08</td><td>46.0</td><td>48.3</td><td>39</td><td>61</td><td>4580</td><td>36912</td><td>329</td></tr><tr><td>ALL</td><td>68.6</td><td>65.7</td><td>666</td><td>181</td><td>20348</td><td>140373</td><td>1532</td></tr></tbody></table>", "caption": "Table A.4: We report private TrackFormer results on each individual sequence evaluated on the MOT20 [13] test set.The arrows indicate low or high optimal metric values.", "list_citation_info": ["[13] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian D. Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix\u00e9. MOT20: A benchmark for multi object tracking in crowded scenes. CoRR, 2020."]}, {"table": "<table><thead><tr><th>Sequence</th><th>sMOTSA \\uparrow</th><th>IDF1 \\uparrow</th><th>MOTSA \\uparrow</th><th>FP \\downarrow</th><th>FN \\downarrow</th><th>ID Sw. \\downarrow</th></tr></thead><tbody><tr><td>MOTS20-01</td><td>59.8</td><td>68.0</td><td>79.6</td><td>255</td><td>364</td><td>16</td></tr><tr><td>MOTS20-06</td><td>63.9</td><td>65.1</td><td>78.7</td><td>595</td><td>1335</td><td>158</td></tr><tr><td>MOTS20-07</td><td>43.2</td><td>53.6</td><td>58.5</td><td>834</td><td>4433</td><td>75</td></tr><tr><td>MOTS20-12</td><td>62.0</td><td>76.8</td><td>74.6</td><td>549</td><td>1063</td><td>29</td></tr><tr><td>ALL</td><td>54.9</td><td>63.6</td><td>69.9</td><td>2233</td><td>7195</td><td>278</td></tr></tbody></table>", "caption": "Table A.5: We present TrackFormer tracking and segmentation results on each individual sequence of the MOTS20 [52] test set.MOTS20 is evaluated in a private detections setting.The arrows indicate low or high optimal metric values.", "list_citation_info": ["[52] Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019."]}, {"table": "<table><tbody><tr><td>Sequence</td><td>Public detection</td><td>MOTA \\uparrow</td><td>IDF1 \\uparrow</td><td>MT \\uparrow</td><td>ML \\downarrow</td><td>FP \\downarrow</td><td>FN \\downarrow</td><td>ID Sw. \\downarrow</td></tr><tr><td>MOT17-01</td><td>DPM [16]</td><td>41.6</td><td>44.2</td><td>5</td><td>8</td><td>496</td><td>3252</td><td>22</td></tr><tr><td>MOT17-03</td><td>DPM</td><td>79.3</td><td>71.6</td><td>94</td><td>8</td><td>1142</td><td>20297</td><td>191</td></tr><tr><td>MOT17-06</td><td>DPM</td><td>54.8</td><td>42.0</td><td>54</td><td>63</td><td>314</td><td>4839</td><td>175</td></tr><tr><td>MOT17-07</td><td>DPM</td><td>44.8</td><td>42.0</td><td>11</td><td>16</td><td>1322</td><td>7851</td><td>147</td></tr><tr><td>MOT17-08</td><td>DPM</td><td>26.5</td><td>32.2</td><td>11</td><td>37</td><td>378</td><td>15066</td><td>88</td></tr><tr><td>MOT17-12</td><td>DPM</td><td>46.1</td><td>53.1</td><td>16</td><td>45</td><td>207</td><td>4434</td><td>30</td></tr><tr><td>MOT17-14</td><td>DPM</td><td>31.6</td><td>36.6</td><td>13</td><td>78</td><td>636</td><td>11812</td><td>196</td></tr><tr><td>MOT17-01</td><td>FRCNN [39]</td><td>41.0</td><td>42.1</td><td>6</td><td>9</td><td>571</td><td>3207</td><td>25</td></tr><tr><td>MOT17-03</td><td>FRCNN</td><td>79.6</td><td>72.7</td><td>93</td><td>7</td><td>1234</td><td>19945</td><td>180</td></tr><tr><td>MOT17-06</td><td>FRCNN</td><td>55.6</td><td>42.9</td><td>57</td><td>59</td><td>363</td><td>4676</td><td>190</td></tr><tr><td>MOT17-07</td><td>FRCNN</td><td>45.5</td><td>41.5</td><td>13</td><td>15</td><td>1263</td><td>7785</td><td>156</td></tr><tr><td>MOT17-08</td><td>FRCNN</td><td>26.5</td><td>31.9</td><td>11</td><td>36</td><td>332</td><td>15113</td><td>89</td></tr><tr><td>MOT17-12</td><td>FRCNN</td><td>46.1</td><td>52.6</td><td>15</td><td>45</td><td>197</td><td>4443</td><td>30</td></tr><tr><td>MOT17-14</td><td>FRCNN</td><td>31.6</td><td>37.6</td><td>13</td><td>77</td><td>780</td><td>11653</td><td>202</td></tr><tr><td>MOT17-01</td><td>SDP [61]</td><td>41.8</td><td>44.3</td><td>7</td><td>8</td><td>612</td><td>3112</td><td>27</td></tr><tr><td>MOT17-03</td><td>SDP</td><td>80.0</td><td>72.0</td><td>93</td><td>8</td><td>1223</td><td>19530</td><td>181</td></tr><tr><td>MOT17-06</td><td>SDP</td><td>55.5</td><td>43.8</td><td>56</td><td>61</td><td>354</td><td>4712</td><td>181</td></tr><tr><td>MOT17-07</td><td>SDP</td><td>45.2</td><td>42.4</td><td>13</td><td>15</td><td>1332</td><td>7775</td><td>147</td></tr><tr><td>MOT17-08</td><td>SDP</td><td>26.6</td><td>32.3</td><td>11</td><td>36</td><td>350</td><td>15067</td><td>91</td></tr><tr><td>MOT17-12</td><td>SDP</td><td>46.0</td><td>53.0</td><td>16</td><td>45</td><td>221</td><td>4426</td><td>30</td></tr><tr><td>MOT17-14</td><td>SDP</td><td>31.7</td><td>37.1</td><td>13</td><td>76</td><td>749</td><td>11677</td><td>205</td></tr><tr><td>All</td><td>All</td><td>61.5</td><td>59.6</td><td>621</td><td>752</td><td>14076</td><td>200672</td><td>2583</td></tr></tbody></table>", "caption": "Table A.6: We report the original per-sequence CenterTrack [69] MOT17 [30] test set results with  Center Distance (CD) public detection filtering.The results do not reflect the varying object detection performance of DPM, FRCNN and SDP, respectively.The arrows indicate low or high optimal metric values.", "list_citation_info": ["[61] Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. IEEE Conf. Comput. Vis. Pattern Recog., 2016.", "[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Adv. Neural Inform. Process. Syst., 2015.", "[16] Pedro F. Felzenszwalb, Ross B. Girshick, David A. McAllester, and Deva Ramanan. Object detection with discriminatively trained part based models. IEEE Trans. Pattern Anal. Mach. Intell., 2009.", "[69] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. ECCV, 2020.", "[30] Anton Milan, Laura Leal-Taix\u00e9, Ian D. Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv:1603.00831, 2016."]}]}