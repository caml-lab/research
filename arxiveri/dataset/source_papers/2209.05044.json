{"title": "Predicting the next action by modeling the abstract goal", "abstract": "The problem of anticipating human actions is an inherently uncertain one. However, we can reduce this uncertainty if we have a sense of the goal that the actor is trying to achieve. Here, we present an action anticipation model that leverages goal information for the purpose of reducing the uncertainty in future predictions. Since we do not possess goal information or the observed actions during inference, we resort to visual representation to encapsulate information about both actions and goals. Through this, we derive a novel concept called abstract goal which is conditioned on observed sequences of visual features for action anticipation. We design the abstract goal as a distribution whose parameters are estimated using a variational recurrent network. We sample multiple candidates for the next action and introduce a goal consistency measure to determine the best candidate that follows from the abstract goal. Our method obtains impressive results on the very challenging Epic-Kitchens55 (EK55), EK100, and EGTEA Gaze+ datasets. We obtain absolute improvements of +13.69, +11.24, and +5.19 for Top-1 verb, Top-1 noun, and Top-1 action anticipation accuracy respectively over prior state-of-the-art methods for seen kitchens (S1) of EK55. Similarly, we also obtain significant improvements in the unseen kitchens (S2) set for Top-1 verb (+10.75), noun (+5.84) and action (+2.87) anticipation. Similar trend is observed for EGTEA Gaze+ dataset, where absolute improvement of +9.9, +13.1 and +6.8 is obtained for noun, verb, and action anticipation. It is through the submission of this paper that our method is currently the new state-of-the-art for action anticipation in EK55 and EGTEA Gaze+ https://competitions.codalab.org/competitions/20071#results Code available at https://github.com/debadityaroy/Abstract_Goal", "authors": ["Debaditya Roy", " Basura Fernando"], "pdf_url": "https://arxiv.org/abs/2209.05044", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><th colspan=\"6\">Seen Kitchens (S1)</th><th colspan=\"6\">Unseen Kitchens (S2)</th></tr><tr><th colspan=\"3\">Top-1 accuracy</th><th colspan=\"3\">Top-5 accuracy</th><th colspan=\"3\">Top-1 accuracy</th><th colspan=\"3\">Top-5 accuracy</th></tr><tr><td>VERB</td><td>NOUN</td><td>ACT.</td><td>VERB</td><td>NOUN</td><td>ACT.</td><td>VERB</td><td>NOUN</td><td>ACT.</td><td>VERB</td><td>NOUN</td><td>ACT.</td></tr><tr><td>RU-LSTM [13]</td><td>33.04</td><td>22.78</td><td>14.39</td><td>79.55</td><td>50.95</td><td>33.73</td><td>27.01</td><td>15.19</td><td>08.16</td><td>69.55</td><td>34.38</td><td>21.10</td></tr><tr><td>Lat. Goal [31]</td><td>27.96</td><td>27.40</td><td>8.10</td><td>78.09</td><td>55.98</td><td>26.46</td><td>22.40</td><td>19.12</td><td>04.78</td><td>72.07</td><td>42.68</td><td>16.97</td></tr><tr><td>SRL [29]</td><td>34.89</td><td>22.84</td><td>14.24</td><td>79.59</td><td>52.03</td><td>34.61</td><td>27.42</td><td>15.47</td><td>08.88</td><td>71.90</td><td>36.80</td><td>22.06</td></tr><tr><td>ImagineRNN [39]</td><td>35.44</td><td>22.79</td><td>14.66</td><td>79.72</td><td>52.09</td><td>34.98</td><td>29.33</td><td>15.50</td><td>09.25</td><td>70.67</td><td>35.78</td><td>22.19</td></tr><tr><td>Temp. Agg. [33]</td><td>37.87</td><td>24.10</td><td>16.64</td><td>79.74</td><td>53.98</td><td>36.06</td><td>29.50</td><td>16.52</td><td>10.04</td><td>70.13</td><td>37.83</td><td>23.42</td></tr><tr><td>MM-Trans [30]</td><td>28.59</td><td>27.18</td><td>10.85</td><td>78.64</td><td>57.66</td><td>30.83</td><td>26.80</td><td>18.40</td><td>06.76</td><td>70.40</td><td>44.18</td><td>20.04</td></tr><tr><td>MM-TCN [40]</td><td>37.16</td><td>23.75</td><td>15.45</td><td>79.48</td><td>51.86</td><td>34.37</td><td>30.66</td><td>14.92</td><td>08.91</td><td>72.00</td><td>36.67</td><td>21.68</td></tr><tr><td>AVT [16]</td><td>34.36</td><td>20.16</td><td>16.84</td><td>80.03</td><td>51.57</td><td>36.52</td><td>30.66</td><td>15.64</td><td>10.41</td><td>72.17</td><td>40.76</td><td>24.27</td></tr><tr><td>Abstract Goal</td><td>51.56</td><td>35.34</td><td>22.03</td><td>82.56</td><td>58.01</td><td>38.29</td><td>41.41</td><td>22.36</td><td>13.28</td><td>73.10</td><td>41.62</td><td>24.24</td></tr></tbody></table>", "caption": "Table 1: Comparison of anticipation accuracy with state-of-the-art on EK55 evaluation server. ACT. is action. ", "list_citation_info": ["[39] Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, and Fei Wu. Learning to anticipate egocentric actions by imagination. IEEE Transactions on Image Processing, 30:1143\u20131152, 2021.", "[30] Debaditya Roy and Basura Fernando. Action anticipation using pairwise human-object interactions and transformers. IEEE Transactions on Image Processing, 2021.", "[40] Olga Zatsarynna, Yazan Abu Farha, and Juergen Gall. Multi-modal temporal convolutional network for anticipating actions in egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2249\u20132258, 2021.", "[33] Fadime Sener, Dipika Singhania, and Angela Yao. Temporal aggregate representations for long-range video understanding. In European Conference on Computer Vision, pages 154\u2013171. Springer, 2020.", "[29] Zhaobo Qi, Shuhui Wang, Chi Su, Li Su, Qingming Huang, and Qi Tian. Self-regulated learning for egocentric video activity anticipation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[16] Rohit Girdhar and Kristen Grauman. Anticipative Video Transformer. In ICCV, 2021.", "[13] Antonino Furnari and Giovanni Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[31] Debaditya Roy and Basura Fernando. Action anticipation using latent goal learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2745\u20132753, January 2022."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">Top-1 accuracy</th><th colspan=\"3\">Mean Class accuracy</th></tr><tr><td>VERB</td><td>NOUN</td><td>ACT.</td><td>VERB</td><td>NOUN</td><td>ACT.</td></tr><tr><th>I3D-Res50 [5]</th><td>48.0</td><td>42.1</td><td>34.8</td><td>31.3</td><td>30.0</td><td>23.2</td></tr><tr><th>FHOI [23]</th><td>49.0</td><td>45.5</td><td>36.6</td><td>32.5</td><td>32.7</td><td>25.3</td></tr><tr><th>RU-LSTM [13]</th><td>50.3</td><td>48.1</td><td>38.6</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AVT [16]</th><td>54.9</td><td>52.2</td><td>43.0</td><td>49.9</td><td>48.3</td><td>35.2</td></tr><tr><th>Abstract Goal</th><td>64.8</td><td>65.3</td><td>49.8</td><td>63.4</td><td>55.6</td><td>37.4</td></tr></tbody></table>", "caption": "Table 2: Comparison on anticipation performance on EGTEA Gaze+. All methods are evaluated on fixed anticipation time of 0.5s following [16]. ", "list_citation_info": ["[5] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.", "[16] Rohit Girdhar and Kristen Grauman. Anticipative Video Transformer. In ICCV, 2021.", "[23] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Forecasting human-object interaction: joint prediction of motor attention and actions in first person video. In European Conference on Computer Vision, pages 704\u2013721. Springer, 2020.", "[13] Antonino Furnari and Giovanni Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020."]}]}