{"title": "Data-free knowledge distillation with soft targeted transfer set synthesis", "abstract": "Knowledge distillation (KD) has proved to be an effective approach for deep neural network compression, which learns a compact network (student) by transferring the knowledge from a pre-trained, over-parameterized network (teacher). In traditional KD, the transferred knowledge is usually obtained by feeding training samples to the teacher network to obtain the class probabilities. However, the original training dataset is not always available due to storage costs or privacy issues. In this study, we propose a novel data-free KD approach by modeling the intermediate feature space of the teacher with a multivariate normal distribution and leveraging the soft targeted labels generated by the distribution to synthesize pseudo samples as the transfer set. Several student networks trained with these synthesized transfer sets present competitive performance compared to the networks trained with the original training set and other data-free KD approaches.", "authors": ["Zi Wang"], "pdf_url": "https://arxiv.org/abs/2104.04868", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td>Data</td><td>Accuracy</td></tr><tr><td>Teacher (standard training)</td><td>\u2713</td><td>99.32%</td></tr><tr><td>Student (standard training)</td><td>\u2713</td><td>98.99%</td></tr><tr><td>Standard KD{}^{*}</td><td>\u2713</td><td>99.18%</td></tr><tr><td>(Kimura et al. 2018)</td><td>\u26ab</td><td>86.70%</td></tr><tr><td>(Lopes, Fenu, and Starner 2017)</td><td>\u25fc</td><td>92.47%</td></tr><tr><td>Noise input</td><td>\u2717</td><td>87.58%</td></tr><tr><td>DAFL</td><td>\u2717</td><td>98.20%</td></tr><tr><td>ZSKD</td><td>\u2717</td><td>98.77%</td></tr><tr><td>Ours</td><td>\u2717</td><td>99.08%</td></tr></tbody></table>", "caption": "Table 1: Result on LeNet-5 with the MNIST dataset. Symbols in the \u201cData\u201d column: \u2713: original training data required, \u2717: no data required, \u26ab: limited amount of original training data required (few-shot), \u25fc: meta-data required. {}^{*} indicates the reported results are based on our own implementation.", "list_citation_info": ["Lopes, Fenu, and Starner (2017) Lopes, R. G.; Fenu, S.; and Starner, T. 2017. Data-free knowledge distillation for deep neural networks. arXiv preprint arXiv:1710.07535 .", "Kimura et al. (2018) Kimura, A.; Ghahramani, Z.; Takeuchi, K.; Iwata, T.; and Ueda, N. 2018. Few-shot learning of neural networks from scratch by pseudo example optimization. arXiv preprint arXiv:1802.03039 ."]}]}