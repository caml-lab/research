{"title": "RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening", "abstract": "Enhancing the generalization capability of deep neural networks to unseen domains is crucial for safety-critical applications in the real world such as autonomous driving. To address this issue, this paper proposes a novel instance selective whitening loss to improve the robustness of the segmentation networks for unseen domains. Our approach disentangles the domain-specific style and domain-invariant content encoded in higher-order statistics (i.e., feature covariance) of the feature representations and selectively removes only the style information causing domain shift. As shown in Fig. 1, our method provides reasonable predictions for (a) low-illuminated, (b) rainy, and (c) unseen structures. These types of images are not included in the training dataset, where the baseline shows a significant performance drop, contrary to ours. Being simple yet effective, our approach improves the robustness of various backbone networks without additional computational cost. We conduct extensive experiments in urban-scene segmentation and show the superiority of our approach to existing work. Our code is available at https://github.com/shachoi/RobustNet.", "authors": ["Sungha Choi", " Sanghun Jung", " Huiwon Yun", " Joanne Kim", " Seungryong Kim", " Jaegul Choo"], "pdf_url": "https://arxiv.org/abs/2103.15597", "list_table_and_caption": [{"table": "<table><thead><tr><th>Models (GTAV)</th><th>C</th><th>B</th><th>M</th><th>S</th><th>G</th></tr></thead><tbody><tr><td>Baseline</td><td>28.95</td><td>25.14</td><td>28.18</td><td>26.23</td><td>73.45</td></tr><tr><td>{}^{\\dagger}SW [45]</td><td>29.91</td><td>27.48</td><td>29.71</td><td>27.61</td><td>73.50</td></tr><tr><td>{}^{\\dagger}IBN-Net [44]</td><td>33.85</td><td>32.30</td><td>37.75</td><td>27.90</td><td>72.90</td></tr><tr><td>{}^{\\dagger}IterNorm [22]</td><td>31.81</td><td>32.70</td><td>33.88</td><td>27.07</td><td>73.19</td></tr><tr><td>Ours (IW)</td><td>33.21</td><td>32.67</td><td>37.35</td><td>27.57</td><td>72.06</td></tr><tr><td>Ours (IRW)</td><td>33.57</td><td>33.18</td><td>38.42</td><td>27.29</td><td>71.96</td></tr><tr><td>Ours (ISW)</td><td>36.58</td><td>35.20</td><td>40.33</td><td>28.30</td><td>72.10</td></tr></tbody></table>", "caption": "Table 1: Comparison of mIoU(%). Compared models are trained on GTAV train set, and validated on Cityscapes (C), BDD-100K (B), Mapillary (M), SYNTHIA (S) and GTAV (G) validation sets. ResNet-50 with an output stride of 16 is used.{}^{\\dagger} denotes our own re-implemented models. SW denotes Switchable Whitening [45].", "list_citation_info": ["[45] Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, and Ping Luo. Switchable whitening for deep representation learning. In International Conference on Computer Vision (ICCV), 2019.", "[22] Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyond standardization towards efficient whitening. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[44] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>Models (Cityscapes)</th><th>B</th><th>M</th><th>G</th><th>S</th><th>C</th></tr></thead><tbody><tr><td>Baseline</td><td>44.96</td><td>51.68</td><td>42.55</td><td>23.29</td><td>77.51</td></tr><tr><td>{}^{\\dagger}SW [45]</td><td>48.49</td><td>55.82</td><td>44.87</td><td>26.10</td><td>77.30</td></tr><tr><td>{}^{\\dagger}IBN-Net [44]</td><td>48.56</td><td>57.04</td><td>45.06</td><td>26.14</td><td>76.55</td></tr><tr><td>{}^{\\dagger}IterNorm [22]</td><td>49.23</td><td>56.26</td><td>45.73</td><td>25.98</td><td>76.02</td></tr><tr><td>Ours (IW)</td><td>48.19</td><td>58.90</td><td>45.21</td><td>25.81</td><td>76.06</td></tr><tr><td>Ours (IRW)</td><td>48.67</td><td>59.20</td><td>45.64</td><td>26.05</td><td>76.13</td></tr><tr><td>Ours (ISW)</td><td>50.73</td><td>58.64</td><td>45.00</td><td>26.20</td><td>76.41</td></tr></tbody></table>", "caption": "Table 2: Comparison of mIoU(%). The models are trained on Cityscapes train set. ResNet-50 with an output stride of 16 is used.{}^{\\dagger} denotes re-implemented models.", "list_citation_info": ["[45] Xingang Pan, Xiaohang Zhan, Jianping Shi, Xiaoou Tang, and Ping Luo. Switchable whitening for deep representation learning. In International Conference on Computer Vision (ICCV), 2019.", "[22] Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyond standardization towards efficient whitening. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[44] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><tbody><tr><td>Models (GTAV)</td><td>C</td><td>B</td><td>M</td><td>S</td><td>G</td></tr><tr><td>Baseline</td><td>25.56</td><td>22.17</td><td>28.60</td><td>23.33</td><td>66.47</td></tr><tr><td>{}^{\\dagger}IBN-Net [44]</td><td>27.10</td><td>31.82</td><td>34.89</td><td>25.56</td><td>65.44</td></tr><tr><td>Ours (ISW)</td><td>30.98</td><td>32.06</td><td>35.31</td><td>24.31</td><td>64.99</td></tr><tr><td>Baseline</td><td>25.92</td><td>25.73</td><td>26.45</td><td>24.03</td><td>68.12</td></tr><tr><td>{}^{\\dagger}IBN-Net [44]</td><td>30.14</td><td>27.66</td><td>27.07</td><td>24.98</td><td>67.66</td></tr><tr><td>Ours (ISW)</td><td>30.86</td><td>30.05</td><td>30.67</td><td>24.43</td><td>67.48</td></tr></tbody></table>", "caption": "Table 3: Comparison of mIoU(%). The models are trained on GTAV train set. The backbone networks of the first group are ShuffleNetV2 [38] and the second group is MobileNetV2 [54].", "list_citation_info": ["[54] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.", "[38] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In European Conference on Computer Vision (ECCV), 2018.", "[44] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>Models (GTAV)</th><th colspan=\"2\">C</th><th colspan=\"2\">B</th><th colspan=\"2\">M</th></tr></thead><tbody><tr><th>Baseline</th><td>22.20</td><td rowspan=\"2\">7.40 \\uparrow</td><td colspan=\"2\" rowspan=\"2\">N/A</td><td colspan=\"2\" rowspan=\"2\">N/A</td></tr><tr><th>IBN-Net [44]</th><td>29.60</td></tr><tr><th>Baseline</th><td>32.45</td><td rowspan=\"2\">4.97\\uparrow</td><td>26.73</td><td rowspan=\"2\">5.41\\uparrow</td><td>25.66</td><td rowspan=\"2\">8.46\\uparrow</td></tr><tr><th>DRPC [64]</th><td>37.42</td><td>32.14</td><td>34.12</td></tr><tr><th>Baseline</th><td>28.95</td><td rowspan=\"2\">7.63\\uparrow</td><td>25.14</td><td rowspan=\"2\">10.06\\uparrow</td><td>28.18</td><td rowspan=\"2\">12.15\\uparrow</td></tr><tr><th>Ours (ISW)</th><td>36.58</td><td>35.20</td><td>40.33</td></tr></tbody></table>", "caption": "Table 5: mIoU(%) comparison with IBN-Net and DRPC trained on GTAV train set. The backbone is ResNet-50. Note that IBN-Net does not report the performance on BDD-100K and Mapillary.", "list_citation_info": ["[64] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. In International Conference on Computer Vision (ICCV), 2019.", "[44] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>Models</th><th># of Params</th><th>GFLOPS</th><th>Inference Time (ms)</th></tr></thead><tbody><tr><td>Baseline</td><td>45.082M</td><td>554.31</td><td>10.48</td></tr><tr><td>{}^{\\dagger}IBN-Net [44]</td><td>45.083M</td><td>554.31</td><td>10.51</td></tr><tr><td>{}^{\\dagger}IterNorm [22]</td><td>45.081M</td><td>554.31</td><td>40.31</td></tr><tr><td>Ours</td><td>45.081M</td><td>554.31</td><td>10.43</td></tr></tbody></table>", "caption": "Table 6: Comparison of computational cost. Tested with the image size of 2048\\times1024 on NVIDIA A100 GPU. The inference time is averaged over 500 trials. {}^{\\dagger} denotes re-implemented models.", "list_citation_info": ["[22] Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyond standardization towards efficient whitening. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[44] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In European Conference on Computer Vision (ECCV), 2018."]}]}