{"title": "MVTN: Multi-View Transformation Network for 3D Shape Recognition", "abstract": "Multi-view projection methods have demonstrated their ability to reach state-of-the-art performance on 3D shape recognition. Those methods learn different ways to aggregate information from multiple views. However, the camera view-points for those views tend to be heuristically set and fixed for all shapes. To circumvent the lack of dynamism of current multi-view methods, we propose to learn those view-points. In particular, we introduce the Multi-View Transformation Network (MVTN) that regresses optimal view-points for 3D shape recognition, building upon advances in differentiable rendering. As a result, MVTN can be trained end-to-end along with any multi-view network for 3D shape classification. We integrate MVTN in a novel adaptive multi-view pipeline that can render either 3D meshes or point clouds. MVTN exhibits clear performance gains in the tasks of 3D shape classification and 3D shape retrieval without the need for extra training supervision. In these tasks, MVTN achieves state-of-the-art performance on ModelNet40, ShapeNet Core55, and the most recent and realistic ScanObjectNN dataset (up to 6% improvement). Interestingly, we also show that MVTN can provide network robustness against rotation and occlusion in the 3D domain. The code is available at https://github.com/ajhamdi/MVTN .", "authors": ["Abdullah Hamdi", " Silvio Giancola", " Bernard Ghanem"], "pdf_url": "https://arxiv.org/abs/2011.13244", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td></td><td colspan=\"2\">Classification Accuracy</td></tr><tr><th>Method</th><td>Data Type</td><td>(Per-Class)</td><td>(Overall)</td></tr><tr><th>VoxNet [59]</th><td>Voxels</td><td>83.0</td><td>85.9</td></tr><tr><th>PointNet [62]</th><td>Points</td><td>86.2</td><td>89.2</td></tr><tr><th>PointNet++ [65]</th><td>Points</td><td>-</td><td>91.9</td></tr><tr><th>PointCNN [52]</th><td>Points</td><td>88.1</td><td>91.8</td></tr><tr><th>DGCNN [74]</th><td>Points</td><td>90.2</td><td>92.2</td></tr><tr><th>SGAS [50]</th><td>Points</td><td>-</td><td>93.2</td></tr><tr><th>KPConv[71]</th><td>Points</td><td>-</td><td>92.9</td></tr><tr><th>PTransformer[86]</th><td>Points</td><td>90.6</td><td>93.7</td></tr><tr><th>MVCNN [69]</th><td>12 Views</td><td>90.1</td><td>90.1</td></tr><tr><th>GVCNN [20]</th><td>12 Views</td><td>90.7</td><td>93.1</td></tr><tr><th>ViewGCN [75]</th><td>20 Views</td><td>96.5</td><td>97.6</td></tr><tr><th>ViewGCN [75]{}^{*}</th><td>12 views</td><td>90.7</td><td>93.0</td></tr><tr><th>ViewGCN [75]{}^{*}</th><td>20 views</td><td>91.3</td><td>93.3</td></tr><tr><th>MVTN (ours){}^{*}</th><td>12 Views</td><td>92.0</td><td>93.8</td></tr><tr><th>MVTN (ours){}^{*}</th><td>20 Views</td><td>92.2</td><td>93.5</td></tr></tbody></table>", "caption": "Table 1: 3D Shape Classification on ModelNet40. We compare MVTN against other methods in 3D classification on ModelNet40 [79]. {}^{*} indicates results from our rendering setup (differentiable pipeline), while other multi-view results are reported from pre-rendered views. Bold denotes the best result in its setup.", "list_citation_info": ["[69] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.", "[62] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.", "[59] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 922\u2013928. IEEE, 2015.", "[75] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1850\u20131859, 2020.", "[86] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. arXiv preprint arXiv:2012.09164, 2020.", "[52] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Advances in neural information processing systems (NIPS), pages 820\u2013830, 2018.", "[50] Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali Thabet, and Bernard Ghanem. Sgas: Sequential greedy architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1620\u20131630, 2020.", "[74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019.", "[20] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue Gao. Gvcnn: Group-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 264\u2013272, 2018.", "[79] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1912\u20131920, 2015.", "[71] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6411\u20136420, 2019.", "[65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural information processing systems (NIPS), pages 5099\u20135108, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">Classification Overall Accuracy</th></tr><tr><th>Method</th><th>OBJ_BG</th><th>OBJ_ONLY</th><th>Hardest</th></tr></thead><tbody><tr><th>3DMFV [3]</th><td>68.2</td><td>73.8</td><td>63.0</td></tr><tr><th>PointNet [62]</th><td>73.3</td><td>79.2</td><td>68.0</td></tr><tr><th>SpiderCNN [81]</th><td>77.1</td><td>79.5</td><td>73.7</td></tr><tr><th>PointNet ++ [65]</th><td>82.3</td><td>84.3</td><td>77.9</td></tr><tr><th>PointCNN [52]</th><td>86.1</td><td>85.5</td><td>78.5</td></tr><tr><th>DGCNN [74]</th><td>82.8</td><td>86.2</td><td>78.1</td></tr><tr><th>SimpleView [24]</th><td>-</td><td>-</td><td>79.5</td></tr><tr><th>BGA-DGCNN [72]</th><td>-</td><td>-</td><td>79.7</td></tr><tr><th>BGA-PN++ [72]</th><td>-</td><td>-</td><td>80.2</td></tr><tr><th>MVTN (ours)</th><td>92.6</td><td>92.3</td><td>82.8</td></tr></tbody></table>", "caption": "Table 2: 3D Point Cloud Classification on ScanObjectNN. We compare the performance of MVTN in 3D point cloud classification on three different variants of ScanObjectNN [72]. The variants include object with background, object only, and the hardest variant.", "list_citation_info": ["[3] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer. 3dmfv: Three-dimensional point cloud classification in real-time using convolutional neural networks. IEEE Robotics and Automation Letters, 3(4):3145\u20133152, 2018.", "[62] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.", "[72] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In International Conference on Computer Vision (ICCV), 2019.", "[52] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Advances in neural information processing systems (NIPS), pages 820\u2013830, 2018.", "[74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019.", "[81] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets with parameterized convolutional filters. In Proceedings of the European Conference on Computer Vision (ECCV), pages 87\u2013102, 2018.", "[24] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with a simple and effective baseline. In ICML, 2021.", "[65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural information processing systems (NIPS), pages 5099\u20135108, 2017."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">Shape Retrieval (mAP)</th></tr><tr><th>Method</th><th>Data Type</th><th>ModelNet40</th><th>ShapeNet Core</th></tr></thead><tbody><tr><th>LFD [11]</th><td>Voxels</td><td>40.9</td><td>-</td></tr><tr><th>3D ShapeNets [79]</th><td>Voxels</td><td>49.2</td><td>-</td></tr><tr><th>Densepoint[54]</th><td>Points</td><td>88.5</td><td>-</td></tr><tr><th>PVNet[83]</th><td>Points</td><td>89.5</td><td>-</td></tr><tr><th>MVCNN [69]</th><td>12 Views</td><td>80.2</td><td>73.5</td></tr><tr><th>GIFT [2]</th><td>20 Views</td><td>-</td><td>64.0</td></tr><tr><th>MVFusionNet [36]</th><td>12 Views</td><td>-</td><td>62.2</td></tr><tr><th>ReVGG [68]</th><td>20 Views</td><td>-</td><td>74.9</td></tr><tr><th>RotNet [40]</th><td>20 Views</td><td>-</td><td>77.2</td></tr><tr><th>ViewGCN [75]</th><td>20 Views</td><td>-</td><td>78.4</td></tr><tr><th>MLVCNN [37]</th><td>24 Views</td><td>92.2</td><td>-</td></tr><tr><th>MVTN (ours)</th><td>12 Views</td><td>92.9</td><td>82.9</td></tr></tbody></table>", "caption": "Table 3: 3D Shape Retrieval. We benchmark the shape retrieval mAP of MVTN on ModelNet40 [79] and ShapeNet Core55 [8, 68]. MVTN achieves the best retrieval performance among recent state-of-the-art methods on both datasets with only 12 views.", "list_citation_info": ["[69] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.", "[54] Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, and Chunhong Pan. Densepoint: Learning densely contextual representation for efficient point cloud processing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5239\u20135248, 2019.", "[75] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1850\u20131859, 2020.", "[36] Kui Jia, Jiehong Lin, Mingkui Tan, and Dacheng Tao. Deep multi-view learning using neuron-wise correlation-maximizing regularizers. IEEE Transactions on Image Processing, 28(10):5121\u20135134, 2019.", "[40] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5010\u20135019, 2018.", "[8] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.", "[2] Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. Gift: A real-time and scalable 3d shape search engine. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5023\u20135032, 2016.", "[37] Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, and Yue Gao. Mlvcnn: Multi-loop-view convolutional neural network for 3d shape retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8513\u20138520, 2019.", "[79] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1912\u20131920, 2015.", "[83] Haoxuan You, Yifan Feng, Rongrong Ji, and Yue Gao. Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition. In Proceedings of the 26th ACM international conference on Multimedia, pages 1310\u20131318, 2018.", "[11] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d model retrieval. In Computer graphics forum, volume 22, pages 223\u2013232. Wiley Online Library, 2003.", "[68] Konstantinos Sfikas, Theoharis Theoharis, and Ioannis Pratikakis. Exploiting the PANORAMA Representation for Convolutional Neural Network Classification and Retrieval. In Ioannis Pratikakis, Florent Dupont, and Maks Ovsjanikov, editors, Eurographics Workshop on 3D Object Retrieval, pages 1\u20137. The Eurographics Association, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">Rotation Perturbations Range</th></tr><tr><th>Method</th><th>0^{\\circ}</th><th>\\pm 90^{\\circ}</th><th>\\pm 180^{\\circ}</th></tr></thead><tbody><tr><th>PointNet [62]</th><td>88.7</td><td>42.5</td><td>38.6</td></tr><tr><th>PointNet ++ [65]</th><td>88.2</td><td>47.9</td><td>39.7</td></tr><tr><th>RSCNN [55]</th><td>90.3</td><td>90.3</td><td>90.3</td></tr><tr><th>MVTN (ours)</th><td>91.7</td><td>90.8</td><td>91.2</td></tr></tbody></table>", "caption": "Table 4: Rotation Robustness on ModelNet40.At test time, we randomly rotate objects in ModelNet40 around the Y-axis (gravity) with different ranges and report the overall accuracy. MVTN displays strong robustness to such Y-rotations.", "list_citation_info": ["[55] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8895\u20138904, 2019.", "[62] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.", "[65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural information processing systems (NIPS), pages 5099\u20135108, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"6\">Occlusion Ratio</th></tr><tr><th>Method</th><th>0</th><th>0.1</th><th>0.2</th><th>0.3</th><th>0.5</th><th>0.75</th></tr></thead><tbody><tr><th>PointNet [62]</th><td>89.1</td><td>88.2</td><td>86.1</td><td>81.6</td><td>53.5</td><td>4.7</td></tr><tr><th>DGCNN [74]</th><td>92.1</td><td>77.1</td><td>74.5</td><td>71.2</td><td>30.1</td><td>4.3</td></tr><tr><th>MVTN (ours)</th><td>92.3</td><td>90.3</td><td>89.9</td><td>88.3</td><td>67.1</td><td>9.5</td></tr></tbody></table>", "caption": "Table 5: Occlusion Robustness of 3D Methods. We report the test accuracy on point cloud ModelNet40 for different occlusion ratios of the data to measure occlusion robustness of different 3D methods. MVTN achieves 13% better accuracy than PointNet (a robust network) when half of the object is occluded.", "list_citation_info": ["[62] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.", "[74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019."]}, {"table": "<table><thead><tr><th>View Selection</th><th colspan=\"3\">Multi-View Networks</th></tr><tr><th></th><th>MVCNN[69]</th><th>RotNet[40]</th><th>ViewGCN[75]</th></tr></thead><tbody><tr><th>fixed views</th><td>90.4</td><td>91.6</td><td>93.0</td></tr><tr><th>with MVTN</th><td>92.6</td><td>93.2</td><td>93.8</td></tr></tbody></table>", "caption": "Table 7: Integrating MVTN with Multi-View Networks.We show overall classification accuracies on ModelNet40 with 12 views on different multi-view networks when fixed views are used versus when MVTN is used.", "list_citation_info": ["[69] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.", "[40] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5010\u20135019, 2018.", "[75] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1850\u20131859, 2020."]}, {"table": "<table><thead><tr><th>Network</th><th>GFLOPs</th><th>Time (ms)</th><th>Parameters # (M)</th></tr></thead><tbody><tr><th>MVCNN [69]</th><td>43.72</td><td>39.89</td><td>11.20</td></tr><tr><th>ViewGCN [75]</th><td>44.19</td><td>26.06</td><td>23.56</td></tr><tr><th>MVTN module</th><td>1.78</td><td>4.24</td><td>3.5</td></tr></tbody></table>", "caption": "Table 8: Time and Memory Requirements. We assess the contribution of the MVTN module to the time and memory requirements in the multi-view pipeline. We note that the MVTN\u2019s time and memory requirements are negligible.", "list_citation_info": ["[69] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.", "[75] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1850\u20131859, 2020."]}, {"table": "<table><tbody><tr><th></th><td></td><td colspan=\"2\">Classification Accuracy</td></tr><tr><th>Method</th><td>Data Type</td><td>(Per-Class)</td><td>(Overall)</td></tr><tr><th>SPH [42]</th><td>Voxels</td><td>68.2</td><td>-</td></tr><tr><th>LFD [11]</th><td>Voxels</td><td>75.5</td><td>-</td></tr><tr><th>3D ShapeNets [79]</th><td>Voxels</td><td>77.3</td><td>-</td></tr><tr><th>VoxNet [59]</th><td>Voxels</td><td>83.0</td><td>85.9</td></tr><tr><th>VRN [6]</th><td>Voxels</td><td>-</td><td>91.3</td></tr><tr><th>MVCNN-MS [64]</th><td>Voxels</td><td>-</td><td>91.4</td></tr><tr><th>FusionNet [33]</th><td>Voxels+MV</td><td>-</td><td>90.8</td></tr><tr><th>PointNet [62]</th><td>Points</td><td>86.2</td><td>89.2</td></tr><tr><th>PointNet++ [65]</th><td>Points</td><td>-</td><td>91.9</td></tr><tr><th>KD-Network [43]</th><td>Points</td><td>88.5</td><td>91.8</td></tr><tr><th>PointCNN [52]</th><td>Points</td><td>88.1</td><td>91.8</td></tr><tr><th>DGCNN [74]</th><td>Points</td><td>90.2</td><td>92.2</td></tr><tr><th>KPConv[71]</th><td>Points</td><td>-</td><td>92.9</td></tr><tr><th>PVNet[83]</th><td>Points</td><td>-</td><td>93.2</td></tr><tr><th>PTransformer[86]</th><td>Points</td><td>90.6</td><td>93.7</td></tr><tr><th>MVCNN [69]</th><td>12 Views</td><td>90.1</td><td>90.1</td></tr><tr><th>GVCNN [20]</th><td>12 Views</td><td>90.7</td><td>93.1</td></tr><tr><th>ViewGCN [75]</th><td>20 Views</td><td>96.5</td><td>97.6</td></tr><tr><th>ViewGCN [75]{}^{*}</th><td>12 views</td><td>90.7 (90.5 \\pm 0.2)</td><td>93.0 (92.8 \\pm 0.1)</td></tr><tr><th>ViewGCN [75]{}^{*}</th><td>20 views</td><td>91.3 (91.0 \\pm 0.2)</td><td>93.3 (93.1 \\pm 0.2)</td></tr><tr><th>MVTN (ours){}^{*}</th><td>12 Views</td><td>92.0 (91.2 \\pm 0.6)</td><td>93.8 (93.4 \\pm 0.3)</td></tr><tr><th>MVTN (ours){}^{*}</th><td>20 Views</td><td>92.2 (91.8 \\pm 0.3)</td><td>93.5 (93.1 \\pm 0.5)</td></tr></tbody></table>", "caption": "Table 9: 3D Shape Classification on ModelNet40. We compare MVTN against other methods in 3D classification on ModelNet40 [79]. {}^{*} indicates results from our rendering setup (differentiable pipeline), while other multi-view results are reported from pre-rendered views. Bold denotes the best result in its setup. In brackets, we report the average and standard deviation of four runs", "list_citation_info": ["[69] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.", "[42] Michael Kazhdan, Thomas Funkhouser, and Szymon Rusinkiewicz. Rotation invariant spherical harmonic representation of 3 d shape descriptors. In Symposium on geometry processing, volume 6, pages 156\u2013164, 2003.", "[62] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.", "[6] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Generative and discriminative voxel modeling with convolutional neural networks. arXiv preprint arXiv:1608.04236, 2016.", "[59] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 922\u2013928. IEEE, 2015.", "[64] Charles R Qi, Hao Su, Matthias Nie\u00dfner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. Volumetric and multi-view cnns for object classification on 3d data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5648\u20135656, 2016.", "[33] Vishakh Hegde and Reza Zadeh. Fusionnet: 3d object classification using multiple data representations. arXiv preprint arXiv:1607.05695, 2016.", "[75] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1850\u20131859, 2020.", "[86] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. arXiv preprint arXiv:2012.09164, 2020.", "[52] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Advances in neural information processing systems (NIPS), pages 820\u2013830, 2018.", "[83] Haoxuan You, Yifan Feng, Rongrong Ji, and Yue Gao. Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition. In Proceedings of the 26th ACM international conference on Multimedia, pages 1310\u20131318, 2018.", "[74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019.", "[20] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue Gao. Gvcnn: Group-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 264\u2013272, 2018.", "[79] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1912\u20131920, 2015.", "[43] Roman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks for the recognition of 3d point cloud models. In Proceedings of the IEEE International Conference on Computer Vision, pages 863\u2013872, 2017.", "[71] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6411\u20136420, 2019.", "[11] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d model retrieval. In Computer graphics forum, volume 22, pages 223\u2013232. Wiley Online Library, 2003.", "[65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural information processing systems (NIPS), pages 5099\u20135108, 2017."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"3\">Classification Overall Accuracy</td></tr><tr><th>Method</th><td>Object with Background</td><td>Object Only</td><td>PB_T50_RS (Hardest)</td></tr><tr><th>3DMFV [3]</th><td>68.2</td><td>73.8</td><td>63.0</td></tr><tr><th>PointNet [62]</th><td>73.3</td><td>79.2</td><td>68.0</td></tr><tr><th>SpiderCNN [81]</th><td>77.1</td><td>79.5</td><td>73.7</td></tr><tr><th>PointNet ++ [65]</th><td>82.3</td><td>84.3</td><td>77.9</td></tr><tr><th>PointCNN [52]</th><td>86.1</td><td>85.5</td><td>78.5</td></tr><tr><th>DGCNN [74]</th><td>82.8</td><td>86.2</td><td>78.1</td></tr><tr><th>SimpleView [24]</th><td>-</td><td>-</td><td>79.5</td></tr><tr><th>BGA-DGCNN [72]</th><td>-</td><td>-</td><td>79.7</td></tr><tr><th>BGA-PN++ [72]</th><td>-</td><td>-</td><td>80.2</td></tr><tr><th>ViewGCN {}^{*}</th><td>91.9 (91.12 \\pm 0.5)</td><td>90.4 (89.7 \\pm 0.5)</td><td>80.5 (80.2 \\pm 0.4)</td></tr><tr><th>MVTN (ours)</th><td>92.6 (92.5 \\pm 0.2)</td><td>92.3 (91.7 \\pm 0.7)</td><td>82.8 (81.8 \\pm 0.7)</td></tr></tbody></table>", "caption": "Table 10: 3D Point Cloud Classification on ScanObjectNN. We compare the performance of MVTN in 3D point cloud classification on three different variants of ScanObjectNN [72]. The variants include object with background, object only, and the hardest variant. {}^{*} indicates results from our rendering setup (differentiable pipeline), and we report the average and standard deviation of four runs in brackets.", "list_citation_info": ["[3] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer. 3dmfv: Three-dimensional point cloud classification in real-time using convolutional neural networks. IEEE Robotics and Automation Letters, 3(4):3145\u20133152, 2018.", "[62] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.", "[72] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In International Conference on Computer Vision (ICCV), 2019.", "[52] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. In Advances in neural information processing systems (NIPS), pages 820\u2013830, 2018.", "[74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019.", "[81] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets with parameterized convolutional filters. In Proceedings of the European Conference on Computer Vision (ECCV), pages 87\u2013102, 2018.", "[24] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with a simple and effective baseline. In ICML, 2021.", "[65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural information processing systems (NIPS), pages 5099\u20135108, 2017."]}, {"table": "<table><tbody><tr><th></th><td></td><td colspan=\"2\">Shape Retrieval (mAP)</td></tr><tr><th>Method</th><td>Data Type</td><td>ModelNet40</td><td>ShapeNet Core</td></tr><tr><th>ZDFR [49]</th><td>Voxels</td><td>-</td><td>19.9</td></tr><tr><th>DLAN [21]</th><td>Voxels</td><td>-</td><td>66.3</td></tr><tr><th>SPH [42]</th><td>Voxels</td><td>33.3</td><td>-</td></tr><tr><th>LFD [11]</th><td>Voxels</td><td>40.9</td><td>-</td></tr><tr><th>3D ShapeNets [79]</th><td>Voxels</td><td>49.2</td><td>-</td></tr><tr><th>PVNet[83]</th><td>Points</td><td>89.5</td><td>-</td></tr><tr><th>MVCNN [69]</th><td>12 Views</td><td>80.2</td><td>73.5</td></tr><tr><th>GIFT [2]</th><td>20 Views</td><td>-</td><td>64.0</td></tr><tr><th>MVFusionNet [36]</th><td>12 Views</td><td>-</td><td>62.2</td></tr><tr><th>ReVGG [68]</th><td>20 Views</td><td>-</td><td>74.9</td></tr><tr><th>RotNet [40]</th><td>20 Views</td><td>-</td><td>77.2</td></tr><tr><th>ViewGCN [75]</th><td>20 Views</td><td>-</td><td>78.4</td></tr><tr><th>MLVCNN [37]</th><td>24 Views</td><td>92.2</td><td>-</td></tr><tr><th>MVTN (ours)</th><td>12 Views</td><td>92.9 (92.4 \\pm 0.6)</td><td>82.9 (82.4 \\pm 0.6)</td></tr></tbody></table>", "caption": "Table 11: 3D Shape Retrieval. We benchmark the shape retrieval capability of MVTN on ModelNet40 [79] and ShapeNet Core55 [8, 68]. MVTN achieves the best retrieval performance among recent state-of-the-art methods on both datasets with only 12 views. In brackets, we report the average and standard deviation of four runs.", "list_citation_info": ["[69] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.", "[42] Michael Kazhdan, Thomas Funkhouser, and Szymon Rusinkiewicz. Rotation invariant spherical harmonic representation of 3 d shape descriptors. In Symposium on geometry processing, volume 6, pages 156\u2013164, 2003.", "[21] Takahiko Furuya and Ryutarou Ohbuchi. Deep aggregation of local 3d geometric features for 3d model retrieval. In BMVC, volume 7, page 8, 2016.", "[49] Bo Li and Henry Johan. 3d model retrieval using hybrid features and class information. Multimedia tools and applications, 62(3):821\u2013846, 2013.", "[75] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based graph convolutional network for 3d shape analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1850\u20131859, 2020.", "[36] Kui Jia, Jiehong Lin, Mingkui Tan, and Dacheng Tao. Deep multi-view learning using neuron-wise correlation-maximizing regularizers. IEEE Transactions on Image Processing, 28(10):5121\u20135134, 2019.", "[40] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. Rotationnet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5010\u20135019, 2018.", "[8] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University \u2014 Princeton University \u2014 Toyota Technological Institute at Chicago, 2015.", "[2] Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. Gift: A real-time and scalable 3d shape search engine. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5023\u20135032, 2016.", "[37] Jianwen Jiang, Di Bao, Ziqiang Chen, Xibin Zhao, and Yue Gao. Mlvcnn: Multi-loop-view convolutional neural network for 3d shape retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8513\u20138520, 2019.", "[79] Zhirong Wu, S. Song, A. Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1912\u20131920, 2015.", "[83] Haoxuan You, Yifan Feng, Rongrong Ji, and Yue Gao. Pvnet: A joint convolutional network of point cloud and multi-view for 3d shape recognition. In Proceedings of the 26th ACM international conference on Multimedia, pages 1310\u20131318, 2018.", "[11] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d model retrieval. In Computer graphics forum, volume 22, pages 223\u2013232. Wiley Online Library, 2003.", "[68] Konstantinos Sfikas, Theoharis Theoharis, and Ioannis Pratikakis. Exploiting the PANORAMA Representation for Convolutional Neural Network Classification and Retrieval. In Ioannis Pratikakis, Florent Dupont, and Maks Ovsjanikov, editors, Eurographics Workshop on 3D Object Retrieval, pages 1\u20137. The Eurographics Association, 2017."]}, {"table": "<table><tbody><tr><td>Views</td><td colspan=\"2\">Backbone</td><td colspan=\"2\">Point Encoder</td><td colspan=\"2\">Setup</td><td colspan=\"2\">Fusion</td><td>Results</td></tr><tr><td>number</td><td>ResNet18</td><td>ResNet50</td><td>PointNet[62]</td><td>DGCNN[74]</td><td>circular</td><td>spherical</td><td>late</td><td>MVTN</td><td>accuracy</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>90.48 %</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>91.13 %</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>89.51 %</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>91.94 %</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>87.80 %</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>91.49 %</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>89.82 %</td></tr><tr><td>6</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>91.29 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>89.10 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>90.40 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>89.22 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>90.76 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>89.99 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>89.91 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>89.95 %</td></tr><tr><td>6</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>90.43 %</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>87.35%</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>90.68%</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>88.41%</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>91.82</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>90.24%</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>90.28%</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>89.83%</td></tr><tr><td>12</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>91.98%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>86.87%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>88.86%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>87.16%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>88.41%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>90.15%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>88.37%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>90.48%</td></tr><tr><td>12</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>89.63%</td></tr></tbody></table>", "caption": "Table 14: Ablation Study. We study the effect of ablating different components of MVTN on the test accuracy on ModelNet40. Namely, we observe that using more complex backbone CNNs (like ResNet50 [31]) or a more complex features extractor (like DGCNN [74]) does not increase the performance significantly compared to ResNet18 and PointNet [62] respectively. Furthermore, combining the shape features extractor with the MVCNN [69] in late fusion does not work as well as MVTN with the same architectures. All the reported results are using MVCNN [69] as multi-view network.", "list_citation_info": ["[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.", "[69] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In Proceedings of the IEEE international conference on computer vision, pages 945\u2013953, 2015.", "[62] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.", "[74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (TOG), 2019."]}]}