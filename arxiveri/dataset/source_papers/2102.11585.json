{"title": "ROAD: The ROad event Awareness Dataset for Autonomous Driving", "abstract": "Humans drive in a holistic fashion which entails, in particular, understanding dynamic road events and their evolution. Injecting these capabilities in autonomous vehicles can thus take situational awareness and decision making closer to human-level performance. To this purpose, we introduce the ROad event Awareness Dataset (ROAD) for Autonomous Driving, to our knowledge the first of its kind. ROAD is designed to test an autonomous vehicle's ability to detect road events, defined as triplets composed by an active agent, the action(s) it performs and the corresponding scene locations. ROAD comprises videos originally from the Oxford RobotCar Dataset annotated with bounding boxes showing the location in the image plane of each road event. We benchmark various detection tasks, proposing as a baseline a new incremental algorithm for online road event awareness termed 3D-RetinaNet. We also report the performance on the ROAD tasks of Slowfast and YOLOv5 detectors, as well as that of the winners of the ICCV2021 ROAD challenge, which highlight the challenges faced by situation awareness in autonomous driving. ROAD is designed to allow scholars to investigate exciting tasks such as complex (road) activity detection, future event anticipation and continual learning. The dataset is available at https://github.com/gurkirt/road-dataset; the baseline can be found at https://github.com/gurkirt/3D-RetinaNet.", "authors": ["Gurkirt Singh", " Stephen Akrigg", " Manuele Di Maio", " Valentina Fontana", " Reza Javanmard Alitappeh", " Suman Saha", " Kossar Jeddisaravi", " Farzad Yousefi", " Jacob Culley", " Tom Nicholson", " Jordan Omokeowa", " Salman Khan", " Stanislao Grazioso", " Andrew Bradley", " Giuseppe Di Gironimo", " Fabio Cuzzolin"], "pdf_url": "https://arxiv.org/abs/2102.11585", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">Class Num.</th><td rowspan=\"2\">Location label</td><td colspan=\"2\">Action Ann</td><td colspan=\"2\">Tube Ann</td></tr><tr><td>Ped.</td><td>Veh.</td><td>Ped.</td><td>Veh.</td></tr><tr><th>SYNTHIA[60]</th><th>13</th><td>pixelwise ann.</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SemKITTI [61]</th><th>28</th><td>3D sem. seg.</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cityscapes [24]</th><th>30</th><td>pixel level sem.</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>A2D2[47]</th><th>14</th><td>3D sem. seg.</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Waymo [45]</th><th>4</th><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td></tr><tr><th>Apolloscape [27]</th><th>25</th><td>pixel level sem.</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td></tr><tr><th>PIE [58]</th><th>6</th><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td></tr><tr><th>TITAN [59]</th><th>50</th><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>KITTI360 [48]</th><th>19</th><td>sem. ann.</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>A*3D [46]</th><th>7</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>H3D [38]</th><th>8</th><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td></tr><tr><th>Argoverse [43]</th><th>15</th><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td></tr><tr><th>NuScense [49]</th><th>23</th><td>3D sem. seg.</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td></tr><tr><th>DriveSeg [36]</th><th>12</th><td>sem. ann.</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th colspan=\"5\">Spatiotemporal action detection datasets</th><td></td><td></td></tr><tr><th>UCF24 [62]</th><th>24</th><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td></tr><tr><th>AVA [63]</th><th>80</th><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td></tr><tr><th>Multisports [64]</th><th>66</th><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td></tr><tr><th>ROAD (ours)</th><th>43</th><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody><tfoot><tr><th colspan=\"7\">Ped. Pedestrian, Veh. Vehicle, ann. annotation, sem. seg. semantic segmentation</th></tr></tfoot></table>", "caption": "TABLE I:  Comparison of ROAD with similar datasets for perception in autonomous driving in terms of diversity of labels. The comparison is based on the number of classes portrayed and the availability of action annotations and tube tracks for both pedestrians and vehicles, as well as location information. Most competitor datasets do not provide action annotation for either pedestrians or vehicles.", "list_citation_info": ["[38] A. Patil, S. Malla, H. Gang, and Y.-T. Chen, \u201cThe h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes,\u201d in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 9552\u20139557.", "[64] Y. Li, L. Chen, R. He, Z. Wang, G. Wu, and L. Wang, \u201cMultisports: A multi-person video dataset of spatio-temporally localized sports actions,\u201d arXiv preprint arXiv:2105.07404, 2021.", "[24] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset for semantic urban scene understanding,\u201d in Proceedings of CVPR 2016, 2016, pp. 3213\u20133223.", "[47] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung, L. Hauswald, V. H. Pham, M. M\u00fchlegg, S. Dorn et al., \u201cA2d2: Aev autonomous driving dataset,\u201d Note: http://www. a2d2. audi Cited by, vol. 1, no. 4, 2019.", "[46] Q.-H. Pham, P. Sevestre, R. S. Pahwa, H. Zhan, C. H. Pang, Y. Chen, A. Mustafa, V. Chandrasekhar, and J. Lin, \u201cA* 3d dataset: Towards autonomous driving in challenging environments,\u201d arXiv preprint arXiv:1909.07541, 2019.", "[48] Y. Liao, J. Xie, and A. Geiger, \u201cKITTI-360: A novel dataset and benchmarks for urban scene understanding in 2d and 3d,\u201d arXiv.org, vol. 2109.13410, 2021.", "[36] L. Ding, J. Terwilliger, R. Sherony, B. Reimer, and L. Fridman, \u201cMIT DriveSeg (Manual) Dataset,\u201d 2020.", "[27] P. Wang, X. Huang, X. Cheng, D. Zhou, Q. Geng, and R. Yang, \u201cThe apolloscape open dataset for autonomous driving and its application,\u201d IEEE transactions on pattern analysis and machine intelligence, 2019.", "[61] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall, \u201cSemantickitti: A dataset for semantic scene understanding of lidar sequences,\u201d 2019.", "[49] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009621\u201311\u2009631.", "[59] S. Malla, B. Dariush, and C. Choi, \u201cTitan: Future forecast using action priors,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009186\u201311\u2009196.", "[60] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, \u201cThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3234\u20133243.", "[45] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, V. Vasudevan, W. Han, J. Ngiam, H. Zhao, A. Timofeev, S. Ettinger, M. Krivokon, A. Gao, A. Joshi, Y. Zhang, J. Shlens, Z. Chen, and D. Anguelov, \u201cScalability in perception for autonomous driving: Waymo open dataset,\u201d 2019.", "[63] C. Gu, C. Sun, S. Vijayanarasimhan, C. Pantofaru, D. A. Ross, G. Toderici, Y. Li, S. Ricco, R. Sukthankar, C. Schmid et al., \u201cAva: A video dataset of spatio-temporally localized atomic visual actions,\u201d arXiv preprint arXiv:1705.08421, 2017.", "[43] J. Chang, Ming-Fang D. Wang, P. Carr, S. Lucey, D. Ramanan, and others et al., \u201cArgoverse: 3d tracking and forecasting with rich maps,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 8748\u20138757.", "[58] A. Rasouli, I. Kotseruba, T. Kunic, and J. K. Tsotsos, \u201cPie: A large-scale dataset and models for pedestrian intention estimation and trajectory prediction,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6262\u20136271.", "[62] Y. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar, \u201cThumos challenge: Action recognition with a large number of classes,\u201d http://crcv.ucf.edu/THUMOS14, 2014."]}, {"table": "<table><tbody><tr><th>Methods / \\delta =</th><td>f-mAP</td><td>0.2</td><td>0.5</td><td>0.75</td><td>0.5:0.9</td></tr><tr><th colspan=\"6\">RGB + FLOW methods</th></tr><tr><th>MR-TS Peng et al. [85]</th><td>\u2013</td><td>73.7</td><td>32.1</td><td>00.9</td><td>07.3</td></tr><tr><th>FasterRCNN Saha et al.[98]</th><td>\u2013</td><td>66.6</td><td>36.4</td><td>07.9</td><td>14.4</td></tr><tr><th>SSD + OJLA Behl et al. [80]{}^{*}</th><td>\u2013</td><td>68.3</td><td>40.5</td><td>14.3</td><td>18.6</td></tr><tr><th>SSD Singh et al. [19]{}^{*}</th><td>\u2013</td><td>76.4</td><td>45.2</td><td>14.4</td><td>20.1</td></tr><tr><th>AMTnet Saha et al. [84]{}^{*}</th><td>\u2013</td><td>78.5</td><td>49.7</td><td>22.2</td><td>24.0</td></tr><tr><th>ACT Kalogeiton et al. [81]{}^{*}</th><td>\u2013</td><td>76.5</td><td>49.2</td><td>19.7</td><td>23.4</td></tr><tr><th>TraMNet Singh et al. [87]{}^{*}</th><td>\u2013</td><td>79.0</td><td>50.9</td><td>20.1</td><td>23.9</td></tr><tr><th>Song et al. [101]</th><td>72.1</td><td>77.5</td><td>52.9</td><td>21.8</td><td>24.1</td></tr><tr><th>Zhao et al. [86]</th><td>\u2013</td><td>78.5</td><td>50.3</td><td>22.2</td><td>24.5</td></tr><tr><th>I3D Gu et al. [102]</th><td>76.3</td><td>\u2013</td><td>59.9</td><td>\u2013</td><td>\u2013</td></tr><tr><th>Li et al. [82]{}^{*}</th><td>78.0</td><td>82.8</td><td>53.8</td><td>29.6</td><td>28.3</td></tr><tr><th colspan=\"6\">RGB only methods</th></tr><tr><th>RGB-SSD Singh et al. [19]{}^{*}</th><td>65.0</td><td>72.1</td><td>40.6</td><td>14.1</td><td>18.5</td></tr><tr><th>RGB-AMTNet Saha et al. [84]{}^{*}</th><td>\u2013</td><td>75.8</td><td>45.3</td><td>19.9</td><td>22.0</td></tr><tr><th>3D-RetinaNet / 2D (ours){}^{*}</th><td>65.2</td><td>73.5</td><td>48.6</td><td>22.0</td><td>22.8</td></tr><tr><th>3D-RetinaNet / I3D (ours)</th><td>75.2</td><td>82.4</td><td>58.2</td><td>25.5</td><td>27.1</td></tr><tr><th colspan=\"5\">{}^{*} online methods</th><td></td></tr></tbody></table>", "caption": "TABLE III: Comparison of the action detection performance (frame-mAP@0.5 (f-mAP) and video-mAP at different IoU thresholds) of the proposed 3D-RetinaNet baseline model with the state-of-the-art on the UCF-101-24 dataset.", "list_citation_info": ["[86] J. Zhao and C. G. Snoek, \u201cDance with flow: Two-in-one stream action detection,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 9935\u20139944.", "[98] S. Saha, G. Singh, M. Sapienza, P. H. S. Torr, and F. Cuzzolin, \u201cDeep learning for detecting multiple space-time action tubes in videos,\u201d in British Machine Vision Conference, 2016.", "[19] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin, \u201cOnline real-time multiple spatiotemporal action localisation and prediction,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 3637\u20133646.", "[87] G. Singh, S. Saha, and F. Cuzzolin, \u201cTramnet-transition matrix network for efficient action tube proposals,\u201d in Asian Conference on Computer Vision. Springer, 2018, pp. 420\u2013437.", "[82] Y. Li, Z. Wang, L. Wang, and G. Wu, \u201cActions as moving points,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[84] S. Saha, G. Singh, and F. Cuzzolin, \u201cAmtnet: Action-micro-tube regression by end-to-end trainable deep architecture,\u201d in Proc. Int. Conf. Computer Vision, 2017.", "[102] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar, C. Schmid, and J. Malik, \u201cAva: A video dataset of spatio-temporally localized atomic visual actions,\u201d 2018.", "[80] H. S. Behl, M. Sapienza, G. Singh, S. Saha, F. Cuzzolin, and P. H. Torr, \u201cIncremental tube construction for human action detection,\u201d arXiv preprint arXiv:1704.01358, 2017.", "[85] X. Peng and C. Schmid, \u201cMulti-region two-stream r-cnn for action detection,\u201d in European Conference on Computer Vision, 2016, pp. 744\u2013759.", "[101] L. Song, S. Zhang, G. Yu, and H. Sun, \u201cTacnet: Transition-aware context network for spatio-temporal action detection,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 11\u2009987\u201311\u2009995.", "[81] V. Kalogeiton, P. Weinzaepfel, V. Ferrari, and C. Schmid, \u201cAction tubelet detector for spatio-temporal action localization,\u201d in Proc. Int. Conf. Computer Vision, 2017."]}]}