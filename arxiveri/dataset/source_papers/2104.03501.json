{"title": "Deepi2p: Image-to-point cloud registration via deep classification", "abstract": "This paper presents DeepI2P: a novel approach for cross-modality registration between an image and a point cloud. Given an image (e.g. from a rgb-camera) and a general point cloud (e.g. from a 3D Lidar scanner) captured at different locations in the same scene, our method estimates the relative rigid transformation between the coordinate frames of the camera and Lidar. Learning common feature descriptors to establish correspondences for the registration is inherently challenging due to the lack of appearance and geometric correlations across the two modalities. We circumvent the difficulty by converting the registration problem into a classification and inverse camera projection optimization problem. A classification neural network is designed to label whether the projection of each point in the point cloud is within or beyond the camera frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results on Oxford Robotcar and KITTI datasets demonstrate the feasibility of our approach. Our source code is available at https://github.com/lijx10/DeepI2P", "authors": ["Jiaxin Li", " Gim Hee Lee"], "pdf_url": "https://arxiv.org/abs/2104.03501", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"2\">Oxford</td><td colspan=\"2\">KITTI</td></tr><tr><th></th><td>RTE (m)</td><td>RRE (\u00b0)</td><td>RTE (m)</td><td>RRE (\u00b0)</td></tr><tr><th>Direct Regression</th><td>5.02\\pm 2.89</td><td>10.45\\pm 16.03</td><td>4.94\\pm 2.87</td><td>21.98\\pm 31.97</td></tr><tr><th>MonoDepth2 [14] + USIP [22]</th><td>33.2\\pm 46.1</td><td>142.5\\pm 139.5</td><td>30.4\\pm 42.9</td><td>140.6\\pm 157.8</td></tr><tr><th>MonoDepth2 [14] + GT-ICP</th><td>\\bf{1.3\\pm 1.5}</td><td>6.4\\pm 7.2</td><td>\\bf{2.9\\pm 2.5}</td><td>12.4\\pm 10.3</td></tr><tr><th>2D3D-MatchNet [11] (No Rot{}^{\\lx@sectionsign})</th><td>1.41</td><td>6.40</td><td>NA</td><td>NA</td></tr><tr><th>Grid Cls. + PnP</th><td>1.91\\pm 1.56</td><td>5.94\\pm 10.72</td><td>3.22\\pm 3.58</td><td>10.15\\pm 13.74</td></tr><tr><th>Frus. Cls. + Inv.Proj. 3D</th><td>2.27\\pm 2.19</td><td>15.00\\pm 13.64</td><td>3.17\\pm 3.22</td><td>15.52\\pm 12.73</td></tr><tr><th>Frus. Cls. + Inv.Proj. 2D</th><td>1.65\\pm 1.36</td><td>\\bf{4.14\\pm 4.90}</td><td>3.28\\pm 3.09</td><td>\\bf{7.56\\pm 7.63}</td></tr></tbody></table><p>{}^{\\lx@sectionsign}Point clouds are not randomly rotated in the experiment setting of 2D3D-MatchNet [11].</p>", "caption": "Table 1: Registration accuracy on the Oxford and KITTI datasets.", "list_citation_info": ["[14] Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE international conference on computer vision, pages 3828\u20133838, 2019.", "[22] Jiaxin Li and Gim Hee Lee. Usip: Unsupervised stable interest point detection from 3d point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 361\u2013370, 2019.", "[11] Mengdan Feng, Sixing Hu, Marcelo H Ang, and Gim Hee Lee. 2d3d-matchnet: Learning to match keypoints across 2d image and 3d point cloud. In 2019 International Conference on Robotics and Automation (ICRA), pages 4790\u20134796. IEEE, 2019."]}]}