{"title": "Motion Representations for Articulated Animation", "abstract": "We propose novel motion representations for animating articulated objects consisting of distinct parts. In a completely unsupervised manner, our method identifies object parts, tracks them in a driving video, and infers their motions by considering their principal axes. In contrast to the previous keypoint-based works, our method extracts meaningful and consistent regions, describing locations, shape, and pose. The regions correspond to semantically relevant and distinct object parts, that are more easily detected in frames of the driving video. To force decoupling of foreground from background, we model non-object related global motion with an additional affine transformation. To facilitate animation and prevent the leakage of the shape of the driving object, we disentangle shape and pose of objects in the region space. Our model can animate a variety of objects, surpassing previous methods by a large margin on existing benchmarks. We present a challenging new benchmark with high-resolution videos and show that the improvement is particularly pronounced when articulated objects are considered, reaching 96.6% user preference vs. the state of the art.", "authors": ["Aliaksandr Siarohin", " Oliver J. Woodford", " Jian Ren", " Menglei Chai", " Sergey Tulyakov"], "pdf_url": "https://arxiv.org/abs/2104.11280", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"3\">5 regions</th><th colspan=\"3\">10 regions</th><th colspan=\"3\">20 regions</th></tr><tr><th></th><th>\\mathcal{L}_{1}</th><th>(AKD, MKR)</th><th>AED</th><th>\\mathcal{L}_{1}</th><th>(AKD, MKR)</th><th>AED</th><th>\\mathcal{L}_{1}</th><th>(AKD, MKR)</th><th>AED</th></tr></thead><tbody><tr><th>FOMM [30]</th><td>0.062</td><td>(7.34, 0.036)</td><td>0.181</td><td>0.056</td><td>(6.53, 0.033)</td><td>0.172</td><td>0.062</td><td>(8.29, 0.049)</td><td>0.196</td></tr><tr><th>Ours w/o bg</th><td>0.061</td><td>(6.67, 0.030)</td><td>0.175</td><td>0.059</td><td>(5.55, 0.026)</td><td>0.165</td><td>0.057</td><td>(5.47, 0.026)</td><td>0.155</td></tr><tr><th>Ours</th><td>0.049</td><td>(6.04, 0.029)</td><td>0.162</td><td>0.047</td><td>(5.59, 0.027)</td><td>0.152</td><td>0.046</td><td>(5.17, 0.026)</td><td>0.141</td></tr></tbody></table>", "caption": "Table 1: Comparing our model with FOMM [30] on TaiChiHD (256), for K= 5, 10 and 20. (Best result in bold.)", "list_citation_info": ["[30] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In Proceedings of the Neural Information Processing Systems Conference, 2019."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Our vs FOMM (%)</th><th>Our vs Standard (%)</th></tr></thead><tbody><tr><td>TaiChiHD (256)</td><td>83.7%</td><td>53.6%</td></tr><tr><td>TED-talks</td><td>96.6%</td><td>65.6%</td></tr></tbody></table>", "caption": "Table 3: User study: second column - the proportion (%) of users that prefer our method over FOMM [30]; third column - the proportion (%) of users that prefer animation via disentanglement over standard animation for our model.", "list_citation_info": ["[30] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In Proceedings of the Neural Information Processing Systems Conference, 2019."]}, {"table": "<table><thead><tr><th>Input</th><th>Ours</th><th>SCOPS [14]</th><th>MSCS [32]</th></tr></thead><tbody><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr><tr><td><img/></td><td><img/><img/></td><td><img/><img/></td><td><img/><img/></td></tr></tbody></table>", "caption": "Figure 9: Additional qualitative co-part segmentation comparisons with recent methods. First column is an input. In next columns, for every method segmentation mask and image with overlayed segmentation are shown.", "list_citation_info": ["[32] Aliaksandr Siarohin, Subhankar Roy, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Motion-supervised co-part segmentation. In Proceedings of the International Conference on Pattern Recognition, 2020.", "[14] Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz. Scops: Self-supervised co-part segmentation. In CVPR, 2019."]}]}