{"title": "All you may need for VQA are image captions", "abstract": "Visual Question Answering (VQA) has benefited from increasingly sophisticated models, but has not enjoyed the same level of engagement in terms of data creation. In this paper, we propose a method that automatically derives VQA examples at volume, by leveraging the abundance of existing image-caption annotations combined with neural models for textual question generation. We show that the resulting data is of high-quality. VQA models trained on our data improve state-of-the-art zero-shot accuracy by double digits and achieve a level of robustness that lacks in the same model trained on human-annotated VQA data.", "authors": ["Soravit Changpinyo", " Doron Kukliansky", " Idan Szpektor", " Xi Chen", " Nan Ding", " Radu Soricut"], "pdf_url": "https://arxiv.org/abs/2205.01883", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"3\">Evaluation Benchmark</td></tr><tr><th>Approach</th><td>VQA2.0</td><td>GQA</td><td>OKVQA</td></tr><tr><th colspan=\"4\">Zero-shot</th></tr><tr><th>\\mathrm{VQ}^{2}\\!\\mathrm{A} COCO, nouns only</th><td>10.5</td><td>-</td><td>-</td></tr><tr><th>COCOQA</th><td>11.7</td><td>4.4</td><td>6.3</td></tr><tr><th>WeaQA ZSL</th><td>46.8</td><td>33.7</td><td>-</td></tr><tr><th>\\mathrm{VQ}^{2}\\!\\mathrm{A} COCO</th><td>60.0</td><td>51.3</td><td>18.0</td></tr><tr><th>\\mathrm{VQ}^{2}\\!\\mathrm{A} CC3M</th><td>56.5</td><td>49.9</td><td>19.1</td></tr><tr><th>\\mathrm{VQ}^{2}\\!\\mathrm{A} CC3M \\xrightarrow{} COCO</th><td>61.1</td><td>52.1</td><td>19.7</td></tr><tr><th>\\mathrm{VQ}^{2}\\!\\mathrm{A} CC3M +D</th><td>57.9</td><td>50.0</td><td>19.8</td></tr><tr><th colspan=\"4\">Fully-supervised</th></tr><tr><th>WeaQA FSL</th><td>65.3</td><td>55.2</td><td>-</td></tr><tr><th>w/o \\mathrm{VQ}^{2}\\!\\mathrm{A} data</th><td>68.8</td><td>61.8</td><td>22.1</td></tr><tr><th>w. \\mathrm{VQ}^{2}\\!\\mathrm{A} COCO</th><td>71.6</td><td>63.3</td><td>36.0</td></tr><tr><th>w. \\mathrm{VQ}^{2}\\!\\mathrm{A} CC3M</th><td>71.3</td><td>63.4</td><td>39.0</td></tr><tr><th>w. \\mathrm{VQ}^{2}\\!\\mathrm{A} CC3M \\xrightarrow{} COCO</th><td>71.4</td><td>64.0</td><td>39.3</td></tr><tr><th>Human performance</th><td>82.4{}^{\\dagger}</td><td>89.3{}^{\\ddagger}</td><td>82.8{}^{\\dagger}</td></tr></tbody></table><p>{}^{\\dagger} from the inter-annotator agreement of ground-truth answers.{}^{\\ddagger} from Hudson and Manning (2019).</p>", "caption": "Table 4: \\mathrm{VQ}^{2}\\!\\mathrm{A} as training data. Accuracy in zero-shot and fully-supervised settings. All results use our architecture, except WeaQA ZSL and WeaQA FSL, which are the zero-shot (ZSL + Patches + Encoder) and fully-supervised (FSL + Patches + Encoder) models in Banerjee et al. (2021), respectively. +D stands for recovered raw CC3M alt-texts with digits.", "list_citation_info": ["Banerjee et al. (2021) Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, and Chitta Baral. 2021. WeaQA: Weak supervision via captions for visual question answering. In Findings of ACL-IJCNLP.", "Hudson and Manning (2019) Drew A. Hudson and Christopher D. Manning. 2019. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR."]}]}