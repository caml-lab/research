{"title": "No rl, no simulation: Learning to navigate without navigating", "abstract": "Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a significant margin. We present NRNS as a strong baseline for any future image-based navigation tasks that use RL or Simulation.", "authors": ["Meera Hahn", " Devendra Chaplot", " Shubham Tulsiani", " Mustafa Mukadam", " James M. Rehg", " Abhinav Gupta"], "pdf_url": "https://arxiv.org/abs/2110.09470", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">Easy</th><th colspan=\"2\">Medium</th><th colspan=\"2\">Hard</th></tr><tr><th>Path Type</th><th>Model</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th></tr></thead><tbody><tr><th rowspan=\"5\">Straight</th><th>RL (10M steps)<sup>*</sup> [13]</th><td>10.50</td><td>6.70</td><td>18.10</td><td>16.17</td><td>11.79</td><td>10.85</td></tr><tr><th>RL (extra data + 50M steps)<sup>*</sup> [13]</th><td>36.30</td><td>34.93</td><td>35.70</td><td>33.98</td><td>5.94</td><td>6.33</td></tr><tr><th>RL (extra data + 100M steps)<sup>*</sup> [13]</th><td>43.20</td><td>38.54</td><td>36.40</td><td>34.89</td><td>7.44</td><td>7.20</td></tr><tr><th>BC w/ ResNet + Metric Map</th><td>24.80</td><td>23.94</td><td>11.50</td><td>11.28</td><td>1.36</td><td>1.26</td></tr><tr><th>BC w/ ResNet + GRU</th><td>34.90</td><td>33.43</td><td>17.60</td><td>17.05</td><td>6.08</td><td>5.93</td></tr><tr><th></th><th>NRNS w/ noise</th><td>64.10</td><td>55.43</td><td>47.90</td><td>39.54</td><td>25.19</td><td>18.09</td></tr><tr><th></th><th>NRNS w/out noise</th><td>68.00</td><td>61.62</td><td>49.10</td><td>44.56</td><td>23.82</td><td>18.28</td></tr><tr><th rowspan=\"5\">Curved</th><th>RL (10M steps)<sup>*</sup> [13]</th><td>7.90</td><td>3.27</td><td>9.50</td><td>7.11</td><td>5.50</td><td>4.72</td></tr><tr><th>RL (extra data + 50M steps)<sup>*</sup> [13]</th><td>18.10</td><td>15.42</td><td>16.30</td><td>14.46</td><td>2.60</td><td>2.23</td></tr><tr><th>RL (extra data + 100M steps)<sup>*</sup> [13]</th><td>22.20</td><td>16.51</td><td>20.70</td><td>18.52</td><td>4.20</td><td>3.71</td></tr><tr><th>BC w/ ResNet + Metric Map</th><td>3.10</td><td>2.53</td><td>0.80</td><td>0.71</td><td>0.20</td><td>0.16</td></tr><tr><th>BC w/ ResNet + GRU</th><td>3.60</td><td>2.86</td><td>1.10</td><td>0.91</td><td>0.50</td><td>0.36</td></tr><tr><th></th><th>NRNS w/ noise</th><td>27.30</td><td>10.55</td><td>23.10</td><td>10.35</td><td>10.50</td><td>5.61</td></tr><tr><th></th><th>NRNS w/out noise</th><td>35.50</td><td>18.38</td><td>23.90</td><td>12.08</td><td>12.50</td><td>6.84</td></tr></tbody></table>", "caption": "Table 1: Comparison of our model (NRNS) with baselines on Image-Goal Navigation on Gibson[41]. We report average Success and Success weighted by inverse Path Length (SPL) @ 1m. Noise refers to injection of sensor &amp; actuation noise into the train videos and test episodes. <sup>*</sup> denotes using simulator.", "list_citation_info": ["Wijmans et al. [2020] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations, 2020.", "Xia et al. [2018] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Computer Vision and Pattern Recognition, 2018."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">Easy</th><th colspan=\"2\">Medium</th><th colspan=\"2\">Hard</th></tr><tr><th>Path Type</th><th>Model</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th></tr></thead><tbody><tr><th rowspan=\"5\">Straight</th><th>RL (10M steps)<sup>*</sup> [13]</th><td>7.50</td><td>4.00</td><td>3.50</td><td>1.73</td><td>1.00</td><td>0.55</td></tr><tr><th>RL (extra data + 50M steps)<sup>*</sup> [13]</th><td>34.50</td><td>30.08</td><td>35.70</td><td>33.80</td><td>10.40</td><td>10.07</td></tr><tr><th>RL (extra data + 100M steps)<sup>*</sup> [13]</th><td>36.40</td><td>30.84</td><td>33.80</td><td>31.42</td><td>12.00</td><td>11.56</td></tr><tr><th>BC w/ ResNet + Metric Map</th><td>25.80</td><td>24.82</td><td>11.30</td><td>10.65</td><td>3.00</td><td>2.93</td></tr><tr><th>BC w/ ResNet + GRU</th><td>30.20</td><td>29.57</td><td>12.70</td><td>12.48</td><td>4.40</td><td>4.34</td></tr><tr><th></th><th>NRNS w/ noise</th><td>63.80</td><td>53.12</td><td>36.20</td><td>26.92</td><td>24.10</td><td>16.93</td></tr><tr><th></th><th>NRNS w/out noise</th><td>64.70</td><td>58.23</td><td>39.70</td><td>32.74</td><td>22.30</td><td>17.33</td></tr><tr><th rowspan=\"5\">Curved</th><th>RL (10M steps)<sup>*</sup> [13]</th><td>4.90</td><td>1.78</td><td>3.20</td><td>1.37</td><td>1.10</td><td>0.46</td></tr><tr><th>RL (extra data + 50M steps)<sup>*</sup> [13]</th><td>15.70</td><td>11.34</td><td>10.70</td><td>9.03</td><td>3.90</td><td>3.57</td></tr><tr><th>RL (extra data + 100M steps)<sup>*</sup> [13]</th><td>17.90</td><td>13.24</td><td>15.00</td><td>12.17</td><td>5.90</td><td>4.87</td></tr><tr><th>BC w/ ResNet + Metric Map</th><td>4.90</td><td>4.23</td><td>1.40</td><td>1.29</td><td>0.40</td><td>0.34</td></tr><tr><th>BC w/ ResNet + GRU</th><td>3.10</td><td>2.61</td><td>0.80</td><td>0.77</td><td>0.10</td><td>0.02</td></tr><tr><th></th><th>NRNS w/ noise</th><td>21.40</td><td>8.19</td><td>15.40</td><td>6.83</td><td>10.0</td><td>4.86</td></tr><tr><th></th><th>NRNS w/out noise</th><td>23.70</td><td>12.68</td><td>16.20</td><td>8.34</td><td>9.10</td><td>5.14</td></tr></tbody></table>", "caption": "Table 2: Comparison of our model (NRNS) with baselines on Image-Goal Navigation on MP3D[42].", "list_citation_info": ["Wijmans et al. [2020] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations, 2020.", "Chang et al. [2017] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environment. International Conference on 3D Vision, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">NRNS Ablation</th><th colspan=\"2\">Easy</th><th colspan=\"2\">Medium</th><th colspan=\"2\">Hard</th></tr><tr><th>Path Type</th><th>\\mathcal{G}_{EA}</th><th>\\mathcal{G}_{T}</th><th>\\mathcal{G}_{D}</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th></tr></thead><tbody><tr><th rowspan=\"4\">Straight</th><th>\u2717</th><th>\u2717</th><th>\u2717</th><td>100.00</td><td>99.75</td><td>100.00</td><td>99.62</td><td>100.00</td><td>99.57</td></tr><tr><th>\u2713</th><th>\u2717</th><th>\u2717</th><td>99.90</td><td>99.05</td><td>98.20</td><td>95.06</td><td>95.91</td><td>90.53</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2717</th><td>79.40</td><td>73.48</td><td>71.30</td><td>67.48</td><td>62.16</td><td>58.06</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2713</th><td>68.00</td><td>61.62</td><td>49.10</td><td>44.56</td><td>23.45</td><td>18.84</td></tr><tr><th rowspan=\"4\">Curved</th><th>\u2717</th><th>\u2717</th><th>\u2717</th><td>100.00</td><td>97.62</td><td>100.00</td><td>97.47</td><td>100.00</td><td>98.18</td></tr><tr><th>\u2713</th><th>\u2717</th><th>\u2717</th><td>99.70</td><td>95.93</td><td>97.50</td><td>90.14</td><td>89.30</td><td>79.95</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2717</th><td>65.00</td><td>56.70</td><td>58.10</td><td>52.51</td><td>47.70</td><td>42.28</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2713</th><td>35.50</td><td>18.38</td><td>23.90</td><td>12.08</td><td>12.50</td><td>6.84</td></tr></tbody></table>", "caption": "Table 3: Ablations of NRNS with baselines on Image-Goal Navigation on Gibson [41]. We report average Success and Success weighted by inverse Path Length (SPL) @ 1m. \u2717 denotes a module being replaced by the ground truth labels and a \u2713 denotes the NRNS module being used. ", "list_citation_info": ["Xia et al. [2018] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Computer Vision and Pattern Recognition, 2018."]}, {"table": "<table><tbody><tr><td></td><th></th><th></th><th colspan=\"2\">Easy</th><th colspan=\"2\">Medium</th></tr><tr><th>Path Type</th><th>Training Data</th><th>Model</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th></tr><tr><td rowspan=\"3\">Straight</td><td>RealEstate10k [40]</td><td>NRNS</td><td>56.42</td><td>48.01</td><td>30.30</td><td>25.67</td></tr><tr><td>MP3D</td><td>NRNS</td><td>59.80</td><td>52.35</td><td>37.00</td><td>31.89</td></tr><tr><td>Gibson</td><td>NRNS</td><td>68.00</td><td>61.62</td><td>49.10</td><td>44.56</td></tr><tr><td></td><td>Gibson</td><td>BC w/ ResNet + GRU</td><td>30.20</td><td>29.57</td><td>12.70</td><td>12.48</td></tr><tr><td rowspan=\"3\">Curved</td><td>RealEstate10k [40]</td><td>NRNS</td><td>21.10</td><td>15.76</td><td>12.90</td><td>5.57</td></tr><tr><td>MP3D</td><td>NRNS</td><td>28.26</td><td>13.59</td><td>11.00</td><td>5.10</td></tr><tr><td>Gibson</td><td>NRNS</td><td>35.50</td><td>18.38</td><td>23.90</td><td>12.08</td></tr><tr><td></td><td>Gibson</td><td>BC w/ ResNet + GRU</td><td>3.10</td><td>2.61</td><td>0.80</td><td>0.77</td></tr></tbody></table>", "caption": "Table 4: Comparison of our model (NRNS) trained with different sets of passive video data and tested on Image-Goal Navigation on Gibson [41]. We report average Success and Success weighted by inverse Path Length (SPL) @ 1m. Results shown are tested without sensor &amp; actuation noise.", "list_citation_info": ["Zhou et al. [2018] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018.", "Xia et al. [2018] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Computer Vision and Pattern Recognition, 2018."]}, {"table": "<table><tbody><tr><td></td><th></th><th></th><th colspan=\"2\">Easy</th><th colspan=\"2\">Medium</th></tr><tr><th>Path Type</th><th>Training Data</th><th>Model</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th></tr><tr><td rowspan=\"3\">Straight</td><td>RealEstate10k [40]</td><td>NRNS</td><td>44.58</td><td>39.27</td><td>15.81</td><td>10.73</td></tr><tr><td>Gibson</td><td>NRNS</td><td>59.20</td><td>54.12</td><td>22.90</td><td>19.34</td></tr><tr><td>MP3D</td><td>NRNS</td><td>64.70</td><td>58.23</td><td>39.70</td><td>32.74</td></tr><tr><td></td><td>MP3D</td><td>BC w/ ResNet + GRU</td><td>30.20</td><td>29.57</td><td>12.70</td><td>12.48</td></tr><tr><td rowspan=\"3\">Curved</td><td>RealEstate10k [40]</td><td>NRNS</td><td>9.43</td><td>4.96</td><td>5.30</td><td>2.86</td></tr><tr><td>Gibson</td><td>NRNS</td><td>12.10</td><td>5.66</td><td>8.48</td><td>4.92</td></tr><tr><td>MP3D</td><td>NRNS</td><td>23.70</td><td>12.68</td><td>16.20</td><td>8.34</td></tr><tr><td></td><td>MP3D</td><td>BC w/ ResNet + GRU</td><td>3.10</td><td>2.61</td><td>0.80</td><td>0.77</td></tr></tbody></table>", "caption": "Table 5: Comparison of our model (NRNS) trained with different sets of passive video data and tested on Image-Goal Navigation on MP3D [42]. We report average Success and Success weighted by inverse Path Length (SPL) @ 1m. Results shown are tested without sensor &amp; actuation noise.", "list_citation_info": ["Zhou et al. [2018] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018.", "Chang et al. [2017] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environment. International Conference on 3D Vision, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">NRNS Ablation</th><th colspan=\"2\">Easy</th><th colspan=\"2\">Medium</th><th colspan=\"2\">Hard</th></tr><tr><th>Path Type</th><th>\\mathcal{G}_{EA}</th><th>\\mathcal{G}_{T}</th><th>\\mathcal{G}_{D}</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th><th>Succ \\uparrow</th><th>SPL \\uparrow</th></tr></thead><tbody><tr><th rowspan=\"4\">Straight</th><th>\u2717</th><th>\u2717</th><th>\u2717</th><td>100.00</td><td>100.00</td><td>99.80</td><td>99.07</td><td>100.00</td><td>99.02</td></tr><tr><th>\u2713</th><th>\u2717</th><th>\u2717</th><td>100.00</td><td>100.00</td><td>99.00</td><td>96.81</td><td>96.10</td><td>92.65</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2717</th><td>74.20</td><td>68.19</td><td>64.50</td><td>60.13</td><td>58.40</td><td>55.38</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2713</th><td>64.70</td><td>58.23</td><td>39.70</td><td>32.74</td><td>22.30</td><td>17.33</td></tr><tr><th rowspan=\"4\">Curved</th><th>\u2717</th><th>\u2717</th><th>\u2717</th><td>100.00</td><td>94.08</td><td>99.90</td><td>95.39</td><td>100.00</td><td>97.00</td></tr><tr><th>\u2713</th><th>\u2717</th><th>\u2717</th><td>100.00</td><td>93.06</td><td>97.70</td><td>90.22</td><td>91.60</td><td>82.87</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2717</th><td>62.20</td><td>53.31</td><td>54.10</td><td>47.51</td><td>51.00</td><td>44.92</td></tr><tr><th>\u2713</th><th>\u2713</th><th>\u2713</th><td>23.70</td><td>12.68</td><td>16.20</td><td>8.34</td><td>9.10</td><td>5.14</td></tr></tbody></table>", "caption": "Table 6: Ablations of NRNS with baselines on Image-Goal Navigation on MP3D [42]. We report average Success and Success weighted by inverse Path Length (SPL) @ 1m. \u2717 denotes a module being replaced by the ground truth labels and a \u2713 denotes the NRNS module being used.", "list_citation_info": ["Chang et al. [2017] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environment. International Conference on 3D Vision, 2017."]}]}