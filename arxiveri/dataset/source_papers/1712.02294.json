{"title": "Joint 3d proposal generation and object detection from view aggregation", "abstract": "We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod", "authors": ["Jason Ku", " Melissa Mozifian", " Jungwook Lee", " Ali Harakeh", " Steven Waslander"], "pdf_url": "https://arxiv.org/abs/1712.02294", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><th colspan=\"2\">Easy</th><th colspan=\"2\">Moderate</th><th colspan=\"2\">Hard</th></tr><tr><td></td><th>AP</th><th>AHS</th><th>AP</th><th>AHS</th><th>AP</th><th>AHS</th></tr><tr><td>Deep3DBox</td><td>5.84</td><td>5.84</td><td>4.09</td><td>4.09</td><td>3.83</td><td>3.83</td></tr><tr><td>MV3D</td><td>83.87</td><td>52.74</td><td>72.35</td><td>43.75</td><td>64.56</td><td>39.86</td></tr><tr><td>Ours (Feature Pyramid)</td><td>84.41</td><td>84.19</td><td>74.44</td><td>74.11</td><td>68.65</td><td>68.28</td></tr></tbody></table>", "caption": "TABLE I: A comparison of the performance of Deep3DBox [14], MV3D [4], and our method evaluated on the car class in the validation set. For evaluation, we show the AP and AHS (in \\%) at 0.7 3D IoU.", "list_citation_info": ["[4] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, \u201cMulti-view 3d object detection network for autonomous driving,\u201d in Computer Vision and Pattern Recognition, 2017. CVPR 2017. IEEE Conference on,.", "[14] A. Mousavian, D. Anguelov, J. Flynn, and J. Kosecka, \u201c3d bounding box estimation using deep learning and geometry,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017."]}, {"table": "<table><tbody><tr><th colspan=\"3\"></th><td colspan=\"3\">AP_{3D}\\ (\\%)</td><td colspan=\"3\">AP_{BEV}(\\%)</td></tr><tr><th>Method</th><th>Runtime (s)</th><th>Class</th><td>Easy</td><td>Moderate</td><td>Hard</td><td>Easy</td><td>Moderate</td><td>Hard</td></tr><tr><th>MV3D [4]</th><th>0.36</th><th rowspan=\"5\">Car</th><td>71.09</td><td>62.35</td><td>55.12</td><td>86.02</td><td>76.90</td><td>68.49</td></tr><tr><th>VoxelNet [9]</th><th>0.23</th><td>77.47</td><td>65.11</td><td>57.73</td><td>89.35</td><td>79.26</td><td>77.39</td></tr><tr><th>F-PointNet [11]</th><th>0.17</th><td>81.20</td><td>70.39</td><td>62.19</td><td>88.70</td><td>84.00</td><td>75.33</td></tr><tr><th>Ours</th><th>0.08</th><td>73.59</td><td>65.78</td><td>58.38</td><td>86.80</td><td>85.44</td><td>77.73</td></tr><tr><th>Ours (Feature Pyramid)</th><th>0.1</th><td>81.94</td><td>71.88</td><td>66.38</td><td>88.53</td><td>83.79</td><td>77.90</td></tr><tr><th>VoxelNet [9]</th><th>0.23</th><th rowspan=\"4\">Ped.</th><td>39.48</td><td>33.69</td><td>31.51</td><td>46.13</td><td>40.74</td><td>38.11</td></tr><tr><th>F-PointNet [11]</th><th>0.17</th><td>51.21</td><td>44.89</td><td>40.23</td><td>58.09</td><td>50.22</td><td>47.20</td></tr><tr><th>Ours</th><th>0.08</th><td>38.28</td><td>31.51</td><td>26.98</td><td>42.51</td><td>35.24</td><td>33.97</td></tr><tr><th>Ours (Feature Pyramid)</th><th>0.1</th><td>50.80</td><td>42.81</td><td>40.88</td><td>58.75</td><td>51.05</td><td>47.54</td></tr><tr><th>VoxelNet [9]</th><th>0.23</th><th rowspan=\"4\">Cyc.</th><td>61.22</td><td>48.36</td><td>44.37</td><td>66.70</td><td>54.76</td><td>50.55</td></tr><tr><th>F-PointNet [11]</th><th>0.17</th><td>71.96</td><td>56.77</td><td>50.39</td><td>75.38</td><td>61.96</td><td>54.68</td></tr><tr><th>Ours</th><th>0.08</th><td>60.11</td><td>44.90</td><td>38.80</td><td>63.66</td><td>47.74</td><td>46.55</td></tr><tr><th>Ours (Feature Pyramid)</th><th>0.1</th><td>64.00</td><td>52.18</td><td>46.61</td><td>68.06</td><td>57.48</td><td>50.77</td></tr></tbody></table>", "caption": "TABLE II: A comparison of the performance of AVOD with the state of the art 3D object detectors evaluated on KITTI\u2019s test set. Results are generated by KITTI\u2019s evaluation server [19].", "list_citation_info": ["[4] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, \u201cMulti-view 3d object detection network for autonomous driving,\u201d in Computer Vision and Pattern Recognition, 2017. CVPR 2017. IEEE Conference on,.", "[19] \u201cKitti 3d object detection benchmark,\u201d http://www.cvlibs.net/datasets/kitti/eval\u02d9object.php?obj\u02d9benchmark=3d, accessed: 2018-02-28.", "[9] Y. Zhou and O. Tuzel, \u201cVoxelnet: End-to-end learning for point cloud based 3d object detection,\u201d arXiv preprint arXiv:1711.06396, 2017.", "[11] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, \u201cFrustum pointnets for 3d object detection from rgb-d data,\u201d arXiv preprint arXiv:1711.08488, 2017."]}]}