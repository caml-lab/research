{"title": "Distilling knowledge from reader to retriever for question answering", "abstract": "The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results.", "authors": ["Gautier Izacard", " Edouard Grave"], "pdf_url": "https://arxiv.org/abs/2012.04584", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td colspan=\"2\">NQ</td><td colspan=\"2\">TriviaQA</td></tr><tr><td></td><td>dev.</td><td>test</td><td>dev.</td><td>test</td></tr><tr><td>DPR (Karpukhin et al., 2020)</td><td>-</td><td>41.5</td><td>-</td><td>57.9</td></tr><tr><td>RAG (Lewis et al., 2020b)</td><td>-</td><td>44.5</td><td>-</td><td>56.1</td></tr><tr><td>ColBERT-QA (Khattab et al., 2020)</td><td>-</td><td>48.2</td><td>-</td><td>63.2</td></tr><tr><td>Fusion-in-Decoder (T5 base) (Izacard &amp; Grave, 2020)</td><td>-</td><td>48.2</td><td>-</td><td>65.0</td></tr><tr><td>Fusion-in-Decoder (T5 large) (Izacard &amp; Grave, 2020)</td><td>-</td><td>51.4</td><td>-</td><td>67.6</td></tr><tr><td>Ours (starting from BERT, T5 base)</td><td>39.3</td><td>40.0</td><td>62.5</td><td>62.7</td></tr><tr><td>Ours (starting from BM25, T5 base)</td><td>47.9</td><td>48.9</td><td>67.7</td><td>67.7</td></tr><tr><td>Ours (starting from DPR, T5 base)</td><td>49.2</td><td>50.1</td><td>68.7</td><td>69.3</td></tr><tr><td>Ours (starting from DPR, T5 large)</td><td>52.7</td><td>54.4</td><td>72.5</td><td>72.5</td></tr></tbody></table>", "caption": "Table 2: Comparison to state-of-the-art models on NaturalQuestions and TriviaQA.", "list_citation_info": ["Khattab et al. (2020) Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with colbert, 2020.", "Izacard & Grave (2020) Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2020.", "Karpukhin et al. (2020) Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.", "Lewis et al. (2020b) Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020b."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"2\">NaturalQuestions</td><td colspan=\"2\">TriviaQA</td></tr><tr><th></th><td>R@20</td><td>R@100</td><td>R@20</td><td>R@100</td></tr><tr><th>DPR (Karpukhin et al., 2020)</th><td>79.4</td><td>86.0</td><td>79.4</td><td>85.0</td></tr><tr><th>ANCE (Xiong et al., )</th><td>82.1</td><td>87.9</td><td>80.3</td><td>85.3</td></tr><tr><th>Starting from BERT</th><td>68.8</td><td>79.5</td><td>78.4</td><td>84.4</td></tr><tr><th>Starting from BM25</th><td>80.0</td><td>87.7</td><td>81.4</td><td>86.5</td></tr><tr><th>Starting from DPR</th><td>84.3</td><td>89.3</td><td>83.6</td><td>87.7</td></tr></tbody></table>", "caption": "Table 3: Retrieval performance on the test sets depending on the initial passages and comparison to the state-of-the-art.", "list_citation_info": ["(37) Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. URL https://arxiv.org/abs/2007.00808.", "Karpukhin et al. (2020) Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Iter.</th><th colspan=\"2\">Rouge-L</th><th colspan=\"2\">Bleu-1</th><th colspan=\"2\">Bleu-4</th><th colspan=\"2\">Meteor</th></tr></thead><tbody><tr><th></th><td></td><td>dev.</td><td>test</td><td>dev.</td><td>test</td><td>dev.</td><td>test</td><td>dev.</td><td>test</td></tr><tr><th>Best from Ko\u010disk\u1ef3 et al. (2018)</th><th>-</th><th>14.5</th><th>14.0</th><th>20.0</th><th>19.1</th><th>2.23</th><th>2.1</th><th>4.6</th><th>4.4</th></tr><tr><th>DPR + FiD</th><td>-</td><td>29.7</td><td>30.8</td><td>33.0</td><td>34.0</td><td>6.7</td><td>6.9</td><td>10.3</td><td>10.8</td></tr><tr><th>Ours starting from BM25</th><th>0</th><th>29.9</th><th>30.3</th><th>34.6</th><th>33.7</th><th>7.1</th><th>6.5</th><th>10.5</th><th>10.4</th></tr><tr><th>Ours starting from BM25</th><td>1</td><td>31.6</td><td>32.0</td><td>34.9</td><td>35.3</td><td>7.6</td><td>7.5</td><td>11.0</td><td>11.1</td></tr></tbody></table>", "caption": "Table 4: Performance on NarrativeQA.", "list_citation_info": ["Ko\u010disk\u1ef3 et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. TACL, 2018."]}]}