{"title": "Fast convergence of DETR with spatially modulated co-attention", "abstract": "The recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR \\cite{carion2020end} from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct regression-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR's convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate the effectiveness of the proposed SMCA.", "authors": ["Peng Gao", " Minghang Zheng", " Xiaogang Wang", " Jifeng Dai", " Hongsheng Li"], "pdf_url": "https://arxiv.org/abs/2101.07448", "list_table_and_caption": [{"table": "<table><tr><td>Model</td><td>Epochs</td><td>GFLOPs</td><td>Params (M)</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><td>DETR-R50 [4]</td><td>500</td><td>86</td><td>41</td><td>42.0</td><td>62.4</td><td>44.2</td><td>20.5</td><td>45.8</td><td>61.1</td></tr><tr><td>DETR-DC5-R50 [4]</td><td>500</td><td>187</td><td>41</td><td>43.3</td><td>63.1</td><td>45.9</td><td>22.5</td><td>47.3</td><td>61.1</td></tr><tr><td>Faster RCNN-FPN-R50 [4]</td><td>36</td><td>180</td><td>42</td><td>40.2</td><td>61.0</td><td>43.8</td><td>24.2</td><td>43.5</td><td>52.0</td></tr><tr><td>Faster RCNN-FPN-R50++ [4]</td><td>108</td><td>180</td><td>42</td><td>42.0</td><td>62.1</td><td>45.5</td><td>26.6</td><td>45.4</td><td>53.4</td></tr><tr><td>Deformable DETR-R50 (Single-scale) [46]</td><td>50</td><td>78</td><td>34</td><td>39.7</td><td>60.1</td><td>42.4</td><td>21.2</td><td>44.3</td><td>56.0</td></tr><tr><td>Deformable DETR-R50 (50 epochs) [46]</td><td>50</td><td>173</td><td>40</td><td>43.8</td><td>62.6</td><td>47.7</td><td>26.4</td><td>47.1</td><td>58.0</td></tr><tr><td>Deformable DETR-R50 (150 epochs) [46]</td><td>150</td><td>173</td><td>40</td><td>45.3</td><td>*</td><td>*</td><td>*</td><td>*</td><td>*</td></tr><tr><td>UP-DETR-R50  [5]</td><td>150</td><td>86</td><td>41</td><td>40.5</td><td>60.8</td><td>42.6</td><td>19.0</td><td>44.4</td><td>60.0</td></tr><tr><td>UP-DETR-R50+ [5]</td><td>300</td><td>86</td><td>41</td><td>42.8</td><td>63.0</td><td>45.3</td><td>20.8</td><td>47.1</td><td>61.7</td></tr><tr><td>TSP-FCOS-R50  [38]</td><td>36</td><td>189</td><td>*</td><td>43.1</td><td>62.3</td><td>47.0</td><td>26.6</td><td>46.8</td><td>55.9</td></tr><tr><td>TSP-RCNN-R50 [38]</td><td>36</td><td>188</td><td>*</td><td>43.8</td><td>63.3</td><td>48.3</td><td>28.6</td><td>46.9</td><td>55.7</td></tr><tr><td>TSP-RCNN+-R50 [38]</td><td>96</td><td>188</td><td>*</td><td>45.0</td><td>64.5</td><td>49.6</td><td>29.7</td><td>47.7</td><td>58.0</td></tr><tr><td>SMCA-R50</td><td>50</td><td>152</td><td>40</td><td>43.7</td><td>63.6</td><td>47.2</td><td>24.2</td><td>47.0</td><td>60.4</td></tr><tr><td>SMCA-R50</td><td>108</td><td>152</td><td>40</td><td>45.6</td><td>65.5</td><td>49.1</td><td>25.9</td><td>49.3</td><td>62.6</td></tr><tr><td>DETR-R101 [4]</td><td>500</td><td>152</td><td>60</td><td>43.5</td><td>63.8</td><td>46.4</td><td>21.9</td><td>48.0</td><td>61.8</td></tr><tr><td>DETR-DC5-R101 [4]</td><td>500</td><td>253</td><td>60</td><td>44.9</td><td>64.7</td><td>47.7</td><td>23.7</td><td>49.5</td><td>62.3</td></tr><tr><td>Faster RCNN-FPN-R101 [4]</td><td>36</td><td>256</td><td>60</td><td>42.0</td><td>62.1</td><td>45.5</td><td>26.6</td><td>45.4</td><td>53.4</td></tr><tr><td>Faster RCNN-FPN-R101+ [4]</td><td>108</td><td>246</td><td>60</td><td>44.0</td><td>63.9</td><td>47.8</td><td>27.2</td><td>48.1</td><td>56.0</td></tr><tr><td>TSP-FCOS-R101 [38]</td><td>36</td><td>255</td><td>*</td><td>44.4</td><td>63.8</td><td>48.2</td><td>27.7</td><td>48.6</td><td>57.3</td></tr><tr><td>TSP-RCNN-R101 [38]</td><td>36</td><td>254</td><td>*</td><td>44.8</td><td>63.8</td><td>49.2</td><td>29.0</td><td>47.9</td><td>57.1</td></tr><tr><td>TSP-RCNN+-R101 [38]</td><td>96</td><td>254</td><td>*</td><td>46.5</td><td>66.0</td><td>51.2</td><td>29.9</td><td>49.7</td><td>59.2</td></tr><tr><td>SMCA-R101</td><td>50</td><td>218</td><td>58</td><td>44.4</td><td>65.2</td><td>48.0</td><td>24.3</td><td>48.5</td><td>61.0</td></tr></table>", "caption": "Table 4: Comparison with DETR-like object detectors on COCO 2017 validation set.", "list_citation_info": ["[5] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training for object detection with transformers. arXiv preprint arXiv:2011.09094, 2020.", "[46] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[38] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set prediction for object detection. arXiv preprint arXiv:2011.10881, 2020.", "[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872, 2020."]}]}