{"title": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2.", "authors": ["Yunjey Choi", " Youngjung Uh", " Jaejun Yoo", " Jung-Woo Ha"], "pdf_url": "https://arxiv.org/abs/1912.01865", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th>Method</th><th>FID</th><th>LPIPS</th></tr></thead><tbody><tr><td>a</td><td>Baseline StarGAN [7]</td><td><p>98.4</p></td><td><p>-</p></td></tr><tr><td>b</td><td>+ Multi-task discriminator</td><td><p>91.4</p></td><td><p>-</p></td></tr><tr><td>c</td><td>+ Tuning (e.g. {R}_{1} regularization)</td><td><p>80.5</p></td><td><p>-</p></td></tr><tr><td>d</td><td>+ Latent code injection</td><td><p>32.3</p></td><td><p>0.312</p></td></tr><tr><td>e</td><td>+ Replace (d) with style code</td><td><p>17.1</p></td><td><p>0.405</p></td></tr><tr><td>f</td><td>+ Diversity regularization</td><td>13.7</td><td>0.452</td></tr></tbody></table>", "caption": "Table 1: Performance of various configurations on CelebA-HQ. Frechet inception distance (FID) indicates the distance between two distributions of real and generated images (lower is better), while learned perceptual image patch similarity (LPIPS) measures the diversity of generated images (higher is better).", "list_citation_info": ["[7] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In CVPR, 2018."]}, {"table": "<table><tbody><tr><th></th><th></th><td colspan=\"2\">CelebA-HQ</td><td colspan=\"2\">AFHQ</td></tr><tr><th colspan=\"2\">Method</th><td>FID</td><td>LPIPS</td><td>FID</td><td>LPIPS</td></tr><tr><th></th><th>MUNIT [16]</th><td>31.4</td><td><p>0.363</p></td><td><p>41.5</p></td><td><p>0.511</p></td></tr><tr><th></th><th>DRIT [28]</th><td><p>52.1</p></td><td><p>0.178</p></td><td><p>95.6</p></td><td><p>0.326</p></td></tr><tr><th></th><th>MSGAN [34]</th><td><p>33.1</p></td><td><p>0.389</p></td><td><p>61.4</p></td><td>0.517</td></tr><tr><th></th><th>StarGAN\u2009v2</th><td>13.7</td><td>0.452</td><td>16.2</td><td><p>0.450</p></td></tr><tr><th></th><th>Real images</th><td>14.8</td><td><p>-</p></td><td><p>12.9</p></td><td><p>-</p></td></tr></tbody></table>", "caption": "Table 2: Quantitative comparison on latent-guided synthesis.The FIDs of real images are computed between the training and test sets. Note that they may not be optimal values since the number of test images is insufficient, but we report them for reference.", "list_citation_info": ["[34] Q. Mao, H.-Y. Lee, H.-Y. Tseng, S. Ma, and M.-H. Yang. Mode seeking generative adversarial networks for diverse image synthesis. In CVPR, 2019.", "[28] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. K. Singh, and M.-H. Yang. Diverse image-to-image translation via disentangled representations. In ECCV, 2018.", "[16] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018."]}, {"table": "<table><tbody><tr><th></th><th></th><td colspan=\"2\">CelebA-HQ</td><td colspan=\"2\">AFHQ</td></tr><tr><th colspan=\"2\">Method</th><td>FID</td><td>LPIPS</td><td>FID</td><td>LPIPS</td></tr><tr><th></th><th>MUNIT [16]</th><td>107.1</td><td><p>0.176</p></td><td><p>223.9</p></td><td><p>0.199</p></td></tr><tr><th></th><th>DRIT [28]</th><td><p>53.3</p></td><td><p>0.311</p></td><td><p>114.8</p></td><td><p>0.156</p></td></tr><tr><th></th><th>MSGAN [34]</th><td><p>39.6</p></td><td><p>0.312</p></td><td><p>69.8</p></td><td><p>0.375</p></td></tr><tr><th></th><th>StarGAN\u2009v2</th><td>23.8</td><td>0.388</td><td>19.8</td><td>0.432</td></tr><tr><th></th><th>Real images</th><td>14.8</td><td><p>-</p></td><td><p>12.9</p></td><td><p>-</p></td></tr></tbody></table>", "caption": "Table 3: Quantitative comparison on reference-guided synthesis. We sample ten reference images to synthesize diverse images.", "list_citation_info": ["[34] Q. Mao, H.-Y. Lee, H.-Y. Tseng, S. Ma, and M.-H. Yang. Mode seeking generative adversarial networks for diverse image synthesis. In CVPR, 2019.", "[28] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. K. Singh, and M.-H. Yang. Diverse image-to-image translation via disentangled representations. In ECCV, 2018.", "[16] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">CelebA-HQ</th><th colspan=\"2\">AFHQ</th></tr><tr><th colspan=\"2\">Method</th><th>Quality</th><th>Style</th><th>Quality</th><th>Style</th></tr></thead><tbody><tr><th></th><th>MUNIT [16]</th><td>6.2</td><td><p>7.4</p></td><td><p>1.6</p></td><td><p>0.2</p></td></tr><tr><th></th><th>DRIT [28]</th><td><p>11.4</p></td><td><p>7.6</p></td><td><p>4.1</p></td><td><p>2.8</p></td></tr><tr><th></th><th>MSGAN [34]</th><td><p>13.5</p></td><td><p>10.1</p></td><td><p>6.2</p></td><td><p>4.9</p></td></tr><tr><th></th><th>StarGAN\u2009v2</th><td>68.9</td><td>74.8</td><td>88.1</td><td>92.1</td></tr></tbody></table>", "caption": "Table 4: Votes from AMT workers for the most preferred method regarding visual quality and style reflection (%). StarGAN\u2009v2 outperforms the baselines with remarkable margins in all aspects.", "list_citation_info": ["[34] Q. Mao, H.-Y. Lee, H.-Y. Tseng, S. Ma, and M.-H. Yang. Mode seeking generative adversarial networks for diverse image synthesis. In CVPR, 2019.", "[28] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. K. Singh, and M.-H. Yang. Diverse image-to-image translation via disentangled representations. In ECCV, 2018.", "[16] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. In ECCV, 2018."]}]}