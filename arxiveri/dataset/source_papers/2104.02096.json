{"title": "Compressing visual-linguistic model via knowledge distillation", "abstract": "Despite exciting progress in pre-training for visual-linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer-based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention distributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Student's detector while the features are from Teacher's own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly improves the performance of small VL models on image captioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its non-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effectiveness of VL distillation in both pre-training and fine-tuning stages.", "authors": ["Zhiyuan Fang", " Jianfeng Wang", " Xiaowei Hu", " Lijuan Wang", " Yezhou Yang", " Zicheng Liu"], "pdf_url": "https://arxiv.org/abs/2104.02096", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">\u2002\u2002Method</td><td rowspan=\"2\"># Param</td><td rowspan=\"2\"># I-T Pairs</td><td rowspan=\"2\">Visual Feat.</td><td rowspan=\"2\">P. D.</td><td rowspan=\"2\">F. D.</td><td colspan=\"4\">COCO Captioning</td><td colspan=\"2\">VQA</td></tr><tr><td>B@4</td><td>M</td><td>C</td><td>S</td><td>test-std</td><td>test-dev</td></tr><tr><td>UVLP [74]</td><td>111.7M</td><td>3M</td><td>ResNeXt101</td><td>\u2717</td><td>\u2717</td><td>36.5</td><td>28.4</td><td>116.9</td><td>21.2</td><td>70.7</td><td>-</td></tr><tr><td>OSCAR{}_{\\text{B}} [37]</td><td>111.7M</td><td>7M</td><td>R101-F</td><td>\u2717</td><td>\u2717</td><td>36.5</td><td>30.3</td><td>123.7</td><td>23.1</td><td>73.4</td><td>73.2</td></tr><tr><td>MiniVLM [65]</td><td>34.5M</td><td>7M</td><td>TEE</td><td>\u2717</td><td>\u2717</td><td>34.3</td><td>28.1</td><td>116.7</td><td>21.3</td><td>-</td><td>-</td></tr><tr><td>MiniVLM [65]</td><td>34.5M</td><td>14M</td><td>TEE</td><td>\u2717</td><td>\u2717</td><td>35.6</td><td>28.6</td><td>119.8</td><td>21.6</td><td>69.4</td><td>69.1</td></tr><tr><td rowspan=\"4\">DistillVLM</td><td rowspan=\"4\">34.5M</td><td rowspan=\"4\">7M</td><td rowspan=\"4\">TEE</td><td>\u2717</td><td>\u2717</td><td>34.0</td><td>28.0</td><td>115.7</td><td>21.1</td><td>69.0</td><td>68.8</td></tr><tr><td>\u2717</td><td>\u2713</td><td>\\cellcolorgray!1534.5</td><td>28.2</td><td>\\cellcolorgray!20117.1</td><td>21.5</td><td>\\cellcolorgray!1069.2</td><td>\\cellcolorgray!1069.0</td></tr><tr><td>\u2713</td><td>\u2717</td><td>\\cellcolorgray!2535.2</td><td>28.6</td><td>\\cellcolorgray!25120.1</td><td>21.9</td><td>\\cellcolorgray!2569.7</td><td>\\cellcolorgray!2569.6</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\\cellcolorgray!3535.6</td><td>28.7</td><td>\\cellcolorgray!35120.8</td><td>22.1</td><td>\\cellcolorgray!2569.8</td><td>\\cellcolorgray!2569.6</td></tr></tbody></table>", "caption": "Table 1: DistillVLM\u2009distills from stronger VL model (as Teacher), and retains high accuracy on COCO captioning task under different evaluating metrics, regardless of the effect brought by the lightweight visual feature extractor (TEE v.s. R101-F).Our model shows competitive results comparing to MiniVLM [65], even only half of the image-text pairs (# I-T Pairs) are available for pre-training. The VL distillation strategy brings consistent improvement in both the pre-training stage (P.D.) and fine-tuning stage (F.D.). All captioning methods are shown with cross-entropy optimization.", "list_citation_info": ["[65] J. Wang, X. Hu, P. Zhang, X. Li, L. Wang, L. Zhang, J. Gao, and Z. Liu. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946, 2020.", "[74] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, and J. Gao. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34-07, pages 13041\u201313049, 2020.", "[37] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020."]}, {"table": "<table><thead><tr><th rowspan=\"2\">\u2003\u2002Methods</th><th colspan=\"4\">COCO Captioning</th><th>VQA</th></tr><tr><th>B@4</th><th>M</th><th>C</th><th>S</th><th>test-dev</th></tr><tr><th>VL Pre-training [37]</th><th>33.0</th><th>27.3</th><th>110.6</th><th>20.4</th><th>68.5</th></tr></thead><tbody><tr><th>Textual Distill</th><td>\\cellcolorgray!1034.1</td><td>27.7</td><td>\\cellcolorgray!10114.3</td><td>20.9</td><td>\\cellcolorgray!1069.0</td></tr><tr><th>MSE + Layerwise</th><td>\\cellcolorgray!1034.2</td><td>27.8</td><td>\\cellcolorgray!10114.8</td><td>21.1</td><td>\\cellcolorgray!1069.2</td></tr><tr><th>MSE + Last-layer {}^{*}</th><td>\\cellcolorgray!533.3</td><td>27.6</td><td>\\cellcolorgray!5112.4</td><td>20.7</td><td>\\cellcolorgray!568.5</td></tr><tr><th>MSE + Last-layer</th><td>\\cellcolorgray!1534.3</td><td>27.8</td><td>\\cellcolorgray!15115.3</td><td>21.2</td><td>\\cellcolorgray!1569.4</td></tr><tr><th>NCE + Last-layer {}^{*}</th><td>\\cellcolorgray!1534.3</td><td>27.9</td><td>\\cellcolorgray!15115.4</td><td>21.2</td><td>\\cellcolorgray!1269.3</td></tr><tr><th>NCE + Last-layer</th><td>\\cellcolorgray!2034.6</td><td>27.9</td><td>\\cellcolorgray!25115.6</td><td>21.3</td><td>\\cellcolorgray!2069.4</td></tr></tbody></table>", "caption": "Table 3: Ablation of DistillVLM using different distillation strategies, i.e.\u2009, layer-to-layer distillation or last-layer distillation, using mean-square-error distance (MSE) or noise-contrastive (NCE) loss. Textual Distill represents applying the distillation only to the textual tokens without using visual tokens.Captioning results are reported after 20 epochs of training/distillation on VL-7M with cross-entropy optimization.  {}^{*} is the result using mean-pooled token embedding for distillation.", "list_citation_info": ["[37] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"3\">     Method</th><th></th><td colspan=\"6\">1K Test Set</td><td colspan=\"6\">5K Test Set</td></tr><tr><th></th><td colspan=\"3\">Text Retrieval</td><td colspan=\"3\">Image Retrieval</td><td colspan=\"3\">Text Retrieval</td><td colspan=\"3\">Image Retrieval</td></tr><tr><th></th><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><th>PixelBERT [23]</th><th>\u2717</th><td>77.8</td><td>95.4</td><td>98.2</td><td>64.1</td><td>91.0</td><td>96.2</td><td>53.4</td><td>80.4</td><td>88.5</td><td>41.1</td><td>69.7</td><td>80.5</td></tr><tr><th>Unicoder-VL [34]</th><th>\u2717</th><td>84.3</td><td>97.3</td><td>99.3</td><td>69.7</td><td>93.5</td><td>97.2</td><td>62.3</td><td>87.1</td><td>92.8</td><td>46.7</td><td>76.0</td><td>85.3</td></tr><tr><th>OSCAR-\\text{base} [37]</th><th>\u2717</th><td>88.4</td><td>99.1</td><td>99.8</td><td>75.7</td><td>95.2</td><td>98.3</td><td>70.0</td><td>91.1</td><td>95.5</td><td>54.0</td><td>80.8</td><td>88.5</td></tr><tr><th>MiniVLM [65]</th><th>\u2717</th><td>81.1</td><td>96.1</td><td>99.2</td><td>68.5</td><td>93.0</td><td>97.1</td><td>58.8</td><td>85.1</td><td>91.7</td><td>45.0</td><td>74.1</td><td>84.0</td></tr><tr><th>Baseline</th><th>\u2717</th><td>77.3</td><td>95.0</td><td>98.5</td><td>64.6</td><td>92.0</td><td>96.7</td><td>54.4</td><td>81.4</td><td>89.3</td><td>41.3</td><td>71.2</td><td>81.9</td></tr><tr><th>DistillVLM</th><th>\u2713</th><td>80.0</td><td>95.5</td><td>98.5</td><td>68.3</td><td>92.3</td><td>96.9</td><td>58.3</td><td>84.1</td><td>91.3</td><td>43.9</td><td>73.7</td><td>83.3</td></tr><tr><th>\u2003\u2002\\Delta</th><th></th><td>+2.7</td><td>+0.5</td><td>+0.0</td><td>+3.7</td><td>+0.3</td><td>+0.2</td><td>+3.9</td><td>+2.7</td><td>+2.0</td><td>+2.6</td><td>+2.5</td><td>+1.4</td></tr></tbody></table>", "caption": "Table 6: Results of DistillVLM\u2009with and without VL distillation transferring to Image-Text Retrieval task on COCO dataset.", "list_citation_info": ["[23] Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020.", "[34] G. Li, N. Duan, Y. Fang, M. Gong, and D. Jiang. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34-07, pages 11336\u201311344, 2020.", "[65] J. Wang, X. Hu, P. Zhang, X. Li, L. Wang, L. Zhang, J. Gao, and Z. Liu. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946, 2020.", "[37] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020."]}]}