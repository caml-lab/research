{"title": "Vidtr: Video transformer without convolutions", "abstract": "We introduce Video Transformer (VidTr) with separable-attention for video classification. Comparing with commonly used 3D networks, VidTr is able to aggregate spatio-temporal information via stacked attentions and provide better performance with higher efficiency. We first introduce the vanilla video transformer and show that transformer module is able to perform spatio-temporal modeling from raw pixels, but with heavy memory usage. We then present VidTr which reduces the memory cost by 3.3$\\times$ while keeping the same performance. To further optimize the model, we propose the standard deviation based topK pooling for attention ($pool_{topK\\_std}$), which reduces the computation by dropping non-informative features along temporal dimension. VidTr achieves state-of-the-art performance on five commonly used datasets with lower computational requirement, showing both the efficiency and effectiveness of our design. Finally, error analysis and visualization show that VidTr is especially good at predicting actions that require long-term temporal reasoning.", "authors": ["Yanyi Zhang", " Xinyu Li", " Chunhui Liu", " Bing Shuai", " Yi Zhu", " Biagio Brattoli", " Hao Chen", " Ivan Marsic", " Joseph Tighe"], "pdf_url": "https://arxiv.org/abs/2104.11746", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td>Input</td><td>GFLOPs</td><td>Lat.</td><td>top-1</td><td>top-5</td></tr><tr><td>I3D50 [60]</td><td>32\\times 2</td><td>167</td><td>74.4</td><td>75.0</td><td>92.2</td></tr><tr><td>I3D101 [60]</td><td>32\\times 2</td><td>342</td><td>118.3</td><td>77.4</td><td>92.7</td></tr><tr><td>NL50 [55]</td><td>32\\times 2</td><td>282</td><td>53.3</td><td>76.5</td><td>92.6</td></tr><tr><td>NL101 [55]</td><td>32\\times 2</td><td>544</td><td>134.1</td><td>77.7</td><td>93.3</td></tr><tr><td>TEA50 [34]</td><td>16\\times 2</td><td>70</td><td>-</td><td>76.1</td><td>92.5</td></tr><tr><td>TEINet [39]</td><td>16\\times 2</td><td>66</td><td>49.5</td><td>76.2</td><td>92.5</td></tr><tr><td>CIDC [32]</td><td>32\\times 2</td><td>101</td><td>82.3</td><td>75.5</td><td>92.1</td></tr><tr><td>SF50 8\\times8 [19]</td><td>(32+8)\\times 2</td><td>66</td><td>49.3</td><td>77.0</td><td>92.6</td></tr><tr><td>SF101 8\\times8 [19]</td><td>(32+8)\\times 2</td><td>106</td><td>71.9</td><td>77.5</td><td>92.3</td></tr><tr><td>SF101 16\\times8 [19]</td><td>(64+16)\\times 2</td><td>213</td><td>124.3</td><td>78.9</td><td>93.5</td></tr><tr><td>TPN50 [60]</td><td>32\\times 2</td><td>199</td><td>89.3</td><td>77.7</td><td>93.3</td></tr><tr><td>TPN101 [60]</td><td>32\\times 2</td><td>374</td><td>133.4</td><td>78.9</td><td>93.9</td></tr><tr><td>CorrNet50 [53]</td><td>32\\times 2</td><td>115</td><td>-</td><td>77.2</td><td>N/A</td></tr><tr><td>CorrNet101 [53]</td><td>32\\times 2</td><td>187</td><td>-</td><td>78.5</td><td>N/A</td></tr><tr><td>X3D-XXL [18]</td><td>16\\times 5</td><td>196</td><td>-</td><td>80.4</td><td>94.6</td></tr><tr><td>Vanilla-Tr</td><td>8\\times 8</td><td>89</td><td>32.8</td><td>77.5</td><td>93.2</td></tr><tr><td>VidTr-S</td><td>8\\times 8</td><td>89</td><td>36.2</td><td>77.7</td><td>93.3</td></tr><tr><td>VidTr-M</td><td>16\\times 4</td><td>179</td><td>61.1</td><td>78.6</td><td>93.5</td></tr><tr><td>VidTr-L</td><td>32\\times 2</td><td>351</td><td>110.2</td><td>79.1</td><td>93.9</td></tr><tr><td>En-I3D-50-101</td><td>32\\times 2</td><td>509</td><td>192.7</td><td>77.7</td><td>93.2</td></tr><tr><td>En-I3D-TPN-101</td><td>32\\times 2</td><td>541</td><td>207.8</td><td>79.1</td><td>94.0</td></tr><tr><td>En-VidTr-S</td><td>8\\times 8</td><td>130</td><td>73.2</td><td>79.4</td><td>94.0</td></tr><tr><td>En-VidTr-M</td><td>16\\times 4</td><td>220</td><td>98.1</td><td>79.7</td><td>94.2</td></tr><tr><td>En-VidTr-L</td><td>32\\times 2</td><td>392</td><td>147.2</td><td>80.5</td><td>94.6</td></tr></tbody></table>", "caption": "Table 2: Results on Kinetics-400 dataset. We report top-1 accuracy(%) on the validation set. The \u2018Input\u2019 column indicates what frames of the 64 frame clip are actually sent to the network. n\\times s input indicates we feed n frames to the network sampled every s frames. Lat. stands for the latency on single crop.", "list_citation_info": ["[18] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.", "[39] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. TEINet: Towards an Efficient Architecture for Video Recognition. In The Conference on Artificial Intelligence (AAAI), 2020.", "[19] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 6202\u20136211, 2019.", "[55] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794\u20137803, 2018.", "[32] Xinyu Li, Bing Shuai, and Joseph Tighe. Directional temporal modeling for action recognition. In European Conference on Computer Vision, pages 275\u2013291. Springer, 2020.", "[60] Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, and Bolei Zhou. Temporal pyramid network for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 591\u2013600, 2020.", "[34] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 909\u2013918, 2020.", "[53] Heng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli. Video modeling with correlation networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 352\u2013361, 2020."]}, {"table": "<table><tbody><tr><td>Model</td><td>Input</td><td>Res.</td><td>GFLOPs</td><td>Latency(ms)</td><td>top-1</td></tr><tr><td>TSM [37]</td><td>8fTSN</td><td>256</td><td>69</td><td>29</td><td>74.7</td></tr><tr><td>TEA [34]</td><td>16\\times4</td><td>256</td><td>70</td><td>-</td><td>76.1</td></tr><tr><td>3DEffi-B4 [18]</td><td>16\\times5</td><td>224</td><td>7</td><td>-</td><td>72.4</td></tr><tr><td>TEINet [39]</td><td>16\\times4</td><td>256</td><td>33</td><td>36</td><td>74.9</td></tr><tr><td>X3D-M [18]</td><td>16\\times5</td><td>224</td><td>5</td><td>40.9</td><td>74.6</td></tr><tr><td>X3D-L [18]</td><td>16\\times5</td><td>312</td><td>19</td><td>59.4</td><td>76.8</td></tr><tr><td>C-VidTr-S</td><td>8\\times8</td><td>224</td><td>39</td><td>17.5</td><td>75.7</td></tr><tr><td>C-VidTr-M</td><td>16\\times4</td><td>224</td><td>59</td><td>26.1</td><td>76.7</td></tr></tbody></table>", "caption": "Table 3: Comparison of VidTr to other fast networks. We present the number of views used for evaluation and FLOPs required for each view. The latency denotes the total time required to get the reported top-1 score.<sup>1</sup><sup>1</sup>1we measure latency of X3D using the authors\u2019 code and fast depth convolution patch: https://github.com/facebookresearch/SlowFast/blob/master/projects/x3d/README.md, which only has models for X3D-M and X3D-L and not the XL and XXL variants", "list_citation_info": ["[34] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 909\u2013918, 2020.", "[39] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. TEINet: Towards an Efficient Architecture for Video Recognition. In The Conference on Artificial Intelligence (AAAI), 2020.", "[18] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.", "[37] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019."]}, {"table": "<table><tbody><tr><th>Model</th><td>Input</td><td>K700</td><td>Chad</td><td>SS</td><td>UCF</td><td>HM</td></tr><tr><th>I3D [7]</th><td>32\\times2</td><td>58.7</td><td>32.9</td><td>50.0</td><td>95.1</td><td>74.3</td></tr><tr><th>TSM [37]</th><td>8(TSN)</td><td>-</td><td>-</td><td>59.3</td><td>94.5</td><td>70.7</td></tr><tr><th>I3D101 [59]</th><td>32 \\times 4</td><td>40.3</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><th>CSN152 [49]</th><td>32\\times 2</td><td>70.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>TEINet[39]</th><td>16 (TSN)</td><td>-</td><td>-</td><td>62.1</td><td>96.7</td><td>73.3</td></tr><tr><th>SF101 [19]</th><td>64\\times2</td><td>70.2</td><td>-</td><td>60.9</td><td>-</td><td>-</td></tr><tr><th>SF101-NL [19]</th><td>64\\times2</td><td>70.6</td><td>45.2</td><td>-</td><td>-</td><td>-</td></tr><tr><th>X3D-XL [18]</th><td>16 \\times 5</td><td>-</td><td>47.1</td><td>-</td><td>-</td><td>-</td></tr><tr><th>VidTr-M</th><td>16 \\times 4</td><td>69.5</td><td>-</td><td>61.9</td><td>96.6</td><td>74.4</td></tr><tr><th>VidTr-L</th><td>32 \\times 2</td><td>70.2</td><td>43.5</td><td>63.0</td><td>96.7</td><td>74.4</td></tr><tr><th>En-VidTr-L</th><td>32 \\times 2</td><td>70.8</td><td>47.3</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 6: Results on Kinetics-700 dataset (K700), Charades dataset (Chad), something-something-V2 dataset (SS), UCF-101 and HMDB (HM) dataset. The evaluation metrics are mean average precision (mAP) in percentage for Charades (32\\times4 input is used), top-1 accuracy for Kinetics 700, something-something-V2 (TSN styled dataloader is used), UCF and HMDB.", "list_citation_info": ["[18] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.", "[39] Zhaoyang Liu, Donghao Luo, Yabiao Wang, Limin Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Tong Lu. TEINet: Towards an Efficient Architecture for Video Recognition. In The Conference on Artificial Intelligence (AAAI), 2020.", "[49] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification with channel-separated convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5552\u20135561, 2019.", "[19] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE International Conference on Computer Vision, pages 6202\u20136211, 2019.", "[59] Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, and Bolei Zhou. Temporal Pyramid Network for Action Recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[37] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.", "[7] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017."]}]}