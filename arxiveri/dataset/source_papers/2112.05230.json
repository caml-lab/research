{"title": "Injecting semantic concepts into end-to-end image captioning", "abstract": "Tremendous progress has been made in recent years in developing better image captioning models, yet most of them rely on a separate object detector to extract regional features. Recent vision-language studies are shifting towards the detector-free trend by leveraging grid representations for more flexible model training and faster inference speed. However, such development is primarily focused on image understanding tasks, and remains less investigated for the caption generation task. In this paper, we are concerned with a better-performing detector-free image captioning model, and propose a pure vision transformer-based image captioning model, dubbed as ViTCAP, in which grid representations are used without extracting the regional features. For improved performance, we introduce a novel Concept Token Network (CTN) to predict the semantic concepts and then incorporate them into the end-to-end captioning. In particular, the CTN is built on the basis of a vision transformer and is designed to predict the concept tokens through a classification task, from which the rich semantic information contained greatly benefits the captioning task. Compared with the previous detector-based models, ViTCAP drastically simplifies the architectures and at the same time achieves competitive performance on various challenging image captioning datasets. In particular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split, 93.8 and 108.6 CIDEr scores on nocaps, and Google-CC captioning datasets, respectively.", "authors": ["Zhiyuan Fang", " Jianfeng Wang", " Xiaowei Hu", " Lin Liang", " Zhe Gan", " Lijuan Wang", " Yezhou Yang", " Zicheng Liu"], "pdf_url": "https://arxiv.org/abs/2112.05230", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">V. ENC.</td><td rowspan=\"2\"># I-T</td><td colspan=\"5\">Cross-Entropy Loss</td><td colspan=\"5\">CIDEr Optimization</td></tr><tr><td>B@4</td><td>M</td><td>R</td><td>C</td><td>S</td><td>B@4</td><td>M</td><td>R</td><td>C</td><td>S</td></tr><tr><td><p>{}^{{\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{Detector}}}}\\text{ w.o. }\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{VLP}}}}</p></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RFNet [31]</td><td>Ensemble</td><td><p>  \u2717</p></td><td>35.8</td><td>27.4</td><td>56.5</td><td>112.5</td><td>20.5</td><td>36.5</td><td>27.7</td><td>57.3</td><td>121.9</td><td>21.2</td></tr><tr><td>BUTD [4]</td><td>F-RCNN{}_{101}</td><td><p>  \u2717</p></td><td>36.2</td><td>27.0</td><td>56.4</td><td>113.5</td><td>20.3</td><td>36.3</td><td>27.7</td><td>56.9</td><td>120.1</td><td>21.4</td></tr><tr><td>LBPF [76]</td><td>F-RCNN{}_{101}</td><td><p>  \u2717</p></td><td>37.4</td><td>28.1</td><td>57.5</td><td>116.4</td><td>21.2</td><td>38.3</td><td>28.5</td><td>58.4</td><td>127.6</td><td>22.0</td></tr><tr><td>SGAE [75]</td><td>F-RCNN{}_{101}</td><td><p>  \u2717</p></td><td>36.9</td><td>27.7</td><td>57.2</td><td>116.7</td><td>20.9</td><td>38.4</td><td>28.4</td><td>58.6</td><td>127.8</td><td>22.1</td></tr><tr><td>AoANet [27]</td><td>F-RCNN{}_{101}</td><td><p>  \u2717</p></td><td>37.2</td><td>28.4</td><td>57.5</td><td>119.8</td><td>21.3</td><td>38.9</td><td>29.2</td><td>58.8</td><td>129.8</td><td>22.4</td></tr><tr><td>M{}^{2} Transfm. [11]</td><td>F-RCNN{}_{101}</td><td><p>  \u2717</p></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>39.1</td><td>29.2</td><td>58.6</td><td>131.2</td><td>22.6</td></tr><tr><td>X-LAN [53]</td><td>F-RCNN{}_{101}</td><td><p>  \u2717</p></td><td>38.2</td><td>28.8</td><td>58.0</td><td>122.0</td><td>21.9</td><td>39.5</td><td>29.5</td><td>59.2</td><td>132.0</td><td>23.4</td></tr><tr><td>RSTNet [81]</td><td>RESNeXt{}_{152}</td><td><p>  \u2717</p></td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>40.1</td><td>29.8</td><td>59.5</td><td>135.6</td><td>23.3</td></tr><tr><td><p>{}^{{\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{Detector-Free}}}}\\text{ w.o. }\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{VLP}}}}</p></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViTCAP    (Ours)</td><td>ViT{}_{b}</td><td><p>   \u2717</p></td><td>35.7</td><td>28.8</td><td>57.6</td><td>121.8</td><td>22.1</td><td>40.1</td><td>29.4</td><td>59.4</td><td>133.1</td><td>23.0</td></tr><tr><td><p>{}^{{\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{Detector}}}}\\text{ w. }\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{VLP}}}}</p></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>UVLP [82]</td><td>F-RCNN{}_{101}</td><td><p> 4M</p></td><td>36.5</td><td>28.4</td><td>-</td><td>116.9</td><td>21.2</td><td>39.5</td><td>29.3</td><td>-</td><td>129.3</td><td>23.2</td></tr><tr><td>MiniVLM [67]</td><td>Eff-DET</td><td><p>14M</p></td><td>35.6</td><td>28.6</td><td>-</td><td>119.8</td><td>21.6</td><td>39.2</td><td>29.7</td><td>-</td><td>131.7</td><td>23.5</td></tr><tr><td>DistillVLM [18]</td><td>Eff-DET</td><td><p> 7M</p></td><td>35.6</td><td>28.7</td><td>-</td><td>120.8</td><td>22.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>OSCAR{}_{\\text{b}} [38]</td><td>F-RCNN{}_{101}</td><td><p> 7M</p></td><td>36.5</td><td>30.3</td><td>-</td><td>123.7</td><td>23.1</td><td>40.5</td><td>29.7</td><td>-</td><td>137.6</td><td>22.8</td></tr><tr><td>UNIMO{}_{\\text{b}} [37]</td><td>F-RCNN{}_{101}</td><td><p> 9M</p></td><td>38.8</td><td>-</td><td>-</td><td>124.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VL-T5 [10]</td><td>F-RCNN{}_{101}</td><td><p> 9M</p></td><td>-</td><td>-</td><td>-</td><td>116.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VinVL{}_{\\text{b}} [79]</td><td>RESNeXt{}_{152}</td><td><p> 9M</p></td><td>38.2</td><td>30.3</td><td>-</td><td>129.3</td><td>23.6</td><td>40.9</td><td>30.9</td><td>-</td><td>140.4</td><td>25.1</td></tr><tr><td><p>{}^{{\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{Detector-Free}}}}\\text{ w. }\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{VLP}}}}</p></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ViLT-CAP{}^{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}{\\ \\spadesuit}}</td><td>ViT{}_{b}</td><td><p> 10M</p></td><td>33.7</td><td>27.7</td><td>56.1</td><td>113.5</td><td>20.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>E2E-VLP [72]</td><td>ResNet{}_{50}</td><td><p>  6M</p></td><td>36.2</td><td>-</td><td>-</td><td>117.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ViTCAP{}^{*}   (Ours)</td><td>ViT{}_{b}</td><td><p> 10M</p></td><td>36.3</td><td>29.3</td><td>58.1</td><td>125.2</td><td>22.6</td><td>41.2</td><td>30.1</td><td>60.1</td><td>138.1</td><td>24.1</td></tr></table>", "caption": "Table 1: Performance comparisons on COCO-caption\u2004Karpathy split [32], where B@4, M, R, C denote BLEU@4, METEOR, ROUGE-L, CIDEr and SPICE scores. All values are reported as percentages (%). We compare the ViTCAP\u2004with previous state-of-the-art detector-based baselines (without the VLP) in the first section, and detector-based baselines (with large scale pre-training) in the third section, and the detector-free methods with pre-training in the last section.V. ENC. denotes visual encoders for feature extraction; # I-T refers to the number of image-text pairs used in pre-training (in millions). ViTCAP{}^{*} is a larger version of ViTCAP with more parameters.{}^{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}{\\spadesuit}} is the results we achieved using the ViLT [33] pre-trained checkpoint for image captioning task (see Appendix for more explanation).", "list_citation_info": ["[53] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10971\u201310980, 2020.", "[79] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358, 2021.", "[33] Wonjae Kim, Son Bokyung, Kim Ildoo, and Wonjae Kim. Vilt: Vision-and-language transformer without convolution or region supervision. International Conference on Machine Learning, 2021.", "[81] Xuying Zhang, Xiaoshuai Sun, Yunpeng Luo, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Feiyue Huang, and Rongrong Ji. Rstnet: Captioning with adaptive attention on visual and non-visual words. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15465\u201315474, 2021.", "[76] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring visual relationship for image captioning. In Proceedings of the European conference on computer vision (ECCV), pages 684\u2013699, 2018.", "[18] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng Liu. Compressing visual-linguistic model via knowledge distillation. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.", "[37] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409, 2020.", "[72] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804, 2021.", "[32] Karpathy. Karpathy/neuraltalk: Neuraltalk is a python+numpy project for learning multimodal recurrent neural networks that describe images with sentences.", "[31] Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, and Tong Zhang. Recurrent fusion network for image captioning. In Proceedings of the European Conference on Computer Vision (ECCV), pages 499\u2013515, 2018.", "[82] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13041\u201313049, 2020.", "[75] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. Auto-encoding scene graphs for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10685\u201310694, 2019.", "[67] Jianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946, 2020.", "[11] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. Meshed-memory transformer for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10578\u201310587, 2020.", "[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086, 2018.", "[38] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.", "[27] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4634\u20134643, 2019.", "[10] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. arXiv preprint arXiv:2102.02779, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Concept Source</td><td colspan=\"4\">     COCO Captioning</td><td></td></tr><tr><td>B@4</td><td>  M</td><td>  R</td><td>  C</td><td>  S</td></tr><tr><td>\u2717</td><td>33.9</td><td>27.8</td><td>56.4</td><td>114.8</td><td>21.3</td></tr><tr><td>BUTD [4]</td><td>35.0</td><td>28.2</td><td>56.9</td><td>117.4</td><td>21.3</td></tr><tr><td>VinVL [79]</td><td>35.6</td><td>28.6</td><td>57.4</td><td>119.7</td><td>21.8</td></tr><tr><td>CAPTION</td><td>35.6</td><td>28.7</td><td>57.6</td><td>120.9</td><td>21.8</td></tr><tr><td>VinVL \\rightarrow CAP.{}^{\\spadesuit}</td><td>35.9</td><td>28.6</td><td>57.6</td><td>121.3</td><td>21.9</td></tr><tr><td>CAPTION{}^{\\spadesuit}</td><td>35.7</td><td>28.8</td><td>57.6</td><td>121.8</td><td>22.1</td></tr></table>", "caption": "Table 2: Adopting various sources of semantic concept leads to different performances. \u201cCAPTION\u201d represents the baseline extracting keywords from open-form captions; \u201c{}^{\\spadesuit}\u201d is the baseline using all words in captions as target concepts; \u201cBUTD\u201d and \u201cVinVL\u201d represent using the object tags produced by the object detector from [4] and [79] as target semantic concepts, respectively. \u201cVinVL \\rightarrow CAP.\u201d represents adopting detector tags [79] during first stage of concept classification and using caption extracted tags during the second stage.", "list_citation_info": ["[79] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358, 2021.", "[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">    Cross-Entropy Loss</td><td></td></tr><tr><td>B@4</td><td>M</td><td>R</td><td>C</td><td>S</td></tr><tr><td>ViT/B</td><td>33.9</td><td>27.8</td><td>56.4</td><td>114.8</td><td>21.3</td></tr><tr><td>ViT/B+\u2005KD</td><td>35.4</td><td>28.5</td><td>57.5</td><td>120.0</td><td>21.7</td></tr><tr><td>ViT/B+\u2005CTN-TAG</td><td>35.2</td><td>28.0</td><td>57.0</td><td>117.1</td><td>21.4</td></tr><tr><td>ViT/B+\u2005OD-TAG</td><td>34.3</td><td>28.2</td><td>57.4</td><td>117.4</td><td>21.7</td></tr><tr><td>ViTCAP+\u2005CTN-TOK</td><td>35.7</td><td>28.8</td><td>57.6</td><td>121.8</td><td>22.1</td></tr><tr><td>ViTCAP+\u2005CTN-TOK+\u2005PRE+KD</td><td>36.3</td><td>29.3</td><td>58.1</td><td>125.2</td><td>22.6</td></tr></table>", "caption": "Table 3: Comparisons of ViTCAP\u2004with or without knowledge distillation, large-scale pre-training and with CTN. Performances are reported on COCO-caption Karpathy split optimized by cross-entropy loss. {}_{+\\text{OD-TAG}} indicates the result using the detector produced off-the-shelf tags as [38]. {}_{+\\text{CTN-TOK}} is the result of ViTCAP\u2004using the initialization after first-stage concept classification. {}_{\\color[rgb]{0.6,0.16,0.25}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6,0.16,0.25}\\pgfsys@color@rgb@stroke{0.6}{0.16}{0.25}\\pgfsys@color@rgb@fill{0.6}{0.16}{0.25}{\\text{{KD}}}} and {}_{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{\\text{{PRE}}}} are results obtained with masked token classification distillation and pre-training at scale respectively.", "list_citation_info": ["[38] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020."]}, {"table": "MethodsCC-3M devCIDErFRCNN [8]89.2Ultra [8]93.7ViLT-CAP [33]{}^{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}{\\spadesuit}}83.8VinVL [79]{}^{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}{\\spadesuit}}103.4CC-3M [9]100.9CC-12M [9]105.4ViTCAP\\textbf{108.6}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{ +3.2}}}nocaps validation setMethodsin-domainnear-domainout-of-domainoverallCSCSCSCSHuman84.414.385.014.395.714.087.114.2UpDown [1]78.111.657.710.331.38.355.310.1UpDown + CBS80.012.073.611.366.49.773.111.1UpDown + ELMO + CBS80.012.073.611.366.49.773.111.1OSCAR [38]79.612.366.111.545.39.763.811.2OSCAR + CBS83.412.081.612.077.610.681.111.7VIVO  [25]90.413.084.912.583.010.785.312.2VIVO + CBS92.212.987.812.687.511.588.312.4ViTCAP99.313.290.412.978.111.989.212.7ViTCAP\u2004+ CBS98.713.392.313.3{95.4}{12.7}93.813.0{}_{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\Delta}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+6.5}}}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+0.4}}}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+4.5}}}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+0.7}}}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+7.9}}}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+1.2}}}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+5.5}}}{}_{\\text{\\color[rgb]{0.0,0.4,0.23}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.4,0.23}\\pgfsys@color@rgb@stroke{0.0}{0.4}{0.23}\\pgfsys@color@rgb@fill{0.0}{0.4}{0.23}{+0.6}}}", "caption": "Table 4: Performances of ViTCAP\u2004model on Conceptual Captions (Google-CC 3M dev-split) [61] benchmark. We compare with the baseline methods FRCNN [8], Ultra [8] and [9]. The ViLT-CAP{}^{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}{\\spadesuit}} and VinVL represent our reproduced results with pre-trained checkpoint from [33] and [79].", "list_citation_info": ["[33] Wonjae Kim, Son Bokyung, Kim Ildoo, and Wonjae Kim. Vilt: Vision-and-language transformer without convolution or region supervision. International Conference on Machine Learning, 2021.", "[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Guided open vocabulary image captioning with constrained beam search. arXiv preprint arXiv:1612.00576, 2016.", "[8] Soravit Changpinyo, Bo Pang, Piyush Sharma, and Radu Soricut. Decoupled box proposal and featurization with ultrafine-grained semantic labels improve image captioning and visual question answering. arXiv preprint arXiv:1909.02097, 2019.", "[61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.", "[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8948\u20138957, 2019.", "[25] Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training. arXiv e-prints, pages arXiv\u20132009, 2020.", "[38] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.", "[79] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint arXiv:2103.15358, 2021.", "[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021."]}, {"table": "<table><tr><td>Source</td><td>VG [34]</td><td>COCO [43]</td><td>CC  [9]</td><td>SBU [51]</td></tr><tr><td>Image</td><td>108K</td><td>113K</td><td>3.1M</td><td>875K</td></tr><tr><td>Text</td><td>5.4M</td><td>567K</td><td>3.1M</td><td>875K</td></tr></table>", "caption": "Table 6: Statistics of the VL pre-training datasets. ", "list_citation_info": ["[51] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24:1143\u20131151, 2011.", "[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.", "[34] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017.", "[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021."]}, {"table": "<table><tr><td>Methods</td><td>SMURF</td></tr><tr><td>w/ only periods removed</td><td></td></tr><tr><td>VinVL</td><td>0.66</td></tr><tr><td>M{}^{2} Transformer</td><td>0.49</td></tr><tr><td>X-Transformer</td><td>0.51</td></tr><tr><td>ViTCAP</td><td>0.55</td></tr><tr><td>w/ all punctuation removed</td><td></td></tr><tr><td>VinVL</td><td>0.59</td></tr><tr><td>M{}^{2} Transformer</td><td>0.42</td></tr><tr><td>X-Transformer</td><td>0.46</td></tr><tr><td>ViTCAP</td><td>0.49</td></tr></table>", "caption": "Table 11: Performance of ViTCAP comparing with previous models under SMURF [19] metric. Note that this results is evaluated using ViTCAP without pre-training.", "list_citation_info": ["[19] Joshua Feinglass and Yezhou Yang. Smurf: Semantic and linguistic understanding fusion for caption evaluation via typicality analysis. arXiv preprint arXiv:2106.01444, 2021."]}]}