{"title": "Salted: A framework for salient long-tail translation error detection", "abstract": "Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems in MT. Examples include translation of numbers, physical units, dropped content and hallucinations. These errors, which occur rarely and unpredictably in Neural Machine Translation (NMT), greatly undermine the reliability of state-of-the-art MT systems. Consequently, it is important to have visibility into these problems during model development. Towards this direction, we introduce SALTED, a specifications-based framework for behavioral testing of MT models that provides fine-grained views of salient long-tail errors, permitting trustworthy visibility into previously invisible problems. At the core of our approach is the development of high-precision detectors that flag errors (or alternatively, verify output correctness) between a source sentence and a system output. We demonstrate that such detectors could be used not just to identify salient long-tail errors in MT systems, but also for higher-recall filtering of the training data, fixing targeted errors with model fine-tuning in NMT and generating novel data for metamorphic testing to elicit further bugs in models.", "authors": ["Vikas Raunak", " Matt Post", " Arul Menezes"], "pdf_url": "https://arxiv.org/abs/2205.09988", "list_table_and_caption": [{"table": "<table><thead><tr><th>K</th><th>Cases in K</th></tr></thead><tbody><tr><th>100</th><td>0</td></tr><tr><th>1000</th><td>0</td></tr><tr><th>10000</th><td>0</td></tr><tr><th>100000</th><td>3</td></tr></tbody></table>", "caption": "Table 20: Number of flagged Error cases present in the K worst scoring translations, as scored by COMET-QE Rei et al. (2020). The erroneous sentences are present very sparsely even in the lowest 100K scoring translations.", "list_citation_info": ["Rei et al. (2020) Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association for Computational Linguistics."]}, {"table": "<table><thead><tr><th>Threshold</th><th>Sentences</th><th>Cases</th></tr></thead><tbody><tr><td>0.1</td><td>251,518</td><td>26</td></tr><tr><td>0.2</td><td>346,083</td><td>36</td></tr><tr><td>0.3</td><td>522,128</td><td>70</td></tr><tr><td>0.4</td><td>649,828</td><td>79</td></tr><tr><td>0.5</td><td>742,194</td><td>88</td></tr></tbody></table>", "caption": "Table 21: Number of flagged Error cases present in the translations with score less than the threshold, as scored by COMET-QE Rei et al. (2020). The erroneous translations are present across a range of scores.", "list_citation_info": ["Rei et al. (2020) Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association for Computational Linguistics."]}, {"table": "<table><tbody><tr><th>Method</th><td>Instance-Level</td><td>Modularized</td><td>Specification-Based</td><td>High-Precision</td><td>Generative</td></tr><tr><th>SIT</th><td>x</td><td>x</td><td>x</td><td>x</td><td>\u2713</td></tr><tr><th>PatInv</th><td>x</td><td>x</td><td>x</td><td>x</td><td>\u2713</td></tr><tr><th>TransRepair</th><td>x</td><td>x</td><td>x</td><td>x</td><td>\u2713</td></tr><tr><th>RTI</th><td>x</td><td>x</td><td>x</td><td>x</td><td>x</td></tr><tr><th>SALTED</th><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody></table>", "caption": "Table 22: A comparison of existing Behavioral Testing Methods for NMT along five dimensions. The compared methods are: SIT He et al. (2020), PatInv Gupta et al. (2020), TransRepair Sun et al. (2020) and RTI He et al. (2021).", "list_citation_info": ["Gupta et al. (2020) Shashij Gupta, Pinjia He, Clara Meister, and Zhendong Su. 2020. Machine translation testing via pathological invariance. In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, page 863\u2013875, New York, NY, USA. Association for Computing Machinery.", "He et al. (2021) Pinjia He, Clara Meister, and Zhendong Su. 2021. Testing machine translation via referential transparency. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 410\u2013422. IEEE.", "Sun et al. (2020) Zeyu Sun, Jie M. Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020. Automatic testing and improvement of machine translation. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE \u201920, page 974\u2013985, New York, NY, USA. Association for Computing Machinery.", "He et al. (2020) Pinjia He, Clara Meister, and Zhendong Su. 2020. Structure-invariant testing for machine translation. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, ICSE \u201920, page 961\u2013973, New York, NY, USA. Association for Computing Machinery."]}]}