{"title": "On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid", "abstract": "The past years have witnessed great progress on remote sensing (RS) image interpretation and its wide applications. With RS images becoming more accessible than ever before, there is an increasing demand for the automatic interpretation of these images. In this context, the benchmark datasets serve as essential prerequisites for developing and testing intelligent interpretation algorithms. After reviewing existing benchmark datasets in the research community of RS image interpretation, this article discusses the problem of how to efficiently prepare a suitable benchmark dataset for RS image interpretation. Specifically, we first analyze the current challenges of developing intelligent algorithms for RS image interpretation with bibliometric investigations. We then present the general guidances on creating benchmark datasets in efficient manners. Following the presented guidances, we also provide an example on building RS image dataset, i.e., Million-AID, a new large-scale benchmark dataset containing a million instances for RS image scene classification. Several challenges and perspectives in RS image annotation are finally discussed to facilitate the research in benchmark dataset construction. We do hope this paper will provide the RS community an overall perspective on constructing large-scale and practical image datasets for further research, especially data-driven ones.", "authors": ["Yang Long", " Gui-Song Xia", " Shengyang Li", " Wen Yang", " Michael Ying Yang", " Xiao Xiang Zhu", " Liangpei Zhang", " Deren Li"], "pdf_url": "https://arxiv.org/abs/2006.12485", "list_table_and_caption": [{"table": "<table><tr><td>Dataset</td><td>#Cat.</td><td>#Images per cat.</td><td>#Instances</td><td>Resolution (m)</td><td>Image size</td><td>GL/IT/SP</td><td>Year</td></tr><tr><td>UC-Merced [14]</td><td>21</td><td>100</td><td>2,100</td><td>0.3</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2010</td></tr><tr><td>WHU-RS19 [10]</td><td>19</td><td>50 to 61</td><td>1,013</td><td>up to 0.5</td><td>600\\times600</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2012</td></tr><tr><td>RSSCN7 [62]</td><td>7</td><td>400</td><td>2,800</td><td>\u2013</td><td>400\\times400</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2015</td></tr><tr><td>SAT-4 [63]</td><td>4</td><td>89,963 to 178,034</td><td>500,000</td><td>1 to 6</td><td>28\\times28</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2015</td></tr><tr><td>SAT-6 [63]</td><td>6</td><td>10,262 to 150,400</td><td>405,000</td><td>1 to 6</td><td>28\\times28</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2015</td></tr><tr><td>BCS [64]</td><td>2</td><td>1,438</td><td>2,876</td><td>\u2013</td><td>600\\times600</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2015</td></tr><tr><td>RSC11 [65]</td><td>11</td><td>\\sim100</td><td>1,232</td><td>\\sim0.2</td><td>512\\times512</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2016</td></tr><tr><td>SIRI-WHU [66]</td><td>12</td><td>200</td><td>2,400</td><td>2</td><td>200\\times200</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2016</td></tr><tr><td>NWPU-RESISC45 [67]</td><td>45</td><td>700</td><td>31,500</td><td>0.2 to 30</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2016</td></tr><tr><td>AID [52]</td><td>30</td><td>220 to 420</td><td>10,000</td><td>0.5 to 8</td><td>600\\times600</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2017</td></tr><tr><td>RSI-CB256 [68]</td><td>35</td><td>198 to 1,331</td><td>24,000</td><td>0.3 to 3</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2017</td></tr><tr><td>RSI-CB128 [68]</td><td>45</td><td>173 to 1,550</td><td>36,000</td><td>0.3 to 3</td><td>128\\times128</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2017</td></tr><tr><td>Planet-UAS [69]</td><td>17</td><td>\u2013</td><td>40,480</td><td>3 to 5</td><td>256\\times256</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2017</td></tr><tr><td>RSD46-WHU [70]</td><td>46</td><td>500 to 3,000</td><td>117,000</td><td>0.5 to 2</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2017</td></tr><tr><td>MASATI [71]</td><td>7</td><td>304 to 1,789</td><td>7,389</td><td>\u2013</td><td>512\\times512</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>EuroSAT [72]</td><td>10</td><td>2,000 to 3,000</td><td>27,000</td><td>10</td><td>64\\times64</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>PatternNet [73]</td><td>38</td><td>800</td><td>30,400</td><td>0.06 to 4.7</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>fMoW [74]</td><td>62</td><td>\u2013</td><td>132,716</td><td> 0.5</td><td>74\\times58 to 16,184\\times16,288</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>WiDS Datathon 2019 [75]</td><td>2</td><td>\u2013</td><td>20,000</td><td>3</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>Optimal-31 [76]</td><td>31</td><td>60</td><td>1,860</td><td>\u2013</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>BigEarthNet [77]</td><td>43</td><td>328 to 217,119</td><td>590,326</td><td>10,20,60</td><td>20\\times20;60\\times60;120\\times120</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2019</td></tr><tr><td>CLRS [78]</td><td>25</td><td>600</td><td>15,000</td><td>0.26 to 8.85</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2020</td></tr><tr><td>MLRSN [79]</td><td>46</td><td>1,500 to 3,000</td><td>109,161</td><td>0.1 to 10</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2020</td></tr></table><ul><li>*<p>As fMoW is constructed with multiple temporal views for each scene, we ignore the #Images per Cat. and count the total number of unique scene instances, i.e., #Instances. Note that MLRSN is a multi-label scene classification dataset. The Cat., GL, IT, and SP are short for Category, Geographic Location, Imaging \u2004Time, and Sensor parameter, respectively. We present the GL/IT/SP column to indicate whether the datasets provide those complete and accurate meta information.</p></li></ul>", "caption": "TABLE II: Comparison among different RS image scene classification datasets.", "list_citation_info": ["[14] Y. Yang and S. Newsam, \u201cBag-of-visual-words and spatial extensions for land-use classification,\u201d in Proc. Int. Conf. Adv. Geographic Inf. Syst. ACM, 2010, pp. 270\u2013279.", "[69] Planet, \u201cPlanet: Understanding the amazon from apace,\u201d 2017, accessed on December 16, 2020. [Online]. Available: https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/overview", "[73] W. Zhou, S. Newsam, C. Li, and Z. Shao, \u201cPatternNet: A benchmark dataset for performance evaluation of remote sensing image retrieval,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 145, pp. 197\u2013209, 2018.", "[65] L. Zhao, P. Tang, and L. Huo, \u201cFeature significance-based multibag-of-visual-words model for remote sensing image scene classification,\u201d J. Appl. Remote. Sens., vol. 10, no. 3, p. 035004, 2016.", "[64] O. A. B. Penatti, K. Nogueira, and J. A. dos Santos, \u201cDo deep features generalize from everyday objects to remote sensing and aerial scenes domains?\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2015, pp. 44\u201351.", "[10] G.-S. Xia, W. Yang, J. Delon, Y. Gousseau, H. Sun, and H. Ma\u00eetre, \u201cStructural high-resolution satellite image indexing,\u201d in Proc. ISPRS TC VII Symposium - 100 Years ISPRS, 2010, pp. 298\u2013303.", "[63] S. Basu, S. Ganguly, S. Mukhopadhyay, R. DiBiano, M. Karki, and R. Nemani, \u201cDeepSat: A learning framework for satellite imagery,\u201d in Proc. Int. Conf. Adv. Geographic Inf. Syst. ACM, 2015, pp. 1\u201310.", "[76] Q. Wang, S. Liu, J. Chanussot, and X. Li, \u201cScene classification with recurrent attention of vhr remote sensing images,\u201d IEEE Trans. Geosci. Remote Sens., vol. 57, no. 2, pp. 1155\u20131167, 2019.", "[67] G. Cheng, J. Han, and X. Lu, \u201cRemote sensing image scene classification: Benchmark and state of the art,\u201d Proc. IEEE, vol. 105, no. 10, pp. 1865\u20131883, 2017.", "[79] X. Qi, P. Zhu, Y. Wang, L. Zhang, J. Peng, M. Wu, J. Chen, X. Zhao, N. Zang, and P. T. Mathiopoulos, \u201cMLRSNet: A multi-label high spatial resolution remote sensing dataset for semantic scene understanding,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 169, pp. 337\u2013350, 2020.", "[72] P. Helber, B. Bischke, A. Dengel, and D. Borth, \u201cEurosat: A novel dataset and deep learning benchmark for land use and land cover classification,\u201d IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., vol. 12, no. 7, pp. 2217\u20132226, 2019.", "[66] Q. Zhu, Y. Zhong, B. Zhao, G.-S. Xia, and L. Zhang, \u201cBag-of-visual-words scene classifier with local and global features for high spatial resolution remote sensing imagery,\u201d IEEE Geosci. Remote Sens. Lett., vol. 13, no. 6, pp. 747\u2013751, 2016.", "[78] H. Li, H. Jiang, X. Gu, J. Peng, W. Li, L. Hong, and C. Tao, \u201cClrs: Continual learning benchmark for remote sensing image scene classification,\u201d Sensors, vol. 20, no. 4, p. 1226, 2020.", "[71] A.-J. Gallego, A. Pertusa, and P. Gil, \u201cAutomatic ship detection from optical aerial images with convolutional neural networks,\u201d Remote Sens., vol. 10, no. 4, p. 511, 2018.", "[52] G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, and X. Lu, \u201cAID: A benchmark data set for performance evaluation of aerial scene classification,\u201d IEEE Trans. Geosci. Remote Sens., vol. 55, no. 7, pp. 3965\u20133981, 2017.", "[70] Z. Xiao, Y. Long, D. Li, C. Wei, G. Tang, and J. Liu, \u201cHigh-resolution remote sensing image retrieval based on cnns from a dimensional perspective,\u201d Remote Sens., vol. 9, no. 7, p. 725, 2017.", "[75] WiDS, \u201cWomen in data science (WiDS) datathon 2019,\u201d 2019, accessed on December 16, 2020. [Online]. Available: https://www.kaggle.com/c/widsdatathon2019/data", "[62] Q. Zou, L. Ni, T. Zhang, and Q. Wang, \u201cDeep learning based feature selection for remote sensing scene classification,\u201d IEEE Geosci. Remote Sens. Lett., vol. 12, no. 11, pp. 2321\u20132325, 2015.", "[68] H. Li, X. Dou, C. Tao, Z. Wu, J. Chen, J. Peng, M. Deng, and L. Zhao, \u201cRSI-CB: A large-scale remote sensing image classification benchmark using crowdsourced data,\u201d Sensors, vol. 20, no. 6, p. 1594, 2020.", "[77] G. Sumbul, M. Charfuelan, B. Demir, and V. Markl, \u201cBigEarthNet: A large-scale benchmark archive for remote sensing image understanding,\u201d in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2019, pp. 5901\u20135904.", "[74] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, \u201cFunctional map of the world,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6172\u20136180."]}, {"table": "<table><tr><td>Datasets</td><td>Annot.</td><td>#Cat.</td><td>#Instances</td><td>#Images</td><td>Resolution (m)</td><td>Image width</td><td>GL/IT/SP</td><td>Year</td></tr><tr><td>TAS [80]</td><td>HBB</td><td>1</td><td>1,319</td><td>30</td><td>\u2013</td><td>792</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2008</td></tr><tr><td>OIRDS [81]</td><td>OBB</td><td>5</td><td>1,800</td><td>900</td><td>up to 0.08</td><td>256 to 640</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2009</td></tr><tr><td>SZTAKI-INRIA [82]</td><td>OBB</td><td>1</td><td>665</td><td>9</td><td>\u2013</td><td>\\sim800</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2012</td></tr><tr><td>NWPU-VHR10 [83]</td><td>HBB</td><td>10</td><td>3,651</td><td>800</td><td>0.08 to 2</td><td>\\sim1,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2014</td></tr><tr><td>DLR-MVDA [84]</td><td>OBB</td><td>2</td><td>14,235</td><td>20</td><td>0.13</td><td>5,616</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2015</td></tr><tr><td>UCAS-AOD [85]</td><td>OBB</td><td>2</td><td>14,596</td><td>1,510</td><td>\u2013</td><td>\\sim1,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2015</td></tr><tr><td>VEDAI [86]</td><td>OBB</td><td>9</td><td>3,640</td><td>1,210</td><td>0.125</td><td>512;1,024</td><td>\u2713\u2009\u2715\u2009\u2715</td><td>2016</td></tr><tr><td>COWC [87]</td><td>CP</td><td>1</td><td>32,716</td><td>53</td><td>0.15</td><td>2,000 to 19,000</td><td>\u2713\u2009\u2715\u2009\u2715</td><td>2016</td></tr><tr><td>HRSC2016 [88]</td><td>OBB</td><td>26</td><td>2,976</td><td>1,061</td><td>\u2013</td><td>\\sim1,100</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2016</td></tr><tr><td>RSOD [89]</td><td>HBB</td><td>4</td><td>6,950</td><td>976</td><td>0.3 to 3</td><td>\\sim1,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2017</td></tr><tr><td>CARPK [90]</td><td>HBB</td><td>1</td><td>89,777</td><td>1,448</td><td>\u2013</td><td>1,280</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2017</td></tr><tr><td>SSDD/SSDD+ [91]</td><td>HBB/OBB</td><td>1</td><td>2,456</td><td>1,160</td><td>1 to 15</td><td>\\sim500</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2017</td></tr><tr><td>SpaceNet1-6* [92]</td><td>Polygon</td><td>1</td><td>859,982</td><td>\u2013</td><td>up to 0.3</td><td>\u2013</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>LEVIR [93]</td><td>HBB</td><td>3</td><td>11,028</td><td>22,000</td><td>0.2 to 1</td><td>800</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>VisDrone [94]</td><td>HBB</td><td>10</td><td>54,200</td><td>10,209</td><td>\u2013</td><td>2,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>xView [95]</td><td>HBB</td><td>60</td><td>1,000,000</td><td>1,413</td><td>0.3</td><td>\\sim3,000</td><td>\u2713\u2009\u2715\u2009\u2713</td><td>2018</td></tr><tr><td>DOTA-v1.0 [28]</td><td>OBB</td><td>15</td><td>188,282</td><td>2,806</td><td>up to 0.3</td><td>800 to 13,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>ITCVD [96]</td><td>HBB</td><td>1</td><td>29,088</td><td>173</td><td>0.1</td><td>3,744;5,616</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>WHU building dataset [97]</td><td>Polygon</td><td>1</td><td>221,107</td><td>25,420</td><td>0.075 to 2.7</td><td>512</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>DeepGlobe Building [98]</td><td>Polygon</td><td>2</td><td>302,701</td><td>24,586</td><td>0.3</td><td>650</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2018</td></tr><tr><td>OpenSARShip [99]</td><td>Chip</td><td>1</td><td>11,346</td><td>41</td><td>\\sim10</td><td>\u2013</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>CrowdAI Mapping Challenge [100]</td><td>Polygon</td><td>1</td><td>2,910,917</td><td>341,058</td><td>\u2013</td><td>300</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>Airbus Ship Detection Challenge [101]</td><td>Polygon</td><td>1</td><td>\\sim131,000</td><td>208,162</td><td>\u2013</td><td>768</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>iSAID [102, 28]</td><td>Polygon</td><td>15</td><td>655,451</td><td>2,806</td><td>up to 0.3</td><td>800 to 4,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>HRRSD [103]</td><td>HBB</td><td>13</td><td>55,740</td><td>21,761</td><td>0.15 to 1.2</td><td>152 to 10,569</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>DIOR [104]</td><td>HBB</td><td>20</td><td>192,472</td><td>23,463</td><td>0.5 to 30</td><td>800</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>DOTA-v1.5 [105]</td><td>OBB</td><td>16</td><td>402,089</td><td>2,806</td><td>up to 0.3</td><td>800 to 13,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>SAR-Ship-Dataset [106]</td><td>HBB</td><td>1</td><td>5,9535</td><td>43,819</td><td>up to 3</td><td>256</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2019</td></tr><tr><td>AIR-SARShip [107]</td><td>HBB</td><td>1</td><td>2,040</td><td>300</td><td>1;3</td><td>1,000</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2020</td></tr><tr><td>HRSID [108]</td><td>HBB</td><td>1</td><td>16,951</td><td>5,604</td><td>0.5;1;3</td><td>800</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2020</td></tr><tr><td>RarePlanes [109]</td><td>Polygon</td><td>1</td><td>644,258</td><td>50,253</td><td>0.3</td><td>\u2013</td><td>\u2713\u2009\u2715\u2009\u2713</td><td>2020</td></tr><tr><td>DOTA-v2.0 [105]</td><td>OBB</td><td>18</td><td>1,793,658</td><td>11,268</td><td>up to 0.3</td><td>800 to 20,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2020</td></tr></table><ul><li>*<p>For simplicity, we summarize the SpaceNet1\\sim6 as a whole, considering their common functionality for building detection. Note that SpaceNet3/5 are also associated with road network detection. SpaceNet7 [92] with 11,080,000 and xBD [110] with 850,736 building footprints (referenced in Table V) can also be used for building object detection and instance segmentation. CrowdAI Mapping Challenge is presented with the train and validation sets for their accessibility. Annot. refers to the Annotation style of instances, i.e., HBB (Horizontal Bounding Box) and OBB (Oriented Bounding Box). CP refers to the annotation with only the Center Point of an instance.</p></li></ul>", "caption": "TABLE III: Comparison among different RS Image object detection datasets.", "list_citation_info": ["[107] X. Sun, Z. Wang, Y. Sun, W. Diao, Y. Zhang, and K. Fu, \u201cAIR-SARShip-1.0: High-resolution sar ship detection dataset,\u201d J. Radars, vol. 8, no. R19097, p. 852, 2019.", "[103] Y. Zhang, Y. Yuan, Y. Feng, and X. Lu, \u201cHierarchical and robust convolutional neural network for very high-resolution remote sensing object detection,\u201d IEEE Trans. Geosci. Remote Sens., vol. 57, no. 8, pp. 5535\u20135548, 2019.", "[87] T. N. Mundhenk, G. Konjevod, W. A. Sakla, and K. Boakye, \u201cA large contextual dataset for classification, detection and counting of cars with deep learning,\u201d in Proc. Eur. Conf. Comput. Vis., 2016, pp. 785\u2013800.", "[102] S. Waqas Zamir, A. Arora, A. Gupta, S. Khan, G. Sun, F. Shahbaz Khan, F. Zhu, L. Shao, G.-S. Xia, and X. Bai, \u201ciSAID: A large-scale dataset for instance segmentation in aerial images,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, 2019, pp. 28\u201337.", "[106] Y. Wang, C. Wang, H. Zhang, Y. Dong, and S. Wei, \u201cA sar dataset of ship detection for deep learning under complex backgrounds,\u201d Remote Sens., vol. 11, no. 7, p. 765, 2019.", "[110] R. Gupta, B. Goodman, N. Patel, R. Hosfelt, S. Sajeev, E. Heim, J. Doshi, K. Lucas, H. Choset, and M. Gaston, \u201cCreating xBD: A dataset for assessing building damage from satellite imagery,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, June 2019, pp. 10\u201317.", "[99] L. Huang, B. Liu, B. Li, W. Guo, W. Yu, Z. Zhang, and W. Yu, \u201cOpenSARShip: A dataset dedicated to sentinel-1 ship interpretation,\u201d IEEE J. Sel. Top. Appl. Earth Obs .Remote Sens., vol. 11, no. 1, pp. 195\u2013208, 2018.", "[108] S. Wei, X. Zeng, Q. Qu, M. Wang, H. Su, and J. Shi, \u201cHrsid: A high-resolution sar images dataset for ship detection and instance segmentation,\u201d IEEE Access, vol. 8, pp. 120\u2009234\u2013120\u2009254, 2020.", "[105] J. Ding, N. Xue, G.-S. Xia, X. Bai, W. Yang, M. Y. Yang, S. Belongie, J. Luo, M. Datcu, M. Pelillo et al., \u201cObject detection in aerial images: A large-scale benchmark and challenges,\u201d arXiv preprint arXiv:2102.12219, 2021.", "[85] H. Zhu, X. Chen, W. Dai, K. Fu, Q. Ye, and J. Jiao, \u201cOrientation robust object detection in aerial images using deep convolutional neural network,\u201d in Proc. IEEE Int. Conf. Image Process., 2015, pp. 3735\u20133739.", "[86] S. Razakarivony and F. Jurie, \u201cVehicle detection in aerial imagery: A small target detection benchmark,\u201d J. Vis. Commun. Image Represent., vol. 34, pp. 187\u2013203, 2016.", "[91] J. Li, C. Qu, and J. Shao, \u201cShip detection in sar images based on an improved faster r-cnn,\u201d in SAR Big Data Era: Mod., Meth., Appl., 2017, pp. 1\u20136.", "[100] CrowdAI, \u201cCrowdai mapping challenge,\u201d 2018, accessed on February 26, 2021. [Online]. Available: https://www.crowdai.org/challenges/mapping-challenge", "[81] F. Tanner, B. Colder, C. Pullen, D. Heagy, M. Eppolito, V. Carlan, C. Oertel, and P. Sallee, \u201cOverhead imagery research data set\u2014an annotated data library & tools to aid in the development of computer vision algorithms,\u201d in Appl. Imagery Pattern Recognit. Workshops, 2009, pp. 1\u20138.", "[98] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu, F. Hughes, D. Tuia, and R. Raska, \u201cDeepglobe 2018: A challenge to parse the earth through satellite images,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops. IEEE, 2018, pp. 172\u201317\u2009209.", "[82] C. Benedek, X. Descombes, and J. Zerubia, \u201cBuilding development monitoring in multitemporal remotely sensed image pairs with stochastic birth-death dynamics,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 1, pp. 33\u201350, 2012.", "[88] Z. Liu, H. Wang, L. Weng, and Y. Yang, \u201cShip rotated bounding box space for ship extraction from high-resolution optical satellite images with complex backgrounds,\u201d IEEE Geosci. Remote Sens. Lett., vol. 13, no. 8, pp. 1074\u20131078, 2016.", "[96] M. Y. Yang, W. Liao, X. Li, and B. Rosenhahn, \u201cDeep learning for vehicle detection in aerial images,\u201d in Proc. IEEE Int. Conf. Image Processing, 2018, pp. 3079\u20133083.", "[83] G. Cheng, J. Han, P. Zhou, and L. Guo, \u201cMulti-class geospatial object detection and geographic image classification based on collection of part detectors,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 98, pp. 119\u2013132, 2014.", "[93] Z. Zou and Z. Shi, \u201cRandom access memories: A new paradigm for target detection in high resolution aerial remote sensing images,\u201d IEEE Trans. Image Process., vol. 27, no. 3, pp. 1100\u20131111, 2017.", "[104] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, \u201cObject detection in optical remote sensing images: A survey and a new benchmark,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 159, pp. 296 \u2013 307, 2020.", "[28] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo, and L. Zhang, \u201cDOTA: A large-scale dataset for object detection in aerial images,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3974\u20133983.", "[97] S. Ji, S. Wei, and M. Lu, \u201cFully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set,\u201d IEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 574\u2013586, 2018.", "[84] K. Liu and G. M\u00e1ttyus, \u201cFast multiclass vehicle detection on aerial images,\u201d IEEE Geosci. Remote Sens. Lett., vol. 12, no. 9, pp. 1938\u20131942, 2015.", "[94] P. Zhu, L. Wen, X. Bian, L. Haibin, and Q. Hu, \u201cVision meets drones: A challenge,\u201d arXiv preprint arXiv:1804.07437, 2018.", "[89] Y. Long, Y. Gong, Z. Xiao, and Q. Liu, \u201cAccurate object localization in remote sensing images based on convolutional neural networks,\u201d IEEE Trans. Geosci. Remote Sens., vol. 55, no. 5, pp. 2486\u20132498, 2017.", "[101] Airbus, \u201cAirbus ship detection challenge,\u201d 2018, accessed on February 26, 2021. [Online]. Available: https://www.kaggle.com/c/airbus-ship-detection/overview", "[95] D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M. Klaric, Y. Bulatov, and B. McCord, \u201cxview: Objects in context in overhead imagery,\u201d arXiv preprint arXiv:1802.07856, 2018.", "[109] J. Shermeyer, T. Hossler, A. V. Etten, D. Hogan, R. Lewis, and D. Kim, \u201cRareplanes: Synthetic data takes flight,\u201d arXiv preprint arXiv:2006.02963, 2020.", "[92] SpaceNet, \u201cDatasets: the spacenet catalog,\u201d October 2018, accessed on January 15, 2021. [Online]. Available: https://spacenet.ai/datasets/", "[80] G. Heitz and D. Koller, \u201cLearning spatial context: Using stuff to find things,\u201d in Proc. Eur. Conf. Comput. Vis., 2008, pp. 30\u201343.", "[90] M.-R. Hsieh, Y.-L. Lin, and W. H. Hsu, \u201cDrone-based object counting by spatially regularized regional proposal networks,\u201d in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 4145\u20134153."]}, {"table": "<table><tr><td>Datasets</td><td>#Cat.</td><td>#Images</td><td>Resolution (m)</td><td>#Channels</td><td>Image size</td><td>GL/IT/SP</td><td>Year</td></tr><tr><td>Kennedy Space Center [133]</td><td>13</td><td>1</td><td>18</td><td>224</td><td>512\\times614</td><td>\u2715\u2009\u2713\u2009\u2713</td><td>2005</td></tr><tr><td>Botswana [133]</td><td>14</td><td>1</td><td>30</td><td>242</td><td>1,476\\times256</td><td>\u2715\u2009\u2713\u2009\u2713</td><td>2005</td></tr><tr><td>Salinas [126]</td><td>16</td><td>1</td><td>3.7</td><td>224</td><td>512\\times217</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>\u2013</td></tr><tr><td>University of Pavia [126]</td><td>9</td><td>1</td><td>1.3</td><td>115</td><td>610\\times340</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>\u2013</td></tr><tr><td>Pavia Centre [126]</td><td>9</td><td>1</td><td>1.3</td><td>115 bands</td><td>1,096\\times492</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>\u2013</td></tr><tr><td>ISPRS Vaihingen [127]</td><td>6</td><td>33</td><td>0.09</td><td>IR,R,G,DSM,nDSM</td><td>\\sim2,500\\times2,500</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2012</td></tr><tr><td>ISPRS Potsdam [127]</td><td>6</td><td>38</td><td>0.05</td><td>IR,RGB,DSM,nDSM</td><td>6,000\\times6,000</td><td>\u2713\u2009\u2715\u2009\u2713</td><td>2012</td></tr><tr><td>Massachusetts Buildings [116]</td><td>2</td><td>151</td><td>1</td><td>RGB</td><td>1,500\\times1,500</td><td>\u2713\u2009\u2713\u2009\u2715</td><td>2013</td></tr><tr><td>Massachusetts Roads [116]</td><td>2</td><td>1,171</td><td>1</td><td>RGB</td><td>1,500\\times1,500</td><td>\u2713\u2009\u2713\u2009\u2715</td><td>2013</td></tr><tr><td>Indian Pines [134]</td><td>16</td><td>1</td><td>20</td><td>224</td><td>145\\times145</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2015</td></tr><tr><td>Zurich Summer [128]</td><td>8</td><td>20</td><td>0.62</td><td>NIR, RGB</td><td>1,000\\times1,150</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2015</td></tr><tr><td>SPARCS Validation [120]</td><td>7</td><td>80</td><td>30</td><td>11</td><td>1,000\\times1,000</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2016</td></tr><tr><td>Biome [122]</td><td>4</td><td>96</td><td>30</td><td>11</td><td>\\sim9,000\\times9,000</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2017</td></tr><tr><td>Inria [117]</td><td>2</td><td>360</td><td>0.3</td><td>RGB</td><td>5,000\\times5,000</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2017</td></tr><tr><td>EvLab-SS [135]</td><td>10</td><td>60</td><td>0.1 to 2</td><td>RGB</td><td>4,500\\times4,500</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2017</td></tr><tr><td>RIT-18 [136]</td><td>18</td><td>3</td><td>0.047</td><td>6</td><td>9,000\\times6,000</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2017</td></tr><tr><td>CITY-OSM [119]</td><td>3</td><td>1,671</td><td> 0.1</td><td>RGB</td><td>2,500\\times2,500 to 3,300\\times3,300</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2017</td></tr><tr><td>Dstl-SIFD*[114]</td><td>10</td><td>57</td><td>up to 0.3</td><td>up to 16</td><td>\\sim3,350\\times3,400</td><td>\u2713\u2009\u2715\u2009\u2713</td><td>2017</td></tr><tr><td>IEEE GRSS Data Fusion Contest 2017</td><td>17</td><td>30</td><td>1,4</td><td>9</td><td>643\\times666;374\\times515</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2017</td></tr><tr><td>IEEE GRSS Data Fusion Contest 2018</td><td>20</td><td>1</td><td>1</td><td>48</td><td>4,172\\times1,202</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>Aeroscapes [137]</td><td>11</td><td>3,269</td><td>\u2013</td><td>RGB</td><td>720\\times1,280</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>DLRSD [138]</td><td>17</td><td>2,100</td><td>0.3</td><td>RGB</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>DeepGlobe Land Cover [98]</td><td>7</td><td>1,146</td><td>0.5</td><td>RGB</td><td>2,448\\times2,448</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2018</td></tr><tr><td>So2Sat LCZ42 [139]</td><td>17</td><td>400,673</td><td>10</td><td>10</td><td>32\\times32</td><td>\u2713\u2009\u2715\u2009\u2713</td><td>2019</td></tr><tr><td>SEN12MS [130]</td><td>33</td><td>180,662 triplets</td><td>10 to 50</td><td>up to 13</td><td>256\\times256</td><td>\u2713\u2009\u2715\u2009\u2713</td><td>2019</td></tr><tr><td>95-Cloud [121]</td><td>1</td><td>43,902</td><td>30</td><td>NIR,RGB</td><td>384\\times384</td><td>\u2713\u2009\u2715\u2009\u2713</td><td>2019</td></tr><tr><td>Shakeel et al. [118]</td><td>1</td><td>2,682</td><td>0.3</td><td>RGB</td><td>300\\times300</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>ALCD Cloud Masks [123]</td><td>8</td><td>38</td><td>10</td><td>RGB</td><td>1,830\\times1,830</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2019</td></tr><tr><td>SkyScapes [132]</td><td>31</td><td>16</td><td>0.13</td><td>RGB</td><td>5,616\\times3,744</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>DroneDeploy [140]</td><td>7</td><td>55</td><td>0.1</td><td>RGB</td><td>up to 12,039\\times13,854</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2019</td></tr><tr><td>Slovenia LULC [141]</td><td>10</td><td>940</td><td>10</td><td>6</td><td>5,000\\times5,000</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2019</td></tr><tr><td>LandCoverNet [111]</td><td>7</td><td>1,980</td><td>10</td><td>NIR,RGB</td><td>256\\times256</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2020</td></tr><tr><td>UAVid [142]</td><td>8</td><td>420</td><td>\u2013</td><td>RGB</td><td>\\sim4,000\\times2,160</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2020</td></tr><tr><td>GID [5]</td><td>15</td><td>150</td><td>0.8 to 10</td><td>4</td><td>6,800\\times7,200</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2020</td></tr><tr><td>LandCover.ai [112]</td><td>3</td><td>41</td><td>0.25,0.5</td><td>RGB</td><td>9,000\\times9,500;4,200\\times4,700</td><td>\u2713\u2009\u2715\u2009\u2715</td><td>2020</td></tr><tr><td>Agriculture-Vision [113]</td><td>9</td><td>94,986</td><td>0.1;0.15;0.2</td><td>NIR,RGB</td><td>512\\times512</td><td>\u2715\u2009\u2715\u2009\u2713</td><td>2020</td></tr><tr><td>S2CMC* [124]</td><td>18</td><td>513</td><td>20</td><td>13</td><td>1,024\\times1,024</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2020</td></tr></table><ul><li>*<p>The UAVid consists of 30 video sequences captured by unmanned aerial vehicle and each sequence is annotated by every 10 frames, resulting in 420 densely annotated images. The S2CMC is short for Sentinel-2 Cloud Mask Catalogue. The DSTL-SIFD is short for the challenge of Dstl Satellite Imagery Feature Detection.</p></li></ul>", "caption": "TABLE IV: Comparison of different RS image semantic segmentation datasets.", "list_citation_info": ["[142] Y. Lyu, G. Vosselman, G.-S. Xia, A. Yilmaz, and M. Y. Yang, \u201cUavid: A semantic segmentation dataset for uav imagery,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 165, pp. 108\u2013119, 2020.", "[123] L. Baetens, C. Desjardins, and O. Hagolle, \u201cValidation of copernicus sentinel-2 cloud masks obtained from maja, sen2cor, and fmask processors using reference cloud masks generated with a supervised active learning procedure,\u201d Remote Sens., vol. 11, no. 4, p. 433, 2019.", "[139] X. X. Zhu, J. Hu, C. Qiu, Y. Shi, J. Kang, L. Mou, H. Bagheri, M. H\u00e4berle, Y. Hua, R. Huang et al., \u201cSo2sat lcz42: A benchmark dataset for global local climate zones classification,\u201d IEEE Geosci. Remote Sens. Mag., 2020.", "[127] ISPRS-Contest, \u201cIsprs 2d semantic labeling contest,\u201d June 2018, accessed on January 15, 2021. [Online]. Available: http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html", "[135] M. Zhang, X. Hu, L. Zhao, Y. Lv, M. Luo, and S. Pang, \u201cLearning dual multi-scale manifold ranking for semantic segmentation of high-resolution images,\u201d Remote Sens., vol. 9, no. 5, p. 500, 2017.", "[132] S. M. Azimi, C. Henry, L. Sommer, A. Schumann, and E. Vig, \u201cSkyscapes fine-grained semantic understanding of aerial scenes,\u201d in Proc. IEEE Int. Conf. Comput. Vis., 2019, pp. 7393\u20137403.", "[138] Z. Shao, K. Yang, and W. Zhou, \u201cPerformance evaluation of single-label and multi-label remote sensing image retrieval using a dense labeling dataset,\u201d Remote Sens., vol. 10, no. 6, p. 964, 2018.", "[116] V. Mnih, \u201cMachine learning for aerial image labeling,\u201d Ph.D. dissertation, University of Toronto, 2013.", "[124] A. Francis, J. Mrziglod, P. Sidiropoulos, and J.-P. Muller, \u201cSentinel-2 cloud mask catalogue,\u201d 2020, accessed on December 16, 2020. [Online]. Available: https://zenodo.org/record/4172871#.YAPb2dAzZPY", "[117] E. Maggiori, Y. Tarabalka, G. Charpiat, and P. Alliez, \u201cCan semantic labeling methods generalize to any city? the inria aerial image labeling benchmark,\u201d in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2017, pp. 3226\u20133229.", "[141] Sentinelhub, \u201cExample dataset of eopatches for slovenia 2019,\u201d 2019, accessed on December 16, 2020. [Online]. Available: http://eo-learn.sentinel-hub.com/?prefix=", "[130] M. Schmitt, L. H. Hughes, C. Qiu, and X. X. Zhu, \u201cSEN12MS\u2013a curated dataset of georeferenced multi-spectral sentinel-1/2 imagery for deep learning and data fusion,\u201d arXiv preprint arXiv:1906.07789, 2019.", "[121] S. Mohajerani and P. Saeedi, \u201cCloud-Net+: A Cloud Segmentation CNN for Landsat 8 Remote Sensing Imagery Optimized with Filtered Jaccard Loss Function,\u201d arXiv preprint arXiv:2001.08768, 2020.", "[133] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, \u201cInvestigation of the random forest framework for classification of hyperspectral data,\u201d IEEE Trans. Geosci. Remote Sens., vol. 43, no. 3, pp. 492\u2013501, 2005.", "[118] A. Shakeel, W. Sultani, and M. Ali, \u201cDeep built-structure counting in satellite imagery using attention based re-weighting,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 151, pp. 313\u2013321, 2019.", "[5] X.-Y. Tong, G.-S. Xia, Q. Lu, H. Shen, S. Li, S. You, and L. Zhang, \u201cLand-cover classi\ufb01cation with high-resolution remote sensing images using transferable deep models,\u201d Remote Sens. Environ., vol. 237, p. 111322, 2020.", "[98] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu, F. Hughes, D. Tuia, and R. Raska, \u201cDeepglobe 2018: A challenge to parse the earth through satellite images,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops. IEEE, 2018, pp. 172\u201317\u2009209.", "[137] I. Nigam, C. Huang, and D. Ramanan, \u201cEnsemble knowledge transfer for semantic segmentation,\u201d in Proc. IEEE Winter Conf. Appl. Comput. Vis., 2018, pp. 1499\u20131508.", "[119] P. Kaiser, J. D. Wegner, A. Lucchi, M. Jaggi, T. Hofmann, and K. Schindler, \u201cLearning aerial image segmentation from online maps,\u201d IEEE Trans. Geosci. Remote Sens., vol. 55, no. 11, pp. 6054\u20136068, 2017.", "[128] M. Volpi and V. Ferrari, \u201cSemantic segmentation of urban scenes by learning local class interactions,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, June 2015, pp. 1\u20139.", "[126] B. A. M Gra\u00f1a, MA Veganzons, \u201cPavia dataset,\u201d 2 2020, accessed on January 15, 2021. [Online]. Available: http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes", "[114] D. S. T. Laboratory, \u201cDstl satellite imagery feature detection,\u201d 2017, accessed on December 16, 2020. [Online]. Available: https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/overview", "[111] S. Alemohammad, A. Ballantyne, G. Y. Bromberg, K. Booth, L. Nakanuku-Diggs, and A. Miglarese, \u201cLandcovernet: A global land cover classification training dataset,\u201d accessed on January 10, 2021. [Online]. Available: http://registry.mlhub.earth/10.34911/rdnt.d2ce8i", "[120] M. J. Hughes and D. J. Hayes, \u201cAutomated detection of cloud and cloud shadow in single-date landsat imagery using neural networks and spatial post-processing,\u201d Remote Sens., vol. 6, no. 6, pp. 4907\u20134926, 2014.", "[136] R. Kemker, C. Salvaggio, and C. Kanan, \u201cAlgorithms for semantic segmentation of multispectral remote sensing imagery using deep learning,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 145, pp. 60\u201377, 2018.", "[113] M. T. Chiu, X. Xu, Y. Wei, Z. Huang, A. Schwing, R. Brunner, H. Khachatrian, H. Karapetyan, I. Dozier, G. Rose, D. Wilson, A. Tudor, N. Hovakimyan, T. S. Huang, and H. Shi, \u201cAgriculture-vision: A large aerial image database for agricultural pattern analysis,\u201d arXiv preprint arXiv:2001.01306, 2020.", "[122] S. Foga, P. L. Scaramuzza, S. Guo, Z. Zhu, R. D. Dilley, T. Beckmann, G. L. Schmidt, J. L. Dwyer, M. Joseph Hughes, and B. Laue, \u201cCloud detection algorithm comparison and validation for operational landsat data products,\u201d Remote Sens. Environ., vol. 194, pp. 379\u2013390, 2017.", "[112] A. Boguszewski, D. Batorski, N. Ziemba-Jankowska, A. Zambrzycka, and T. Dziedzic, \u201cLandcover. ai: Dataset for automatic mapping of buildings, woodlands and water from aerial imagery,\u201d arXiv preprint arXiv:2005.02264, 2020.", "[140] DroneDeploy, \u201cDronedeploy machine learning segmentation benchmark,\u201d 2019, accessed on December 16, 2020. [Online]. Available: https://github.com/dronedeploy/dd-ml-segmentation-benchmark", "[134] M. F. Baumgardner, L. L. Biehl, and D. A. Landgrebe, \u201c220 band aviris hyperspectral image data set: June 12, 1992 indian pine test site 3,\u201d Sep 2015, accessed on January 10, 2021. [Online]. Available: https://purr.purdue.edu/publications/1947/1"]}, {"table": "<table><tr><td>Datasets</td><td>#Cat.</td><td>#Image pairs</td><td>Resolution (m)</td><td>#Channels</td><td>Image size</td><td>GL/IT/SP</td><td>Year</td></tr><tr><td>SZTAKI AirChange [143]</td><td>2</td><td>13</td><td>1.5</td><td>RGB</td><td>952\\times640</td><td>\u2715\u2009\u2713\u2009\u2715</td><td>2009</td></tr><tr><td>AICD [144]</td><td>2</td><td>1,000</td><td>0.5</td><td>115</td><td>800\\times600</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2011</td></tr><tr><td>Taizhou Data [145]</td><td>4</td><td>1</td><td>30</td><td>6</td><td>400\\times400</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2014</td></tr><tr><td>Kunshan Data [145]</td><td>3</td><td>1</td><td>30</td><td>6</td><td>800\\times800</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2014</td></tr><tr><td>Cross-sensor Bastrop [146]</td><td>2</td><td>4</td><td>30,120</td><td>7,9</td><td>444\\times300; 1,534\\times808</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2015</td></tr><tr><td>MtS-WH [147]</td><td>9</td><td>1</td><td>1</td><td>NIR, RGB</td><td>7,200\\times6,000</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2017</td></tr><tr><td>Yancheng [148]</td><td>4</td><td>2</td><td>30</td><td>242</td><td>400\\times145</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>GETNET dataset [149]</td><td>2</td><td>1</td><td>30</td><td>198</td><td>463\\times241</td><td>\u2715\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>Urban-rural boundary of Wuhan [150]</td><td>20</td><td>1</td><td>4/30</td><td>4, 9</td><td>960\\times960</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>Hermiston City, Oregon [151]</td><td>5</td><td>1</td><td>30</td><td>242</td><td>390\\times200</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>OSCD [152]</td><td>2</td><td>24</td><td>10</td><td>13</td><td>600\\times600</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>WHU building dataset [97]</td><td>2</td><td>1</td><td>0.2</td><td>RGB</td><td>32,507\\times15,354</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2018</td></tr><tr><td>Season-varing dataset [153]</td><td>2</td><td>16,000</td><td>0.03 to 0.1</td><td>RGB</td><td>256\\times256</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2018</td></tr><tr><td>ABCD [154]</td><td>2</td><td>16,950</td><td>0.4</td><td>RGB</td><td>128\\times128;160\\times160</td><td>\u2715\u2009\u2713\u2009\u2715</td><td>2018</td></tr><tr><td>California flood dataset [155]</td><td>2</td><td>1</td><td>5,30</td><td>RGB,11</td><td>1534\\times808</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2019</td></tr><tr><td>L\u00f3pez-Fandi\u00f1o et al. [156]</td><td>5</td><td>2</td><td>20</td><td>224</td><td>984\\times740; 600\\times500</td><td>\u2715\u2009\u2713\u2009\u2713</td><td>2019</td></tr><tr><td>xBD [110]</td><td>6</td><td>11,034</td><td> up to 0.8</td><td>RGB</td><td>1,024\\times1,024</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2019</td></tr><tr><td>HRSCD [157]</td><td>6</td><td>291</td><td>0.5</td><td>RGB</td><td>10,000\\times10,000</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2019</td></tr><tr><td>LEVIR-CD [158]</td><td>2</td><td>637</td><td>0.5</td><td>RGB</td><td>1,024\\times1,024</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2020</td></tr><tr><td>SECOND [131]</td><td>30</td><td>4,214</td><td>0.5 to 3</td><td>RGB</td><td>512\\times512</td><td>\u2715\u2009\u2715\u2009\u2715</td><td>2020</td></tr><tr><td>Google Dataset [159]</td><td>2</td><td>1,067</td><td>0.55</td><td>RGB</td><td>256\\times256</td><td>\u2713\u2009\u2713\u2009\u2715</td><td>2020</td></tr><tr><td>Zhang et al. [160]</td><td>2</td><td>4</td><td>2;2.4;5.8</td><td>NIR, RGB</td><td>1,431\\times1,431; 458\\times559; 1,154\\times740</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2020</td></tr><tr><td>Hi-UCD [115]</td><td>9</td><td>1,293</td><td>0.1</td><td>RGB</td><td>1,024\\times1,024</td><td>\u2013/\u2013/Y</td><td>2020</td></tr><tr><td>SpaceNet7[92]</td><td>\u2013</td><td>24</td><td>4</td><td>RGB</td><td>\u2013</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2020</td></tr><tr><td>S2MTCP [129]</td><td>2</td><td>1,520</td><td>up to 10</td><td>13</td><td>600\\times600</td><td>\u2713\u2009\u2713\u2009\u2713</td><td>2021</td></tr></table>", "caption": "TABLE V: Comparison of different RS Image change detection datasets.", "list_citation_info": ["[150] D. He, Y. Zhong, and L. Zhang, \u201cLand cover change detection based on spatial-temporal sub-pixel evolution mapping: A case study for urban expansion,\u201d in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2018, pp. 1970\u20131973.", "[110] R. Gupta, B. Goodman, N. Patel, R. Hosfelt, S. Sajeev, E. Heim, J. Doshi, K. Lucas, H. Choset, and M. Gaston, \u201cCreating xBD: A dataset for assessing building damage from satellite imagery,\u201d in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, June 2019, pp. 10\u201317.", "[143] C. Benedek and T. Szir\u00e1nyi, \u201cChange detection in optical aerial images by a multilayer conditional mixed markov model,\u201d IEEE Trans. Geosci. Remote Sens., vol. 47, no. 10, pp. 3416\u20133430, 2009.", "[160] M. Zhang and W. Shi, \u201cA feature difference convolutional neural network-based change detection method,\u201d IEEE Trans. Geosci. Remote Sens., vol. 58, no. 10, pp. 7232\u20137246, 2020.", "[152] R. C. Daudt, B. Le Saux, A. Boulch, and Y. Gousseau, \u201cUrban change detection for multispectral earth observation using convolutional neural networks,\u201d in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2018, pp. 2115\u20132118.", "[154] A. Fujita, K. Sakurada, T. Imaizumi, R. Ito, S. Hikosaka, and R. Nakamura, \u201cDamage detection from aerial images via convolutional neural networks,\u201d in Proc. IAPR Int. Conf. Mach. Vis. Applicat., 2017, pp. 5\u20138.", "[149] Q. Wang, Z. Yuan, Q. Du, and X. Li, \u201cGETNET: A general end-to-end 2-d cnn framework for hyperspectral image change detection,\u201d IEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 3\u201313, 2019.", "[158] H. Chen and Z. Shi, \u201cA spatial-temporal attention-based method and a new dataset for remote sensing image change detection,\u201d Remote Sens., vol. 12, no. 10, p. 1662, 2020.", "[151] J. L\u00f3pez-Fandi\u00f1o, A. S. Garea, D. B. Heras, and F. Arg\u00fcello, \u201cStacked autoencoders for multiclass change detection in hyperspectral images,\u201d in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2018, pp. 1906\u20131909.", "[156] J. L\u00f3pez-Fandi\u00f1o, D. B. Heras, F. Arg\u00fcello, and M. Dalla Mura, \u201cGpu framework for change detection in multitemporal hyperspectral images,\u201d Int. J. Parallel Program., vol. 47, no. 2, pp. 272\u2013292, 2019.", "[97] S. Ji, S. Wei, and M. Lu, \u201cFully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set,\u201d IEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 574\u2013586, 2018.", "[129] M. Leenstra, D. Marcos, F. Bovolo, and D. Tuia, \u201cSelf-supervised pre-training enhances change detection in sentinel-2 imagery,\u201d arXiv preprint arXiv:2101.08122, 2021.", "[155] L. T. Luppino, F. M. Bianchi, G. Moser, and S. N. Anfinsen, \u201cUnsupervised image regression for heterogeneous change detection,\u201d IEEE Trans. Geosci. Remote Sens., vol. 57, no. 12, pp. 9960\u20139975, 2019.", "[159] D. Peng, L. Bruzzone, Y. Zhang, H. Guan, H. Ding, and X. Huang, \u201cSemicdnet: A semisupervised convolutional neural network for change detection in high resolution remote-sensing images,\u201d IEEE Trans. Geosci. Remote Sens., pp. 1\u201316, 2020.", "[153] M. Lebedev, Y. V. Vizilter, O. Vygolov, V. Knyaz, and A. Y. Rubis, \u201cChange detection in remote sensing images using conditional adversarial networks.\u201d Int. arch. photogramm. remote sens. spat. inf. sci., vol. 42, no. 2, 2018.", "[131] K. Yang, G.-S. Xia, Z. Liu, B. Du, W. Yang, and M. Pelillo, \u201cAsymmetric siamese networks for semantic change detection,\u201d arXiv preprint arXiv:2010.05687, 2020.", "[147] C. Wu, L. Zhang, and B. Du, \u201cKernel slow feature analysis for scene change detection,\u201d IEEE Trans. Geosci. Remote Sens., vol. 55, no. 4, pp. 2367\u20132384, 2017.", "[145] C. Wu, B. Du, and L. Zhang, \u201cSlow feature analysis for change detection in multispectral imagery,\u201d IEEE Trans. Geosci. Remote Sens., vol. 52, no. 5, pp. 2858\u20132874, 2013.", "[148] A. Song, J. Choi, Y. Han, and Y. Kim, \u201cChange detection in hyperspectral images using recurrent 3d fully convolutional networks,\u201d Remote Sens., vol. 10, no. 11, p. 1827, 2018.", "[146] M. Volpi, G. Camps-Valls, and D. Tuia, \u201cSpectral alignment of multi-temporal cross-sensor images with automated kernel canonical correlation analysis,\u201d ISPRS J. Photogrammetry Remote Sens., vol. 107, pp. 50 \u2013 63, 2015.", "[92] SpaceNet, \u201cDatasets: the spacenet catalog,\u201d October 2018, accessed on January 15, 2021. [Online]. Available: https://spacenet.ai/datasets/", "[144] N. Bourdis, D. Marraud, and H. Sahbi, \u201cConstrained optical flow for aerial image change detection,\u201d in Proc. IEEE Int. Geosci. Remote Sens. Symp., 2011, pp. 4176\u20134179.", "[157] C. D. Rodrigo, L. S. Bertrand, B. Alexandre, and G. Yann, \u201cMultitask learning for large-scale semantic change detection,\u201d Comput. Vis. Image Underst., vol. 187, p. 102783, 2019.", "[115] S. Tian, Y. Zhong, A. Ma, and Z. Zheng, \u201cHi-UCD: A large-scale dataset for urban semantic change detection in remote sensing imagery,\u201d arXiv preprint arXiv:2011.03247, 2020."]}]}