{"title": "Progressive coordinate transforms for monocular 3d object detection", "abstract": "Recognizing and localizing objects in the 3D space is a crucial ability for an AI agent to perceive its surrounding environment. While significant progress has been achieved with expensive LiDAR point clouds, it poses a great challenge for 3D object detection given only a monocular image. While there exist different alternatives for tackling this problem, it is found that they are either equipped with heavy networks to fuse RGB and depth information or empirically ineffective to process millions of pseudo-LiDAR points. With in-depth examination, we realize that these limitations are rooted in inaccurate object localization. In this paper, we propose a novel and lightweight approach, dubbed {\\em Progressive Coordinate Transforms} (PCT) to facilitate learning coordinate representations. Specifically, a localization boosting mechanism with confidence-aware loss is introduced to progressively refine the localization prediction. In addition, semantic image representation is also exploited to compensate for the usage of patch proposals. Despite being lightweight and simple, our strategy leads to superior improvements on the KITTI and Waymo Open Dataset monocular 3D detection benchmarks. At the same time, our proposed PCT shows great generalization to most coordinate-based 3D detection frameworks. The code is available at: https://github.com/amazon-research/progressive-coordinate-transforms .", "authors": ["Li Wang", " Li Zhang", " Yi Zhu", " Zhi Zhang", " Tong He", " Mu Li", " Xiangyang Xue"], "pdf_url": "https://arxiv.org/abs/2108.05793", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Factor</th><td colspan=\"3\">PatchNet* [\\rm AP_{3D}/\\rm AP_{BEV}]</td><td colspan=\"3\">Pseudo-LiDAR* [\\rm AP_{3D}/\\rm AP_{BEV}]</td></tr><tr><td>Mod.</td><td>Easy</td><td>Hard</td><td>Mod.</td><td>Easy</td><td>Hard</td></tr><tr><th>Baseline</th><td>26.31/34.14</td><td>36.40/46.80</td><td>21.07/28.04</td><td>23.04/31.06</td><td>32.27/42.45</td><td>19.67/25.67</td></tr><tr><th>dimension</th><td>27.26/34.62</td><td>40.32/47.24</td><td>24.29/28.38</td><td>25.88/31.97</td><td>36.09/44.35</td><td>20.88/26.60</td></tr><tr><th>rotation</th><td>26.25/34.04</td><td>36.09/46.25</td><td>23.49/27.99</td><td>23.88/31.31</td><td>32.42/42.74</td><td>19.85/26.07</td></tr><tr><th>x</th><td>32.80/41.43</td><td>45.60/56.22</td><td>27.38/34.63</td><td>28.36/36.77</td><td>39.69/50.78</td><td>25.08/29.92</td></tr><tr><th>y</th><td>30.16/34.14</td><td>40.94/46.80</td><td>24.58/28.04</td><td>25.53/31.06</td><td>35.19/42.45</td><td>20.69/25.67</td></tr><tr><th>z</th><td>42.42/53.48</td><td>55.42/68.29</td><td>35.54/45.60</td><td>38.37/50.81</td><td>50.04/63.96</td><td>32.24/43.32</td></tr><tr><th>location(xyz)</th><td>72.58/75.27</td><td>81.41/85.14</td><td>57.69/66.10</td><td>64.36/73.37</td><td>79.13/83.77</td><td>55.64/58.27</td></tr></tbody></table>", "caption": "Table 1: Probing investigation on coordinate-based methods, PatchNet (28) and Pseudo-LiDAR (43). We examine the potential improvement by replacing the predicted factor with the corresponding ground truth. * indicates our reproduced performance. We can see that coordinate-based methods mostly suffer from inaccurate localization.", "list_citation_info": ["[28] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang. Rethinking pseudo-lidar representation. In ECCV, 2020.", "[43] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In CVPR, 2019."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">\\rm AP_{3D}</td><td colspan=\"3\">\\rm AP_{BEV}</td></tr><tr><td>Mod.</td><td>Easy</td><td>Hard</td><td>Mod.</td><td>Easy</td><td>Hard</td></tr><tr><th>Pseudo-LiDAR*(43)</th><td>23.04</td><td>32.27</td><td>19.67</td><td>31.06</td><td>42.45</td><td>25.67</td></tr><tr><th>Pseudo-LiDAR + CLB</th><td>24.14</td><td>34.46</td><td>20.16</td><td>32.41</td><td>44.98</td><td>26.82</td></tr><tr><th>Pseudo-LiDAR + CLB + GCE</th><td>24.43</td><td>34.34</td><td>20.18</td><td>32.50</td><td>45.35</td><td>26.91</td></tr></tbody></table>", "caption": "Table 7: Comparison of generalization on KITTI validation set. * denotes that the method is reproduced by ourselves.", "list_citation_info": ["[43] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In CVPR, 2019."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Depth</th><th rowspan=\"2\">Type</th><th colspan=\"3\">\\rm AP_{3D}</th><th colspan=\"3\">\\rm AP_{BEV}</th></tr><tr><th>Mod.</th><th>Easy</th><th>Hard</th><th>Mod.</th><th>Easy</th><th>Hard</th></tr></thead><tbody><tr><th>RTM3D (24)</th><th>no</th><th>Pixel</th><td>10.34</td><td>14.41</td><td>8.77</td><td>14.20</td><td>19.17</td><td>11.99</td></tr><tr><th>AM3D (30)</th><th>yes</th><th>Coordinate</th><td>10.74</td><td>16.50</td><td>9.52</td><td>17.32</td><td>25.30</td><td>14.91</td></tr><tr><th>PatchNet (28)</th><th>yes</th><th>Coordinate</th><td>11.12</td><td>15.68</td><td>10.17</td><td>16.86</td><td>22.97</td><td>14.97</td></tr><tr><th>\\rm D^{4}LCN (10)</th><th>yes</th><th>Pixel</th><td>11.72</td><td>16.65</td><td>9.51</td><td>16.02</td><td>22.51</td><td>12.55</td></tr><tr><th>Kinematic3D (3)</th><th>yes</th><th>Pixel</th><td>12.72</td><td>19.07</td><td>9.17</td><td>17.52</td><td>26.69</td><td>13.10</td></tr><tr><th>Liu et al. (27)</th><th>yes</th><th>Pixel</th><td>13.25</td><td>21.65</td><td>9.91</td><td>17.982</td><td>29.81</td><td>13.08</td></tr><tr><th>PCT</th><th>yes</th><th>Coordinate</th><td>13.37</td><td>21.00</td><td>11.31</td><td>19.03</td><td>29.65</td><td>15.92</td></tr></tbody></table>", "caption": "Table 8: Comparison with SoTA methods on the KITTI test set at IoU = 0.7. Our algorithm achieves new SoTA performance. \u201cDepth\u201d means if the method belongs to depth-assisted methods or not. \u201cType\u201d indicates the method input pattern, \u201cPixel\u201d denotes the methods with image as inputs directly while \u201cCoordinate\u201d means the coordinate-based methods with 3D coordinates as inputs. ", "list_citation_info": ["[27] Y. Liu, Y. Yuan, and M. Liu. Ground-aware monocular 3d object detection for autonomous driving. IEEE Robotics and Automation Letters, 2021.", "[3] Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt Schiele. Kinematic 3d object detection in monocular video. arXiv preprint, 2020.", "[30] Xinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Wanli Ouyang, and Xin Fan. Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving. In ICCV, 2019.", "[10] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided convolutions for monocular 3d object detection. In CVPR, 2019.", "[24] Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao. Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving. In ECCV, 2020.", "[28] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang. Rethinking pseudo-lidar representation. In ECCV, 2020."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Difficulty</th><th rowspan=\"2\">Threshold</th><th rowspan=\"2\">Method</th><th colspan=\"4\">3D mAP / 3D mAPH</th></tr><tr><th>Overall</th><th>0 - 30m</th><th>30 - 50m</th><th>50 - \\infty</th></tr></thead><tbody><tr><th rowspan=\"4\">LEVEL_1</th><th rowspan=\"2\">IoU=0.7</th><th>PatchNet</th><td>0.39 / 0.37</td><td>1.67 / 1.63</td><td>0.13 / 0.12</td><td>0.03 / 0.03</td></tr><tr><th>PCT</th><td>0.89 / 0.88</td><td>3.18 / 3.15</td><td>0.27 / 0.27</td><td>0.07 / 0.07</td></tr><tr><th rowspan=\"2\">IoU=0.5</th><th>PatchNet</th><td>2.92 / 2.74</td><td>10.03 / 9.75</td><td>1.09 / 0.96</td><td>0.23 / 0.18</td></tr><tr><th>PCT</th><td>4.20 / 4.15</td><td>14.70 / 14.54</td><td>1.78 / 1.75</td><td>0.39 / 0.39</td></tr><tr><th rowspan=\"4\">LEVEL_2</th><th rowspan=\"2\">IoU=0.7</th><th>PatchNet</th><td>0.38 / 0.36</td><td>1.67 / 1.63</td><td>0.13 / 0.11</td><td>0.03 / 0.03</td></tr><tr><th>PCT</th><td>0.66 / 0.66</td><td>3.18 / 3.15</td><td>0.27 / 0.26</td><td>0.07 / 0.07</td></tr><tr><th rowspan=\"2\">IoU=0.5</th><th>PatchNet</th><td>2.42 / 2.28</td><td>10.01 / 9.73</td><td>1.07 / 0.94</td><td>0.22 / 0.16</td></tr><tr><th>PCT</th><td>4.03 / 3.99</td><td>14.67 / 14.51</td><td>1.74 / 1.71</td><td>0.36 / 0.35</td></tr></tbody></table>", "caption": "Table 9: 3D performance on Waymo validation set. We demonstrate results of base method PatchNet (28) and corresponding PCT at IoU = 0.7 and I0U = 0.5. Our proposed PCT achieves consistent improvements on all settings.", "list_citation_info": ["[28] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang. Rethinking pseudo-lidar representation. In ECCV, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td colspan=\"3\">RTM3D* [\\rm AP_{3D}/\\rm AP_{BEV}]</td><td colspan=\"3\">DDMP-3D* [\\rm AP_{3D}/\\rm AP_{BEV}]</td></tr><tr><td>Mod.</td><td>Easy</td><td>Hard</td><td>Mod.</td><td>Easy</td><td>Hard</td></tr><tr><th>PatchNet* [28]</th><td>26.31/34.14</td><td>36.40/46.80</td><td>21.07/28.04</td><td>22.16/32.63</td><td>33.84/45.64</td><td>20.17/27.28</td></tr><tr><th>PatchNet* + PCT</th><td>27.53/34.65</td><td>38.39/47.16</td><td>24.44/28.47</td><td>25.90/33.70</td><td>37.00/46.45</td><td>23.57/27.96</td></tr></tbody></table>", "caption": "Table 10: 3D detection performance on the KITTI validation set. We explore two 2D detectors, one is from RTM3D [24] and the other is from DDMP-3D [42]. PatchNet [28] is used as baseline, and * denotes our reproduced version. We demonstrate that different 2D detectors will lead to drastically different 3D detection results, and the performance of 2D detectors is not positively related to the final 3D detection accuracy.", "list_citation_info": ["[24] Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao. Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving. In ECCV, 2020.", "[28] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang. Rethinking pseudo-lidar representation. In ECCV, 2020.", "[42] Li Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong Guo, Xiangyang Xue, Jianfeng Feng, and Li Zhang. Depth-conditioned dynamic message propagation for monocular 3d object detection. In CVPR, 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><td colspan=\"3\">DORN [\\rm AP_{3D}/\\rm AP_{BEV}]</td><td colspan=\"3\">PSMNet [\\rm AP_{3D}/\\rm AP_{BEV}]</td></tr><tr><td>Mod.</td><td>Easy</td><td>Hard</td><td>Mod.</td><td>Easy</td><td>Hard</td></tr><tr><th>Pseudo-LiDAR* [43]</th><td>23.04/31.06</td><td>32.27/42.45</td><td>19.67/25.67</td><td>42.01/52.63</td><td>58.27/70.91</td><td>34.99/44.61</td></tr><tr><th>PatchNet* [28]</th><td>26.31/34.14</td><td>36.40/46.80</td><td>21.07/28.04</td><td>47.30/56.59</td><td>68.88/74.87</td><td>39.13/47.80</td></tr><tr><th>Pseudo-LiDAR* + PCT</th><td>24.43/32.50</td><td>34.34/45.35</td><td>20.18/26.91</td><td>45.31/54.59</td><td>61.90/72.96</td><td>37.61/45.79</td></tr><tr><th>PatchNet* + PCT</th><td>27.53/34.65</td><td>38.39/47.16</td><td>24.44/28.47</td><td>48.27/57.11</td><td>70.73/80.65</td><td>39.97/48.14</td></tr></tbody></table>", "caption": "Table 11: 3D detection performance on the KITTI validation set. We explore two depth estimators, DORN and PSMNet. We adopt two baselines, Pseudo-LiDAR [43] and PatchNet [28], * denotes our reproduced version.We demonstrate that a better depth estimator will lead to better 3D detection performance. In addition, our proposed PCT is able to achieve consistent improvements.", "list_citation_info": ["[28] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang. Rethinking pseudo-lidar representation. In ECCV, 2020.", "[43] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In CVPR, 2019."]}]}