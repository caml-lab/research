{"title": "Stubborn: A strong baseline for indoor object navigation", "abstract": "We present a strong baseline that surpasses the performance of previously published methods on the Habitat Challenge task of navigating to a target object in indoor environments. Our method is motivated from primary failure modes of prior state-of-the-art: poor exploration, inaccurate object identification, and agent getting trapped due to imprecise map construction. We make three contributions to mitigate these issues: (i) First, we show that existing map-based methods fail to effectively use semantic clues for exploration. We present a semantic-agnostic exploration strategy (called Stubborn) without any learning that surprisingly outperforms prior work. (ii) We propose a strategy for integrating temporal information to improve object identification. (iii) Lastly, due to inaccurate depth observation the agent often gets trapped in small regions. We develop a multi-scale collision map for obstacle identification that mitigates this issue.", "authors": ["Haokuan Luo", " Albert Yue", " Zhang-Wei Hong", " Pulkit Agrawal"], "pdf_url": "https://arxiv.org/abs/2203.07359", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Failure Mode</th><td>SemExp %</td><td>EEAux %</td><td>EEAux GT %</td><td>Stubborn %</td></tr><tr><th>False Detection</th><td>47.7\\pm 9.0</td><td>35.3\\pm 6.3</td><td>{1.4\\pm 1.1}</td><td>45.1\\pm 8.8</td></tr><tr><th>Missed Detection</th><td>2.7\\pm 2.3</td><td>6.3\\pm 3.2</td><td>{1.4\\pm 1.1}</td><td>5.3\\pm 3.6 </td></tr><tr><th>\\cdashline1-5Loop</th><td>10.8\\pm 5.9</td><td>14.7\\pm 4.6</td><td>22.2\\pm 6.9</td><td>{0.9\\pm 0.9} </td></tr><tr><th>Trapped</th><td>10.8\\pm 5.4</td><td>12.5\\pm 4.3</td><td>13.9\\pm 5.8</td><td>{8.0\\pm 5.0}</td></tr><tr><th>Explore</th><td>8.1\\pm 5.0</td><td>8.9\\pm 3.7</td><td>12.5\\pm 5.4</td><td>13.3\\pm 6.1 </td></tr><tr><th>\\cdashline1-5Stairs</th><td>12.6\\pm 5.9</td><td>0.4\\pm 0.9</td><td>6.9\\pm 4.0</td><td>15.9\\pm 6.0 </td></tr><tr><th>Misc</th><td>7.2\\pm 4.0</td><td>21.9\\pm 5.4</td><td>41.7\\pm 8.5</td><td>11.5\\pm 5.7 </td></tr><tr><th>Success Rate</th><td>17.9</td><td>23.7</td><td>40.4<sup>1</sup></td><td>23.7</td></tr></tbody></table><ul><li>1<p>Estimated based on results from [4]</p></li></ul>", "caption": "TABLE I: Analyzing failure modes of previous winners of the Habitat Object Navigation Challenge [10]. Each row reports the percentage of failures attributed to a particular cause along with 95\\% confidence intervals. The primary cause of failure is false detection followed by exploration related issues of loop, trapped and explore. The last row reports the overall success rate.", "list_citation_info": ["[4] J. Ye, D. Batra, A. Das, and E. Wijmans, \u201cAuxiliary tasks and exploration enable object navigation,\u201d ICCV, 2021.", "[10] D. Batra, A. Gokaslan, A. Kembhavi, O. Maksymets, R. Mottaghi, M. Savva, A. Toshev, and E. Wijmans, \u201cObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects,\u201d in arXiv:2006.13171, 2020."]}, {"table": "<table><thead><tr><th>Models</th><th>SPL</th><th>Success Rate %</th></tr></thead><tbody><tr><td>(1) Habitat on Web (IL-HD)</td><td>0.099</td><td>27.8</td></tr><tr><td>(2) yuumi_the_magic_cat(Our Method)</td><td>0.098</td><td>23.7</td></tr><tr><td>(3) TreasureHunt[17]</td><td>0.089</td><td>21.4</td></tr><tr><td>(4) PONI (PF)</td><td>0.088</td><td>20</td></tr><tr><td>(5) EmbCLIP [18]</td><td>0.078</td><td>18.1</td></tr><tr><td>(6) Arnold (SemExp){}^{2020} [3]</td><td>0.0707</td><td>17.85</td></tr><tr><td>(7) OVRL (RGBD)</td><td>0.076</td><td>23.2</td></tr><tr><td>(8) Red Rabbit 6-Act Base (EEAux){}^{2021} [4]</td><td>0.062</td><td>23.7</td></tr></tbody></table>", "caption": "TABLE VI: We report the leaderboard entries on standard test split of the 2021 Habitat Object Navigation Challenge Entries with the superscript 2020 and 2021 indicate challenge winners in the corresponding year. Our Method (named Yuumi_the_magic_cat) is ranked second by a minute margin based on SPL and accuracy. It substantially outperforms SemExp, the agent we have built upon.", "list_citation_info": ["[18] A. Khandelwal, L. Weihs, R. Mottaghi, and A. Kembhavi, \u201cSimple but effective: Clip embeddings for embodied ai,\u201d arXiv preprint arXiv:2111.09888, 2021.", "[4] J. Ye, D. Batra, A. Das, and E. Wijmans, \u201cAuxiliary tasks and exploration enable object navigation,\u201d ICCV, 2021.", "[17] O. Maksymets, V. Cartillier, A. Gokaslan, E. Wijmans, W. Galuba, S. Lee, and D. Batra, \u201cThda: Treasure hunt data augmentation for semantic navigation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 15\u2009374\u201315\u2009383.", "[3] D. S. Chaplot, D. Gandhi, A. Gupta, and R. Salakhutdinov, \u201cObject goal navigation using goal-oriented semantic exploration,\u201d NeurIPS, 2020."]}]}