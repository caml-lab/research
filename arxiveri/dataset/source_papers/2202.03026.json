{"title": "Context Autoencoder for Self-Supervised Representation Learning", "abstract": "We present a novel masked image modeling (MIM) approach, context autoencoder (CAE), for self-supervised representation pretraining. The goal is to pretrain an encoder by solving the pretext task: estimate the masked patches from the visible patches in an image. Our approach first feeds the visible patches into the encoder, extracting the representations. Then, we make predictions from visible patches to masked patches in the encoded representation space. We introduce an alignment constraint, encouraging that the representations for masked patches, predicted from the encoded representations of visible patches, are aligned with the masked patch presentations computed from the encoder. In other words, the predicted representations are expected to lie in the encoded representation space, which empirically shows the benefit to representation learning. Last, the predicted masked patch representations are mapped to the targets of the pretext task through a decoder. In comparison to previous MIM methods (e.g., BEiT) that couple the encoding and pretext task completion roles, our approach benefits the separation of the representation learning (encoding) role and the pretext task completion role, improving the representation learning capacity and accordingly helping more on downstream tasks. In addition, we present the explanations about why contrastive pretraining and supervised pretraining perform similarly and why MIM potentially performs better. We demonstrate the effectiveness of our CAE through superior transfer performance in downstream tasks: semantic segmentation, and object detection and instance segmentation.", "authors": ["Xiaokang Chen", " Mingyu Ding", " Xiaodi Wang", " Ying Xin", " Shentong Mo", " Yunhao Wang", " Shumin Han", " Ping Luo", " Gang Zeng", " Jingdong Wang"], "pdf_url": "https://arxiv.org/abs/2202.03026", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>#Epochs</td><td>#Crops</td><td>FT</td><td>LIN</td><td>ATT</td></tr><tr><th colspan=\"5\">Methods using ViT-S:</th><td></td></tr><tr><th>DeiT</th><td>300</td><td>-</td><td>-</td><td>-</td><td>79.9</td></tr><tr><th>MoCo v3</th><td>300</td><td>2</td><td>81.7</td><td>73.1</td><td>73.8</td></tr><tr><th>BEiT</th><td>300</td><td>1</td><td>81.7</td><td>15.7</td><td>23.6</td></tr><tr><th>CAE</th><td>300</td><td>1</td><td>82.0</td><td>51.8</td><td>65.0</td></tr><tr><th colspan=\"5\">Methods using ViT-B:</th><td></td></tr><tr><th>DeiT</th><td>300</td><td>-</td><td>-</td><td>-</td><td>81.8</td></tr><tr><th>MoCo v3</th><td>300</td><td>2</td><td>83.0</td><td>76.2</td><td>77.0</td></tr><tr><th>DINO</th><td>400</td><td>12</td><td>83.3</td><td>77.3</td><td>77.8</td></tr><tr><th>BEiT</th><td>300</td><td>1</td><td>83.0</td><td>37.6</td><td>49.4</td></tr><tr><th>MAE</th><td>300</td><td>1</td><td>82.9</td><td>61.5</td><td>71.1</td></tr><tr><th>MAE</th><td>1600</td><td>1</td><td>83.6</td><td>67.8</td><td>74.2</td></tr><tr><th>CAE</th><td>300</td><td>1</td><td>83.6</td><td>64.1</td><td>73.8</td></tr><tr><th>CAE</th><td>800</td><td>1</td><td>83.8</td><td>68.6</td><td>75.9</td></tr><tr><th>CAE</th><td>1600</td><td>1</td><td>83.9</td><td>70.4</td><td>77.1</td></tr><tr><th colspan=\"5\">Methods using ViT-L:</th><td></td></tr><tr><th>MoCo v3{}^{\\dagger}</th><td>300</td><td>2</td><td>84.1</td><td>-</td><td>-</td></tr><tr><th>BEiT{}^{\\dagger}</th><td>1600</td><td>1</td><td>85.2</td><td>-</td><td>-</td></tr><tr><th>MAE</th><td>1600</td><td>1</td><td>86.0</td><td>76.0</td><td>78.8</td></tr><tr><th>CAE</th><td>1600</td><td>1</td><td>86.3</td><td>78.1</td><td>81.2</td></tr></tbody></table>", "caption": "Table 1: Pretraining quality evaluationin terms offine-tuning (FT),linear probing (LIN),and attentive probing (ATT).#Epochs refers to the number of pretraining epochs.For reference,we report the top-1 accuracy(in the column ATT)of the supervised training approachDeiT (Touvron et al., 2020)to show how far our ATT score is from supervised training.The resultsfor other modelsand our modelsare based on our implementationsfor fine-tuning,linear probing,and attentive probing.MoCo v3 and DINO adopt multi-crop pretraining augmentationin each minibatch.MoCo v3: 2 global crops of 224\\times 224. DINO: 2 global crops of 224\\times 224 and 10 local crops of 96\\times 96.{}^{\\dagger}: these results are from (He et al., 2021).", "list_citation_info": ["He et al. (2021) He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., and Girshick, R. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.", "Touvron et al. (2020) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><td>#Epochs</td><td>Supervised</td><td>Self-supervised</td><td>mIoU</td></tr><tr><th colspan=\"5\">Methods using ViT-B:</th></tr><tr><th>DeiT</th><td>300</td><td>\\surd</td><td>\\times</td><td>47.0</td></tr><tr><th>MoCo v3{}^{*}</th><td>300</td><td>\\times</td><td>\\surd</td><td>47.2</td></tr><tr><th>DINO{}^{*}</th><td>400</td><td>\\times</td><td>\\surd</td><td>47.2</td></tr><tr><th>BEiT</th><td>300</td><td>\\times</td><td>\\surd</td><td>45.5</td></tr><tr><th>BEiT</th><td>800</td><td>\\times</td><td>\\surd</td><td>46.5</td></tr><tr><th>MAE</th><td>300</td><td>\\times</td><td>\\surd</td><td>45.8</td></tr><tr><th>MAE</th><td>1600</td><td>\\times</td><td>\\surd</td><td>48.1</td></tr><tr><th>CAE</th><td>300</td><td>\\times</td><td>\\surd</td><td>{48.3}</td></tr><tr><th>CAE</th><td>800</td><td>\\times</td><td>\\surd</td><td>49.7</td></tr><tr><th>CAE</th><td>1600</td><td>\\times</td><td>\\surd</td><td>\\mathbf{50.2}</td></tr><tr><th colspan=\"5\">Methods using ViT-L:</th></tr><tr><th>MoCo v3{}^{\\dagger}</th><td>300</td><td>\\times</td><td>\\surd</td><td>49.1</td></tr><tr><th>BEiT{}^{\\dagger}</th><td>1600</td><td>\\times</td><td>\\surd</td><td>53.3</td></tr><tr><th>MAE</th><td>1600</td><td>\\times</td><td>\\surd</td><td>53.6</td></tr><tr><th>CAE</th><td>1600</td><td>\\times</td><td>\\surd</td><td>\\mathbf{54.7}</td></tr></tbody></table>", "caption": "Table 3: Semantic segmentation on ADE20K.All the results are based on the sameimplementationfor semantic segmentation. #Epochs refers to the number of pretraining epochs.{}^{*}: use multi-crop pretraining augmentation(See Table 1)and equivalently take a larger number of epochs comparedto one-crop augmentation.{}^{\\dagger}: these results are from (He et al., 2021).", "list_citation_info": ["He et al. (2021) He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., and Girshick, R. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Backbone</th><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">#Epochs</th><th>ADE</th><th colspan=\"2\">COCO</th></tr><tr><th>mIoU</th><th>\\text{AP}^{b}</th><th>\\text{AP}^{m}</th></tr></thead><tbody><tr><td>SplitMask (El-Nouby et al., 2021)</td><td>ViT-B</td><td>ADE20K</td><td>21000</td><td>45.7</td><td>-</td><td>-</td></tr><tr><td>MAE (He et al., 2021)</td><td>ViT-B</td><td>ImageNet-1K</td><td>1600</td><td>48.1</td><td>51.3</td><td>44.3</td></tr><tr><td>PeCo (Dong et al., 2021)</td><td>ViT-B</td><td>ImageNet-1K</td><td>300</td><td>46.7</td><td>-</td><td>-</td></tr><tr><td>CAE</td><td>ViT-B</td><td>ImageNet-1K</td><td>300</td><td>48.3</td><td>51.6</td><td>44.6</td></tr><tr><td>CAE</td><td>ViT-B</td><td>ImageNet-1K</td><td>800</td><td>49.7</td><td>52.8</td><td>45.5</td></tr><tr><td>CAE</td><td>ViT-B</td><td>ImageNet-1K</td><td>1600</td><td>50.2</td><td>52.9</td><td>45.5</td></tr></tbody></table>", "caption": "Table 5: The results of concurrently-developed MIM methodsfor semantic segmentation on ADE20K,and object detection and instance segmentation (1\\times schedule) on COCO with the Cascaded Mask-RCNN framework.The segmentation results of other methodsare from the corresponding paper,and all the detection resultsare from our implementation.", "list_citation_info": ["Dong et al. (2021) Dong, X., Bao, J., Zhang, T., Chen, D., Zhang, W., Yuan, L., Chen, D., Wen, F., and Yu, N. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.", "El-Nouby et al. (2021) El-Nouby, A., Izacard, G., Touvron, H., Laptev, I., Jegou, H., and Grave, E. Are large-scale datasets necessary for self-supervised pre-training? arXiv preprint arXiv:2112.10740, 2021.", "He et al. (2021) He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., and Girshick, R. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021."]}]}