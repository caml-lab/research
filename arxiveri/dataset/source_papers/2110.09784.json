{"title": "SSAST: Self-Supervised Audio Spectrogram Transformer", "abstract": "Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST.\n  This paper focuses on audio and speech classification, and aims to reduce the need for large amounts of labeled data for AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST.", "authors": ["Yuan Gong", " Cheng-I Jeff Lai", " Yu-An Chung", " James Glass"], "pdf_url": "https://arxiv.org/abs/2110.09784", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td colspan=\"3\">Task</td></tr><tr><th></th><td>KS1</td><td>SID</td><td>ER</td></tr><tr><th>APC (Chung et al. 2019)</th><td>94.0</td><td>60.4</td><td>59.3</td></tr><tr><th>Wav2vec (Schneider et al. 2019)</th><td>96.2</td><td>56.6</td><td>59.8</td></tr><tr><th>Wav2vec 2.0 (Baevski et al. 2020){}^{*}</th><td>96.2</td><td>75.2</td><td>63.4</td></tr><tr><th>HuBERT (Hsu et al. 2021){}^{*}</th><td>96.3</td><td>81.4</td><td>64.9</td></tr><tr><th>SSAST-Patch (Librispeech Only)</th><td>95.6</td><td>60.8</td><td>58.3</td></tr><tr><th>SSAST-Patch</th><td>96.0</td><td>64.3</td><td>59.6</td></tr><tr><th>SSAST-Frame</th><td>96.7</td><td>80.8</td><td>60.5</td></tr></tbody></table>", "caption": "Table 5: Comparison of SSAST and existing speech self-supervised pretraining frameworks ({}^{*}frozen setting results).", "list_citation_info": ["Baevski et al. (2020) Baevski, A.; Zhou, Y.; Mohamed, A.; and Auli, M. 2020. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. Advances in Neural Information Processing Systems, 33.", "Chung et al. (2019) Chung, Y.-A.; Hsu, W.-N.; Tang, H.; and Glass, J. 2019. An unsupervised autoregressive model for speech representation learning. In Interspeech.", "Schneider et al. (2019) Schneider, S.; Baevski, A.; Collobert, R.; and Auli, M. 2019. wav2vec: Unsupervised pre-training for speech recognition. arXiv preprint arXiv:1904.05862.", "Hsu et al. (2021) Hsu, W.-N.; Tsai, Y.-H. H.; Bolte, B.; Salakhutdinov, R.; and Mohamed, A. 2021. HuBERT: How much can a bad teacher benefit ASR pre-training? In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6533\u20136537. IEEE."]}]}