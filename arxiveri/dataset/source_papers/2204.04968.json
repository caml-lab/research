{"title": "Bimodal camera pose prediction for endoscopy", "abstract": "Deducing the 3D structure of endoscopic scenes from images remains extremely challenging. In addition to deformation and view-dependent lighting, tubular structures like the colon present problems stemming from the self-occluding, repetitive anatomical structures. In this paper, we propose SimCol, a synthetic dataset for camera pose estimation in colonoscopy and a novel method that explicitly learns a bimodal distribution to predict the endoscope pose. Our dataset replicates real colonoscope motion and highlights drawbacks of existing methods. We publish 18k RGB images from simulated colonoscopy with corresponding depth and camera poses and make our data generation environment in Unity publicly available. We evaluate different camera pose prediction methods and demonstrate that, when trained on our data, they generalize to real colonoscopy sequences and our bimodal approach outperforms prior unimodal work.", "authors": ["Anita Rau", " Binod Bhattarai", " Lourdes Agapito", " Danail Stoyanov"], "pdf_url": "https://arxiv.org/abs/2204.04968", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th><p>Description</p></th><th>R/V/P</th><th>Public</th><th>Depths</th><th>Camera pose</th><th>Intrinsics</th><th>Tubular</th><th># Frames</th><th># Trajectories</th></tr></thead><tbody><tr><td>Ozyoruk et al. [6]</td><td><p>Ex-vivo porcine 3D scanned colon</p></td><td>R</td><td>some</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>&gt;8k</td><td>18</td></tr><tr><td>Ozyoruk et al. [6]</td><td><p>Simulated capsule endoscope in Unity</p></td><td>V</td><td>some</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>22k</td><td>1</td></tr><tr><td>Armin et al. [7]</td><td><p>Simulated colonopscoy video</p></td><td>V</td><td>\\times</td><td>?</td><td>\u2713</td><td>?</td><td>\u2713</td><td>30k</td><td>&gt;15</td></tr><tr><td>Turan et al.[8]</td><td><p>Ex-vivo porcine stomach</p></td><td>R</td><td>\\times</td><td>\\times</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>12k</td><td>&gt; 4</td></tr><tr><td>Ma et al. [9]</td><td><p>COLMAP labels from colonoscopy video</p></td><td>R</td><td>\\times</td><td>\\times</td><td>\\times</td><td>?</td><td>\u2713</td><td>1.2m</td><td>60</td></tr><tr><td>Widya et al. [10]</td><td><p>Stomach with real meshed texture</p></td><td>R/V</td><td>\\times</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\\times</td><td>?</td><td>7</td></tr><tr><td>Freedman et al. [11]</td><td><p>Rendered synthetic dataset</p></td><td>V</td><td>\\times</td><td>\u2713</td><td>?</td><td>?</td><td>\u2713</td><td>187k</td><td>?</td></tr><tr><td>Fulton et al.[12]</td><td><p>Colon phantom magnetic tracker dataset</p></td><td>P</td><td>\u2713</td><td>\\times</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>24k</td><td>7</td></tr><tr><td>Bae et al.[13]</td><td><p>SfM labels from colonoscopy video</p></td><td>R</td><td>\\times</td><td>\\times</td><td>\\times</td><td>?</td><td>\u2713</td><td>&gt;34k</td><td>51</td></tr><tr><td>Ours</td><td><p>Simulated colonoscope in Unity</p></td><td>V</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>18k</td><td>15</td></tr></tbody></table><p><br/>R = Real, V = Virtually simulated, P = Physical phantom, ? = Could not be verified, &gt; could be partially verified but it appears there are more.</p>", "caption": "Table 1: An overview and comparison of colonoscopy or gastroscopy datasets", "list_citation_info": ["[11] D. Freedman et al., \u201cDetecting deficient coverage in colonoscopies,\u201d arXiv preprint arXiv:2001.08589, 2020.", "[9] R. Ma, R. Wang, S. Pizer, J. Rosenman, S. K. McGill, and J.-M. Frahm, \u201cReal-time 3d reconstruction of colonoscopic surfaces for determining missing regions,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 573\u2013582, Springer, 2019.", "[8] M. Turan et al., \u201cUnsupervised odometry and depth learning for endoscopic capsule robots,\u201d in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1801\u20131807, IEEE, 2018.", "[12] M. J. Fulton, J. M. Prendergast, E. R. DiTommaso, and M. E. Rentschler, \u201cComparing visual odometry systems in actively deforming simulated colon environments,\u201d in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4988\u20134995, IEEE, 2020.", "[7] M. A. Armin, N. Barnes, J. Alvarez, H. Li, F. Grimpen, and O. Salvado, \u201cLearning camera pose from optical colonoscopy frames through deep convolutional neural network (cnn),\u201d in Computer assisted and robotic endoscopy and clinical image-based procedures, pp. 50\u201359, Springer, 2017.", "[6] K. B. Ozyoruk et al., \u201cEndoslam dataset and an unsupervised monocular visual odometry and depth estimation approach for endoscopic videos,\u201d Medical image analysis, vol. 71, p. 102058, 2021.", "[13] G. Bae, I. Budvytis, C.-K. Yeung, and R. Cipolla, \u201cDeep multi-view stereo for dense 3d reconstruction from monocular endoscopic video,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 774\u2013783, Springer, 2020.", "[10] A. R. Widya, Y. Monno, M. Okutomi, S. Suzuki, T. Gotoda, and K. Miki, \u201cWhole stomach 3d reconstruction and frame localization from monocular endoscope video,\u201d IEEE journal of translational engineering in health and medicine, vol. 7, pp. 1\u201310, 2019."]}, {"table": "<table><tbody><tr><th>Test trajectory</th><td colspan=\"4\">1</td><td colspan=\"4\">2</td><td colspan=\"4\">3</td></tr><tr><th>Total trajectory length</th><td colspan=\"4\">105.1 cm</td><td colspan=\"4\">103.6 cm</td><td colspan=\"4\">102.2 cm</td></tr><tr><th>Mean step size</th><td colspan=\"4\">4.4 mm</td><td colspan=\"4\">4.3 mm</td><td colspan=\"4\">4.3 mm</td></tr><tr><th>Mean rotation per step</th><td colspan=\"4\">4.6\u00b0</td><td colspan=\"4\">4.8\u00b0</td><td colspan=\"4\">4.8\u00b0</td></tr><tr><th></th><td>ATE(cm)</td><td>RTE(mm)</td><td>ROT(\u00b0)</td><td>Acc(%)</td><td>ATE(cm)</td><td>RTE(mm)</td><td>ROT(\u00b0)</td><td>Acc(%)</td><td>ATE(cm)</td><td>RTE(mm)</td><td>ROT(\u00b0)</td><td>Acc(%)</td></tr><tr><th>COLMAP [2]</th><td>0.01</td><td>0.07</td><td>0.18</td><td>8</td><td>0.02</td><td>0.10</td><td>0.28</td><td>6</td><td>0.37</td><td>0.68</td><td>1.42</td><td>7</td></tr><tr><th>EndoSLAM [6]</th><td>14.9/17.8</td><td>3.10/4.26</td><td>1.3/5.7</td><td>100/41</td><td>18.0/15.3</td><td>3.50/4.28</td><td>1.8/5.0</td><td>100/35</td><td>14.9/15.7</td><td>3.24/4.16</td><td>1.6/5.5</td><td>100/37</td></tr><tr><th>Ours unimodal</th><td>7.08/12.5</td><td>0.75/0.79</td><td>1.3/1.3</td><td>100/99</td><td>2.79/2.63</td><td>0.76/0.76</td><td>1.5/1.5</td><td>100/100</td><td>7.56/10.0</td><td>0.87/0.90</td><td>1.5/1.6</td><td>100/99</td></tr><tr><th>Ours bimodal</th><td>8.86/13.3</td><td>0.72/0.72</td><td>1.5/1.5</td><td>100/99</td><td>2.83/6.03</td><td>0.69/0.71</td><td>1.7/1.7</td><td>100/100</td><td>6.17/9.62</td><td>0.85/0.89</td><td>1.6/1.6</td><td>99/99</td></tr><tr><th>Ours bimodal w/L_{c}</th><td>8.81/9.79</td><td>0.69/0.72</td><td>1.5/1.5</td><td>100/99</td><td>2.35/5.23</td><td>0.67/0.70</td><td>1.6/1.6</td><td>100/100</td><td>3.78/9.00</td><td>0.82/0.85</td><td>1.6/1.5</td><td>100/99</td></tr><tr><th>Ours bimodal w/L_{c} w/o Corr</th><td>14.0/13.9</td><td>0.77/0.76</td><td>1.7/1.6</td><td>99/99</td><td>8.32/5.46</td><td>0.73/0.71</td><td>2.1/2.0</td><td>99/98</td><td>8.70/8.10</td><td>0.93/0.98</td><td>1.9/1.8</td><td>99/98</td></tr></tbody></table><p></p><p>ATE &amp; RTE, ROTation error, and accuracy for our test trajectories\u2019 forward/backward traversal. COLMAP performs global optimization and thus direction does not matter (only one value reported). The accuracy for regression methods (Ours and EndoSLAM) denotes the percentage of correctly predicted directions. The accuracy for COLMAP reports the percentage of frames from the trajectory that COLMAP was able to reconstruct. Note that although COLMAP yields the smallest errors, the method successfully reconstructs only a small fraction of each trajectory. Bold indicates the best result as the sum of forward and backward errors. W/o Corr refers to our model without the correlation layer in the classfication net.</p>", "caption": "Table 2: Absolute translation errors, Relative translation errors, and Rotation errors on our test trajectories", "list_citation_info": ["[6] K. B. Ozyoruk et al., \u201cEndoslam dataset and an unsupervised monocular visual odometry and depth estimation approach for endoscopic videos,\u201d Medical image analysis, vol. 71, p. 102058, 2021.", "[2] J. L. Sch\u00f6nberger and J.-M. Frahm, \u201cStructure-from-motion revisited,\u201d in Conference on Computer Vision and Pattern Recognition (CVPR), 2016."]}]}