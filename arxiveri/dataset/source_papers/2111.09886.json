{"title": "Simmim: A simple framework for masked image modeling", "abstract": "This paper presents SimMIM, a simple framework for masked image modeling. We simplify recently proposed related approaches without special designs such as block-wise masking and tokenization via discrete VAE or clustering. To study what let the masked image modeling task learn good representations, we systematically study the major components in our framework, and find that simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting raw pixels of RGB values by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6%. When applied on a larger model of about 650 million parameters, SwinV2-H, it achieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to facilitate the training of a 3B model (SwinV2-G), that by $40\\times$ less data than that in previous practice, we achieve the state-of-the-art on four representative vision benchmarks. The code and models will be publicly available at https://github.com/microsoft/SimMIM.", "authors": ["Zhenda Xie", " Zheng Zhang", " Yue Cao", " Yutong Lin", " Jianmin Bao", " Zhuliang Yao", " Qi Dai", " Han Hu"], "pdf_url": "https://arxiv.org/abs/2111.09886", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Methods</td><td>Input</td><td>Fine-tuning</td><td>Linear eval</td><td>Pre-training</td></tr><tr><td>Size</td><td>Top-1 acc (%)</td><td>Top-1 acc (%)</td><td>costs</td></tr><tr><td>Sup. baseline [46]</td><td>224^{2}</td><td>81.8</td><td>-</td><td>-</td></tr><tr><td>DINO [5]</td><td>224^{2}</td><td>82.8</td><td>78.2</td><td>2.0\\times</td></tr><tr><td>MoCo v3 [9]</td><td>224^{2}</td><td>83.2</td><td>76.7</td><td>1.8\\times</td></tr><tr><td>ViT [15]</td><td>384^{2}</td><td>79.9</td><td>-</td><td>\\sim4.0\\times</td></tr><tr><td>BEiT [1]</td><td>224^{2}</td><td>83.2</td><td>56.7</td><td>1.5\\times^{\\dagger}</td></tr><tr><td>Ours</td><td>224^{2}</td><td>83.8</td><td>56.7</td><td>1.0\\times</td></tr></table>", "caption": "Table 6: System-level comparison using ViT-B as the encoder. Training costs are counted in relative to our approach. {}^{\\dagger} BEiT requires an additional stage to pre-train dVAE, which is not counted.", "list_citation_info": ["[1] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.", "[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.", "[46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.", "[9] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021.", "[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021."]}, {"table": "<table><tr><td>Methods</td><td>Pre-train</td><td>Fine-tune</td><td>Backbone</td><td>Top-1 acc (%)</td><td>Param</td></tr><tr><td>Sup.</td><td>192^{2}</td><td>224^{2}</td><td>Swin-B</td><td>83.3</td><td>88M</td></tr><tr><td>Sup.</td><td>192^{2}</td><td>224^{2}</td><td>Swin-L</td><td>83.5</td><td>197M</td></tr><tr><td>Sup.</td><td>192^{2}</td><td>224^{2}</td><td>SwinV2-H</td><td>83.3</td><td>658M</td></tr><tr><td>Ours</td><td>192^{2}</td><td>224^{2}</td><td>Swin-B</td><td>84.0</td><td>88M</td></tr><tr><td>Ours</td><td>192^{2}</td><td>224^{2}</td><td>Swin-L</td><td>85.4</td><td>197M</td></tr><tr><td>Ours</td><td>192^{2}</td><td>224^{2}</td><td>SwinV2-H</td><td>85.7</td><td>658M</td></tr><tr><td>Ours</td><td>192^{2}</td><td>512^{2}</td><td>SwinV2-H</td><td>87.1</td><td>658M</td></tr><tr><td>Ours</td><td>192^{2}</td><td>640^{2}</td><td>SwinV2-G</td><td>90.2</td><td>3.0B</td></tr></table>", "caption": "Table 7: Scaling experiments with Swin Transformer as backbone architectures. All our models are pre-trained with input of 192^{2}. Different to other models, Swin-G is trained on a privately collected ImageNet-22K-ext dataset, with details described in [33].", "list_citation_info": ["[33] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883, 2021."]}]}