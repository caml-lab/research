{"title": "Combined Scaling for Zero-Shot Transfer Learning", "abstract": "We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.", "authors": ["Hieu Pham", " Zihang Dai", " Golnaz Ghiasi", " Kenji Kawaguchi", " Hanxiao Liu", " Adams Wei Yu", " Jiahui Yu", " Yi-Ting Chen", " Minh-Thang Luong", " Yonghui Wu", " Mingxing Tan", " Quoc V. Le"], "pdf_url": "https://arxiv.org/abs/2111.10050", "list_table_and_caption": [{"table": "<table><tr><td></td><td>ALIGN (Jia et al., 2021)</td><td>CLIP (Radford et al., 2021)</td><td colspan=\"2\">\u2004BASIC (ours)</td></tr><tr><td>ImageNet</td><td>76.4</td><td>76.2</td><td>85.7</td><td>(+9.3)</td></tr><tr><td>ImageNet-A</td><td>75.8</td><td>77.2</td><td>85.6</td><td>(+8.4)</td></tr><tr><td>ImageNet-R</td><td>92.2</td><td>88.9</td><td>95.7</td><td>(+3.5)</td></tr><tr><td>ImageNet-V2</td><td>70.1</td><td>70.1</td><td>80.6</td><td>(+10.5)</td></tr><tr><td>ImageNet-Sketch</td><td>64.8</td><td>60.2</td><td>76.1</td><td>(+11.3)</td></tr><tr><td>ObjectNet</td><td>72.2</td><td>72.3</td><td>82.3</td><td>(+10.1)</td></tr><tr><td>Average</td><td>74.5</td><td>74.2</td><td>84.3</td><td>(+10.1)</td></tr></table>", "caption": "Table 1: Highlights of our key results. Shown are the top-1 accuracy of our method, BASIC, and similar baselines \u2013 CLIP and ALIGN \u2013 on ImageNet and other robustness test sets. None of these models have learned from any labeled training example in ImageNet. On average, BASIC surpasses the baselines by the significant 10.1 percentage points.", "list_citation_info": ["Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021."]}, {"table": "<table><tr><td><p>Datasets</p></td><td><p>Birdsnap</p></td><td><p>Caltech101</p></td><td><p>CIFAR10</p></td><td><p>CIFAR100</p></td><td><p>DTD</p></td><td><p>EuroSAT</p></td><td><p>Flowers</p></td><td><p>Food101</p></td><td><p>ImageNet</p></td><td><p>MNIST</p></td><td><p>IIIT-Pets</p></td><td><p>PCam</p></td><td><p>RESISC45</p></td><td><p>STL10</p></td><td><p>SUN397</p></td><td><p>UCF101</p></td><td><p>VOC2007</p></td></tr><tr><td>ResNet-50</td><td>32.6</td><td>82.1</td><td>75.6</td><td>41.6</td><td>41.7</td><td>41.1</td><td>65.9</td><td>81.1</td><td>59.6</td><td>66.6</td><td>85.4</td><td>57.6</td><td>54.2</td><td>94.3</td><td>59.6</td><td>63.6</td><td>82.1</td></tr><tr><td rowspan=\"2\">BASIC-S</td><td>38.6</td><td>91.6</td><td>86.4</td><td>57.8</td><td>54.3</td><td>29.1</td><td>76.8</td><td>86.0</td><td>71.9</td><td>32.4</td><td>93.2</td><td>54.3</td><td>53.5</td><td>96.7</td><td>67.3</td><td>65.5</td><td>83.4</td></tr><tr><td>(+6.0)</td><td>(+9.5)</td><td>(+10.8)</td><td>(+16.2)</td><td>(+12.6)</td><td>(-12.0)</td><td>(+10.9)</td><td>(+4.9)</td><td>(+12.3)</td><td>(-34.2)</td><td>(+7.8)</td><td>(-3.3)</td><td>(-0.7)</td><td>(+2.4)</td><td>(+7.7)</td><td>(+1.9)</td><td>(+1.3)</td></tr><tr><td>ViT-B/16</td><td>39.1</td><td>89.3</td><td>91.6</td><td>68.7</td><td>46.0</td><td>54.1</td><td>70.4</td><td>89.2</td><td>68.6</td><td>56.0</td><td>88.9</td><td>48.1</td><td>65.5</td><td>98.2</td><td>65.2</td><td>69.8</td><td>83.9</td></tr><tr><td rowspan=\"2\">BASIC-M</td><td>49.4</td><td>94.2</td><td>94.8</td><td>72.2</td><td>60.2</td><td>39.5</td><td>86.0</td><td>92.3</td><td>81.5</td><td>33.6</td><td>95.3</td><td>58.3</td><td>65.4</td><td>99.3</td><td>72.9</td><td>77.4</td><td>84.2</td></tr><tr><td>(+10.3)</td><td>(+4.9)</td><td>(+3.2)</td><td>(+3.5)</td><td>(+14.2)</td><td>(-14.6)</td><td>(+15.6)</td><td>(+3.1)</td><td>(+12.9)</td><td>(-22.4)</td><td>(+6.4)</td><td>(+10.2)</td><td>(-0.1)</td><td>(+1.1)</td><td>(+7.7)</td><td>(+7.6)</td><td>(+0.3)</td></tr><tr><td>ViT-L/14-336</td><td>49.5</td><td>92.8</td><td>95.7</td><td>77.5</td><td>55.7</td><td>59.6</td><td>78.3</td><td>93.8</td><td>76.2</td><td>88.3</td><td>93.5</td><td>63.0</td><td>71.7</td><td>99.4</td><td>68.4</td><td>76.9</td><td>84.3</td></tr><tr><td rowspan=\"2\">BASIC-L</td><td>59.2</td><td>94.7</td><td>97.5</td><td>82.3</td><td>64.6</td><td>51.0</td><td>91.2</td><td>95.1</td><td>85.7</td><td>40.3</td><td>97.9</td><td>59.6</td><td>72.7</td><td>99.6</td><td>76.2</td><td>84.8</td><td>84.6</td></tr><tr><td>(+9.7)</td><td>(+1.9)</td><td>(+1.8)</td><td>(+4.8)</td><td>(+8.9)</td><td>(-8.6)</td><td>(+13.1)</td><td>(+1.3)</td><td>(+9.5)</td><td>(-48.0)</td><td>(+4.4)</td><td>(-3.4)</td><td>(+1.0)</td><td>(+0.2)</td><td>(+7.8)</td><td>(+7.9)</td><td>(+0.3)</td></tr></table>", "caption": "Table 3: Performances of BASIC and CLIP models (Radford et al., 2021) on 17 image classification benchmarks. The first two blocks compare models of similar numbers of weights and FLOPs. The last block compares the largest CLIP and BASIC models.", "list_citation_info": ["Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021."]}, {"table": "<table><tr><td></td><td colspan=\"3\">Image model</td><td colspan=\"5\">Text model</td></tr><tr><td></td><td>Model (Dai et al., 2021)</td><td>#Params</td><td>#FLOPs</td><td>#Layers</td><td>HiddenDim</td><td>HeadDim</td><td>#Params</td><td>#FLOPs</td></tr><tr><td>BASIC-S</td><td>CoAtNet-0</td><td>25M</td><td>4.2B</td><td>6</td><td>1024</td><td>64</td><td>108M</td><td>10.7B</td></tr><tr><td>BASIC-M</td><td>CoAtNet-3</td><td>168M</td><td>34.7B</td><td>12</td><td>1024</td><td>128</td><td>184M</td><td>49.4B</td></tr><tr><td>BASIC-L</td><td>CoAtNet-7</td><td>2.4B</td><td>495.8B</td><td>12</td><td>2048</td><td>128</td><td>670M</td><td>212.6B</td></tr></table>", "caption": "Table 5: Model sizes. For the image models, all specifications can be found from the model names in Dai et al. (2021).", "list_citation_info": ["Dai et al. (2021) Zihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. In NeurIPS, 2021."]}, {"table": "<table><tr><td>Dataset</td><td>Reference</td><td>Abbreviation in Table 3</td><td>#Classes</td><td>Test size</td><td>Evaluation metric</td></tr><tr><td>ILSVRC-2012, i.e., ImageNet</td><td>(Russakovsky et al., 2009)</td><td>ImageNet</td><td>1000</td><td>50000</td><td>accuracy</td></tr><tr><td>ImageNet-A</td><td>(Hendrycks et al., 2021b)</td><td>N/A</td><td>1000</td><td>7500</td><td>accuracy</td></tr><tr><td>ImageNet-R</td><td>(Hendrycks et al., 2021a)</td><td>N/A</td><td>1000</td><td>30000</td><td>accuracy</td></tr><tr><td>ImageNet-V2</td><td>(Recht et al., 2019)</td><td>N/A</td><td>1000</td><td>10000</td><td>accuracy</td></tr><tr><td>ImageNet-Sketch</td><td>(Wang et al., 2019)</td><td>N/A</td><td>1000</td><td>50889</td><td>accuracy</td></tr><tr><td>ObjectNet</td><td>(Barbu et al., 2019)</td><td>N/A</td><td>1000</td><td>18574</td><td>accuracy</td></tr><tr><td>CIFAR-10</td><td>(Krizhevsky, 2009)</td><td>CIFAR10</td><td>10</td><td>10000</td><td>accuracy</td></tr><tr><td>CIFAR-100</td><td>(Krizhevsky, 2009)</td><td>CIFAR100</td><td>100</td><td>10000</td><td>accuracy</td></tr><tr><td>Birdsnap</td><td>(Berg et al., 2014)</td><td>Birdsnap</td><td>500</td><td>2443</td><td>accuracy</td></tr><tr><td>Describable Textures</td><td>(Cimpoi et al., 2014)</td><td>DTD</td><td>47</td><td>1880</td><td>accuracy</td></tr><tr><td>Oxford Flowers-102</td><td>(Nilsback and Zisserman, 2008)</td><td>Flowers</td><td>102</td><td>6149</td><td>mean per-class recall</td></tr><tr><td>Food-101</td><td>(Bossard et al., 2014)</td><td>Food101</td><td>101</td><td>25250</td><td>accuracy</td></tr><tr><td>Caltech101</td><td>(Fei-Fei et al., 2004)</td><td>Caltech101</td><td>102</td><td>6084</td><td>mean per-class recall</td></tr><tr><td>Oxford IIIT-Pets</td><td>(Parkhi et al., 2012)</td><td>IIIT-Pets</td><td>37</td><td>3669</td><td>mean per-class recall</td></tr><tr><td>MNIST</td><td>(LeCun et al., 2010)</td><td>MNIST</td><td>10</td><td>10000</td><td>accuracy</td></tr><tr><td>EuroSAT</td><td>(Helber et al., 2018)</td><td>EuroSAT</td><td>10</td><td>27000</td><td>accuracy</td></tr><tr><td>PatchCamelyon</td><td>(Veeling et al., 2018)</td><td>PCam</td><td>2</td><td>32768</td><td>accuracy</td></tr><tr><td>RESICS45</td><td>(Cheng et al., 2017)</td><td>RESICS45</td><td>45</td><td>31500</td><td>accuracy</td></tr><tr><td>STL10</td><td>(Coates et al., 2011)</td><td>STL10</td><td>10</td><td>8000</td><td>accuracy</td></tr><tr><td>SUN397</td><td>(Xiao et al., 2010)</td><td>SUN397</td><td>397</td><td>21750</td><td>accuracy</td></tr><tr><td>UCF101</td><td>(Soomro et al., 2012)</td><td>UCF101</td><td>101</td><td>3783</td><td>accuracy</td></tr><tr><td>Pascal VOC 2007 Classification</td><td>(Everingham et al., )</td><td>VOC2007</td><td>20</td><td>4952</td><td>11-points mAP</td></tr></table>", "caption": "Table 7: Details of the datasets used in this paper to evaluate BASIC models. The evaluation results are presented in Table 1 and Table 3.", "list_citation_info": ["Wang et al. (2019) Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019.", "Helber et al. (2018) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. In IEEE International Geoscience and Remote Sensing Symposium, 2018.", "Coates et al. (2011) Adam Coates, Andrew Ng, and Honglak Lee. An Analysis of Single Layer Networks in Unsupervised Feature Learning. In AISTATS, 2011.", "Barbu et al. (2019) Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, 2019.", "Fei-Fei et al. (2004) Li Fei-Fei, Fergus Rob, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR, 2004.", "Bossard et al. (2014) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. ECCV, 2014.", "Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.", "Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021a.", "Soomro et al. (2012) Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. Arxiv 1212.0402, 2012.", "Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedald. Describing textures in the wild. In CVPR, 2014.", "Veeling et al. (2018) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. Medical Image Computing and Computer Assisted Intervention, 2018.", "Berg et al. (2014) Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. CVPR, 2014.", "Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In ICML, 2019.", "Nilsback and Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008.", "Cheng et al. (2017) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017.", "Parkhi et al. (2012) Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. CVPR, 2012.", "Russakovsky et al. (2009) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. IJCV, 2009.", "(23) M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.", "Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. CVPR, 2021b.", "LeCun et al. (2010) Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.", "Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010."]}]}