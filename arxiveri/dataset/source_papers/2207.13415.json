{"title": "Transnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model", "abstract": "In the past few years, convolutional neural networks (CNNs), particularly U-Net, have been the prevailing technique in the medical image processing era. Specifically, the seminal U-Net, as well as its alternatives, have successfully managed to address a wide variety of medical image segmentation tasks. However, these architectures are intrinsically imperfect as they fail to exhibit long-range interactions and spatial dependencies leading to a severe performance drop in the segmentation of medical images with variable shapes and structures. Transformers, preliminary proposed for sequence-to-sequence prediction, have arisen as surrogate architectures to precisely model global information assisted by the self-attention mechanism. Despite being feasibly designed, utilizing a pure Transformer for image segmentation purposes can result in limited localization capacity stemming from inadequate low-level features. Thus, a line of research strives to design robust variants of Transformer-based U-Net. In this paper, we propose Trans-Norm, a novel deep segmentation framework which concomitantly consolidates a Transformer module into both encoder and skip-connections of the standard U-Net. We argue that the expedient design of skip-connections can be crucial for accurate segmentation as it can assist in feature fusion between the expanding and contracting paths. In this respect, we derive a Spatial Normalization mechanism from the Transformer module to adaptively recalibrate the skip connection path. Extensive experiments across three typical tasks for medical image segmentation demonstrate the effectiveness of TransNorm. The codes and trained models are publicly available at https://github.com/rezazad68/transnorm.", "authors": ["Reza Azad", " Mohammad T. AL-Antary", " Moein Heidari", " Dorit Merhof"], "pdf_url": "https://arxiv.org/abs/2207.13415", "list_table_and_caption": [{"table": "<table><thead><tr><th>Methods</th><th>DSC \\uparrow</th><th>HD \\downarrow</th><th>Aorta</th><th>Gallbladder</th><th>Kidney(L)</th><th>Kidney(R)</th><th>Liver</th><th>Pancreas</th><th>Spleen</th><th>Stomach</th></tr></thead><tbody><tr><th>V-Net [47]</th><td>68.81</td><td>-</td><td>75.34</td><td>51.87</td><td>77.10</td><td>80.75</td><td>87.84</td><td>40.05</td><td>80.56</td><td>56.98</td></tr><tr><th>R50 U-Net [4]</th><td>74.68</td><td>36.87</td><td>87.74</td><td>63.66</td><td>80.60</td><td>78.19</td><td>93.74</td><td>56.90</td><td>85.87</td><td>74.16</td></tr><tr><th>R50 Att-UNet [4]</th><td>75.57</td><td>36.97</td><td>55.92</td><td>63.91</td><td>79.20</td><td>72.71</td><td>93.56</td><td>49.37</td><td>87.19</td><td>74.95</td></tr><tr><th>Att-UNet [28]</th><td>77.77</td><td>36.02</td><td>89.55</td><td>68.88</td><td>77.98</td><td>71.11</td><td>93.57</td><td>58.04</td><td>87.30</td><td>75.75</td></tr><tr><th>R50 ViT [4]</th><td>71.29</td><td>32.87</td><td>73.73</td><td>55.13</td><td>75.80</td><td>72.20</td><td>91.51</td><td>45.99</td><td>81.99</td><td>73.95</td></tr><tr><th>TransUnet [4]</th><td>77.48</td><td>31.69</td><td>87.23</td><td>63.13</td><td>81.87</td><td>77.02</td><td>94.08</td><td>55.86</td><td>85.08</td><td>75.62</td></tr><tr><th>Baseline</th><td>76.85</td><td>39.70</td><td>89.07</td><td>69.72</td><td>77.77</td><td>68.60</td><td>93.43</td><td>53.98</td><td>86.67</td><td>75.58</td></tr><tr><th>TransNorm (Proposed)</th><td>78.40</td><td>30.25</td><td>86.23</td><td>65.10</td><td>82.18</td><td>78.63</td><td>94.22</td><td>55.34</td><td>89.50</td><td>76.01</td></tr></tbody></table>", "caption": "TABLE I: Comparision results of the proposed method on Synaps multi-organ segmentation dataset.", "list_citation_info": ["[4] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.", "[47] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565\u2013571. IEEE, 2016.", "[28] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018."]}, {"table": "<table><thead><tr><th>Methods</th><th>DSC</th><th>SE</th><th>SP</th><th>ACC</th></tr></thead><tbody><tr><td>U-Net [14]</td><td>0.8159</td><td>0.8172</td><td>0.9680</td><td>0.9164</td></tr><tr><td>Att U-Net [28]</td><td>0.8082</td><td>0.7998</td><td>0.9776</td><td>0.9145</td></tr><tr><td>DAGAN [48]</td><td>0.8425</td><td>0.8363</td><td>0.9716</td><td>0.9304</td></tr><tr><td>TransUNet [4]</td><td>0.8123</td><td>0.8263</td><td>0.9577</td><td>0.9207</td></tr><tr><td>MCGU-Net [49]</td><td>0.8927</td><td>0.8502</td><td>0.9855</td><td>0.9570</td></tr><tr><td>MedT [50]</td><td>0.8037</td><td>0.8064</td><td>0.9546</td><td>0.9090</td></tr><tr><td>FAT-Net [37]</td><td>0.8500</td><td>0.8392</td><td>0.9725</td><td>0.9326</td></tr><tr><td>Proposed Method</td><td>0.8933</td><td>0.8532</td><td>0.9859</td><td>0.9582</td></tr></tbody></table>", "caption": "TABLE II: Quantitative analysis of the proposed method against the SOTA approaches for skin lesion segmentation on ISIC 2017.", "list_citation_info": ["[48] Baiying Lei, Zaimin Xia, Feng Jiang, Xudong Jiang, Zongyuan Ge, Yanwu Xu, Jing Qin, Siping Chen, Tianfu Wang, and Shuqiang Wang. Skin lesion segmentation via generative adversarial networks with dual discriminators. Medical Image Analysis, 64:101716, 2020.", "[49] Maryam Asadi-Aghbolaghi, Reza Azad, Mahmood Fathy, and Sergio Escalera. Multi-level context gating of embedded collective knowledge for medical image segmentation. arXiv preprint arXiv:2003.05056, 2020.", "[50] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer: Gated axial-attention for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 36\u201346. Springer, 2021.", "[4] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.", "[14] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.", "[37] Huisi Wu, Shihuai Chen, Guilian Chen, Wei Wang, Baiying Lei, and Zhenkun Wen. Fat-net: Feature adaptive transformers for automated skin lesion segmentation. Medical Image Analysis, 76:102327, 2022.", "[28] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018."]}, {"table": "<table><thead><tr><th>Methods</th><th>DSC</th><th>SE</th><th>SP</th><th>ACC</th></tr></thead><tbody><tr><td>U-Net [14]</td><td>0.8545</td><td>0.8800</td><td>0.9697</td><td>0.9404</td></tr><tr><td>Att U-Net [28]</td><td>0.8566</td><td>0.8674</td><td>0.9863</td><td>0.9376</td></tr><tr><td>DAGAN [48]</td><td>0.8807</td><td>0.9072</td><td>0.9588</td><td>0.9324</td></tr><tr><td>TransUNet [4]</td><td>0.8499</td><td>0.8578</td><td>0.9653</td><td>0.9452</td></tr><tr><td>MCGU-Net [49]</td><td>0.895</td><td>0.848</td><td>0.986</td><td>0.955</td></tr><tr><td>MedT [50]</td><td>0.8389</td><td>0.8252</td><td>0.9637</td><td>0.9358</td></tr><tr><td>FAT-Net [37]</td><td>0.8903</td><td>0.9100</td><td>0.9699</td><td>0.9578</td></tr><tr><td>Proposed Method</td><td>0.8951</td><td>0.8750</td><td>0.9790</td><td>0.9580</td></tr></tbody></table>", "caption": "TABLE III: Performance comparison of the suggested network against the SOTA counterparts for skin lesion segmentation on the ISIC 2018 dataset.", "list_citation_info": ["[48] Baiying Lei, Zaimin Xia, Feng Jiang, Xudong Jiang, Zongyuan Ge, Yanwu Xu, Jing Qin, Siping Chen, Tianfu Wang, and Shuqiang Wang. Skin lesion segmentation via generative adversarial networks with dual discriminators. Medical Image Analysis, 64:101716, 2020.", "[49] Maryam Asadi-Aghbolaghi, Reza Azad, Mahmood Fathy, and Sergio Escalera. Multi-level context gating of embedded collective knowledge for medical image segmentation. arXiv preprint arXiv:2003.05056, 2020.", "[50] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer: Gated axial-attention for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 36\u201346. Springer, 2021.", "[4] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.", "[14] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.", "[37] Huisi Wu, Shihuai Chen, Guilian Chen, Wei Wang, Baiying Lei, and Zhenkun Wen. Fat-net: Feature adaptive transformers for automated skin lesion segmentation. Medical Image Analysis, 76:102327, 2022.", "[28] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018."]}, {"table": "<table><thead><tr><th>Methods</th><th>DSC</th><th>SE</th><th>SP</th><th>ACC</th></tr></thead><tbody><tr><td>U-Net [14]</td><td>0.8936</td><td>0.9125</td><td>0.9588</td><td>0.9233</td></tr><tr><td>Att U-Net [28]</td><td>0.9003</td><td>0.9205</td><td>0.9640</td><td>0.9276</td></tr><tr><td>DAGAN [48]</td><td>0.9201</td><td>0.8320</td><td>0.9640</td><td>0.9425</td></tr><tr><td>TransUNet [4]</td><td>0.8840</td><td>0.9063</td><td>0.9427</td><td>0.9200</td></tr><tr><td>MCGU-Net [49]</td><td>0.9263</td><td>0.8322</td><td>0.9714</td><td>0.9537</td></tr><tr><td>MedT [50]</td><td>0.9122</td><td>0.8472</td><td>0.9657</td><td>0.9416</td></tr><tr><td>FAT-Net [37]</td><td>0.9440</td><td>0.9441</td><td>0.9741</td><td>0.9703</td></tr><tr><td>Proposed Method</td><td>0.9437</td><td>0.9438</td><td>0.9810</td><td>0.9723</td></tr></tbody></table>", "caption": "TABLE IV: Performance comparison on PH^{2} dataset.", "list_citation_info": ["[48] Baiying Lei, Zaimin Xia, Feng Jiang, Xudong Jiang, Zongyuan Ge, Yanwu Xu, Jing Qin, Siping Chen, Tianfu Wang, and Shuqiang Wang. Skin lesion segmentation via generative adversarial networks with dual discriminators. Medical Image Analysis, 64:101716, 2020.", "[49] Maryam Asadi-Aghbolaghi, Reza Azad, Mahmood Fathy, and Sergio Escalera. Multi-level context gating of embedded collective knowledge for medical image segmentation. arXiv preprint arXiv:2003.05056, 2020.", "[50] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer: Gated axial-attention for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 36\u201346. Springer, 2021.", "[4] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.", "[14] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.", "[37] Huisi Wu, Shihuai Chen, Guilian Chen, Wei Wang, Baiying Lei, and Zhenkun Wen. Fat-net: Feature adaptive transformers for automated skin lesion segmentation. Medical Image Analysis, 76:102327, 2022.", "[28] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018."]}, {"table": "<table><thead><tr><th>Methods</th><th>mIOU</th></tr></thead><tbody><tr><td>Frequency recalibration U-Net [41]</td><td>0.9392</td></tr><tr><td>XLAB Insights [7]</td><td>0.9360</td></tr><tr><td>DSC-IITISM [7]</td><td>0.9356</td></tr><tr><td>Multi-scale attention deeplabv3+ [7]</td><td>0.9065</td></tr><tr><td>U-Net [14]</td><td>0.7665</td></tr><tr><td>Contexual attention [6]</td><td>0.9395</td></tr><tr><td>Baseline</td><td>0.9172</td></tr><tr><td>Proposed</td><td>0.9399</td></tr></tbody></table>", "caption": "TABLE V: Performance evaluation on the SegPC challenge (best result is highlighted).", "list_citation_info": ["[14] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.", "[7] Afshin Bozorgpour, Reza Azad, Eman Showkatian, and Alaa Sulaiman. Multi-scale regional attention deeplab3+: Multiple myeloma plasma cells segmentation in microscopic images. arXiv preprint arXiv:2105.06238, 2021.", "[6] Azad Reza, Heidari Moein, Wu Yuli, and Merhof Dorit. Contextual attention network: Transformer meets u-net. arXiv preprint arXiv:2203.01932, 2022.", "[41] Reza Azad, Afshin Bozorgpour, Maryam Asadi-Aghbolaghi, Dorit Merhof, and Sergio Escalera. Deep frequency re-calibration u-net for medical image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3274\u20133283, 2021."]}]}