{"title": "End-to-end object detection with fully convolutional network", "abstract": "Mainstream object detectors based on the fully convolutional network has achieved impressive performance. While most of them still need a hand-designed non-maximum suppression (NMS) post-processing, which impedes fully end-to-end training. In this paper, we give the analysis of discarding NMS, where the results reveal that a proper label assignment plays a crucial role. To this end, for fully convolutional detectors, we introduce a Prediction-aware One-To-One (POTO) label assignment for classification to enable end-to-end detection, which obtains comparable performance with NMS. Besides, a simple 3D Max Filtering (3DMF) is proposed to utilize the multi-scale features and improve the discriminability of convolutions in the local region. With these techniques, our end-to-end framework achieves competitive performance against many state-of-the-art detectors with NMS on COCO and CrowdHuman datasets. The code is available at https://github.com/Megvii-BaseDetection/DeFCN .", "authors": ["Jianfeng Wang", " Lin Song", " Zeming Li", " Hongbin Sun", " Jian Sun", " Nanning Zheng"], "pdf_url": "https://arxiv.org/abs/2012.03544", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Assignment</th><th rowspan=\"2\">Rule</th><th rowspan=\"2\">Method</th><td colspan=\"3\">mAP</td><td colspan=\"3\">mAR</td></tr><tr><td>w/ NMS</td><td>w/o NMS</td><td>\\Delta</td><td>w/ NMS</td><td>w/o NMS</td><td>\\Delta</td></tr><tr><th>One-to-many</th><th>Hand-designed</th><th>FCOS [46] baseline <sup>*</sup></th><td>40.5</td><td>12.1</td><td>-28.4</td><td>58.3</td><td>52.8</td><td>-5.5</td></tr><tr><th rowspan=\"4\">One-to-one</th><th rowspan=\"2\">Hand-designed</th><th>Anchor</th><td>37.2</td><td>35.8</td><td>-1.4</td><td>57.0</td><td>59.2</td><td>+2.2</td></tr><tr><th>Center</th><td>37.2</td><td>33.6</td><td>-3.6</td><td>57.8</td><td>59.7</td><td>+1.9</td></tr><tr><th rowspan=\"3\">Prediction-aware</th><th>Foreground loss</th><td>38.3</td><td>37.1</td><td>-1.2</td><td>58.6</td><td>61.4</td><td>+2.8</td></tr><tr><th>POTO</th><td>38.6</td><td>38.0</td><td>-0.6</td><td>57.9</td><td>60.5</td><td>+2.6</td></tr><tr><th></th><th>POTO+3DMF</th><td>40.0</td><td>39.8</td><td>-0.2</td><td>58.8</td><td>60.9</td><td>+2.1</td></tr><tr><th>Mixture <sup>**</sup></th><th>Prediction-aware</th><th>POTO+3DMF+Aux</th><td>41.2</td><td>41.1</td><td>-0.1</td><td>58.9</td><td>61.2</td><td>+2.3</td></tr></tbody></table><ul><li>*<p>We remove its centerness branch to achieve a head-to-head comparison.</p></li><li>**<p>We adopt a one-to-one assignment in POTO and a one-to-many assignment in the auxiliary loss, respectively.</p></li></ul>", "caption": "Table 1: The comparison of different label assignment rules for end-to-end object detection on COCO val set. \\Delta indicates the gap between with and without NMS. \u2018Aux\u2019 is the proposed auxiliary loss. All models are based on ResNet-50 backbone with 180k training iterations.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019."]}, {"table": "<table><tbody><tr><th>Model</th><td>Across scales</td><td>Spatial range</td><td>mAP</td></tr><tr><th rowspan=\"6\">FCOS [46]</th><td rowspan=\"3\">\u2717</td><td>1\\times 1</td><td>19.0</td></tr><tr><td>3\\times 3</td><td>37.4</td></tr><tr><td>5\\times 5</td><td>39.2</td></tr><tr><td>\u2717</td><td rowspan=\"2\">\\infty\\times\\infty</td><td>39.2</td></tr><tr><td>\u2713</td><td>40.9</td></tr></tbody></table>", "caption": "Table 2: Comparison of different configurations for NMS post-processing on COCO val set. \u2018Across scales\u2019 indicates applying NMS to the multiple adjacent stages of the feature pyramid network. \u2018Spatial range\u2019 denotes the spatial range for duplicate removal in each scale.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019."]}, {"table": "<table><thead><tr><th>\\alpha</th><th>center sampling</th><th>inside box</th><th>global</th></tr></thead><tbody><tr><th>0.0</th><td>33.5 / 33.6</td><td>24.1 / 24.2</td><td>1.9 / 2.1</td></tr><tr><th>0.2</th><td>33.7 / 33.9</td><td>28.8 / 28.8</td><td>19.4 / 19.5</td></tr><tr><th>0.4</th><td>35.0 / 35.2</td><td>32.7 / 32.8</td><td>28.3 / 28.4</td></tr><tr><th>0.6</th><td>36.6 / 36.9</td><td>35.3 / 35.5</td><td>34.7 / 34.9</td></tr><tr><th>0.8</th><td>38.0 / 38.6</td><td>37.4 / 37.9</td><td>37.3 / 37.9</td></tr><tr><th>1.0</th><td>11.8 / 29.7</td><td>4.5 / 13.0</td><td>non-convergence</td></tr></tbody></table>", "caption": "Table 3: Results of POTO with different configurations of \\alpha and spatial prior on COCO val set. \\alpha=0 is equivalent to considering classification alone, \\alpha=1 is equivalent to considering regression alone. \u2018center sampling\u2019 and \u2018inside box\u2019 both follow FCOS [46]. \u2018/\u2019 is used to distinguish between results without and with NMS.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019."]}, {"table": "<table><tbody><tr><th>Model</th><td>3DMF</td><td>Aux Loss</td><td>mAP</td></tr><tr><th rowspan=\"3\">FCOS [46]</th><td>\u2717</td><td>\u2717</td><td>19.0 / 40.9</td></tr><tr><td>\u2717</td><td>\u2713</td><td>18.9 / 41.3</td></tr><tr><td>\u2713 <sup>*</sup></td><td>\u2717</td><td>38.7 / 40.0</td></tr><tr><th rowspan=\"3\">Ours</th><td>\u2717</td><td>\u2717</td><td>38.0 / 38.6</td></tr><tr><td>\u2713</td><td>\u2717</td><td>39.8 / 40.0</td></tr><tr><td>\u2713</td><td>\u2713</td><td>41.1 / 41.2</td></tr></tbody></table><ul><li>*<p>We modify 3D Max Filtering as a post-processing.</p></li></ul>", "caption": "Table 5: The effect of sub-modules in the proposed 3DMF module on COCO val set. \u20183DMF\u2019 and \u2018Aux Loss\u2019 indicate using the 3D Max Filtering and the auxiliary loss, respectively. \u2018/\u2019 is used to distinguish between results without and with NMS.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019."]}, {"table": "<table><tbody><tr><td>Backbone</td><td>Model</td><td>Epochs</td><td>mAP</td></tr><tr><td rowspan=\"4\">ResNet-101</td><td>RetinaNet [22]</td><td>36</td><td>41.0</td></tr><tr><td>FCOS [46]</td><td>36</td><td>43.1</td></tr><tr><td>DETR [3]</td><td>500</td><td>43.5</td></tr><tr><td>Ours (w/o NMS)</td><td>36</td><td>43.6</td></tr><tr><td rowspan=\"3\">ResNeXt-101+DCN</td><td>RetinaNet [22]</td><td>24</td><td>44.5</td></tr><tr><td>FCOS [46]</td><td>24</td><td>46.5</td></tr><tr><td>Ours (w/o NMS)</td><td>24</td><td>47.6</td></tr></tbody></table>", "caption": "Table 7: The experiments of the proposed framework with larger backbone on COCO2017 test-dev set. The hyper-parameters of all the models follow the official settings.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019.", "[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In IEEE International Conference on Computer Vision, 2017.", "[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. European Conference on Computer Vision, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><th>Epochs</th><td>AP{}_{50}</td><td>mMR</td><td>Recall</td></tr><tr><th>RetinaNet [22]</th><th>32</th><td>81.7</td><td>57.6</td><td>88.6</td></tr><tr><th>FCOS [46]</th><th>32</th><td>86.1</td><td>54.9</td><td>94.2</td></tr><tr><th>ATSS [50]</th><th>32</th><td>87.2</td><td>49.7</td><td>94.0</td></tr><tr><th>DETR [3]</th><th>300</th><td>72.8</td><td>80.1</td><td>82.7</td></tr><tr><th>Ground-truth (w/ NMS)</th><th>-</th><td>-</td><td>-</td><td>95.1</td></tr><tr><th>POTO</th><th>32</th><td>88.5</td><td>52.2</td><td>96.3</td></tr><tr><th>POTO+3DMF</th><th>32</th><td>88.8</td><td>51.0</td><td>96.6</td></tr><tr><th>POTO+3DMF+Aux</th><th>32</th><td>89.1</td><td>48.9</td><td>96.5</td></tr></tbody></table>", "caption": "Table 8: The comparison of fully convolutional detectors on CrowdHuman val set. All models are based on the ResNet-50 backbone. \u2018Aux\u2019 indicates the auxiliary loss.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019.", "[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In IEEE International Conference on Computer Vision, 2017.", "[50] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.", "[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. European Conference on Computer Vision, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><td>mAP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><th>None</th><td>39.8 / 40.0</td><td>57.4 / 59.1</td><td>43.6 / 43.1</td></tr><tr><th colspan=\"4\">Hand-designed</th></tr><tr><th>FCOS [46]</th><td>39.4 / 39.8</td><td>57.0 / 59.1</td><td>43.4 / 43.0</td></tr><tr><th>ATSS [50]</th><td>39.8 / 40.1</td><td>57.5 / 59.5</td><td>44.1 / 43.4</td></tr><tr><th colspan=\"4\">Prediction-aware</th></tr><tr><th>Quality-FCOS</th><td>39.7 / 40.0</td><td>57.7 / 59.6</td><td>43.6 / 43.0</td></tr><tr><th>Quality-ATSS</th><td>41.1 / 41.2</td><td>59.0 / 60.7</td><td>45.4 / 44.8</td></tr><tr><th>Quality-Top-k</th><td>40.7 / 41.0</td><td>58.7 / 60.4</td><td>44.9 / 44.3</td></tr></tbody></table>", "caption": "Table 9: The results of different one-to-many label assignment rules for the auxiliary loss on COCO val set. All the models are based on the ResNet-50 backbone. \u2018/\u2019 is used to distinguish between results without and with NMS.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019.", "[50] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In IEEE Conference on Computer Vision and Pattern Recognition, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Epochs</th><th>\\rm{mAP}</th><th>\\rm{AP_{s}}</th><th>\\rm{AP_{m}}</th><th>\\rm{AP_{l}}</th><th>\\rm{\\#Param}</th></tr></thead><tbody><tr><th>DETR [3]</th><th>500</th><th>42.0</th><td>20.5</td><td>45.8</td><td>61.1</td><td>41.5 M</td></tr><tr><th>FCOS [46]</th><th>36</th><th>41.1</th><td>25.9</td><td>44.8</td><td>52.3</td><td>36.4 M</td></tr><tr><th>Ours</th><th>36</th><th>41.5</th><td>26.4</td><td>44.7</td><td>52.8</td><td>37.0 M</td></tr><tr><th>Ours<sup>*</sup></th><th>36</th><th>43.5</th><td>26.3</td><td>46.6</td><td>55.4</td><td>40.3 M</td></tr></tbody></table><ul><li>*<p>adopts two extra deformable convolutions in the head.</p></li></ul>", "caption": "Table 10: The comparison on COCO val set.", "list_citation_info": ["[46] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In IEEE International Conference on Computer Vision, 2019.", "[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. European Conference on Computer Vision, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Queries</th><th>Epochs</th><th>\\rm{AP_{50}}</th><th>\\rm{mMR}</th><th>\\rm{Recall}</th></tr></thead><tbody><tr><td>DETR [3]</td><td>100</td><td>300</td><td>72.8</td><td>80.1</td><td>82.7</td></tr><tr><td>DETR</td><td>200</td><td>300</td><td>78.8</td><td>66.3</td><td>90.2</td></tr><tr><td>DETR</td><td>300</td><td>300</td><td>70.6</td><td>79.1</td><td>89.7</td></tr><tr><td>Ours</td><td>-</td><td>32</td><td>89.1</td><td>48.9</td><td>96.5</td></tr></tbody></table>", "caption": "Table 11: The comparison on CrowdHuman val set.", "list_citation_info": ["[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. European Conference on Computer Vision, 2020."]}]}