{"title": "Sa-det3d: Self-attention based context-aware 3d object detection", "abstract": "Existing point-cloud based 3D object detectors use convolution-like operators to process information in a local neighbourhood with fixed-weight kernels and aggregate global context hierarchically. However, non-local neural networks and self-attention for 2D vision have shown that explicitly modeling long-range interactions can lead to more robust and competitive models. In this paper, we propose two variants of self-attention for contextual modeling in 3D object detection by augmenting convolutional features with self-attention features. We first incorporate the pairwise self-attention mechanism into the current state-of-the-art BEV, voxel and point-based detectors and show consistent improvement over strong baseline models of up to 1.5 3D AP while simultaneously reducing their parameter footprint and computational cost by 15-80% and 30-50%, respectively, on the KITTI validation set. We next propose a self-attention variant that samples a subset of the most representative features by learning deformations over randomly sampled locations. This not only allows us to scale explicit global contextual modeling to larger point-clouds, but also leads to more discriminative and informative feature descriptors. Our method can be flexibly applied to most state-of-the-art detectors with increased accuracy and parameter and compute efficiency. We show our proposed method improves 3D object detection performance on KITTI, nuScenes and Waymo Open datasets. Code is available at https://github.com/AutoVision-cloud/SA-Det3D.", "authors": ["Prarthana Bhattacharyya", " Chengjie Huang", " Krzysztof Czarnecki"], "pdf_url": "https://arxiv.org/abs/2101.02672", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>Task</td><td>Modality</td><td>Context</td><td>Scalability</td><td> Attention + ConvolutionCombination </td><td>Stage Added</td></tr><tr><td>HG-Net [3]</td><td>detection</td><td>points</td><td>global-static</td><td>-</td><td>gating</td><td>Attention modules are</td></tr><tr><td>PCAN [55]</td><td>place-recognition</td><td>points</td><td>local-adaptive</td><td>-</td><td>gating</td><td>added at the end.</td></tr><tr><td>Point-GNN [33]</td><td>detection</td><td>points</td><td>local-adaptive</td><td>-</td><td>-</td><td></td></tr><tr><td>GAC [40]</td><td>segmentation</td><td>points</td><td>local-adaptive</td><td>-</td><td>-</td><td>Attention modules fully</td></tr><tr><td>PAT [49]</td><td>classification</td><td>points</td><td>global-adaptive</td><td>randomly sample points subset</td><td>-</td><td>replace convolution and</td></tr><tr><td>ASCN [47]</td><td>segmentation</td><td>points</td><td>global-adaptive</td><td>randomly sample points subset</td><td>-</td><td>set-abstraction layers.</td></tr><tr><td>Pointformer [22]</td><td>detection</td><td>points</td><td>global-adaptive</td><td>sample points subset and refine</td><td>-</td><td></td></tr><tr><td>MLCVNet [46]</td><td>detection</td><td>points</td><td>global-static</td><td>-</td><td>residual addition</td><td></td></tr><tr><td>TANet [17]</td><td>detection</td><td>voxels</td><td>local-adaptive</td><td>-</td><td>gating</td><td>Attention modules are</td></tr><tr><td>PMPNet [51]</td><td>detection</td><td>pillars</td><td>local-adaptive</td><td>-</td><td>gated-recurrent-unit</td><td>inserted into</td></tr><tr><td>SCANet [18]</td><td>detection</td><td>BEV</td><td>global-static</td><td>-</td><td>gating</td><td>the backbone.</td></tr><tr><td>A-PointNet [21]</td><td>detection</td><td>points</td><td>global-adaptive</td><td>attend sequentially to small regions</td><td>gating</td><td></td></tr><tr><td> Ours(FSA/DSA) </td><td>detection</td><td> points, voxels,pillars, hybrid </td><td>global-adaptive</td><td> attend to salient regionsusing learned deformations </td><td>residual addition</td><td> Attention modules areinserted intothe backbone. </td></tr></table>", "caption": "Table 1: Properties of recent attention-based models for point-clouds", "list_citation_info": ["[51] Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, and Ruigang Yang. LiDAR-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. In CVPR, 2020.", "[40] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang, and Jie Shan. Graph attention convolution for point cloud semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10296\u201310305. Computer Vision Foundation / IEEE, 2019.", "[21] Anshul Paigwar, \u00d6zg\u00fcr Erkent, Christian Wolf, and Christian Laugier. Attentional pointnet for 3d-object detection in point clouds. In IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2019, Long Beach, CA, USA, June 16-20, 2019, pages 1297\u20131306. Computer Vision Foundation / IEEE, 2019.", "[3] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z. Chen, and Jian Wu. A hierarchical graph network for 3d object detection on point clouds. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 389\u2013398. IEEE, 2020.", "[17] Zhe Liu, Xin Zhao, Tengteng Huang, Ruolan Hu, Yu Zhou, and Xiang Bai. TANet: Robust 3d object detection from point clouds with triple attention. AAAI, 2020.", "[47] Saining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. Attentional shapecontextnet for point cloud recognition. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 4606\u20134615. IEEE Computer Society, 2018.", "[33] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 1708\u20131716. IEEE, 2020.", "[18] Haihua Lu, Xuesong Chen, Guiying Zhang, Qiuhao Zhou, Yanbo Ma, and Yong Zhao. SCANet: Spatial-channel attention network for 3d object detection. In ICASSP. IEEE, 2019.", "[49] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li, Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point clouds with self-attention and gumbel subset sampling. In CVPR, 2019.", "[46] Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, and Jun Wang. MLCVNet: Multi-level context votenet for 3d object detection. In CVPR, 2020.", "[22] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao Huang. 3d object detection with pointformer. CoRR, abs/2012.11409, 2020.", "[55] Wenxiao Zhang and Chunxia Xiao. PCAN: 3d attention map learning using contextual information for point cloud based retrieval. CoRR, abs/1904.09793, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"4\">PointPillars [15]</td><td colspan=\"4\">SECOND [48]</td><td colspan=\"4\">Point-RCNN [32]</td><td colspan=\"4\">PV-RCNN [31]</td></tr><tr><td>3D</td><td>BEV</td><td>Param</td><td>FLOPs</td><td>3D</td><td>BEV</td><td>Param</td><td>FLOPs</td><td>3D</td><td>BEV</td><td>Param</td><td>FLOPs</td><td>3D</td><td>BEV</td><td>Param</td><td>FLOPs</td></tr><tr><td>Baseline</td><td>78.39</td><td>88.06</td><td>4.8 M</td><td>63.4 G</td><td>81.61</td><td>88.55</td><td>4.6 M</td><td>76.9 G</td><td>80.52</td><td>88.80</td><td>4.0 M</td><td>27.4 G</td><td>84.83</td><td>91.11</td><td>12 M</td><td>89 G</td></tr><tr><td>DSA</td><td>78.94</td><td>88.39</td><td>1.1 M</td><td>32.4 G</td><td>82.03</td><td>89.82</td><td>2.2 M</td><td>52.6 G</td><td>81.80</td><td>88.14</td><td>2.3 M</td><td>19.3 G</td><td>84.71</td><td>90.72</td><td>10 M</td><td>64 G</td></tr><tr><td>FSA</td><td>79.04</td><td>88.47</td><td>1.0 M</td><td>31.7 G</td><td>81.86</td><td>90.01</td><td>2.2 M</td><td>51.9 G</td><td>82.10</td><td>88.37</td><td>2.5 M</td><td>19.8 G</td><td>84.95</td><td>90.92</td><td>10 M</td><td>64.3 G</td></tr><tr><td>Improve.</td><td>+0.65</td><td>+0.41</td><td>-79%</td><td>-50%</td><td>+0.42</td><td>+1.46</td><td>-52%</td><td>-32%</td><td>+1.58</td><td>-</td><td>-37%</td><td>-38%</td><td>+0.12</td><td>-</td><td>-16%</td><td>-27%</td></tr></table>", "caption": "Table 2: Performance comparison for moderate difficulty Car class on KITTI val split with 40 recall positions", "list_citation_info": ["[32] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. PointRCNN: 3d object proposal generation and detection from point cloud. In CVPR, 2019.", "[48] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.", "[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-voxel feature set abstraction for 3d object detection. In CVPR, 2020.", "[15] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In CVPR, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">Car - 3D</td><td colspan=\"3\">Car - BEV</td><td colspan=\"3\">Cyclist - 3D</td><td colspan=\"3\">Cyclist - BEV</td></tr><tr><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td></tr><tr><td>MV3D [5]</td><td>74.97</td><td>63.63</td><td>54.00</td><td>86.62</td><td>78.93</td><td>69.80</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PointPillars [15]</td><td>82.58</td><td>74.31</td><td>68.99</td><td>90.07</td><td>86.56</td><td>82.81</td><td>77.10</td><td>58.65</td><td>51.92</td><td>79.90</td><td>62.73</td><td>55.58</td></tr><tr><td>SECOND [48]</td><td>83.34</td><td>72.55</td><td>65.82</td><td>89.39</td><td>83.77</td><td>78.59</td><td>71.33</td><td>52.08</td><td>45.83</td><td>76.50</td><td>56.05</td><td>49.45</td></tr><tr><td>PointRCNN [32]</td><td>86.96</td><td>75.64</td><td>70.70</td><td>92.13</td><td>87.39</td><td>82.72</td><td>74.96</td><td>58.82</td><td>52.53</td><td>82.56</td><td>67.24</td><td>60.28</td></tr><tr><td>STD [52]</td><td>87.95</td><td>79.71</td><td>75.09</td><td>94.74</td><td>89.19</td><td>86.42</td><td>78.69</td><td>61.59</td><td>55.30</td><td>81.36</td><td>67.23</td><td>59.35</td></tr><tr><td>3DSSD [50]</td><td>88.36</td><td>79.57</td><td>74.55</td><td>92.66</td><td>89.02</td><td>85.86</td><td>82.48</td><td>64.10</td><td>56.90</td><td>85.04</td><td>67.62</td><td>61.14</td></tr><tr><td>SA-SSD [12]</td><td>88.75</td><td>79.79</td><td>74.16</td><td>95.03</td><td>91.03</td><td>85.96</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TANet [17]</td><td>83.81</td><td>75.38</td><td>67.66</td><td>-</td><td>-</td><td>-</td><td>73.84</td><td>59.86</td><td>53.46</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Point-GNN [33]</td><td>88.33</td><td>79.47</td><td>72.29</td><td>93.11</td><td>89.17</td><td>83.90</td><td>78.60</td><td>63.48</td><td>57.08</td><td>81.17</td><td>67.28</td><td>59.67</td></tr><tr><td>PV-RCNN [31]</td><td>90.25</td><td>81.43</td><td>76.82</td><td>94.98</td><td>90.65</td><td>86.14</td><td>78.60</td><td>63.71</td><td>57.65</td><td>82.49</td><td>68.89</td><td>62.41</td></tr><tr><td>PV-RCNN + DSA (Ours)</td><td>88.25</td><td>81.46</td><td>76.96</td><td>92.42</td><td>90.13</td><td>85.93</td><td>82.19</td><td>68.54</td><td>61.33</td><td>83.93</td><td>72.61</td><td>65.82</td></tr></table>", "caption": "Table 3: Performance comparison of 3D detection on KITTI test split with AP calculated with 40 recall positions. The best and second-best performances are highlighted across all datasets.", "list_citation_info": ["[17] Zhe Liu, Xin Zhao, Tengteng Huang, Ruolan Hu, Yu Zhou, and Xiang Bai. TANet: Robust 3d object detection from point clouds with triple attention. AAAI, 2020.", "[33] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 1708\u20131716. IEEE, 2020.", "[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-voxel feature set abstraction for 3d object detection. In CVPR, 2020.", "[15] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In CVPR, 2019.", "[50] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3DSSD: Point-based 3d single stage object detector. In CVPR, 2020.", "[5] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In CVPR, 2017.", "[12] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detection from point cloud. In CVPR, 2020.", "[52] Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, and Ruigang Yang. LiDAR-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. In CVPR, 2020.", "[32] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. PointRCNN: 3d object proposal generation and detection from point cloud. In CVPR, 2019.", "[48] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018."]}, {"table": "<table><tr><td>Model</td><td>Mode</td><td>mAP</td><td>NDS</td><td>Car</td><td>Truck</td><td>Bus</td><td>Trailer</td><td>CV</td><td>Ped</td><td>Moto</td><td>Bike</td><td>Tr. Cone</td><td>Barrier</td></tr><tr><td>PointPillars [15]</td><td>Lidar</td><td>30.5</td><td>45.3</td><td>68.4</td><td>23.0</td><td>28.2</td><td>23.4</td><td>4.1</td><td>59.7</td><td>27.4</td><td>1.1</td><td>30.8</td><td>38.9</td></tr><tr><td>WYSIWYG [13]</td><td>Lidar</td><td>35.0</td><td>41.9</td><td>79.1</td><td>30.4</td><td>46.6</td><td>40.1</td><td>7.1</td><td>65.0</td><td>18.2</td><td>0.1</td><td>28.8</td><td>34.7</td></tr><tr><td>PointPillars+ [39]</td><td>Lidar</td><td>40.1</td><td>55.0</td><td>76.0</td><td>31.0</td><td>32.1</td><td>36.6</td><td>11.3</td><td>64.0</td><td>34.2</td><td>14.0</td><td>45.6</td><td>56.4</td></tr><tr><td>PMPNet [51]</td><td>Lidar</td><td>45.4</td><td>53.1</td><td>79.7</td><td>33.6</td><td>47.1</td><td>43.0</td><td>18.1</td><td>76.5</td><td>40.7</td><td>7.9</td><td>58.8</td><td>48.8</td></tr><tr><td>SSN [59]</td><td>Lidar</td><td>46.3</td><td>56.9</td><td>80.7</td><td>37.5</td><td>39.9</td><td>43.9</td><td>14.6</td><td>72.3</td><td>43.7</td><td>20.1</td><td>54.2</td><td>56.3</td></tr><tr><td>Point-Painting [39]</td><td>RGB + Lidar</td><td>46.4</td><td>58.1</td><td>77.9</td><td>35.8</td><td>36.2</td><td>37.3</td><td>15.8</td><td>73.3</td><td>41.5</td><td>24.1</td><td>62.4</td><td>60.2</td></tr><tr><td>PointPillars + DSA (Ours)</td><td>Lidar</td><td>47.0</td><td>59.2</td><td>81.2</td><td>43.8</td><td>57.2</td><td>47.8</td><td>11.3</td><td>73.3</td><td>32.1</td><td>7.9</td><td>60.6</td><td>55.3</td></tr></table>", "caption": "Table 4: Performance comparison of 3D detection with PointPillars backbone on nuScenes test split. \u201cCV\u201d, \u201dPed\u201d , \u201cMoto\u201d, \u201cBike\u201d, \u201cTr. Cone\u201d indicate construction vehicle, pedestrian, motorcycle, bicycle and traffic cone respectively.The values are taken from the official evaluation server https://eval.ai/web/challenges/challenge-page/356/leaderboard/1012.", "list_citation_info": ["[51] Junbo Yin, Jianbing Shen, Chenye Guan, Dingfu Zhou, and Ruigang Yang. LiDAR-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention. In CVPR, 2020.", "[59] Xinge Zhu, Yuexin Ma, Tai Wang, Yan Xu, Jianping Shi, and Dahua Lin. SSN: shape signature networks for multi-class object detection from point clouds. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXV, volume 12370 of Lecture Notes in Computer Science, pages 581\u2013597. Springer, 2020.", "[39] Sourabh Vora, Alex H Lang, Bassam Helou, and Oscar Beijbom. Pointpainting: Sequential fusion for 3d object detection. In CVPR, 2020.", "[13] Peiyun Hu, Jason Ziglar, David Held, and Deva Ramanan. What you see is what you get: Exploiting visibility for 3d object detection. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10998\u201311006. IEEE, 2020.", "[15] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In CVPR, 2019."]}, {"table": "<table><tr><td>Difficulty</td><td>Method</td><td colspan=\"2\">Vehicle</td></tr><tr><td></td><td></td><td>3D AP</td><td>3D APH</td></tr><tr><td></td><td>StarNet [19]</td><td>53.7</td><td>-</td></tr><tr><td></td><td>PointPillars [15]</td><td>56.6</td><td>-</td></tr><tr><td></td><td>PPBA [6]</td><td>62.4</td><td>-</td></tr><tr><td></td><td>MVF [5]</td><td>62.9</td><td>-</td></tr><tr><td>L1</td><td>AFDet [9]</td><td>63.7</td><td>-</td></tr><tr><td></td><td>CVCNet [4]</td><td>65.2</td><td>-</td></tr><tr><td></td><td>Pillar-OD [42]</td><td>69.8</td><td>-</td></tr><tr><td></td><td>\\daggerSECOND [48]</td><td>70.2</td><td>69.7</td></tr><tr><td></td><td>PV-RCNN [31]</td><td>70.3</td><td>69.7</td></tr><tr><td></td><td>SECOND + DSA (Ours)</td><td>71.1</td><td>70.7</td></tr><tr><td>L2</td><td>\\daggerSECOND [48]</td><td>62.5</td><td>62.0</td></tr><tr><td></td><td>PV-RCNN [31]</td><td>65.4</td><td>64.8</td></tr><tr><td></td><td>SECOND + DSA (Ours)</td><td>63.4</td><td>63.0</td></tr></table>", "caption": "Table 5: Comparison on Waymo Open Dataset validation split for 3D vehicle detection. Our DSA model has {\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}52\\%} fewer parameters and {\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}32\\%} fewer FLOPs compared to SECOND and {\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}80\\%} fewer parameters and {\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}41\\%} fewer FLOPs compared to PV-RCNN. \\daggerRe-implemented by [36]", "list_citation_info": ["[19] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Alsharif, Patrick Nguyen, Zhifeng Chen, Jonathon Shlens, and Vijay Vasudevan. Starnet: Targeted computation for object detection in point clouds. CoRR, abs/1908.11069, 2019.", "[42] Yue Wang, Alireza Fathi, Abhijit Kundu, David A. Ross, Caroline Pantofaru, Thomas A. Funkhouser, and Justin Solomon. Pillar-based object detection for autonomous driving. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXII, volume 12367 of Lecture Notes in Computer Science, pages 18\u201334. Springer, 2020.", "[9] Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Sijia Chen, Li Huang, and Yuan Li. Afdet: Anchor free one stage 3d object detection. CoRR, abs/2006.12671, 2020.", "[48] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.", "[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-voxel feature set abstraction for 3d object detection. In CVPR, 2020.", "[15] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In CVPR, 2019.", "[4] Qi Chen, Lin Sun, Ernest Cheung, and Alan L. Yuille. Every view counts: Cross-view consistency in 3d object detection with hybrid-cylindrical-spherical voxelization. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.", "[6] Shuyang Cheng, Zhaoqi Leng, Ekin Dogus Cubuk, Barret Zoph, Chunyan Bai, Jiquan Ngiam, Yang Song, Benjamin Caine, Vijay Vasudevan, Congcong Li, Quoc V. Le, Jonathon Shlens, and Dragomir Anguelov. Improving 3d object detection through progressive population based augmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXI, volume 12366 of Lecture Notes in Computer Science, pages 279\u2013294. Springer, 2020.", "[36] OpenPCDet Development Team. Openpcdet: An open-source toolbox for 3d object detection from point clouds. https://github.com/open-mmlab/OpenPCDet, 2020.", "[5] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In CVPR, 2017."]}, {"table": "<table><tr><td>Attribute</td><td>PP [15]</td><td>PP{}_{red}</td><td>FSA-PP</td><td>DSA-PP</td></tr><tr><td colspan=\"5\">Layer: 2D CNN Backbone</td></tr><tr><td>Layer-nums</td><td>[3, 5, 5]</td><td>[3, 5, 5]</td><td>[3, 5, 5]</td><td>[3, 5, 5]</td></tr><tr><td>Layer-stride</td><td>[2, 2, 2]</td><td>[2, 2, 2]</td><td>[2, 2, 2]</td><td>[2, 2, 2]</td></tr><tr><td>Num-filters</td><td>[64, 128, 256]</td><td>[64, 64, 128]</td><td>[64, 64, 64]</td><td>[64, 64, 64]</td></tr><tr><td>Upsample-stride</td><td>[1, 2, 4]</td><td>[1, 2, 4]</td><td>[1, 2, 4]</td><td>[1, 2, 4]</td></tr><tr><td>Num-upsample-filters</td><td>[128, 128, 128]</td><td>[128, 128, 128]</td><td>[128, 128, 128]</td><td>[128, 128, 128]</td></tr><tr><td colspan=\"5\">Layer: Self-Attention</td></tr><tr><td>Stage Added</td><td>-</td><td>-</td><td>Pillar feature</td><td>Pillar feature</td></tr><tr><td>Num layers</td><td>-</td><td>-</td><td>2</td><td>2</td></tr><tr><td>Num heads</td><td>-</td><td>-</td><td>4</td><td>4</td></tr><tr><td>Context Linear Dim</td><td>-</td><td>-</td><td>64</td><td>64</td></tr><tr><td>Num Keypoints</td><td>-</td><td>-</td><td>-</td><td>2048</td></tr><tr><td>Deform radius</td><td>-</td><td>-</td><td>-</td><td>3.0m</td></tr><tr><td>Feature pool radius</td><td>-</td><td>-</td><td>-</td><td>2.0m</td></tr><tr><td>Interpolation MLP Dim</td><td>-</td><td>-</td><td>-</td><td>64</td></tr><tr><td>Interpolation radius</td><td>-</td><td>-</td><td>-</td><td>1.6m</td></tr><tr><td>Interpolation samples</td><td>-</td><td>-</td><td>-</td><td>16</td></tr></table>", "caption": "Table 8: Architectural details of PointPillars [15], our reduced parameter PointPillars version, proposed FSA-PointPillars and DSA-PointPillars <br/><br/>", "list_citation_info": ["[15] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In CVPR, 2019."]}, {"table": "<table><tr><td>Attribute</td><td>SECOND [48]</td><td>SECOND{}_{red}</td><td>FSA-SECOND</td><td>DSA-SECOND</td></tr><tr><td colspan=\"5\">Layer: 3D CNN Backbone</td></tr><tr><td>Layer-nums in Sparse Blocks</td><td>[1, 3, 3, 3]</td><td>[1, 3, 3, 2]</td><td>[1, 3, 3, 2]</td><td>[1, 3, 3, 2]</td></tr><tr><td>Sparse tensor size</td><td>128</td><td>64</td><td>64</td><td>64</td></tr><tr><td colspan=\"5\">Layer: 2D CNN Backbone</td></tr><tr><td>Layer-nums</td><td>[5, 5]</td><td>[5, 5]</td><td>[5, 5]</td><td>[5, 5]</td></tr><tr><td>Layer-stride</td><td>[1, 2]</td><td>[1, 2]</td><td>[1, 2]</td><td>[1, 2]</td></tr><tr><td>Num-filters</td><td>[128, 256]</td><td>[128, 160]</td><td>[128, 128]</td><td>[128, 128]</td></tr><tr><td>Upsample-stride</td><td>[1, 2]</td><td>[1, 2]</td><td>[1, 2]</td><td>[1, 2]</td></tr><tr><td>Num-upsample-filters</td><td>[256, 256]</td><td>[256, 256]</td><td>[256, 256]</td><td>[256, 256]</td></tr><tr><td colspan=\"5\">Layer: Self-Attention</td></tr><tr><td>Stage Added</td><td>-</td><td>-</td><td>Sparse Tensor</td><td>Sparse Tensor</td></tr><tr><td>Num layers</td><td>-</td><td>-</td><td>2</td><td>2</td></tr><tr><td>Num heads</td><td>-</td><td>-</td><td>4</td><td>4</td></tr><tr><td>Context Linear Dim</td><td>-</td><td>-</td><td>64</td><td>64</td></tr><tr><td>Num Keypoints</td><td>-</td><td>-</td><td>-</td><td>2048</td></tr><tr><td>Deform radius</td><td>-</td><td>-</td><td>-</td><td>4.0m</td></tr><tr><td>Feature pool radius</td><td>-</td><td>-</td><td>-</td><td>4.0m</td></tr><tr><td>Interpolation MLP Dim</td><td>-</td><td>-</td><td>-</td><td>64</td></tr><tr><td>Interpolation radius</td><td>-</td><td>-</td><td>-</td><td>1.6m</td></tr><tr><td>Interpolation samples</td><td>-</td><td>-</td><td>-</td><td>16</td></tr></table>", "caption": "Table 9: Architectural details of SECOND [48], our reduced parameter SECOND version, and proposed FSA-SECOND and DSA-SECOND", "list_citation_info": ["[48] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018."]}, {"table": "<table><thead><tr><th> Attribute </th><th> Point-RCNN [32] </th><th> Point-RCNN{}_{red} </th><th> FSA-Point-RCNN </th><th> DSA-Point-RCNN </th></tr></thead><tbody><tr><td colspan=\"5\">Layer: Multi-Scale Aggregation</td></tr><tr><td>N-Points</td><td>[4096, 1024, 256, 64]</td><td>[4096, 1024, 256, 64]</td><td>[4096, 1024, 256, 64]</td><td>[4096, 1024, 128, 64]</td></tr><tr><td>Radius</td><td> [0.1, 0.5], [0.5, 1.0],[1.0, 2.0], [2.0, 4.0] </td><td> [0.1, 0.5], [0.5, 1.0],[1.0, 2.0], [2.0, 4.0] </td><td> [0.1, 0.5], [0.5, 1.0],[1.0, 2.0], [2.0, 4.0] </td><td> [0.1, 0.5], [0.5, 1.0],[1.0, 2.0], [2.0, 4.0] </td></tr><tr><td>N-samples</td><td>[16, 32]</td><td>[16, 32]</td><td>[16, 32]</td><td>[16, 32]</td></tr><tr><td>MLPs</td><td> [16, 16, 32], [32, 32, 64],[64, 64, 128], [64, 96, 128][128, 196, 256], [128, 196, 256][256, 256, 512], [256, 384, 512] </td><td> [16, 32], [32, 64],[64, 128], [64, 128][128, 256], [128, 256][256, 512], [256, 512] </td><td> [16, 32], [32, 64],[64, 128], [64, 128][128, 256], [128, 256][256, 512], [256, 512] </td><td> [16, 32], [32, 64],[64, 128], [64, 128][128, 256], [128, 256][256, 512], [256, 512] </td></tr><tr><td>FP-MLPs</td><td> [128, 128],[256, 256],[512, 512],[512, 512] </td><td> [128, 128],[128, 128],[128, 128],[128, 512] </td><td> [128, 128],[128, 128],[128, 128],[128, 128] </td><td> [128, 128],[128, 128],[128, 128],[128, 128] </td></tr><tr><td colspan=\"5\">Layer: Self-Attention</td></tr><tr><td>Stage Added</td><td>-</td><td>-</td><td>MSG-3 and MSG-4</td><td>MSG-3 and MSG-4</td></tr><tr><td>Num layers</td><td>-</td><td>-</td><td>2</td><td>2</td></tr><tr><td>Num heads</td><td>-</td><td>-</td><td>4</td><td>4</td></tr><tr><td>Context Linear Dim</td><td>-</td><td>-</td><td>64</td><td>64</td></tr><tr><td>Num Keypoints</td><td>-</td><td>-</td><td>-</td><td>(128, 64)</td></tr><tr><td>Deform radius</td><td>-</td><td>-</td><td>-</td><td>(2.0, 4.0)m</td></tr><tr><td>Feature pool radius</td><td>-</td><td>-</td><td>-</td><td>(1.0, 2.0)m</td></tr><tr><td>Interpolation MLP Dim</td><td>-</td><td>-</td><td>-</td><td>(64, 64)</td></tr><tr><td>Interpolation radius</td><td>-</td><td>-</td><td>-</td><td>(1.0, 2.0)m</td></tr><tr><td>Interpolation samples</td><td>-</td><td>-</td><td>-</td><td>(16, 16)</td></tr></tbody></table>", "caption": "Table 10: Architectural details of Point-RCNN [32], our reduced parameter Point-RCNN version, proposed FSA-Point-RCNN and DSA-Point-RCNN <br/><br/><br/><br/><br/><br/><br/><br/><br/>", "list_citation_info": ["[32] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. PointRCNN: 3d object proposal generation and detection from point cloud. In CVPR, 2019."]}, {"table": "<table><tr><td>Attribute</td><td>PV-RCNN [31]</td><td>FSA-PVRCNN</td><td>DSA-PVRCNN</td></tr><tr><td colspan=\"4\">Layer: 3D CNN Backbone</td></tr><tr><td>Layer-nums in Sparse Blocks</td><td>[1, 3, 3, 3]</td><td>[1, 3, 3, 2]</td><td>[1, 3, 3, 3]</td></tr><tr><td>Sparse tensor size</td><td>128</td><td>64</td><td>128</td></tr><tr><td colspan=\"4\">Layer: 2D CNN Backbone</td></tr><tr><td>Layer-nums</td><td>[5, 5]</td><td>[5, 5]</td><td>[5, 5]</td></tr><tr><td>Layer-stride</td><td>[1, 2]</td><td>[1, 2]</td><td>[1, 2]</td></tr><tr><td>Num-filters</td><td>[128, 256]</td><td>[128, 128]</td><td>[128, 256]</td></tr><tr><td>Upsample-stride</td><td>[1, 2]</td><td>[1, 2]</td><td>[1, 2]</td></tr><tr><td>Num-upsample-filters</td><td>[256, 256]</td><td>[256, 256]</td><td>[256, 256]</td></tr><tr><td colspan=\"4\">Layer: Self-Attention</td></tr><tr><td>Stage Added</td><td>-</td><td>Sparse Tensor and VSA</td><td>VSA</td></tr><tr><td>Num layers</td><td>-</td><td>2</td><td>2</td></tr><tr><td>Num heads</td><td>-</td><td>4</td><td>4</td></tr><tr><td>Context Linear Dim</td><td>-</td><td>128</td><td>128</td></tr><tr><td>Num Keypoints</td><td>-</td><td>-</td><td>2048</td></tr><tr><td>Deform radius</td><td>-</td><td>-</td><td> [0.4, 0.8], [0.8, 1.2],[1.2, 2.4], [2.4, 4.8] </td></tr><tr><td>Feature pool radius</td><td>-</td><td>-</td><td>Multi-scale: (0.8, 1.6)m</td></tr><tr><td>Interpolation MLP Dim</td><td>-</td><td>-</td><td>Multi-scale: (64, 64)</td></tr><tr><td>Interpolation radius</td><td>-</td><td>-</td><td>Multi-scale: (0.8, 1.6)m</td></tr><tr><td>Interpolation samples</td><td>-</td><td>-</td><td>Multi-scale: (16, 16)</td></tr></table>", "caption": "Table 11: Architectural details of PV-RCNN [31], and proposed FSA-PVRCNN and DSA-PVRCNN", "list_citation_info": ["[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-voxel feature set abstraction for 3d object detection. In CVPR, 2020."]}, {"table": "<table><tr><td>Model</td><td>Modality</td><td>Params</td><td>GFLOPs</td><td colspan=\"3\">Car 3D AP</td></tr><tr><td></td><td></td><td>(M)</td><td></td><td>Easy</td><td>Moderate</td><td>Hard</td></tr><tr><td>PP [15]</td><td>BEV</td><td>4.8</td><td>63.4</td><td>87.75</td><td>78.39</td><td>75.18</td></tr><tr><td>PP{}_{red}</td><td>BEV</td><td>1.5</td><td>31.5</td><td>88.09</td><td>78.07</td><td>75.14</td></tr><tr><td>PP-DSA</td><td>BEV</td><td>1.1</td><td>32.4</td><td>89.37</td><td>78.94</td><td>75.99</td></tr><tr><td>PP-FSA</td><td>BEV</td><td>1.0</td><td>31.7</td><td>90.10</td><td>79.04</td><td>76.02</td></tr><tr><td>SECOND [48]</td><td>Voxel</td><td>4.6</td><td>76.7</td><td>90.55</td><td>81.61</td><td>78.61</td></tr><tr><td>SECOND{}_{red}</td><td>Voxel</td><td>2.5</td><td>51.2</td><td>89.93</td><td>81.11</td><td>78.30</td></tr><tr><td>SECOND-DSA</td><td>Voxel</td><td>2.2</td><td>52.6</td><td>90.70</td><td>82.03</td><td>79.07</td></tr><tr><td>SECOND-FSA</td><td>Voxel</td><td>2.2</td><td>51.9</td><td>89.05</td><td>81.86</td><td>78.84</td></tr><tr><td>Point-RCNN [32]</td><td>Points</td><td>4.0</td><td>27.4</td><td>91.94</td><td>80.52</td><td>78.31</td></tr><tr><td>Point-RCNN{}_{red}</td><td>Points</td><td>2.2</td><td>24.1</td><td>91.47</td><td>80.40</td><td>78.07</td></tr><tr><td>Point-RCNN-DSA</td><td>Points</td><td>2.3</td><td>19.3</td><td>91.55</td><td>81.80</td><td>79.74</td></tr><tr><td>Point-RCNN-FSA</td><td>Points</td><td>2.5</td><td>19.8</td><td>91.63</td><td>82.10</td><td>80.05</td></tr></table>", "caption": "Table 12: Detailed comparison of 3D AP with baseline on KITTI val split with 40 recall positions", "list_citation_info": ["[32] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. PointRCNN: 3d object proposal generation and detection from point cloud. In CVPR, 2019.", "[48] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.", "[15] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. PointPillars: Fast encoders for object detection from point clouds. In CVPR, 2019."]}, {"table": "<table><tr><td></td><td>3D</td><td>BEV</td></tr><tr><td>PV-RCNN [31]</td><td>70.38</td><td>74.5</td></tr><tr><td>PV-RCNN + DSA</td><td>73.03</td><td>75.45</td></tr><tr><td>PV-RCNN + FSA</td><td>71.46</td><td>74.73</td></tr></table>", "caption": "Table 13: Performance comparison for moderate difficulty cyclist class on KITTI val split.", "list_citation_info": ["[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-voxel feature set abstraction for 3d object detection. In CVPR, 2020."]}, {"table": "<table><tr><td>Distance</td><td>Model</td><td>Car</td><td>Cyclist</td><td>Pedestrian</td></tr><tr><td rowspan=\"3\">0-30m</td><td>PV-RCNN [31]</td><td>91.71</td><td>73.76</td><td>56.82</td></tr><tr><td>DSA</td><td>91.65</td><td>74.89</td><td>59.61</td></tr><tr><td>FSA</td><td>93.44</td><td>74.10</td><td>61.65</td></tr><tr><td rowspan=\"3\">30-50m</td><td>PV-RCNN [31]</td><td>50.00</td><td>35.15</td><td>-</td></tr><tr><td>DSA</td><td>52.02</td><td>47.00</td><td>-</td></tr><tr><td>FSA</td><td>52.76</td><td>39.74</td><td>-</td></tr></table>", "caption": "Table 14: Comparison of nearby and distant-object detection on the moderate level of KITTI val split with AP calculated by 40 recall positions", "list_citation_info": ["[31] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-voxel feature set abstraction for 3d object detection. In CVPR, 2020."]}]}