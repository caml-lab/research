{"title": "Vggface2: A dataset for recognising faces across pose and age", "abstract": "In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians). The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimize the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity. To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VGGFace2, on MS- Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A, IJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin. Datasets and models are publicly available.", "authors": ["Qiong Cao", " Li Shen", " Weidi Xie", " Omkar M. Parkhi", " Andrew Zisserman"], "pdf_url": "https://arxiv.org/abs/1710.08092", "list_table_and_caption": [{"table": "<table><thead><tr><th>Datasets</th><th># of subjects</th><th># of images</th><th># of images per subject</th><th>manual identity labelling</th><th>pose</th><th>age</th><th>year</th></tr></thead><tbody><tr><td>LFW [10]</td><td>5,749</td><td>13,233</td><td>1/2.3/530</td><td>-</td><td>-</td><td>-</td><td>2007</td></tr><tr><td>YTF [24]</td><td>1,595</td><td>3,425 videos</td><td>-</td><td>-</td><td>-</td><td>-</td><td>2011</td></tr><tr><td>CelebFaces+ [21]</td><td>10,177</td><td>202,599</td><td>19.9</td><td>-</td><td>-</td><td>-</td><td>2014</td></tr><tr><td>CASIA-WebFace [26]</td><td>10,575</td><td>494,414</td><td>2/46.8/804</td><td>-</td><td>-</td><td>-</td><td>2014</td></tr><tr><td>IJB-A [13]</td><td>500</td><td>5,712 images, 2,085 videos</td><td>11.4</td><td>-</td><td>-</td><td>-</td><td>2015</td></tr><tr><td>IJB-B [23]</td><td>1,845</td><td>11,754 images, 7,011 videos</td><td>36.2</td><td>-</td><td>-</td><td>-</td><td>2017</td></tr><tr><td>IJB-C [14]</td><td>3,531</td><td>31,334 images, 11,779 videos</td><td>36.3</td><td>-</td><td>-</td><td>-</td><td>2018</td></tr><tr><td>VGGFace [17]</td><td>2,622</td><td>2.6 M</td><td>1,000/1,000/1,000</td><td>-</td><td>-</td><td>Yes</td><td>2015</td></tr><tr><td>MegaFace [12]</td><td>690,572</td><td>4.7 M</td><td>3/7/2469</td><td>-</td><td>-</td><td>-</td><td>2016</td></tr><tr><td>MS-Celeb-1M [7]</td><td>100,000</td><td>10 M</td><td>100</td><td>-</td><td>-</td><td>-</td><td>2016</td></tr><tr><td>UMDFaces [5]</td><td>8,501</td><td>367,920</td><td>43.3</td><td>Yes</td><td>Yes</td><td>Yes</td><td>2016</td></tr><tr><td>UMDFaces-Videos [4]</td><td>3,107</td><td>22,075 videos</td><td>-</td><td>-</td><td>-</td><td>-</td><td>2017</td></tr><tr><td>VGGFace2 (this paper)</td><td>9,131</td><td>3.31 M</td><td>80/362.6/843</td><td>Yes</td><td>Yes</td><td>Yes</td><td>2018</td></tr></tbody></table>", "caption": "TABLE I: Statistics for recent public face datasets. The three entries in the \u2018per subject\u2019 column are the minimum/average/maximum per subject.", "list_citation_info": ["[21] Y. Sun, X. Wang, and X. Tang. Deep learning face representation from predicting 10,000 classes. In CVPR, pages 1891\u20131898, 2014.", "[26] D. Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014.", "[23] C. Whitelam, E. Taborsky, A. Blanton, B. Maze, J. Adams, T. Miller, N. Kalka, A. K. Jain, J. A. Duncan, K. Allen, et al. Iarpa janus benchmark-b face dataset. In CVPR Workshop on Biometrics, 2017.", "[13] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney, K. Allen, P. Grother, A. Mah, and A. K. Jain. Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a. In CVPR, pages 1931\u20131939, 2015.", "[12] I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard. The megaface benchmark: 1 million faces for recognition at scale. In CVPR, pages 4873\u20134882, 2016.", "[14] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto, A. K. Jain, W. T. Niggel, J. Anderson, J. Cheney, and P. Grother. IARPA janus benchmark-c: Face dataset and protocol. In 11th IAPR International Conference on Biometrics, 2018.", "[17] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC., 2015.", "[7] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. arXiv preprint arXiv:1607.08221, 2016.", "[5] A. Bansal, A. Nanduri, C. Castillo, R. Ranjan, and R. Chellappa. Umdfaces: An annotated face dataset for training deep networks. arXiv preprint arXiv:1611.01484, 2016.", "[4] A. Bansal, C. Castillo, R. Ranjan, and R. Chellappa. The do\u2019s and don\u2019ts for cnn-based face verification. arXiv preprint arXiv:1705.07426, 2017.", "[24] L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained videos with matched background similarity. In CVPR, pages 529\u2013534. IEEE, 2011.", "[10] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical report, 2007."]}, {"table": "<table><tbody><tr><th>Training dataset</th><th>Arch.</th><td colspan=\"3\">1:1 Verification TAR</td><td colspan=\"5\">1:N Identification TPIR</td></tr><tr><th></th><th></th><td>FAR=0.001</td><td>FAR=0.01</td><td>FAR=0.1</td><td>FPIR=0.01</td><td>FPIR=0.1</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td></tr><tr><th>VGGFace [17]</th><th>ResNet-50</th><td>0.620\\pm 0.043</td><td>0.834\\pm 0.021</td><td>0.954\\pm 0.005</td><td>0.454\\pm 0.058</td><td>0.748\\pm 0.024</td><td>0.925\\pm 0.008</td><td>0.972\\pm 0.005</td><td>0.983\\pm 0.003</td></tr><tr><th>MS1M [7]</th><th>ResNet-50</th><td>0.851\\pm 0.030</td><td>0.939\\pm 0.013</td><td>0.980\\pm 0.003</td><td>0.807\\pm 0.041</td><td>0.920\\pm 0.012</td><td>0.961\\pm 0.006</td><td>0.982\\pm 0.004</td><td>0.990\\pm 0.002</td></tr><tr><th>VGGFace2</th><th>ResNet-50</th><td>0.895\\pm 0.019</td><td>0.950\\pm 0.005</td><td>0.980\\pm 0.003</td><td>0.844\\pm 0.035</td><td>0.924\\pm 0.006</td><td>0.976\\pm 0.004</td><td>0.992\\pm 0.002</td><td>0.995\\pm 0.001</td></tr><tr><th>VGGFace2_ft</th><th>ResNet-50</th><td>0.908\\pm 0.017</td><td>0.957\\pm 0.007</td><td>0.986\\pm 0.002</td><td>0.861\\pm 0.027</td><td>0.936\\pm 0.007</td><td>0.978\\pm 0.005</td><td>0.992\\pm 0.003</td><td>0.995\\pm 0.001</td></tr><tr><th>VGGFace2</th><th>SENet</th><td>0.904\\pm 0.020</td><td>0.958\\pm 0.004</td><td>0.985\\pm 0.002</td><td>0.847\\pm 0.051</td><td>0.930\\pm 0.007</td><td>0.981\\pm 0.003</td><td>\\mathbf{0.994\\pm 0.002}</td><td>\\mathbf{0.996\\pm 0.001}</td></tr><tr><th>VGGFace2_ft</th><th>SENet</th><td>\\mathbf{0.921\\pm 0.014}</td><td>\\mathbf{0.968\\pm 0.006}</td><td>\\mathbf{0.990\\pm 0.002}</td><td>\\mathbf{0.883\\pm 0.038}</td><td>\\mathbf{0.946\\pm 0.004}</td><td>\\mathbf{0.982\\pm 0.004}</td><td>0.993\\pm 0.002</td><td>0.994\\pm 0.001</td></tr><tr><th>Crosswhite et al. [6]</th><th>-</th><td>0.836\\pm 0.027</td><td>0.939\\pm 0.013</td><td>0.979\\pm 0.004</td><td>0.774\\pm 0.049</td><td>0.882\\pm 0.016</td><td>0.928\\pm 0.010</td><td>0.977\\pm 0.004</td><td>0.986\\pm 0.003</td></tr><tr><th>Sohn et al. [20]</th><th>-</th><td>0.649\\pm 0.022</td><td>0.864\\pm 0.007</td><td>0.970\\pm 0.001</td><td>-</td><td>-</td><td>0.895\\pm 0.003</td><td>0.957\\pm 0.002</td><td>0.968\\pm 0.002</td></tr><tr><th>Bansalit et al. [4]</th><th>-</th><td>0.730^{\\dagger}</td><td>0.874</td><td>0.960^{\\dagger}</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Yang et al. [25]</th><th>-</th><td>0.881\\pm 0.011</td><td>0.941\\pm 0.008</td><td>0.978\\pm 0.003</td><td>0.817\\pm 0.041</td><td>0.917\\pm 0.009</td><td>0.958\\pm 0.005</td><td>0.980\\pm 0.005</td><td>0.986\\pm 0.003</td></tr></tbody></table>", "caption": "TABLE VI: Performance evaluation on the IJB-A dataset. A higher value is better. The values with {\\dagger} are read from [4].", "list_citation_info": ["[20] K. Sohn, S. Liu, G. Zhong, X. Yu, M.-H. Yang, and M. Chandraker. Unsupervised domain adaptation for face recognition in unlabeled videos. arXiv preprint arXiv:1708.02191, 2017.", "[17] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC., 2015.", "[7] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. arXiv preprint arXiv:1607.08221, 2016.", "[6] N. Crosswhite, J. Byrne, C. Stauffer, O. Parkhi, Q. Cao, and A. Zisserman. Template adaptation for face verification and identification. In Automatic Face & Gesture Recognition (FG), pages 1\u20138. IEEE, 2017.", "[4] A. Bansal, C. Castillo, R. Ranjan, and R. Chellappa. The do\u2019s and don\u2019ts for cnn-based face verification. arXiv preprint arXiv:1705.07426, 2017.", "[25] J. Yang, P. Ren, D. Zhang, D. Chen, F. Wen, H. Li, and G. Hua. Neural aggregation network for video face recognition. In CVPR, pages 4362\u20134371, 2017."]}, {"table": "<table><tbody><tr><th>Training dataset</th><th>Arch.</th><td colspan=\"4\">1:1 Verification TAR</td><td colspan=\"5\">1:N Identification TPIR</td></tr><tr><th></th><th></th><td>FAR=1E-5</td><td>FAR=1E-4</td><td>FAR=1E-3</td><td>FAR=1E-2</td><td>FPIR=0.01</td><td>FPIR=0.1</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td></tr><tr><th>VGGFace [17]</th><th>ResNet-50</th><td>0.342</td><td>0.535</td><td>0.711</td><td>0.850</td><td>0.429\\pm 0.024</td><td>0.635\\pm 0.015</td><td>0.752\\pm 0.038</td><td>0.843\\pm 0.032</td><td>0.874\\pm 0.026</td></tr><tr><th>MS1M [7]</th><th>ResNet-50</th><td>0.548</td><td>0.743</td><td>0.857</td><td>0.935</td><td>0.662\\pm 0.036</td><td>0.810\\pm 0.028</td><td>0.865\\pm 0.053</td><td>0.917\\pm 0.032</td><td>0.936\\pm 0.024</td></tr><tr><th>VGGFace2</th><th>ResNet-50</th><td>0.647</td><td>0.784</td><td>0.878</td><td>0.938</td><td>0.701\\pm 0.038</td><td>0.824\\pm 0.034</td><td>0.886\\pm 0.032</td><td>0.936\\pm 0.019</td><td>0.953\\pm 0.013</td></tr><tr><th>VGGFace2_ft</th><th>ResNet-50</th><td>0.671</td><td>0.804</td><td>0.891</td><td>0.947</td><td>0.702\\pm 0.041</td><td>0.843\\pm 0.032</td><td>0.894\\pm 0.039</td><td>0.940\\pm 0.022</td><td>0.954\\pm 0.016</td></tr><tr><th>VGGFace2</th><th>SENet</th><td>0.671</td><td>0.800</td><td>0.888</td><td>0.949</td><td>0.706\\pm 0.047</td><td>0.839\\pm 0.035</td><td>0.901\\pm 0.030</td><td>0.945\\pm 0.016</td><td>0.958\\pm 0.010</td></tr><tr><th>VGGFace2_ft</th><th>SENet</th><td>\\mathbf{0.705}</td><td>\\mathbf{0.831}</td><td>\\mathbf{0.908}</td><td>\\mathbf{0.956}</td><td>\\mathbf{0.743\\pm 0.037}</td><td>\\mathbf{0.863\\pm 0.032}</td><td>\\mathbf{0.902\\pm 0.036}</td><td>\\mathbf{0.946\\pm 0.022}</td><td>\\mathbf{0.959\\pm 0.015}</td></tr><tr><th>Whitelam et al. [23]</th><th>-</th><td>0.350</td><td>0.540</td><td>0.700</td><td>0.840</td><td>0.420</td><td>0.640</td><td>0.790</td><td>0.850</td><td>0.900</td></tr></tbody></table>", "caption": "TABLE VII: Performance evaluation on the IJB-B dataset. A higher value isbetter. The results of [23] are read from thecurves reported in the paper. Note, [23] has a differentevaluation for the verification protocol where pairsgenerated from different galleries are evaluated separately andaveraged to get the final results.", "list_citation_info": ["[23] C. Whitelam, E. Taborsky, A. Blanton, B. Maze, J. Adams, T. Miller, N. Kalka, A. K. Jain, J. A. Duncan, K. Allen, et al. Iarpa janus benchmark-b face dataset. In CVPR Workshop on Biometrics, 2017.", "[17] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC., 2015.", "[7] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. arXiv preprint arXiv:1607.08221, 2016."]}, {"table": "<table><tbody><tr><th>Training dataset</th><th>Arch.</th><td colspan=\"4\">1:1 Verification TAR</td><td colspan=\"5\">1:N Identification TPIR</td></tr><tr><th></th><th></th><td>FAR=1E-5</td><td>FAR=1E-4</td><td>FAR=1E-3</td><td>FAR=1E-2</td><td>FPIR=0.01</td><td>FPIR=0.1</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td></tr><tr><th>VGGFace2</th><th>ResNet-50</th><td>0.734</td><td>0.825</td><td>0.900</td><td>0.950</td><td>0.735\\pm 0.022</td><td>0.830\\pm 0.021</td><td>0.898\\pm 0.017</td><td>0.939\\pm 0.013</td><td>0.953\\pm 0.009</td></tr><tr><th>VGGFace2_ft</th><th>ResNet-50</th><td>0.749</td><td>0.846</td><td>0.913</td><td>0.958</td><td>0.749\\pm 0.021</td><td>0.849\\pm 0.018</td><td>0.908\\pm 0.022</td><td>0.946\\pm 0.014</td><td>0.958\\pm 0.011</td></tr><tr><th>VGGFace2</th><th>SENet</th><td>0.747</td><td>0.840</td><td>0.910</td><td>0.960</td><td>0.746\\pm 0.018</td><td>0.842\\pm 0.022</td><td>0.912\\pm 0.017</td><td>0.949\\pm 0.010</td><td>0.962\\pm 0.007</td></tr><tr><th>VGGFace2_ft</th><th>SENet</th><td>\\mathbf{0.768}</td><td>\\mathbf{0.862}</td><td>\\mathbf{0.927}</td><td>\\mathbf{0.967}</td><td>\\mathbf{0.763\\pm 0.018}</td><td>\\mathbf{0.865\\pm 0.018}</td><td>\\mathbf{0.914\\pm 0.020}</td><td>\\mathbf{0.951\\pm 0.013}</td><td>\\mathbf{0.961\\pm 0.010}</td></tr><tr><th>Maze et al. [14]</th><th></th><td>0.600</td><td>0.750</td><td>0.860</td><td>0.950</td><td>0.450</td><td>0.620</td><td>0.790</td><td>0.870</td><td>0.900</td></tr></tbody></table>", "caption": "TABLE VIII: Performance evaluation on the IJB-C dataset. A higher value isbetter. The results of [14] are read from thecurves reported in the paper. Note, [14] has a differentevaluation for the verification protocol where pairsgenerated from different galleries are evaluated separately andaveraged to get the final results.", "list_citation_info": ["[14] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto, A. K. Jain, W. T. Niggel, J. Anderson, J. Cheney, and P. Grother. IARPA janus benchmark-c: Face dataset and protocol. In 11th IAPR International Conference on Biometrics, 2018."]}]}