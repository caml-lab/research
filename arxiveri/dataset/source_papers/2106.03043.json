{"title": "Occlusion-aware unsupervised learning of depth from 4-d light fields", "abstract": "Depth estimation is a fundamental issue in 4-D light field processing and analysis. Although recent supervised learning-based light field depth estimation methods have significantly improved the accuracy and efficiency of traditional optimization-based ones, these methods rely on the training over light field data with ground-truth depth maps which are challenging to obtain or even unavailable for real-world light field data. Besides, due to the inevitable gap (or domain difference) between real-world and synthetic data, they may suffer from serious performance degradation when generalizing the models trained with synthetic data to real-world data. By contrast, we propose an unsupervised learning-based method, which does not require ground-truth depth as supervision during training. Specifically, based on the basic knowledge of the unique geometry structure of light field data, we present an occlusion-aware strategy to improve the accuracy on occlusion areas, in which we explore the angular coherence among subsets of the light field views to estimate initial depth maps, and utilize a constrained unsupervised loss to learn their corresponding reliability for final depth prediction. Additionally, we adopt a multi-scale network with a weighted smoothness loss to handle the textureless areas. Experimental results on synthetic data show that our method can significantly shrink the performance gap between the previous unsupervised method and supervised ones, and produce depth maps with comparable accuracy to traditional methods with obviously reduced computational cost. Moreover, experiments on real-world datasets show that our method can avoid the domain shift problem presented in supervised methods, demonstrating the great potential of our method.", "authors": ["Jing Jin", " Junhui Hou"], "pdf_url": "https://arxiv.org/abs/2106.03043", "list_table_and_caption": [{"table": "<table><tbody><tr><td colspan=\"2\" rowspan=\"2\">Methods</td><td colspan=\"4\">HCI</td><td colspan=\"6\">HCIold</td></tr><tr><td>Boxes</td><td>Cotton</td><td>Dino</td><td>Sideboard</td><td>Buddha</td><td>Horses</td><td>Medieval</td><td>Monasroom</td><td>Papillon</td><td>StillLife</td></tr><tr><td rowspan=\"3\">Non-Learning</td><td>ACC [27]</td><td>24.91</td><td>8.70</td><td>1.25</td><td>12.64</td><td>1.21</td><td>1.74</td><td>1.05</td><td>11.02</td><td>4.88</td><td>13.07</td></tr><tr><td>OCC [11]</td><td>8.14</td><td>1.04</td><td>0.59</td><td>2.31</td><td>0.76</td><td>0.70</td><td>0.85</td><td>0.55</td><td>0.75</td><td>2.49</td></tr><tr><td>CAE [16]</td><td>10.18</td><td>1.01</td><td>0.62</td><td>1.55</td><td>0.77</td><td>1.20</td><td>1.14</td><td>0.68</td><td>0.85</td><td>1.45</td></tr><tr><td rowspan=\"2\">Supervised</td><td>EPINet [19]</td><td>6.35</td><td>0.26</td><td>0.18</td><td>0.88</td><td>0.37</td><td>6.85</td><td>2.23</td><td>1.36</td><td>6.15</td><td>2.55</td></tr><tr><td>LFattNet [21]</td><td>4.17</td><td>0.19</td><td>0.10</td><td>0.54</td><td>0.36</td><td>5.79</td><td>1.45</td><td>0.75</td><td>5.15</td><td>15.36</td></tr><tr><td rowspan=\"2\">Unsupervised</td><td>Unsup [24]</td><td>12.74</td><td>7.31</td><td>1.88</td><td>4.62</td><td>1.11</td><td>1.65</td><td>1.27</td><td>1.92</td><td>4.58</td><td>33.90</td></tr><tr><td>Ours</td><td>7.45</td><td>0.80</td><td>0.63</td><td>1.79</td><td>0.34</td><td>1.52</td><td>0.70</td><td>0.57</td><td>1.11</td><td>1.57</td></tr></tbody></table>", "caption": "TABLE I:  Quantitative comparisons (MSE \\times 100) of the depth estimation results from different methods on synthetic LF data. The smaller, the better.", "list_citation_info": ["[27] H.-G. Jeon, J. Park, G. Choe, J. Park, Y. Bok, Y.-W. Tai, and I. So Kweon, \u201cAccurate depth map estimation from a lenslet light field camera,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1547\u20131555.", "[11] T.-C. Wang, A. A. Efros, and R. Ramamoorthi, \u201cOcclusion-aware depth estimation using light-field cameras,\u201d in IEEE International Conference on Computer Vision (ICCV), 2015, pp. 3487\u20133495.", "[21] Y.-J. Tsai, Y.-L. Liu, M. Ouhyoung, and Y.-Y. Chuang, \u201cAttention-based view selection networks for light-field disparity estimation,\u201d in Proceedings of the 34th Conference on Artificial Intelligence (AAAI), 2020, pp. 1\u20131.", "[19] C. Shin, H.-G. Jeon, Y. Yoon, I. S. Kweon, and S. J. Kim, \u201cEpinet: A fully-convolutional neural network using epipolar geometry for depth from light field images,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4748\u20134757.", "[24] J. Peng, Z. Xiong, D. Liu, and X. Chen, \u201cUnsupervised depth estimation from light field using a convolutional neural network,\u201d in International Conference on 3D Vision (3DV), 2018, pp. 295\u2013303.", "[16] Williem, I. K. Park, and K. M. Lee, \u201cRobust light field depth estimation using occlusion-noise aware data costs,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484\u20132497, 2018."]}, {"table": "<table><tbody><tr><td colspan=\"2\" rowspan=\"2\">Methods</td><td colspan=\"4\">HCI</td><td colspan=\"6\">HCIold</td></tr><tr><td>Boxes</td><td>Cotton</td><td>Dino</td><td>Sideboard</td><td>Buddha</td><td>Horses</td><td>Medieval</td><td>Monasroom</td><td>Papillon</td><td>StillLife</td></tr><tr><td rowspan=\"3\">Non-Learning</td><td>ACC [27]</td><td>25.39</td><td>7.36</td><td>17.57</td><td>24.40</td><td>9.33</td><td>14.44</td><td>7.92</td><td>12.07</td><td>18.14</td><td>22.32</td></tr><tr><td>OCC [11]</td><td>65.68</td><td>4.57</td><td>9.83</td><td>22.92</td><td>6.74</td><td>14.36</td><td>12.74</td><td>10.54</td><td>22.73</td><td>13.06</td></tr><tr><td>CAE [16]</td><td>29.48</td><td>7.84</td><td>18.17</td><td>21.47</td><td>5.69</td><td>14.82</td><td>23.80</td><td>9.56</td><td>18.59</td><td>20.41</td></tr><tr><td rowspan=\"2\">Supervised</td><td>EPINet [19]</td><td>13.10</td><td>0.47</td><td>1.41</td><td>5.19</td><td>1.62</td><td>16.59</td><td>18.83</td><td>10.56</td><td>36.16</td><td>11.87</td></tr><tr><td>LFattNet [21]</td><td>11.51</td><td>0.26</td><td>0.88</td><td>2.97</td><td>2.22</td><td>16.75</td><td>18.64</td><td>9.44</td><td>36.03</td><td>13.02</td></tr><tr><td rowspan=\"2\">Unsupervised</td><td>Unsup [24]</td><td>43.82</td><td>28.02</td><td>22.08</td><td>28.06</td><td>9.39</td><td>19.64</td><td>18.41</td><td>14.63</td><td>28.30</td><td>44.97</td></tr><tr><td>Ours</td><td>26.24</td><td>8.46</td><td>8.25</td><td>14.20</td><td>4.11</td><td>26.95</td><td>16.48</td><td>10.57</td><td>36.36</td><td>17.14</td></tr></tbody></table>", "caption": "TABLE II:  Quantitative comparisons (BPR (&gt;0.07)) of the depth estimation results from different methods on synthetic LF data. The smaller, the better.", "list_citation_info": ["[27] H.-G. Jeon, J. Park, G. Choe, J. Park, Y. Bok, Y.-W. Tai, and I. So Kweon, \u201cAccurate depth map estimation from a lenslet light field camera,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1547\u20131555.", "[11] T.-C. Wang, A. A. Efros, and R. Ramamoorthi, \u201cOcclusion-aware depth estimation using light-field cameras,\u201d in IEEE International Conference on Computer Vision (ICCV), 2015, pp. 3487\u20133495.", "[21] Y.-J. Tsai, Y.-L. Liu, M. Ouhyoung, and Y.-Y. Chuang, \u201cAttention-based view selection networks for light-field disparity estimation,\u201d in Proceedings of the 34th Conference on Artificial Intelligence (AAAI), 2020, pp. 1\u20131.", "[19] C. Shin, H.-G. Jeon, Y. Yoon, I. S. Kweon, and S. J. Kim, \u201cEpinet: A fully-convolutional neural network using epipolar geometry for depth from light field images,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4748\u20134757.", "[24] J. Peng, Z. Xiong, D. Liu, and X. Chen, \u201cUnsupervised depth estimation from light field using a convolutional neural network,\u201d in International Conference on 3D Vision (3DV), 2018, pp. 295\u2013303.", "[16] Williem, I. K. Park, and K. M. Lee, \u201cRobust light field depth estimation using occlusion-noise aware data costs,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484\u20132497, 2018."]}, {"table": "<table><thead><tr><th colspan=\"2\" rowspan=\"2\">Methods</th><th colspan=\"4\">Test</th><th colspan=\"4\">Stratified</th></tr><tr><th>Bedroom</th><th>Bicycle</th><th>Herbs</th><th>Origami</th><th>Backgammon</th><th>Dots</th><th>Pyramids</th><th>Stripes</th></tr></thead><tbody><tr><th rowspan=\"3\">Non-Learning</th><th>ACC [27]</th><td>0.467</td><td>11.729</td><td>21.335</td><td>6.757</td><td>13.007</td><td>5.676</td><td>0.273</td><td>17.454</td></tr><tr><th>OCC [11]</th><td>0.633</td><td>7.669</td><td>22.202</td><td>2.300</td><td>21.587</td><td>3.301</td><td>0.098</td><td>8.131</td></tr><tr><th>CAE [16]</th><td>0.234</td><td>5.135</td><td>11.665</td><td>1.778</td><td>6.074</td><td>5.082</td><td>0.048</td><td>3.556</td></tr><tr><th rowspan=\"3\">Supervised</th><th>EPINet [19]</th><td>0.213</td><td>4.682</td><td>9.700</td><td>1.466</td><td>3.629</td><td>1.635</td><td>0.008</td><td>0.950</td></tr><tr><th>LFattNet [21]</th><td>0.366</td><td>3.350</td><td>6.605</td><td>1.733</td><td>3.648</td><td>1.425</td><td>0.004</td><td>0.892</td></tr><tr><th>AttMLFNet [23]</th><td>0.129</td><td>3.082</td><td>6.374</td><td>0.991</td><td>3.863</td><td>1.035</td><td>0.003</td><td>0.814</td></tr><tr><th rowspan=\"3\">Unsupervised</th><th>Unsup [24]</th><td>0.924</td><td>11.737</td><td>145.551</td><td>8.817</td><td>34.709</td><td>72.998</td><td>0.035</td><td>11.759</td></tr><tr><th>Mono [26]</th><td>0.415</td><td>9.232</td><td>26.816</td><td>3.679</td><td>11.833</td><td>2.536</td><td>0.027</td><td>2.677</td></tr><tr><th>Ours</th><td>0.385</td><td>6.232</td><td>13.941</td><td>1.921</td><td>6.684</td><td>6.565</td><td>0.213</td><td>5.200</td></tr></tbody></table>", "caption": "TABLE III:  Quantitative comparisons (MSE \\times 100) of the depth estimation results from different methods on the HCI 4D benchmark. ", "list_citation_info": ["[27] H.-G. Jeon, J. Park, G. Choe, J. Park, Y. Bok, Y.-W. Tai, and I. So Kweon, \u201cAccurate depth map estimation from a lenslet light field camera,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1547\u20131555.", "[23] J. Chen, S. Zhang, and Y. Lin, \u201cAttention-based multi-level fusion network for light field depth estimation,\u201d in Proceedings of the Conference on Artificial Intelligence (AAAI), vol. 35, no. 2, 2021, pp. 1009\u20131017.", "[11] T.-C. Wang, A. A. Efros, and R. Ramamoorthi, \u201cOcclusion-aware depth estimation using light-field cameras,\u201d in IEEE International Conference on Computer Vision (ICCV), 2015, pp. 3487\u20133495.", "[21] Y.-J. Tsai, Y.-L. Liu, M. Ouhyoung, and Y.-Y. Chuang, \u201cAttention-based view selection networks for light-field disparity estimation,\u201d in Proceedings of the 34th Conference on Artificial Intelligence (AAAI), 2020, pp. 1\u20131.", "[19] C. Shin, H.-G. Jeon, Y. Yoon, I. S. Kweon, and S. J. Kim, \u201cEpinet: A fully-convolutional neural network using epipolar geometry for depth from light field images,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4748\u20134757.", "[24] J. Peng, Z. Xiong, D. Liu, and X. Chen, \u201cUnsupervised depth estimation from light field using a convolutional neural network,\u201d in International Conference on 3D Vision (3DV), 2018, pp. 295\u2013303.", "[16] Williem, I. K. Park, and K. M. Lee, \u201cRobust light field depth estimation using occlusion-noise aware data costs,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484\u20132497, 2018.", "[26] W. Zhou, E. Zhou, G. Liu, L. Lin, and A. Lumsdaine, \u201cUnsupervised monocular depth estimation from light field image,\u201d IEEE Transactions on Image Processing, vol. 29, pp. 1606\u20131617, 2020."]}, {"table": "<table><thead><tr><th colspan=\"2\" rowspan=\"2\">Methods</th><th colspan=\"4\">Test</th><th colspan=\"4\">Stratified</th></tr><tr><th>Bedroom</th><th>Bicycle</th><th>Herbs</th><th>Origami</th><th>Backgammon</th><th>Dots</th><th>Pyramids</th><th>Stripes</th></tr></thead><tbody><tr><th rowspan=\"3\">Non-Learning</th><th>ACC [27]</th><td>13.855</td><td>19.791</td><td>18.108</td><td>14.183</td><td>5.516</td><td>2.900</td><td>12.354</td><td>35.741</td></tr><tr><th>OCC [11]</th><td>17.565</td><td>21.562</td><td>36.830</td><td>22.431</td><td>19.006</td><td>5.822</td><td>3.172</td><td>18.408</td></tr><tr><th>CAE [16]</th><td>5.788</td><td>11.223</td><td>9.550</td><td>10.027</td><td>3.924</td><td>12.401</td><td>1.681</td><td>7.872</td></tr><tr><th rowspan=\"3\">Supervised</th><th>EPINet [19]</th><td>2.403</td><td>9.896</td><td>12.100</td><td>5.918</td><td>3.580</td><td>3.183</td><td>0.192</td><td>2.462</td></tr><tr><th>LFattNet [21]</th><td>2.792</td><td>9.511</td><td>5.219</td><td>4.824</td><td>3.126</td><td>1.432</td><td>0.195</td><td>2.933</td></tr><tr><th>AttMLFNet [23]</th><td>2.074</td><td>8.837</td><td>5.426</td><td>4.406</td><td>3.228</td><td>1.606</td><td>0.174</td><td>2.932</td></tr><tr><th rowspan=\"2\">Unsupervised</th><th>Unsup [24]</th><td>21.604</td><td>30.239</td><td>63.940</td><td>53.408</td><td>36.419</td><td>56.102</td><td>0.809</td><td>62.864</td></tr><tr><th>Mono [26]</th><td>7.413</td><td>20.098</td><td>13.443</td><td>10.949</td><td>12.311</td><td>3.651</td><td>0.262</td><td>11.136</td></tr><tr><th></th><th>Ours</th><td>12.687</td><td>21.650</td><td>16.959</td><td>19.821</td><td>14.371</td><td>45.340</td><td>7.348</td><td>41.359</td></tr></tbody></table>", "caption": "TABLE IV:  Quantitative comparisons (BPR (&gt;0.07)) of the depth estimation results from different methods on the HCI 4D benchmark. ", "list_citation_info": ["[27] H.-G. Jeon, J. Park, G. Choe, J. Park, Y. Bok, Y.-W. Tai, and I. So Kweon, \u201cAccurate depth map estimation from a lenslet light field camera,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1547\u20131555.", "[23] J. Chen, S. Zhang, and Y. Lin, \u201cAttention-based multi-level fusion network for light field depth estimation,\u201d in Proceedings of the Conference on Artificial Intelligence (AAAI), vol. 35, no. 2, 2021, pp. 1009\u20131017.", "[11] T.-C. Wang, A. A. Efros, and R. Ramamoorthi, \u201cOcclusion-aware depth estimation using light-field cameras,\u201d in IEEE International Conference on Computer Vision (ICCV), 2015, pp. 3487\u20133495.", "[21] Y.-J. Tsai, Y.-L. Liu, M. Ouhyoung, and Y.-Y. Chuang, \u201cAttention-based view selection networks for light-field disparity estimation,\u201d in Proceedings of the 34th Conference on Artificial Intelligence (AAAI), 2020, pp. 1\u20131.", "[19] C. Shin, H.-G. Jeon, Y. Yoon, I. S. Kweon, and S. J. Kim, \u201cEpinet: A fully-convolutional neural network using epipolar geometry for depth from light field images,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4748\u20134757.", "[24] J. Peng, Z. Xiong, D. Liu, and X. Chen, \u201cUnsupervised depth estimation from light field using a convolutional neural network,\u201d in International Conference on 3D Vision (3DV), 2018, pp. 295\u2013303.", "[16] Williem, I. K. Park, and K. M. Lee, \u201cRobust light field depth estimation using occlusion-noise aware data costs,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484\u20132497, 2018.", "[26] W. Zhou, E. Zhou, G. Liu, L. Lin, and A. Lumsdaine, \u201cUnsupervised monocular depth estimation from light field image,\u201d IEEE Transactions on Image Processing, vol. 29, pp. 1606\u20131617, 2020."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">Non-Learning</th><th colspan=\"2\">Supervised</th><th colspan=\"2\">Unsupervised</th></tr><tr><th></th><th>ACC [27]</th><th>OCC [11]</th><th>CAE [16]</th><th>EPINet [19]</th><th>LFattNet [21]</th><th>Unsup [24]</th><th>Ours</th></tr></thead><tbody><tr><td>w/o GPU</td><td>645.24</td><td>139.26</td><td>229.31</td><td>15.78</td><td>242.35</td><td>11.51</td><td>12.65</td></tr><tr><td>w/ GPU</td><td>-</td><td>-</td><td>-</td><td>1.35</td><td>7.04</td><td>5.57</td><td>0.16</td></tr></tbody></table>", "caption": "TABLE V: Comparison of the running time (in seconds) of different methods for inferring the depth map from a 512\\times 512\\times 7\\times 7 LF image. The inference time with and without GPU acceleration are provided. ", "list_citation_info": ["[27] H.-G. Jeon, J. Park, G. Choe, J. Park, Y. Bok, Y.-W. Tai, and I. So Kweon, \u201cAccurate depth map estimation from a lenslet light field camera,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1547\u20131555.", "[11] T.-C. Wang, A. A. Efros, and R. Ramamoorthi, \u201cOcclusion-aware depth estimation using light-field cameras,\u201d in IEEE International Conference on Computer Vision (ICCV), 2015, pp. 3487\u20133495.", "[21] Y.-J. Tsai, Y.-L. Liu, M. Ouhyoung, and Y.-Y. Chuang, \u201cAttention-based view selection networks for light-field disparity estimation,\u201d in Proceedings of the 34th Conference on Artificial Intelligence (AAAI), 2020, pp. 1\u20131.", "[19] C. Shin, H.-G. Jeon, Y. Yoon, I. S. Kweon, and S. J. Kim, \u201cEpinet: A fully-convolutional neural network using epipolar geometry for depth from light field images,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 4748\u20134757.", "[24] J. Peng, Z. Xiong, D. Liu, and X. Chen, \u201cUnsupervised depth estimation from light field using a convolutional neural network,\u201d in International Conference on 3D Vision (3DV), 2018, pp. 295\u2013303.", "[16] Williem, I. K. Park, and K. M. Lee, \u201cRobust light field depth estimation using occlusion-noise aware data costs,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 10, pp. 2484\u20132497, 2018."]}]}