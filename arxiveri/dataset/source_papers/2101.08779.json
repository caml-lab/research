{"title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++", "abstract": "We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses -- the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict $N$ future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively.", "authors": ["Ruilong Li", " Shan Yang", " David A. Ross", " Angjoo Kanazawa"], "pdf_url": "https://arxiv.org/abs/2101.08779", "list_table_and_caption": [{"table": "<table><tbody><tr><td><p>Dataset</p></td><td>Music</td><td>3D \\text{Joint}_{\\text{pos}}</td><td>3D \\text{Joint}_{\\text{rot}}</td><td>2D Kpt</td><td>Views</td><td>Images</td><td>Genres</td><td>Subjects</td><td>Sequences</td><td>Seconds</td></tr><tr><td><p>AMASS[59]</p></td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>0</td><td>0</td><td>0</td><td>344</td><td>11265</td><td>145251</td></tr><tr><td><p>Human3.6M[39]</p></td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>4</td><td>3.6M</td><td>0</td><td>11</td><td>210</td><td>71561</td></tr><tr><td><p>Dance with Melody[79]</p></td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>0</td><td>0</td><td>4</td><td>-</td><td>61</td><td>5640</td></tr><tr><td><p>GrooveNet [5]</p></td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>0</td><td>0</td><td>1</td><td>1</td><td>2</td><td>1380</td></tr><tr><td><p>DanceNet [96]</p></td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>0</td><td>0</td><td>2</td><td>2</td><td>2</td><td>3472</td></tr><tr><td><p>EA-MUD [78]</p></td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>0</td><td>0</td><td>4</td><td>-</td><td>17</td><td>1254</td></tr><tr><td><p>AIST++</p></td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>9</td><td>10.1M</td><td>10</td><td>30</td><td>1408</td><td>18694</td></tr></tbody></table>", "caption": "Table 1: 3D Dance Datasets Comparisons. The proposed AIST++ dataset is the largest dataset with 3D dance motion paired with music. We also have the largest variety of subjects and genres. Furthermore, our dataset is the only one that comes with image frames, as other dance datasets only contain motion capture dataset. We include popular 3D motion dataset without any music in the first two rows for reference.", "list_citation_info": ["[5] Omid Alemi, Jules Fran\u00e7oise, and Philippe Pasquier. Groovenet: Real-time music-driven dance movement generation using artificial neural networks. networks, 8(17):26, 2017.", "[79] Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody: An lstm-autoencoder approach to music-oriented dance synthesis. In Proceedings of the 26th ACM international conference on Multimedia, pages 1598\u20131606, 2018.", "[96] Wenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, and Yangang Wang. Music2dance: Music-driven dance generation using wavenet. arXiv preprint arXiv:2002.03761, 2020.", "[78] Guofei Sun, Yongkang Wong, Zhiyong Cheng, Mohan S Kankanhalli, Weidong Geng, and Xiangdong Li. Deepdance: Music-to-dance motion choreography with adversarial learning. IEEE Transactions on Multimedia, 2020.", "[39] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7):1325\u20131339, jul 2014.", "[59] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE International Conference on Computer Vision, pages 5442\u20135451, 2019."]}, {"table": "<table><thead><tr><th>Genres</th><th>Musics</th><th>Music Tempo</th><th>Motions</th><th>Choreographs</th><th>Motion Duration (sec.)</th><th>Total Seconds</th></tr></thead><tbody><tr><td>ballet jazz</td><td>6</td><td>80 - 130</td><td>141</td><td rowspan=\"10\">85% basic + 15% advanced</td><td>7.4 - 12.0 basic / 29.5 - 48.0 adv.</td><td>1910.8</td></tr><tr><td>street jazz</td><td>6</td><td>80 - 130</td><td>141</td><td>7.4 - 12.0 basic / 14.9 - 48.0 adv.</td><td>1875.3</td></tr><tr><td>krump</td><td>6</td><td>80 - 130</td><td>141</td><td>7.4 - 12.0 basic / 29.5 - 48.0 adv.</td><td>1904.3</td></tr><tr><td>house</td><td>6</td><td>110 - 135</td><td>141</td><td>7.1 - 8.7 basic / 28.4 - 34.9 adv.</td><td>1607.6</td></tr><tr><td>LA-style hip-hop</td><td>6</td><td>80 - 130</td><td>141</td><td>7.4 - 12.0 basic / 29.5 - 48.0 adv.</td><td>1935.8</td></tr><tr><td>middle hip-hop</td><td>6</td><td>80 - 130</td><td>141</td><td>7.4 - 12.0 basic / 29.5 - 48.0 adv.</td><td>1934.0</td></tr><tr><td>waack</td><td>6</td><td>80 - 130</td><td>140</td><td>7.4 - 12.0 basic / 29.5 - 48.0 adv.</td><td>1897.1</td></tr><tr><td>lock</td><td>6</td><td>80 - 130</td><td>141</td><td>7.4 - 12.0 basic / 29.5 - 48.0 adv.</td><td>1898.5</td></tr><tr><td>pop</td><td>6</td><td>80 - 130</td><td>140</td><td>7.4 - 12.0 basic / 29.5 - 48.0 adv.</td><td>1872.9</td></tr><tr><td>break</td><td>6</td><td>80 - 130</td><td>141</td><td>7.4 - 12.0 basic / 23.8 - 48.0 adv.</td><td>1858.3</td></tr><tr><td>total</td><td>60</td><td></td><td>1408</td><td></td><td></td><td>18694.6</td></tr></tbody></table>", "caption": "Table 5: AIST++ Dataset Statistics. AIST++ is built upon a subset of AIST database [82] that contains single-person dance.", "list_citation_info": ["[82] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing. In Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, pages 501\u2013510, Delft, Netherlands, Nov. 2019."]}]}