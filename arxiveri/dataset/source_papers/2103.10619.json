{"title": "Scalable vision transformers with hierarchical pooling", "abstract": "The recently proposed Visual image Transformers (ViT) with pure attention have achieved promising performance on image recognition tasks, such as image classification. However, the routine of the current ViT model is to maintain a full-length patch sequence during inference, which is redundant and lacks hierarchical representation. To this end, we propose a Hierarchical Visual Transformer (HVT) which progressively pools visual tokens to shrink the sequence length and hence reduces the computational cost, analogous to the feature maps downsampling in Convolutional Neural Networks (CNNs). It brings a great benefit that we can increase the model capacity by scaling dimensions of depth/width/resolution/patch size without introducing extra computational complexity due to the reduced sequence length. Moreover, we empirically find that the average pooled visual tokens contain more discriminative information than the single class token. To demonstrate the improved scalability of our HVT, we conduct extensive experiments on the image classification task. With comparable FLOPs, our HVT outperforms the competitive baselines on ImageNet and CIFAR-100 datasets. Code is available at https://github.com/MonashAI/HVT", "authors": ["Zizheng Pan", " Bohan Zhuang", " Jing Liu", " Haoyu He", " Jianfei Cai"], "pdf_url": "https://arxiv.org/abs/2103.10619", "list_table_and_caption": [{"table": "<table><thead><tr><th>Model</th><th>Embedding Dim</th><th>#Heads</th><th>#Blocks</th><th>FLOPs (G)</th><th>Params (M)</th><th>Top-1 Acc. (%)</th><th>Top-5 Acc. (%)</th></tr></thead><tbody><tr><td>DeiT-Ti [36]</td><td>192</td><td>3</td><td>12</td><td>1.25</td><td>5.72</td><td>72.20</td><td>91.10</td></tr><tr><td>DeiT-Ti + PoWER [13]</td><td>192</td><td>3</td><td>12</td><td>0.80</td><td>5.72</td><td>69.40  (-2.80)</td><td>89.20 (-1.90)</td></tr><tr><td>HVT-Ti-1</td><td>192</td><td>3</td><td>12</td><td>0.64</td><td>5.74</td><td>69.64 (-2.56)</td><td>89.40 (-1.70)</td></tr><tr><td>Scale HVT-Ti-4</td><td>384</td><td>6</td><td>12</td><td>1.39</td><td>22.12</td><td>75.23  (+3.03)</td><td>92.30  (+1.20)</td></tr><tr><td>DeiT-S [36]</td><td>384</td><td>6</td><td>12</td><td>4.60</td><td>22.05</td><td>79.80</td><td>95.00</td></tr><tr><td>DeiT-S + PoWER [13]</td><td>384</td><td>6</td><td>12</td><td>2.70</td><td>22.05</td><td>78.30 (-1.50)</td><td>94.00 (-1.00)</td></tr><tr><td>HVT-S-1</td><td>384</td><td>6</td><td>12</td><td>2.40</td><td>22.09</td><td>78.00 (-1.80)</td><td>93.83 (-1.17)</td></tr></tbody></table>", "caption": "Table 1: Performance comparisons with DeiT and PoWER on ImageNet. \u201cEmbedding Dim\u201d refers to the dimension of each token in the sequence. \u201c#Heads\u201d and \u201c#Blocks\u201d are the number of self-attention heads and blocks in Transformer, respectively. \u201cFLOPs\u201d is measured with a 224\\times 224 image.\u201cTi\u201d and \u201cS\u201d are short for the tiny and small settings, respectively. \u201cArchitecture-M\u201d denotes the model with M pooling stages. \u201cScale\u201d denotes that we scale up the embedding dimension and/or the number of self-attention heads. \u201cDeiT-Ti/S + PoWER\u201d refers to the model that applies the techniques in PoWER-BERT [13] to DeiT-Ti/S.", "list_citation_info": ["[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In ICML, pages 10347\u201310357, 2021.", "[13] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan T. Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating BERT inference via progressive word-vector elimination. In ICML, 2020."]}]}