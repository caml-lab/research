{"title": "Detr3d: 3d object detection from multi-view images via 3d-to-2d queries", "abstract": "We introduce a framework for multi-camera 3D object detection. In contrast to existing works, which estimate 3D bounding boxes directly from monocular images or use depth prediction networks to generate input for 3D object detection from 2D information, our method manipulates predictions directly in 3D space. Our architecture extracts 2D features from multiple camera images and then uses a sparse set of 3D object queries to index into these 2D features, linking 3D positions to multi-view images using camera transformation matrices. Finally, our model makes a bounding box prediction per object query, using a set-to-set loss to measure the discrepancy between the ground-truth and the prediction. This top-down approach outperforms its bottom-up counterpart in which object bounding box prediction follows per-pixel depth estimation, since it does not suffer from the compounding error introduced by a depth prediction model. Moreover, our method does not require post-processing such as non-maximum suppression, dramatically improving inference speed. We achieve state-of-the-art performance on the nuScenes autonomous driving benchmark.", "authors": ["Yue Wang", " Vitor Guizilini", " Tianyuan Zhang", " Yilun Wang", " Hang Zhao", " Justin Solomon"], "pdf_url": "https://arxiv.org/abs/2110.06922", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>NDS \\uparrow</td><td>mAP \\uparrow</td><td>mATE \\downarrow</td><td>mASE \\downarrow</td><td>mAOE \\downarrow</td><td>mAVE \\downarrow</td><td>mAAE \\downarrow</td><td>NMS</td></tr><tr><td>CenterNet \\ast</td><td>0.328</td><td>0.306</td><td>0.716</td><td>0.264</td><td>0.609</td><td>1.426</td><td>0.658</td><td>\u2713</td></tr><tr><td>FCOS3D</td><td>0.373</td><td>0.299</td><td>0.785</td><td>0.268</td><td>0.557</td><td>1.396</td><td>0.154</td><td>\u2713</td></tr><tr><td>FCOS3D {\\ddagger}</td><td>0.393</td><td>0.321</td><td>0.746</td><td>0.265</td><td>0.503</td><td>1.351</td><td>0.160</td><td>\u2713</td></tr><tr><td>FCOS3D \\lx@sectionsign</td><td>0.402</td><td>0.326</td><td>0.743</td><td>0.259</td><td>0.441</td><td>1.341</td><td>0.163</td><td>\u2713</td></tr><tr><td>FCOS3D \\lx@paragraphsign</td><td>0.415</td><td>0.343</td><td>0.725</td><td>0.263</td><td>0.422</td><td>1.292</td><td>0.153</td><td>\u2713</td></tr><tr><td>DETR3D (Ours)</td><td>0.374</td><td>0.303</td><td>0.860</td><td>0.278</td><td>0.437</td><td>0.967</td><td>0.235</td><td>-</td></tr><tr><td>DETR3D (Ours) {\\dagger}</td><td>0.425</td><td>0.346</td><td>0.773</td><td>0.268</td><td>0.383</td><td>0.842</td><td>0.216</td><td>-</td></tr><tr><td>DETR3D (Ours) \\#</td><td>0.434</td><td>0.349</td><td>0.716</td><td>0.268</td><td>0.379</td><td>0.842</td><td>0.200</td><td>-</td></tr></tbody></table>", "caption": "Table 1: Comparisons to recent works on the validation set. Our method is robust to the usage of NMS. \\ast: CenterNet uses a customized backbone DLA [38]. {\\ddagger}: this model is trained with depth weight 1.0 and initialized from a FCOS3D checkpoint; the checkpoint is trained on the same dataset with depth weight 0.2. \\lx@sectionsign: with test-time augmentation. \\lx@paragraphsign: with test-time augmentation, more epochs, and model ensemble. For details, see [2]. {\\dagger}: our model is also initialized from a FCOS3D backbone; the detection head is initialized randomly. \\#: trained with CBGS [39]  ", "list_citation_info": ["Yu et al. [2018] F. Yu, D. Wang, E. Shelhamer, and T. Darrell. Deep layer aggregation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "Wang et al. [2021] T. Wang, X. Zhu, J. Pang, and D. Lin. FCOS3D: Fully convolutional one-stage monocular 3d object detection. arXiv:2104.10956, 2021.", "Zhu et al. [2019] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu. Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection. arXiv e-prints, art. arXiv:1908.09492, Aug 2019."]}, {"table": "<table><tbody><tr><td>Method</td><td>NDS \\uparrow</td><td>mAP \\uparrow</td><td>mATE \\downarrow</td><td>mASE \\downarrow</td><td>mAOE \\downarrow</td><td>mAVE \\downarrow</td><td>mAAE \\downarrow</td><td>NMS</td></tr><tr><td>Mono3D</td><td>0.429</td><td>0.366</td><td>0.642</td><td>0.252</td><td>0.523</td><td>1.591</td><td>0.119</td><td>N/A</td></tr><tr><td>DHNet</td><td>0.437</td><td>0.363</td><td>0.667</td><td>0.259</td><td>0.402</td><td>1.589</td><td>0.120</td><td>N/A</td></tr><tr><td>PGD [40]</td><td>0.448</td><td>0.386</td><td>0.626</td><td>0.245</td><td>0.451</td><td>1.509</td><td>0.127</td><td>\u2713</td></tr><tr><td>DD3D [37] {\\dagger}</td><td>0.477</td><td>0.418</td><td>0.572</td><td>0.249</td><td>0.368</td><td>1.014</td><td>0.124</td><td>\u2713</td></tr><tr><td>DETR3D (Ours) \\#</td><td>0.479</td><td>0.412</td><td>0.641</td><td>0.255</td><td>0.394</td><td>0.845</td><td>0.133</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Comparisons to top-performing works on the test set from the leaderboard. \\#: initialized from a DD3D checkpoint. {\\dagger}: initialized from a backbone pre-trained on extra data. ", "list_citation_info": ["Park et al. [2021] D. Park, R. Ambrus, V. Guizilini, J. Li, and A. Gaidon. Is pseudo-lidar needed for monocular 3d object detection? In IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "Wang et al. [2021] T. Wang, X. ZHU, J. Pang, and D. Lin. Probabilistic and geometric depth: Detecting objects in perspective. In 5th Annual Conference on Robot Learning, 2021. URL https://openreview.net/forum?id=bEito8UUUmf."]}, {"table": "<table><thead><tr><th>Method</th><th>NDS \\uparrow</th><th>mAP \\uparrow</th><th>mATE \\downarrow</th><th>mASE \\downarrow</th><th>mAOE \\downarrow</th><th>mAVE \\downarrow</th><th>mAAE \\downarrow</th><th>NMS</th></tr></thead><tbody><tr><td>FCOS3D</td><td>0.317</td><td>0.213</td><td>0.841</td><td>0.276</td><td>0.604</td><td>1.122</td><td>0.173</td><td>\u2713</td></tr><tr><td>FCOS3D{\\ddagger}</td><td>0.329</td><td>0.229</td><td>0.816</td><td>0.272</td><td>0.571</td><td>1.084</td><td>0.195</td><td>\u2713</td></tr><tr><td>DETR3D (Ours)</td><td>0.356</td><td>0.231</td><td>0.825</td><td>0.280</td><td>0.400</td><td>0.863</td><td>0.223</td><td>-</td></tr><tr><td>DETR3D (Ours) {\\dagger}</td><td>0.384</td><td>0.268</td><td>0.807</td><td>0.273</td><td>0.453</td><td>0.788</td><td>0.184</td><td>-</td></tr></tbody></table>", "caption": "Table 3: Comparisons in Overlap Region. {\\ddagger}: this model is trained with depth weight 1.0 and initialized from a FCOS3D checkpoint; the checkpoint is trained on the same dataset with depth weight 0.2. For details, see [2]. {\\dagger}: our model is also initialized from a FCOS3D backbone; the detection head is initialized randomly.  ", "list_citation_info": ["Wang et al. [2021] T. Wang, X. Zhu, J. Pang, and D. Lin. FCOS3D: Fully convolutional one-stage monocular 3d object detection. arXiv:2104.10956, 2021."]}]}