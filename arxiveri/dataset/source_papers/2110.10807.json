{"title": "Text-Based Person Search with Limited Data", "abstract": "Text-based person search (TBPS) aims at retrieving a target person from an image gallery with a descriptive text query. Solving such a fine-grained cross-modal retrieval task is challenging, which is further hampered by the lack of large-scale datasets. In this paper, we present a framework with two novel components to handle the problems brought by limited data. Firstly, to fully utilize the existing small-scale benchmarking datasets for more discriminative feature learning, we introduce a cross-modal momentum contrastive learning framework to enrich the training data for a given mini-batch. Secondly, we propose to transfer knowledge learned from existing coarse-grained large-scale datasets containing image-text pairs from drastically different problem domains to compensate for the lack of TBPS training data. A transfer learning method is designed so that useful information can be transferred despite the large domain gap. Armed with these components, our method achieves new state of the art on the CUHK-PEDES dataset with significant improvements over the prior art in terms of Rank-1 and mAP. Our code is available at https://github.com/BrandonHanx/TextReID.", "authors": ["Xiao Han", " Sen He", " Li Zhang", " Tao Xiang"], "pdf_url": "https://arxiv.org/abs/2110.10807", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Arch.</td><td rowspan=\"2\">Dim.</td><td colspan=\"4\">Text to Image w/o Rerank</td><td colspan=\"4\">Image to Text w/o Rerank</td></tr><tr><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td></tr><tr><td>GNA-RNN Li et al.(2017)Li, Xiao, Li, Zhou, Yue, and Wang</td><td>S</td><td>512</td><td>19.05</td><td>-</td><td>53.64</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Dual Path Zheng et al.(2020b)Zheng, Zheng, Garrett, Yang, Xu, andShen</td><td>S</td><td>2048</td><td>44.40</td><td>66.26</td><td>75.07</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CMPM/C\\dagger Zhang and Lu(2018)</td><td>S</td><td>512</td><td>49.37</td><td>71.69</td><td>79.27</td><td>-</td><td>60.96</td><td>84.42</td><td>90.83</td><td>-</td></tr><tr><td>MIA Niu et al.(2020)Niu, Huang, Ouyang, and Wang</td><td>M</td><td>1024</td><td>53.10</td><td>75.00</td><td>82.90</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PMA Jing et al.(2020)Jing, Si, Wang, Wang, Wang, and Tan</td><td>M</td><td>1024</td><td>54.12</td><td>75.45</td><td>82.97</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TIMAM\\dagger Sarafianos et al.(2019)Sarafianos, Xu, andKakadiaris</td><td>S</td><td>512</td><td>54.51</td><td>77.56</td><td>84.78</td><td>-</td><td>67.40</td><td>88.65</td><td>93.91</td><td>-</td></tr><tr><td>CKMA Chen et al.(2021a)Chen, Huang, Chang, Tan, Xue, andMa</td><td>S</td><td>512</td><td>54.69</td><td>73.65</td><td>81.86</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ViTAA\\ddagger Wang et al.(2020a)Wang, Fang, Wang, andYang</td><td>M</td><td>256</td><td>54.92</td><td>75.18</td><td>82.90</td><td>51.60</td><td>65.71</td><td>88.68</td><td>93.75</td><td>45.75</td></tr><tr><td>CMAAM Aggarwal et al.(2020)Aggarwal, Babu, andChakraborty</td><td>M</td><td>512</td><td>56.68</td><td>77.18</td><td>84.86</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HGAN\\dagger Zheng et al.(2020a)Zheng, Liu, Liu, Zha, andMei</td><td>M</td><td>512</td><td>59.00</td><td>79.49</td><td>86.62</td><td>-</td><td>71.16</td><td>90.05</td><td>95.06</td><td>-</td></tr><tr><td>NAFS (G)\\ddagger Gao et al.(2021)Gao, Cai, Jiang, Zheng, Zhang, Gong, Peng, Guo, andSun</td><td>M</td><td>768</td><td>59.36</td><td>79.13</td><td>86.00</td><td>54.07</td><td>71.89</td><td>90.99</td><td>95.28</td><td>50.16</td></tr><tr><td>MGEL Wang et al.(2021)Wang, Luo, Lin, and Li</td><td>M</td><td>512</td><td>60.27</td><td>80.01</td><td>86.74</td><td>-</td><td>71.87</td><td>91.38</td><td>95.42</td><td>-</td></tr><tr><td>AXM-Net Farooq et al.(2021)Farooq, Awais, Kittler, and Khalid</td><td>M</td><td>512</td><td>61.90</td><td>79.41</td><td>85.75</td><td>57.38</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TIPCB\\ddagger Chen et al.(2021b)Chen, Zhang, Lu, Wang, Zheng, andWang</td><td>M</td><td>2048</td><td>63.63</td><td>82.82</td><td>89.01</td><td>56.78</td><td>73.55</td><td>92.26</td><td>96.03</td><td>51.78</td></tr><tr><td>Ours (ResNet50)</td><td>S</td><td>256</td><td>61.65</td><td>80.98</td><td>86.78</td><td>58.29</td><td>75.96</td><td>93.40</td><td>96.55</td><td>55.05</td></tr><tr><td>Ours (ResNet101)</td><td>S</td><td>256</td><td>64.08</td><td>81.73</td><td>88.19</td><td>60.08</td><td>78.99</td><td>95.02</td><td>97.17</td><td>56.78</td></tr><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Arch.</td><td rowspan=\"2\">Dim.</td><td colspan=\"4\">Text to Image w/ Rerank</td><td colspan=\"4\">Image to Text w/ Rerank</td></tr><tr><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td></tr><tr><td>ViTAA\\ddagger Wang et al.(2020a)Wang, Fang, Wang, andYang</td><td>M</td><td>256</td><td>54.92</td><td>74.77</td><td>82.49</td><td>52.60</td><td>66.17</td><td>88.61</td><td>93.56</td><td>46.39</td></tr><tr><td>NAFS (G)\\ddagger Gao et al.(2021)Gao, Cai, Jiang, Zheng, Zhang, Gong, Peng, Guo, andSun</td><td>M</td><td>768</td><td>59.62</td><td>78.90</td><td>85.72</td><td>55.02</td><td>72.67</td><td>90.92</td><td>95.12</td><td>50.92</td></tr><tr><td>TIPCB\\ddagger Chen et al.(2021b)Chen, Zhang, Lu, Wang, Zheng, andWang</td><td>M</td><td>2048</td><td>63.37</td><td>81.56</td><td>87.57</td><td>60.02</td><td>74.04</td><td>92.06</td><td>95.61</td><td>53.78</td></tr><tr><td>Ours (ResNet50)</td><td>S</td><td>256</td><td>61.94</td><td>80.52</td><td>86.45</td><td>59.45</td><td>76.26</td><td>93.46</td><td>96.58</td><td>55.67</td></tr><tr><td>Ours (ResNet101)</td><td>S</td><td>256</td><td>64.40</td><td>81.27</td><td>87.96</td><td>61.19</td><td>78.99</td><td>95.02</td><td>97.23</td><td>57.31</td></tr></tbody></table>", "caption": "Table 1: Comparisons with previous methods on the CUHK-PEDES. Only global features are used during inference for our reproduced NAFS Gao et al.(2021)Gao, Cai, Jiang, Zheng, Zhang, Gong, Peng, Guo, andSun. \"Arch.\"/\"Dim.\" is the abbreviation for architecture/feature dimension. S/M stands for the methods designed in single-/multi-scale architecture, and all single-scale methods are highlighted with gray background. \\dagger stands for the results from HGAN Zheng et al.(2020a)Zheng, Liu, Liu, Zha, andMei. \\ddagger stands for the results reproduced with public codes/checkpoints released by their authors. Overall 1^{st}/2^{nd} best in red/blue.", "list_citation_info": ["[Li et al.(2017)Li, Xiao, Li, Zhou, Yue, and Wang] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, and Xiaogang Wang. Person search with natural language description. In CVPR, 2017.", "[Chen et al.(2021a)Chen, Huang, Chang, Tan, Xue, and Ma] Yucheng Chen, Rui Huang, Hong Chang, Chuanqi Tan, Tao Xue, and Bingpeng Ma. Cross-modal knowledge adaptation for language-based person search. IEEE TIP, 2021a.", "[Sarafianos et al.(2019)Sarafianos, Xu, and Kakadiaris] Nikolaos Sarafianos, Xiang Xu, and Ioannis A. Kakadiaris. Adversarial representation learning for text-to-image matching. In ICCV, 2019.", "[Chen et al.(2021b)Chen, Zhang, Lu, Wang, Zheng, and Wang] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang, Yuhui Zheng, and Ruili Wang. Tipcb: A simple but effective part-based convolutional baseline for text-based person search. arXiv preprint, 2021b.", "[Wang et al.(2021)Wang, Luo, Lin, and Li] Chengji Wang, Zhiming Luo, Yaojin Lin, and Shaozi Li. Text-based person search via multi-granularity embedding learning. In IJCAI, 2021.", "[Niu et al.(2020)Niu, Huang, Ouyang, and Wang] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Improving description-based person re-identification by multi-granularity image-text alignments. IEEE TIP, 2020.", "[Zheng et al.(2020b)Zheng, Zheng, Garrett, Yang, Xu, and Shen] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional image-text embeddings with instance loss. ACM TOMM, 2020b.", "[Wang et al.(2020a)Wang, Fang, Wang, and Yang] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vitaa: Visual-textual attributes alignment in person search by natural language. In ECCV, 2020a.", "[Gao et al.(2021)Gao, Cai, Jiang, Zheng, Zhang, Gong, Peng, Guo, and Sun] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng, Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing Sun. Contextual non-local alignment over full-scale representation for text-based person search. arXiv preprint, 2021.", "[Aggarwal et al.(2020)Aggarwal, Babu, and Chakraborty] Surbhi Aggarwal, R. Venkatesh Babu, and Anirban Chakraborty. Text-based person search via attribute-aided matching. In WACV, 2020.", "[Jing et al.(2020)Jing, Si, Wang, Wang, Wang, and Tan] Ya Jing, Chenyang Si, Junbo Wang, Wei Wang, Liang Wang, and Tieniu Tan. Pose-guided multi-granularity attention network for text-based person search. In AAAI, 2020.", "[Zheng et al.(2020a)Zheng, Liu, Liu, Zha, and Mei] Kecheng Zheng, Wu Liu, Jiawei Liu, Zheng-Jun Zha, and Tao Mei. Hierarchical gumbel attention network for text-based person search. In ACM MM, 2020a.", "[Zhang and Lu(2018)] Ying Zhang and Huchuan Lu. Deep cross-modal projection learning for image-text matching. In ECCV, 2018.", "[Farooq et al.(2021)Farooq, Awais, Kittler, and Khalid] Ammarah Farooq, Muhammad Awais, Josef Kittler, and Syed Safwan Khalid. Axm-net: Cross-modal context sharing attention network for person re-id. arXiv preprint, 2021."]}, {"table": "<table><thead><tr><th>Visual Backbone</th><th>Textual Backbone</th><th>Paired Data</th><th>Transfer Strategy</th><th>CM-MoCo</th><th>Rank-1</th><th>Rank-5</th><th>Rank-10</th></tr></thead><tbody><tr><td>ResNet101</td><td>BERT</td><td>-</td><td></td><td></td><td>51.95</td><td>71.30</td><td>79.78</td></tr><tr><td>ResNet101</td><td>BERT</td><td>-</td><td>\u2713</td><td></td><td>58.24</td><td>78.64</td><td>85.62</td></tr><tr><td>ResNet101</td><td>BERT</td><td>-</td><td>\u2713</td><td>\u2713</td><td>59.62</td><td>79.13</td><td>86.08</td></tr><tr><td>ResNet101</td><td>BERT</td><td>MSCOCO Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,Doll\u00e1r, and Zitnick</td><td></td><td></td><td>53.30</td><td>72.77</td><td>80.02</td></tr><tr><td>ResNet101</td><td>BERT</td><td>MSCOCO Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,Doll\u00e1r, and Zitnick</td><td>\u2713</td><td></td><td>59.96</td><td>79.74</td><td>86.83</td></tr><tr><td>ResNet101</td><td>BERT</td><td>MSCOCO Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,Doll\u00e1r, and Zitnick</td><td>\u2713</td><td>\u2713</td><td>60.79</td><td>79.58</td><td>87.35</td></tr><tr><td>CLIP ResNet101</td><td>CLIP-TE</td><td>WIT Radford et al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,Sastry, Askell, Mishkin, Clark, et al.</td><td></td><td></td><td>0.15</td><td>0.76</td><td>1.23</td></tr><tr><td>CLIP ResNet101</td><td>CLIP-TE</td><td>WIT Radford et al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,Sastry, Askell, Mishkin, Clark, et al.</td><td>\u2713</td><td></td><td>62.52</td><td>80.57</td><td>87.12</td></tr><tr><td>CLIP ResNet101</td><td>CLIP-TE</td><td>WIT Radford et al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,Sastry, Askell, Mishkin, Clark, et al.</td><td>\u2713</td><td>\u2713</td><td>64.08</td><td>81.73</td><td>88.19</td></tr></tbody></table>", "caption": "Table 2: Ablation experimental results for proposed components. Only the results without rerank for text-to-image task are reported. Paired data denotes the dataset used for pre-training. Transfer strategy denotes the proposed strategy mentioned in Section 3.2.", "list_citation_info": ["[Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll\u00e1r, and Zitnick] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.", "[Radford et al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et al.] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th rowspan=\"2\">Trainset</th><th colspan=\"3\">Image Retrieval</th><th colspan=\"3\">Caption Retrieval</th></tr><tr><th>R@1</th><th>R@5</th><th>R@10</th><th>R@1</th><th>R@5</th><th>R@10</th></tr></thead><tbody><tr><td>VSE++ (VGG19, GRU, FT)</td><td>RC+rV</td><td>24.1</td><td>52.8</td><td>66.2</td><td>32.9</td><td>61.7</td><td>74.7</td></tr><tr><td>VSE++ (ResNet152, GRU, FT)</td><td>RC+rV</td><td>30.3</td><td>59.4</td><td>72.4</td><td>41.3</td><td>71.1</td><td>81.2</td></tr><tr><td>Ours (ResNet101, BERT)</td><td>RC+rV</td><td>33.2</td><td>63.5</td><td>75.1</td><td>46.8</td><td>76.3</td><td>85.7</td></tr></tbody></table>", "caption": "Table 5: Comparison between our pre-trained model and VSE++ Faghri et al.(2018)Faghri, Fleet, Kiros, and Fidler on MSCOCO Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,Doll\u00e1r, and Zitnick. All results are calculated in MSCOCO 5k test split. FT, RC and rV denote fine-tune, random crop and rest validation set, respectively. Please refer to the paper of VSE++ Faghri et al.(2018)Faghri, Fleet, Kiros, and Fidler for details.", "list_citation_info": ["[Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll\u00e1r, and Zitnick] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.", "[Faghri et al.(2018)Faghri, Fleet, Kiros, and Fidler] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. In BMVC, 2018."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Rank-1 \\uparrow</td><td rowspan=\"2\">Params (M) \\downarrow</td><td rowspan=\"2\">Retrieval Time (s) \\downarrow</td><td colspan=\"2\">Offline Feature Storage \\downarrow</td></tr><tr><td>Visual Side</td><td>Textual Side</td></tr><tr><td>ViTAA Wang et al.(2020a)Wang, Fang, Wang, andYang</td><td>54.92</td><td>176.53</td><td>0.02</td><td>3MB</td><td>6MB</td></tr><tr><td>NAFS Gao et al.(2021)Gao, Cai, Jiang, Zheng, Zhang, Gong, Peng, Guo, andSun</td><td>59.36</td><td>188.75</td><td>0.07</td><td>9MB</td><td>18MB</td></tr><tr><td>TIPCB Chen et al.(2021b)Chen, Zhang, Lu, Wang, Zheng, andWang</td><td>63.63</td><td>184.75</td><td>0.20</td><td>24MB</td><td>48MB</td></tr><tr><td>Ours (ResNet50)</td><td>61.65</td><td>42.33</td><td>0.02</td><td>3MB</td><td>6MB</td></tr><tr><td>Ours (ResNet101)</td><td>64.08</td><td>60.20</td><td>0.02</td><td>3MB</td><td>6MB</td></tr></tbody></table>", "caption": "Table 6: Comparisons of model size and retrieval efficiency among ViTAA Wang et al.(2020a)Wang, Fang, Wang, andYang, NAFS Gao et al.(2021)Gao, Cai, Jiang, Zheng, Zhang, Gong, Peng, Guo, andSun and our method. Retrieval time is computed by retrieving all text queries (6156) through the whole image gallery (3074) of CUHK-PEDES test set Li et al.(2017)Li, Xiao, Li, Zhou, Yue, and Wang.", "list_citation_info": ["[Li et al.(2017)Li, Xiao, Li, Zhou, Yue, and Wang] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, and Xiaogang Wang. Person search with natural language description. In CVPR, 2017.", "[Gao et al.(2021)Gao, Cai, Jiang, Zheng, Zhang, Gong, Peng, Guo, and Sun] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng, Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing Sun. Contextual non-local alignment over full-scale representation for text-based person search. arXiv preprint, 2021.", "[Chen et al.(2021b)Chen, Zhang, Lu, Wang, Zheng, and Wang] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang, Yuhui Zheng, and Ruili Wang. Tipcb: A simple but effective part-based convolutional baseline for text-based person search. arXiv preprint, 2021b.", "[Wang et al.(2020a)Wang, Fang, Wang, and Yang] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vitaa: Visual-textual attributes alignment in person search by natural language. In ECCV, 2020a."]}]}