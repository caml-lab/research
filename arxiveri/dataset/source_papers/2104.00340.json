{"title": "Reconstructing 3D Human Pose by Watching Humans in the Mirror", "abstract": "In this paper, we introduce the new task of reconstructing 3D human pose from a single image in which we can see the person and the person's image through a mirror. Compared to general scenarios of 3D pose estimation from a single view, the mirror reflection provides an additional view for resolving the depth ambiguity. We develop an optimization-based approach that exploits mirror symmetry constraints for accurate 3D pose reconstruction. We also provide a method to estimate the surface normal of the mirror from vanishing points in the single image. To validate the proposed approach, we collect a large-scale dataset named Mirrored-Human, which covers a large variety of human subjects, poses and backgrounds. The experiments demonstrate that, when trained on Mirrored-Human with our reconstructed 3D poses as pseudo ground-truth, the accuracy and generalizability of existing single-view 3D pose estimators can be largely improved.", "authors": ["Qi Fang", " Qing Shuai", " Junting Dong", " Hujun Bao", " Xiaowei Zhou"], "pdf_url": "https://arxiv.org/abs/2104.00340", "list_table_and_caption": [{"table": "<table><tr><td>Methods</td><td>MPJPE \\downarrow</td><td>PA-MPJPE \\downarrow</td><td>MRPE \\downarrow</td></tr><tr><td>SMPLify-X [42]</td><td>143.73</td><td>90.57</td><td>368.00</td></tr><tr><td>SPIN [25]</td><td>109.79</td><td>67.42</td><td>167.62</td></tr><tr><td>Baseline ([25]+[42])</td><td>82.92</td><td>61.47</td><td>147.44</td></tr><tr><td>Ours (w/o L_{s}, L_{n})</td><td>53.81</td><td>34.48</td><td>108.43</td></tr><tr><td>Ours (w/o L_{n})</td><td>39.52</td><td>33.24</td><td>101.91</td></tr><tr><td>Ours (full)</td><td>38.77</td><td>32.96</td><td>93.22</td></tr></table>", "caption": "Table 1: Quantitative analysis. \u2018Baseline\u2019 uses [25] as initialization and [42] as the optimization method to fit the body model to 2D keypoints for each person separately.", "list_citation_info": ["[25] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In ICCV, 2019.", "[42] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from a single image. In CVPR, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Datasets</td><td rowspan=\"2\">Frames(k)</td><td rowspan=\"2\">Subjects</td><td colspan=\"3\">Annotations</td></tr><tr><td>3D</td><td>2D</td><td>Internet</td></tr><tr><td>InstaVariety [23]</td><td>2100</td><td>28272</td><td></td><td></td><td>\u2713</td></tr><tr><td>Penn Action [65]</td><td>77</td><td>2326</td><td></td><td>\u2713</td><td>\u2713</td></tr><tr><td>Human3.6M [19]</td><td>581</td><td>11</td><td>\u2713</td><td>\u2713</td><td></td></tr><tr><td>3DPW [56]</td><td>51</td><td>18</td><td>\u2713</td><td>\u2713</td><td></td></tr><tr><td>Mirrored-Human</td><td>1800</td><td>200+</td><td>\u2713*</td><td>\u2713</td><td>\u2713</td></tr></table>", "caption": "Table 2: Comparison of relevant datasets in terms of the number of frames, subjects and annotation types. *Our Mirrored-Human dataset adopts our reconstructed 3D poses as pseudo ground-truth.", "list_citation_info": ["[23] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3d human dynamics from video. In CVPR, 2019.", "[19] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE TPAMI, 2014.", "[56] Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In ECCV, 2018.", "[65] Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In ICCV, 2013."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">3DPW</td><td colspan=\"2\">Human3.6M</td></tr><tr><td>MPJPE \\downarrow</td><td>PA-MPJPE \\downarrow</td><td>MPJPE \\downarrow</td><td>PA-MPJPE \\downarrow</td></tr><tr><td>HMR [22]</td><td>-</td><td>81.3</td><td>88.0</td><td>56.8</td></tr><tr><td>HMMR [23]</td><td>-</td><td>72.6</td><td>-</td><td>56.9</td></tr><tr><td>Arnab. [3]</td><td>-</td><td>72.2</td><td>77.8</td><td>54.3</td></tr><tr><td>CMR [26]</td><td>-</td><td>70.2</td><td>-</td><td>50.1</td></tr><tr><td>SPIN [25]</td><td>98.2*</td><td>59.2</td><td>62.3*</td><td>41.1</td></tr><tr><td>MeshNet [37]</td><td>93.2</td><td>58.6</td><td>55.7</td><td>41.7</td></tr><tr><td>Baseline</td><td>90.0</td><td>57.5</td><td>54.7</td><td>41.7</td></tr><tr><td>[37]+MiHu</td><td>85.1</td><td>54.8</td><td>53.6</td><td>41.0</td></tr></table>", "caption": "Table 3: Results on 3DPW and Human3.6M datasets. \u2018MiHu\u2019 is our Mirrored-Human dataset. \u2018Baseline\u2019 means training MeshNet [37] on Mirrored-Human with 3D annotations given by SMPLify-X [42]. *MPJPE of [25] is obtained by their released model.", "list_citation_info": ["[25] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In ICCV, 2019.", "[3] Anurag* Arnab, Carl* Doersch, and Andrew Zisserman. Exploiting temporal context for 3d human pose estimation in the wild. In CVPR, 2019.", "[42] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from a single image. In CVPR, 2019.", "[23] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. Learning 3d human dynamics from video. In CVPR, 2019.", "[22] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In CVPR, 2018.", "[26] Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis. Convolutional mesh regression for single-image human shape reconstruction. In CVPR, 2019.", "[37] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image. In ECCV, 2020."]}, {"table": "<table><tr><td></td><td>Methods</td><td>AP{}_{root}^{25}\\uparrow</td><td>PCK{}_{rel}\\uparrow</td><td>PCK{}_{abs}\\uparrow</td></tr><tr><td rowspan=\"7\">TD</td><td>LCRNet [46]</td><td>-</td><td>53.8</td><td>-</td></tr><tr><td>LCRNet++ [47]</td><td>-</td><td>70.6</td><td>-</td></tr><tr><td>Dabral. [10]</td><td>-</td><td>71.3</td><td>-</td></tr><tr><td>PandaNet [4]</td><td>-</td><td>72.0</td><td>-</td></tr><tr><td>HMOR [29]</td><td>-</td><td>82.0</td><td>43.8</td></tr><tr><td>Moon. [36]</td><td>31.0</td><td>81.8</td><td>31.5</td></tr><tr><td>Moon. [36]+MiHu</td><td>42.2</td><td>82.3</td><td>43.0</td></tr><tr><td rowspan=\"4\">BU</td><td>Mehta. [35]</td><td>-</td><td>65.0</td><td>-</td></tr><tr><td>Xnect [34]</td><td>-</td><td>70.4</td><td>-</td></tr><tr><td>SMAP [66]</td><td>37.3</td><td>73.5</td><td>35.4</td></tr><tr><td>SMAP [66]+MiHu</td><td>42.3</td><td>74.1</td><td>38.0</td></tr></table>", "caption": "Table 4: Results on the MuPoTS-3D dataset. The numbers are calculated for all people. \u2018MiHu\u2019 is our Mirrored-Human dataset. \u2018TD\u2019 and \u2018BU\u2019 mean \u2018top-down\u2019 and \u2018bottom-up\u2019, respectively.", "list_citation_info": ["[10] Rishabh Dabral, Nitesh B Gundavarapu, Rahul Mitra, Abhishek Sharma, Ganesh Ramakrishnan, and Arjun Jain. Multi-person 3d human pose estimation from monocular images. In 3DV, 2019.", "[29] Jiefeng Li, Can Wang, Wentao Liu, Chen Qian, and Cewu Lu. Hmor: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation. In ECCV, 2020.", "[36] Gyeongsik Moon, Juyong Chang, and Kyoung Mu Lee. Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image. In ICCV, 2019.", "[35] Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, and Christian Theobalt. Single-shot multi-person 3d pose estimation from monocular rgb. In 3DV, 2018.", "[4] Abdallah Benzine, Florian Chabot, Bertrand Luvison, Quoc Cuong Pham, and Catherine Achard. Pandanet: Anchor-based single-shot multi-person 3d pose estimation. In CVPR, 2020.", "[34] Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard Pons-Moll, and Christian Theobalt. Xnect: Real-time multi-person 3d human pose estimation with a single rgb camera. ACM TOG, 2020.", "[46] Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net: Localization-classification-regression for human pose. In CVPR, 2017.", "[66] Jianan Zhen, Qi Fang, Jiaming Sun, Wentao Liu, Wei Jiang, Hujun Bao, and Xiaowei Zhou. Smap: Single-shot multi-person absolute 3d pose estimation. In ECCV, 2020.", "[47] Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net++: Multi-person 2d and 3d pose detection in natural images. IEEE TPAMI, 2019."]}]}