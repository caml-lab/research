{"title": "Joint visual semantic reasoning: Multi-stage decoder for text recognition", "abstract": "Although text recognition has significantly evolved over the years, state-of-the-art (SOTA) models still struggle in the wild scenarios due to complex backgrounds, varying fonts, uncontrolled illuminations, distortions and other artefacts. This is because such models solely depend on visual information for text recognition, thus lacking semantic reasoning capabilities. In this paper, we argue that semantic information offers a complementary role in addition to visual only. More specifically, we additionally utilize semantic information by proposing a multi-stage multi-scale attentional decoder that performs joint visual-semantic reasoning. Our novelty lies in the intuition that for text recognition, the prediction should be refined in a stage-wise manner. Therefore our key contribution is in designing a stage-wise unrolling attentional decoder where non-differentiability, invoked by discretely predicted character labels, needs to be bypassed for end-to-end training. While the first stage predicts using visual features, subsequent stages refine on top of it using joint visual-semantic information. Additionally, we introduce multi-scale 2D attention along with dense and residual connections between different stages to deal with varying scales of character sizes, for better performance and faster convergence during training. Experimental results show our approach to outperform existing SOTA methods by a considerable margin.", "authors": ["Ayan Kumar Bhunia", " Aneeshan Sain", " Amandeep Kumar", " Shuvozit Ghose", " Pinaki Nath Chowdhury", " Yi-Zhe Song"], "pdf_url": "https://arxiv.org/abs/2107.12090", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Methods</td><td>Year</td><td>IIIT-5K</td><td>SVT</td><td>IC13</td><td>IC15</td><td>SVT-P</td><td>CUTE80</td><td>Remarks</td></tr><tr><td>Shi et al. [51]</td><td>2015</td><td>81.2</td><td>82.7</td><td>89.6</td><td>-</td><td>66.8</td><td>54.9</td><td>\u2022\u2004End-to-end trainable CNN + RNN + CTC.</td></tr><tr><td>Lee et al. [32]</td><td>2016</td><td>78.4</td><td>80.7</td><td>-</td><td>90.8</td><td>-</td><td>42.7</td><td>\u2022\u2004Recursive CNN + RNN + Atten. decoder.</td></tr><tr><td>Shi et al. [52]</td><td>2016</td><td>81.9</td><td>81.9</td><td>88.6</td><td>-</td><td>-</td><td>-</td><td>\u2022\u2004Introduce rectification network for irregular images.</td></tr><tr><td>Cheng et al. [14]</td><td>2017</td><td>87.4</td><td>85.9</td><td>93.3</td><td>70.6</td><td>71.5</td><td>63.9</td><td>\u2022\u2004Learning to focus on character centre, but needs char. location label.</td></tr><tr><td>Liu et al. [39]</td><td>2018</td><td>83.6</td><td>84.4</td><td>-</td><td>60.0</td><td>73.5</td><td>-</td><td>\u2022\u2004Rectify the distortion at individual character level.</td></tr><tr><td>Bai et al. [3]</td><td>2018</td><td>88.3</td><td>87.5</td><td>94.4</td><td>73.9</td><td>-</td><td>-</td><td>\u2022\u2004Edit distance based seq. dissimilarity modeled to handle noisy characters.</td></tr><tr><td>Liu et al. [40]</td><td>2018</td><td>89.4</td><td>87.1</td><td>94.0</td><td>-</td><td>73.9</td><td>62.5</td><td>\u2022\u2004Leverage rendering parameters of synth. word image generation for training.</td></tr><tr><td>Shi et al. [53]</td><td>2018</td><td>93.4</td><td>93.6</td><td>91.8</td><td>76.1</td><td>78.5</td><td>79.5</td><td>\u2022\u2004Improved rectification network by Thin-Plate Spline.</td></tr><tr><td>Cheng et al. [15]</td><td>2018</td><td>87.0</td><td>82.8</td><td>-</td><td>68.2</td><td>73.0</td><td>76.8</td><td>\u2022\u2004Four directional convolutional feature extraction for irregular images.</td></tr><tr><td>Liao et al. [35]</td><td>2019</td><td>91.9</td><td>86.4</td><td>91.5</td><td>-</td><td>-</td><td>79.9</td><td>\u2022\u2004 Segment individual character + discrete char. recog. and word formation.</td></tr><tr><td>Yang et al. [59]</td><td>2019</td><td>94.4</td><td>88.9</td><td>93.9</td><td>78.7</td><td>80.8</td><td>87.5</td><td>\u2022\u2004Models geometrical attributes of text for better images rectification.</td></tr><tr><td>Li et al. [33]</td><td>2019</td><td>95.0</td><td>91.2</td><td>94.0</td><td>78.8</td><td>86.4</td><td>89.6</td><td>\u2022\u2004Introduce 2D-attention to deal with irregular images.</td></tr><tr><td>Baek et al. [1]</td><td>2019</td><td>87.9</td><td>87.5</td><td>92.3</td><td>71.8</td><td>79.2</td><td>74.0</td><td>\u2022\u2004Comparative study of different methods and insightful analysis.</td></tr><tr><td>Zhan et al. [63]</td><td>2019</td><td>93.3</td><td>90.2</td><td>91.3</td><td>76.9</td><td>79.6</td><td>83.3</td><td>\u2022\u2004Iterative image rectification.</td></tr><tr><td>Litman et al. [37]</td><td>2020</td><td>93.7</td><td>92.7</td><td>93.9</td><td>82.2</td><td>86.9</td><td>87.5</td><td>\u2022\u2004Stacking more Bi-LSTM layers + gated fusion of visual-contextual feature.</td></tr><tr><td>Qiao et al. [46]</td><td>2020</td><td>93.8</td><td>89.6</td><td>92.8</td><td>80.0</td><td>81.4</td><td>83.6</td><td>\u2022\u2004Tries to predict the word-embedding vector to initialise the state of decoder.</td></tr><tr><td>Yu et al. [61]</td><td>2020</td><td>94.8</td><td>91.5</td><td>95.5</td><td>82.7</td><td>85.1</td><td>87.8</td><td>\u2022\u2004Faster parallel decoding + semantic reasoning block (non-differentiable).</td></tr><tr><td>Our Baseline (Stage-0)</td><td>-</td><td>88.0</td><td>84.9</td><td>90.4</td><td>74.5</td><td>75.3</td><td>82.6</td><td rowspan=\"4\">\u2022\u2004Joint visual-semantic reasoning through multi-stage decoding usingmulti-scale feature maps and differential semantic space.</td></tr><tr><td>Our Baseline (Stage-1)</td><td>-</td><td>92.6</td><td>89.5</td><td>93.9</td><td>80.3</td><td>81.5</td><td>87.2</td></tr><tr><td>Proposed (Stage-2)</td><td>-</td><td>95.2</td><td>92.2</td><td>95.5</td><td>84.0</td><td>85.7</td><td>89.7</td></tr><tr><td>Our Baseline (Stage-3)</td><td>-</td><td>95.2</td><td>92.1</td><td>95.5</td><td>83.6</td><td>85.5</td><td>89.6</td></tr></tbody></table>", "caption": "Table 1: Comparison of proposed method with different state-of-the-art methods.", "list_citation_info": ["[15] Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang Pu, and Shuigeng Zhou. Aon: Towards arbitrarily-oriented text recognition. In CVPR, 2018.", "[33] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. Show, attend and read: A simple and strong baseline for irregular text recognition. In AAAI, 2019.", "[61] Deli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han, Jingtuo Liu, and Errui Ding. Towards accurate scene text recognition with semantic reasoning networks. In CVPR, 2020.", "[3] Fan Bai, Zhanzhan Cheng, Yi Niu, Shiliang Pu, and Shuigeng Zhou. Edit probability for scene text recognition. In CVPR, 2018.", "[59] MingKun Yang, Yushuo Guan, Minghui Liao, Xin He, Kaigui Bian, Song Bai, Cong Yao, and Xiang Bai. Symmetry-constrained rectification network for scene text recognition. In ICCV, 2019.", "[53] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Aster: An attentional scene text recognizer with flexible rectification. T-PAMI, 2018.", "[32] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets with attention modeling for ocr in the wild. In CVPR, 2016.", "[37] Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor, and R Manmatha. Scatter: selective context attentional scene text recognizer. In CVPR, 2020.", "[40] Yang Liu, Zhaowen Wang, Hailin Jin, and Ian Wassell. Synthetically supervised feature learning for scene text recognition. In ECCV, 2018.", "[51] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. T-PAMI, 2016.", "[14] Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, and Shuigeng Zhou. Focusing attention: Towards accurate text recognition in natural images. In ICCV, 2017.", "[46] Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang. Seed: Semantics enhanced encoder-decoder framework for scene text recognition. In CVPR, 2020.", "[63] Fangneng Zhan and Shijian Lu. Esir: End-to-end scene text recognition via iterative image rectification. In CVPR, 2019.", "[35] Minghui Liao, Jian Zhang, Zhaoyi Wan, Fengming Xie, Jiajun Liang, Pengyuan Lyu, Cong Yao, and Xiang Bai. Scene text recognition from two-dimensional perspective. In AAAI, 2019.", "[39] Wei Liu, Chaofeng Chen, and Kwan-Yee K Wong. Charnet: A character-aware neural network for distorted scene text recognition. In AAAI, 2018.", "[1] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis. In ICCV, 2019.", "[52] Baoguang Shi, Xinggang Wang, Pengyuan Lyu, and Cong Yao. Robust scene text recognition with automatic rectification. In CVPR, 2016."]}]}