{"title": "Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers", "abstract": "Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.", "authors": ["Hila Chefer", " Shir Gur", " Lior Wolf"], "pdf_url": "https://arxiv.org/abs/2103.15679", "list_table_and_caption": [{"table": "<table><tr><td></td><td>Supervised</td><td colspan=\"6\">Weakly supervised segmentation</td></tr><tr><td></td><td rowspan=\"2\">detection</td><td rowspan=\"2\">rollout [1]</td><td>raw</td><td rowspan=\"2\">Grad-CAM [32]</td><td>partial</td><td>Trans.</td><td rowspan=\"2\">Ours</td></tr><tr><td></td><td>attention</td><td>LRP [41]</td><td>attribution [5]</td></tr><tr><td>AP</td><td>51.8</td><td>0.1</td><td>5.6</td><td>2.3</td><td>4.7</td><td>7.2</td><td>13.1 (+5.9)</td></tr><tr><td>\\text{AP}_{medium}</td><td>56.3</td><td>0.1</td><td>9.6</td><td>2.3</td><td>8.0</td><td>10.4</td><td>14.4 (+4.0)</td></tr><tr><td>\\text{AP}_{large}</td><td>67.6</td><td>0.2</td><td>6.9</td><td>4.7</td><td>5.1</td><td>12.4</td><td>24.6 (+12.2)</td></tr><tr><td>AR</td><td>67.4</td><td>0.4</td><td>11.7</td><td>5.5</td><td>10.4</td><td>13.4</td><td>19.3 (+5.9)</td></tr><tr><td>\\text{AR}_{medium}</td><td>72.8</td><td>0.1</td><td>21.8</td><td>5.9</td><td>19.9</td><td>21.0</td><td>23.9 (+2.1)</td></tr><tr><td>\\text{AR}_{large}</td><td>85.1</td><td>0.9</td><td>10.8</td><td>10.7</td><td>8.0</td><td>19.4</td><td>33.2 (+13.8)</td></tr></table>", "caption": "Table 1: DETR [49]-based weakly supervised segmentation results on the MSCOCO [20] validation set, higher is better. AP=average precision, AR=average recall. The subscripts indicate benchmark subsets. The first column is the DETR [49] bounding boxes detection scores obtained for each category, while the rest of the columns are for segmentation maps.", "list_citation_info": ["[41] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797\u20135808, 2019.", "[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928, 2020.", "[49] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}: Deformable transformers for end-to-end object detection. In International Conference on Learning Representations, 2021.", "[5] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. arXiv preprint arXiv:2012.09838, 2020.", "[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.", "[32] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017."]}, {"table": "<table><tr><td></td><td></td><td>rollout</td><td>raw att.</td><td>GCAM</td><td>LRP</td><td>T. Attr</td><td>Ours</td></tr><tr><td rowspan=\"2\">N</td><td>Predicted</td><td>53.10</td><td>45.55</td><td>41.52</td><td>50.49</td><td>54.16</td><td>54.61</td></tr><tr><td>Target</td><td>-</td><td>-</td><td>42.02</td><td>50.49</td><td>55.04</td><td>55.67</td></tr><tr><td rowspan=\"2\">P</td><td>Predicted</td><td>20.05</td><td>23.99</td><td>34.06</td><td>19.64</td><td>17.03</td><td>17.32</td></tr><tr><td>Target</td><td>-</td><td>-</td><td>33.56</td><td>19.64</td><td>16.04</td><td>16.72</td></tr></table>", "caption": "Table 2: ViT [9] positive (P) and negative (N) perturbation AUC results for the predicted and target classes, on the ImageNet [31] validation set. For negative perturbation, larger AUC is better; positive perturbation, smaller AUC is better. GCAM=Grad-CAM; T. Attr = Transformer attribution [5].", "list_citation_info": ["[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.", "[5] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. arXiv preprint arXiv:2012.09838, 2020.", "[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020."]}]}