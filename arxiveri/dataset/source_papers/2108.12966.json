{"title": "Digging into Uncertainty in Self-supervised Multi-View Stereo", "abstract": "Self-supervised Multi-view stereo (MVS) with a pretext task of image reconstruction has achieved significant progress recently. However, previous methods are built upon intuitions, lacking comprehensive explanations about the effectiveness of the pretext task in self-supervised MVS. To this end, we propose to estimate epistemic uncertainty in self-supervised MVS, accounting for what the model ignores. Specially, the limitations can be categorized into two types: ambiguious supervision in foreground and invalid supervision in background. To address these issues, we propose a novel Uncertainty reduction Multi-view Stereo (UMVS) framework for self-supervised learning. To alleviate ambiguous supervision in foreground, we involve extra correspondence prior with a flow-depth consistency loss. The dense 2D correspondence of optical flows is used to regularize the 3D stereo correspondence in MVS. To handle the invalid supervision in background, we use Monte-Carlo Dropout to acquire the uncertainty map and further filter the unreliable supervision signals on invalid regions. Extensive experiments on DTU and Tank&Temples benchmark show that our U-MVS framework achieves the best performance among unsupervised MVS methods, with competitive performance with its supervised opponents.", "authors": ["Hongbin Xu", " Zhipeng Zhou", " Yali Wang", " Wenxiong Kang", " Baigui Sun", " Hao Li", " Yu Qiao"], "pdf_url": "https://arxiv.org/abs/2108.12966", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th>Method</th><th><p>Acc.</p></th><th><p>Comp.</p></th><th><p>Overall</p></th></tr></thead><tbody><tr><th rowspan=\"4\">Geo.</th><th>Furu [10]</th><td><p>0.613</p></td><td><p>0.941</p></td><td><p>0.777</p></td></tr><tr><th>Tola [34]</th><td><p>0.342</p></td><td><p>1.190</p></td><td><p>0.766</p></td></tr><tr><th>Camp [3]</th><td><p>0.835</p></td><td><p>0.554</p></td><td><p>0.694</p></td></tr><tr><th>Gipuma [12]</th><td><p>0.283</p></td><td><p>0.873</p></td><td><p>0.578</p></td></tr><tr><th rowspan=\"9\">Sup.</th><th>Surfacenet [19]</th><td><p>0.450</p></td><td><p>1.040</p></td><td><p>0.745</p></td></tr><tr><th>MVSNet [39]</th><td><p>0.396</p></td><td><p>0.527</p></td><td><p>0.462</p></td></tr><tr><th>CIDER [37]</th><td><p>0.417</p></td><td><p>0.437</p></td><td><p>0.427</p></td></tr><tr><th>P-MVSNet [26]</th><td><p>0.406</p></td><td><p>0.434</p></td><td><p>0.420</p></td></tr><tr><th>R-MVSNet [40]</th><td><p>0.383</p></td><td><p>0.452</p></td><td><p>0.417</p></td></tr><tr><th>Point-MVSNet [5]</th><td><p>0.342</p></td><td><p>0.411</p></td><td><p>0.376</p></td></tr><tr><th>Fast-MVSNet [41]</th><td><p>0.336</p></td><td><p>0.403</p></td><td><p>0.370</p></td></tr><tr><th>CascadeMVSNet [15]</th><td><p>0.325</p></td><td><p>0.385</p></td><td><p>0.355</p></td></tr><tr><th>UCS-Net [6]</th><td><p>0.330</p></td><td><p>0.372</p></td><td><p>0.351</p></td></tr><tr><th></th><th>CVP-MVSNet [38]</th><td><p>0.296</p></td><td><p>0.406</p></td><td><p>0.351</p></td></tr><tr><th></th><th>PatchMatchNet [35]</th><td><p>0.427</p></td><td><p>0.277</p></td><td><p>0.352</p></td></tr><tr><th rowspan=\"8\">UnSup.</th><th>Unsup_MVS [21]</th><td><p>0.881</p></td><td><p>1.073</p></td><td><p>0.977</p></td></tr><tr><th>MVS{}^{2} [7]</th><td><p>0.760</p></td><td><p>0.515</p></td><td><p>0.637</p></td></tr><tr><th>M{}^{3}VSNet [17]</th><td><p>0.636</p></td><td><p>0.531</p></td><td><p>0.583</p></td></tr><tr><th>Meta_MVS [27]</th><td><p>0.594</p></td><td><p>0.779</p></td><td><p>0.687</p></td></tr><tr><th>JDACS[36]</th><td><p>0.571</p></td><td><p>0.515</p></td><td><p>0.543</p></td></tr><tr><th>Ours+MVSNet</th><td><p>0.470</p></td><td><p>0.430</p></td><td><p>0.450</p></td></tr><tr><th>Ours+CascadeMVSNet</th><td><p>0.354</p></td><td><p>0.3535</p></td><td><p>0.3537</p></td></tr></tbody></table>", "caption": "Table 1: Quantitative results on DTU evaluation benchmark. \u201cGeo.\u201d/\u201cSup.\u201d/\u201cUnsup.\u201d are respectively the abbreviation of Geometric/Supervised/Unsupervisedmethods.", "list_citation_info": ["[12] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In Proceedings of the IEEE International Conference on Computer Vision, pages 873\u2013881, 2015.", "[40] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.", "[10] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE transactions on pattern analysis and machine intelligence, 32(8):1362\u20131376, 2009.", "[38] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4877\u20134886, 2020.", "[21] Tejas Khot, Shubham Agrawal, Shubham Tulsiani, Christoph Mertz, Simon Lucey, and Martial Hebert. Learning unsupervised multi-view stereopsis via robust photometric consistency. arXiv preprint arXiv:1905.02706, 2019.", "[15] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.", "[17] Baichuan Huang, Can Huang, Yijia He, Jingbin Liu, and Xiao Liu. M^ 3vsnet: Unsupervised multi-metric multi-view stereo network. arXiv preprint arXiv:2005.00363, 2020.", "[19] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d neural network for multiview stereopsis. In Proceedings of the IEEE International Conference on Computer Vision, pages 2307\u20132315, 2017.", "[3] Neill DF Campbell, George Vogiatzis, Carlos Hern\u00e1ndez, and Roberto Cipolla. Using multiple hypotheses to improve depth-maps for multi-view stereo. In European Conference on Computer Vision, pages 766\u2013779. Springer, 2008.", "[34] Engin Tola, Christoph Strecha, and Pascal Fua. Efficient large-scale multi-view stereo for ultra high-resolution image sets. Machine Vision and Applications, 23(5):903\u2013920, 2012.", "[6] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2524\u20132534, 2020.", "[35] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14194\u201314203, June 2021.", "[27] Arijit Mallick, J\u00f6rg St\u00fcckler, and Hendrik Lensch. Learning to adapt multi-view stereo by self-supervision. In Proceedings of the British Machine Vision Conference (BMVC), 2020. to appear.", "[7] Yuchao Dai, Zhidong Zhu, Zhibo Rao, and Bo Li. Mvs2: Deep unsupervised multi-view stereo with multi-view symmetry. In 2019 International Conference on 3D Vision (3DV), pages 1\u20138. IEEE, 2019.", "[37] Qingshan Xu and Wenbing Tao. Learning inverse depth regression for multi-view stereo with correlation cost volume. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12508\u201312515, 2020.", "[26] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10452\u201310461, 2019.", "[5] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In Proceedings of the IEEE International Conference on Computer Vision, pages 1538\u20131547, 2019.", "[41] Zehao Yu and Shenghua Gao. Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1949\u20131958, 2020.", "[36] Hongbin Xu, Zhipeng Zhou, Yu Qiao, Wenxiong Kang, and Qiuxia Wu. Self-supervised multi-view stereo via effective co-segmentation and data-augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.", "[39] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European Conference on Computer Vision (ECCV), pages 767\u2013783, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Sup.</th><th>Mean</th><th><p>Family</p></th><th><p>Francis</p></th><th><p>Horse</p></th><th>Lighthouse</th><th><p>M60</p></th><th>Panther</th><th>Playground</th><th><p>Train</p></th></tr></thead><tbody><tr><td>OpenMVG [30] + MVE [9]</td><td>-</td><td>38.00</td><td><p>49.91</p></td><td><p>28.19</p></td><td><p>20.75</p></td><td>43.35</td><td><p>44.51</p></td><td>44.76</td><td>36.58</td><td><p>35.95</p></td></tr><tr><td>OpenMVG [30] + OpenMVS [4]</td><td>-</td><td>41.71</td><td><p>58.86</p></td><td><p>32.59</p></td><td><p>26.25</p></td><td>43.12</td><td><p>44.73</p></td><td>46.85</td><td>45.97</td><td><p>35.27</p></td></tr><tr><td>COLMAP [31]</td><td>-</td><td>42.14</td><td><p>50.41</p></td><td><p>22.25</p></td><td><p>25.63</p></td><td>56.43</td><td><p>44.83</p></td><td>46.97</td><td>48.53</td><td><p>42.04</p></td></tr><tr><td>MVSNet [39]</td><td>\\checkmark</td><td>43.48</td><td><p>55.99</p></td><td><p>28.55</p></td><td><p>25.07</p></td><td>50.79</td><td><p>53.96</p></td><td>50.86</td><td>47.90</td><td><p>34.69</p></td></tr><tr><td>CIDER [37]</td><td>\\checkmark</td><td>46.76</td><td><p>56.79</p></td><td><p>32.39</p></td><td><p>29.89</p></td><td>54.67</td><td><p>53.46</p></td><td>53.51</td><td>50.48</td><td><p>42.85</p></td></tr><tr><td>R-MVSNet [40]</td><td>\\checkmark</td><td>48.40</td><td><p>69.96</p></td><td><p>46.65</p></td><td><p>32.59</p></td><td>42.95</td><td><p>51.88</p></td><td>48.80</td><td>52.00</td><td><p>42.38</p></td></tr><tr><td>CVP-MVSNet [38]</td><td>\\checkmark</td><td>54.03</td><td><p>76.50</p></td><td><p>47.74</p></td><td><p>36.34</p></td><td>55.12</td><td>57.28</td><td>54.28</td><td>57.43</td><td><p>47.54</p></td></tr><tr><td>CascadeMVSNet [15]</td><td>\\checkmark</td><td>56.42</td><td><p>76.36</p></td><td><p>58.45</p></td><td><p>46.20</p></td><td>55.53</td><td><p>56.11</p></td><td>54.02</td><td>58.17</td><td><p>46.56</p></td></tr><tr><td>MVS{}^{2} [7]</td><td>\\times</td><td>37.21</td><td><p>47.74</p></td><td><p>21.55</p></td><td><p>19.50</p></td><td>44.54</td><td><p>44.86</p></td><td>46.32</td><td>43.38</td><td><p>29.72</p></td></tr><tr><td>M{}^{3}VSNet [17]</td><td>\\times</td><td>37.67</td><td><p>47.74</p></td><td><p>24.38</p></td><td><p>18.74</p></td><td>44.42</td><td><p>43.45</p></td><td>44.95</td><td>47.39</td><td><p>30.31</p></td></tr><tr><td>JDACS [36]</td><td>\\times</td><td>45.48</td><td><p>66.62</p></td><td><p>38.25</p></td><td><p>36.11</p></td><td>46.12</td><td><p>46.66</p></td><td>45.25</td><td>47.69</td><td><p>37.16</p></td></tr><tr><td>Ours + CascadeMVSNet</td><td>\\times</td><td>57.15</td><td>76.49</td><td>60.04</td><td>49.20</td><td>55.52</td><td><p>55.33</p></td><td>51.22</td><td>56.77</td><td>52.63</td></tr></tbody></table>", "caption": "Table 4: Quantitative results on the intermediate partition of Tanks and Temples benchmark without any finetuning. We present the f-score result of all submissions from the official leaderboard of Tanks and Temples benchmark.", "list_citation_info": ["[36] Hongbin Xu, Zhipeng Zhou, Yu Qiao, Wenxiong Kang, and Qiuxia Wu. Self-supervised multi-view stereo via effective co-segmentation and data-augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.", "[17] Baichuan Huang, Can Huang, Yijia He, Jingbin Liu, and Xiao Liu. M^ 3vsnet: Unsupervised multi-metric multi-view stereo network. arXiv preprint arXiv:2005.00363, 2020.", "[40] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.", "[31] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4104\u20134113, 2016.", "[30] P Moulon, P Monasse, R Marlet, et al. Openmvg. an open multiple view geometry library.(2013).", "[7] Yuchao Dai, Zhidong Zhu, Zhibo Rao, and Bo Li. Mvs2: Deep unsupervised multi-view stereo with multi-view symmetry. In 2019 International Conference on 3D Vision (3DV), pages 1\u20138. IEEE, 2019.", "[38] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4877\u20134886, 2020.", "[37] Qingshan Xu and Wenbing Tao. Learning inverse depth regression for multi-view stereo with correlation cost volume. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12508\u201312515, 2020.", "[15] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.", "[4] Dan Cernea. OpenMVS: Multi-view stereo reconstruction library. 2020.", "[9] Simon Fuhrmann, Fabian Langguth, and Michael Goesele. Mve-a multi-view reconstruction environment. In GCH, pages 11\u201318. Citeseer, 2014.", "[39] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European Conference on Computer Vision (ECCV), pages 767\u2013783, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Sup.</th><th>Mean</th><th>Auditorium</th><th>Ballroom</th><th>Courtroom</th><th>Museum</th><th>Palace</th><th>Temple</th></tr><tr><th>COLMAP [31]</th><th>-</th><th>27.24</th><th>16.02</th><th>25.23</th><th>34.70</th><th>41.51</th><th>18.05</th><th>27.94</th></tr></thead><tbody><tr><th>R-MVSNet [40]</th><th>\\checkmark</th><th>24.91</th><td>12.55</td><td>29.09</td><td>25.06</td><td>38.68</td><td>19.14</td><td>24.96</td></tr><tr><th>CIDER [37]</th><th>\\checkmark</th><th>23.12</th><td>12.77</td><td>24.94</td><td>25.01</td><td>33.64</td><td>19.18</td><td>23.15</td></tr><tr><th>CascadeMVSNet [15]</th><th>\\checkmark</th><th>31.12</th><td>19.81</td><td>38.46</td><td>29.10</td><td>43.87</td><td>27.36</td><td>28.11</td></tr><tr><th>Ours + CascadeMVSNet</th><th>\\times</th><th>30.97</th><td>22.79</td><td>35.39</td><td>28.90</td><td>36.70</td><td>28.77</td><td>33.25</td></tr></tbody></table>", "caption": "Table 5: Quantitative results on the advanced partition of Tanks and Temples benchmark without any finetuning. We present the f-score result of all submissions from the official leaderboard of Tanks and Temples benchmark.", "list_citation_info": ["[15] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.", "[40] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.", "[31] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4104\u20134113, 2016.", "[37] Qingshan Xu and Wenbing Tao. Learning inverse depth regression for multi-view stereo with correlation cost volume. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12508\u201312515, 2020."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">DTU</th><th colspan=\"3\">Intermediate of Tanks&amp;Temples</th><th colspan=\"3\">Advanced of Tanks&amp;Temples</th></tr><tr><th>Sup</th><th>Accuracy</th><th>Completeness</th><th>Overall</th><th>Precision</th><th>Recall</th><th>F-score</th><th>Precision</th><th>Recall</th><th>F-score</th></tr></thead><tbody><tr><th>\\checkmark</th><td>0.325</td><td>0.385</td><td>0.355</td><td>47.62</td><td>74.01</td><td>56.84</td><td>29.68</td><td>35.24</td><td>31.12</td></tr><tr><th>\\times</th><td>0.354</td><td>0.3535</td><td>0.3537</td><td>45.45</td><td>78.52</td><td>57.15</td><td>24.22</td><td>44.46</td><td>30.97</td></tr></tbody></table>", "caption": "Table 8: Performance comparison of our self-supervised method and supervised method on DTU evaluation set, intermediate and advanced partition set of Tanks&amp;Temples. CascadeMVSNet [15] is utilized as the backbone. Under the metrics of DTU benchmark, the smaller the value the better the performance; Under the metrics of Tanks&amp;Temples, the larger the value the better the performance.", "list_citation_info": ["[15] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020."]}]}