{"title": "Rethinking bisenet for real-time semantic segmentation", "abstract": "BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images.", "authors": ["Mingyuan Fan", " Shenqi Lai", " Junshi Huang", " Xiaoming Wei", " Zhenhua Chai", " Junfeng Luo", " Xiaolin Wei"], "pdf_url": "https://arxiv.org/abs/2104.13188", "list_table_and_caption": [{"table": "<table><tr><td> Backbone</td><td>Resolution</td><td>mIoU(%)</td><td>FPS</td></tr><tr><td>GhostNet [8]</td><td>512\\times 1024</td><td>67.8</td><td>135.0</td></tr><tr><td>MobileNetV3 [12]</td><td>512\\times 1024</td><td>70.1</td><td>148.3</td></tr><tr><td>EfficientNet-B0 [26]</td><td>512\\times 1024</td><td>72.2</td><td>99.9</td></tr><tr><td>STDC2</td><td>512\\times 1024</td><td>74.2</td><td>188.6</td></tr><tr><td> GhostNet [8]</td><td>768\\times 1536</td><td>71.3</td><td>60.9</td></tr><tr><td>MobileNetV3 [12]</td><td>768\\times 1536</td><td>73.0</td><td>70.4</td></tr><tr><td>EfficientNet-B0 [26]</td><td>768\\times 1536</td><td>73.9</td><td>45.9</td></tr><tr><td>STDC2</td><td>768\\times 1536</td><td>77.0</td><td>97.0</td></tr><tr><td> </td><td></td><td></td><td></td></tr></table>", "caption": "Table 3: Lightweight backbone comparison on Cityscapes val set.All experients utilize the same decoder and same experiment settings.", "list_citation_info": ["[26] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.", "[12] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE International Conference on Computer Vision, pages 1314\u20131324, 2019.", "[8] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features from cheap operations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1580\u20131589, 2020."]}, {"table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\">SP</td><td colspan=\"3\">DG</td><td rowspan=\"2\">mIoU(%)</td><td rowspan=\"2\">FPS</td></tr><tr><td>4x</td><td>2x</td><td>1x</td></tr><tr><td>BiSeNetV1 [28]</td><td>\u2713</td><td></td><td></td><td></td><td>69.0</td><td>105.8</td></tr><tr><td>STDC2-50</td><td>\u2713</td><td></td><td></td><td></td><td>73.7</td><td>171.6</td></tr><tr><td>STDC2-50</td><td></td><td></td><td></td><td></td><td>73.0</td><td>188.6</td></tr><tr><td>STDC2-50</td><td></td><td>\u2713</td><td></td><td></td><td>73.4</td><td>188.6</td></tr><tr><td>STDC2-50</td><td></td><td></td><td>\u2713</td><td></td><td>73.6</td><td>188.6</td></tr><tr><td>STDC2-50</td><td></td><td></td><td></td><td>\u2713</td><td>73.8</td><td>188.6</td></tr><tr><td>STDC2-50</td><td></td><td></td><td>\u2713</td><td>\u2713</td><td>73.9</td><td>188.6</td></tr><tr><td>STDC2-50</td><td></td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>74.2</td><td>188.6</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 4: Detail information comparison on Cityscapes val set.SP means method with Spatial Pathand DG indicates Detail Guidance, inwhich 1x, 2x, 4x denotes detail features with different down-sample strides in Detail Aggregation module.", "list_citation_info": ["[28] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 325\u2013341, 2018."]}, {"table": "<table><tr><td> Model</td><td>Top1 Acc.</td><td>Params</td><td>FLOPs</td><td>FPS</td></tr><tr><td>ResNet-18 [9]</td><td>69.0%</td><td>11.2M</td><td>1800M</td><td>1058.7</td></tr><tr><td>ResNet-50 [9]</td><td>75.3%</td><td>23.5M</td><td>3800M</td><td>378.7</td></tr><tr><td>DF1 [21]</td><td>69.8%</td><td>8.0M</td><td>746M</td><td>1281.3</td></tr><tr><td>DF2 [21]</td><td>73.9%</td><td>17.5M</td><td>1770M</td><td>713.2</td></tr><tr><td>DenseNet121 [15]</td><td>75.0%</td><td>9.9M</td><td>2882M</td><td>363.6</td></tr><tr><td>DenseNet161 [15]</td><td>76.2%</td><td>28.6M</td><td>7818M</td><td>255.0</td></tr><tr><td>GhostNet(x1.0) [8]</td><td>73.9%</td><td>5.2M</td><td>141M</td><td>699.1</td></tr><tr><td>GhostNet(x1.3) [8]</td><td>75.7%</td><td>7.3M</td><td>226M</td><td>566.2</td></tr><tr><td>MobileNetV2 [25]</td><td>72.0%</td><td>3.4M</td><td>300M</td><td>998.8</td></tr><tr><td>MobileNetV3 [12]</td><td>75.2%</td><td>5.4M</td><td>219M</td><td>661.2</td></tr><tr><td>EfficientNet-B0 [26]</td><td>76.3%</td><td>5.3M</td><td>390M</td><td>443.0</td></tr><tr><td>STDC1</td><td>73.9%</td><td>8.4M</td><td>813M</td><td>1289.0</td></tr><tr><td>STDC2</td><td>76.4%</td><td>12.5M</td><td>1446M</td><td>813.6</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 5: Comparisons with other popular networks on ImageNet Classification.", "list_citation_info": ["[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.", "[26] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019.", "[15] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.", "[12] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE International Conference on Computer Vision, pages 1314\u20131324, 2019.", "[25] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510\u20134520, 2018.", "[21] Xin Li, Yiming Zhou, Zheng Pan, and Jiashi Feng. Partial order pruning: for best speed/accuracy trade-off in neural architecture search. In Proceedings of the IEEE Conference on computer vision and pattern recognition, pages 9145\u20139153, 2019.", "[8] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features from cheap operations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1580\u20131589, 2020."]}, {"table": "<table><tr><td rowspan=\"2\"> Model</td><td rowspan=\"2\">Resolution</td><td rowspan=\"2\">Backbone</td><td colspan=\"2\">mIoU(%)</td><td rowspan=\"2\">FPS</td></tr><tr><td>val</td><td>test</td></tr><tr><td>ENet [24]</td><td>512\\times 1024</td><td>no</td><td>-</td><td>58.3</td><td>76.9</td></tr><tr><td>ICNet [31]</td><td>1024\\times 2048</td><td>PSPNet50</td><td>-</td><td>69.5</td><td>30.3</td></tr><tr><td>DABNet [17]</td><td>1024\\times 2048</td><td>no</td><td>-</td><td>70.1</td><td>27.7</td></tr><tr><td>DFANet B [18]</td><td>1024\\times 1024</td><td>Xception B</td><td>-</td><td>67.1</td><td>120</td></tr><tr><td>DFANet A\u2019 [18]</td><td>512\\times 1024</td><td>Xception A</td><td>-</td><td>70.3</td><td>160</td></tr><tr><td>DFANet A [18]</td><td>1024\\times 1024</td><td>Xception A</td><td>-</td><td>71.3</td><td>100</td></tr><tr><td>BiSeNetV1 [28]</td><td>768\\times 1536</td><td>Xception39</td><td>69.0</td><td>68.4</td><td>105.8</td></tr><tr><td>BiSeNetV1 [28]</td><td>768\\times 1536</td><td>ResNet18</td><td>74.8</td><td>74.7</td><td>65.5</td></tr><tr><td>CAS [30]</td><td>768\\times 1536</td><td>no</td><td>-</td><td>70.5</td><td>108.0</td></tr><tr><td>GAS [22]</td><td>769\\times 1537</td><td>no</td><td>-</td><td>71.8</td><td>108.4</td></tr><tr><td>DF1-Seg-d8 [21]</td><td>1024\\times 2048</td><td>DF1</td><td>72.4</td><td>71.4</td><td>136.9</td></tr><tr><td>DF1-Seg[21]</td><td>1024\\times 2048</td><td>DF1</td><td>74.1</td><td>73.0</td><td>106.4</td></tr><tr><td>DF2-Seg1[21]</td><td>1024\\times 2048</td><td>DF2</td><td>75.9</td><td>74.8</td><td>67.2</td></tr><tr><td>DF2-Seg2[21]</td><td>1024\\times 2048</td><td>DF2</td><td>76.9</td><td>75.3</td><td>56.3</td></tr><tr><td>SFNet [20]</td><td>1024\\times 2048</td><td>DF1</td><td>-</td><td>74.5</td><td>121</td></tr><tr><td>HMSeg [19]</td><td>768\\times 1536</td><td>no</td><td>-</td><td>74.3</td><td>83.2</td></tr><tr><td>TinyHMSeg [19]</td><td>768\\times 1536</td><td>no</td><td>-</td><td>71.4</td><td>172.4</td></tr><tr><td>BiSeNetV2 [27]</td><td>512\\times 1024</td><td>no</td><td>73.4</td><td>72.6</td><td>156</td></tr><tr><td>BiSeNetV2-L [27]</td><td>512\\times 1024</td><td>no</td><td>75.8</td><td>75.3</td><td>47.3</td></tr><tr><td>FasterSeg [4]</td><td>1024\\times 2048</td><td>no</td><td>73.1</td><td>71.5</td><td>163.9</td></tr><tr><td>STDC1-Seg50</td><td>512\\times 1024</td><td>STDC1</td><td>72.2</td><td>71.9</td><td>250.4</td></tr><tr><td>STDC2-Seg50</td><td>512\\times 1024</td><td>STDC2</td><td>74.2</td><td>73.4</td><td>188.6</td></tr><tr><td>STDC1-Seg75</td><td>768\\times 1536</td><td>STDC1</td><td>74.5</td><td>75.3</td><td>126.7</td></tr><tr><td>STDC2-Seg75</td><td>768\\times 1536</td><td>STDC2</td><td>77.0</td><td>76.8</td><td>97.0</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 6: Comparisons with other state-of-the-art methods on Cityscapes. no indicates the method do not have a backbone.", "list_citation_info": ["[17] Gen Li and Joongkyu Kim. Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation. In British Machine Vision Conference, 2019.", "[20] Xiangtai Li, Ansheng You, Zhen Zhu, Houlong Zhao, Maoke Yang, Kuiyuan Yang, and Yunhai Tong. Semantic flow for fast and accurate scene parsing. arXiv preprint arXiv:2002.10120, 2020.", "[4] Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, and Zhangyang Wang. Fasterseg: Searching for faster real-time semantic segmentation. In International Conference on Learning Representations, 2020.", "[27] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, and Nong Sang. Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation. arXiv preprint arXiv:2004.02147, 2020.", "[24] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural network architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147, 2016.", "[28] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 325\u2013341, 2018.", "[19] Peike Li, Xuanyi Dong, Xin Yu, and Yi Yang. a. when humans meet machines: Towards efficient segmentation networks. In Proceedings of the British Machine Vision Conference (BMVC), 2020.", "[22] Peiwen Lin, Peng Sun, Guangliang Cheng, Sirui Xie, Xi Li, and Jianping Shi. Graph-guided architecture search for real-time semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4203\u20134212, 2020.", "[31] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proceedings of the European Conference on Computer Vision (ECCV), pages 405\u2013420, 2018.", "[21] Xin Li, Yiming Zhou, Zheng Pan, and Jiashi Feng. Partial order pruning: for best speed/accuracy trade-off in neural architecture search. In Proceedings of the IEEE Conference on computer vision and pattern recognition, pages 9145\u20139153, 2019.", "[18] Hanchao Li, Pengfei Xiong, Haoqiang Fan, and Jian Sun. Dfanet: Deep feature aggregation for real-time semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9522\u20139531, 2019.", "[30] Yiheng Zhang, Zhaofan Qiu, Jingen Liu, Ting Yao, Dong Liu, and Tao Mei. Customizable architecture search for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11641\u201311650, 2019."]}, {"table": "<table><tr><td> Model</td><td>Resolution</td><td>Backbone</td><td>mIoU(%)</td><td>FPS</td></tr><tr><td>ENet [24]</td><td>720\\times 960</td><td>no</td><td>51.3</td><td>61.2</td></tr><tr><td>ICNet [31]</td><td>720\\times 960</td><td>PSPNet50</td><td>67.1</td><td>34.5</td></tr><tr><td>BiSeNetV1 [28]</td><td>720\\times 960</td><td>Xception39</td><td>65.6</td><td>175</td></tr><tr><td>BiSeNetV1 [28]</td><td>720\\times 960</td><td>ResNet18</td><td>68.7</td><td>116.3</td></tr><tr><td>CAS [30]</td><td>720\\times 960</td><td>no</td><td>71.2</td><td>169</td></tr><tr><td>GAS [22]</td><td>720\\times 960</td><td>no</td><td>72.8</td><td>153.1</td></tr><tr><td>BiSeNetV2 [27]</td><td>720\\times 960</td><td>no</td><td>72.4</td><td>124.5</td></tr><tr><td>BiSeNetV2-L [27]</td><td>720\\times 960</td><td>no</td><td>73.2</td><td>32.7</td></tr><tr><td>STDC1-Seg</td><td>720\\times 960</td><td>STDC1</td><td>73.0</td><td>197.6</td></tr><tr><td>STDC2-Seg</td><td>720\\times 960</td><td>STDC2</td><td>73.9</td><td>152.2</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 7: Comparisons with other state-of-the-art methods on CamVid. no indicates the method do not have a backbone.", "list_citation_info": ["[24] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural network architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147, 2016.", "[28] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 325\u2013341, 2018.", "[27] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, and Nong Sang. Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation. arXiv preprint arXiv:2004.02147, 2020.", "[22] Peiwen Lin, Peng Sun, Guangliang Cheng, Sirui Xie, Xi Li, and Jianping Shi. Graph-guided architecture search for real-time semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4203\u20134212, 2020.", "[31] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In Proceedings of the European Conference on Computer Vision (ECCV), pages 405\u2013420, 2018.", "[30] Yiheng Zhang, Zhaofan Qiu, Jingen Liu, Ting Yao, Dong Liu, and Tao Mei. Customizable architecture search for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11641\u201311650, 2019."]}]}