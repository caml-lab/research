{"title": "Sparse auxiliary networks for unified monocular depth prediction and completion", "abstract": "Estimating scene geometry from data obtained with cost-effective sensors is key for robots and self-driving cars. In this paper, we study the problem of predicting dense depth from a single RGB image (monodepth) with optional sparse measurements from low-cost active depth sensors. We introduce Sparse Auxiliary Networks (SANs), a new module enabling monodepth networks to perform both the tasks of depth prediction and completion, depending on whether only RGB images or also sparse point clouds are available at inference time. First, we decouple the image and depth map encoding stages using sparse convolutions to process only the valid depth map pixels. Second, we inject this information, when available, into the skip connections of the depth prediction network, augmenting its features. Through extensive experimental analysis on one indoor (NYUv2) and two outdoor (KITTI and DDAD) benchmarks, we demonstrate that our proposed SAN architecture is able to simultaneously learn both tasks, while achieving a new state of the art in depth prediction by a significant margin.", "authors": ["Vitor Guizilini", " Rares Ambrus", " Wolfram Burgard", " Adrien Gaidon"], "pdf_url": "https://arxiv.org/abs/2103.16690", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Input</td><td colspan=\"5\">Lower is better \\downarrow</td><td colspan=\"3\">Higher is better \\uparrow</td></tr><tr><td>Abs.Rel</td><td>Sqr.Rel</td><td>RMSE</td><td>RMSE{}_{log}</td><td>SILog</td><td>\\delta&lt;1.25</td><td>\\delta&lt;1.25^{2}</td><td>\\delta&lt;1.25^{3}</td></tr><tr><td>Kuznietsov et al. [28]</td><td>RGB</td><td>0.113</td><td>0.741</td><td>4.621</td><td>0.189</td><td>\u2014</td><td>0.862</td><td>0.960</td><td>0.986</td></tr><tr><td>Gan et al. [10]</td><td>RGB</td><td>0.098</td><td>0.666</td><td>3.933</td><td>0.173</td><td>\u2014</td><td>0.890</td><td>0.964</td><td>0.985</td></tr><tr><td>Guizilini et al.  [20]</td><td>RGB</td><td>0.078</td><td>0.378</td><td>3.330</td><td>0.121</td><td>\u2014</td><td>0.927</td><td>\u2014</td><td>\u2014</td></tr><tr><td>Fu et al. [9]</td><td>RGB</td><td>0.072</td><td>0.307</td><td>2.727</td><td>0.120</td><td>\u2014</td><td>0.932</td><td>0.984</td><td>0.994</td></tr><tr><td>Yin et al. [53]</td><td>RGB</td><td>0.072</td><td>\u2014</td><td>3.258</td><td>0.117</td><td>\u2014</td><td>0.938</td><td>0.990</td><td>0.998</td></tr><tr><td>Lee et al.  [30]</td><td>RGB</td><td>0.059</td><td>0.245</td><td>2.756</td><td>0.096</td><td>\u2014</td><td>0.956</td><td>0.993</td><td>0.998</td></tr><tr><td rowspan=\"2\">BTS-SAN</td><td>RGB</td><td>0.057</td><td>0.229</td><td>2.704</td><td>0.092</td><td>8.926</td><td>0.961</td><td>0.994</td><td>0.999</td></tr><tr><td>RGB+D</td><td>0.021</td><td>0.038</td><td>1.094</td><td>0.037</td><td>3.749</td><td>0.996</td><td>0.999</td><td>1.000</td></tr><tr><td rowspan=\"2\">PackNet-SAN</td><td>RGB</td><td>0.052</td><td>0.175</td><td>2.233</td><td>0.083</td><td>7.618</td><td>0.970</td><td>0.996</td><td>0.999</td></tr><tr><td>RGB+D</td><td>0.015</td><td>0.028</td><td>0.909</td><td>0.032</td><td>3.149</td><td>0.997</td><td>0.999</td><td>1.000</td></tr><tr><td>Improvement</td><td>RGB</td><td>11.9%</td><td>28.5%</td><td>18.9%</td><td>13.5%</td><td>\u2014</td><td>1.4%</td><td>0.0%</td><td>0.0%</td></tr></tbody></table>", "caption": "Table 1: Depth estimation results on the KITTI dataset, for the Eigen test split [7] and distances up to 80m. The Improvements row indicate the percentual improvement between our best model (PackNet-SAN) and the current state of the art (BTS, by Lee et al. [30], underlined).", "list_citation_info": ["[30] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[20] Vitor Guizilini, Jie Li, Rares Ambrus, Sudeep Pillai, and Adrien Gaidon. Robust semi-supervised monocular depth estimation with reprojected distances. In Conference on Robot Learning (CoRL), October 2019.", "[9] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[10] Yukang Gan, Xiangyu Xu, Wenxiu Sun, and Liang Lin. Monocular depth estimation with affinity, vertical pooling, and label enhancement. In ECCV, 2018.", "[7] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366\u20132374, 2014.", "[28] Yevhen Kuznietsov, J\u00f6rg St\u00fcckler, and Bastian Leibe. Semi-supervised deep learning for monocular depth map prediction. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6647\u20136655, 2017.", "[53] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In The IEEE International Conference on Computer Vision (ICCV), 2019."]}, {"table": "<table><tbody><tr><th rowspan=\"11\"><p>Prediction</p></th><th>Method</th><th>SILog</th><td>SqRel</td><td>AbsRel</td><td>iRMSE</td></tr><tr><th>SGDepth [26]</th><th>15.30</th><td>5.00%</td><td>13.29%</td><td>15.80</td></tr><tr><th>SDNet [35]</th><th>14.68</th><td>3.90%</td><td>12.31%</td><td>15.96</td></tr><tr><th>VGG26-UNet [21]</th><th>13.41</th><td>2.86%</td><td>10.60%</td><td>15.06</td></tr><tr><th>PAP [56]</th><th>13.08</th><td>2.72%</td><td>10.27%</td><td>13.95</td></tr><tr><th>VNL [50]</th><th>12.65</th><td>2.46%</td><td>10.15%</td><td>13.02</td></tr><tr><th>SORD [6]</th><th>12.39</th><td>2.49%</td><td>10.10%</td><td>13.48</td></tr><tr><th>RefinedMPL [48]</th><th>11.80</th><td>2.31%</td><td>10.09%</td><td>13.39</td></tr><tr><th>DORN [9]</th><th>11.77</th><td>2.23%</td><td>8.78%</td><td>12.98</td></tr><tr><th>BTS [30]</th><th>11.67</th><td>2.21%</td><td>9.04%</td><td>12.23</td></tr><tr><th>PackNet-SAN</th><th>11.54</th><td>2.35%</td><td>9.12%</td><td>12.38</td></tr><tr><th rowspan=\"12\"><p>Completion</p></th><th>Method</th><th>RMSE</th><td>iRMSE</td><td>MAE</td><td>iMAE</td></tr><tr><th>DCDC [24]</th><th>1109.04</th><td>2.95</td><td>234.01</td><td>1.07</td></tr><tr><th>CSPN [3]</th><th>1019.64</th><td>2.93</td><td>279.46</td><td>1.15</td></tr><tr><th>Conf-Net [22]</th><th>962.28</th><td>3.10</td><td>257.54</td><td>1.09</td></tr><tr><th>Sparse-to-Dense [33]</th><th>954.36</th><td>3.21</td><td>288.64</td><td>1.35</td></tr><tr><th>DFineNet [55]</th><th>943.89</th><td>1.39</td><td>304.17</td><td>1.39</td></tr><tr><th>CrossGuidance [31]</th><th>807.42</th><td>2.73</td><td>253.98</td><td>1.33</td></tr><tr><th>FusionNet [47]</th><th>772.87</th><td>2.19</td><td>215.02</td><td>0.93</td></tr><tr><th>GuideNet [42]</th><th>736.24</th><td>2.25</td><td>218.83</td><td>0.99</td></tr><tr><th>PackNet-SAN</th><th>914.35</th><td>2.78</td><td>298.04</td><td>1.36</td></tr></tbody></table>", "caption": "Table 2: Depth estimation results on the official KITTI testset benchmark relative to other published methods, for both prediction and completion tasks (bold metrics are used for leaderboard scoring). Note that the same model was used in both submissions, only modifying the input information (RGB for prediction and RGB+D for completion).", "list_citation_info": ["[3] Xinjing Cheng, Peng Wang, and Ruigang Yang. Depth estimation via affinity learned with convolutional spatial propagation network. In Proceedings of the European Conference on Computer Vision (ECCV), pages 103\u2013119, 2018.", "[6] Raul Diaz and Amit Marathe. Soft labels for ordinal regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4738\u20134747, 2019.", "[22] Hamid Hekmatian, Jingfu Jin, and Samir Al-Stouhi. Conf-net: Toward high-confidence dense 3d point-cloud with error-map prediction. arXiv preprint arXiv:1907.10148, 2019.", "[47] Wouter Van Gansbeke, Davy Neven, Bert De Brabandere, and Luc Van Gool. Sparse and noisy lidar completion with rgb guidance and uncertainty. In 2019 16th International Conference on Machine Vision Applications (MVA), pages 1\u20136. IEEE, 2019.", "[56] Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, and Jian Yang. Pattern-affinitive propagation across depth, surface normal and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4106\u20134115, 2019.", "[30] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[48] Jean Marie Uwabeza Vianney, Shubhra Aich, and Bingbing Liu. Refinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving. arXiv preprint arXiv:1911.09712, 2019.", "[50] Yin Wei, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. arXiv preprint arXiv:1907.12209, 2019.", "[42] Jie Tang, Fei-Peng Tian, Wei Feng, Jian Li, and Ping Tan. Learning guided convolutional network for depth completion. arXiv preprint arXiv:1908.01238, 2019.", "[9] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[35] Matthias Ochs, Adrian Kretz, and Rudolf Mester. Sdnet: Semantically guided depth estimation network. In German Conference on Pattern Recognition, pages 288\u2013302. Springer, 2019.", "[31] Sihaeng Lee, Janghyeon Lee, Doyeon Kim, and Junmo Kim. Deep architecture with cross guidance between single image and sparse lidar data for depth completion. IEEE Access, 8:79801\u201379810, 2020.", "[21] Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy Ren, and Xiaogang Wang. Learning monocular depth by distilling cross-domain stereo networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 484\u2013500, 2018.", "[24] Saif Imran, Yunfei Long, Xiaoming Liu, and Daniel Morris. Depth coefficients for depth completion. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12438\u201312447. IEEE, 2019.", "[26] Marvin Klingner, Jan-Aike Term\u00f6hlen, Jonas Mikolajczyk, and Tim Fingscheidt. Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance. arXiv preprint arXiv:2007.06936, 2020.", "[55] Yilun Zhang, Ty Nguyen, Ian D Miller, Shreyas S Shivakumar, Steven Chen, Camillo J Taylor, and Vijay Kumar. Dfinenet: Ego-motion estimation and depth refinement from sparse, noisy depth input with rgb guidance. arXiv preprint arXiv:1903.06397, 2019.", "[33] Fangchang Ma, Guilherme Venturelli Cavalheiro, and Sertac Karaman. Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera. arXiv preprint arXiv:1807.00275, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Input</th><th>Abs.Rel\\downarrow</th><th>RMSE\\downarrow</th><th>SILog\\downarrow</th><th>\\delta&lt;1.25\\uparrow</th></tr></thead><tbody><tr><th rowspan=\"2\">SRB x1</th><th>RGB</th><td>0.057</td><td>2.483</td><td>8.064</td><td>0.966</td></tr><tr><th>RGB+D</th><td>0.019</td><td>0.994</td><td>3.343</td><td>0.997</td></tr><tr><th rowspan=\"2\">SRB x2</th><th>RGB</th><td>0.055</td><td>2.328</td><td>7.862</td><td>0.966</td></tr><tr><th>RGB+D</th><td>0.017</td><td>0.949</td><td>3.287</td><td>0.997</td></tr><tr><th>Unfreeze</th><th>RGB</th><td>0.055</td><td>2.306</td><td>7.978</td><td>0.967</td></tr><tr><th>Pred. Encoder</th><th>RGB+D</th><td>0.021</td><td>0.965</td><td>3.333</td><td>0.996</td></tr><tr><th>Freeze</th><th>RGB</th><td>0.054</td><td>2.318</td><td>7.901</td><td>0.968</td></tr><tr><th>Pred. Decoder</th><th>RGB+D</th><td>0.024</td><td>1.070</td><td>3.805</td><td>0.995</td></tr><tr><th>W/o W_{i} and</th><th>RGB</th><td>0.056</td><td>2.374</td><td>8.324</td><td>0.962</td></tr><tr><th>B_{i} parameters</th><th>RGB+D</th><td>0.019</td><td>0.958</td><td>3.395</td><td>0.995</td></tr><tr><th>Train from</th><th>RGB</th><td>0.062</td><td>2.888</td><td>9.579</td><td>0.955</td></tr><tr><th>Scratch</th><th>RGB+D</th><td>0.019</td><td>1.049</td><td>3.631</td><td>0.996</td></tr><tr><th>Prediction</th><th>RGB</th><td>0.054</td><td>2.476</td><td>8.081</td><td>0.966</td></tr><tr><th>Completion</th><th>RGB+D</th><td>0.015</td><td>0.878</td><td>3.238</td><td>0.997</td></tr><tr><th rowspan=\"2\">PackNet-SAN</th><th>RGB</th><td>0.052</td><td>2.233</td><td>7.618</td><td>0.970</td></tr><tr><th>RGB+D</th><td>0.015</td><td>0.909</td><td>3.149</td><td>0.997</td></tr></tbody></table>", "caption": "Table 3: Ablation analysis on the KITTI dataset, considering the Eigen test split [7] and PackNet [18] as the depth prediction network. SRB xX uses Sparse Residual Blocks with fewer branches; Unfreeze Pred. Encoder also updates the prediction encoder during the second stage of training; Freeze Pred. Decoder also freezes the prediction decoder during the second stage of training; w/o W_{i} and B_{i} removes the shared parameters for each skip connection; Train from scratch does not use a pre-trained model; and Prediction and Completion are trained only for that particular task.", "list_citation_info": ["[7] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366\u20132374, 2014.", "[18] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>AP3D@easy</th><th>AP3D@medium</th><th>AP3D@hard</th></tr></thead><tbody><tr><th>DORN [9]</th><td>34.8/35.1</td><td>22.0/22.0</td><td>19.5/19.6</td></tr><tr><th>PackNet-SAN</th><td>35.5/35.7</td><td>22.6/22.8</td><td>19.9/20.1</td></tr></tbody></table>", "caption": "Table 4: 3D object detection results on the validation set of KITI3D for the Car category, using PatchNet [34] and different monocular pointclouds (no input sparse depth), for the validation split. The same detection architecture and learning hyperparameters were used in both cases.", "list_citation_info": ["[9] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[34] Xinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang. Rethinking pseudo-lidar representation. In Proceedings of the European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><tbody><tr><td colspan=\"6\">Depth Prediction</td></tr><tr><td>Method</td><td>AbsRel</td><td>RMSE</td><td>\\delta&lt;1.25</td><td>\\delta&lt;1.25^{2}</td><td>\\delta&lt;1.25^{3}</td></tr><tr><td>Qi et al. [38]</td><td>0.128</td><td>0.569</td><td>0.834</td><td>0.960</td><td>0.990</td></tr><tr><td>Alhashim et al. [1]</td><td>0.123</td><td>0.465</td><td>0.846</td><td>0.974</td><td>0.994</td></tr><tr><td>Fu et al. [9]</td><td>0.115</td><td>0.509</td><td>0.828</td><td>0.965</td><td>0.992</td></tr><tr><td>Yin et al. [53]</td><td>0.108</td><td>0.416</td><td>0.875</td><td>0.976</td><td>0.994</td></tr><tr><td>Lee et al. [30]</td><td>0.110</td><td>0.392</td><td>0.885</td><td>0.978</td><td>0.994</td></tr><tr><td>PackNet [18]</td><td>0.110</td><td>0.397</td><td>0.886</td><td>0.979</td><td>0.995</td></tr><tr><td>PackNet-SAN</td><td>0.106</td><td>0.393</td><td>0.892</td><td>0.979</td><td>0.995</td></tr><tr><td colspan=\"6\">Depth Completion - 200 samples</td></tr><tr><td>Ma et al. [33]\\dagger</td><td>0.044</td><td>0.230</td><td>0.971</td><td>0.994</td><td>0.998</td></tr><tr><td>NConv-CNN [8]\\dagger</td><td>0.027</td><td>0.173</td><td>0.982</td><td>0.996</td><td>0.999</td></tr><tr><td>Tang et al. [42]</td><td>0.024</td><td>0.142</td><td>0.988</td><td>0.998</td><td>1.000</td></tr><tr><td>PackNet-SAN</td><td>0.027</td><td>0.155</td><td>0.989</td><td>0.998</td><td>0.999</td></tr><tr><td colspan=\"6\">Depth Completion - 500 samples</td></tr><tr><td>Ma et al. [33]</td><td>0.043</td><td>0.204</td><td>0.978</td><td>0.996</td><td>0.999</td></tr><tr><td>DeepLidar [39]</td><td>0.022</td><td>0.115</td><td>0.993</td><td>0.999</td><td>1.000</td></tr><tr><td>EncDec-Net[EF] [8]</td><td>0.017</td><td>0.123</td><td>0.991</td><td>0.998</td><td>1.000</td></tr><tr><td>CSPN [3]</td><td>0.016</td><td>0.117</td><td>0.992</td><td>0.999</td><td>1.000</td></tr><tr><td>Tang et al. [42]</td><td>0.015</td><td>0.101</td><td>0.995</td><td>0.999</td><td>1.000</td></tr><tr><td>PackNet-SAN</td><td>0.019</td><td>0.120</td><td>0.994</td><td>0.999</td><td>1.000</td></tr></tbody></table>", "caption": "Table 6: Depth estimation results on the test split of the NYUv2 dataset. relative to other published methods, for both depth prediction and completion tasks. Note that the same model was used in both submissions, the only modification being the input information (RGB for prediction and RGB+D for completion). \\dagger - results from [42].", "list_citation_info": ["[3] Xinjing Cheng, Peng Wang, and Ruigang Yang. Depth estimation via affinity learned with convolutional spatial propagation network. In Proceedings of the European Conference on Computer Vision (ECCV), pages 103\u2013119, 2018.", "[30] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[18] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[42] Jie Tang, Fei-Peng Tian, Wei Feng, Jian Li, and Ping Tan. Learning guided convolutional network for depth completion. arXiv preprint arXiv:1908.01238, 2019.", "[9] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[39] Jiaxiong Qiu, Zhaopeng Cui, Yinda Zhang, Xingdi Zhang, Shuaicheng Liu, Bing Zeng, and Marc Pollefeys. Deeplidar: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3313\u20133322, 2019.", "[1] Ibraheem Alhashim and Peter Wonka. High quality monocular depth estimation via transfer learning. arXiv preprint arXiv:1812.11941, 2018.", "[8] Abdelrahman Eldesokey, Michael Felsberg, and Fahad Khan. Confidence propagation through cnns for guided sparse depth regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP:1\u20131, 07 2019.", "[53] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In The IEEE International Conference on Computer Vision (ICCV), 2019.", "[38] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia. Geonet: Geometric neural network for joint depth and surface normal estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 283\u2013291, 2018.", "[33] Fangchang Ma, Guilherme Venturelli Cavalheiro, and Sertac Karaman. Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera. arXiv preprint arXiv:1807.00275, 2018."]}]}