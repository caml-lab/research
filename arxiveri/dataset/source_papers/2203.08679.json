{"title": "Decoupled knowledge distillation", "abstract": "State-of-the-art distillation methods are mainly based on distilling deep features from intermediate layers, while the significance of logit distillation is greatly overlooked. To provide a novel viewpoint to study logit distillation, we reformulate the classical KD loss into two parts, i.e., target class knowledge distillation (TCKD) and non-target class knowledge distillation (NCKD). We empirically investigate and prove the effects of the two parts: TCKD transfers knowledge concerning the \"difficulty\" of training samples, while NCKD is the prominent reason why logit distillation works. More importantly, we reveal that the classical KD loss is a coupled formulation, which (1) suppresses the effectiveness of NCKD and (2) limits the flexibility to balance these two parts. To address these issues, we present Decoupled Knowledge Distillation (DKD), enabling TCKD and NCKD to play their roles more efficiently and flexibly. Compared with complex feature-based methods, our DKD achieves comparable or even better results and has better training efficiency on CIFAR-100, ImageNet, and MS-COCO datasets for image classification and object detection tasks. This paper proves the great potential of logit distillation, and we hope it will be helpful for future research. The code is available at https://github.com/megvii-research/mdistiller.", "authors": ["Borui Zhao", " Quan Cui", " Renjie Song", " Yiyu Qiu", " Jiajun Liang"], "pdf_url": "https://arxiv.org/abs/2203.08679", "list_table_and_caption": [{"table": "<table><tr><td>student</td><td>TCKD</td><td>top-1</td><td>\\Delta</td></tr><tr><td rowspan=\"2\"> ResNet8\\times4</td><td></td><td>73.82</td><td>-</td></tr><tr><td>\u2713</td><td>75.33</td><td>+1.51</td></tr><tr><td rowspan=\"2\">ShuffleNet-V1</td><td></td><td>77.13</td><td>-</td></tr><tr><td>\u2713</td><td>77.98</td><td>+0.85</td></tr></table>", "caption": "Table 2: Accuracy(%) on the CIFAR-100 validation. We set ResNet32\\times4 as the teacher and ResNet8\\times4 as the student. Both teachers and students are trained with AutoAugment[5].", "list_citation_info": ["[5] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In CVPR, 2019."]}, {"table": "<table><tr><td rowspan=\"4\">distillationmanner</td><td rowspan=\"2\">teacher</td><td>ResNet56</td><td>ResNet110</td><td>ResNet32\\times4</td><td>WRN-40-2</td><td>WRN-40-2</td><td>VGG13</td></tr><tr><td>72.34</td><td>74.31</td><td>79.42</td><td>75.61</td><td>75.61</td><td>74.64</td></tr><tr><td rowspan=\"2\">student</td><td>ResNet20</td><td>ResNet32</td><td>ResNet8\\times4</td><td>WRN-16-2</td><td>WRN-40-1</td><td>VGG8</td></tr><tr><td>69.06</td><td>71.14</td><td>72.50</td><td>73.26</td><td>71.98</td><td>70.36</td></tr><tr><td rowspan=\"5\"> features</td><td>FitNet[28]</td><td>69.21</td><td>71.06</td><td>73.50</td><td>73.58</td><td>72.24</td><td>71.02</td></tr><tr><td>RKD[23]</td><td>69.61</td><td>71.82</td><td>71.90</td><td>73.35</td><td>72.22</td><td>71.48</td></tr><tr><td>CRD[33]</td><td>71.16</td><td>73.48</td><td>75.51</td><td>75.48</td><td>74.14</td><td>73.94</td></tr><tr><td>OFD[10]</td><td>70.98</td><td>73.23</td><td>74.95</td><td>75.24</td><td>74.33</td><td>73.95</td></tr><tr><td>ReviewKD[1]</td><td>71.89</td><td>73.89</td><td>75.63</td><td>76.12</td><td>75.09</td><td>74.84</td></tr><tr><td rowspan=\"3\">logits</td><td>KD[12]</td><td>70.66</td><td>73.08</td><td>73.33</td><td>74.92</td><td>73.54</td><td>72.98</td></tr><tr><td>DKD</td><td>71.97</td><td>74.11</td><td>76.32</td><td>76.24</td><td>74.81</td><td>74.68</td></tr><tr><td>\\Delta</td><td>+1.31</td><td>+1.03</td><td>+2.99</td><td>+1.32</td><td>+1.27</td><td>+1.70</td></tr></table>", "caption": "Table 6: Results on the CIFAR-100 validation. Teachers and students are in the same architectures. And \\Delta represents the performance improvement over the classical KD. All results are the average over 5 trials.", "list_citation_info": ["[10] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. In ICCV, 2019.", "[23] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In CVPR, 2019.", "[28] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. ICLR, 2015.", "[33] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In ICLR, 2020.", "[1] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In CVPR, 2021.", "[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In arXiv:1503.02531, 2015."]}, {"table": "<table><tr><td rowspan=\"4\">distillationmanner</td><td rowspan=\"2\">teacher</td><td>ResNet32\\times4</td><td>WRN-40-2</td><td>VGG13</td><td>ResNet50</td><td>ResNet32\\times4</td></tr><tr><td>79.42</td><td>75.61</td><td>74.64</td><td>79.34</td><td>79.42</td></tr><tr><td rowspan=\"2\">student</td><td>ShuffleNet-V1</td><td>ShuffleNet-V1</td><td>MobileNet-V2</td><td>MobileNet-V2</td><td>ShuffleNet-V2</td></tr><tr><td>70.50</td><td>70.50</td><td>64.60</td><td>64.60</td><td>71.82</td></tr><tr><td rowspan=\"5\"> features</td><td>FitNet[28]</td><td>73.59</td><td>73.73</td><td>64.14</td><td>63.16</td><td>73.54</td></tr><tr><td>RKD[23]</td><td>72.28</td><td>72.21</td><td>64.52</td><td>64.43</td><td>73.21</td></tr><tr><td>CRD[33]</td><td>75.11</td><td>76.05</td><td>69.73</td><td>69.11</td><td>75.65</td></tr><tr><td>OFD[10]</td><td>75.98</td><td>75.85</td><td>69.48</td><td>69.04</td><td>76.82</td></tr><tr><td>ReviewKD[1]</td><td>77.45</td><td>77.14</td><td>70.37</td><td>69.89</td><td>77.78</td></tr><tr><td rowspan=\"3\">logits</td><td>KD[12]</td><td>74.07</td><td>74.83</td><td>67.37</td><td>67.35</td><td>74.45</td></tr><tr><td>DKD</td><td>76.45</td><td>76.70</td><td>69.71</td><td>70.35</td><td>77.07</td></tr><tr><td>\\Delta</td><td>+2.38</td><td>+1.87</td><td>+2.34</td><td>+3.00</td><td>+2.62</td></tr></table>", "caption": "Table 7: Results on the CIFAR-100 validation. Teachers and students are in different architectures. And \\Delta represents the performance improvement over the classical KD. All results are the average over 5 trials.", "list_citation_info": ["[10] Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. In ICCV, 2019.", "[23] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In CVPR, 2019.", "[28] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. ICLR, 2015.", "[33] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In ICLR, 2020.", "[1] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In CVPR, 2021.", "[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In arXiv:1503.02531, 2015."]}, {"table": "<table><tr><td></td><td colspan=\"3\">R-101 &amp; R-18</td><td colspan=\"3\">R-101 &amp; R-50</td><td colspan=\"3\">R-50 &amp; MV2</td></tr><tr><td></td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><td> teacher</td><td>42.04</td><td>62.48</td><td>45.88</td><td>42.04</td><td>62.48</td><td>45.88</td><td>40.22</td><td>61.02</td><td>43.81</td></tr><tr><td>student</td><td>33.26</td><td>53.61</td><td>35.26</td><td>37.93</td><td>58.84</td><td>41.05</td><td>29.47</td><td>48.87</td><td>30.90</td></tr><tr><td>KD[12]</td><td>33.97</td><td>54.66</td><td>36.62</td><td>38.35</td><td>59.41</td><td>41.71</td><td>30.13</td><td>50.28</td><td>31.35</td></tr><tr><td>FitNet[28]</td><td>34.13</td><td>54.16</td><td>36.71</td><td>38.76</td><td>59.62</td><td>41.80</td><td>30.20</td><td>49.80</td><td>31.69</td></tr><tr><td>FGFI[38]</td><td>35.44</td><td>55.51</td><td>38.17</td><td>39.44</td><td>60.27</td><td>43.04</td><td>31.16</td><td>50.68</td><td>32.92</td></tr><tr><td>ReviewKD[1]</td><td>36.75</td><td>56.72</td><td>34.00</td><td>40.36</td><td>60.97</td><td>44.08</td><td>33.71</td><td>53.15</td><td>36.13</td></tr><tr><td>DKD</td><td>35.05</td><td>56.60</td><td>37.54</td><td>39.25</td><td>60.90</td><td>42.73</td><td>32.34</td><td>53.77</td><td>34.01</td></tr><tr><td>DKD+ReviewKD</td><td>37.01</td><td>57.53</td><td>39.85</td><td>40.65</td><td>61.51</td><td>44.44</td><td>34.35</td><td>54.89</td><td>36.61</td></tr></table>", "caption": "Table 10: Results on MS-COCO based on Faster-RCNN[27]-FPN[19]: AP evaluated on val2017. Teacher-student pairs are ResNet-101 (R-101) &amp; ResNet-18 (R-18), ResNet-101 &amp; ResNet-50 (R-50) and ResNet-50 &amp; MobileNet-V2 (MV2) respectively. All results are the average over 3 trials. More details are attached in supplement.", "list_citation_info": ["[38] Tao Wang, Li Yuan, Xiaopeng Zhang, and Jiashi Feng. Distilling object detectors with fine-grained feature imitation. In CVPR, 2019.", "[19] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017.", "[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.", "[28] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. ICLR, 2015.", "[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In arXiv:1503.02531, 2015.", "[1] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling knowledge via knowledge review. In CVPR, 2021."]}, {"table": "<table><tr><td>\\beta</td><td>ResNet56</td><td>WRN-40-2</td><td>ResNet32x4</td></tr><tr><td> 1.0</td><td>76.02</td><td>75.94</td><td>74.95</td></tr><tr><td>2.0</td><td>76.32</td><td>76.25</td><td>75.64</td></tr><tr><td>4.0</td><td>75.91</td><td>76.17</td><td>75.82</td></tr><tr><td>6.0</td><td>75.62</td><td>76.70</td><td>76.34</td></tr><tr><td>8.0</td><td>75.33</td><td>76.44</td><td>76.45</td></tr><tr><td>10.0</td><td>75.35</td><td>76.21</td><td>76.32</td></tr><tr><td>z_{t}-z_{max}</td><td>5.36</td><td>7.24</td><td>8.40</td></tr></table>", "caption": "Table A.1: Accuracy(%) on CIFAR-100[16] with different \\beta and different teachers. The gap (z_{t}-z_{max}) is also reported.", "list_citation_info": ["[16] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009."]}]}