{"title": "IBRNet: Learning Multi-View Image-Based Rendering", "abstract": "We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods. Project page: https://ibrnet.github.io/", "authors": ["Qianqian Wang", " Zhicheng Wang", " Kyle Genova", " Pratul Srinivasan", " Howard Zhou", " Jonathan T. Barron", " Ricardo Martin-Brualla", " Noah Snavely", " Thomas Funkhouser"], "pdf_url": "https://arxiv.org/abs/2102.13090", "list_table_and_caption": [{"table": "<table><tr><td></td><td></td><td colspan=\"3\">Diffuse Synthetic 360^{\\circ} [59]</td><td colspan=\"3\">Realistic Synthetic 360^{\\circ} [42]</td><td colspan=\"3\">Real Forward-Facing [41]</td></tr><tr><td>Method</td><td>Settings</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td></tr><tr><td>LLFF [41]</td><td rowspan=\"2\"> No per-sceneoptimization </td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr><tr><td>Ours</td><td>\\mathbf{37.17}</td><td>\\mathbf{0.990}</td><td>\\mathbf{0.017}</td><td>\\mathbf{25.49}</td><td>\\mathbf{0.916}</td><td>\\mathbf{0.100}</td><td>\\mathbf{25.13}</td><td>\\mathbf{0.817}</td><td>\\mathbf{0.205}</td></tr><tr><td>SRN [60]</td><td rowspan=\"2\"> Per-sceneoptimization </td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr><tr><td>NV [36]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NeRF [42]</td><td></td><td>40.15</td><td>0.991</td><td>0.023</td><td>\\mathbf{31.01}</td><td>\\mathbf{0.947}</td><td>0.081</td><td>26.50</td><td>0.811</td><td>0.250</td></tr><tr><td>Ours{}_{\\text{ft}}</td><td></td><td>\\mathbf{42.93}</td><td>\\mathbf{0.997}</td><td>\\mathbf{0.009}</td><td>28.14</td><td>0.942</td><td>\\mathbf{0.072}</td><td>\\mathbf{26.73}</td><td>\\mathbf{0.851}</td><td>\\mathbf{0.175}</td></tr></table>", "caption": "Table 1: Quantitative comparison on datasets of synthetic and real images. Our evaluation metrics are PSNR/SSIM (higher is better) and LPIPS [73] (lower is better).Both Ours and LLFF [41] are trained on large amounts of training data and then evaluated on all test scenes without any per-scene tuning. Ours consistently outperforms LLFF [41] on all datasets. We also compare our method with neural rendering methods (SRN [60], NV [36], and NeRF [42]) that train a separate network for each scene. To compete fairly with these methods, we also fine-tune our pretrained model on each scene (Ours{}_{\\text{ft}}). After fine-tuning, Ours{}_{\\text{ft}} is competitive with the state-of-the-art method NeRF [42].", "list_citation_info": ["[36] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. ACM TOG, 2019.", "[42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020.", "[41] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima K. Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM TOG, 2019.", "[59] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, and Michael Zollhofer. Deepvoxels: Learning persistent 3d feature embeddings. CVPR, 2019.", "[73] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. CVPR, 2018.", "[60] Vincent Sitzmann, Michael Zollhoefer, and Gordon Wetzstein. Scene representation networks: Continuous 3D-structure-aware neural scene representations. NeurIPS, 2019."]}, {"table": "<table><tr><td></td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td></tr><tr><td>No ray transformer</td><td>21.31</td><td>0.675</td><td>0.355</td></tr><tr><td>No view directions</td><td>24.20</td><td>0.796</td><td>0.243</td></tr><tr><td>Direct color regression</td><td>24.73</td><td>0.810</td><td>0.220</td></tr><tr><td>Full model Ours</td><td>\\mathbf{25.13}</td><td>\\mathbf{0.817}</td><td>\\mathbf{0.205}</td></tr></table>", "caption": "Table 2: Ablation study on Real Forward-Facing [41] data. We report the metrics for the pretrained model of each ablation without per-scene fine-tuning.", "list_citation_info": ["[41] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima K. Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM TOG, 2019."]}, {"table": "<table><tr><td>Method</td><td>#Params</td><td>#Src.Views</td><td>#FLOPs</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td></tr><tr><td>SRN</td><td>0.55M</td><td>-</td><td>5M</td><td>22.84</td><td>0.668</td><td>0.378</td></tr><tr><td>NeRF</td><td>1.19M</td><td>-</td><td>304M</td><td>26.50</td><td>0.811</td><td>0.250</td></tr><tr><td rowspan=\"3\">Ours{}_{\\text{ft}}</td><td rowspan=\"3\">0.04M</td><td>5</td><td>29M</td><td>25.80</td><td>0.828</td><td>0.190</td></tr><tr><td>8</td><td>45M</td><td>26.56</td><td>0.847</td><td>0.176</td></tr><tr><td>10</td><td>55M</td><td>\\mathbf{26.73}</td><td>\\mathbf{0.851}</td><td>\\mathbf{0.175}</td></tr></table>", "caption": "Table 3: Network size and computational cost at inference. The network size of our MLP is much smaller than SRN [60] and NeRF [42]. All #FLOPs reported are for rendering a single pixel. Both NeRF [42] and Ours{}_{\\text{ft}} use hierarchical volume sampling. M_{c}, M_{f} are set to 64,128 respectively for NeRF, and 64,64 for Ours{}_{\\text{ft}}. SRN [60] uses ray marching with only 10 steps thus having much fewer #FLOPs.For our method, the number of #FLOPs scales roughly linearly with the number of source views (#Src.Views).", "list_citation_info": ["[42] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020.", "[60] Vincent Sitzmann, Michael Zollhoefer, and Gordon Wetzstein. Scene representation networks: Continuous 3D-structure-aware neural scene representations. NeurIPS, 2019."]}, {"table": "<table><tr><td>Input (id: dimension)</td><td>Layer</td><td>Output (id: dimension)</td></tr><tr><td>0: 640\\times 480\\times 3</td><td>7\\times 7 Conv, 64, stride 2</td><td>1: 320\\times 240\\times 64</td></tr><tr><td>1: 320\\times 240\\times 64</td><td>Residual Block 1</td><td>2: 160\\times 120\\times 64</td></tr><tr><td>2: 160\\times 120\\times 64</td><td>Residual Block 2</td><td>3: 80\\times 60\\times 128</td></tr><tr><td>3: 80\\times 60\\times 128</td><td>Residual Block 3</td><td>4: 40\\times 30\\times 256</td></tr><tr><td>5: 40\\times 30\\times 256</td><td>3\\times 3 Upconv, 128, factor 2</td><td>6: 80\\times 60\\times 128</td></tr><tr><td>[3, 6]: 80\\times 60\\times 256</td><td>3\\times 3 Conv, 128</td><td>7: 80\\times 60\\times 128</td></tr><tr><td>7: 80\\times 60\\times 128</td><td>3\\times 3 Upconv, 64, factor 2</td><td>8: 160\\times 120\\times 64</td></tr><tr><td>[2, 8]: 160\\times 120\\times 128</td><td>3\\times 3 Conv, 64</td><td>9: 160\\times 120\\times 64</td></tr><tr><td>9: 160\\times 120\\times 64</td><td>1\\times 1 Conv, 64</td><td>Out: 160\\times 120\\times 64</td></tr></table>", "caption": "Table 4: Feature extraction network architecture. \u2018Conv\u201d stands for a sequence of operations: convolution, rectified linear units (ReLU) and Instance Normalization [68]. \u201cUpconv\u201d stands for a bilinear upsampling with certain factor, followed by a \u201cConv\u201d operation with stride 1. \u201c\\text{[}\\cdot,\\cdot\\text{]}\u201d represents channel-wise concatenation of two feature maps.The residual blocks have similar structures to those in the original ResNet34 [16] design, except that the first residual block has stride equal to 2 and all Batch Normalization layers are replaced with Instance Normalization layers. The output 64-dimensional feature map will be split into two feature maps of 32 dimension, which are then used as inputs to the coarse and fine IBRNet, respectively.", "list_citation_info": ["[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016.", "[68] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016."]}]}