{"title": "Deep Vit Features as Dense Visual Descriptors", "abstract": "We study the use of deep features extracted from a pretrained Vision Transformer (ViT) as dense visual descriptors. We observe and empirically demonstrate that such features, when extractedfrom a self-supervised ViT model (DINO-ViT), exhibit several striking properties, including: (i) the features encode powerful, well-localized semantic information, at high spatial granularity, such as object parts; (ii) the encoded semantic information is shared across related, yet different object categories, and (iii) positional bias changes gradually throughout the layers. These properties allow us to design simple methods for a variety of applications, including co-segmentation, part co-segmentation and semantic correspondences. To distill the power of ViT features from convoluted design choices, we restrict ourselves to lightweight zero-shot methodologies (e.g., binning and clustering) applied directly to the features. Since our methods require no additional training nor data, they are readily applicable across a variety of domains. We show by extensive qualitative and quantitative evaluation that our simple methodologies achieve competitive results with recent state-of-the-art supervised methods, and outperform previous unsupervised methods by a large margin. Code is available in dino-vit-features.github.io.", "authors": ["Shir Amir", " Yossi Gandelsman", " Shai Bagon", " Tali Dekel"], "pdf_url": "https://arxiv.org/abs/2112.05814", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">key-point regression \\downarrow</td><td rowspan=\"2\">FG-NMI \\uparrow</td><td rowspan=\"2\">FG-ARI \\uparrow</td><td rowspan=\"2\">NMI \\uparrow</td><td rowspan=\"2\">ARI \\uparrow</td></tr><tr><td>CUB-01</td><td>CUB-02</td><td>CUB-03</td></tr><tr><th>supervised</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>SCOPS [24]{}^{\\ddagger} (model)</th><td>18.3</td><td>17.7</td><td>17.0</td><td>39.1</td><td>17.9</td><td>24.4</td><td>7.1</td></tr><tr><th>Huang and Li [23]{}^{\\dagger}</th><td>15.1</td><td>17.1</td><td>15.7</td><td>-</td><td>-</td><td>26.1</td><td>13.2</td></tr><tr><th>Choudhury et al.[9]{}^{\\ddagger}</th><td>11.3</td><td>15.0</td><td>10.6</td><td>46.0</td><td>21.0</td><td>43.5</td><td>19.6</td></tr><tr><th>unsupervised</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>ULD [51, 59]</th><td>30.1</td><td>29.4</td><td>28.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DFF [10]</th><td>22.4</td><td>21.6</td><td>22.0</td><td>32.4</td><td>14.3</td><td>25.9</td><td>12.4</td></tr><tr><th>SCOPS [24] (paper)</th><td>18.5</td><td>18.8</td><td>21.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>17.1</td><td>14.7</td><td>19.6</td><td>39.4</td><td>19.2</td><td>38.9</td><td>16.1</td></tr></tbody></table>", "caption": "Table 1: Part Co-segmentation results: We report mean error of landmark regression on three CUB [55] test sets, and NMI and ARI [9] measures on the entire CUB test set. All methods predict k=4 parts. \\dagger method uses image-level supervision, \\ddagger methods use ground truth foreground masks as supervision.", "list_citation_info": ["[23] Huang, Z., Li, Y.: Interpretable and accurate fine-grained recognition via region grouping. CVPR (2020)", "[55] Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., Perona, P.: Caltech-UCSD Birds 200. Tech. Rep. CNS-TR-2010-001, California Institute of Technology (2010)", "[9] Choudhury, S., Laina, I., Rupprecht, C., Vedaldi, A.: Unsupervised part discovery from contrastive reconstruction. NeurIPS (2021)", "[51] Thewlis, J., Bilen, H., Vedaldi, A.: Unsupervised learning of object landmarks by factorized spatial embeddings. ICCV (2017)", "[24] Hung, W.C., Jampani, V., Liu, S., Molchanov, P., Yang, M.H., Kautz, J.: Scops: Self-supervised co-part segmentation. CVPR (2019)", "[10] Collins, E., Achanta, R., Susstrunk, S.: Deep feature factorization for concept discovery. In: The European Conference on Computer Vision (ECCV) (2018)"]}, {"table": "<table><tbody><tr><th>Method</th><td>Training Set</td><td colspan=\"2\">MSRC [47]</td><td colspan=\"2\">Internet300 [44]</td><td colspan=\"2\">PASCAL-VOC [14]</td><td colspan=\"2\">PASCAL-CO</td></tr><tr><th></th><td></td><td>\\mathcal{J}_{m}</td><td>\\mathcal{P}_{m}</td><td>\\mathcal{J}_{m}</td><td>\\mathcal{P}_{m}</td><td>\\mathcal{J}_{m}</td><td>\\mathcal{P}_{m}</td><td>\\mathcal{J}_{m}</td><td>\\mathcal{P}_{m}</td></tr><tr><th>supervised</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>SSNM [56]</th><td>COCO-SEG</td><td>81.9</td><td>95.2</td><td>74.1</td><td>93.6</td><td>71.0</td><td>94.9</td><td>74.2</td><td>94.5</td></tr><tr><th>DOCS [31]</th><td>VOC2012</td><td>82.9</td><td>95.4</td><td>72.5</td><td>93.5</td><td>65.0</td><td>94.2</td><td>34.9</td><td>53.7</td></tr><tr><th>CycleSegNet [30]</th><td>VOC2012</td><td>87.2</td><td>97.9</td><td>80.4</td><td>-</td><td>75.4</td><td>95.8</td><td>-</td><td>-</td></tr><tr><th>Li et al. [29]</th><td>COCO</td><td>-</td><td>-</td><td>84.0</td><td>97.1</td><td>63.0</td><td>94.1</td><td>-</td><td>-</td></tr><tr><th>unsupervised</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>Hsu et al.[22]</th><td>-</td><td>-</td><td>-</td><td>69.8</td><td>92.3</td><td>60.0</td><td>91.0</td><td>-</td><td>-</td></tr><tr><th>DeepCO3 [28]</th><td>-</td><td>54.7</td><td>87.2</td><td>53.4</td><td>88.0</td><td>46.3</td><td>88.5</td><td>37.3</td><td>74.1</td></tr><tr><th>TokenCut[54]</th><td>-</td><td>81.2</td><td>94.9</td><td>65.2</td><td>91.3</td><td>57.8</td><td>90.6</td><td>75.8</td><td>93.0</td></tr><tr><th>Faktor et al.[15]</th><td>-</td><td>77.0</td><td>92.0</td><td>-</td><td>-</td><td>46.0</td><td>84.0</td><td>41.4</td><td>79.9</td></tr><tr><th>Rubinstein et al.[44]</th><td>-</td><td>74.0</td><td>92.2</td><td>57.3</td><td>85.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>-</td><td>86.7</td><td>96.5</td><td>79.5</td><td>94.6</td><td>60.7</td><td>88.2</td><td>79.5</td><td>94.7</td></tr></tbody></table>", "caption": "Table 2:  Co-segmentation evaluation: We report mean Jaccard index \\mathcal{J}_{m} and precision \\mathcal{P}_{m} over all sets in each dataset. We compare to unsupervised methods [15, 44] and methods supervised with ground truth segmentation masks [56, 31, 30, 29].", "list_citation_info": ["[22] Hsu, K.J., Lin, Y.Y., Chuang, Y.Y.: Co-attention cnns for unsupervised object co-segmentation. IJCAI (2018)", "[56] Zhang, K., Chen, J., Liu, B., Liu, Q.: Deep object co-segmentation via spatial-semantic network modulation. AAAI (2020)", "[54] Wang, Y., Shen, X., Hu, S.X., Yuan, Y., Crowley, J., Vaufreydaz, D.: Self-supervised transformers for unsupervised object discovery using normalized cut. CVPR (2022)", "[47] Shotton, J., Winn, J., Rother, C., Criminisi, A.: Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation. ECCV (2006)", "[28] Kuang-Jui Hsu, Yen-Yu Lin, Y.Y.C.: Deepco3: Deep instance co-segmentation by co-peak search and co-saliency detection. CVPR (2019)", "[30] Li, G., Zhang, C., Lin, G.: Cyclesegnet: Object co-segmentation with cycle refinement and region correspondence. TIP (2021)", "[44] Rubinstein, M., Joulin, A., Kopf, J., Liu, C.: Unsupervised joint object discovery and segmentation in internet images. CVPR (2013)", "[14] Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes challenge: A retrospective. IJCV (2015)", "[29] Li, B., Sun, Z., Li, Q., Wu, Y., Hu, A.: Group-wise deep object co-segmentation with co-attention recurrent neural network. ICCV (2019)", "[15] Faktor, A., Irani, M.: Co-segmentation by composition. ICCV (2013)", "[31] Li, W., Jafari, O.H., Rother, C.: Deep object co-segmentation. ACCV (2018)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"4\">Layer 9</th><th colspan=\"4\">Layer 11</th><th rowspan=\"2\">NBB [1]</th><th rowspan=\"2\">Supervised [7]</th></tr><tr><th>key</th><th>query</th><th>value</th><th>token</th><th>key</th><th>query</th><th>value</th><th>token</th></tr></thead><tbody><tr><th>with bins</th><td>56.48</td><td>54.96</td><td>52.33</td><td>56.03</td><td>53.45</td><td>52.35</td><td>49.37</td><td>50.34</td><td rowspan=\"2\">26.98</td><td rowspan=\"2\">61.43</td></tr><tr><th>without bins</th><td>52.27</td><td>49.35</td><td>43.97</td><td>50.14</td><td>47.08</td><td>42.64</td><td>41.56</td><td>46.09</td></tr></tbody></table>", "caption": "Table 4: Correspondence Evaluation on Spair71k: We randomly sample 20 image pairs per category, and report the mean PCK across all categories (\\alpha=0.1); higher is better. We include a recent supervised method [7] for reference.", "list_citation_info": ["[1] Aberman, K., Liao, J., Shi, M., Lischinski, D., Chen, B., Cohen-Or, D.: Neural best-buddies: Sparse cross-domain correspondence. TOG (2018)", "[7] Cho, S., Hong, S., Jeon, S., Lee, Y., Sohn, K., Kim, S.: Semantic correspondence with transformers. NeurIPS (2021)"]}]}