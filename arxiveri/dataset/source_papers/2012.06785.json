{"title": "Detr for Crowd Pedestrian Detection", "abstract": "Pedestrian detection in crowd scenes poses a challenging problem due to the heuristic defined mapping from anchors to pedestrians and the conflict between NMS and highly overlapped pedestrians. The recently proposed end-to-end detectors(ED), DETR and deformable DETR, replace hand designed components such as NMS and anchors using the transformer architecture, which gets rid of duplicate predictions by computing all pairwise interactions between queries. Inspired by these works, we explore their performance on crowd pedestrian detection. Surprisingly, compared to Faster-RCNN with FPN, the results are opposite to those obtained on COCO. Furthermore, the bipartite match of ED harms the training efficiency due to the large ground truth number in crowd scenes. In this work, we identify the underlying motives driving ED's poor performance and propose a new decoder to address them. Moreover, we design a mechanism to leverage the less occluded visible parts of pedestrian specifically for ED, and achieve further improvements. A faster bipartite match algorithm is also introduced to make ED training on crowd dataset more practical. The proposed detector PED(Pedestrian End-to-end Detector) outperforms both previous EDs and the baseline Faster-RCNN on CityPersons and CrowdHuman. It also achieves comparable performance with state-of-the-art pedestrian detection methods. Code will be released soon.", "authors": ["Matthieu Lin", " Chuming Li", " Xingyuan Bu", " Ming Sun", " Chen Lin", " Junjie Yan", " Wanli Ouyang", " Zhidong Deng"], "pdf_url": "https://arxiv.org/abs/2012.06785", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>OR-CNN [40]</th><th>TLL [32]</th><th>RepLoss [36]</th></tr></thead><tbody><tr><td>Heavy</td><td>55.7</td><td>53.6</td><td>56.9</td></tr><tr><td>Methods</td><td>ALFNet [26]</td><td>CSP [27]</td><td>Ours</td></tr><tr><td>Heavy</td><td>51.9</td><td>49.3</td><td>47.70</td></tr></tbody></table>", "caption": "Table 8: Results on Heavy subset on CityPerson shows that our method is effective in occluded and crowded scene.", "list_citation_info": ["[26] Wei Liu, Shengcai Liao, Weidong Hu, Xuezhi Liang, and Xiao Chen. Learning efficient single-stage pedestrian detectors by asymptotic localization fitting. In Proceedings of the European Conference on Computer Vision (ECCV), pages 618\u2013634, 2018.", "[40] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and Stan Z Li. Occlusion-aware r-cnn: detecting pedestrians in a crowd. In Proceedings of the European Conference on Computer Vision (ECCV), pages 637\u2013653, 2018.", "[27] Wei Liu, Shengcai Liao, Weiqiang Ren, Weidong Hu, and Yinan Yu. High-level semantic feature detection: A new perspective for pedestrian detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5187\u20135196, 2019.", "[32] Tao Song, Leiyu Sun, Di Xie, Haiming Sun, and Shiliang Pu. Small-scale pedestrian detection based on topological line localization and temporal feature aggregation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 536\u2013551, 2018.", "[36] Xinlong Wang, Tete Xiao, Yuning Jiang, Shuai Shao, Jian Sun, and Chunhua Shen. Repulsion loss: Detecting pedestrians in a crowd. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7774\u20137783, 2018."]}, {"table": "<table><tbody><tr><td>Method</td><td>AP</td><td>MR{}^{-2}</td><td>Recall</td></tr><tr><td>PBM  [17]</td><td>89.3</td><td>43.3</td><td>93.33</td></tr><tr><td>Ours</td><td>90.08</td><td>44.37</td><td>93.95</td></tr><tr><td>Faster-RCNN{}^{*} [30]</td><td>85.0</td><td>50.4</td><td>90.24</td></tr><tr><td>AdaptiveNMS{}^{*}  [24]</td><td>84.7</td><td>49.7</td><td>91.27</td></tr><tr><td>Deformable DETR{}^{*}  [43]</td><td>86.74</td><td>53.98</td><td>92.51</td></tr><tr><td>Ours{}^{*}</td><td>89.54</td><td>45.57</td><td>94.00</td></tr></tbody></table>", "caption": "Table 9: Results on CrowdHuman. * stands for no usage of visible boxes", "list_citation_info": ["[43] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[17] Xin Huang, Zheng Ge, Zequn Jie, and Osamu Yoshie. Nms by representative region: Towards crowded pedestrian detection by proposal pairing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10750\u201310759, 2020.", "[24] Songtao Liu, Di Huang, and Yunhong Wang. Adaptive nms: Refining pedestrian detection in a crowd. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6459\u20136468, 2019.", "[30] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91\u201399, 2015."]}]}