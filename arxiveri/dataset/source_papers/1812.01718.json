{"title": "Deep learning for classical Japanese literature", "abstract": "Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature. Dataset available at https://github.com/rois-codh/kmnist", "authors": ["Tarin Clanuwat", " Mikel Bober-Irizar", " Asanobu Kitamoto", " Alex Lamb", " Kazuaki Yamamoto", " David Ha"], "pdf_url": "https://arxiv.org/abs/1812.01718", "list_table_and_caption": [{"table": "<table><thead><tr><th>Model</th><th>MNIST [16]</th><th>Kuzushiji-MNIST</th><th>Kuzushiji-49</th></tr></thead><tbody><tr><th>4-Nearest Neighbour Baseline</th><td>97.14%</td><td>91.56%</td><td>86.01%</td></tr><tr><th>Keras Simple CNN Benchmark [4]</th><td>99.06%</td><td>95.12%</td><td>89.25%</td></tr><tr><th>PreActResNet-18 [11]</th><td>99.56%</td><td>97.82%</td><td>96.64%</td></tr><tr><th>PreActResNet-18 + Input Mixup [26]</th><td>99.54%</td><td>98.41%</td><td>97.04%</td></tr><tr><th>PreActResNet-18 + Manifold Mixup [22]</th><td>99.54%</td><td>98.83%</td><td>97.33%</td></tr></tbody></table>", "caption": "Table 1: Test set accuracy, computed as mean of per-class accuracies to address class imbalance.", "list_citation_info": ["[11] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630\u2013645. Springer, 2016.", "[22] V. Verma, A. Lamb, C. Beckham, A. Najafi, A. Courville, I. Mitliagkas, and Y. Bengio. Manifold Mixup: Learning Better Representations by Interpolating Hidden States. ArXiv e-prints, June 2018.", "[4] F. Chollet et al. Keras, 2015. https://keras.io.", "[26] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond Empirical Risk Minimization. In International Conference on Learning Representations, 2018.", "[16] Y. LeCun. The MNIST database of handwritten digits, 1998. http://yann.lecun.com/exdb/mnist/."]}]}