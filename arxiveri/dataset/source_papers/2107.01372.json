{"title": "Learning Debiased Representation via Disentangled Feature Augmentation", "abstract": "Image classification models tend to make decisions based on peripheral attributes of data items that have strong correlation with a target variable (i.e., dataset bias). These biased models suffer from the poor generalization capability when evaluated on unbiased datasets. Existing approaches for debiasing often identify and emphasize those samples with no such correlation (i.e., bias-conflicting) without defining the bias type in advance. However, such bias-conflicting samples are significantly scarce in biased datasets, limiting the debiasing capability of these approaches. This paper first presents an empirical analysis revealing that training with \"diverse\" bias-conflicting samples beyond a given training set is crucial for debiasing as well as the generalization capability. Based on this observation, we propose a novel feature-level data augmentation technique in order to synthesize diverse bias-conflicting samples. To this end, our method learns the disentangled representation of (1) the intrinsic attributes (i.e., those inherently defining a certain class) and (2) bias attributes (i.e., peripheral attributes causing the bias), from a large number of bias-aligned samples, the bias attributes of which have strong correlation with the target variable. Using the disentangled representation, we synthesize bias-conflicting samples that contain the diverse intrinsic attributes of bias-aligned samples by swapping their latent features. By utilizing these diversified bias-conflicting features during the training, our approach achieves superior classification accuracy and debiasing results against the existing baselines on synthetic and real-world datasets.", "authors": ["Jungsoo Lee", " Eungyeup Kim", " Juyoung Lee", " Jihyeon Lee", " Jaegul Choo"], "pdf_url": "https://arxiv.org/abs/2107.01372", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Dataset</td><td rowspan=\"2\">Ratio (%)</td><td>Vanilla He2015resnet </td><td>HEX wang2018hex </td><td>EnD EnD </td><td>ReBias bahng2019rebias </td><td>LfF nam2020learning </td><td>Ours</td></tr><tr><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt,\u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt,\u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt,\u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt,\u2713</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt,\u2717</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt,\u2717</td></tr><tr><td rowspan=\"4\"> Colored MNIST </td><td>0.5</td><td>35.19\\pm3.49</td><td>30.33\\pm0.76</td><td>34.28\\pm1.20</td><td>70.47\\pm1.84</td><td>52.50\\pm2.43</td><td>65.22\\pm4.41</td></tr><tr><td>1.0</td><td>52.09\\pm2.88</td><td>43.73\\pm5.50</td><td>49.50\\pm2.51</td><td>87.4\\pm0.78</td><td>61.89\\pm4.97</td><td>81.73\\pm2.34</td></tr><tr><td>2.0</td><td>65.86\\pm3.59</td><td>56.85\\pm2.58</td><td>68.45\\pm2.16</td><td>92.91\\pm0.15</td><td>71.03\\pm2.44</td><td>84.79\\pm0.95</td></tr><tr><td>5.0</td><td>82.17\\pm0.74</td><td>74.62\\pm3.20</td><td>81.15\\pm1.43</td><td>96.96\\pm0.04</td><td>80.57\\pm3.84</td><td>89.66\\pm1.09</td></tr><tr><td rowspan=\"4\"> Corrupted CIFAR-10 </td><td>0.5</td><td>23.08\\pm1.25</td><td>13.87\\pm0.06</td><td>22.89\\pm0.27</td><td>22.27\\pm0.41</td><td>28.57\\pm1.30</td><td>29.95\\pm0.71</td></tr><tr><td>1.0</td><td>25.82\\pm0.33</td><td>14.81\\pm0.42</td><td>25.46\\pm0.41</td><td>25.72\\pm0.20</td><td>33.07\\pm0.77</td><td>36.49\\pm1.79</td></tr><tr><td>2.0</td><td>30.06\\pm0.71</td><td>15.20\\pm0.54</td><td>31.31\\pm0.35</td><td>31.66\\pm0.43</td><td>39.91\\pm0.30</td><td>41.78\\pm2.29</td></tr><tr><td>5.0</td><td>39.42\\pm0.64</td><td>16.04\\pm0.63</td><td>40.26\\pm0.85</td><td>43.43\\pm0.41</td><td>50.27\\pm1.56</td><td>51.13\\pm1.28</td></tr><tr><td> BFFHQ </td><td>0.5</td><td>56.87\\pm2.69</td><td>52.83\\pm0.90</td><td>56.87\\pm1.42</td><td>59.46\\pm0.64</td><td>62.2\\pm1.0</td><td>63.87\\pm0.31</td></tr></table>", "caption": "Table 2: Image classification accuracy evaluated on unbiased test sets of Colored MNIST and Corrupted CIFAR-10, and the bias-conflicting test set of BFFHQ with varying ratio of bias-conflicting samples. We denote whether the model requires a bias type in advance by cross mark (i.e., not required), and check mark (i.e., required).Best performing results are marked in bold, while second-best results are denoted with underlines.", "list_citation_info": ["[7] Haohan Wang, Zexue He, Zachary L. Lipton, and Eric P. Xing. Learning robust representations by projecting superficial statistics out. In International Conference on Learning Representations, 2019.", "[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.", "[6] Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased representations with biased representations. In International Conference on Machine Learning (ICML), 2020.", "[12] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: Training debiased classifier from biased classifier. In Advances in Neural Information Processing Systems, 2020.", "[27] Enzo Tartaglione, Carlo Alberto Barbano, and Marco Grangetto. End: Entangling and disentangling deep representations for bias correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13508\u201313517, June 2021."]}, {"table": "<table><tr><td>Dataset</td><td>Ratio (%)</td><td>LfF nam2020learning </td><td>Ours</td></tr><tr><td rowspan=\"4\"> Corrupted CIFAR-10Type 0 </td><td>0.5</td><td>33.95\\pm3.97</td><td>36.89\\pm0.83</td></tr><tr><td>1.0</td><td>41.54\\pm3.26</td><td>44.43\\pm1.29</td></tr><tr><td>2.0</td><td>50.45\\pm0.39</td><td>52.01\\pm0.44</td></tr><tr><td>5.0</td><td>58.99\\pm0.23</td><td>60.18\\pm1.05</td></tr><tr><td rowspan=\"4\"> Corrupted CIFAR-10Type 1 </td><td>0.5</td><td>35.07\\pm0.63</td><td>36.52\\pm1.05</td></tr><tr><td>1.0</td><td>42.32\\pm2.58</td><td>43.64\\pm1.10</td></tr><tr><td>2.0</td><td>49.05\\pm1.96</td><td>52.23\\pm1.51</td></tr><tr><td>5.0</td><td>58.77\\pm0.99</td><td>59.3\\pm0.85</td></tr></table>", "caption": "Table 5: Image classification accuracy evaluated on unbiased test sets of Corrupted CIFAR-10 Type 0 and Type 1 with varying ratio of bias-conflicting samples. Best performing results are marked in bold.", "list_citation_info": ["[12] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: Training debiased classifier from biased classifier. In Advances in Neural Information Processing Systems, 2020."]}]}