{"title": "One-shot object affordance detection", "abstract": "Affordance detection refers to identifying the potential action possibilities of objects in an image, which is a crucial ability for robot perception and manipulation. To empower robots with this ability in unseen scenarios, we first study the challenging one-shot affordance detection problem in this paper, i.e., given a support image that depicts the action purpose, all objects in a scene with the common affordance should be detected. To this end, we devise a One-Shot Affordance Detection Network (OSAD-Net) that firstly estimates the human action purpose and then transfers it to help detect the common affordance from all candidate images. Through collaboration learning, OSAD-Net can capture the common characteristics between objects having the same underlying affordance and learn a good adaptation capability for perceiving unseen affordances. Besides, we build a large-scale Purpose-driven Affordance Dataset v2 (PADv2) by collecting and labeling 30k images from 39 affordance and 103 object categories. With complex scenes and rich annotations, our PADv2 dataset can be used as a test bed to benchmark affordance detection methods and may also facilitate downstream vision tasks, such as scene understanding, action recognition, and robot manipulation. Specifically, we conducted comprehensive experiments on PADv2 dataset by including 11 advanced models from several related research fields. Experimental results demonstrate the superiority of our model over previous representative ones in terms of both objective metrics and visual quality. The benchmark suite is available at https://github.com/lhc1224/OSAD Net.", "authors": ["Wei Zhai", " Hongchen Luo", " Jing Zhang", " Yang Cao", " Dacheng Tao"], "pdf_url": "https://arxiv.org/abs/2108.03658", "list_table_and_caption": [{"table": "<table><tr><td></td><td>Dataset</td><td>Pub.</td><td>Year</td><td>Format</td><td>Pixel</td><td>HQ</td><td>BG</td><td>\\sharpObj.</td><td>\\sharpAff.</td><td>\\sharpImg.</td></tr><tr><td>1</td><td>2011 ICRA (Hermans et al. 2011)</td><td>ICRA</td><td>2011</td><td>RGB-D</td><td>-</td><td>-</td><td>Fixed</td><td>-</td><td>7</td><td>375</td></tr><tr><td>2</td><td>UMD (Myers et al. 2015)</td><td>ICRA</td><td>2015</td><td>RGB-D</td><td>\u2713</td><td>\u2717</td><td>Fixed</td><td>-</td><td>17</td><td>30,000</td></tr><tr><td>3</td><td>2016 TASE (Song et al. 2015)</td><td>T-ASE</td><td>2016</td><td>RGB</td><td>-</td><td>-</td><td>Fixed</td><td>8</td><td>1</td><td>10,360</td></tr><tr><td>4</td><td>IIT-AFF (Nguyen et al. 2017)</td><td>IROS</td><td>2017</td><td>RGB-D</td><td>\u2713</td><td>\u2717</td><td>General</td><td>10</td><td>9</td><td>8,835</td></tr><tr><td>5</td><td>CERTH-SOR3D (Thermos et al. 2017)</td><td>CVPR</td><td>2017</td><td>RGB-D</td><td>-</td><td>-</td><td>Fixed</td><td>14</td><td>13</td><td>20,800</td></tr><tr><td>6</td><td>ADE-Aff (Chuang et al. 2018)</td><td>CVPR</td><td>2018</td><td>RGB</td><td>\u2713</td><td>\u2713</td><td>General</td><td>150</td><td>7</td><td>10,000</td></tr><tr><td>7</td><td>PAD (Luo et al. 2021)</td><td>IJCAI</td><td>2021</td><td>RGB</td><td>\u2713</td><td>\u2713</td><td>General</td><td>72</td><td>31</td><td>4,002</td></tr><tr><td>8</td><td>PADv2</td><td></td><td></td><td>RGB-D</td><td>\u2713</td><td>\u2713</td><td>General</td><td>103</td><td>39</td><td>30,000</td></tr></table><p>{}^{2}http://users.umiacs.umd.edu/ amyers/part-affordance-dataset/   {}^{4}https://sites.google.com/site/ocnncrf/<br/>{}^{5}http://sor3d.vcl.iti.gr/  {}^{6}http://www.cs.utoronto.ca/ cychuang/learning2act/  {}^{7,8}https://github.com/lhc1224/OSAD_Net/</p>", "caption": "Table 1: Statistics of existing image-based affordance datasets and the proposed PADv2 dataset. PADv2 dataset provides higher-quality annotations and covers much richer affordance categories. Pub.: Publication venue. Pixel: whether or not pixel-wise labels are provided. HQ: high-quality annotation. BG: the background is fixed or from general scenarios. \\sharpObj: number of object categories. \\sharpAff.: number of affordance categories. \\sharpImg: number of images.", "list_citation_info": ["Song et al. (2015) Song HO, Fritz M, Goehring D, Darrell T (2015) Learning to detect visual grasp affordance. IEEE Transactions on Automation Science and Engineering 13(2):798\u2013809", "Chuang et al. (2018) Chuang CY, Li J, Torralba A, Fidler S (2018) Learning to act properly: Predicting and explaining affordances from images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 975\u2013983", "Myers et al. (2015) Myers A, Teo CL, Ferm\u00fcller C, Aloimonos Y (2015) Affordance detection of tool parts from geometric features. In: 2015 IEEE International Conference on Robotics and Automation (ICRA), IEEE, pp 1374\u20131381", "Thermos et al. (2017) Thermos S, Papadopoulos GT, Daras P, Potamianos G (2017) Deep affordance-grounded sensorimotor object recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 6167\u20136175", "Nguyen et al. (2017) Nguyen A, Kanoulas D, Caldwell DG, Tsagarakis NG (2017) Object-based affordances detection with convolutional neural networks and dense conditional random fields. In: 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE, pp 5908\u20135915", "Hermans et al. (2011) Hermans T, Rehg JM, Bobick A (2011) Affordance prediction via learned object attributes. In: IEEE International Conference on Robotics and Automation (ICRA): Workshop on Semantic Perception, Mapping, and Exploration, Citeseer, pp 181\u2013184", "Luo et al. (2021) Luo H, Zhai W, Zhang J, Cao Y, Tao D (2021) One-shot affordance detection. In: IJCAI"]}, {"table": "<table><tr><td>Attr.</td><td>Description</td></tr><tr><td>AC</td><td>Appearance Change. Significant lighting changes appear in the object area of the image.</td></tr><tr><td>BO</td><td>Big Objects. This refers to the ratio of object area to image area greater than 0.5.</td></tr><tr><td>HO</td><td>Heterogeneus Object. Refers to Obj. that are composed of visually distinct or dissimilar parts.</td></tr><tr><td>OV</td><td>Out-of-View. Object is partially clipped by the image boundaries.</td></tr><tr><td>SC</td><td>Shape Complexity. The object has complex boundaries such asthin parts and holes.</td></tr><tr><td>SO</td><td>Small Object. Refers to the ratio of object area to image area less than 0.1.</td></tr></table>", "caption": "Table 3: The attribute list and associated description of the affordance object image. The choice of these attributes is inspired by Perazzi et al. (2016).", "list_citation_info": ["Perazzi et al. (2016) Perazzi F, Pont-Tuset J, McWilliams B, Van Gool L, Gross M, Sorkine-Hornung A (2016) A benchmark dataset and evaluation methodology for video object segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 724\u2013732"]}, {"table": "<table><tr><td></td><td>Method</td><td>UNet</td><td>PSPNet</td><td>DLabV3+</td><td>CPD</td><td>BASNet</td><td>CSNet</td><td>CoEGNet</td><td>CANet</td><td>PFENet</td><td>RANet</td><td>OSAD-Net{}^{\\lozenge}</td><td>Ours</td></tr><tr><td></td><td>Params (M)</td><td>16.48</td><td>50.85</td><td>38.48</td><td>45.63</td><td>84.15</td><td>0.752</td><td>106.52</td><td>18.15</td><td>32.85</td><td>46.18</td><td>60.90</td><td>42.77</td></tr><tr><td rowspan=\"4\">i=1</td><td>IoU (\\uparrow)</td><td>0.290</td><td>0.279</td><td>0.195</td><td>0.381</td><td>0.362</td><td>0.285</td><td>0.374</td><td>0.402</td><td>0.371</td><td>0.301</td><td>0.444</td><td>0.496</td></tr><tr><td>F_{\\beta} (\\uparrow)</td><td>0.377</td><td>0.411</td><td>0.300</td><td>0.488</td><td>0.458</td><td>0.390</td><td>0.480</td><td>0.417</td><td>0.469</td><td>0.393</td><td>0.521</td><td>0.565</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.611</td><td>0.631</td><td>0.456</td><td>0.675</td><td>0.652</td><td>0.600</td><td>0.675</td><td>0.600</td><td>0.666</td><td>0.598</td><td>0.711</td><td>0.745</td></tr><tr><td>CC (\\uparrow)</td><td>0.438</td><td>0.476</td><td>0.374</td><td>0.526</td><td>0.432</td><td>0.506</td><td>0.510</td><td>0.512</td><td>0.553</td><td>0.502</td><td>0.570</td><td>0.611</td></tr><tr><td></td><td>MAE (\\downarrow)</td><td>0.163</td><td>0.130</td><td>0.126</td><td>0.107</td><td>0.112</td><td>0.156</td><td>0.107</td><td>0.192</td><td>0.106</td><td>0.139</td><td>0.115</td><td>0.105</td></tr><tr><td rowspan=\"4\">i=2</td><td>IoU (\\uparrow)</td><td>0.260</td><td>0.278</td><td>0.271</td><td>0.357</td><td>0.354</td><td>0.282</td><td>0.358</td><td>0.347</td><td>0.397</td><td>0.306</td><td>0.394</td><td>0.432</td></tr><tr><td>F_{\\beta} (\\uparrow)</td><td>0.334</td><td>0.370</td><td>0.372</td><td>0.437</td><td>0.436</td><td>0.351</td><td>0.442</td><td>0.392</td><td>0.461</td><td>0.366</td><td>0.455</td><td>0.492</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.585</td><td>0.609</td><td>0.577</td><td>0.656</td><td>0.651</td><td>0.578</td><td>0.660</td><td>0.623</td><td>0.656</td><td>0.579</td><td>0.666</td><td>0.689</td></tr><tr><td>CC (\\uparrow)</td><td>0.382</td><td>0.417</td><td>0.391</td><td>0.465</td><td>0.421</td><td>0.435</td><td>0.469</td><td>0.458</td><td>0.530</td><td>0.437</td><td>0.502</td><td>0.542</td></tr><tr><td></td><td>MAE (\\downarrow)</td><td>0.185</td><td>0.170</td><td>0.139</td><td>0.148</td><td>0.141</td><td>0.188</td><td>0.140</td><td>0.164</td><td>0.145</td><td>0.195</td><td>0.137</td><td>0.140</td></tr><tr><td rowspan=\"4\">i=3</td><td>IoU (\\uparrow)</td><td>0.347</td><td>0.350</td><td>0.292</td><td>0.461</td><td>0.440</td><td>0.360</td><td>0.471</td><td>0.474</td><td>0.481</td><td>0.394</td><td>0.490</td><td>0.520</td></tr><tr><td>F_{\\beta} (\\uparrow)</td><td>0.450</td><td>0.466</td><td>0.428</td><td>0.559</td><td>0.536</td><td>0.457</td><td>0.575</td><td>0.526</td><td>0.565</td><td>0.479</td><td>0.563</td><td>0.608</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.624</td><td>0.635</td><td>0.571</td><td>0.698</td><td>0.686</td><td>0.611</td><td>0.716</td><td>0.661</td><td>0.697</td><td>0.632</td><td>0.695</td><td>0.735</td></tr><tr><td>CC (\\uparrow)</td><td>0.467</td><td>0.509</td><td>0.432</td><td>0.560</td><td>0.503</td><td>0.538</td><td>0.559</td><td>0.600</td><td>0.626</td><td>0.545</td><td>0.593</td><td>0.609</td></tr><tr><td></td><td>MAE (\\downarrow)</td><td>0.184</td><td>0.163</td><td>0.164</td><td>0.139</td><td>0.150</td><td>0.181</td><td>0.137</td><td>0.160</td><td>0.130</td><td>0.169</td><td>0.140</td><td>0.125</td></tr><tr><td rowspan=\"4\">Mean</td><td>IoU (\\uparrow)</td><td>0.299</td><td>0.302</td><td>0.253</td><td>0.400</td><td>0.385</td><td>0.309</td><td>0.401</td><td>0.408</td><td>0.408</td><td>0.334</td><td>\\uline{0.443}</td><td>0.483</td></tr><tr><td>F_{\\beta} (\\uparrow)</td><td>0.387</td><td>0.416</td><td>0.358</td><td>0.495</td><td>0.477</td><td>0.399</td><td>0.499</td><td>0.445</td><td>0.494</td><td>0.413</td><td>\\uline{0.513}</td><td>0.555</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.607</td><td>0.625</td><td>0.535</td><td>0.676</td><td>0.663</td><td>0.596</td><td>0.684</td><td>0.628</td><td>0.673</td><td>0.603</td><td>\\uline{0.691}</td><td>0.723</td></tr><tr><td>CC (\\uparrow)</td><td>0.429</td><td>0.467</td><td>0.399</td><td>0.517</td><td>0.452</td><td>0.493</td><td>0.513</td><td>0.523</td><td>\\uline{0.570}</td><td>0.495</td><td>0.555</td><td>0.587</td></tr><tr><td></td><td>MAE (\\downarrow)</td><td>0.177</td><td>0.154</td><td>0.143</td><td>0.131</td><td>0.134</td><td>0.175</td><td>0.128</td><td>0.172</td><td>\\uline{0.127}</td><td>0.168</td><td>0.131</td><td>0.123</td></tr></table>", "caption": "Table 5: The experimental results of 11 models (UNet (Ronneberger et al. 2015), PSPNet (Zhao et al. 2017), DeeplabV3+ (DLabV3+) (Chen et al. 2018), CPD (Wu et al. 2019b), BASNet (Qin et al. 2019), CSNet (Gao et al. 2020), CoEGNet (Fan et al. 2021), CANet (Zhang et al. 2019), PFENet (Tian et al. 2020), RANet (Zhao et al. 2020), OSAD-Net{}^{\\lozenge} (Luo et al. 2021)) on the PADv2 dataset in terms of five metrics (IoU (\\uparrow) (Long et al. 2015), F_{\\beta} (\\uparrow) (Arbelaez et al. 2010), E_{\\phi} (\\uparrow) (Fan et al. 2018), CC (\\uparrow) (Le Meur et al. 2007), and MAE (\\downarrow) (Perazzi et al. 2012)). Bold and underline indicate the best and the second-best scores, respectively.", "list_citation_info": ["Zhao et al. (2020) Zhao X, Cao Y, Kang Y (2020) Object affordance detection with relationship-aware network. Neural Computing and Applications 32(18):14321\u201314333", "Arbelaez et al. (2010) Arbelaez P, Maire M, Fowlkes C, Malik J (2010) Contour detection and hierarchical image segmentation. IEEE transactions on pattern analysis and machine intelligence 33(5):898\u2013916", "Gao et al. (2020) Gao SH, Tan YQ, Cheng MM, Lu C, Chen Y, Yan S (2020) Highly efficient salient object detection with 100k parameters. In: ECCV", "Tian et al. (2020) Tian Z, Zhao H, Shu M, Yang Z, Li R, Jia J (2020) Prior guided feature enrichment network for few-shot segmentation. IEEE Annals of the History of Computing pp 1\u20131", "Fan et al. (2021) Fan DP, Li T, Lin Z, Ji GP, Zhang D, Cheng MM, Fu H, Shen J (2021) Re-thinking co-salient object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence", "Chen et al. (2018) Chen LC, Zhu Y, Papandreou G, Schroff F, Adam H (2018) Encoder-decoder with atrous separable convolution for semantic image segmentation. In: ECCV", "Ronneberger et al. (2015) Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional networks for biomedical image segmentation. In: MICCAI", "Fan et al. (2018) Fan DP, Gong C, Cao Y, Ren B, Cheng MM, Borji A (2018) Enhanced-alignment measure for binary foreground map evaluation. In: IJCAI", "Zhao et al. (2017) Zhao H, Shi J, Qi X, Wang X, Jia J (2017) Pyramid scene parsing network. In: CVPR", "Zhang et al. (2019) Zhang C, Lin G, Liu F, Yao R, Shen C (2019) Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 5217\u20135226", "Long et al. (2015) Long J, Shelhamer E, Darrell T (2015) Fully convolutional networks for semantic segmentation. In: CVPR", "Le Meur et al. (2007) Le Meur O, Le Callet P, Barba D (2007) Predicting visual fixations on video based on low-level visual features. Vision research", "Perazzi et al. (2012) Perazzi F, Kr\u00e4henb\u00fchl P, Pritch Y, Hornung A (2012) Saliency filters: Contrast based filtering for salient region detection. In: CVPR", "Wu et al. (2019b) Wu Z, Su L, Huang Q (2019b) Cascaded partial decoder for fast and accurate salient object detection. In: CVPR", "Luo et al. (2021) Luo H, Zhai W, Zhang J, Cao Y, Tao D (2021) One-shot affordance detection. In: IJCAI", "Qin et al. (2019) Qin X, Zhang Z, Huang C, Gao C, Dehghan M, Jagersand M (2019) Basnet: Boundary-aware salient object detection. In: CVPR"]}, {"table": "<table><tr><td></td><td>Metric</td><td>UNet</td><td>PSPNet</td><td>CPD</td><td>BASNet</td><td>CSNet</td><td>CoEGNet</td><td>OSAD-Net{}^{\\lozenge}</td><td>Ours</td></tr><tr><td></td><td>Params (M)</td><td>16.48</td><td>50.85</td><td>45.63</td><td>84.15</td><td>0.752</td><td>106.52</td><td>60.90</td><td>42.77</td></tr><tr><td rowspan=\"4\">i=1</td><td>IoU (\\uparrow)</td><td>0.184</td><td>0.231</td><td>0.249</td><td>0.239</td><td>0.204</td><td>0.277</td><td>0.401</td><td>0.424</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.574</td><td>0.640</td><td>0.615</td><td>0.604</td><td>0.557</td><td>0.674</td><td>0.732</td><td>0.751</td></tr><tr><td>CC (\\uparrow)</td><td>0.338</td><td>0.427</td><td>0.413</td><td>0.310</td><td>0.394</td><td>0.389</td><td>0.540</td><td>0.531</td></tr><tr><td>MAE (\\downarrow)</td><td>0.162</td><td>0.144</td><td>0.123</td><td>0.130</td><td>0.184</td><td>0.116</td><td>0.103</td><td>0.098</td></tr><tr><td rowspan=\"4\">i=2</td><td>IoU (\\uparrow)</td><td>0.215</td><td>0.227</td><td>0.251</td><td>0.263</td><td>0.210</td><td>0.259</td><td>0.375</td><td>0.411</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.558</td><td>0.601</td><td>0.601</td><td>0.598</td><td>0.555</td><td>0.637</td><td>0.653</td><td>0.684</td></tr><tr><td>CC (\\uparrow)</td><td>0.377</td><td>0.409</td><td>0.386</td><td>0.318</td><td>0.392</td><td>0.350</td><td>0.507</td><td>0.502</td></tr><tr><td>MAE (\\downarrow)</td><td>0.163</td><td>0.142</td><td>0.106</td><td>0.124</td><td>0.162</td><td>0.110</td><td>0.116</td><td>0.109</td></tr><tr><td rowspan=\"4\">i=3</td><td>IoU (\\uparrow)</td><td>0.227</td><td>0.265</td><td>0.285</td><td>0.281</td><td>0.238</td><td>0.284</td><td>0.407</td><td>0.410</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.578</td><td>0.636</td><td>0.630</td><td>0.628</td><td>0.557</td><td>0.645</td><td>0.691</td><td>0.665</td></tr><tr><td>CC (\\uparrow)</td><td>0.344</td><td>0.402</td><td>0.433</td><td>0.339</td><td>0.386</td><td>0.362</td><td>0.501</td><td>0.494</td></tr><tr><td>MAE (\\downarrow)</td><td>0.169</td><td>0.137</td><td>0.132</td><td>0.146</td><td>0.184</td><td>0.134</td><td>0.122</td><td>0.130</td></tr><tr><td rowspan=\"4\">Mean</td><td>IoU (\\uparrow)</td><td>0.209</td><td>0.241</td><td>0.262</td><td>0.261</td><td>0.217</td><td>0.273</td><td>\\uline{0.394}</td><td>0.415</td></tr><tr><td>E_{\\phi} (\\uparrow)</td><td>0.570</td><td>0.626</td><td>0.615</td><td>0.610</td><td>0.556</td><td>0.652</td><td>\\uline{0.691}</td><td>0.700</td></tr><tr><td>CC (\\uparrow)</td><td>0.353</td><td>0.413</td><td>0.411</td><td>0.322</td><td>0.391</td><td>0.367</td><td>0.516</td><td>\\uline{0.510}</td></tr><tr><td>MAE (\\downarrow)</td><td>0.165</td><td>0.141</td><td>0.120</td><td>0.133</td><td>0.177</td><td>0.120</td><td>\\uline{0.114}</td><td>0.112</td></tr></table>", "caption": "Table 7: The experimental results of 7 models (UNet (Ronneberger et al. 2015), PSPNet (Zhao et al. 2017), CPD (Wu et al. 2019b), BASNet (Qin et al. 2019), CSNet (Gao et al. 2020), CoEGNet (Fan et al. 2021), OSAD-Net{}^{\\lozenge} (Luo et al. 2021)) on the PAD dataset in terms of four metrics (IoU (\\uparrow) (Long et al. 2015), E_{\\phi} (\\uparrow) (Fan et al. 2018), CC (\\uparrow) (Le Meur et al. 2007), and MAE (\\downarrow) (Perazzi et al. 2012)). Bold and underline indicate the best and the second-best scores, respectively.", "list_citation_info": ["Gao et al. (2020) Gao SH, Tan YQ, Cheng MM, Lu C, Chen Y, Yan S (2020) Highly efficient salient object detection with 100k parameters. In: ECCV", "Perazzi et al. (2012) Perazzi F, Kr\u00e4henb\u00fchl P, Pritch Y, Hornung A (2012) Saliency filters: Contrast based filtering for salient region detection. In: CVPR", "Fan et al. (2021) Fan DP, Li T, Lin Z, Ji GP, Zhang D, Cheng MM, Fu H, Shen J (2021) Re-thinking co-salient object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence", "Ronneberger et al. (2015) Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional networks for biomedical image segmentation. In: MICCAI", "Fan et al. (2018) Fan DP, Gong C, Cao Y, Ren B, Cheng MM, Borji A (2018) Enhanced-alignment measure for binary foreground map evaluation. In: IJCAI", "Zhao et al. (2017) Zhao H, Shi J, Qi X, Wang X, Jia J (2017) Pyramid scene parsing network. In: CVPR", "Le Meur et al. (2007) Le Meur O, Le Callet P, Barba D (2007) Predicting visual fixations on video based on low-level visual features. Vision research", "Long et al. (2015) Long J, Shelhamer E, Darrell T (2015) Fully convolutional networks for semantic segmentation. In: CVPR", "Wu et al. (2019b) Wu Z, Su L, Huang Q (2019b) Cascaded partial decoder for fast and accurate salient object detection. In: CVPR", "Luo et al. (2021) Luo H, Zhai W, Zhang J, Cao Y, Tao D (2021) One-shot affordance detection. In: IJCAI", "Qin et al. (2019) Qin X, Zhang Z, Huang C, Gao C, Dehghan M, Jagersand M (2019) Basnet: Boundary-aware salient object detection. In: CVPR"]}]}