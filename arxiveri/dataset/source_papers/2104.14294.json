{"title": "Emerging properties in self-supervised vision transformers", "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.", "authors": ["Mathilde Caron", " Hugo Touvron", " Ishan Misra", " Herv\u00e9 J\u00e9gou", " Julien Mairal", " Piotr Bojanowski", " Armand Joulin"], "pdf_url": "https://arxiv.org/abs/2104.14294", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Arch.</td><td>Param.</td><td>im/s</td><td>Linear</td><td>k-NN</td></tr><tr><td>Supervised</td><td>RN50</td><td>23</td><td>1237</td><td>79.3</td><td>79.3</td></tr><tr><td>SCLR [12]</td><td>RN50</td><td>23</td><td>1237</td><td>69.1</td><td>60.7</td></tr><tr><td>MoCov2 [15]</td><td>RN50</td><td>23</td><td>1237</td><td>71.1</td><td>61.9</td></tr><tr><td>InfoMin [67]</td><td>RN50</td><td>23</td><td>1237</td><td>73.0</td><td>65.3</td></tr><tr><td>BarlowT [81]</td><td>RN50</td><td>23</td><td>1237</td><td>73.2</td><td>66.0</td></tr><tr><td>OBoW [27]</td><td>RN50</td><td>23</td><td>1237</td><td>73.8</td><td>61.9</td></tr><tr><td>BYOL [30]</td><td>RN50</td><td>23</td><td>1237</td><td>74.4</td><td>64.8</td></tr><tr><td>DCv2 [10]</td><td>RN50</td><td>23</td><td>1237</td><td>75.2</td><td>67.1</td></tr><tr><td>SwAV [10]</td><td>RN50</td><td>23</td><td>1237</td><td>75.3</td><td>65.7</td></tr><tr><td>DINO</td><td>RN50</td><td>23</td><td>1237</td><td>75.3</td><td>67.5</td></tr><tr><td>Supervised</td><td>ViT-S</td><td>21</td><td>1007</td><td>79.8</td><td>79.8</td></tr><tr><td>BYOL{}^{*} [30]</td><td>ViT-S</td><td>21</td><td>1007</td><td>71.4</td><td>66.6</td></tr><tr><td>MoCov2{}^{*} [15]</td><td>ViT-S</td><td>21</td><td>1007</td><td>72.7</td><td>64.4</td></tr><tr><td>SwAV{}^{*} [10]</td><td>ViT-S</td><td>21</td><td>1007</td><td>73.5</td><td>66.3</td></tr><tr><td>DINO</td><td>ViT-S</td><td>21</td><td>1007</td><td>77.0</td><td>74.5</td></tr><tr><td colspan=\"5\">Comparison across architectures</td><td></td></tr><tr><td>SCLR [12]</td><td>RN50w4</td><td>375</td><td>117</td><td>76.8</td><td>69.3</td></tr><tr><td>SwAV [10]</td><td>RN50w2</td><td>93</td><td>384</td><td>77.3</td><td>67.3</td></tr><tr><td>BYOL [30]</td><td>RN50w2</td><td>93</td><td>384</td><td>77.4</td><td>\u2013</td></tr><tr><td>DINO</td><td>ViT-B/16</td><td>85</td><td>312</td><td>78.2</td><td>76.1</td></tr><tr><td>SwAV [10]</td><td>RN50w5</td><td>586</td><td>76</td><td>78.5</td><td>67.1</td></tr><tr><td>BYOL [30]</td><td>RN50w4</td><td>375</td><td>117</td><td>78.6</td><td>\u2013</td></tr><tr><td>BYOL [30]</td><td>RN200w2</td><td>250</td><td>123</td><td>79.6</td><td>73.9</td></tr><tr><td>DINO</td><td>ViT-S/8</td><td>21</td><td>180</td><td>79.7</td><td>78.3</td></tr><tr><td>SCLRv2 [13]</td><td>RN152w3+SK</td><td>794</td><td>46</td><td>79.8</td><td>73.1</td></tr><tr><td>DINO</td><td>ViT-B/8</td><td>85</td><td>63</td><td>80.1</td><td>77.4</td></tr></tbody></table>", "caption": "Table 2: Linear and k-NN classification on ImageNet.We report top-1 accuracy for linear and k-NN evaluations on the validation set of ImageNet for different self-supervised methods.We focus on ResNet-50 and ViT-small architectures, but also report the best results obtained across architectures.{}^{*} are run by us.We run the k-NN evaluation for models with official released weights.The throughput (im/s) is calculated on a NVIDIA V100 GPU with 128 samples per forward.Parameters (M) are of the feature extractor.", "list_citation_info": ["[30] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.", "[27] Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick P\u00e9rez. Online bag-of-visual-words generation for unsupervised representation learning. arXiv preprint arXiv:2012.11552, 2020.", "[13] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In NeurIPS, 2020.", "[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. preprint arXiv:2002.05709, 2020.", "[81] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.", "[67] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. NeurIPS, 2020.", "[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.", "[15] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. preprint arXiv:2003.04297, 2020."]}, {"table": "<table><tbody><tr><th></th><th></th><td></td><td colspan=\"2\">\\mathcal{R}Ox</td><td colspan=\"2\">\\mathcal{R}Par</td></tr><tr><th>Pretrain</th><th>Arch.</th><td>Pretrain</td><td>M</td><td>H</td><td>M</td><td>H</td></tr><tr><th>Sup. [57]</th><th>RN101+R-MAC</th><td>ImNet</td><td>49.8</td><td>18.5</td><td>74.0</td><td>52.1</td></tr><tr><th>Sup.</th><th>ViT-S/16</th><td>ImNet</td><td>33.5</td><td>8.9</td><td>63.0</td><td>37.2</td></tr><tr><th>DINO</th><th>ResNet-50</th><td>ImNet</td><td>35.4</td><td>11.1</td><td>55.9</td><td>27.5</td></tr><tr><th>DINO</th><th>ViT-S/16</th><td>ImNet</td><td>41.8</td><td>13.7</td><td>63.1</td><td>34.4</td></tr><tr><th>DINO</th><th>ViT-S/16</th><td>GLDv2</td><td>51.5</td><td>24.3</td><td>75.3</td><td>51.6</td></tr></tbody></table>", "caption": "Table 3: Image retrieval.We compare the performance in retrieval of off-the-shelf features pretrained with supervision or with DINO on ImageNet and Google Landmarks v2 (GLDv2) dataset.We report mAP on revisited Oxford and Paris.Pretraining with DINO on a landmark dataset performs particularly well.For reference, we also report the best retrieval method with off-the-shelf features [57].", "list_citation_info": ["[57] Jerome Revaud, Jon Almaz\u00e1n, Rafael S Rezende, and Cesar Roberto de Souza. Learning with average precision: Training image retrieval with a listwise loss. In ICCV, 2019."]}, {"table": "<table><tbody><tr><td>Method</td><td>Arch.</td><td>Dim.</td><td>Resolution</td><td>mAP</td></tr><tr><td>Multigrain [5]</td><td>ResNet-50</td><td>2048</td><td>224^{2}</td><td>75.1</td></tr><tr><td>Multigrain [5]</td><td>ResNet-50</td><td>2048</td><td>largest side 800</td><td>82.5</td></tr><tr><td>Supervised [69]</td><td>ViT-B/16</td><td>1536</td><td>224^{2}</td><td>76.4</td></tr><tr><td>DINO</td><td>ViT-B/16</td><td>1536</td><td>224^{2}</td><td>81.7</td></tr><tr><td>DINO</td><td>ViT-B/8</td><td>1536</td><td>320^{2}</td><td>85.5</td></tr></tbody></table>", "caption": "Table 4: Copy detection.We report the mAP performance in copy detection on Copydays \u201cstrong\u201d subset [21].For reference, we also report the performance of the multigrain model [5], trained specifically for particular object retrieval.", "list_citation_info": ["[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. preprint arXiv:2012.12877, 2020.", "[5] Maxim Berman, Herv\u00e9 J\u00e9gou, Vedaldi Andrea, Iasonas Kokkinos, and Matthijs Douze. MultiGrain: a unified image embedding for classes and instances. arXiv preprint arXiv:1902.05509, 2019.", "[21] Matthijs Douze, Herv\u00e9 J\u00e9gou, Harsimrat Sandhawalia, Laurent Amsaleg, and Cordelia Schmid. Evaluation of gist descriptors for web-scale image search. In CIVR, 2009."]}, {"table": "<table><tbody><tr><th>Method</th><th>Data</th><th>Arch.</th><td>(\\mathcal{J}&amp;\\mathcal{F})_{m}</td><td>\\mathcal{J}_{m}</td><td>\\mathcal{F}_{m}</td></tr><tr><th colspan=\"4\">Supervised</th><td></td><td></td></tr><tr><th>ImageNet</th><th>INet</th><th>ViT-S/8</th><td>66.0</td><td>63.9</td><td>68.1</td></tr><tr><th>STM [48]</th><th>I/D/Y</th><th>RN50</th><td>81.8</td><td>79.2</td><td>84.3</td></tr><tr><th colspan=\"4\">Self-supervised</th><td></td><td></td></tr><tr><th>CT [71]</th><th>VLOG</th><th>RN50</th><td>48.7</td><td>46.4</td><td>50.0</td></tr><tr><th>MAST [40]</th><th>YT-VOS</th><th>RN18</th><td>65.5</td><td>63.3</td><td>67.6</td></tr><tr><th>STC [37]</th><th>Kinetics</th><th>RN18</th><td>67.6</td><td>64.8</td><td>70.2</td></tr><tr><th>DINO</th><th>INet</th><th>ViT-S/16</th><td>61.8</td><td>60.2</td><td>63.4</td></tr><tr><th>DINO</th><th>INet</th><th>ViT-B/16</th><td>62.3</td><td>60.7</td><td>63.9</td></tr><tr><th>DINO</th><th>INet</th><th>ViT-S/8</th><td>69.9</td><td>66.6</td><td>73.1</td></tr><tr><th>DINO</th><th>INet</th><th>ViT-B/8</th><td>71.4</td><td>67.9</td><td>74.9</td></tr></tbody></table>", "caption": "Table 5: DAVIS 2017 Video object segmentation.We evaluate the quality of frozen features on video instance tracking.We report mean region similarity \\mathcal{J}_{m} and mean contour-based accuracy \\mathcal{F}_{m}.We compare with existing self-supervised methods and a supervised ViT-S/8 trained on ImageNet.Image resolution is 480p.", "list_citation_info": ["[40] Zihang Lai, Erika Lu, and Weidi Xie. Mast: A memory-augmented self-supervised tracker. In CVPR, 2020.", "[48] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In ICCV, 2019.", "[71] Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of time. In CVPR, 2019.", "[37] Allan Jabri, Andrew Owens, and Alexei A Efros. Space-time correspondence as a contrastive random walk. 2020."]}, {"table": "<table><thead><tr><th></th><th>Cifar{}_{\\text{10}}</th><th>Cifar{}_{\\text{100}}</th><th>INat{}_{\\text{18}}</th><th>INat{}_{\\text{19}}</th><th>Flwrs</th><th>Cars</th><th>INet</th></tr></thead><tbody><tr><th>ViT-S/16</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>Sup. [69]</th><td>99.0</td><td>89.5</td><td>70.7</td><td>76.6</td><td>98.2</td><td>92.1</td><td>79.9</td></tr><tr><th>DINO</th><td>99.0</td><td>90.5</td><td>72.0</td><td>78.2</td><td>98.5</td><td>93.0</td><td>81.5</td></tr><tr><th>ViT-B/16</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>Sup. [69]</th><td>99.0</td><td>90.8</td><td>73.2</td><td>77.7</td><td>98.4</td><td>92.1</td><td>81.8</td></tr><tr><th>DINO</th><td>99.1</td><td>91.7</td><td>72.6</td><td>78.6</td><td>98.8</td><td>93.0</td><td>82.8</td></tr></tbody></table>", "caption": "Table 6: Transfer learning by finetuning pretrained models on different datasets.We report top-1 accuracy.Self-supervised pretraining with DINO transfers better than supervised pretraining.", "list_citation_info": ["[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. preprint arXiv:2012.12877, 2020."]}, {"table": "<table><thead><tr><th></th><th>Method</th><th>Mom.</th><th>SK</th><th>MC</th><th>Loss</th><th>Pred.</th><th>k-NN</th><th>Lin.</th></tr></thead><tbody><tr><th>1</th><th>DINO</th><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>CE</td><td>\u2717</td><td>72.8</td><td>76.1</td></tr><tr><th>2</th><th></th><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>CE</td><td>\u2717</td><td>0.1</td><td>0.1</td></tr><tr><th>3</th><th></th><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>CE</td><td>\u2717</td><td>72.2</td><td>76.0</td></tr><tr><th>4</th><th></th><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>CE</td><td>\u2717</td><td>67.9</td><td>72.5</td></tr><tr><th>5</th><th></th><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>MSE</td><td>\u2717</td><td>52.6</td><td>62.4</td></tr><tr><th>6</th><th></th><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>CE</td><td>\u2713</td><td>71.8</td><td>75.6</td></tr><tr><th>7</th><th>BYOL</th><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>MSE</td><td>\u2713</td><td>66.6</td><td>71.4</td></tr><tr><th>8</th><th>MoCov2</th><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>INCE</td><td>\u2717</td><td>62.0</td><td>71.6</td></tr><tr><th>9</th><th>SwAV</th><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>CE</td><td>\u2717</td><td>64.7</td><td>71.8</td></tr></tbody><tfoot><tr><th colspan=\"9\">SK: Sinkhorn-Knopp, MC: Multi-Crop, Pred.: Predictor</th></tr><tr><th colspan=\"9\">CE: Cross-Entropy, MSE: Mean Square Error, INCE: InfoNCE</th></tr></tfoot></table>", "caption": "Table 7: Important component for self-supervised ViT pretraining.Models are trained for 300 epochs with ViT-S/16.We study the different components that matter for the k-NN and linear (\u201cLin.\u201d) evaluations.For the different variants, we highlight the differences from the default DINO setting.The best combination is the momentum encoder with the multicrop augmentation and the cross-entropy loss.We also report results with BYOL [30], MoCo-v2 [15] and SwAV [10].", "list_citation_info": ["[30] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.", "[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.", "[15] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. preprint arXiv:2003.04297, 2020."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">Logistic</th><th colspan=\"3\">k-NN</th></tr><tr><th></th><th>RN50</th><th>ViT-S</th><th>\\Delta</th><th>RN50</th><th>ViT-S</th><th>\\Delta</th></tr></thead><tbody><tr><th>Inet 100%</th><td>72.1</td><td>75.7</td><td>3.6</td><td>67.5</td><td>74.5</td><td>7.0</td></tr><tr><th>Inet 10%</th><td>67.8</td><td>72.2</td><td>4.4</td><td>59.3</td><td>69.1</td><td>9.8</td></tr><tr><th>Inet 1%</th><td>55.1</td><td>64.5</td><td>9.4</td><td>47.2</td><td>61.3</td><td>14.1</td></tr><tr><th>Pl. 10%</th><td>53.4</td><td>52.1</td><td>-1.3</td><td>46.9</td><td>48.6</td><td>1.7</td></tr><tr><th>Pl. 1%</th><td>46.5</td><td>46.3</td><td>-0.2</td><td>39.2</td><td>41.3</td><td>2.1</td></tr><tr><th>VOC07</th><td>88.9</td><td>89.2</td><td>0.3</td><td>84.9</td><td>88.0</td><td>3.1</td></tr><tr><th>FLOWERS</th><td>95.6</td><td>96.4</td><td>0.8</td><td>87.9</td><td>89.1</td><td>1.2</td></tr><tr><th>Average \\Delta</th><td></td><td></td><td>2.4</td><td></td><td></td><td>5.6</td></tr></tbody></table>", "caption": "Table 10: k-NN and linear evaluation for ViT-S/16 and ResNet-50 pre-trained with DINO.We use ImageNet-1k [60] (\u201cInet\u201d), Places205 [84], PASCAL VOC [24] and Oxford-102 flowers (\u201cFLOWERS\u201d) [46].ViT trained with DINO provides features that are particularly k-NN friendly.", "list_citation_info": ["[46] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, 2008.", "[60] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. IJCV, 2015.", "[84] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. In NeurIPS, 2014.", "[24] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010."]}, {"table": "<table><tbody><tr><td colspan=\"2\">Pretraining</td><td></td><td></td><td></td></tr><tr><td>method</td><td>data</td><td>res.</td><td>tr. proc.</td><td>Top-1</td></tr><tr><td colspan=\"3\">Pretrain on additional data</td><td></td><td></td></tr><tr><td>MMP</td><td>JFT-300M</td><td>384</td><td>[19]</td><td>79.9</td></tr><tr><td>Supervised</td><td>JFT-300M</td><td>384</td><td>[19]</td><td>84.2</td></tr><tr><td colspan=\"3\">Train with additional model</td><td></td><td></td></tr><tr><td>Rand. init.</td><td>-</td><td>224</td><td>[69]</td><td>83.4</td></tr><tr><td colspan=\"3\">No additional data nor model</td><td></td><td></td></tr><tr><td>Rand. init.</td><td>-</td><td>224</td><td>[19]</td><td>77.9</td></tr><tr><td>Rand. init.</td><td>-</td><td>224</td><td>[69]</td><td>81.8</td></tr><tr><td>Supervised</td><td>ImNet</td><td>224</td><td>[69]</td><td>81.9</td></tr><tr><td>DINO</td><td>ImNet</td><td>224</td><td>[69]</td><td>82.8</td></tr></tbody></table>", "caption": "Table 11: ImageNet classification with different pretraining.Top-1 accuracy on ImageNet for supervised ViT-B/16 models using different pretrainings or using an additional pretrained convnet to guide the training.The methods use different image resolution (\u201cres.\u201d) and training procedure (\u201ctr. proc.\u201d), i.e., data augmentation and optimization.\u201cMPP\u201d is Masked Patch Prediction.", "list_citation_info": ["[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. preprint arXiv:2012.12877, 2020.", "[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. preprint arXiv:2010.11929, 2020."]}, {"table": "<table><tbody><tr><td></td><td></td><td></td><td colspan=\"2\">Top 1</td></tr><tr><td>Method</td><td>Arch</td><td>Param.</td><td>1%</td><td>10%</td></tr><tr><td colspan=\"5\">Self-supervised pretraining with finetuning</td></tr><tr><td>UDA [75]</td><td>RN50</td><td>23</td><td>\u2013</td><td>68.1</td></tr><tr><td>SimCLRv2 [13]</td><td>RN50</td><td>23</td><td>57.9</td><td>68.4</td></tr><tr><td>BYOL [30]</td><td>RN50</td><td>23</td><td>53.2</td><td>68.8</td></tr><tr><td>SwAV [10]</td><td>RN50</td><td>23</td><td>53.9</td><td>70.2</td></tr><tr><td>SimCLRv2 [16]</td><td>RN50w4</td><td>375</td><td>63.0</td><td>74.4</td></tr><tr><td>BYOL [30]</td><td>RN200w2</td><td>250</td><td>71.2</td><td>77.7</td></tr><tr><td colspan=\"5\">Semi-supervised methods</td></tr><tr><td>SimCLRv2+KD [13]</td><td>RN50</td><td>23</td><td>60.0</td><td>70.5</td></tr><tr><td>SwAV+CT [3]</td><td>RN50</td><td>23</td><td>\u2013</td><td>70.8</td></tr><tr><td>FixMatch [64]</td><td>RN50</td><td>23</td><td>\u2013</td><td>71.5</td></tr><tr><td>MPL [49]</td><td>RN50</td><td>23</td><td>\u2013</td><td>73.9</td></tr><tr><td>SimCLRv2+KD [13]</td><td>RN152w3+SK</td><td>794</td><td>76.6</td><td>80.9</td></tr><tr><td colspan=\"5\">Frozen self-supervised features</td></tr><tr><td>DINO -frozen</td><td>ViT-S/16</td><td>21</td><td>64.5</td><td>72.2</td></tr></tbody></table>", "caption": "Table 12: Low-shot learning on ImageNet with frozen ViT features.We train a logistic regression on frozen features (frozen).Note that this frozen evaluation is performed without any finetuning nor data augmentation.We report top-1 accuracy.For reference, we show previously published results that uses finetuning and semi-supervised learning.", "list_citation_info": ["[30] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020.", "[75] Qizhe Xie, Zihang Dai Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data augmentation for consistency training. preprint arXiv:1904.12848, 2020.", "[13] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In NeurIPS, 2020.", "[49] Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc V Le. Meta pseudo labels. preprint arXiv:2003.10580, 2020.", "[3] Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, and Michael Rabbat. Recovering petaflops in contrastive semi-supervised learning of visual representations. preprint arXiv:2006.10803, 2020.", "[64] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In NeurIPS, 2020.", "[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020.", "[16] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. preprint arXiv:2011.10566, 2020."]}]}