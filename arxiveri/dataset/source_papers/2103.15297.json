{"title": "Lidar r-cnn: An efficient and universal 3d object detector", "abstract": "LiDAR-based 3D detection in point cloud is essential in the perception system of autonomous driving. In this paper, we present LiDAR R-CNN, a second stage detector that can generally improve any existing 3D detector. To fulfill the real-time and high precision requirement in practice, we resort to point-based approach other than the popular voxel-based approach. However, we find an overlooked issue in previous work: Naively applying point-based methods like PointNet could make the learned features ignore the size of proposals. To this end, we analyze this problem in detail and propose several methods to remedy it, which bring significant performance improvement. Comprehensive experimental results on real-world datasets like Waymo Open Dataset (WOD) and KITTI dataset with various popular detectors demonstrate the universality and superiority of our LiDAR R-CNN. In particular, based on one variant of PointPillars, our method could achieve new state-of-the-art results with minor cost. Codes will be released at https://github.com/tusimple/LiDAR_RCNN .", "authors": ["Zhichao Li", " Feng Wang", " Naiyan Wang"], "pdf_url": "https://arxiv.org/abs/2103.15297", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th></th><td colspan=\"4\">3D AP (IoU=0.7)</td><td colspan=\"4\">3D APH (IoU=0.7)</td><td colspan=\"4\">BEV AP (IoU=0.7)</td><td colspan=\"4\">BEV APH (IoU=0.7)</td></tr><tr><th rowspan=\"-2\">Difficulty</th><th rowspan=\"-2\">Method</th><td>Overall</td><td>0-30m</td><td>30-50m</td><td>50m-Inf</td><td>Overall</td><td>0-30m</td><td>30-50m</td><td>50m-Inf</td><td>Overall</td><td>0-30m</td><td>30-50m</td><td>50m-Inf</td><td>Overall</td><td>0-30m</td><td>30-50m</td><td>50m-Inf</td></tr><tr><th>LEVEL_1</th><th>StarNet [22]</th><td>53.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th></th><th>PointPillar [12]</th><td>56.6</td><td>81.0</td><td>51.8</td><td>27.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>75.6</td><td>92.1</td><td>74.1</td><td>55.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th></th><th>MVF [51]</th><td>62.9</td><td>86.3</td><td>60.0</td><td>36.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>80.4</td><td>93.6</td><td>79.2</td><td>63.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th></th><th>Pillar-od [39]</th><td>69.8</td><td>88.5</td><td>66.5</td><td>42.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>87.1</td><td>95.8</td><td>84.7</td><td>72.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th></th><th>PV-RCNN [29]</th><td>70.3</td><td>91.9</td><td>69.2</td><td>42.2</td><td>69.7</td><td>91.3</td><td>68.5</td><td>41.3</td><td>83.0</td><td>97.4</td><td>83.0</td><td>65.0</td><td>82.1</td><td>96.7</td><td>82.0</td><td>63.2</td></tr><tr><th></th><th>RCD  [2]</th><td>66.4</td><td>86.6</td><td>65.6</td><td>40.0</td><td>66.1</td><td>86.3</td><td>65.3</td><td>39.9</td><td>82.1</td><td>93.3</td><td>80.9</td><td>67.2</td><td>81.4</td><td>92.8</td><td>80.2</td><td>66.2</td></tr><tr><th></th><th>RV first stage</th><td>53.4</td><td>73.0</td><td>49.0</td><td>28.1</td><td>52.8</td><td>72.3</td><td>48.4</td><td>27.8</td><td>70.5</td><td>86.2</td><td>68.5</td><td>51.9</td><td>69.5</td><td>85.2</td><td>67.6</td><td>51.0</td></tr><tr><th></th><th>LiDAR R-CNN (rv)</th><td>68.7</td><td>84.8</td><td>67.6</td><td>47.3</td><td>68.2</td><td>84.3</td><td>67.1</td><td>46.6</td><td>81.1</td><td>90.5</td><td>80.5</td><td>68.9</td><td>80.4</td><td>89.9</td><td>79.8</td><td>67.8</td></tr><tr><th></th><th>PointPillars*</th><td>72.1</td><td>88.3</td><td>69.9</td><td>48.0</td><td>71.5</td><td>87.8</td><td>69.3</td><td>47.3</td><td>87.9</td><td>96.6</td><td>87.1</td><td>78.1</td><td>87.1</td><td>96.0</td><td>86.2</td><td>76.5</td></tr><tr><th></th><th>LiDAR R-CNN (pp)</th><td>75.6</td><td>92.1</td><td>74.3</td><td>53.3</td><td>75.1</td><td>91.6</td><td>73.8</td><td>52.6</td><td>88.2</td><td>97.1</td><td>87.6</td><td>78.3</td><td>87.5</td><td>96.6</td><td>86.9</td><td>76.8</td></tr><tr><th></th><th>LiDAR R-CNN (2x)</th><td>75.6</td><td>91.9</td><td>74.2</td><td>53.5</td><td>75.1</td><td>91.5</td><td>73.6</td><td>52.7</td><td>90.0</td><td>97.0</td><td>89.4</td><td>78.5</td><td>89.2</td><td>96.6</td><td>88.6</td><td>77.0</td></tr><tr><th></th><th>LiDAR R-CNN (2xc)</th><td>76.0</td><td>92.1</td><td>74.6</td><td>54.5</td><td>75.5</td><td>91.6</td><td>74.1</td><td>53.4</td><td>90.1</td><td>97.0</td><td>89.5</td><td>78.9</td><td>89.3</td><td>96.5</td><td>88.6</td><td>77.4</td></tr><tr><th>LEVEL_2</th><th>PV-RCNN [29]</th><td>65.4</td><td>91.6</td><td>65.1</td><td>36.5</td><td>64.8</td><td>91.0</td><td>64.5</td><td>35.7</td><td>77.5</td><td>94.6</td><td>80.4</td><td>55.4</td><td>76.6</td><td>94.0</td><td>79.4</td><td>53.8</td></tr><tr><th></th><th>RV first stage</th><td>46.0</td><td>70.6</td><td>44.1</td><td>21.0</td><td>45.5</td><td>69.8</td><td>43.6</td><td>20.7</td><td>63.5</td><td>85.5</td><td>62.7</td><td>41.1</td><td>62.7</td><td>84.5</td><td>61.9</td><td>40.3</td></tr><tr><th></th><th>LiDAR R-CNN (rv)</th><td>60.1</td><td>84.1</td><td>61.8</td><td>35.7</td><td>59.7</td><td>83.6</td><td>61.3</td><td>35.2</td><td>74.2</td><td>89.8</td><td>74.7</td><td>54.8</td><td>73.5</td><td>89.2</td><td>74.0</td><td>53.9</td></tr><tr><th></th><th>PointPillars*</th><td>63.6</td><td>87.4</td><td>62.9</td><td>37.2</td><td>63.1</td><td>86.9</td><td>62.3</td><td>36.7</td><td>81.3</td><td>94.0</td><td>81.7</td><td>65.5</td><td>80.4</td><td>93.5</td><td>80.8</td><td>64.1</td></tr><tr><th></th><th>LiDAR R-CNN (pp)</th><td>66.5</td><td>89.1</td><td>68.3</td><td>40.8</td><td>66.1</td><td>88.7</td><td>67.7</td><td>40.3</td><td>81.2</td><td>94.2</td><td>81.8</td><td>63.6</td><td>80.5</td><td>93.7</td><td>81.0</td><td>62.3</td></tr><tr><th></th><th>LiDAR R-CNN (2x)</th><td>68.0</td><td>91.1</td><td>68.1</td><td>41.1</td><td>67.6</td><td>90.7</td><td>67.6</td><td>40.5</td><td>81.6</td><td>94.3</td><td>82.2</td><td>65.6</td><td>80.9</td><td>93.9</td><td>81.4</td><td>64.2</td></tr><tr><th></th><th>LiDAR R-CNN (2xc)</th><td>68.3</td><td>91.3</td><td>68.5</td><td>42.4</td><td>67.9</td><td>90.9</td><td>68.0</td><td>41.8</td><td>81.7</td><td>94.3</td><td>82.3</td><td>65.8</td><td>81.0</td><td>93.9</td><td>81.5</td><td>64.5</td></tr></tbody></table>", "caption": "Table 1: Vehicle detection results on Waymo Open Dataset validation sequences. Here PointPillars [12] is reproduced by  [51] and PointPillar* is implemented in mmdetection3d. Our LiDAR R-CNN (pp) results are based on PointPillars*\u2019 proposals. LiDAR R-CNN (2x) means we double the channels in points encoding network and LiDAR R-CNN (2xc) means we cascade another stage on the LiDAR R-CNN (2x)\u2019s outputs. The RV first stage is reproduced by ourselves following [2], but without the Range Conditioned Covolution operator because it is not open-sourced.", "list_citation_info": ["[12] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019.", "[39] Yue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Caroline Pantofaru, Tom Funkhouser, and Justin Solomon. Pillar-based object detection for autonomous driving. arXiv preprint arXiv:2007.10323, 2020.", "[51] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Vasudevan. End-to-end multi-view fusion for 3D object detection in LiDAR point clouds. In CoRL, 2020.", "[2] Alex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, and Cristian Sminchisescu. Range conditioned dilated convolutions for scale invariant 3D object detection. arXiv preprint arXiv:2005.09927, 2020.", "[29] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. PV-RCNN: Point-voxel feature set abstraction for 3D object detection. In CVPR, 2020.", "[22] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang, Yuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Alsharif, Patrick Nguyen, et al. StarNet: Targeted computation for object detection in point clouds. arXiv preprint arXiv:1908.11069, 2019."]}, {"table": "<table><tbody><tr><th></th><th></th><td colspan=\"2\">vehicle (IoU=0.7)</td><td colspan=\"2\">pedestrian (IoU=0.5)</td><td colspan=\"2\">cyclist (IoU=0.5)</td></tr><tr><th rowspan=\"-2\">Difficulty</th><th rowspan=\"-2\">Method</th><td>3D AP</td><td>3D APH</td><td>3D AP</td><td>3D APH</td><td>3D AP</td><td>3D APH</td></tr><tr><th>LEVEL_1</th><th>SECOND [44]</th><td>58.5</td><td>57.9</td><td>63.9</td><td>54.9</td><td>48.6</td><td>47.6</td></tr><tr><th></th><th>LiDAR R-CNN (sec)</th><td>62.6</td><td>62.1</td><td>68.2</td><td>59.5</td><td>52.8</td><td>51.6</td></tr><tr><th></th><th>PointPillars [12]</th><td>71.6</td><td>71.0</td><td>70.6</td><td>56.7</td><td>64.4</td><td>62.3</td></tr><tr><th></th><th>LiDAR R-CNN (pp)</th><td>73.4</td><td>72.9</td><td>70.6</td><td>57.8</td><td>66.8</td><td>64.8</td></tr><tr><th></th><th>LiDAR R-CNN (2x)</th><td>73.5</td><td>73.0</td><td>71.2</td><td>58.7</td><td>68.6</td><td>66.9</td></tr><tr><th>LEVEL_2</th><th>SECOND [44]</th><td>51.6</td><td>51.1</td><td>56.0</td><td>48.0</td><td>46.8</td><td>45.8</td></tr><tr><th></th><th>LiDAR R-CNN (sec)</th><td>54.5</td><td>54.0</td><td>59.3</td><td>51.7</td><td>50.9</td><td>49.7</td></tr><tr><th></th><th>PointPillars [12]</th><td>63.1</td><td>62.5</td><td>62.9</td><td>50.2</td><td>61.9</td><td>59.9</td></tr><tr><th></th><th>LiDAR R-CNN (pp)</th><td>64.6</td><td>64.1</td><td>62.5</td><td>50.9</td><td>64.3</td><td>62.4</td></tr><tr><th></th><th>LiDAR R-CNN (2x)</th><td>64.7</td><td>64.2</td><td>63.1</td><td>51.7</td><td>66.1</td><td>64.4</td></tr></tbody></table>", "caption": "Table 2: Multi-class 3D detection results on Waymo Open Dataset validation sequences. Both PointPillars [12] and SECOND [44] baselines are implemented in mmdetection3d. 2x means we double the channels in points encoding network.", "list_citation_info": ["[12] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019.", "[44] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018."]}, {"table": "<table><tbody><tr><th>Methods</th><td>3D AP@70</td><td>BEV AP@70</td></tr><tr><th>PointPillars [12]</th><td>71.6</td><td>87.1</td></tr><tr><th>PointNet refinement</th><td>74.1</td><td>87.9</td></tr><tr><th>voxel</th><td>72.9</td><td>87.2</td></tr><tr><th>anchor</th><td>75.2</td><td>88.2</td></tr><tr><th>size normalization</th><td>75.4</td><td>88.1</td></tr><tr><th>virtual point</th><td>75.4</td><td>88.1</td></tr><tr><th>boundary offset</th><td>75.6</td><td>88.3</td></tr></tbody></table>", "caption": "Table 4: Ablation studies for different scale-aware methods proposed in \\secrefsubsec:Problem on Waymo Open Dataset vehicle detection. The baseline uses a basic PointNet with [x,y,z] inputs as second-stage on PointPillars. ", "list_citation_info": ["[12] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019."]}, {"table": "<table><tbody><tr><th>Methods</th><td>vehicle</td><td>pedestrian</td><td>cyclist</td></tr><tr><th>PointPillars [12]</th><td>71.6</td><td>70.6</td><td>64.4</td></tr><tr><th>PointNet refinement</th><td>72.1</td><td>69.2</td><td>62.2</td></tr><tr><th>voxel</th><td>72.1</td><td>69.8</td><td>64.5</td></tr><tr><th>anchor</th><td>72.5</td><td>70.2</td><td>63.5</td></tr><tr><th>size normalization</th><td>72.7</td><td>69.9</td><td>64.4</td></tr><tr><th>virtual point</th><td>73.3</td><td>70.4</td><td>66.2</td></tr><tr><th>boundary offset</th><td>73.4</td><td>70.6</td><td>66.8</td></tr></tbody></table>", "caption": "Table 5: 3D AP results on WOD with three classes trained in one model.", "list_citation_info": ["[12] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019."]}, {"table": "<table><thead><tr><th>Methods</th><th>3D AP@70</th><th>BEV AP@70</th></tr></thead><tbody><tr><th>RV baseline</th><td>53.4</td><td>70.5</td></tr><tr><th>RV + voxel</th><td>64.1</td><td>76.7</td></tr><tr><th>RV+ LiDAR R-CNN</th><td>68.7</td><td>81.1</td></tr></tbody></table>", "caption": "Table 6: Comparison between voxel-based second-stage and proposed LiDAR R-CNN on a range-view-based detector (RV). The RV baseline is re-implemented from the baseline of RCD [2]. The RV+LiDAR R-CNN replaces the second-stage with our method. ", "list_citation_info": ["[2] Alex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, and Cristian Sminchisescu. Range conditioned dilated convolutions for scale invariant 3D object detection. arXiv preprint arXiv:2005.09927, 2020."]}, {"table": "<table><thead><tr><th>Methods</th><th>vehicle</th><th>pedestrian</th><th>cyclist</th></tr></thead><tbody><tr><th>SECOND [44]</th><td>58.5</td><td>63.9</td><td>48.6</td></tr><tr><th>LiDAR R-CNN (sec)</th><td>62.6</td><td>68.2</td><td>52.8</td></tr><tr><th>LiDAR R-CNN (pp)</th><td>62.1</td><td>67.2</td><td>52.9</td></tr></tbody></table>", "caption": "Table 7: We show generalization of our LiDAR R-CNN in this Table. LiDAR R-CNN (sec) use the outputs from SECOND for training and testing. LiDAR R-CNN (pp) trains with PointPillars\u2019 outputs while inferencing on SECOND without finetuning.", "list_citation_info": ["[44] Yan Yan, Yuxing Mao, and Bo Li. SECOND: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018."]}]}