{"title": "GLaM: Efficient scaling of language models with mixture-of-experts", "abstract": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.", "authors": ["Nan Du", " Yanping Huang", " Andrew M. Dai", " Simon Tong", " Dmitry Lepikhin", " Yuanzhong Xu", " Maxim Krikun", " Yanqi Zhou", " Adams Wei Yu", " Orhan Firat", " Barret Zoph", " Liam Fedus", " Maarten Bosma", " Zongwei Zhou", " Tao Wang", " Yu Emma Wang", " Kellie Webster", " Marie Pellat", " Kevin Robinson", " Kathleen Meier-Hellstern", " Toju Duke", " Lucas Dixon", " Kun Zhang", " Quoc V Le", " Yonghui Wu ", " et al. (2 additional authors not shown)"], "pdf_url": "https://arxiv.org/abs/2112.06905", "list_table_and_caption": [{"table": "<table><tr><td><p>Model Name</p></td><td><p>Model Type</p></td><td><p>n_{\\text{params}}</p></td><td><p>n_{\\text{act-params}}</p></td></tr><tr><td><p>BERT</p></td><td><p>Dense Encoder-only</p></td><td><p>340M</p></td><td><p>340M</p></td></tr><tr><td><p>T5</p></td><td><p>Dense Encoder-decoder</p></td><td><p>13B</p></td><td><p>13B</p></td></tr><tr><td><p>GPT-3</p></td><td><p>Dense Decoder-only</p></td><td><p>175B</p></td><td><p>175B</p></td></tr><tr><td><p>Jurassic-1</p></td><td><p>Dense Decoder-only</p></td><td><p>178B</p></td><td><p>178B</p></td></tr><tr><td><p>Gopher</p></td><td><p>Dense Decoder-only</p></td><td><p>280B</p></td><td><p>280B</p></td></tr><tr><td><p>Megatron-530B</p></td><td><p>Dense Decoder-only</p></td><td><p>530B</p></td><td><p>530B</p></td></tr><tr><td><p>GShard-M4</p></td><td><p>MoE Encoder-decoder</p></td><td><p>600B</p></td><td><p>1.5B</p></td></tr><tr><td><p>Switch-C</p></td><td><p>MoE Encoder-decoder</p></td><td><p>1.5T</p></td><td><p>1.5B</p></td></tr><tr><td><p>GLaM (64B/64E)</p></td><td><p>MoE Decoder-only</p></td><td><p>1.2T</p></td><td><p>96.6B</p></td></tr></table>", "caption": "Table 2: A sample of related models (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Shoeybi et al., 2019; Lepikhin et al., 2021; Fedus et al., 2021) pre-trained on text corpora. n_{\\text{params}} is the total number of trainable model parameters, n_{\\text{act-params}} is the number of activated model parameters per input token.", "list_citation_info": ["Devlin et al. (2019) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019."]}, {"table": "<table><tr><td>Model</td><td> TriviaQA(Open-Domain) </td></tr><tr><td>KG-FiD (large) (Yu et al., 2022)(finetuned, test) </td><td>69.8</td></tr><tr><td>Switch-C (finetuned, dev)</td><td>47.5</td></tr><tr><td>GPT-3 One-shot (dev)</td><td>68.0</td></tr><tr><td>GPT-3 64-shot (test)</td><td>71.2</td></tr><tr><td>GLaM One-shot (test)</td><td>75.0</td></tr><tr><td>GLaM One-shot (dev)</td><td>75.8</td></tr></table>", "caption": "Table 5: GLaM (64B/64E) one-shot performance significantly outperforms prior SOTAs for open domain settings in the wiki split.", "list_citation_info": ["Yu et al. (2022) Yu, D., Zhu, C., Fang, Y., Yu, W., Wang, S., Xu, Y., Ren, X., Yang, Y., and Zeng, M. KG-FiD: Infusing knowledge graph in fusion-in-decoder for open-domain question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4961\u20134974, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.340. URL https://aclanthology.org/2022.acl-long.340."]}]}