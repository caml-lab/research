{"title": "Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection", "abstract": "RGB-D salient object detection (SOD) recently has attracted increasing research interest by benefiting conventional RGB SOD with extra depth information. However, existing RGB-D SOD models often fail to perform well in terms of both efficiency and accuracy, which hinders their potential applications on mobile devices and real-world problems. An underlying challenge is that the model accuracy usually degrades when the model is simplified to have few parameters. To tackle this dilemma and also inspired by the fact that depth quality is a key factor influencing the accuracy, we propose a novel depth quality-inspired feature manipulation (DQFM) process, which is efficient itself and can serve as a gating mechanism for filtering depth features to greatly boost the accuracy. DQFM resorts to the alignment of low-level RGB and depth features, as well as holistic attention of the depth stream to explicitly control and enhance cross-modal fusion. We embed DQFM to obtain an efficient light-weight model called DFM-Net, where we also design a tailored depth backbone and a two-stage decoder for further efficiency consideration. Extensive experimental results demonstrate that our DFM-Net achieves state-of-the-art accuracy when comparing to existing non-efficient models, and meanwhile runs at 140ms on CPU (2.2$\\times$ faster than the prior fastest efficient model) with only $\\sim$8.5Mb model size (14.9% of the prior lightest). Our code will be available at https://github.com/zwbx/DFM-Net.", "authors": ["Wenbo Zhang", " Ge-Peng Ji", " Zhuo Wang", " Keren Fu", " Qijun Zhao"], "pdf_url": "https://arxiv.org/abs/2107.01779", "list_table_and_caption": [{"table": "<table><thead><tr><th>Input</th><th>Output</th><th>Block</th><th>t</th><th>c</th><th>n</th><th>s</th></tr></thead><tbody><tr><td>256\\times 256\\times 1</td><td>128\\times 128\\times 16</td><td>IRB</td><td>3</td><td>16</td><td>1</td><td>2</td></tr><tr><td>128\\times 128\\times 16</td><td>64\\times 64\\times 24</td><td>IRB</td><td>3</td><td>24</td><td>3</td><td>2</td></tr><tr><td>64\\times 64\\times 24</td><td>32\\times 32\\times 32</td><td>IRB</td><td>3</td><td>32</td><td>7</td><td>2</td></tr><tr><td>32\\times 32\\times 32</td><td>16\\times 16\\times 96</td><td>IRB</td><td>2</td><td>96</td><td>3</td><td>2</td></tr><tr><td>16\\times 16\\times 96</td><td>16\\times 16\\times 320</td><td>IRB</td><td>2</td><td>320</td><td>1</td><td>1</td></tr></tbody></table>", "caption": "Table 1. Detailed structure of the proposed tailored depth backbone (TDB), which is based on inverted residual bottleneck blocks (IRB) of MobileNet-V2 (Sandler et al., 2018). About notations, t: expansion factor of IRB, c: output channels, n: times the block is repeated, and s: stride of hierarchy, which is applied to the first block of the repeating blocks.", "list_citation_info": ["Sandler et al. (2018) Mark Sandler, A. Howard, Menglong Zhu, A. Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. In CVPR. 4510\u20134520."]}, {"table": "<table><tbody><tr><td></td><td rowspan=\"3\">Metric</td><td>PCF</td><td>MMCI</td><td>CPFP</td><td>DMRA</td><td>D3Net</td><td>JL-DCF</td><td>UCNet</td><td>SSF</td><td>S2MA</td><td>CoNet</td><td>cmMS</td><td>DANet</td><td>ATSA</td><td>DFM-Net{}^{*}</td><td>A2dele</td><td>PGAR</td><td>DFM-Net</td></tr><tr><td></td><td>CVPR18</td><td>PR19</td><td>CVPR19</td><td>ICCV19</td><td>TNNLS20</td><td>CVPR20</td><td>CVPR20</td><td>CVPR20</td><td>CVPR20</td><td>ECCV20</td><td>ECCV20</td><td>ECCV20</td><td>ECCV20</td><td>Ours</td><td>CVPR20</td><td>ECCV20</td><td>Ours</td></tr><tr><td></td><td>(et al., 2018)</td><td>(Chen et al., 2019)</td><td>(Zhaoet al., 2019)</td><td>(Piaoet al., 2019)</td><td>(Fanet al., 2020b)</td><td>(Fuet al., 2020)</td><td>(Zhang et al., 2020a)</td><td>(Zhanget al., 2020c)</td><td>(et al., 2020a)</td><td>(Jiet al., 2020)</td><td>(Liet al., 2020a)</td><td>(Zhaoet al., 2020)</td><td>(Zhanget al., 2020b)</td><td>-</td><td>(Piaoet al., 2020)</td><td>(et al., 2020b)</td><td>-</td></tr><tr><td rowspan=\"3\"></td><td>Size (Mb)\\downarrow</td><td>534</td><td>930</td><td>278</td><td>228</td><td>530</td><td>520</td><td>119</td><td>125</td><td>330</td><td>167</td><td>430</td><td>102</td><td>123</td><td>93</td><td>57</td><td>62</td><td>8.5</td></tr><tr><td>CPU (ms)\\downarrow</td><td>35762</td><td>46886</td><td>16946</td><td>1521</td><td>851</td><td>7588</td><td>1011</td><td>650</td><td>1900</td><td>667</td><td>946</td><td>1147</td><td>1344</td><td>357</td><td>313</td><td>755</td><td>140</td></tr><tr><td>GPU (FPS)\\uparrow</td><td>5</td><td>8</td><td>6</td><td>13</td><td>32</td><td>9</td><td>42</td><td>21</td><td>25</td><td>36</td><td>15</td><td>32</td><td>29</td><td>70</td><td>120</td><td>61</td><td>64</td></tr><tr><td rowspan=\"4\">SIP</td><td>S_{\\alpha}\\uparrow</td><td>0.842</td><td>0.833</td><td>0.850</td><td>0.806</td><td>0.860</td><td>0.879</td><td>0.875</td><td>0.874</td><td>0.878</td><td>0.858</td><td>0.867</td><td>0.878</td><td>0.864</td><td>0.885</td><td>0.829</td><td>0.875</td><td>0.883</td></tr><tr><td>F_{\\beta}^{\\rm max}\\uparrow</td><td>0.838</td><td>0.818</td><td>0.851</td><td>0.821</td><td>0.861</td><td>0.885</td><td>0.879</td><td>0.880</td><td>0.884</td><td>0.867</td><td>0.871</td><td>0.884</td><td>0.873</td><td>0.890</td><td>0.834</td><td>0.877</td><td>0.887</td></tr><tr><td>E_{\\xi}^{\\rm max}\\uparrow</td><td>0.901</td><td>0.897</td><td>0.903</td><td>0.875</td><td>0.909</td><td>0.923</td><td>0.919</td><td>0.921</td><td>0.920</td><td>0.913</td><td>0.091</td><td>0.920</td><td>0.911</td><td>0.926</td><td>0.889</td><td>0.914</td><td>0.926</td></tr><tr><td>\\mathcal{M}\\downarrow</td><td>0.071</td><td>0.086</td><td>0.064</td><td>0.085</td><td>0.063</td><td>0.051</td><td>0.051</td><td>0.053</td><td>0.054</td><td>0.063</td><td>0.061</td><td>0.054</td><td>0.058</td><td>0.049</td><td>0.070</td><td>0.059</td><td>0.051</td></tr><tr><td rowspan=\"4\">NLPR</td><td>S_{\\alpha}\\uparrow</td><td>0.874</td><td>0.856</td><td>0.888</td><td>0.899</td><td>0.912</td><td>0.925</td><td>0.920</td><td>0.914</td><td>0.915</td><td>0.908</td><td>0.915</td><td>0.915</td><td>0.907</td><td>0.926</td><td>0.890</td><td>0.918</td><td>0.923</td></tr><tr><td>F_{\\beta}^{\\rm max}\\uparrow</td><td>0.841</td><td>0.815</td><td>0.867</td><td>0.879</td><td>0.897</td><td>0.916</td><td>0.903</td><td>0.896</td><td>0.902</td><td>0.887</td><td>0.896</td><td>0.903</td><td>0.876</td><td>0.912</td><td>0.875</td><td>0.898</td><td>0.908</td></tr><tr><td>E_{\\xi}^{\\rm max}\\uparrow</td><td>0.925</td><td>0.913</td><td>0.932</td><td>0.947</td><td>0.953</td><td>0.962</td><td>0.956</td><td>0.953</td><td>0.950</td><td>0.945</td><td>0.949</td><td>0.953</td><td>0.945</td><td>0.961</td><td>0.937</td><td>0.948</td><td>0.957</td></tr><tr><td>\\mathcal{M}\\downarrow</td><td>0.044</td><td>0.059</td><td>0.036</td><td>0.031</td><td>0.025</td><td>0.022</td><td>0.025</td><td>0.026</td><td>0.030</td><td>0.031</td><td>0.027</td><td>0.029</td><td>0.028</td><td>0.024</td><td>0.031</td><td>0.028</td><td>0.026</td></tr><tr><td rowspan=\"4\">NJU2K</td><td>S_{\\alpha}\\uparrow</td><td>0.877</td><td>0.858</td><td>0.879</td><td>0.886</td><td>0.900</td><td>0.903</td><td>0.897</td><td>0.899</td><td>0.894</td><td>0.895</td><td>0.900</td><td>0.891</td><td>0.901</td><td>0.912</td><td>0.868</td><td>0.906</td><td>0.906</td></tr><tr><td>F_{\\beta}^{\\rm max}\\uparrow</td><td>0.872</td><td>0.852</td><td>0.877</td><td>0.886</td><td>0.900</td><td>0.903</td><td>0.895</td><td>0.896</td><td>0.889</td><td>0.892</td><td>0.897</td><td>0.880</td><td>0.893</td><td>0.913</td><td>0.872</td><td>0.905</td><td>0.910</td></tr><tr><td>E_{\\xi}^{\\rm max}\\uparrow</td><td>0.924</td><td>0.915</td><td>0.926</td><td>0.927</td><td>0.950</td><td>0.944</td><td>0.936</td><td>0.935</td><td>0.930</td><td>0.937</td><td>0.936</td><td>0.932</td><td>0.921</td><td>0.950</td><td>0.914</td><td>0.940</td><td>0.947</td></tr><tr><td>\\mathcal{M}\\downarrow</td><td>0.059</td><td>0.079</td><td>0.053</td><td>0.051</td><td>0.041</td><td>0.043</td><td>0.043</td><td>0.043</td><td>0.053</td><td>0.047</td><td>0.044</td><td>0.048</td><td>0.040</td><td>0.039</td><td>0.052</td><td>0.045</td><td>0.042</td></tr><tr><td rowspan=\"4\">RGBD135</td><td>S_{\\alpha}\\uparrow</td><td>0.842</td><td>0.848</td><td>0.872</td><td>0.900</td><td>0.898</td><td>0.929</td><td>0.934</td><td>0.905</td><td>0.941</td><td>0.910</td><td>0.932</td><td>0.904</td><td>0.907</td><td>0.938</td><td>0.884</td><td>0.894</td><td>0.931</td></tr><tr><td>F_{\\beta}^{\\rm max}\\uparrow</td><td>0.804</td><td>0.822</td><td>0.846</td><td>0.888</td><td>0.885</td><td>0.919</td><td>0.930</td><td>0.883</td><td>0.935</td><td>0.896</td><td>0.922</td><td>0.894</td><td>0.885</td><td>0.932</td><td>0.873</td><td>0.879</td><td>0.922</td></tr><tr><td>E_{\\xi}^{\\rm max}\\uparrow</td><td>0.893</td><td>0.928</td><td>0.923</td><td>0.943</td><td>0.946</td><td>0.968</td><td>0.976</td><td>0.941</td><td>0.973</td><td>0.945</td><td>0.970</td><td>0.957</td><td>0.952</td><td>0.973</td><td>0.920</td><td>0.929</td><td>0.972</td></tr><tr><td>\\mathcal{M}\\downarrow</td><td>0.049</td><td>0.065</td><td>0.038</td><td>0.030</td><td>0.031</td><td>0.022</td><td>0.019</td><td>0.025</td><td>0.021</td><td>0.029</td><td>0.020</td><td>0.029</td><td>0.024</td><td>0.019</td><td>0.030</td><td>0.032</td><td>0.021</td></tr><tr><td rowspan=\"4\">LFSD</td><td>S_{\\alpha}\\uparrow</td><td>0.786</td><td>0.787</td><td>0.828</td><td>0.839</td><td>0.825</td><td>0.862</td><td>0.864</td><td>0.859</td><td>0.837</td><td>0.862</td><td>0.849</td><td>0.845</td><td>0.865</td><td>0.870</td><td>0.834</td><td>0.833</td><td>0.865</td></tr><tr><td>F_{\\beta}^{\\rm max}\\uparrow</td><td>0.775</td><td>0.771</td><td>0.826</td><td>0.852</td><td>0.810</td><td>0.866</td><td>0.864</td><td>0.867</td><td>0.835</td><td>0.859</td><td>0.869</td><td>0.846</td><td>0.862</td><td>0.866</td><td>0.832</td><td>0.831</td><td>0.864</td></tr><tr><td>E_{\\xi}^{\\rm max}\\uparrow</td><td>0.827</td><td>0.839</td><td>0.863</td><td>0.893</td><td>0.862</td><td>0.901</td><td>0.905</td><td>0.900</td><td>0.873</td><td>0.907</td><td>0.896</td><td>0.886</td><td>0.905</td><td>0.903</td><td>0.874</td><td>0.893</td><td>0.903</td></tr><tr><td>\\mathcal{M}\\downarrow</td><td>0.119</td><td>0.132</td><td>0.088</td><td>0.083</td><td>0.095</td><td>0.071</td><td>0.066</td><td>0.066</td><td>0.094</td><td>0.071</td><td>0.074</td><td>0.083</td><td>0.064</td><td>0.068</td><td>0.077</td><td>0.093</td><td>0.072</td></tr><tr><td rowspan=\"4\">STERE</td><td>S_{\\alpha}\\uparrow</td><td>0.875</td><td>0.873</td><td>0.879</td><td>0.835</td><td>0.899</td><td>0.905</td><td>0.903</td><td>0.893</td><td>0.890</td><td>0.908</td><td>0.895</td><td>0.892</td><td>0.897</td><td>0.908</td><td>0.885</td><td>0.903</td><td>0.898</td></tr><tr><td>F_{\\beta}^{\\rm max}\\uparrow</td><td>0.860</td><td>0.863</td><td>0.874</td><td>0.847</td><td>0.891</td><td>0.901</td><td>0.899</td><td>0.890</td><td>0.882</td><td>0.904</td><td>0.891</td><td>0.881</td><td>0.884</td><td>0.904</td><td>0.885</td><td>0.893</td><td>0.893</td></tr><tr><td>E_{\\xi}^{\\rm max}\\uparrow</td><td>0.925</td><td>0.927</td><td>0.925</td><td>0.911</td><td>0.938</td><td>0.946</td><td>0.944</td><td>0.936</td><td>0.932</td><td>0.948</td><td>0.937</td><td>0.930</td><td>0.921</td><td>0.948</td><td>0.935</td><td>0.936</td><td>0.941</td></tr><tr><td>\\mathcal{M}\\downarrow</td><td>0.064</td><td>0.068</td><td>0.051</td><td>0.066</td><td>0.046</td><td>0.042</td><td>0.039</td><td>0.044</td><td>0.051</td><td>0.040</td><td>0.042</td><td>0.048</td><td>0.039</td><td>0.040</td><td>0.043</td><td>0.044</td><td>0.045</td></tr></tbody></table>", "caption": "Table 2. Quantitative benchmark results. \\uparrow/\\downarrow for a metric denotes that a larger/smaller value is better. Our results are highlighted in bold. The scores/numbers better than ours are underlined (efficient and non-efficient models are labeled separately).", "list_citation_info": ["et al. (2020b) Shuhan Chen et al. 2020b. Progressively Guided Alternate Refinement Network for RGB-D Salient Object Detection. In ECCV. 520\u2013538.", "Zhang et al. (2020c) Miao Zhang, Weisong Ren, Yongri Piao, Zhengkun Rong, and Huchuan Lu. 2020c. Select, Supplement and Focus for RGB-D Saliency Detection. In CVPR. 3472\u20133481.", "Fu et al. (2020) Keren Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao. 2020. JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection. In CVPR. 3052\u20133062.", "Li et al. (2020a) Chongyi Li, Runmin Cong, Piao Yongri, Xu Qianqian, and Chen Change Loy. 2020a. RGB-D salient object detection with cross-modality modulation and selection. In ECCV. 225\u2013241.", "Piao et al. (2019) Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, and Huchuan Lu. 2019. Depth-induced multi-scale recurrent attention network for saliency detection. In ICCV. 7254\u20137263.", "Fan et al. (2020b) Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. 2020b. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE TNNLS (2020).", "Zhang et al. (2020b) Miao Zhang, S. Fei, Jie Liu, Shuang Xu, Yongri Piao, and Huchuan Lu. 2020b. Asymmetric Two-Stream Architecture for Accurate RGB-D Saliency Detection. In ECCV. 374\u2013390.", "Zhao et al. (2019) Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming Cheng, Xuan-Yi Li, and Le Zhang. 2019. Contrast prior and fluid pyramid integration for RGBD salient object detection. In CVPR. 3927\u20133936.", "Piao et al. (2020) Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren, and Huchuan Lu. 2020. A2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection. In CVPR. 9060\u20139069.", "Zhang et al. (2020a) Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. 2020a. UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders. In CVPR. 8582\u20138519.", "et al. (2020a) Nian Liu et al. 2020a. Learning Selective Self-Mutual Attention for RGB-D Saliency Detection. In CVPR. 13756\u201313765.", "Ji et al. (2020) Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan Lu. 2020. Accurate RGB-D Salient Object Detection via Collaborative Learning. In ECCV. 52\u201369.", "Zhao et al. (2020) Xiaoqi Zhao, Lihe Zhang, Youwei Pang, Huchuan Lu, and Lei Zhang. 2020. A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection. In ECCV. 520\u2013538.", "Chen et al. (2019) Hao Chen, Youfu Li, and Dan Su. 2019. Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection. Pattern Recognition 86 (2019), 376\u2013385.", "et al. (2018) H. Chen et al. 2018. Progressively Complementarity-Aware Fusion Network for RGB-D Salient Object Detection. In CVPR. 3051\u20133060."]}, {"table": "<table><thead><tr><th rowspan=\"2\">#</th><th rowspan=\"2\">DQW</th><th rowspan=\"2\">DHA</th><th>Size</th><th colspan=\"4\">SIP (Fanet al., 2020b)</th><th colspan=\"4\">NLPR (Penget al., 2014)</th><th colspan=\"4\">NJU2K (Juet al., 2014)</th><th colspan=\"4\">RGBD135 (Chenget al., 2014)</th><th colspan=\"4\">LFSD (Liet al., 2014)</th><th colspan=\"4\">STERE (Niuet al., 2012)</th></tr><tr><th>(Mb)</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th></tr></thead><tbody><tr><th>1</th><th></th><th></th><th>8.409</th><td>0.849</td><td>0.842</td><td>0.897</td><td>0.070</td><td>0.907</td><td>0.884</td><td>0.946</td><td>0.032</td><td>0.890</td><td>0.887</td><td>0.931</td><td>0.052</td><td>0.929</td><td>0.915</td><td>0.968</td><td>0.025</td><td>0.847</td><td>0.841</td><td>0.883</td><td>0.084</td><td>0.880</td><td>0.872</td><td>0.931</td><td>0.054</td></tr><tr><th>2</th><th>\u2713</th><th></th><th>8.416</th><td>0.878</td><td>0.884</td><td>0.922</td><td>0.054</td><td>0.918</td><td>0.902</td><td>0.957</td><td>0.027</td><td>0.898</td><td>0.898</td><td>0.938</td><td>0.047</td><td>0.932</td><td>0.922</td><td>0.971</td><td>0.022</td><td>0.857</td><td>0.856</td><td>0.896</td><td>0.078</td><td>0.894</td><td>0.886</td><td>0.939</td><td>0.047</td></tr><tr><th>3</th><th></th><th>\u2713</th><th>8.451</th><td>0.861</td><td>0.861</td><td>0.908</td><td>0.063</td><td>0.918</td><td>0.901</td><td>0.953</td><td>0.028</td><td>0.899</td><td>0.896</td><td>0.940</td><td>0.046</td><td>0.921</td><td>0.911</td><td>0.966</td><td>0.024</td><td>0.857</td><td>0.855</td><td>0.895</td><td>0.076</td><td>0.897</td><td>0.891</td><td>0.942</td><td>0.045</td></tr><tr><th>4</th><th>\u2713</th><th>\u2713</th><th>8.459</th><td>0.883</td><td>0.887</td><td>0.926</td><td>0.051</td><td>0.923</td><td>0.908</td><td>0.957</td><td>0.026</td><td>0.906</td><td>0.910</td><td>0.947</td><td>0.042</td><td>0.931</td><td>0.922</td><td>0.972</td><td>0.021</td><td>0.865</td><td>0.864</td><td>0.903</td><td>0.072</td><td>0.898</td><td>0.893</td><td>0.941</td><td>0.045</td></tr></tbody></table>", "caption": "Table 3. Ablation analyses for DQFM, where the effectiveness of DQW and DHA is validated. Details are in Sec. 4.4.", "list_citation_info": ["Ju et al. (2014) Ran Ju, L. Ge, W. Geng, T. Ren, and Gangshan Wu. 2014. Depth saliency based on anisotropic center-surround difference. In ICIP. 1115\u20131119.", "Li et al. (2014) Nianyi Li, J. Ye, Y. Ji, Haibin Ling, and J. Yu. 2014. Saliency Detection on Light Field. IEEE TPAMI 39 (2014), 1605\u20131616.", "Fan et al. (2020b) Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. 2020b. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE TNNLS (2020).", "Niu et al. (2012) Y. Niu, Yujie Geng, X. Li, and Feng Liu. 2012. Leveraging stereopsis for saliency analysis. In CVPR. 454\u2013461.", "Peng et al. (2014) Houwen Peng, B. Li, Weihua Xiong, W. Hu, and R. Ji. 2014. RGBD Salient Object Detection: A Benchmark and Algorithms. In ECCV. 92\u2013109.", "Cheng et al. (2014) Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao. 2014. Depth enhanced saliency detection method. In ICIMCS. 23\u201327."]}, {"table": "<table><thead><tr><th rowspan=\"2\">#</th><th rowspan=\"2\">\\textbf{F}_{rec}</th><th colspan=\"4\">SIP (Fanet al., 2020b)</th><th colspan=\"4\">NLPR (Penget al., 2014)</th><th colspan=\"4\">NJU2K (Juet al., 2014)</th><th colspan=\"4\">RGBD135 (Chenget al., 2014)</th><th colspan=\"4\">LFSD (Liet al., 2014)</th><th colspan=\"4\">STERE (Niuet al., 2012)</th></tr><tr><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th></tr></thead><tbody><tr><th>5</th><th>0</th><td>0.877</td><td>0.881</td><td>0.919</td><td>0.054</td><td>0.920</td><td>0.904</td><td>0.953</td><td>0.027</td><td>0.904</td><td>0.904</td><td>0.943</td><td>0.043</td><td>0.934</td><td>0.924</td><td>0.973</td><td>0.021</td><td>0.855</td><td>0.855</td><td>0.894</td><td>0.076</td><td>0.897</td><td>0.890</td><td>0.940</td><td>0.045</td></tr><tr><th>6</th><th>1</th><td>0.872</td><td>0.873</td><td>0.918</td><td>0.056</td><td>0.920</td><td>0.904</td><td>0.957</td><td>0.027</td><td>0.903</td><td>0.902</td><td>0.943</td><td>0.043</td><td>0.929</td><td>0.922</td><td>0.973</td><td>0.023</td><td>0.864</td><td>0.865</td><td>0.902</td><td>0.072</td><td>0.900</td><td>0.893</td><td>0.942</td><td>0.044</td></tr><tr><th>4</th><th>2</th><td>0.883</td><td>0.887</td><td>0.926</td><td>0.051</td><td>0.923</td><td>0.908</td><td>0.957</td><td>0.026</td><td>0.906</td><td>0.910</td><td>0.947</td><td>0.042</td><td>0.931</td><td>0.922</td><td>0.972</td><td>0.021</td><td>0.865</td><td>0.864</td><td>0.903</td><td>0.072</td><td>0.898</td><td>0.893</td><td>0.941</td><td>0.045</td></tr><tr><th>7</th><th>3</th><td>0.864</td><td>0.868</td><td>0.916</td><td>0.059</td><td>0.921</td><td>0.9068</td><td>0.957</td><td>0.026</td><td>0.898</td><td>0.897</td><td>0.941</td><td>0.042</td><td>0.917</td><td>0.905</td><td>0.963</td><td>0.025</td><td>0.858</td><td>0.858</td><td>0.896</td><td>0.077</td><td>0.905</td><td>0.897</td><td>0.946</td><td>0.042</td></tr></tbody></table>", "caption": "Table 4. Effectiveness of the recalibration process \\textbf{F}_{rec} in DHA. The number below \u201c\\textbf{F}_{rec}\u201d in the table means the times of recalibration. Specifically, zero means that no recalibration is conducted.", "list_citation_info": ["Ju et al. (2014) Ran Ju, L. Ge, W. Geng, T. Ren, and Gangshan Wu. 2014. Depth saliency based on anisotropic center-surround difference. In ICIP. 1115\u20131119.", "Li et al. (2014) Nianyi Li, J. Ye, Y. Ji, Haibin Ling, and J. Yu. 2014. Saliency Detection on Light Field. IEEE TPAMI 39 (2014), 1605\u20131616.", "Fan et al. (2020b) Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. 2020b. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE TNNLS (2020).", "Niu et al. (2012) Y. Niu, Yujie Geng, X. Li, and Feng Liu. 2012. Leveraging stereopsis for saliency analysis. In CVPR. 454\u2013461.", "Peng et al. (2014) Houwen Peng, B. Li, Weihua Xiong, W. Hu, and R. Ji. 2014. RGBD Salient Object Detection: A Benchmark and Algorithms. In ECCV. 92\u2013109.", "Cheng et al. (2014) Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao. 2014. Depth enhanced saliency detection method. In ICIMCS. 23\u201327."]}, {"table": "<table><thead><tr><th rowspan=\"2\">#</th><th rowspan=\"2\">Strategy</th><th colspan=\"4\">SIP (Fanet al., 2020b)</th><th colspan=\"4\">NLPR (Penget al., 2014)</th><th colspan=\"4\">NJU2K (Juet al., 2014)</th><th colspan=\"4\">RGBD135 (Chenget al., 2014)</th><th colspan=\"4\">LFSD (Liet al., 2014)</th><th colspan=\"4\">STERE (Niuet al., 2012)</th></tr><tr><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th></tr></thead><tbody><tr><th>8</th><th>Identical</th><td>0.880</td><td>0.884</td><td>0.924</td><td>0.053</td><td>0.922</td><td>0.907</td><td>0.958</td><td>0.026</td><td>0.901</td><td>0.901</td><td>0.944</td><td>0.044</td><td>0.924</td><td>0.911</td><td>0.961</td><td>0.024</td><td>0.859</td><td>0.857</td><td>0.899</td><td>0.073</td><td>0.898</td><td>0.892</td><td>0.942</td><td>0.045</td></tr><tr><th>4</th><th>Multiple</th><td>0.883</td><td>0.887</td><td>0.926</td><td>0.051</td><td>0.923</td><td>0.908</td><td>0.957</td><td>0.026</td><td>0.906</td><td>0.910</td><td>0.947</td><td>0.042</td><td>0.931</td><td>0.922</td><td>0.972</td><td>0.021</td><td>0.865</td><td>0.864</td><td>0.903</td><td>0.072</td><td>0.898</td><td>0.893</td><td>0.941</td><td>0.045</td></tr></tbody></table>", "caption": "Table 5. DQFM gating strategy: using identical (only one) \\alpha_{i} and \\beta_{i} vs. using multiple (five different) \\alpha_{i} and \\beta_{i}. ", "list_citation_info": ["Ju et al. (2014) Ran Ju, L. Ge, W. Geng, T. Ren, and Gangshan Wu. 2014. Depth saliency based on anisotropic center-surround difference. In ICIP. 1115\u20131119.", "Li et al. (2014) Nianyi Li, J. Ye, Y. Ji, Haibin Ling, and J. Yu. 2014. Saliency Detection on Light Field. IEEE TPAMI 39 (2014), 1605\u20131616.", "Fan et al. (2020b) Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. 2020b. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE TNNLS (2020).", "Niu et al. (2012) Y. Niu, Yujie Geng, X. Li, and Feng Liu. 2012. Leveraging stereopsis for saliency analysis. In CVPR. 454\u2013461.", "Peng et al. (2014) Houwen Peng, B. Li, Weihua Xiong, W. Hu, and R. Ji. 2014. RGBD Salient Object Detection: A Benchmark and Algorithms. In ECCV. 92\u2013109.", "Cheng et al. (2014) Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao. 2014. Depth enhanced saliency detection method. In ICIMCS. 23\u201327."]}, {"table": "<table><thead><tr><th rowspan=\"2\">#</th><th>Depth</th><th>Size</th><th colspan=\"4\">SIP (Fanet al., 2020b)</th><th colspan=\"4\">NLPR (Penget al., 2014)</th><th colspan=\"4\">NJU2K (Juet al., 2014)</th><th colspan=\"4\">RGBD135 (Chenget al., 2014)</th><th colspan=\"4\">LFSD (Liet al., 2014)</th><th colspan=\"4\">STERE (Niuet al., 2012)</th></tr><tr><th>backbone</th><th>(Mb)</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th><th>S_{\\alpha}</th><th>F_{\\beta}^{\\rm max}</th><th>E_{\\xi}^{\\rm max}</th><th>\\mathcal{M}</th></tr></thead><tbody><tr><td>9</td><td>MoblieNet-V2</td><td>6.9</td><td>0.879</td><td>0.886</td><td>0.923</td><td>0.054</td><td>0.919</td><td>0.904</td><td>0.954</td><td>0.027</td><td>0.906</td><td>0.908</td><td>0.948</td><td>0.042</td><td>0.930</td><td>0.922</td><td>0.969</td><td>0.022</td><td>0.864</td><td>0.864</td><td>0.902</td><td>0.070</td><td>0.893</td><td>0.889</td><td>0.942</td><td>0.046</td></tr><tr><td>4</td><td>Tailored</td><td>0.9</td><td>0.883</td><td>0.887</td><td>0.926</td><td>0.051</td><td>0.923</td><td>0.908</td><td>0.957</td><td>0.026</td><td>0.906</td><td>0.910</td><td>0.947</td><td>0.042</td><td>0.931</td><td>0.922</td><td>0.972</td><td>0.021</td><td>0.865</td><td>0.864</td><td>0.903</td><td>0.072</td><td>0.898</td><td>0.893</td><td>0.941</td><td>0.045</td></tr></tbody></table>", "caption": "Table 6. Performance of the proposed TDB (tailored depth backbone) against MobileNet-V2. Details are in Sec. 4.4.", "list_citation_info": ["Ju et al. (2014) Ran Ju, L. Ge, W. Geng, T. Ren, and Gangshan Wu. 2014. Depth saliency based on anisotropic center-surround difference. In ICIP. 1115\u20131119.", "Li et al. (2014) Nianyi Li, J. Ye, Y. Ji, Haibin Ling, and J. Yu. 2014. Saliency Detection on Light Field. IEEE TPAMI 39 (2014), 1605\u20131616.", "Fan et al. (2020b) Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. 2020b. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE TNNLS (2020).", "Niu et al. (2012) Y. Niu, Yujie Geng, X. Li, and Feng Liu. 2012. Leveraging stereopsis for saliency analysis. In CVPR. 454\u2013461.", "Peng et al. (2014) Houwen Peng, B. Li, Weihua Xiong, W. Hu, and R. Ji. 2014. RGBD Salient Object Detection: A Benchmark and Algorithms. In ECCV. 92\u2013109.", "Cheng et al. (2014) Yupeng Cheng, Huazhu Fu, Xingxing Wei, Jiangjian Xiao, and Xiaochun Cao. 2014. Depth enhanced saliency detection method. In ICIMCS. 23\u201327."]}]}