{"title": "End-to-end temporal action detection with transformer", "abstract": "Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR.", "authors": ["Xiaolong Liu", " Qimeng Wang", " Yao Hu", " Xu Tang", " Shiwei Zhang", " Song Bai", " Xiang Bai"], "pdf_url": "https://arxiv.org/abs/2106.10271", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Feature</td><td><p>SN</p></td><td>E2E</td><td><p>mAP{}_{0.3}</p></td><td><p>mAP{}_{0.4}</p></td><td><p>mAP{}_{0.5}</p></td><td><p>mAP{}_{0.6}</p></td><td><p>mAP{}_{0.7}</p></td><td><p>mAP</p></td><td>Time/ms</td><td>GFLOPs</td></tr><tr><td>Yeung et al. [38]</td><td>VGG16</td><td><p>\u2713</p></td><td>-</td><td><p>36.0</p></td><td><p>26.4</p></td><td><p>17.1</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td><td>-</td></tr><tr><td>Yuan et al. [9]</td><td>TS</td><td><p>-</p></td><td>-</td><td><p>36.5</p></td><td><p>27.8</p></td><td><p>17.8</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td><td>-</td></tr><tr><td>SSAD [8]</td><td>TS</td><td><p>\u2713</p></td><td>-</td><td><p>43.0</p></td><td><p>35.0</p></td><td><p>24.6</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td><td>-</td></tr><tr><td>R-C3D [6]</td><td>C3D</td><td><p>\u2713</p></td><td>-</td><td><p>44.8</p></td><td><p>35.6</p></td><td><p>28.9</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td><td>-</td></tr><tr><td>SSN [69]</td><td>TS</td><td><p>-</p></td><td>-</td><td><p>51.9</p></td><td><p>41.0</p></td><td><p>29.8</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td><td>-</td></tr><tr><td>TAL-Net [7]</td><td>I3D</td><td><p>\u2713</p></td><td>-</td><td><p>53.2</p></td><td><p>48.5</p></td><td><p>42.8</p></td><td><p>33.8</p></td><td><p>20.8</p></td><td><p>39.8</p></td><td>-</td><td>-</td></tr><tr><td>BSN [29]</td><td>TS</td><td><p>-</p></td><td>-</td><td><p>53.5</p></td><td><p>45.0</p></td><td><p>36.9</p></td><td><p>28.4</p></td><td><p>20.0</p></td><td><p>36.8</p></td><td>&gt;2065</td><td>&gt;3.4</td></tr><tr><td>MGG [31]</td><td>TS</td><td><p>-</p></td><td>-</td><td><p>53.9</p></td><td><p>46.8</p></td><td><p>37.4</p></td><td><p>29.5</p></td><td><p>21.3</p></td><td><p>37.8</p></td><td>-</td><td>-</td></tr><tr><td>BMN [11]</td><td>TS</td><td><p>-</p></td><td>-</td><td><p>56.0</p></td><td><p>47.4</p></td><td><p>38.8</p></td><td><p>29.7</p></td><td><p>20.5</p></td><td><p>38.5</p></td><td>&gt;483</td><td>&gt;171.0</td></tr><tr><td>BC-GNN [64]</td><td>TS</td><td><p>-</p></td><td>-</td><td><p>57.1</p></td><td><p>49.1</p></td><td><p>40.4</p></td><td><p>31.2</p></td><td><p>23.1</p></td><td><p>40.2</p></td><td>-</td><td>-</td></tr><tr><td>G-TAD [2]</td><td>TS</td><td><p>-</p></td><td>-</td><td><p>54.5</p></td><td><p>47.6</p></td><td><p>40.2</p></td><td><p>30.8</p></td><td><p>23.4</p></td><td><p>39.3</p></td><td>&gt;4440</td><td>&gt;639.8</td></tr><tr><td>BMN{}^{\\dagger}  [11]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>56.4</p></td><td><p>47.9</p></td><td><p>39.2</p></td><td><p>30.2</p></td><td><p>21.2</p></td><td><p>39.0</p></td><td>-</td><td>-</td></tr><tr><td>G-TAD{}^{\\ddagger}  [2]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>58.7</p></td><td><p>52.7</p></td><td><p>44.9</p></td><td><p>33.6</p></td><td><p>23.8</p></td><td><p>42.7</p></td><td>&gt;3552</td><td>&gt;368.9</td></tr><tr><td>MR [70]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>53.9</p></td><td><p>50.7</p></td><td><p>45.4</p></td><td><p>38.0</p></td><td><p>28.5</p></td><td><p>43.3</p></td><td>&gt;644</td><td>&gt;36.8</td></tr><tr><td>A2Net [36]</td><td>I3D</td><td><p>\u2713</p></td><td>-</td><td><p>58.6</p></td><td><p>54.1</p></td><td><p>45.5</p></td><td><p>32.5</p></td><td><p>17.2</p></td><td><p>41.6</p></td><td>1554</td><td>30.4</td></tr><tr><td>P-GCN [13]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>63.6</p></td><td><p>57.8</p></td><td><p>49.1</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>7298</td><td>4.4</td></tr><tr><td>P-GCN{}^{\\ddagger} [13]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>64.9</p></td><td><p>59.0</p></td><td><p>49.4</p></td><td><p>36.7</p></td><td><p>22.6</p></td><td><p>46.5</p></td><td>7298</td><td>4.4</td></tr><tr><td>G-TAD [2]+P-GCN [13]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>66.4</p></td><td><p>60.4</p></td><td><p>51.6</p></td><td><p>37.6</p></td><td><p>22.9</p></td><td><p>47.8</p></td><td>-</td><td>-</td></tr><tr><td>AGT [61]</td><td>I3D</td><td><p>\u2713</p></td><td>\u2713</td><td><p>65.0</p></td><td><p>58.1</p></td><td><p>50.2</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td><td>-</td></tr><tr><td>PCG-TAL [37]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>64.2</p></td><td><p>57.3</p></td><td><p>48.3</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td><td>-</td></tr><tr><td>RTD-Net [62]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>68.3</p></td><td><p>62.3</p></td><td><p>51.9</p></td><td><p>38.8</p></td><td><p>23.7</p></td><td><p>49.0</p></td><td>&gt;211</td><td>&gt;32.1</td></tr><tr><td>AFSD [35]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>67.3</p></td><td><p>62.4</p></td><td><p>55.5</p></td><td><p>43.7</p></td><td><p>31.1</p></td><td><p>52.0</p></td><td>3245</td><td>84.1</td></tr><tr><td>MUSES [27]</td><td>I3D</td><td><p>-</p></td><td>-</td><td><p>68.9</p></td><td><p>64.0</p></td><td><p>56.9</p></td><td><p>46.3</p></td><td><p>31.0</p></td><td><p>53.4</p></td><td>2101</td><td>34.1</td></tr><tr><td>TadTR* (Ours)</td><td>I3D</td><td><p>\u2713</p></td><td>\u2713</td><td>74.8</td><td>69.1</td><td>60.1</td><td>46.6</td><td>32.8</td><td>56.7</td><td>195</td><td>1.07</td></tr><tr><td>TadTR-lite* (Ours)</td><td>I3D</td><td><p>\u2713</p></td><td>\u2713</td><td><p>71.3</p></td><td><p>65.9</p></td><td><p>57.0</p></td><td><p>44.6</p></td><td><p>30.4</p></td><td><p>53.8</p></td><td>155</td><td>0.85</td></tr><tr><td>TadTR{}^{\\lx@sectionsign} (Ours)</td><td>I3D</td><td><p>\u2713</p></td><td>\u2713</td><td><p>70.3</p></td><td><p>64.3</p></td><td><p>55.7</p></td><td><p>44.0</p></td><td><p>30.0</p></td><td><p>52.9</p></td><td>195</td><td>1.07</td></tr><tr><td>TadTR (Ours)</td><td>I3D</td><td><p>\u2713</p></td><td>\u2713</td><td><p>67.1</p></td><td><p>61.1</p></td><td><p>52.0</p></td><td><p>39.9</p></td><td><p>26.2</p></td><td><p>49.3</p></td><td>195</td><td>1.07</td></tr></tbody></table>", "caption": "TABLE I: Comparison with state-of-the-art methods on THUMOS14. Run time is the average inference time per video, including post-processing operations, such as NMS. SN: single-network. E2E: end-to-end. TS: two-stream. {}^{\\sharp}For proposal generation methods, the computation cost of the extra classifiers is not included (marked with &gt;). \\dagger Results copied from [36]. \\ddagger Our implementation. * With focal loss and IBIF. {}^{\\lx@sectionsign} With IBIF.", "list_citation_info": ["[37] R. Su, D. Xu, L. Sheng, and W. Ouyang, \u201cPcg-tal: Progressive cross-granularity cooperation for temporal action localization,\u201d IEEE Transactions on Image Processing, vol. 30, pp. 2103\u20132113, 2021.", "[31] Y. Liu, L. Ma, Y. Zhang, W. Liu, and S.-F. Chang, \u201cMulti-granularity generator for temporal action proposal,\u201d in CVPR, 2019, pp. 3604\u20133613.", "[69] Y. Zhao, B. Zhang, Z. Wu, S. Yang, L. Zhou, S. Yan, L. Wang, Y. Xiong, W. Yali, D. Lin, Y. Qiao, and X. Tang, \u201cCUHK & ETHZ & SIAT submission to ActivityNet challenge 2017,\u201d arXiv preprint arXiv:1710.08011, pp. 20\u201324, 2017.", "[11] T. Lin, X. Liu, X. Li, E. Ding, and S. Wen, \u201cBmn: Boundary-matching network for temporal action proposal generation,\u201d in ICCV, 2019, pp. 3889\u20133898.", "[6] H. Xu, A. Das, and K. Saenko, \u201cR-c3d: region convolutional 3d network for temporal activity detection,\u201d in ICCV, 2017, pp. 5794\u20135803.", "[9] Z.-H. Yuan, J. C. Stroud, T. Lu, and J. Deng, \u201cTemporal action localization by structured maximal sums,\u201d in CVPR, 2017, pp. 3684\u20133692.", "[36] L. Yang, H. Peng, D. Zhang, J. Fu, and J. Han, \u201cRevisiting anchor mechanisms for temporal action localization,\u201d IEEE Transactions on Image Processing, vol. 29, pp. 8535\u20138548, 2020.", "[38] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei, \u201cEnd-to-end learning of action detection from frame glimpses in videos,\u201d in CVPR, 2016, pp. 2678\u20132687.", "[29] T. Lin, X. Zhao, H. Su, C. Wang, and M. Yang, \u201cBsn: Boundary sensitive network for temporal action proposal generation,\u201d in ECCV, September 2018, pp. 3\u201321.", "[70] P. Zhao, L. Xie, C. Ju, Y. Zhang, Y. Wang, and Q. Tian, \u201cBottom-up temporal action localization with mutual regularization,\u201d in ECCV, 2020.", "[7] Y.-W. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross, J. Deng, and R. Sukthankar, \u201cRethinking the faster r-cnn architecture for temporal action localization,\u201d in CVPR, 2018, pp. 1130\u20131139.", "[13] R. Zeng, W. Huang, M. Tan, Y. Rong, P. Zhao, J. Huang, and C. Gan, \u201cGraph convolutional networks for temporal action localization,\u201d in ICCV, 2019, pp. 7094\u20137103.", "[62] J. Tan, J. Tang, L. Wang, and G. Wu, \u201cRelaxed transformer decoders for direct action proposal generation,\u201d in ICCV, October 2021, pp. 13\u2009526\u201313\u2009535.", "[2] M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem, \u201cG-TAD: Sub-graph localization for temporal action detection,\u201d in CVPR, 2020, pp. 10\u2009156\u201310\u2009165.", "[35] C. Lin, C. Xu, D. Luo, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang, and Y. Fu, \u201cLearning salient boundary feature for anchor-free temporal action localization,\u201d in CVPR, 2021, pp. 3320\u20133329.", "[61] M. Nawhal and G. Mori, \u201cActivity graph transformer for temporal action localization,\u201d arXiv preprint arXiv:2101.08540, 2021.", "[8] T. Lin, X. Zhao, and Z. Shou, \u201cSingle shot temporal action detection,\u201d in ACM MM, 2017, pp. 988\u2013996.", "[64] Y. Bai, Y. Wang, Y. Tong, Y. Yang, Q. Liu, and J. Liu, \u201cBoundary content graph neural network for temporal action proposal generation,\u201d in ECCV, 2020, pp. 121\u2013137.", "[27] X. Liu, Y. Hu, S. Bai, F. Ding, X. Bai, and P. H. S. Torr, \u201cMulti-shot temporal event localization: A benchmark,\u201d in CVPR, June 2021, pp. 12\u2009596\u201312\u2009606."]}, {"table": "<table><thead><tr><th>Method</th><th><p>mAP{}_{0.5}</p></th><th><p>mAP{}_{0.75}</p></th><th><p>mAP{}_{0.95}</p></th><th><p>mAP</p></th><th>Time/ms</th><th>GFLOPs</th></tr></thead><tbody><tr><td>SSN [69]</td><td><p>28.82</p></td><td><p>18.80</p></td><td><p>5.32</p></td><td><p>18.97</p></td><td>-</td><td>-</td></tr><tr><td>G-TAD [2]</td><td><p>41.08</p></td><td><p>27.59</p></td><td><p>8.34</p></td><td><p>27.48</p></td><td>941</td><td>45.7</td></tr><tr><td>TadTR</td><td><p>45.16</p></td><td><p>30.70</p></td><td>11.78</td><td><p>30.83</p></td><td>19</td><td>0.1</td></tr><tr><td>TadTR*</td><td>47.14</td><td>32.11</td><td><p>10.94</p></td><td>32.09</td><td>19</td><td>0.1</td></tr></tbody></table>", "caption": "TABLE II: Comparison of different methods on the validation set of HACS Segments. The results of SSN are from [21]. * With focal loss.", "list_citation_info": ["[69] Y. Zhao, B. Zhang, Z. Wu, S. Yang, L. Zhou, S. Yan, L. Wang, Y. Xiong, W. Yali, D. Lin, Y. Qiao, and X. Tang, \u201cCUHK & ETHZ & SIAT submission to ActivityNet challenge 2017,\u201d arXiv preprint arXiv:1710.08011, pp. 20\u201324, 2017.", "[2] M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem, \u201cG-TAD: Sub-graph localization for temporal action detection,\u201d in CVPR, 2020, pp. 10\u2009156\u201310\u2009165.", "[21] H. Zhao, A. Torralba, L. Torresani, and Z. Yan, \u201cHACS: human action clips and segments dataset for recognition and temporal localization,\u201d in ICCV, 2019, pp. 8667\u20138677."]}, {"table": "<table><tbody><tr><td>Method</td><td>Feature</td><td><p>SN</p></td><td><p>E2E</p></td><td><p>mAP{}_{0.5}</p></td><td><p>mAP{}_{0.75}</p></td><td><p>mAP{}_{0.95}</p></td><td><p>mAP</p></td><td>GFLOPs</td></tr><tr><td colspan=\"9\">Self-contained methods</td></tr><tr><td>R-C3D [6]</td><td>C3D</td><td><p>\u2713</p></td><td><p>-</p></td><td><p>26.80</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>-</td></tr><tr><td>SSN [10]</td><td>TS</td><td><p>-</p></td><td><p>-</p></td><td><p>39.12</p></td><td><p>23.48</p></td><td><p>5.49</p></td><td><p>23.98</p></td><td>-</td></tr><tr><td>TAL-Net [7]</td><td>I3D</td><td><p>\u2713</p></td><td><p>-</p></td><td><p>38.23</p></td><td><p>18.30</p></td><td><p>1.30</p></td><td><p>20.22</p></td><td>-</td></tr><tr><td>P-GCN [13]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>42.90</p></td><td><p>28.14</p></td><td><p>2.47</p></td><td><p>26.99</p></td><td>5.0</td></tr><tr><td>PCG-TAL [37]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>42.14</p></td><td><p>28.34</p></td><td><p>6.12</p></td><td><p>27.34</p></td><td>-</td></tr><tr><td>TadTR (Ours)</td><td>TS</td><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>41.40</p></td><td><p>28.85</p></td><td><p>7.86</p></td><td><p>28.21</p></td><td>0.038</td></tr><tr><td>TadTR* (Ours)</td><td>TS</td><td><p>\u2713</p></td><td><p>\u2713</p></td><td>43.67</td><td>30.58</td><td>8.32</td><td>29.90</td><td>0.038</td></tr><tr><td colspan=\"9\">Combined with an ensemble of action classifiers [69]</td></tr><tr><td>CDC [72]</td><td>C3D</td><td><p>-</p></td><td><p>-</p></td><td><p>43.83</p></td><td><p>25.88</p></td><td><p>0.21</p></td><td><p>22.77</p></td><td>-</td></tr><tr><td>BMN [11]</td><td>TS</td><td><p>-</p></td><td><p>-</p></td><td><p>50.07</p></td><td><p>34.78</p></td><td><p>8.29</p></td><td><p>33.85</p></td><td>45.6</td></tr><tr><td>G-TAD [2]</td><td>TS</td><td><p>-</p></td><td><p>-</p></td><td><p>50.36</p></td><td><p>34.60</p></td><td><p>9.02</p></td><td><p>34.09</p></td><td>45.7</td></tr><tr><td>P-GCN [13]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>48.26</p></td><td><p>33.16</p></td><td><p>3.27</p></td><td><p>31.11</p></td><td>5.0</td></tr><tr><td>MR [70]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>43.47</p></td><td><p>33.91</p></td><td><p>9.21</p></td><td><p>30.12</p></td><td>-</td></tr><tr><td>A2Net [36]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>43.55</p></td><td><p>28.69</p></td><td><p>3.70</p></td><td><p>27.75</p></td><td>1.2</td></tr><tr><td>PCG-TAL [37]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>50.24</p></td><td><p>35.21</p></td><td><p>7.84</p></td><td><p>34.01</p></td><td>-</td></tr><tr><td>RTD-Net [62]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>47.21</p></td><td><p>30.68</p></td><td><p>8.61</p></td><td><p>30.83</p></td><td>3.1</td></tr><tr><td>AFSD [35]</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td><p>52.38</p></td><td><p>35.27</p></td><td><p>6.47</p></td><td><p>34.39</p></td><td>3.3</td></tr><tr><td>TadTR* (Ours)</td><td>TS</td><td><p>-</p></td><td><p>-</p></td><td><p>51.29</p></td><td><p>34.99</p></td><td><p>9.49</p></td><td><p>34.64</p></td><td>0.038</td></tr><tr><td>TadTR+BMN (Ours)</td><td>TS</td><td><p>-</p></td><td><p>-</p></td><td><p>50.51</p></td><td><p>35.35</p></td><td><p>8.18</p></td><td><p>34.55</p></td><td>45.6</td></tr><tr><td>TadTR* (Ours)</td><td>I3D</td><td><p>-</p></td><td><p>-</p></td><td>52.83</td><td>37.05</td><td>10.83</td><td>36.11</td><td>0.038</td></tr><tr><td>BMN [11]</td><td>TSP</td><td><p>-</p></td><td><p>-</p></td><td><p>51.23</p></td><td><p>36.78</p></td><td><p>9.50</p></td><td><p>35.67</p></td><td>45.6</td></tr><tr><td>G-TAD [2]</td><td>TSP</td><td><p>-</p></td><td><p>-</p></td><td><p>51.26</p></td><td><p>37.12</p></td><td><p>9.29</p></td><td><p>35.81</p></td><td>45.7</td></tr><tr><td>TadTR* (Ours)</td><td>TSP</td><td><p>-</p></td><td><p>-</p></td><td>53.62</td><td>37.52</td><td>10.56</td><td>36.75</td><td>0.038</td></tr></tbody></table>", "caption": "TABLE III: Comparison of different methods on ActivityNet-1.3.Methods in the second group are combined with an ensemble of action classifiers [69]. The computation costs (in FLOPs) of the action classifiers are not included. The results of BMN and G-TAD with TSP [71] features are from [71]. TS: two-stream. SN: single-network. * With focal loss.", "list_citation_info": ["[37] R. Su, D. Xu, L. Sheng, and W. Ouyang, \u201cPcg-tal: Progressive cross-granularity cooperation for temporal action localization,\u201d IEEE Transactions on Image Processing, vol. 30, pp. 2103\u20132113, 2021.", "[10] Y. Zhao, Y. Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin, \u201cTemporal action detection with structured segment networks,\u201d ICCV, pp. 2914\u20132923, 2017.", "[13] R. Zeng, W. Huang, M. Tan, Y. Rong, P. Zhao, J. Huang, and C. Gan, \u201cGraph convolutional networks for temporal action localization,\u201d in ICCV, 2019, pp. 7094\u20137103.", "[71] H. Alwassel, S. Giancola, and B. Ghanem, \u201cTsp: Temporally-sensitive pretraining of video encoders for localization tasks,\u201d in ICCV Workshops, 2021, pp. 3166\u20133176.", "[72] Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang, \u201cCdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos,\u201d in ICCV, 2017, pp. 1417\u20131426.", "[62] J. Tan, J. Tang, L. Wang, and G. Wu, \u201cRelaxed transformer decoders for direct action proposal generation,\u201d in ICCV, October 2021, pp. 13\u2009526\u201313\u2009535.", "[2] M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem, \u201cG-TAD: Sub-graph localization for temporal action detection,\u201d in CVPR, 2020, pp. 10\u2009156\u201310\u2009165.", "[69] Y. Zhao, B. Zhang, Z. Wu, S. Yang, L. Zhou, S. Yan, L. Wang, Y. Xiong, W. Yali, D. Lin, Y. Qiao, and X. Tang, \u201cCUHK & ETHZ & SIAT submission to ActivityNet challenge 2017,\u201d arXiv preprint arXiv:1710.08011, pp. 20\u201324, 2017.", "[35] C. Lin, C. Xu, D. Luo, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang, and Y. Fu, \u201cLearning salient boundary feature for anchor-free temporal action localization,\u201d in CVPR, 2021, pp. 3320\u20133329.", "[11] T. Lin, X. Liu, X. Li, E. Ding, and S. Wen, \u201cBmn: Boundary-matching network for temporal action proposal generation,\u201d in ICCV, 2019, pp. 3889\u20133898.", "[70] P. Zhao, L. Xie, C. Ju, Y. Zhang, Y. Wang, and Q. Tian, \u201cBottom-up temporal action localization with mutual regularization,\u201d in ECCV, 2020.", "[6] H. Xu, A. Das, and K. Saenko, \u201cR-c3d: region convolutional 3d network for temporal activity detection,\u201d in ICCV, 2017, pp. 5794\u20135803.", "[7] Y.-W. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross, J. Deng, and R. Sukthankar, \u201cRethinking the faster r-cnn architecture for temporal action localization,\u201d in CVPR, 2018, pp. 1130\u20131139.", "[36] L. Yang, H. Peng, D. Zhang, J. Fu, and J. Han, \u201cRevisiting anchor mechanisms for temporal action localization,\u201d IEEE Transactions on Image Processing, vol. 29, pp. 8535\u20138548, 2020."]}]}