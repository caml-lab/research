{"title": "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning", "abstract": "Contrastive learning methods for unsupervised visual representation learning have reached remarkable levels of transfer performance. We argue that the power of contrastive learning has yet to be fully unleashed, as current methods are trained only on instance-level pretext tasks, leading to representations that may be sub-optimal for downstream tasks requiring dense pixel predictions. In this paper, we introduce pixel-level pretext tasks for learning dense feature representations. The first task directly applies contrastive learning at the pixel level. We additionally propose a pixel-to-propagation consistency task that produces better results, even surpassing the state-of-the-art approaches by a large margin. Specifically, it achieves 60.2 AP, 41.4 / 40.5 mAP and 77.2 mIoU when transferred to Pascal VOC object detection (C4), COCO object detection (FPN / C4) and Cityscapes semantic segmentation using a ResNet-50 backbone network, which are 2.6 AP, 0.8 / 1.0 mAP and 1.0 mIoU better than the previous best methods built on instance-level contrastive learning. Moreover, the pixel-level pretext tasks are found to be effective for pre-training not only regular backbone networks but also head networks used for dense downstream tasks, and are complementary to instance-level contrastive methods. These results demonstrate the strong potential of defining pretext tasks at the pixel level, and suggest a new path forward in unsupervised visual representation learning. Code is available at \\url{https://github.com/zdaxie/PixPro}.", "authors": ["Zhenda Xie", " Yutong Lin", " Zheng Zhang", " Yue Cao", " Stephen Lin", " Han Hu"], "pdf_url": "https://arxiv.org/abs/2011.10043", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\">#. Epoch</td><td colspan=\"3\">Pascal VOC (R50-C4)</td><td colspan=\"3\">COCO (R50-FPN)</td><td colspan=\"3\">COCO (R50-C4)</td><td>Cityscapes (R50)</td></tr><tr><td>AP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mAP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mAP</td><td>\\text{AP}_{\\text{50}}</td><td>\\text{AP}_{\\text{75}}</td><td>mIoU</td></tr><tr><td>scratch</td><td>-</td><td>33.8</td><td>60.2</td><td>33.1</td><td>32.8</td><td>51.0</td><td>35.3</td><td>26.4</td><td>44.0</td><td>27.8</td><td>65.3</td></tr><tr><td>supervised</td><td>100</td><td>53.5</td><td>81.3</td><td>58.8</td><td>39.7</td><td>59.5</td><td>43.3</td><td>38.2</td><td>58.2</td><td>41.2</td><td>74.6</td></tr><tr><td>MoCo [19]</td><td>200</td><td>55.9</td><td>81.5</td><td>62.6</td><td>39.4</td><td>59.1</td><td>43.0</td><td>38.5</td><td>58.3</td><td>41.6</td><td>75.3</td></tr><tr><td>SimCLR [9]</td><td>1000</td><td>56.3</td><td>81.9</td><td>62.5</td><td>39.8</td><td>59.5</td><td>43.6</td><td>38.4</td><td>58.3</td><td>41.6</td><td>75.8</td></tr><tr><td>MoCo v2 [10]</td><td>800</td><td>57.6</td><td>82.7</td><td>64.4</td><td>40.4</td><td>60.1</td><td>44.3</td><td>39.5</td><td>59.0</td><td>42.6</td><td>76.2</td></tr><tr><td>InfoMin [35]</td><td>200</td><td>57.6</td><td>82.7</td><td>64.6</td><td>40.6</td><td>60.6</td><td>44.6</td><td>39.0</td><td>58.5</td><td>42.0</td><td>75.6</td></tr><tr><td>InfoMin [35]</td><td>800</td><td>57.5</td><td>82.5</td><td>64.0</td><td>40.4</td><td>60.4</td><td>44.3</td><td>38.8</td><td>58.2</td><td>41.7</td><td>75.6</td></tr><tr><td>PixPro (ours)</td><td>100</td><td>58.8</td><td>83.0</td><td>66.5</td><td>41.3</td><td>61.3</td><td>45.4</td><td>40.0</td><td>59.3</td><td>43.4</td><td>76.8</td></tr><tr><td>PixPro (ours)</td><td>400</td><td>60.2</td><td>83.8</td><td>67.7</td><td>41.4</td><td>61.6</td><td>45.4</td><td>40.5</td><td>59.8</td><td>44.0</td><td>77.2</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Comparing the proposed pixel-level pre-training method, PixPro, to previous supervised/unsupervised pre-training methods. For Pascal VOC object detection, a Faster R-CNN (R50-C4) detector is adopted for all methods. For COCO object detection, a Mask R-CNN detector (R50-FPN and R50-C4) with 1\\times setting is adopted for all methods. For Cityscapes semantic segmentation, an FCN method (R50) is used. Only a pixel-level pretext task is involved in PixPro pre-training. For Pascal VOC (R50-C4), COCO (R50-C4) and Cityscapes (R50), a regular backbone network of R50 with output feature map of C5 is adopted for PixPro pre-training. For COCO (R50-FPN), an FPN network with P_{3}-P_{6} feature maps is used. Note that InfoMin [35] reports results for only its 200 epoch model, so we reproduce it with longer training lengths, where saturation is observed.", "list_citation_info": ["[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. ICML, 2020.", "[35] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.", "[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. CVPR, 2020.", "[10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020."]}, {"table": "<table><tr><td rowspan=\"2\">+FPN</td><td rowspan=\"2\">+head</td><td rowspan=\"2\">+instance</td><td colspan=\"3\">COCO (FCOS)</td></tr><tr><td>mAP</td><td>AP{}_{\\text{50}}</td><td>AP{}_{\\text{75}}</td></tr><tr><td></td><td></td><td></td><td>37.8</td><td>56.2</td><td>40.6</td></tr><tr><td>\u2713</td><td></td><td></td><td>38.1</td><td>56.7</td><td>41.2</td></tr><tr><td>\u2713</td><td>\u2713</td><td></td><td>38.6</td><td>57.3</td><td>41.5</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>39.8</td><td>58.4</td><td>42.7</td></tr></table>", "caption": "Table 5: FPN and head pre-training with transfer to COCO using an FCOS detector [36]. 100 epoch pre-training is adopted for all experiments.", "list_citation_info": ["[36] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: Fully convolutional one-stage object detection. In ICCV, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">arch.</td><td rowspan=\"2\"> +COCOpre-train </td><td colspan=\"2\"> maskR-CNN </td></tr><tr><td>1%</td><td>10%</td></tr><tr><td>supervised</td><td></td><td>10.4</td><td>20.4</td></tr><tr><td>MoCo [19]</td><td></td><td>10.9</td><td>23.8</td></tr><tr><td>MoCo v2 [10]</td><td></td><td>10.9</td><td>23.9</td></tr><tr><td>InfoMin [35]</td><td></td><td>10.6</td><td>24.5</td></tr><tr><td>C5 backbone</td><td></td><td>13.2</td><td>25.9</td></tr><tr><td>FPN</td><td></td><td>14.1</td><td>26.6</td></tr><tr><td>FPN</td><td>\u2713</td><td>14.8</td><td>26.8</td></tr></table>", "caption": "Table 6: Semi-supervised object detection on COCO. 100-epoch pre-training is adopted for our method, and other methods use the models with their longest training.", "list_citation_info": ["[35] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.", "[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. CVPR, 2020.", "[10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020."]}]}