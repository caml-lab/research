{"title": "ISD: Self-Supervised Learning by Iterative Similarity Distillation", "abstract": "Recently, contrastive learning has achieved great results in self-supervised learning, where the main idea is to push two augmentations of an image (positive pairs) closer compared to other random images (negative pairs). We argue that not all random images are equal. Hence, we introduce a self supervised learning algorithm where we use a soft similarity for the negative images rather than a binary distinction between positive and negative pairs. We iteratively distill a slowly evolving teacher model to the student model by capturing the similarity of a query image to some random images and transferring that knowledge to the student. We argue that our method is less constrained compared to recent contrastive learning methods, so it can learn better features. Specifically, our method should handle unbalanced and unlabeled data better than existing contrastive learning methods, because the randomly chosen negative set might include many samples that are semantically similar to the query image. In this case, our method labels them as highly similar while standard contrastive methods label them as negative pairs. Our method achieves comparable results to the state-of-the-art models. We also show that our method performs better in the settings where the unlabeled data is unbalanced. Our code is available here: https://github.com/UMBCvision/ISD.", "authors": ["Ajinkya Tejankar", " Soroush Abbasi Koohpayegani", " Vipin Pillai", " Paolo Favaro", " Hamed Pirsiavash"], "pdf_url": "https://arxiv.org/abs/2012.09259", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Ref</td><td>Batch</td><td>Epochs</td><td>Sym. Loss</td><td>Top-1</td><td>NN</td><td>20-NN</td></tr><tr><td></td><td></td><td>Size</td><td></td><td>2x FLOPS</td><td>Linear</td><td></td><td></td></tr><tr><td colspan=\"8\">ResNet-50</td></tr><tr><td>Supervised</td><td>-</td><td>256</td><td>100</td><td>-</td><td>76.2</td><td>71.4</td><td>74.8</td></tr><tr><td>\\hdashlineSwAV [8]</td><td>[11]</td><td>4096</td><td>200</td><td>\u2713</td><td>69.1</td><td>-</td><td>-</td></tr><tr><td>SimCLR[9]</td><td>[9]</td><td>4096</td><td>1000</td><td>\u2713</td><td>69.3</td><td>-</td><td>-</td></tr><tr><td>MoCo-V2 [20]</td><td>[11]</td><td>256</td><td>200</td><td>\u2713</td><td>69.9</td><td>-</td><td>-</td></tr><tr><td>SimSiam [11]</td><td>[11]</td><td>256</td><td>200</td><td>\u2713</td><td>70.0</td><td>-</td><td>-</td></tr><tr><td>BYOL [18]</td><td>[11]</td><td>4096</td><td>200</td><td>\u2713</td><td>70.6</td><td>-</td><td>-</td></tr><tr><td>MoCo-V2 [10]</td><td>[10]</td><td>256</td><td>400</td><td>\u2713</td><td>71.0</td><td>-</td><td>-</td></tr><tr><td>MoCo-V2 [20]</td><td>[20]</td><td>256</td><td>800</td><td>\u2717</td><td>71.1</td><td>57.3</td><td>61.0</td></tr><tr><td>CompRess* [1]</td><td>[1]</td><td>256</td><td>1K+130</td><td>\u2717</td><td>71.9</td><td>63.3</td><td>66.8</td></tr><tr><td>BYOL [18]</td><td>[18]</td><td>4096</td><td>1000</td><td>\u2713</td><td>74.3</td><td>62.8</td><td>66.9</td></tr><tr><td>SwAV <sup>\\dagger</sup> [8]</td><td>[8]</td><td>4096</td><td>800</td><td>\u2713</td><td>75.3</td><td>-</td><td>-</td></tr><tr><td>\\hdashlineMoCo-V2 [20]</td><td>[11]</td><td>256</td><td>200</td><td>\u2717</td><td>67.5</td><td>-</td><td>-</td></tr><tr><td>CO2 [45]</td><td>[45]</td><td>256</td><td>200</td><td>\u2717</td><td>68.0</td><td>-</td><td>-</td></tr><tr><td>BYOL-asym</td><td>-</td><td>256</td><td>200</td><td>\u2717</td><td>69.3</td><td>55.0</td><td>59.2</td></tr><tr><td>MSF <sup>\\ddagger</sup> [25]</td><td>[25]</td><td>256</td><td>200</td><td>\u2717</td><td>72.4</td><td>62.0</td><td>64.9</td></tr><tr><td>ISD</td><td>-</td><td>256</td><td>200</td><td>\u2717</td><td>69.8</td><td>59.2</td><td>62.0</td></tr><tr><td colspan=\"8\">ResNet-18</td></tr><tr><td>Supervised</td><td>-</td><td>256</td><td>100</td><td>-</td><td>69.8</td><td>63.0</td><td>67.6</td></tr><tr><td>\\hdashlineMoCo-V2 [20]</td><td>[11]</td><td>256</td><td>200</td><td>\u2717</td><td>51.0</td><td>37.7</td><td>42.1</td></tr><tr><td>BYOL-asym</td><td>-</td><td>256</td><td>200</td><td>\u2717</td><td>52.6</td><td>40.0</td><td>44.8</td></tr><tr><td>ISD</td><td>-</td><td>256</td><td>200</td><td>\u2717</td><td>53.8</td><td>41.5</td><td>46.6</td></tr></tbody></table>", "caption": "Table 1: Evaluation on full ImageNet:  We compare our method with other state-of-the-art SSL methods by evaluating the learned features on the full ImageNet. A single linear layer is trained on top of a frozen backbone. Note that methods using symmetric losses use 2\\times computation per mini-batch. Thus, it is not fair to compare them with the asymmetric loss methods. Further, we find that given a similar computational budget, both asymmetric MoCo-V2 (400 epochs) and symmetric MoCo-V2 (800 epochs) have similar accuracies (71.0 vs 71.1). Under similar resource constraints, our method performs competitively with other state-of-the-art methods. * is compressed from ResNet-50x4. \\dagger: SwAV is not comparable as it uses multiple crops together. \\ddagger: is our concurrent (future of this!) work. ", "list_citation_info": ["[1] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised learning by compressing representations. Advances in Neural Information Processing Systems, 33, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.", "[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.", "[11] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning, 2020.", "[45] Chen Wei, Huiyu Wang, Wei Shen, and Alan Yuille. Co2: Consistent contrast for unsupervised visual representation learning. arXiv preprint arXiv:2010.02217, 2020.", "[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.", "[10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.", "[25] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Mean shift for self-supervised learning, 2021.", "[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><th>Ref.</th><th>Epochs</th><td>Food</td><td>CIFAR</td><td>CIFAR</td><td>SUN</td><td>Cars</td><td>DTD</td><td>Pets</td><td>Caltech</td><td>Flowers</td><td>Mean</td></tr><tr><th></th><th></th><th></th><td>101</td><td>10</td><td>100</td><td>397</td><td>196</td><td></td><td></td><td>101</td><td>102</td><td></td></tr><tr><th colspan=\"13\">ResNet-50</th></tr><tr><th>Sup-IN</th><th>[18]</th><th></th><td>72.3</td><td>93.6</td><td>78.3</td><td>61.9</td><td>66.7</td><td>74.9</td><td>91.5</td><td>94.5</td><td>94.7</td><td>80.9</td></tr><tr><th>SimCLR [9]</th><th>[18]</th><th>1000</th><td>72.8</td><td>90.5</td><td>74.4</td><td>60.6</td><td>49.3</td><td>75.7</td><td>84.6</td><td>89.3</td><td>92.6</td><td>68.6</td></tr><tr><th>MoCo v2 [10]</th><th>-</th><th>800</th><td>72.5</td><td>92.2</td><td>74.6</td><td>59.6</td><td>50.5</td><td>74.4</td><td>84.6</td><td>90.0</td><td>90.5</td><td>76.5</td></tr><tr><th>BYOL [18]</th><th>rep.</th><th>1000</th><td>75.4</td><td>92.7</td><td>78.1</td><td>62.1</td><td>67.1</td><td>76.8</td><td>89.8</td><td>92.2</td><td>95.5</td><td>81.1</td></tr><tr><th>BYOL [18]</th><th>[18]</th><th>1000</th><td>75.3</td><td>91.3</td><td>78.4</td><td>62.2</td><td>67.8</td><td>75.5</td><td>90.4</td><td>94.2</td><td>96.1</td><td>81.2</td></tr><tr><th>BYOL-asym [18]</th><th>-</th><th>200</th><td>70.2</td><td>91.5</td><td>74.2</td><td>59.0</td><td>54.0</td><td>73.4</td><td>86.2</td><td>90.4</td><td>92.1</td><td>76.8</td></tr><tr><th>MoCo v2 [10]</th><th>-</th><th>200</th><td>70.4</td><td>91.0</td><td>73.5</td><td>57.5</td><td>47.7</td><td>73.9</td><td>81.3</td><td>88.7</td><td>91.1</td><td>75.0</td></tr><tr><th>MSF <sup>\\ddagger</sup> [25]</th><th>[25]</th><th>200</th><td>71.2</td><td>92.6</td><td>76.3</td><td>59.2</td><td>55.6</td><td>73.2</td><td>88.7</td><td>92.7</td><td>92.0</td><td>77.9</td></tr><tr><th>ISD</th><th>-</th><th>200</th><td>68.6</td><td>90.8</td><td>72.0</td><td>55.8</td><td>45.8</td><td>68.6</td><td>89.1</td><td>90.3</td><td>87.4</td><td>74.3</td></tr><tr><th colspan=\"13\">ResNet-18</th></tr><tr><th>BYOL-asym [18]</th><th>-</th><th>200</th><td>55.0</td><td>83.4</td><td>59.3</td><td>48.2</td><td>26.6</td><td>65.4</td><td>74.1</td><td>82.7</td><td>82.3</td><td>64.1</td></tr><tr><th>MoCo v2 [10]</th><th>-</th><th>200</th><td>56.7</td><td>83.0</td><td>59.7</td><td>48.8</td><td>30.4</td><td>64.4</td><td>70.1</td><td>80.5</td><td>83.1</td><td>64.1</td></tr><tr><th>ISD</th><th>-</th><th>200</th><td>58.3</td><td>83.3</td><td>62.7</td><td>49.6</td><td>36.1</td><td>65.6</td><td>76.4</td><td>84.5</td><td>87.4</td><td>67.1</td></tr></tbody></table>", "caption": "Table 2: Linear transfer evaluation:  We linear classifiers on top of frozen features for various downstream datasets. Hyperparameters are tuned individually for each method and the results are reported on the hold-out test sets. Our ResNet-18 is significantly better than other state-of-the-art SSL methods. \u201crep.\u201d refers to the reproduction with our framework for a fair comparison. \\ddagger: our concurrent work.", "list_citation_info": ["[25] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Mean shift for self-supervised learning, 2021.", "[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.", "[10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Epochs</th><td colspan=\"2\">Top-1</td><td colspan=\"2\">Top-5</td></tr><tr><td>1%</td><td>10%</td><td>1%</td><td>10%</td></tr><tr><th colspan=\"6\">Entire network is fine-tuned.</th></tr><tr><th>Supervised</th><th></th><td>25.4</td><td>56.4</td><td>48.4</td><td>80.4</td></tr><tr><th>PIRL [28]</th><th>800</th><td>-</td><td>-</td><td>57.2</td><td>83.8</td></tr><tr><th>CO2 [45]</th><th>200</th><td>-</td><td>-</td><td>71.0</td><td>85.7</td></tr><tr><th>SimCLR [9]</th><th>1000</th><td>48.3</td><td>65.6</td><td>75.5</td><td>87.8</td></tr><tr><th>InvP [44]</th><th>800</th><td>-</td><td>-</td><td>78.2</td><td>88.7</td></tr><tr><th>BYOL [18]</th><th>1000</th><td>53.2</td><td>68.8</td><td>78.4</td><td>89.0</td></tr><tr><th>\\text{SwAV}^{\\dagger} [8]</th><th>800</th><td>53.9</td><td>70.2</td><td>78.5</td><td>89.9</td></tr><tr><th colspan=\"6\">Only the linear layer is trained.</th></tr><tr><th>\\text{BYOL}^{{\\ddagger}} [18]</th><th>1000</th><td>55.7</td><td>68.6</td><td>80.0</td><td>88.6</td></tr><tr><th>CompRess* [1]</th><th>1K+130</th><td>59.7</td><td>67.0</td><td>82.3</td><td>87.5</td></tr><tr><th>\\hdashline</th><th></th><td></td><td></td><td></td><td></td></tr><tr><th>MoCo v2 [10]</th><th>200</th><td>43.6</td><td>58.4</td><td>71.2</td><td>82.9</td></tr><tr><th>BYOL-asym [18]</th><th>200</th><td>47.9</td><td>61.3</td><td>74.6</td><td>84.7</td></tr><tr><th>ISD</th><th>200</th><td>53.4</td><td>63.0</td><td>78.8</td><td>85.9</td></tr></tbody></table>", "caption": "Table 3: Evaluation on limited labels ImageNet for ResNet-50:  We evaluate our model for the 1% and 10% ImageNet linear evaluation. Unlike other methods, we only train a single linear layer on top of the frozen backbone. We observe that our method is better than other state-of-the-art methods given similar computational budgets. * is compressed from ResNet-50x4", "list_citation_info": ["[1] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised learning by compressing representations. Advances in Neural Information Processing Systems, 33, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.", "[28] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. arXiv preprint arXiv:1912.01991, 2019.", "[8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.", "[45] Chen Wei, Huiyu Wang, Wei Shen, and Alan Yuille. Co2: Consistent contrast for unsupervised visual representation learning. arXiv preprint arXiv:2010.02217, 2020.", "[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.", "[10] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.", "[44] Feng Wang, Huaping Liu, Di Guo, and Sun Fuchun. Unsupervised representation learning by invariance propagation. In Advances in Neural Information Processing Systems, 2020."]}]}