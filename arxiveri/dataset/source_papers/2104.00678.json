{"title": "Group-free 3d object detection via transformers", "abstract": "Recently, directly detecting 3D objects from 3D point clouds has received increasing attention. To extract object representation from an irregular point cloud, existing methods usually take a point grouping step to assign the points to an object candidate so that a PointNet-like network could be used to derive object features from the grouped points. However, the inaccurate point assignments caused by the hand-crafted grouping scheme decrease the performance of 3D object detection.\n  In this paper, we present a simple yet effective method for directly detecting 3D objects from the 3D point cloud. Instead of grouping local points to each object candidate, our method computes the feature of an object from all the points in the point cloud with the help of an attention mechanism in the Transformers \\cite{vaswani2017attention}, where the contribution of each point is automatically learned in the network training. With an improved attention stacking scheme, our method fuses object features in different stages and generates more accurate object detection results. With few bells and whistles, the proposed method achieves state-of-the-art 3D object detection performance on two widely used benchmarks, ScanNet V2 and SUN RGB-D. The code and models are publicly available at \\url{https://github.com/zeliu98/Group-Free-3D}", "authors": ["Ze Liu", " Zheng Zhang", " Yue Cao", " Han Hu", " Xin Tong"], "pdf_url": "https://arxiv.org/abs/2104.00678", "list_table_and_caption": [{"table": "<table><thead><tr><th>methods</th><th>backbone</th><th>mAP@0.25</th><th>mAP@0.5</th></tr></thead><tbody><tr><th>HGNet [3]</th><th>GU-net</th><td>61.3</td><td>34.4</td></tr><tr><th>GSDN [14]</th><th>MinkNet</th><td>62.8</td><td>34.8</td></tr><tr><th>3D-MPA [8]</th><th>MinkNet</th><td>64.2</td><td>49.2</td></tr><tr><th>VoteNet [26]<sup>2</sup><sup>2</sup>2We report the results of MMDetection3D(https://github.com/open-mmlab/mmdetection3d) instead of the official paper, which reported 46.8 mAP@0.25 and 24.7 mAP@0.5 on ScanNet V2, and 57.7 mAP@0.25 and 32.0 mAP@0.5 on SUN RGB-D.</th><th>PointNet++</th><td>62.9</td><td>39.9</td></tr><tr><th>MLCVNet [31]</th><th>PointNet++</th><td>64.5</td><td>41.4</td></tr><tr><th>H3DNet [51]</th><th>PointNet++</th><td>64.4</td><td>43.4</td></tr><tr><th>H3DNet [51]</th><th>4\\timesPointNet++</th><td>67.2</td><td>48.1</td></tr><tr><th>Ours (L6, O256)</th><th>PointNet++</th><td>67.3 (66.3)</td><td>48.9 (48.5)</td></tr><tr><th>Ours (L12, O256)</th><th>PointNet++</th><td>67.2 (66.6)</td><td>49.7 (49.0)</td></tr><tr><th>Ours (L12, O256)</th><th>PointNet++w2\\times</th><td>68.8 (67.7)</td><td>52.1 (50.6)</td></tr><tr><th>Ours (L12, O512)</th><th>PointNet++w2\\times</th><td>69.1 (68.6)</td><td>52.8 (51.8)</td></tr></tbody></table>", "caption": "Table 1: System level comparison on ScanNet V2 with state-of-the-arts. The main comparison is based on the best results of multiple experiments between different methods, and the number within the bracket is the average result.<br/>Notations: 4\\timesPointNet++ denotes 4 individual PointNet++; PointNet++w2\\times denotes the backbone width is expanded by 2 times; L denotes the decoder depth, and O denotes the number of object candidates, e.g. Ours (L6, O256) denotes a model with 6-layer decoder(i.e. 6 attention modules) and 256 object candidates.<br/>", "list_citation_info": ["[31] Xie Qian, Lai Yu-kun, Wu Jing, Wang Zhoutao, Zhang Yiming, Xu Kai, and Wang Jun. Mlcvnet: Multi-level context votenet for 3d object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[3] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, and Jian Wu. A hierarchical graph network for 3d object detection on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 392\u2013401, 2020.", "[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. arXiv preprint arXiv:2006.05682, 2020.", "[14] JunYoung Gwak, Christopher Choy, and Silvio Savarese. Generative sparse detection networks for 3d single-shot object detection. arXiv preprint arXiv:2006.12356, 2020.", "[8] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias Nie\u00dfner. 3d-mpa: Multi-proposal aggregation for 3d semantic instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9031\u20139040, 2020.", "[26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 9277\u20139286, 2019."]}, {"table": "<table><thead><tr><th>methods</th><th>backbone</th><th>inputs</th><th>mAP@0.25</th><th>mAP@0.5</th></tr></thead><tbody><tr><td>VoteNet [26]<sup>2</sup><sup>2</sup>2We report the results of MMDetection3D(https://github.com/open-mmlab/mmdetection3d) instead of the official paper, which reported 46.8 mAP@0.25 and 24.7 mAP@0.5 on ScanNet V2, and 57.7 mAP@0.25 and 32.0 mAP@0.5 on SUN RGB-D.</td><td>PointNet++</td><td>point</td><td>59.1</td><td>35.8</td></tr><tr><td>MLCVNet [31]</td><td>PointNet++</td><td>point</td><td>59.8</td><td>-</td></tr><tr><td>HGNet [3]</td><td>GU-net</td><td>point</td><td>61.6</td><td>-</td></tr><tr><td>H3DNet [51]</td><td>4\\timesPointNet++</td><td>point</td><td>60.1</td><td>39.0</td></tr><tr><td>imVoteNet [25]{}^{*}</td><td>PointNet++</td><td>point+RGB</td><td>63.4</td><td>-</td></tr><tr><td>Ours (L6, O256)</td><td>PointNet++</td><td>point</td><td>63.0 (62.6)</td><td>45.2 (44.4)</td></tr></tbody></table>", "caption": "Table 2: System level comparison on SUN RGB-D with state-of-the-arts. The main comparison is based on the best results of multiple experiments between different methods, and the number within the bracket is the average result. {}^{*}imVoteNet use RGB images as addition inputs.", "list_citation_info": ["[31] Xie Qian, Lai Yu-kun, Wu Jing, Wang Zhoutao, Zhang Yiming, Xu Kai, and Wang Jun. Mlcvnet: Multi-level context votenet for 3d object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[3] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, and Jian Wu. A hierarchical graph network for 3d object detection on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 392\u2013401, 2020.", "[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. arXiv preprint arXiv:2006.05682, 2020.", "[25] Charles R Qi, Xinlei Chen, Or Litany, and Leonidas J Guibas. Imvotenet: Boosting 3d object detection in point clouds with image votes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4404\u20134413, 2020.", "[26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 9277\u20139286, 2019."]}, {"table": "<table><thead><tr><th rowspan=\"2\">method</th><th rowspan=\"2\">backbone</th><th colspan=\"2\">mAP</th><th rowspan=\"2\">frames/s</th></tr><tr><th>0.25</th><th>0.5</th></tr></thead><tbody><tr><td>MLCVNet [31]</td><td>PointNet++</td><td>64.5</td><td>41.4</td><td>5.44</td></tr><tr><td>H3DNet [51]</td><td>4\\timesPointNet++</td><td>67.2</td><td>48.1</td><td>3.76</td></tr><tr><td>Ours (L6, O256)</td><td>PointNet++</td><td>67.3</td><td>48.9</td><td>6.71</td></tr><tr><td>Ours (L12, O256)</td><td>PointNet++</td><td>67.2</td><td>49.7</td><td>5.70</td></tr><tr><td>Ours (L12, O256)</td><td>PointNet++w2\\times</td><td>68.8</td><td>52.1</td><td>5.23</td></tr><tr><td>Ours (L12, O512)</td><td>PointNet++w2\\times</td><td>69.1</td><td>52.8</td><td>5.17</td></tr></tbody></table>", "caption": "Table 9: Comparison on realistic inference speed on ScanNet V2.", "list_citation_info": ["[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. arXiv preprint arXiv:2006.05682, 2020.", "[31] Xie Qian, Lai Yu-kun, Wu Jing, Wang Zhoutao, Zhang Yiming, Xu Kai, and Wang Jun. Mlcvnet: Multi-level context votenet for 3d object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><tbody><tr><th>methods</th><th>backbone</th><td>cab</td><td>bed</td><td>chair</td><td>sofa</td><td>tabl</td><td>door</td><td>wind</td><td>bkshf</td><td>pic</td><td>cntr</td><td>desk</td><td>curt</td><td>fridg</td><td>showr</td><td>toil</td><td>sink</td><td>bath</td><td>ofurn</td><td>mAP</td></tr><tr><th>VoteNet [26]</th><th>PointNet++</th><td>47.7</td><td>88.7</td><td>89.5</td><td>89.3</td><td>62.1</td><td>54.1</td><td>40.8</td><td>54.3</td><td>12.0</td><td>63.9</td><td>69.4</td><td>52.0</td><td>52.5</td><td>73.3</td><td>95.9</td><td>52.0</td><td>92.5</td><td>42.4</td><td>62.9</td></tr><tr><th>MLCVNet [31]</th><th>PointNet++</th><td>42.5</td><td>88.5</td><td>90.0</td><td>87.4</td><td>63.5</td><td>56.9</td><td>47.0</td><td>56.9</td><td>11.9</td><td>63.9</td><td>76.1</td><td>56.7</td><td>60.9</td><td>65.9</td><td>98.3</td><td>59.2</td><td>87.2</td><td>47.9</td><td>64.5</td></tr><tr><th>H3DNet [51]</th><th>4\\timesPointNet++</th><td>49.4</td><td>88.6</td><td>91.8</td><td>90.2</td><td>64.9</td><td>61.0</td><td>51.9</td><td>54.9</td><td>18.6</td><td>62.0</td><td>75.9</td><td>57.3</td><td>57.2</td><td>75.3</td><td>97.9</td><td>67.4</td><td>92.5</td><td>53.6</td><td>67.2</td></tr><tr><th>Ours (L6, O256)</th><th>PointNet++</th><td>54.1</td><td>86.2</td><td>92.0</td><td>84.8</td><td>67.8</td><td>55.8</td><td>46.9</td><td>48.5</td><td>15.0</td><td>59.4</td><td>80.4</td><td>64.2</td><td>57.2</td><td>76.3</td><td>97.6</td><td>76.8</td><td>92.5</td><td>55.0</td><td>67.3</td></tr><tr><th>Ours (L12, O256)</th><th>PointNet++</th><td>55.4</td><td>86.6</td><td>91.8</td><td>86.6</td><td>73.0</td><td>54.5</td><td>49.4</td><td>47.7</td><td>13.1</td><td>63.3</td><td>82.4</td><td>63.3</td><td>53.2</td><td>74.0</td><td>99.2</td><td>67.7</td><td>91.7</td><td>55.8</td><td>67.2</td></tr><tr><th>Ours (L12, O256)</th><th>PointNet++w2\\times</th><td>56.5</td><td>88.2</td><td>92.5</td><td>88.2</td><td>71.6</td><td>57.5</td><td>48.3</td><td>53.7</td><td>17.5</td><td>71.0</td><td>79.5</td><td>63.4</td><td>58.1</td><td>71.7</td><td>99.4</td><td>71.1</td><td>93.0</td><td>57.8</td><td>68.8</td></tr><tr><th>Ours (L12, O512)</th><th>PointNet++w2\\times</th><td>52.1</td><td>91.9</td><td>93.6</td><td>88.0</td><td>70.7</td><td>60.7</td><td>53.7</td><td>62.4</td><td>16.1</td><td>58.5</td><td>80.9</td><td>67.9</td><td>47.0</td><td>76.3</td><td>99.6</td><td>72.0</td><td>95.3</td><td>56.4</td><td>69.1</td></tr></tbody></table>", "caption": "Table 12: Performance of mAP@0.25 for each category on the ScanNet V2 dataset.", "list_citation_info": ["[31] Xie Qian, Lai Yu-kun, Wu Jing, Wang Zhoutao, Zhang Yiming, Xu Kai, and Wang Jun. Mlcvnet: Multi-level context votenet for 3d object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. arXiv preprint arXiv:2006.05682, 2020.", "[26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 9277\u20139286, 2019."]}, {"table": "<table><tbody><tr><th>methods</th><th>backbone</th><td>cab</td><td>bed</td><td>chair</td><td>sofa</td><td>tabl</td><td>door</td><td>wind</td><td>bkshf</td><td>pic</td><td>cntr</td><td>desk</td><td>curt</td><td>fridg</td><td>showr</td><td>toil</td><td>sink</td><td>bath</td><td>ofurn</td><td>mAP</td></tr><tr><th>VoteNet [26]</th><th>PointNet++</th><td>14.6</td><td>77.8</td><td>73.1</td><td>80.5</td><td>46.5</td><td>25.1</td><td>16.0</td><td>41.8</td><td>2.5</td><td>22.3</td><td>33.3</td><td>25.0</td><td>31.0</td><td>17.6</td><td>87.8</td><td>23.0</td><td>81.6</td><td>18.7</td><td>39.9</td></tr><tr><th>H3DNet [51]</th><th>4\\timesPointNet++</th><td>20.5</td><td>79.7</td><td>80.1</td><td>79.6</td><td>56.2</td><td>29.0</td><td>21.3</td><td>45.5</td><td>4.2</td><td>33.5</td><td>50.6</td><td>37.3</td><td>41.4</td><td>37.0</td><td>89.1</td><td>35.1</td><td>90.2</td><td>35.4</td><td>48.1</td></tr><tr><th>Ours (L6, O256)</th><th>PointNet++</th><td>23.0</td><td>78.4</td><td>78.9</td><td>68.7</td><td>55.1</td><td>35.3</td><td>23.6</td><td>39.4</td><td>7.5</td><td>27.2</td><td>66.4</td><td>43.3</td><td>43.0</td><td>41.2</td><td>89.7</td><td>38.0</td><td>83.4</td><td>37.3</td><td>48.9</td></tr><tr><th>Ours (L12, O256)</th><th>PointNet++</th><td>23.8</td><td>77.2</td><td>81.6</td><td>65.1</td><td>62.8</td><td>35.0</td><td>21.3</td><td>39.4</td><td>7.0</td><td>33.1</td><td>66.3</td><td>39.3</td><td>43.9</td><td>47.0</td><td>91.2</td><td>38.5</td><td>85.2</td><td>37.4</td><td>49.7</td></tr><tr><th>Ours (L12, O256)</th><th>PointNet++w2\\times</th><td>26.2</td><td>80.7</td><td>83.5</td><td>70.7</td><td>57.0</td><td>37.4</td><td>21.2</td><td>47.7</td><td>8.8</td><td>45.3</td><td>60.7</td><td>42.2</td><td>43.5</td><td>42.7</td><td>95.5</td><td>42.3</td><td>89.7</td><td>43.4</td><td>52.1</td></tr><tr><th>Ours (L12, O512)</th><th>PointNet++w2\\times</th><td>26.0</td><td>81.3</td><td>82.9</td><td>70.7</td><td>62.2</td><td>41.7</td><td>26.5</td><td>55.8</td><td>7.8</td><td>34.7</td><td>67.2</td><td>43.9</td><td>44.3</td><td>44.1</td><td>92.8</td><td>37.4</td><td>89.7</td><td>40.6</td><td>52.8</td></tr></tbody></table>", "caption": "Table 13: Performance of mAP@0.5 for each category on the ScanNet V2 dataset.", "list_citation_info": ["[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. arXiv preprint arXiv:2006.05682, 2020.", "[26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 9277\u20139286, 2019."]}, {"table": "<table><thead><tr><th>methods</th><th>backbone</th><th>bathtub</th><th>bed</th><th>bkshf</th><th>chair</th><th>desk</th><th>drser</th><th>nigtstd</th><th>sofa</th><th>table</th><th>toilet</th><th>mAP</th></tr></thead><tbody><tr><th>VoteNet [26]</th><th>PointNet++</th><td>75.5</td><td>85.6</td><td>31.9</td><td>77.4</td><td>24.8</td><td>27.9</td><td>58.6</td><td>67.4</td><td>51.1</td><td>90.5</td><td>59.1</td></tr><tr><th>MLCVNet [31]</th><th>PointNet++</th><td>79.2</td><td>85.8</td><td>31.9</td><td>75.8</td><td>26.5</td><td>31.3</td><td>61.5</td><td>66.3</td><td>50.4</td><td>89.1</td><td>59.8</td></tr><tr><th>HGNet [3]</th><th>PointNet++ w/ FPN</th><td>78.0</td><td>84.5</td><td>35.7</td><td>75.2</td><td>34.3</td><td>37.6</td><td>61.7</td><td>65.7</td><td>51.6</td><td>91.1</td><td>61.6</td></tr><tr><th>H3DNet [51]</th><th>4\\timesPointNet++</th><td>73.8</td><td>85.6</td><td>31.0</td><td>76.7</td><td>29.6</td><td>33.4</td><td>65.5</td><td>66.5</td><td>50.8</td><td>88.2</td><td>60.1</td></tr><tr><th>Ours (L6, O256)</th><th>PointNet++</th><td>80.0</td><td>87.8</td><td>32.5</td><td>79.4</td><td>32.6</td><td>36.0</td><td>66.7</td><td>70.0</td><td>53.8</td><td>91.1</td><td>63.0</td></tr></tbody></table>", "caption": "Table 14: Performance of mAP@0.25 for each category on the SUN RGB-D validation set.", "list_citation_info": ["[31] Xie Qian, Lai Yu-kun, Wu Jing, Wang Zhoutao, Zhang Yiming, Xu Kai, and Wang Jun. Mlcvnet: Multi-level context votenet for 3d object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. arXiv preprint arXiv:2006.05682, 2020.", "[26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 9277\u20139286, 2019.", "[3] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, and Jian Wu. A hierarchical graph network for 3d object detection on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 392\u2013401, 2020."]}, {"table": "<table><thead><tr><th>methods</th><th>backbone</th><th>bathtub</th><th>bed</th><th>bkshf</th><th>chair</th><th>desk</th><th>drser</th><th>nigtstd</th><th>sofa</th><th>table</th><th>toilet</th><th>mAP</th></tr></thead><tbody><tr><th>VoteNet [26]</th><th>PointNet++</th><td>45.4</td><td>53.4</td><td>6.8</td><td>56.5</td><td>5.9</td><td>12.0</td><td>38.6</td><td>49.1</td><td>21.3</td><td>68.5</td><td>35.8</td></tr><tr><th>H3DNet [51]</th><th>4\\timesPointNet++</th><td>47.6</td><td>52.9</td><td>8.6</td><td>60.1</td><td>8.4</td><td>20.6</td><td>45.6</td><td>50.4</td><td>27.1</td><td>69.1</td><td>39.0</td></tr><tr><th>Ours (L6, O256)</th><th>PointNet++</th><td>64.0</td><td>67.1</td><td>12.4</td><td>62.6</td><td>14.5</td><td>21.9</td><td>49.8</td><td>58.2</td><td>29.2</td><td>72.2</td><td>45.2</td></tr></tbody></table>", "caption": "Table 15: Performance of mAP@0.5 for each category on the SUN RGB-D validation set.", "list_citation_info": ["[51] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang. H3dnet: 3d object detection using hybrid geometric primitives. arXiv preprint arXiv:2006.05682, 2020.", "[26] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 9277\u20139286, 2019."]}]}