{"title": "PVT v2: Improved baselines with pyramid vision transformer", "abstract": "Transformer recently has presented encouraging progress in computer vision. In this work, we present new baselines by improving the original Pyramid Vision Transformer (PVT v1) by adding three designs, including (1) linear complexity attention layer, (2) overlapping patch embedding, and (3) convolutional feed-forward network. With these modifications, PVT v2 reduces the computational complexity of PVT v1 to linear and achieves significant improvements on fundamental vision tasks such as classification, detection, and segmentation. Notably, the proposed PVT v2 achieves comparable or better performances than recent works such as Swin Transformer. We hope this work will facilitate state-of-the-art Transformer researches in computer vision. Code is available at https://github.com/whai362/PVT.", "authors": ["Wenhai Wang", " Enze Xie", " Xiang Li", " Deng-Ping Fan", " Kaitao Song", " Ding Liang", " Tong Lu", " Ping Luo", " Ling Shao"], "pdf_url": "https://arxiv.org/abs/2106.13797", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>#Param (M)</td><td>GFLOPs</td><td>Top-1 Acc (%)</td></tr><tr><td>PVTv2-B0 (ours)</td><td>3.4</td><td>0.6</td><td>70.5</td></tr><tr><td>ResNet18 [14]</td><td>11.7</td><td>1.8</td><td>69.8</td></tr><tr><td>DeiT-Tiny/16 [31]</td><td>5.7</td><td>1.3</td><td>72.2</td></tr><tr><td>PVTv1-Tiny [33]</td><td>13.2</td><td>1.9</td><td>75.1</td></tr><tr><td>PVTv2-B1 (ours)</td><td>13.1</td><td>2.1</td><td>78.7</td></tr><tr><td>ResNet50 [14]</td><td>25.6</td><td>4.1</td><td>76.1</td></tr><tr><td>ResNeXt50-32x4d [35]</td><td>25.0</td><td>4.3</td><td>77.6</td></tr><tr><td>RegNetY-4G [26]</td><td>21.0</td><td>4.0</td><td>80.0</td></tr><tr><td>DeiT-Small/16 [31]</td><td>22.1</td><td>4.6</td><td>79.9</td></tr><tr><td>T2T-ViT{}_{t}-14 [37]</td><td>22.0</td><td>6.1</td><td>80.7</td></tr><tr><td>PVTv1-Small [33]</td><td>24.5</td><td>3.8</td><td>79.8</td></tr><tr><td>TNT-S [11]</td><td>23.8</td><td>5.2</td><td>81.3</td></tr><tr><td>Swin-T [23]</td><td>29.0</td><td>4.5</td><td>81.3</td></tr><tr><td>CvT-13 [34]</td><td>20.0</td><td>4.5</td><td>81.6</td></tr><tr><td>CoaT-Lite Small [36]</td><td>20.0</td><td>4.0</td><td>81.9</td></tr><tr><td>Twins-SVT-S [5]</td><td>24.0</td><td>2.8</td><td>81.7</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>22.6</td><td>3.9</td><td>82.1</td></tr><tr><td>PVTv2-B2 (ours)</td><td>25.4</td><td>4.0</td><td>82.0</td></tr><tr><td>ResNet101 [14]</td><td>44.7</td><td>7.9</td><td>77.4</td></tr><tr><td>ResNeXt101-32x4d [35]</td><td>44.2</td><td>8.0</td><td>78.8</td></tr><tr><td>RegNetY-8G [26]</td><td>39.0</td><td>8.0</td><td>81.7</td></tr><tr><td>T2T-ViT{}_{t}-19 [37]</td><td>39.0</td><td>9.8</td><td>81.4</td></tr><tr><td>PVTv1-Medium [33]</td><td>44.2</td><td>6.7</td><td>81.2</td></tr><tr><td>CvT-21 [34]</td><td>32.0</td><td>7.1</td><td>82.5</td></tr><tr><td>PVTv2-B3 (ours)</td><td>45.2</td><td>6.9</td><td>83.2</td></tr><tr><td>ResNet152 [14]</td><td>60.2</td><td>11.6</td><td>78.3</td></tr><tr><td>T2T-ViT{}_{t}-24  [37]</td><td>64.0</td><td>15.0</td><td>82.2</td></tr><tr><td>PVTv1-Large  [33]</td><td>61.4</td><td>9.8</td><td>81.7</td></tr><tr><td>TNT-B  [11]</td><td>66.0</td><td>14.1</td><td>82.8</td></tr><tr><td>Swin-S  [23]</td><td>50.0</td><td>8.7</td><td>83.0</td></tr><tr><td>Twins-SVT-B [5]</td><td>56.0</td><td>8.3</td><td>83.2</td></tr><tr><td>PVTv2-B4 (ours)</td><td>62.6</td><td>10.1</td><td>83.6</td></tr><tr><td>ResNeXt101-64x4d [35]</td><td>83.5</td><td>15.6</td><td>79.6</td></tr><tr><td>RegNetY-16G [26]</td><td>84.0</td><td>16.0</td><td>82.9</td></tr><tr><td>ViT-Base/16 [8]</td><td>86.6</td><td>17.6</td><td>81.8</td></tr><tr><td>DeiT-Base/16 [31]</td><td>86.6</td><td>17.6</td><td>81.8</td></tr><tr><td>Swin-B [23]</td><td>88.0</td><td>15.4</td><td>83.3</td></tr><tr><td>Twins-SVT-L [5]</td><td>99.2</td><td>14.8</td><td>83.7</td></tr><tr><td>PVTv2-B5 (ours)</td><td>82.0</td><td>11.8</td><td>83.8</td></tr></table>", "caption": "Table 2: Image classification performance on the ImageNet validation set.\u201c#Param\u201d refers to the number of parameters.\u201cGFLOPs\u201d is calculated under the input scale of 224\\times 224. \u201c*\u201d indicates the performance of the method trained under the strategy of its original paper.\u201c-Li\u201d denotes PVT v2 with linear SRA.", "list_citation_info": ["[37] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.", "[33] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.", "[36] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. arXiv preprint arXiv:2104.06399, 2021.", "[5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 2021.", "[34] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.", "[35] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.", "[31] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proc. Int. Conf. Mach. Learn., 2021.", "[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Proc. Int. Conf. Learn. Representations, 2021.", "[11] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.", "[26] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020.", "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016.", "[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Backbone</td><td colspan=\"7\">RetinaNet 1\\times</td><td colspan=\"7\">Mask R-CNN 1\\times</td></tr><tr><td>#P (M)</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td><td>#P (M)</td><td>AP{}^{\\rm b}</td><td>AP{}_{50}^{\\rm b}</td><td>AP{}_{75}^{\\rm b}</td><td>AP{}^{\\rm m}</td><td>AP{}_{50}^{\\rm m}</td><td>AP{}_{75}^{\\rm m}</td></tr><tr><td>PVTv2-B0</td><td>13.0</td><td>37.2</td><td>57.2</td><td>39.5</td><td>23.1</td><td>40.4</td><td>49.7</td><td>23.5</td><td>38.2</td><td>60.5</td><td>40.7</td><td>36.2</td><td>57.8</td><td>38.6</td></tr><tr><td>ResNet18 [14]</td><td>21.3</td><td>31.8</td><td>49.6</td><td>33.6</td><td>16.3</td><td>34.3</td><td>43.2</td><td>31.2</td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td></tr><tr><td>PVTv1-Tiny [33]</td><td>23.0</td><td>36.7</td><td>56.9</td><td>38.9</td><td>22.6</td><td>38.8</td><td>50.0</td><td>32.9</td><td>36.7</td><td>59.2</td><td>39.3</td><td>35.1</td><td>56.7</td><td>37.3</td></tr><tr><td>PVTv2-B1 (ours)</td><td>23.8</td><td>41.2</td><td>61.9</td><td>43.9</td><td>25.4</td><td>44.5</td><td>54.3</td><td>33.7</td><td>41.8</td><td>64.3</td><td>45.9</td><td>38.8</td><td>61.2</td><td>41.6</td></tr><tr><td>ResNet50 [14]</td><td>37.7</td><td>36.3</td><td>55.3</td><td>38.6</td><td>19.3</td><td>40.0</td><td>48.8</td><td>44.2</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td></tr><tr><td>PVTv1-Small [33]</td><td>34.2</td><td>40.4</td><td>61.3</td><td>43.0</td><td>25.0</td><td>42.9</td><td>55.7</td><td>44.1</td><td>40.4</td><td>62.9</td><td>43.8</td><td>37.8</td><td>60.1</td><td>40.3</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>32.3</td><td>43.6</td><td>64.7</td><td>46.8</td><td>28.3</td><td>47.6</td><td>57.4</td><td>42.2</td><td>44.1</td><td>66.3</td><td>48.4</td><td>40.5</td><td>63.2</td><td>43.6</td></tr><tr><td>PVTv2-B2 (ours)</td><td>35.1</td><td>44.6</td><td>65.6</td><td>47.6</td><td>27.4</td><td>48.8</td><td>58.6</td><td>45.0</td><td>45.3</td><td>67.1</td><td>49.6</td><td>41.2</td><td>64.2</td><td>44.4</td></tr><tr><td>ResNet101 [14]</td><td>56.7</td><td>38.5</td><td>57.8</td><td>41.2</td><td>21.4</td><td>42.6</td><td>51.1</td><td>63.2</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td></tr><tr><td>ResNeXt101-32x4d [35]</td><td>56.4</td><td>39.9</td><td>59.6</td><td>42.7</td><td>22.3</td><td>44.2</td><td>52.5</td><td>62.8</td><td>41.9</td><td>62.5</td><td>45.9</td><td>37.5</td><td>59.4</td><td>40.2</td></tr><tr><td>PVTv1-Medium [33]</td><td>53.9</td><td>41.9</td><td>63.1</td><td>44.3</td><td>25.0</td><td>44.9</td><td>57.6</td><td>63.9</td><td>42.0</td><td>64.4</td><td>45.6</td><td>39.0</td><td>61.6</td><td>42.1</td></tr><tr><td>PVTv2-B3 (ours)</td><td>55.0</td><td>45.9</td><td>66.8</td><td>49.3</td><td>28.6</td><td>49.8</td><td>61.4</td><td>64.9</td><td>47.0</td><td>68.1</td><td>51.7</td><td>42.5</td><td>65.7</td><td>45.7</td></tr><tr><td>PVTv1-Large [33]</td><td>71.1</td><td>42.6</td><td>63.7</td><td>45.4</td><td>25.8</td><td>46.0</td><td>58.4</td><td>81.0</td><td>42.9</td><td>65.0</td><td>46.6</td><td>39.5</td><td>61.9</td><td>42.5</td></tr><tr><td>PVTv2-B4 (ours)</td><td>72.3</td><td>46.1</td><td>66.9</td><td>49.2</td><td>28.4</td><td>50.0</td><td>62.2</td><td>82.2</td><td>47.5</td><td>68.7</td><td>52.0</td><td>42.7</td><td>66.1</td><td>46.1</td></tr><tr><td>ResNeXt101-64x4d [35]</td><td>95.5</td><td>41.0</td><td>60.9</td><td>44.0</td><td>23.9</td><td>45.2</td><td>54.0</td><td>101.9</td><td>42.8</td><td>63.8</td><td>47.3</td><td>38.4</td><td>60.6</td><td>41.3</td></tr><tr><td>PVTv2-B5 (ours)</td><td>91.7</td><td>46.2</td><td>67.1</td><td>49.5</td><td>28.5</td><td>50.0</td><td>62.5</td><td>101.6</td><td>47.4</td><td>68.6</td><td>51.9</td><td>42.5</td><td>65.7</td><td>46.0</td></tr></table>", "caption": "Table 3: Object detection and instance segmentation on COCO val2017.\u201c#P\u201d refers to parameter number. AP{}^{\\rm b} and AP{}^{\\rm m} denote bounding box AP and mask AP, respectively.\u201c-Li\u201d denotes PVT v2 with linear SRA.", "list_citation_info": ["[35] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.", "[33] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.", "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016."]}, {"table": "<table><tr><td>Backbone</td><td>Method</td><td>AP{}^{\\text{b}}</td><td>AP{}^{\\text{b}}_{\\text{50}}</td><td>AP{}^{\\text{b}}_{\\text{75}}</td><td>#P (M)</td><td>GFLOPs</td></tr><tr><td>ResNet50 [14]</td><td rowspan=\"4\"> CascadeMaskR-CNN [1] </td><td>46.3</td><td>64.3</td><td>50.5</td><td>82</td><td>739</td></tr><tr><td>Swin-T [23]</td><td>50.5</td><td>69.3</td><td>54.9</td><td>86</td><td>745</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>50.9</td><td>69.5</td><td>55.2</td><td>80</td><td>725</td></tr><tr><td>PVTv2-B2 (ours)</td><td>51.1</td><td>69.8</td><td>55.3</td><td>83</td><td>788</td></tr><tr><td>ResNet50 [14]</td><td rowspan=\"4\">ATSS [39]</td><td>43.5</td><td>61.9</td><td>47.0</td><td>32</td><td>205</td></tr><tr><td>Swin-T [23]</td><td>47.2</td><td>66.5</td><td>51.3</td><td>36</td><td>215</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>48.9</td><td>68.1</td><td>53.4</td><td>30</td><td>194</td></tr><tr><td>PVTv2-B2 (ours)</td><td>49.9</td><td>69.1</td><td>54.1</td><td>33</td><td>258</td></tr><tr><td>ResNet50 [14]</td><td rowspan=\"4\">GFL [19]</td><td>44.5</td><td>63.0</td><td>48.3</td><td>32</td><td>208</td></tr><tr><td>Swin-T [23]</td><td>47.6</td><td>66.8</td><td>51.7</td><td>36</td><td>215</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>49.2</td><td>68.2</td><td>53.7</td><td>30</td><td>197</td></tr><tr><td>PVTv2-B2 (ours)</td><td>50.2</td><td>69.4</td><td>54.7</td><td>33</td><td>261</td></tr><tr><td>ResNet50 [14]</td><td rowspan=\"4\"> SparseR-CNN [28] </td><td>44.5</td><td>63.4</td><td>48.2</td><td>106</td><td>166</td></tr><tr><td>Swin-T [23]</td><td>47.9</td><td>67.3</td><td>52.3</td><td>110</td><td>172</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>48.9</td><td>68.3</td><td>53.4</td><td>104</td><td>151</td></tr><tr><td>PVTv2-B2 (ours)</td><td>50.1</td><td>69.5</td><td>54.9</td><td>107</td><td>215</td></tr></table>", "caption": "Table 4: Compare with Swin Transformer on object detection.\u201cAP{}^{\\rm b}\u201d denotes bounding box AP.\u201c#P\u201d refers to parameter number.\u201cGFLOPs\u201d is calculated under the input scale of 1280\\times 800. \u201c-Li\u201d denotes PVT v2 with linear SRA.", "list_citation_info": ["[1] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018.", "[28] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2021.", "[39] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020.", "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016.", "[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[19] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. In Proc. Advances in Neural Inf. Process. Syst., 2020."]}, {"table": "<table><tr><td rowspan=\"2\">Backbone</td><td colspan=\"3\">Semantic FPN</td></tr><tr><td>#Param (M)</td><td>GFLOPs</td><td>mIoU (%)</td></tr><tr><td>PVTv2-B0 (ours)</td><td>7.6</td><td>25.0</td><td>37.2</td></tr><tr><td>ResNet18 [14]</td><td>15.5</td><td>32.2</td><td>32.9</td></tr><tr><td>PVTv1-Tiny [33]</td><td>17.0</td><td>33.2</td><td>35.7</td></tr><tr><td>PVTv2-B1 (ours)</td><td>17.8</td><td>34.2</td><td>42.5</td></tr><tr><td>ResNet50 [14]</td><td>28.5</td><td>45.6</td><td>36.7</td></tr><tr><td>PVTv1-Small [33]</td><td>28.2</td><td>44.5</td><td>39.8</td></tr><tr><td>PVTv2-B2-Li (ours)</td><td>26.3</td><td>41.0</td><td>45.1</td></tr><tr><td>PVTv2-B2 (ours)</td><td>29.1</td><td>45.8</td><td>45.2</td></tr><tr><td>ResNet101 [14]</td><td>47.5</td><td>65.1</td><td>38.8</td></tr><tr><td>ResNeXt101-32x4d [35]</td><td>47.1</td><td>64.7</td><td>39.7</td></tr><tr><td>PVTv1-Medium [33]</td><td>48.0</td><td>61.0</td><td>41.6</td></tr><tr><td>PVTv2-B3 (ours)</td><td>49.0</td><td>62.4</td><td>47.3</td></tr><tr><td>PVTv1-Large [33]</td><td>65.1</td><td>79.6</td><td>42.1</td></tr><tr><td>PVTv2-B4 (ours)</td><td>66.3</td><td>81.3</td><td>47.9</td></tr><tr><td>ResNeXt101-64x4d [35]</td><td>86.4</td><td>103.9</td><td>40.2</td></tr><tr><td>PVTv2-B5 (ours)</td><td>85.7</td><td>91.1</td><td>48.7</td></tr></table>", "caption": "Table 5: Semantic segmentation performance of different backbones on the ADE20K validation set.\u201cGFLOPs\u201d is calculated under the input scale of 512\\times 512.\u201c-Li\u201d denotes PVT v2 with linear SRA.", "list_citation_info": ["[35] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.", "[33] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.", "[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016."]}, {"table": "<table><tr><td rowspan=\"2\">#</td><td rowspan=\"2\">Setting</td><td rowspan=\"2\">Top-1Acc (%)</td><td colspan=\"3\">RetinaNet 1x</td></tr><tr><td>#P (M)</td><td>GFLOPs</td><td>AP</td></tr><tr><td>1</td><td>PVTv1-Small [33]</td><td>79.8</td><td>34.2</td><td>285.8</td><td>40.4</td></tr><tr><td>2</td><td>+ OPE</td><td>81.1</td><td>34.9</td><td>288.6</td><td>42.2</td></tr><tr><td>3</td><td>++ CFFN (PVTv2-B2)</td><td>82.0</td><td>35.1</td><td>290.7</td><td>44.6</td></tr><tr><td>4</td><td>+++ LSRA (PVTv2-B2-Li)</td><td>82.1</td><td>32.3</td><td>227.4</td><td>43.6</td></tr></table>", "caption": "Table 6: Ablation experiments of PVT v2. \u201cOPE\u201d, \u201cCFFN\u201d, and \u201cLSRA\u201d represent overlapping patch embedding, convolutional feed-forward network, and linear SRA, respectively.", "list_citation_info": ["[33] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021."]}]}