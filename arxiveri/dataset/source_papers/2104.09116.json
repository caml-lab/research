{"title": "TransCrowd: Weakly-supervised crowd counting with transformer", "abstract": "The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count-level annotations, a more economical way of labeling. Current weakly-supervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus cannot achieve satisfactory performance, with limited applications in the real world. The transformer is a popular sequence-to-sequence prediction model in natural language processing (NLP), which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on transformers. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of transformer. To the best of our knowledge, this is the first work to adopt a pure transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods.", "authors": ["Dingkang Liang", " Xiwu Chen", " Wei Xu", " Yu Zhou", " Xiang Bai"], "pdf_url": "https://arxiv.org/abs/2104.09116", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Year</td><td colspan=\"2\">Training label</td><td colspan=\"2\">UCF-QNRF</td><td colspan=\"2\">Part A</td><td colspan=\"2\">Part B</td></tr><tr><td>Location</td><td>Crowd number</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>MCNN [66]</td><td>CVPR 16</td><td>\\surd</td><td>\\surd</td><td>277.0</td><td>426.0</td><td>110.2</td><td>173.2</td><td>26.4</td><td>41.3</td></tr><tr><td>CL [14]</td><td>ECCV 18</td><td>\\surd</td><td>\\surd</td><td>132.0</td><td>191.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CSRNet [21]</td><td>CVPR 18</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>68.2</td><td>115.0</td><td>10.6</td><td>16.0</td></tr><tr><td>L2R [30]</td><td>TPAMI 19</td><td>\\surd</td><td>\\surd</td><td>124.0</td><td>196.0</td><td>73.6</td><td>112.0</td><td>13.7</td><td>21.4</td></tr><tr><td>CFF [42]</td><td>ICCV 19</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>65.2</td><td>109.4</td><td>7.2</td><td>12.2</td></tr><tr><td>PGCNet [60]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>57.0</td><td>86.0</td><td>8.8</td><td>13.7</td></tr><tr><td>TEDnet [15]</td><td>CVPR 19</td><td>\\surd</td><td>\\surd</td><td>113.0</td><td>188.0</td><td>64.2</td><td>109.1</td><td>8.2</td><td>12.8</td></tr><tr><td>BL [34]</td><td>ICCV 19</td><td>\\surd</td><td>\\surd</td><td>88.7</td><td>154.8</td><td>62.8</td><td>101.8</td><td>7.7</td><td>12.7</td></tr><tr><td>ASNet [16]</td><td>CVPR 20</td><td>\\surd</td><td>\\surd</td><td>91.5</td><td>159.7</td><td>57.7</td><td>90.1</td><td>-</td><td>-</td></tr><tr><td>LibraNet [25]</td><td>ECCV20</td><td>\\surd</td><td>\\surd</td><td>88.1</td><td>143.7</td><td>55.9</td><td>97.1</td><td>7.3</td><td>11.3</td></tr><tr><td>NoisyCC [50]</td><td>NeurIPS 20</td><td>\\surd</td><td>\\surd</td><td>85.8</td><td>150.6</td><td>61.9</td><td>99.6</td><td>7.4</td><td>11.3</td></tr><tr><td>DM-Count [50]</td><td>NeurIPS 20</td><td>\\surd</td><td>\\surd</td><td>85.6</td><td>148.3</td><td>59.7</td><td>95.7</td><td>7.4</td><td>11.8</td></tr><tr><td>Method in [35]</td><td>MM 20</td><td>\\surd</td><td>\\surd</td><td>84.7</td><td>147.2</td><td>58.1</td><td>91.7</td><td>6.5</td><td>10.1</td></tr><tr><td>S3 [23]</td><td>IJCAI 21</td><td>\\surd</td><td>\\surd</td><td>80.6</td><td>139.8</td><td>57.0</td><td>96.0</td><td>6.3</td><td>10.6</td></tr><tr><td>UOT [36]</td><td>AAAI 21</td><td>\\surd</td><td>\\surd</td><td>83.3</td><td>142.3</td><td>58.1</td><td>95.9</td><td>6.5</td><td>10.2</td></tr><tr><td>Method in [62]*</td><td>ECCV 20</td><td>-</td><td>\\surd</td><td>-</td><td>-</td><td>104.6</td><td>145.2</td><td>12.3</td><td>21.2</td></tr><tr><td>MATT [20]*</td><td>PR 21</td><td>-</td><td>\\surd</td><td>-</td><td>-</td><td>80.1</td><td>129.4</td><td>11.7</td><td>17.5</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>98.9</td><td>176.1</td><td>69.0</td><td>116.5</td><td>10.6</td><td>19.7</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>97.2</td><td>168.5</td><td>66.1</td><td>105.1</td><td>9.3</td><td>16.1</td></tr></table>", "caption": "Table 1: Quantitative comparison (in terms of MAE and MSE) of the proposed method and some popular methods on three widely adopted benchmark datasets. * represents the weakly-supervised method.", "list_citation_info": ["[50] Jia Wan and Antoni Chan. Modeling noisy annotations for crowd counting. Advances in Neural Information Processing Systems, 2020.", "[36] Zhiheng Ma, Xing Wei, Xiaopeng Hong, Hui Lin, Yunfeng Qiu, and Yihong Gong. Learning to count via unbalanced optimal transport. In Proc. of the AAAI Conf. on Artificial Intelligence, volume 35, pages 2319\u20132327, 2021.", "[42] Zenglin Shi, Pascal Mettes, and Cees GM Snoek. Counting with focus for free. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 4200\u20134209, 2019.", "[15] Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin Cao, David Doermann, and Ling Shao. Crowd counting and density estimation by trellis encoder-decoder networks. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[62] Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, and Nicu Sebe. Weakly-supervised crowd counting learns from sorting rather than locations. In Proc. of European Conference on Computer Vision, 2020.", "[35] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Learning scales from points: A scale-aware probabilistic model for crowd counting. In Proc. of ACM Multimedia, pages 220\u2013228, 2020.", "[14] Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak Shah. Composition loss for counting, density map estimation and localization in dense crowds. In Proc. of European Conference on Computer Vision, 2018.", "[34] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[25] Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua Shen. Weighing counts: Sequential crowd counting by reinforcement learning. In Proc. of European Conference on Computer Vision, pages 164\u2013181. Springer, 2020.", "[23] Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yunfeng Qiu, Yaowei Wang, and Yihong Gong. Direct measure matching for crowd counting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 2021.", "[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[30] Xialei Liu, Joost Van De Weijer, and Andrew D Bagdanov. Exploiting unlabeled data in cnns by self-supervised learning to rank. IEEE transactions on pattern analysis and machine intelligence, 2019.", "[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016.", "[16] Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, and Yanwei Pang. Attention scaling for crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2020.", "[60] Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, and Errui Ding. Perspective-guided convolution networks for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[20] Yinjie Lei, Yan Liu, Pingping Zhang, and Lingqiao Liu. Towards using count-level weak supervision for crowd counting. Pattern Recognition, 109:107616, 2021."]}, {"table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">Year</td><td colspan=\"2\" rowspan=\"2\">Training label</td><td colspan=\"8\">Val set</td></tr><tr><td colspan=\"2\">Low</td><td colspan=\"2\">Medium</td><td colspan=\"2\">High</td><td colspan=\"2\">Overall</td></tr><tr><td>Location</td><td>Crowd number</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>MCNN [66]</td><td>CVPR16</td><td>\\surd</td><td>\\surd</td><td>90.6</td><td>202.9</td><td>125.3</td><td>259.5</td><td>494.9</td><td>856.0</td><td>160.6</td><td>377.7</td></tr><tr><td>CMTL [45]</td><td>AVSS17</td><td>\\surd</td><td>\\surd</td><td>50.2</td><td>129.2</td><td>88.1</td><td>170.7</td><td>583.1</td><td>986.5</td><td>138.1</td><td>379.5</td></tr><tr><td>DSSI-Net [26]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>50.3</td><td>85.9</td><td>82.4</td><td>164.5</td><td>436.6</td><td>814.0</td><td>116.6</td><td>317.4</td></tr><tr><td>CAN [29]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>34.2</td><td>69.5</td><td>65.6</td><td>115.3</td><td>336.4</td><td>619.7</td><td>89.5</td><td>239.3</td></tr><tr><td>SANet [3]</td><td>ECCV18</td><td>\\surd</td><td>\\surd</td><td>13.6</td><td>26.8</td><td>50.4</td><td>78.0</td><td>397.8</td><td>749.2</td><td>82.1</td><td>272.6</td></tr><tr><td>CSRNet [21]</td><td>CVPR18</td><td>\\surd</td><td>\\surd</td><td>22.2</td><td>40.0</td><td>49.0</td><td>99.5</td><td>302.5</td><td>669.5</td><td>72.2</td><td>249.9</td></tr><tr><td>CG-DRCN [44]</td><td>PAMI20</td><td>\\surd</td><td>\\surd</td><td>17.1</td><td>44.7</td><td>40.8</td><td>71.2</td><td>317.4</td><td>719.8</td><td>67.9</td><td>262.1</td></tr><tr><td>MBTTBF [47]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>23.3</td><td>48.5</td><td>53.2</td><td>119.9</td><td>294.5</td><td>674.5</td><td>73.8</td><td>256.8</td></tr><tr><td>SFCN [56]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>11.8</td><td>19.8</td><td>39.3</td><td>73.4</td><td>297.3</td><td>679.4</td><td>62.9</td><td>247.5</td></tr><tr><td>BL [34]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>6.9</td><td>10.3</td><td>39.7</td><td>85.2</td><td>279.8</td><td>620.4</td><td>59.3</td><td>229.2</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>7.1</td><td>10.7</td><td>33.3</td><td>54.6</td><td>302.5</td><td>557.4</td><td>58.4</td><td>201.1</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>6.7</td><td>9.5</td><td>34.5</td><td>55.8</td><td>285.9</td><td>532.8</td><td>56.8</td><td>193.6</td></tr></table>", "caption": "Table 2: Quantitative results on the JHU-Crowd++ (val set) dataset. \u201dLow\u201d, \u201dMedium\u201d and \u201dHigh\u201d respectively indicates three categories based on different ranges:[0,50], (50,500], and &gt;500. * represents the weakly-supervised crowd counting methods.", "list_citation_info": ["[44] Vishwanath Sindagi, Rajeev Yasarla, and Vishal MM Patel. Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[56] Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learning from synthetic data for crowd counting in the wild. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[26] Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, and Liang Lin. Crowd counting with deep structured scale integration network. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[3] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale aggregation network for accurate and efficient crowd counting. In Proc. of European Conference on Computer Vision, 2018.", "[29] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[34] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016.", "[47] Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom feature fusion for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[45] Vishwanath A Sindagi and Vishal M Patel. Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. In Proc. of IEEE Intl. Conf. on Advanced Video and Signal Based Surveillance, 2017."]}, {"table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">Year</td><td colspan=\"2\" rowspan=\"2\">Training label</td><td colspan=\"8\">Testing set</td></tr><tr><td colspan=\"2\">Low</td><td colspan=\"2\">Medium</td><td colspan=\"2\">High</td><td colspan=\"2\">Overall</td></tr><tr><td>Location</td><td>Crowd number</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>MCNN [66]</td><td>CVPR16</td><td>\\surd</td><td>\\surd</td><td>97.1</td><td>192.3</td><td>121.4</td><td>191.3</td><td>618.6</td><td>1,166.7</td><td>188.9</td><td>483.4</td></tr><tr><td>CMTL [45]</td><td>AVSS17</td><td>\\surd</td><td>\\surd</td><td>58.5</td><td>136.4</td><td>81.7</td><td>144.7</td><td>635.3</td><td>1,225.3</td><td>157.8</td><td>490.4</td></tr><tr><td>DSSI-Net [26]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>53.6</td><td>112.8</td><td>70.3</td><td>108.6</td><td>525.5</td><td>1,047.4</td><td>133.5</td><td>416.5</td></tr><tr><td>CAN [29]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>37.6</td><td>78.8</td><td>56.4</td><td>86.2</td><td>384.2</td><td>789.0</td><td>100.1</td><td>314.0</td></tr><tr><td>SANet [3]</td><td>ECCV18</td><td>\\surd</td><td>\\surd</td><td>17.3</td><td>37.9</td><td>46.8</td><td>69.1</td><td>397.9</td><td>817.7</td><td>91.1</td><td>320.4</td></tr><tr><td>CSRNet [21]</td><td>CVPR18</td><td>\\surd</td><td>\\surd</td><td>27.1</td><td>64.9</td><td>43.9</td><td>71.2</td><td>356.2</td><td>784.4</td><td>85.9</td><td>309.2</td></tr><tr><td>CG-DRCN [44]</td><td>PAMI20</td><td>\\surd</td><td>\\surd</td><td>19.5</td><td>58.7</td><td>38.4</td><td>62.7</td><td>367.3</td><td>837.5</td><td>82.3</td><td>328.0</td></tr><tr><td>MBTTBF [47]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>19.2</td><td>58.8</td><td>41.6</td><td>66.0</td><td>352.2</td><td>760.4</td><td>81.8</td><td>299.1</td></tr><tr><td>SFCN [56]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>16.5</td><td>55.7</td><td>38.1</td><td>59.8</td><td>341.8</td><td>758.8</td><td>77.5</td><td>297.6</td></tr><tr><td>BL [34]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>10.1</td><td>32.7</td><td>34.2</td><td>54.5</td><td>352.0</td><td>768.7</td><td>75.0</td><td>299.9</td></tr><tr><td>UOT [36]</td><td>AAAI21</td><td>\\surd</td><td>\\surd</td><td>11.2</td><td>26.2</td><td>28.7</td><td>45.3</td><td>274.1</td><td>648.2</td><td>60.5</td><td>252.7</td></tr><tr><td>S3 [23]</td><td>IJCAI21</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>59.4</td><td>244.0</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>8.5</td><td>23.2</td><td>33.3</td><td>71.5</td><td>368.3</td><td>816.4</td><td>76.4</td><td>319.8</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>7.6</td><td>16.7</td><td>34.8</td><td>73.6</td><td>354.8</td><td>752.8</td><td>74.9</td><td>295.6</td></tr></table>", "caption": "Table 3: Quantitative results on the JHU-Crowd++ (testing set) dataset. \u201dLow\u201d, \u201dMedium\u201d and \u201dHigh\u201d respectively indicates three categories based on different ranges:[0,50], (50,500], and &gt;500. * represents the weakly-supervised crowd counting methods.", "list_citation_info": ["[44] Vishwanath Sindagi, Rajeev Yasarla, and Vishal MM Patel. Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[56] Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learning from synthetic data for crowd counting in the wild. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[36] Zhiheng Ma, Xing Wei, Xiaopeng Hong, Hui Lin, Yunfeng Qiu, and Yihong Gong. Learning to count via unbalanced optimal transport. In Proc. of the AAAI Conf. on Artificial Intelligence, volume 35, pages 2319\u20132327, 2021.", "[26] Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, and Liang Lin. Crowd counting with deep structured scale integration network. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[3] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale aggregation network for accurate and efficient crowd counting. In Proc. of European Conference on Computer Vision, 2018.", "[29] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[34] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[23] Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yunfeng Qiu, Yaowei Wang, and Yihong Gong. Direct measure matching for crowd counting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 2021.", "[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016.", "[47] Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom feature fusion for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[45] Vishwanath A Sindagi and Vishal M Patel. Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. In Proc. of IEEE Intl. Conf. on Advanced Video and Signal Based Surveillance, 2017."]}, {"table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">Year</td><td colspan=\"2\" rowspan=\"2\">Training label</td><td colspan=\"2\">Val set</td><td colspan=\"4\">Testing set</td></tr><tr><td colspan=\"2\">Overall</td><td colspan=\"2\">Overall</td><td colspan=\"2\">Scene Level (only MAE)</td></tr><tr><td>Location</td><td>Crowd number</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>Avg.</td><td>S0\\sim S4</td></tr><tr><td>C3F-VGG [9]</td><td>Tech19</td><td>\\surd</td><td>\\surd</td><td>105.79</td><td>504.39</td><td>127.0</td><td>439.6</td><td>666.9</td><td>140.9/26.5/58.0/307.1/2801.8</td></tr><tr><td>CSRNet [21]</td><td>CVPR18</td><td>\\surd</td><td>\\surd</td><td>104.89</td><td>433.48</td><td>121.3</td><td>387.8</td><td>522.7</td><td>176.0/35.8/59.8/285.8/2055.8</td></tr><tr><td>PCC-Net-VGG  [10]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>100.77</td><td>573.19</td><td>112.3</td><td>457.0</td><td>777.6</td><td>103.9/13.7/42.0/259.5/3469.1</td></tr><tr><td>CAN [29]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>93.58</td><td>489.90</td><td>106.3</td><td>386.5</td><td>612.2</td><td>82.6/14.7/46.6/269.7/2647.0</td></tr><tr><td>SFCN\u2020 [56]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>95.46</td><td>608.32</td><td>105.7</td><td>424.1</td><td>712.7</td><td>54.2/14.8/44.4/249.6/3200.5</td></tr><tr><td>BL [34]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>93.64</td><td>470.38</td><td>105.4</td><td>454.2</td><td>750.5</td><td>66.5/8.7/41.2/249.9/3386.4</td></tr><tr><td>KDMG [52]</td><td>PAMI20</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>100.5</td><td>415.5</td><td>632.7</td><td>77.3/10.3/38.5/259.4/2777.9</td></tr><tr><td>NoisyCC [50]</td><td>NeurIPS20</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>96.9</td><td>534.2</td><td>608.1</td><td>218.7/10.7/35.2/203.2/2572.8</td></tr><tr><td>DM-Count  [53]</td><td>NeurIPS20</td><td>\\surd</td><td>\\surd</td><td>70.5</td><td>357.6</td><td>88.4</td><td>388.6</td><td>498.0</td><td>146.6/7.6/31.2/228.7/2075.8</td></tr><tr><td>S3 [23]</td><td>IJCAI21</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>87.8</td><td>387.5</td><td>566.5</td><td>80.7 / 7.9 / 36.3 / 212.0 / 2495.4</td></tr><tr><td>UOT [36]</td><td>AAAI21</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>83.5</td><td>346.9</td><td>-</td><td>-</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>88.2</td><td>446.9</td><td>119.6</td><td>463.9</td><td>736.0</td><td>88.0/12.7/47.2/311.2/3216.1</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>88.4</td><td>400.5</td><td>117.7</td><td>451.0</td><td>737.8</td><td>69.3/12.8/46.0/309.0/3252.2</td></tr></table>", "caption": "Table 4: Comparison of the counting performance on the NWPU-Crowd. S0\\!\\sim\\!S4 respectively indicate five categories according to the different number range: 0, (0,100], (100,500], (500,5000], \\textgreater 5000. * represents the weakly-supervised crowd counting methods.", "list_citation_info": ["[50] Jia Wan and Antoni Chan. Modeling noisy annotations for crowd counting. Advances in Neural Information Processing Systems, 2020.", "[56] Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learning from synthetic data for crowd counting in the wild. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[53] Boyu Wang, Huidong Liu, Dimitris Samaras, and Minh Hoai. Distribution matching for crowd counting. In Proc. of Advances in Neural Information Processing Systems, 2020.", "[36] Zhiheng Ma, Xing Wei, Xiaopeng Hong, Hui Lin, Yunfeng Qiu, and Yihong Gong. Learning to count via unbalanced optimal transport. In Proc. of the AAAI Conf. on Artificial Intelligence, volume 35, pages 2319\u20132327, 2021.", "[52] Jia Wan, Qingzhong Wang, and Antoni B Chan. Kernel-based density map generation for dense object counting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[29] Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[34] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[23] Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yunfeng Qiu, Yaowei Wang, and Yihong Gong. Direct measure matching for crowd counting. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 2021.", "[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[9] Junyu Gao, Wei Lin, Bin Zhao, Dong Wang, Chenyu Gao, and Jun Wen. C^ 3 framework: An open-source pytorch code for crowd counting. arXiv preprint arXiv:1907.02724, 2019.", "[10] Junyu Gao, Qi Wang, and Xuelong Li. Pcc net: Perspective crowd counting via spatial convolutional network. IEEE Transactions on Circuits and Systems for Video Technology, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Training label</td><td rowspan=\"2\">MAE</td><td rowspan=\"2\">MSE</td></tr><tr><td>Location</td><td>Crowd number</td></tr><tr><td>MCNN [65]</td><td>\\surd</td><td>\\surd</td><td>377.6</td><td>509.1</td></tr><tr><td>CSRNet [21]</td><td>\\surd</td><td>\\surd</td><td>266.1</td><td>397.5</td></tr><tr><td>ADCrowdNet [27]</td><td>\\surd</td><td>\\surd</td><td>257.1</td><td>363.5</td></tr><tr><td>MATT [20]*</td><td>-</td><td>\\surd</td><td>355.0</td><td>550.2</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>\\surd</td><td>288.9</td><td>407.6</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>\\surd</td><td>272.2</td><td>395.3</td></tr></table>", "caption": "Table 5: The performance comparison on the UCF_CC_50 dataset. * represents the weakly-supervised crowd counting methods.", "list_citation_info": ["[65] Shanghang Zhang, Guanhang Wu, Joao P Costeira, and Jos\u00e9 MF Moura. Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. In Porc. of IEEE Intl. Conf. on Computer Vision, 2017.", "[27] Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, and Hefeng Wu. Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[20] Yinjie Lei, Yan Liu, Pingping Zhang, and Lingqiao Liu. Towards using count-level weak supervision for crowd counting. Pattern Recognition, 109:107616, 2021.", "[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Training label</td><td colspan=\"6\">MAE</td></tr><tr><td>Location</td><td>Crowd number</td><td>S1</td><td>S2</td><td>S3</td><td>S4</td><td>S5</td><td>Ave</td></tr><tr><td>M-CNN [66]</td><td>\\surd</td><td>\\surd</td><td>3.4</td><td>20.6</td><td>12.9</td><td>13.0</td><td>8.1</td><td>11.6</td></tr><tr><td>CP-CNN[46]</td><td>\\surd</td><td>\\surd</td><td>2.9</td><td>14.7</td><td>10.5</td><td>10.4</td><td>5.8</td><td>8.8</td></tr><tr><td>Liu et al.[24]</td><td>\\surd</td><td>\\surd</td><td>2.0</td><td>13.1</td><td>8.9</td><td>17.4</td><td>4.8</td><td>9.2</td></tr><tr><td>IC-CNN[37]</td><td>\\surd</td><td>\\surd</td><td>17.0</td><td>12.3</td><td>9.2</td><td>8.1</td><td>4.7</td><td>10.3</td></tr><tr><td>CSR-Net[21]</td><td>\\surd</td><td>\\surd</td><td>2.9</td><td>11.5</td><td>8.6</td><td>16.6</td><td>3.4</td><td>8.6</td></tr><tr><td>SA-Net[3]</td><td>\\surd</td><td>\\surd</td><td>2.6</td><td>13.2</td><td>9.0</td><td>13.3</td><td>3.0</td><td>8.2</td></tr><tr><td>LSC-CNN[39]</td><td>\\surd</td><td>\\surd</td><td>2.9</td><td>11.3</td><td>9.4</td><td>12.3</td><td>4.3</td><td>8.0</td></tr><tr><td>MATT[20]*</td><td>-</td><td>\\surd</td><td>3.8</td><td>13.1</td><td>10.4</td><td>15.9</td><td>5.3</td><td>9.7</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>\\surd</td><td>2.3</td><td>14.2</td><td>9.9</td><td>14.0</td><td>4.3</td><td>8.9</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>\\surd</td><td>2.1</td><td>13.3</td><td>8.9</td><td>13.8</td><td>4.4</td><td>8.5</td></tr></table>", "caption": "Table 6: Comparison results of different methods on 5 scenes in the WorldExpo\u201910 dataset. * represents the weakly-supervised crowd counting methods.", "list_citation_info": ["[20] Yinjie Lei, Yan Liu, Pingping Zhang, and Lingqiao Liu. Towards using count-level weak supervision for crowd counting. Pattern Recognition, 109:107616, 2021.", "[3] Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale aggregation network for accurate and efficient crowd counting. In Proc. of European Conference on Computer Vision, 2018.", "[24] Jiang Liu, Chenqiang Gao, Deyu Meng, and Alexander G Hauptmann. Decidenet: counting varying density crowds through attention guided detection and density estimation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016.", "[39] Deepak Babu Sam, Skand Vishwanath Peri, Mukuntha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[37] Viresh Ranjan, Hieu Le, and Minh Hoai. Iterative crowd counting. In Proc. of European Conference on Computer Vision, 2018.", "[46] Vishwanath A Sindagi and Vishal M Patel. Generating high-quality crowd density maps using contextual pyramid cnns. In Porc. of IEEE Intl. Conf. on Computer Vision, 2017."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Training label</td><td rowspan=\"2\">MAE</td><td rowspan=\"2\">MSE</td></tr><tr><td>Location</td><td>Crowd number</td></tr><tr><td>FCN-HA [65]</td><td>\\surd</td><td>\\surd</td><td>4.21</td><td>-</td></tr><tr><td>CSRNet [21]</td><td>\\surd</td><td>\\surd</td><td>3.56</td><td>-</td></tr><tr><td>ADCrowdNet [27]</td><td>\\surd</td><td>\\surd</td><td>2.44</td><td>-</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>\\surd</td><td>3.28</td><td>4.80</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>\\surd</td><td>3.23</td><td>4.66</td></tr></table>", "caption": "Table 7: The performance comparison on the Trancos dataset. * represents the weakly-supervised crowd counting methods.", "list_citation_info": ["[65] Shanghang Zhang, Guanhang Wu, Joao P Costeira, and Jos\u00e9 MF Moura. Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. In Porc. of IEEE Intl. Conf. on Computer Vision, 2017.", "[27] Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, and Hefeng Wu. Adcrowdnet: An attention-injective deformable convolutional network for crowd understanding. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018."]}, {"table": "<table><tr><td>Method</td><td>Resolution</td><td>Parameters</td><td>Backbone</td><td>FPS</td></tr><tr><td>CSRNET [21]</td><td>384 \\times 384</td><td>16.2 M</td><td>VGG16</td><td>21.67</td></tr><tr><td>BL [34]</td><td>384 \\times 384</td><td>21.6 M</td><td>VGG19</td><td>45.66</td></tr><tr><td>TransCrowd-Token</td><td>384 \\times 384</td><td>86.8 M</td><td>Transformer</td><td>46.41</td></tr><tr><td>TransCrowd-GAP</td><td>384 \\times 384</td><td>90.4 M</td><td>Transformer</td><td>46.73</td></tr></table>", "caption": "Table 8: Comparison with BL [34] and CSRNET [21] using the same input image resolution on a Titan XP.", "list_citation_info": ["[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[34] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Year</td><td colspan=\"2\">Training label</td><td colspan=\"2\">None</td><td colspan=\"2\">Pre-ImgNet</td><td colspan=\"2\">Pre-GCC</td></tr><tr><td>Location</td><td>Crowd number</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>CSRNet [21]</td><td>CVPR18</td><td>\\surd</td><td>\\surd</td><td>120.0</td><td>179.4</td><td>68.2</td><td>115.0</td><td>67.4</td><td>112.3</td></tr><tr><td>TransCrowd-Token (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>142.0</td><td>212.5</td><td>69.0</td><td>116.5</td><td>67.2</td><td>111.9</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>139.9</td><td>231.0</td><td>66.1</td><td>105.1</td><td>63.8</td><td>102.3</td></tr></table>", "caption": "Table 9: The fine-tuning CSRNet\u2019s and TransCrowd-GAP\u2019s results on ShanghaiTech part A dataset by using three different pre-trained strategies. ", "list_citation_info": ["[21] Yuhong Li, Xiaofan Zhang, and Deming Chen. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Year</td><td colspan=\"2\">Training label</td><td colspan=\"2\">Part B\\rightarrowPart A</td><td colspan=\"2\">Part A\\rightarrowPart B</td><td colspan=\"2\">QNRF\\rightarrowPart A</td><td colspan=\"2\">QNRF\\rightarrowPart B</td></tr><tr><td>Location</td><td>Crowd number</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td></tr><tr><td>MCNN [66]</td><td>CVPR16</td><td>\\surd</td><td>\\surd</td><td>221.4</td><td>357.8</td><td>85.2</td><td>142.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>D-ConvNet [43]</td><td>ECCV18</td><td>\\surd</td><td>\\surd</td><td>140.4</td><td>226.1</td><td>49.1</td><td>99.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>RRSP[51]</td><td>CVPR19</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>40.0</td><td>68.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BL [34]</td><td>ICCV19</td><td>\\surd</td><td>\\surd</td><td>-</td><td>-</td><td>-</td><td>-</td><td>69.8</td><td>123.8</td><td>15.3</td><td>26.5</td></tr><tr><td>TransCrowd-GAP (ours)*</td><td>-</td><td>-</td><td>\\surd</td><td>141.3</td><td>258.9</td><td>18.9</td><td>31.1</td><td>78.7</td><td>122.5</td><td>13.5</td><td>21.9</td></tr></table>", "caption": "Table 10: Experimental results on the transferability of different methods under cross-dataset evaluation. ", "list_citation_info": ["[43] Zenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yangdong Ye, Ming-Ming Cheng, and Guoyan Zheng. Crowd counting with deep negative correlation learning. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018.", "[34] Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019.", "[51] Jia Wan, Wenhan Luo, Baoyuan Wu, Antoni B Chan, and Wei Liu. Residual regression with semantic prior for crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019.", "[66] Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016."]}]}