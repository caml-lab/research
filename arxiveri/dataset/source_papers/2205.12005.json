{"title": "mplug: Effective and efficient vision-language learning by cross-modal skip-connections", "abstract": "Large-scale pretrained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from the problems of low computational efficiency and information asymmetry brought by the long visual sequence in cross-modal alignment. To address these problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections, which creates inter-layer shortcuts that skip a certain number of layers for time-consuming full self-attention on the vision side. mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, such as image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability when directly transferred to multiple video-language tasks.", "authors": ["Chenliang Li", " Haiyang Xu", " Junfeng Tian", " Wei Wang", " Ming Yan", " Bin Bi", " Jiabo Ye", " Hehong Chen", " Guohai Xu", " Zheng Cao", " Ji Zhang", " Songfang Huang", " Fei Huang", " Jingren Zhou", " Luo Si"], "pdf_url": "https://arxiv.org/abs/2205.12005", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"3\">Models</th><th rowspan=\"3\">Data</th><th colspan=\"8\">COCO Caption</th><th colspan=\"2\">NoCaps</th></tr><tr><th colspan=\"4\">Cross-entropy Optimization</th><th colspan=\"4\">CIDEr Optimization</th><th colspan=\"2\"></th></tr><tr><th>B@4</th><th>M</th><th>C</th><th>S</th><th>B@4</th><th>M</th><th>C</th><th>S</th><th>C</th><th>S</th></tr></thead><tbody><tr><td>Encoder-Decoder</td><td>CC12M</td><td>-</td><td>-</td><td>110.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>90.2</td><td>12.1</td></tr><tr><td>E2E-VLP [19]</td><td>4M</td><td>36.2</td><td>-</td><td>117.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VinVL [9]</td><td>5.65M</td><td>38.5</td><td>30.4</td><td>130.8</td><td>23.4</td><td>41.0</td><td>31.1</td><td>140.9</td><td>25.2</td><td>97.3</td><td>13.8</td></tr><tr><td>OSCAR [4]</td><td>6.5M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>41.7</td><td>30.6</td><td>140.0</td><td>24.5</td><td>83.4</td><td>11.4</td></tr><tr><td>SimVLM{}_{large} [7]</td><td>1.8B</td><td>40.3</td><td>33.4</td><td>142.6</td><td>24.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LEMON{}_{large} [33]</td><td>200M</td><td>40.6</td><td>30.4</td><td>135.7</td><td>23.5</td><td>42.3</td><td>31.2</td><td>144.3</td><td>25.3</td><td>113.4</td><td>15.0</td></tr><tr><td>BLIP [34]</td><td>129M</td><td>40.4</td><td>-</td><td>136.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>113.2</td><td>14.8</td></tr><tr><td>OFA [35]</td><td>18M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>43.5</td><td>31.9</td><td>149.6</td><td>26.1</td><td>-</td><td>-</td></tr><tr><td>mPLUG</td><td>14M</td><td>43.1</td><td>31.4</td><td>141.0</td><td>24.2</td><td>46.5</td><td>32.0</td><td>155.1</td><td>26.0</td><td>114.8</td><td>14.8</td></tr></tbody></table>", "caption": "Table 1: Evaluation Results on COCO Caption \u201dKarpathy\u201d test split and NoCaps validation set. B@4: BLEU@4, M: METEOR, C: CIDEr, S: SPICE.", "list_citation_info": ["Zhang et al. [2021] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5579\u20135588.", "Xu et al. [2021a] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. 2021a. E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804.", "Li et al. [2020a] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020a. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer.", "Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086.", "Hu et al. [2021] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. 2021. Scaling up vision-language pre-training for image captioning. CoRR, abs/2111.12233.", "Wang et al. [2021a] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021a. Simvlm: Simple visual language model pretraining with weak supervision. CoRR, abs/2108.10904.", "Wang et al. [2022] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052."]}, {"table": "<table><tbody><tr><th>Models</th><td>Data</td><td>Test-dev</td><td>Test-std</td></tr><tr><th colspan=\"4\">Pretrained on COCO, VG, SBU and CC datasets</th></tr><tr><th>VLBERT [43]</th><td>4M</td><td>71.16</td><td>-</td></tr><tr><th>E2E-VLP [19]</th><td>4M</td><td>73.25</td><td>73.67</td></tr><tr><th>VL-T5 [44]</th><td>4M</td><td>-</td><td>71.30</td></tr><tr><th>UNITER[2]</th><td>4M</td><td>72.70</td><td>72.91</td></tr><tr><th>OSCAR[4]</th><td>4M</td><td>73.16</td><td>73.44</td></tr><tr><th>CLIP-ViL[26]</th><td>4M</td><td>76.48</td><td>76.94</td></tr><tr><th>METER[11]</th><td>4M</td><td>77.68</td><td>77.64</td></tr><tr><th>ALBEF[6]</th><td>4M</td><td>74.54</td><td>74.70</td></tr><tr><th>mPLUGViT-B</th><td>4M</td><td>77.94</td><td>77.96</td></tr><tr><th colspan=\"4\">Models Pretrained on More Data</th></tr><tr><th>ALBEF [6]</th><td>14M</td><td>75.84</td><td>76.04</td></tr><tr><th>BLIP [34]</th><td>129M</td><td>78.25</td><td>78.32</td></tr><tr><th>SimVLM [7]</th><td>1.8B</td><td>80.03</td><td>80.34</td></tr><tr><th>Florence [45]</th><td>0.9B</td><td>80.16</td><td>80.36</td></tr><tr><th>OFA [35]</th><td>18M</td><td>79.87</td><td>80.02</td></tr><tr><th>VLMo [20]</th><td>-</td><td>79.94</td><td>79.98</td></tr><tr><th>mPLUGViT-B</th><td>14M</td><td>79.79</td><td>79.81</td></tr><tr><th>mPLUGViT-L</th><td>14M</td><td>81.27</td><td>81.26</td></tr></tbody></table>", "caption": "Table 2: Evaluation Results on VQA test set.", "list_citation_info": ["Li et al. [2021a] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021a. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34.", "Shen et al. [2021] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. 2021. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383.", "Xu et al. [2021a] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. 2021a. E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804.", "Li et al. [2020a] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020a. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer.", "Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086.", "Wang et al. [2021b] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. 2021b. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. arXiv preprint arXiv:2111.02358.", "Cho et al. [2021] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying vision-and-language tasks via text generation. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1931\u20131942. PMLR.", "Dou et al. [2021] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Zicheng Liu, Michael Zeng, et al. 2021. An empirical study of training end-to-end vision-and-language transformers. arXiv preprint arXiv:2111.02387.", "Chen et al. [2020] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer.", "Wang et al. [2021a] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021a. Simvlm: Simple visual language model pretraining with weak supervision. CoRR, abs/2108.10904.", "Wang et al. [2022] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052.", "Lu et al. [2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems, pages 13\u201323.", "Yuan et al. [2021] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Models</th><th># Pretrain</th><td colspan=\"6\">MSCOCO (5K test set)</td><td colspan=\"6\">Flickr30K (1K test set)</td></tr><tr><th>data</th><td colspan=\"3\">TR</td><td colspan=\"3\">IR</td><td colspan=\"3\">TR</td><td colspan=\"3\">IR</td></tr><tr><th></th><th></th><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><th>E2E-VLP [19]</th><th>4M</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>86.2</td><td>97.5</td><td>98.92</td><td>73.6</td><td>92.4</td><td>96.0</td></tr><tr><th>UNITER [2]</th><th>4M</th><td>65.7</td><td>88.6</td><td>93.8</td><td>52.9</td><td>79.9</td><td>88.0</td><td>87.3</td><td>98.0</td><td>99.2</td><td>75.6</td><td>94.1</td><td>96.8</td></tr><tr><th>OSCAR [4]</th><th>4M</th><td>70.0</td><td>91.1</td><td>95.5</td><td>54.0</td><td>80.8</td><td>88.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>UNIMO  [46]</th><th>4M</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>89.4</td><td>98.9</td><td>99.8</td><td>78.0</td><td>94.2</td><td>97.1</td></tr><tr><th>VLMo  [20]</th><th>4M</th><td>78.2</td><td>94.4</td><td>97.4</td><td>60.6</td><td>84.4</td><td>91.0</td><td>95.3</td><td>99.9</td><td>100.0</td><td>84.5</td><td>97.3</td><td>98.6</td></tr><tr><th>ALIGN [18]</th><th>1.8B</th><td>77.0</td><td>93.5</td><td>96.9</td><td>59.9</td><td>83.3</td><td>89.8</td><td>95.3</td><td>99.8</td><td>100.0</td><td>84.9</td><td>97.4</td><td>98.6</td></tr><tr><th>ALBEF  [6]</th><th>14M</th><td>77.6</td><td>94.3</td><td>97.2</td><td>60.7</td><td>84.3</td><td>90.5</td><td>95.9</td><td>99.8</td><td>100.0</td><td>85.6</td><td>97.5</td><td>98.9</td></tr><tr><th>Florence  [45]</th><th>0.9B</th><td>81.8</td><td>95.2</td><td>-</td><td>63.2</td><td>85.7</td><td>-</td><td>97.2</td><td>99.9</td><td>-</td><td>87.9</td><td>98.1</td><td>-</td></tr><tr><th>BLIP  [34]</th><th>14M</th><td>80.6</td><td>95.2</td><td>97.6</td><td>63.1</td><td>85.3</td><td>91.1</td><td>96.6</td><td>99.8</td><td>100.0</td><td>87.2</td><td>97.5</td><td>98.8</td></tr><tr><th>BLIP  [34]</th><th>129M</th><td>82.4</td><td>95.4</td><td>97.9</td><td>65.1</td><td>86.3</td><td>91.8</td><td>97.4</td><td>99.8</td><td>99.9</td><td>87.6</td><td>97.7</td><td>99.0</td></tr><tr><th>mPLUG</th><th>14M</th><td>82.8</td><td>96.1</td><td>98.3</td><td>65.8</td><td>87.3</td><td>92.6</td><td>97.6</td><td>100.0</td><td>100.0</td><td>88.4</td><td>97.9</td><td>99.1</td></tr></tbody></table><br/>", "caption": "Table 3: Image-text retrieval results on Flickr30K and COCO datasets.", "list_citation_info": ["Li et al. [2021a] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021a. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34.", "Xu et al. [2021a] Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. 2021a. E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804.", "Li et al. [2020a] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020a. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer.", "Wang et al. [2021b] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. 2021b. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. arXiv preprint arXiv:2111.02358.", "Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086.", "Chen et al. [2020] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer.", "Yuan et al. [2021] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432.", "Jia et al. [2021] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918.", "Li et al. [2020b] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2020b. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th colspan=\"3\">RefCOCO</th><th colspan=\"3\">RefCOCO+</th><th colspan=\"2\">RefCOCOg</th></tr><tr><th>val</th><th>testA</th><th>testB</th><th>val</th><th>testA</th><th>testB</th><th>val-u</th><th>test-u</th></tr></thead><tbody><tr><th>VLBERT [43]</th><td>-</td><td>-</td><td>-</td><td>72.59</td><td>78/57</td><td>62.30</td><td>-</td><td>-</td></tr><tr><th>UNITER [2]</th><td>81.41</td><td>87.04</td><td>74.17</td><td>75.90</td><td>81.45</td><td>66.70</td><td>74.86</td><td>75.77</td></tr><tr><th>VILLA [50]</th><td>82.39</td><td>87.48</td><td>74.84</td><td>76.17</td><td>81.54</td><td>66.84</td><td>76.18</td><td>76.71</td></tr><tr><th>MDETR [51]</th><td>86.75</td><td>89.58</td><td>81.41</td><td>79.52</td><td>84.09</td><td>70.62</td><td>81.64</td><td>80.89</td></tr><tr><th>UNICORN [52]</th><td>88.29</td><td>90.42</td><td>83.06</td><td>80.30</td><td>85.05</td><td>71.88</td><td>83.44</td><td>83.93</td></tr><tr><th>OFA [35]</th><td>90.05</td><td>92.93</td><td>85.26</td><td>84.49</td><td>90.10</td><td>77.77</td><td>84.54</td><td>85.20</td></tr><tr><th>mPLUG</th><td>92.40</td><td>94.51</td><td>88.42</td><td>86.02</td><td>90.17</td><td>78.17</td><td>85.88</td><td>86.42</td></tr></tbody></table>", "caption": "Table 4: Visual grounding results (Acc@0.5) on ReferCOCO, ReferCOCO+, and ReferCOCOg.", "list_citation_info": ["Kamath et al. [2021] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. 2021. MDETR - modulated detection for end-to-end multi-modal understanding. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 1760\u20131770. IEEE.", "Chen et al. [2020] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer.", "Wang et al. [2022] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052.", "Lu et al. [2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems, pages 13\u201323.", "Yang et al. [2021a] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang. 2021a. Crossing the format boundary of text and boxes: Towards unified vision-language modeling. CoRR, abs/2111.12085.", "Gan et al. [2020] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. 2020. Large-scale adversarial training for vision-and-language representation learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th colspan=\"2\">NLVR2</th><th colspan=\"2\">SNLI-VE</th></tr><tr><th>dev</th><th>test-P</th><th>dev</th><th>test</th></tr></thead><tbody><tr><th>LXMERT[1]</th><td>74.90</td><td>74.50</td><td>-</td><td>-</td></tr><tr><th>VL-T5[44]</th><td>-</td><td>73.6</td><td>-</td><td>-</td></tr><tr><th>UNITER[2]</th><td>79.12</td><td>79.98</td><td>79.39</td><td>79.38</td></tr><tr><th>CLIP-ViL[26]</th><td>-</td><td>-</td><td>80.61</td><td>80.20</td></tr><tr><th>METER[11]</th><td>82.33</td><td>83.05</td><td>80.86</td><td>81.19</td></tr><tr><th>UNIMO[46]</th><td>-</td><td>-</td><td>81.11</td><td>80.63</td></tr><tr><th>ALBEF[6]</th><td>82.55</td><td>83.14</td><td>80.80</td><td>80.91</td></tr><tr><th>BLIP[34]</th><td>82.67</td><td>82.30</td><td>-</td><td>-</td></tr><tr><th>SimVLM{}_{large}[7]</th><td>84.13</td><td>84.84</td><td>85.68</td><td>85.62</td></tr><tr><th>VLMo[20]</th><td>85.64</td><td>86.86</td><td>-</td><td>-</td></tr><tr><th>OFA[35]</th><td>-</td><td>-</td><td>90.30</td><td>90.20</td></tr><tr><th>mPLUG</th><td>84.58</td><td>84.95</td><td>89.45</td><td>89.29</td></tr></tbody></table>", "caption": "Table 5: Evaluation Results on NLVR2 and SNLI-VE.", "list_citation_info": ["Li et al. [2021a] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021a. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34.", "Shen et al. [2021] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. 2021. How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383.", "Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086.", "Wang et al. [2021b] Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. 2021b. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. arXiv preprint arXiv:2111.02358.", "Cho et al. [2021] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying vision-and-language tasks via text generation. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1931\u20131942. PMLR.", "Dou et al. [2021] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Zicheng Liu, Michael Zeng, et al. 2021. An empirical study of training end-to-end vision-and-language transformers. arXiv preprint arXiv:2111.02387.", "Chen et al. [2020] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer.", "Wang et al. [2021a] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021a. Simvlm: Simple visual language model pretraining with weak supervision. CoRR, abs/2108.10904.", "Wang et al. [2022] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052.", "Tan and Bansal [2019] Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490.", "Li et al. [2020b] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2020b. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409."]}, {"table": "<table><tbody><tr><th>Model</th><td>In</td><td>Near</td><td>Out</td><td>Overall</td></tr><tr><th>SimVLM{}_{base}[7]</th><td>83.2</td><td>84.1</td><td>82.5</td><td>83.5</td></tr><tr><th>SimVLM{}_{huge}[7]</th><td>101.2</td><td>100.4</td><td>102.3</td><td>101.4</td></tr><tr><th>Oscar\\dagger[4]</th><td>85.4</td><td>84.0</td><td>80.3</td><td>83.4</td></tr><tr><th>VinVL\\dagger[9]</th><td>103.7</td><td>95.6</td><td>83.8</td><td>94.3</td></tr><tr><th>SimVLM{}_{huge}\\dagger[7]</th><td>113.7</td><td>110.9</td><td>115.2</td><td>112.2</td></tr><tr><th>mPLUG</th><td>86.34</td><td>81.5</td><td>90.49</td><td>84.02</td></tr><tr><th>mPLUG\\dagger</th><td>116.7</td><td>113.75</td><td>117.0</td><td>114.8</td></tr></tbody></table>", "caption": "Table 7: Image captioning results on NoCaps validation split (zero-shot and finetuned), and {In, Near, Out} refer to in-domain, near-domain and out-of-domain respectively. \\dagger denotes the models finetuned on COCO Caption dataset.", "list_citation_info": ["Zhang et al. [2021] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5579\u20135588.", "Wang et al. [2021a] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2021a. Simvlm: Simple visual language model pretraining with weak supervision. CoRR, abs/2108.10904.", "Li et al. [2020a] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020a. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Model</th><td colspan=\"2\">TR</td><td colspan=\"2\">IR</td></tr><tr><td>R@1</td><td>R@5</td><td>R@1</td><td>R@5</td></tr><tr><th colspan=\"5\">Zero-Shot</th></tr><tr><th>CLIP [17]</th><td>88.0</td><td>98.7</td><td>68.7</td><td>90.6</td></tr><tr><th>ALIGN  [18]</th><td>88.6</td><td>98.7</td><td>75.7</td><td>93.8</td></tr><tr><th>FLIP  [56]</th><td>89.8</td><td>99.2</td><td>75.0</td><td>93.4</td></tr><tr><th>Florence  [45]</th><td>90.9</td><td>99.1</td><td>76.7</td><td>93.6</td></tr><tr><th>ALBEF\\dagger  [6]</th><td>94.1</td><td>99.5</td><td>82.8</td><td>96.3</td></tr><tr><th>BLIP\\dagger  [34]</th><td>94.8</td><td>99.7</td><td>84.9</td><td>96.7</td></tr><tr><th>mPLUG</th><td>93.0</td><td>99.5</td><td>82.2</td><td>95.8</td></tr><tr><th>mPLUG\\dagger</th><td>95.8</td><td>99.8</td><td>86.4</td><td>97.6</td></tr></tbody></table>", "caption": "Table 8: Zero-shot image-text retrieval results on Flickr30K. \\dagger denotes the models finetuned on COCO. ", "list_citation_info": ["Li et al. [2021a] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021a. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34.", "Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086.", "Yao et al. [2021] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783.", "Yuan et al. [2021] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432.", "Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Jia et al. [2021] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Model</th><th># Pretrain</th><td colspan=\"3\">MSRVTT-Retrieval</td></tr><tr><th>data</th><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><th colspan=\"5\">Zero-Shot</th></tr><tr><th>MIL-NCE [57]</th><th>How100M</th><td>9.9</td><td>24.0</td><td>32.4</td></tr><tr><th>VideoCLIP [58]</th><th>How100M</th><td>10.4</td><td>22.2</td><td>30.0</td></tr><tr><th>VATT  [59]</th><th>How100M, AudSet</th><td>-</td><td>-</td><td>29.7</td></tr><tr><th>ALPRO  [60]</th><th>W2M, C3M</th><td>24.1</td><td>44.7</td><td>55.4</td></tr><tr><th>VIOLET  [61]</th><th>Y180M, W2M, C3M</th><td>25.9</td><td>49.5</td><td>59.7</td></tr><tr><th>CLIP [17]</th><th>WIT400M</th><td>26.0</td><td>49.4</td><td>60.7</td></tr><tr><th>Florence  [45]</th><th>FLD900M</th><td>37.6</td><td>63.8</td><td>72.6</td></tr><tr><th>BLIP \\dagger  [34]</th><th>129M</th><td>43.3</td><td>65.6</td><td>74.7</td></tr><tr><th>mPLUG</th><th>14M</th><td>38.1</td><td>59.2</td><td>68.2</td></tr><tr><th>mPLUG \\dagger</th><th>14M</th><td>44.3</td><td>66.4</td><td>75.4</td></tr><tr><th colspan=\"5\">Fine-Tuning</th></tr><tr><th>VideoCLIP [58]</th><th>How100M</th><td>30.9</td><td>55.4</td><td>66.8</td></tr><tr><th>ALPRO  [60]</th><th>C3M, W2M</th><td>33.9</td><td>60.7</td><td>73.2</td></tr><tr><th>VIOLET  [61]</th><th>Y180M, C3M, W2M</th><td>34.5</td><td>63.0</td><td>73.4</td></tr></tbody></table>", "caption": "Table 9: Zero-shot video-language results on text-to-video retrieval on the 1k test split of the MSRVTT dataset. \\dagger denotes the models finetuned on COCO. Video datasets include HowTo100M [62], WebVid-2M(W2M) [63], YT-Temporal-180M( Y180M) [64]. Image datasets include CC3M(C3M) [38], FLD900M [45], WIT400M [17]. Audio datasets include AudioSet(AudSet) [65].", "list_citation_info": ["Li et al. [2021b] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. 2021b. Align and prompt: Video-and-language pre-training with entity prompts. arXiv preprint arXiv:2112.09583.", "Miech et al. [2019] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630\u20132640.", "Zellers et al. [2021] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34.", "Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086.", "Sharma et al. [2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565.", "Xu et al. [2021b] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021b. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800.", "Bain et al. [2021] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738.", "Akbari et al. [2021] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. 2021. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34.", "Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.", "Fu et al. [2021] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. 2021. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681.", "Gemmeke et al. [2017] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776\u2013780. IEEE.", "Yuan et al. [2021] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432.", "Miech et al. [2020] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879\u20139889."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th>MSRVTT-QA</th><th>MSVD-QA</th><th>VATEX-Cap</th></tr><tr><th>Acc</th><th>Acc</th><th>CIDEr</th></tr><tr><th colspan=\"4\">Zero-Shot</th></tr></thead><tbody><tr><th>VQA-T  [66]</th><td>2.9</td><td>7.5</td><td>-</td></tr><tr><th>BLIP  [34]</th><td>19.2</td><td>35.2</td><td>37.4</td></tr><tr><th>mPLUG</th><td>21.1</td><td>37.2</td><td>42.0</td></tr></tbody></table>", "caption": "Table 10: Zero-shot video-language results on Question-Answer and Caption tasks.", "list_citation_info": ["Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086.", "Yang et al. [2021b] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2021b. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697."]}]}