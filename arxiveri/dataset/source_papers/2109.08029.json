{"title": "Image captioning for effective use of language models in knowledge-based visual question answering", "abstract": "Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we propose to use a unimodal (text-only) train and inference procedure based on automatic off-the-shelf captioning of images and pretrained language models. Our results on a visual question answering task which requires external knowledge (OK-VQA) show that our text-only model outperforms pretrained multimodal (image-text) models of comparable number of parameters. In contrast, our model is less effective in a standard VQA task (VQA 2.0) confirming that our text-only method is specially effective for tasks requiring external knowledge. In addition, we show that increasing the language model's size improves notably its performance, yielding results comparable to the state-of-the-art with our largest model, significantly outperforming current multimodal systems, even though augmented with external knowledge. Our qualitative analysis on OK-VQA reveals that automatic captions often fail to capture relevant information in the images, which seems to be balanced by the better inference ability of the text-only language models. Our work opens up possibilities to further improve inference in visio-linguistic tasks", "authors": ["Ander Salaberria", " Gorka Azkune", " Oier Lopez de Lacalle", " Aitor Soroa", " Eneko Agirre"], "pdf_url": "https://arxiv.org/abs/2109.08029", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td>Score</td><td></td><td>Parameters</td></tr><tr><th>ConceptBERT garderes2020conceptbert  *</th><td>31.4</td><td>(+sym. 33.7)</td><td>348M</td></tr><tr><th>MAVEx wu2021multi </th><td>35.2</td><td>(+sym. 41.4)</td><td>353M</td></tr><tr><th>KRISP marino2020krisp </th><td>37.1</td><td>(+sym. 38.9)</td><td>116M</td></tr><tr><th>RVL shevchenko2021reasoning  *\u2020</th><td>37.3</td><td>(+sym. 39.0)</td><td>208M</td></tr><tr><th>PICa-Base yang2021pica </th><td>42.0</td><td>(+tags  43.3)</td><td>175B</td></tr><tr><th>PICa-Full yang2021pica  (Ensemble)</th><td>46.9</td><td>(+tags  48.0)</td><td>175B</td></tr><tr><th>CBM<sub>BERT</sub> (ours)</th><td>36.0</td><td></td><td>112M</td></tr><tr><th>CBM<sub>T5-11B</sub> (ours)</th><td>47.9</td><td></td><td>11B</td></tr></tbody></table>", "caption": "Table 3: Comparison to the state-of-the-art on OK-VQA. +sym. stands for systems additionally using symbolic knowledge, and +tags for the additional use of object tags. Results of models marked with * are in OK-VQA v1.0 and \u2020 specifies contaminated results (see main text).", "list_citation_info": ["(4) K. Marino, X. Chen, D. Parikh, A. Gupta, M. Rohrbach, Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa, arXiv e-prints (2020) arXiv\u20132012.", "(26) V. Shevchenko, D. Teney, A. Dick, A. van den Hengel, Reasoning over vision and language: Exploring the benefits of supplemental knowledge, in: Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN), Association for Computational Linguistics, Kyiv, Ukraine, 2021, pp. 1\u201318.", "(25) J. Wu, J. Lu, A. Sabharwal, R. Mottaghi, Multi-modal answer validation for knowledge-based vqa, arXiv preprint arXiv:2103.12248.", "(8) Z. Yang, Z. Gan, J. Wang, X. Hu, Y. Lu, Z. Liu, L. Wang, An empirical study of GPT-3 for few-shot knowledge-based VQA, CoRR abs/2109.05014. arXiv:2109.05014. URL https://arxiv.org/abs/2109.05014", "(21) F. Gard\u00e8res, M. Ziaeefard, B. Abeloos, F. Lecue, ConceptBert: Concept-aware representation for visual question answering, in: Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020, pp. 489\u2013498. doi:10.18653/v1/2020.findings-emnlp.44."]}]}