{"title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "abstract": "Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer~(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.", "authors": ["Wenhai Wang", " Enze Xie", " Xiang Li", " Deng-Ping Fan", " Kaitao Song", " Ding Liang", " Tong Lu", " Ping Luo", " Ling Shao"], "pdf_url": "https://arxiv.org/abs/2102.12122", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>#Param (M)</td><td>GFLOPs</td><td>Top-1 Err (%)</td></tr><tr><td>ResNet18* [22]</td><td>11.7</td><td>1.8</td><td>30.2</td></tr><tr><td>ResNet18 [22]</td><td>11.7</td><td>1.8</td><td>31.5</td></tr><tr><td>DeiT-Tiny/16 [63]</td><td>5.7</td><td>1.3</td><td>27.8</td></tr><tr><td>PVT-Tiny (ours)</td><td>13.2</td><td>1.9</td><td>24.9</td></tr><tr><td>ResNet50* [22]</td><td>25.6</td><td>4.1</td><td>23.9</td></tr><tr><td>ResNet50 [22]</td><td>25.6</td><td>4.1</td><td>21.5</td></tr><tr><td>ResNeXt50-32x4d* [73]</td><td>25.0</td><td>4.3</td><td>22.4</td></tr><tr><td>ResNeXt50-32x4d [73]</td><td>25.0</td><td>4.3</td><td>20.5</td></tr><tr><td>T2T-ViT{}_{t}-14 [75]</td><td>22.0</td><td>6.1</td><td>19.3</td></tr><tr><td>TNT-S [19]</td><td>23.8</td><td>5.2</td><td>18.7</td></tr><tr><td>DeiT-Small/16 [63]</td><td>22.1</td><td>4.6</td><td>20.1</td></tr><tr><td>PVT-Small (ours)</td><td>24.5</td><td>3.8</td><td>20.2</td></tr><tr><td>ResNet101* [22]</td><td>44.7</td><td>7.9</td><td>22.6</td></tr><tr><td>ResNet101 [22]</td><td>44.7</td><td>7.9</td><td>20.2</td></tr><tr><td>ResNeXt101-32x4d* [73]</td><td>44.2</td><td>8.0</td><td>21.2</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>44.2</td><td>8.0</td><td>19.4</td></tr><tr><td>T2T-ViT{}_{t}-19 [75]</td><td>39.0</td><td>9.8</td><td>18.6</td></tr><tr><td>ViT-Small/16 [13]</td><td>48.8</td><td>9.9</td><td>19.2</td></tr><tr><td>PVT-Medium (ours)</td><td>44.2</td><td>6.7</td><td>18.8</td></tr><tr><td>ResNeXt101-64x4d* [73]</td><td>83.5</td><td>15.6</td><td>20.4</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>83.5</td><td>15.6</td><td>18.5</td></tr><tr><td>ViT-Base/16 [13]</td><td>86.6</td><td>17.6</td><td>18.2</td></tr><tr><td>T2T-ViT{}_{t}-24 [75]</td><td>64.0</td><td>15.0</td><td>17.8</td></tr><tr><td>TNT-B [19]</td><td>66.0</td><td>14.1</td><td>17.2</td></tr><tr><td>DeiT-Base/16 [63]</td><td>86.6</td><td>17.6</td><td>18.2</td></tr><tr><td>PVT-Large (ours)</td><td>61.4</td><td>9.8</td><td>18.3</td></tr></table>", "caption": "Table 2: Image classification performance on the ImageNet validation set.\u201c#Param\u201d refers to the number of parameters.\u201cGFLOPs\u201d is calculated under the input scale of 224\\times 224. \u201c*\u201d indicates the performance of the method trained under the strategy of its original paper.", "list_citation_info": ["[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016.", "[73] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.", "[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Proc. Int. Conf. Learn. Representations, 2021.", "[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proc. Int. Conf. Mach. Learn., 2021.", "[75] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.", "[19] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">#Param(M)</td><td colspan=\"6\">RetinaNet 1x</td><td colspan=\"6\">RetinaNet 3x + MS</td></tr><tr><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><td>ResNet18 [22]</td><td>21.3</td><td>31.8</td><td>49.6</td><td>33.6</td><td>16.3</td><td>34.3</td><td>43.2</td><td>35.4</td><td>53.9</td><td>37.6</td><td>19.5</td><td>38.2</td><td>46.8</td></tr><tr><td>PVT-Tiny (ours)</td><td>23.0</td><td>36.7(+4.9)</td><td>56.9</td><td>38.9</td><td>22.6</td><td>38.8</td><td>50.0</td><td>39.4(+4.0)</td><td>59.8</td><td>42.0</td><td>25.5</td><td>42.0</td><td>52.1</td></tr><tr><td>ResNet50 [22]</td><td>37.7</td><td>36.3</td><td>55.3</td><td>38.6</td><td>19.3</td><td>40.0</td><td>48.8</td><td>39.0</td><td>58.4</td><td>41.8</td><td>22.4</td><td>42.8</td><td>51.6</td></tr><tr><td>PVT-Small (ours)</td><td>34.2</td><td>40.4(+4.1)</td><td>61.3</td><td>43.0</td><td>25.0</td><td>42.9</td><td>55.7</td><td>42.2(+3.2)</td><td>62.7</td><td>45.0</td><td>26.2</td><td>45.2</td><td>57.2</td></tr><tr><td>ResNet101 [22]</td><td>56.7</td><td>38.5</td><td>57.8</td><td>41.2</td><td>21.4</td><td>42.6</td><td>51.1</td><td>40.9</td><td>60.1</td><td>44.0</td><td>23.7</td><td>45.0</td><td>53.8</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>56.4</td><td>39.9(+1.4)</td><td>59.6</td><td>42.7</td><td>22.3</td><td>44.2</td><td>52.5</td><td>41.4(+0.5)</td><td>61.0</td><td>44.3</td><td>23.9</td><td>45.5</td><td>53.7</td></tr><tr><td>PVT-Medium (ours)</td><td>53.9</td><td>41.9(+3.4)</td><td>63.1</td><td>44.3</td><td>25.0</td><td>44.9</td><td>57.6</td><td>43.2(+2.3)</td><td>63.8</td><td>46.1</td><td>27.3</td><td>46.3</td><td>58.9</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>95.5</td><td>41.0</td><td>60.9</td><td>44.0</td><td>23.9</td><td>45.2</td><td>54.0</td><td>41.8</td><td>61.5</td><td>44.4</td><td>25.2</td><td>45.4</td><td>54.6</td></tr><tr><td>PVT-Large (ours)</td><td>71.1</td><td>42.6(+1.6)</td><td>63.7</td><td>45.4</td><td>25.8</td><td>46.0</td><td>58.4</td><td>43.4(+1.6)</td><td>63.6</td><td>46.1</td><td>26.1</td><td>46.0</td><td>59.5</td></tr></table>", "caption": "Table 3: Object detection performance on COCO val2017.\u201cMS\u201d means that multi-scale training [39, 21] is used.", "list_citation_info": ["[73] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.", "[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016.", "[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proc. IEEE Int. Conf. Comp. Vis., 2017."]}, {"table": "<table><tr><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">#Param(M)</td><td colspan=\"6\">Mask R-CNN 1x</td><td colspan=\"6\">Mask R-CNN 3x + MS</td></tr><tr><td>AP{}^{\\rm b}</td><td>AP{}_{50}^{\\rm b}</td><td>AP{}_{75}^{\\rm b}</td><td>AP{}^{\\rm m}</td><td>AP{}_{50}^{\\rm m}</td><td>AP{}_{75}^{\\rm m}</td><td>AP{}^{\\rm b}</td><td>AP{}_{50}^{\\rm b}</td><td>AP{}_{75}^{\\rm b}</td><td>AP{}^{\\rm m}</td><td>AP{}_{50}^{\\rm m}</td><td>AP{}_{75}^{\\rm m}</td></tr><tr><td>ResNet18 [22]</td><td>31.2</td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td><td>36.9</td><td>57.1</td><td>40.0</td><td>33.6</td><td>53.9</td><td>35.7</td></tr><tr><td>PVT-Tiny (ours)</td><td>32.9</td><td>36.7(+2.7)</td><td>59.2</td><td>39.3</td><td>35.1(+3.9)</td><td>56.7</td><td>37.3</td><td>39.8(+2.9)</td><td>62.2</td><td>43.0</td><td>37.4(+3.8)</td><td>59.3</td><td>39.9</td></tr><tr><td>ResNet50 [22]</td><td>44.2</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><td>PVT-Small (ours)</td><td>44.1</td><td>40.4(+2.4)</td><td>62.9</td><td>43.8</td><td>37.8(+3.4)</td><td>60.1</td><td>40.3</td><td>43.0(+2.0)</td><td>65.3</td><td>46.9</td><td>39.9(+2.8)</td><td>62.5</td><td>42.8</td></tr><tr><td>ResNet101 [22]</td><td>63.2</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td><td>42.8</td><td>63.2</td><td>47.1</td><td>38.5</td><td>60.1</td><td>41.3</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>62.8</td><td>41.9(+1.5)</td><td>62.5</td><td>45.9</td><td>37.5(+1.1)</td><td>59.4</td><td>40.2</td><td>44.0(+1.2)</td><td>64.4</td><td>48.0</td><td>39.2(+0.7)</td><td>61.4</td><td>41.9</td></tr><tr><td>PVT-Medium (ours)</td><td>63.9</td><td>42.0(+1.6)</td><td>64.4</td><td>45.6</td><td>39.0(+2.6)</td><td>61.6</td><td>42.1</td><td>44.2(+1.4)</td><td>66.0</td><td>48.2</td><td>40.5(+2.0)</td><td>63.1</td><td>43.5</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>101.9</td><td>42.8</td><td>63.8</td><td>47.3</td><td>38.4</td><td>60.6</td><td>41.3</td><td>44.4</td><td>64.9</td><td>48.8</td><td>39.7</td><td>61.9</td><td>42.6</td></tr><tr><td>PVT-Large (ours)</td><td>81.0</td><td>42.9(+0.1)</td><td>65.0</td><td>46.6</td><td>39.5(+1.1)</td><td>61.9</td><td>42.5</td><td>44.5(+0.1)</td><td>66.0</td><td>48.3</td><td>40.7(+1.0)</td><td>63.4</td><td>43.7</td></tr></table><p>.</p>", "caption": "Table 4: Object detection and instance segmentation performance on COCO val2017.AP{}^{\\rm b} and AP{}^{\\rm m} denote bounding box AP and mask AP, respectively.", "list_citation_info": ["[73] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.", "[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016."]}, {"table": "<table><tr><td rowspan=\"2\">Backbone</td><td colspan=\"3\">Semantic FPN</td></tr><tr><td>#Param (M)</td><td>GFLOPs</td><td>mIoU (%)</td></tr><tr><td>ResNet18 [22]</td><td>15.5</td><td>32.2</td><td>32.9</td></tr><tr><td>PVT-Tiny (ours)</td><td>17.0</td><td>33.2</td><td>35.7(+2.8)</td></tr><tr><td>ResNet50 [22]</td><td>28.5</td><td>45.6</td><td>36.7</td></tr><tr><td>PVT-Small (ours)</td><td>28.2</td><td>44.5</td><td>39.8(+3.1)</td></tr><tr><td>ResNet101 [22]</td><td>47.5</td><td>65.1</td><td>38.8</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>47.1</td><td>64.7</td><td>39.7(+0.9)</td></tr><tr><td>PVT-Medium (ours)</td><td>48.0</td><td>61.0</td><td>41.6(+2.8)</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>86.4</td><td>103.9</td><td>40.2</td></tr><tr><td>PVT-Large (ours)</td><td>65.1</td><td>79.6</td><td>42.1(+1.9)</td></tr><tr><td>PVT-Large* (ours)</td><td>65.1</td><td>79.6</td><td>44.8</td></tr></table>", "caption": "Table 5: Semantic segmentation performance of different backbones on the ADE20K validation set.\u201cGFLOPs\u201d is calculated under the input scale of 512\\times 512.\u201c*\u201d indicates 320K iterations training and multi-scale flip testing.", "list_citation_info": ["[73] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.", "[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"6\">DETR (50 Epochs)</td></tr><tr><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><td>ResNet50 [22]</td><td>32.3</td><td>53.9</td><td>32.3</td><td>10.7</td><td>33.8</td><td>53.0</td></tr><tr><td>PVT-Small (ours)</td><td>34.7(+2.4)</td><td>55.7</td><td>35.4</td><td>12.0</td><td>36.4</td><td>56.7</td></tr></table>", "caption": "Table 6: Performance of the pure Transformer object detection pipeline. We build a pure Transformer detector by combining PVT and DETR [6], whose AP is 2.4 points higher than the original DETR based on ResNet50 [22].", "list_citation_info": ["[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Proc. Eur. Conf. Comp. Vis., 2020.", "[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016."]}, {"table": "<table><tr><td>Method</td><td>#Param (M)</td><td>GFLOPs</td><td>mIoU (%)</td></tr><tr><td>ResNet50-d8+DeeplabV3+ [9]</td><td>26.8</td><td>120.5</td><td>41.5</td></tr><tr><td>ResNet50-d16+DeeplabV3+ [9]</td><td>26.8</td><td>45.5</td><td>40.6</td></tr><tr><td>ResNet50-d16+Trans2Seg [72]</td><td>56.1</td><td>79.3</td><td>39.7</td></tr><tr><td>PVT-Small+Trans2Seg</td><td>32.1</td><td>31.6</td><td>42.6(+2.9)</td></tr></table>", "caption": "Table 7: Performance of the pure Transformer semantic segmentation pipeline. We build a pure Transformer detector by combining PVT and Trans2Seg [72]. It is 2.9% higher than ResNet50-d16+Trans2Seg and 1.1% higher than ResNet50-d8+DeeplabV3+ with lower GFlops. \u201cd8\u201d and \u201cd16\u201d means dilation 8 and 16, respectively. ", "list_citation_info": ["[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proc. Eur. Conf. Comp. Vis., 2018.", "[72] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Segmenting transparent object in the wild with transformer. In Proc. Int. Joint Conf. Artificial Intell., 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">#Param(M)</td><td colspan=\"6\">RetinaNet 1x</td></tr><tr><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><td>ViT-Small/4 [13]</td><td>60.9</td><td colspan=\"6\">Out of Memory</td></tr><tr><td>ViT-Small/32 [13]</td><td>60.8</td><td>31.7</td><td>51.3</td><td>32.3</td><td>14.8</td><td>33.7</td><td>47.9</td></tr><tr><td>PVT-Small (ours)</td><td>34.2</td><td>40.4</td><td>61.3</td><td>43.0</td><td>25.0</td><td>42.9</td><td>55.7</td></tr></table>", "caption": "Table 8: Performance comparison between ViT and our PVT using RetinaNet for object detection. ViT-Small/4 runs out of GPU memory due to small patch size (i.e., 4\\!\\times\\!4 per patch). ViT-Small/32 obtains 31.7 AP on COCO val2017, which is 8.7 points lower than our PVT-Small.", "list_citation_info": ["[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Proc. Int. Conf. Learn. Representations, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">#Param(M)</td><td rowspan=\"2\">GFLOPs</td><td colspan=\"3\">Mask R-CNN 1x</td></tr><tr><td>AP{}^{\\rm m}</td><td>AP{}_{50}^{\\rm m}</td><td>AP{}_{75}^{\\rm m}</td></tr><tr><td>ResNet50+GC r4 [5]</td><td>54.2</td><td>279.6</td><td>36.2</td><td>58.7</td><td>38.3</td></tr><tr><td>PVT-Small (ours)</td><td>44.1</td><td>304.4</td><td>37.8</td><td>60.1</td><td>40.3</td></tr></table>", "caption": "Table 10: PVT vs. CNN w/ non-local. AP{}^{\\rm m} denotes mask AP. Under similar parameter nubmer and GFLOPs, our PVT outperform the CNN backbone w/ Non-Local (ResNet50+GC r4) by 1.6 AP{}^{\\rm m} (37.8 vs. 36.2).", "list_citation_info": ["[5] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-excitation networks and beyond. In Proc. IEEE Int. Conf. Comp. Vis., 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Scale</td><td rowspan=\"2\">GFLOPs</td><td rowspan=\"2\">Time(ms)</td><td colspan=\"3\">RetinaNet 1x</td></tr><tr><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><td>ResNet50 [22]</td><td>800</td><td>239.3</td><td>55.9</td><td>36.3</td><td>55.3</td><td>38.6</td></tr><tr><td rowspan=\"2\">PVT-Small (ours)</td><td>640</td><td>157.2</td><td>51.7</td><td>38.7</td><td>59.3</td><td>40.8</td></tr><tr><td>800</td><td>285.8</td><td>76.9</td><td>40.4</td><td>61.3</td><td>43.0</td></tr></table>", "caption": "Table 11: Latency and AP under different input scales. \u201cScale\u201d and \u201cTime\u201d denote the input scale and time cost per image. When the shorter sideis 640 pixels, the PVT-Small+RetinaNet has a lowerGFLOPs and time cost (on a V100 GPU) than ResNet50+RetinaNet, while obtaining 2.4 points better AP (38.7 vs. 36.3).", "list_citation_info": ["[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016."]}]}