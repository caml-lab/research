{"title": "Large-scale unsupervised person re-identification with contrastive learning", "abstract": "Existing public person Re-Identification~(ReID) datasets are small in modern terms because of labeling difficulty. Although unlabeled surveillance video is abundant and relatively easy to obtain, it is unclear how to leverage these footage to learn meaningful ReID representations. In particular, most existing unsupervised and domain adaptation ReID methods utilize only the public datasets in their experiments, with labels removed. In addition, due to small data sizes, these methods usually rely on fine tuning by the unlabeled training data in the testing domain to achieve good performance. Inspired by the recent progress of large-scale self-supervised image classification using contrastive learning, we propose to learn ReID representation from large-scale unlabeled surveillance video alone. Assisted by off-the-shelf pedestrian detection tools, we apply the contrastive loss at both the image and the tracklet levels. Together with a principal component analysis step using camera labels freely available, our evaluation using a large-scale unlabeled dataset shows far superior performance among unsupervised methods that do not use any training data in the testing domain. Furthermore, the accuracy improves with the data size and therefore our method has great potential with even larger and more diversified datasets.", "authors": ["Weiquan Huang", " Yan Bai", " Qiuyu Ren", " Xinbo Zhao", " Ming Feng", " Yin Wang"], "pdf_url": "https://arxiv.org/abs/2105.07914", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Tracking Method</th><th colspan=\"2\">Market</th><th colspan=\"2\">DukeMTMC-ReID</th></tr><tr><th>Rank-1</th><th>mAP</th><th>Rank-1</th><th>mAP</th></tr></thead><tbody><tr><th>Our MNN</th><td>66.8</td><td>34.4</td><td>43.7</td><td>24.5</td></tr><tr><th>TRMOT[21]</th><td>68.0</td><td>35.2</td><td>43.9</td><td>25.0</td></tr></tbody></table>", "caption": "Table 2: Performance using different tracking methods to generate tracklets, using 10% of the data.", "list_citation_info": ["[21] Zhongdao Wang, Liang Zheng, Yixuan Liu, and Shengjin Wang. Towards real-time multi-object tracking. arXiv preprint arXiv:1909.12605, 2019."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Training Data</td><td colspan=\"4\">Market-1501</td><td colspan=\"4\">DukeMTMC-reID</td></tr><tr><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td><td>Rank-1</td><td>Rank-5</td><td>Rank-10</td><td>mAP</td></tr><tr><td>Supervised [15]</td><td>labeled data from testing</td><td>95.4</td><td>-</td><td>-</td><td>94.2</td><td>90.3</td><td>-</td><td>-</td><td>89.1</td></tr><tr><td>SPGAN [2]</td><td rowspan=\"6\">labeled data froma source domain,unlabeled data fromthe testing domain(i.e., unsuperviseddomain adaptation)</td><td>57.0</td><td>73.9</td><td>80.3</td><td>27.1</td><td>41.1</td><td>56.6</td><td>63.0</td><td>22.3</td></tr><tr><td>HHL [37]</td><td>62.2</td><td>78.8</td><td>84.0</td><td>31.4</td><td>46.9</td><td>61.0</td><td>66.7</td><td>27.2</td></tr><tr><td>MAR [30]</td><td>67.7</td><td>81.9</td><td>87.3</td><td>40.0</td><td>67.1</td><td>79.8</td><td>84.2</td><td>48.0</td></tr><tr><td>ENC [38]</td><td>75.1</td><td>87.6</td><td>91.6</td><td>43.0</td><td>63.3</td><td>75.8</td><td>80.4</td><td>40.4</td></tr><tr><td>SSG [4]</td><td>80.0</td><td>90.0</td><td>92.4</td><td>58.3</td><td>73.0</td><td>80.6</td><td>83.2</td><td>53.4</td></tr><tr><td>MMT [5]</td><td>87.7</td><td>94.9</td><td>96.9</td><td>71.2</td><td>78.0</td><td>88.8</td><td>92.5</td><td>65.1</td></tr><tr><td>Wu et al. [26]</td><td rowspan=\"8\">unlabeled data fromthe testing domain</td><td>55.8</td><td>72.3</td><td>78.4</td><td>26.2</td><td>48.8</td><td>63.4</td><td>68.4</td><td>28.5</td></tr><tr><td>DECAMEL [29]</td><td>60.2</td><td>76.0</td><td>-</td><td>31.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BUC [13]</td><td>61.9</td><td>73.5</td><td>78.2</td><td>29.6</td><td>40.4</td><td>52.5</td><td>58.2</td><td>22.1</td></tr><tr><td>PAUL [28]</td><td>68.5</td><td>82.4</td><td>87.4</td><td>40.1</td><td>72.0</td><td>82.7</td><td>86.0</td><td>53.2</td></tr><tr><td>UTAL [12]</td><td>69.2</td><td>-</td><td>-</td><td>46.2</td><td>62.3</td><td>-</td><td>-</td><td>44.6</td></tr><tr><td>HCT [31]</td><td>80.0</td><td>91.6</td><td>65.2</td><td>56.4</td><td>69.6</td><td>83.4</td><td>87.4</td><td>50.7</td></tr><tr><td>MetaCam+DSCE [27]</td><td>83.9</td><td>92.3</td><td>-</td><td>61.7</td><td>73.8</td><td>84.2</td><td>-</td><td>53.8</td></tr><tr><td>MTML [39]</td><td>85.3</td><td>-</td><td>96.2</td><td>65.2</td><td>71.1</td><td>-</td><td>86.9</td><td>50.7</td></tr><tr><td>UGA [24]</td><td></td><td>87.2</td><td>-</td><td>-</td><td>70.3</td><td>75.0</td><td>-</td><td>-</td><td>53.3</td></tr><tr><td rowspan=\"2\">Direct transfer [4, 20]</td><td rowspan=\"2\">labeled data froma source domain</td><td>54.6</td><td>71.1</td><td>77.1</td><td>26.6</td><td>30.5</td><td>45.0</td><td>51.8</td><td>16.1</td></tr><tr><td>43.2</td><td>-</td><td>-</td><td>19.8</td><td>26.3</td><td>-</td><td>-</td><td>13.1</td></tr><tr><td>BUC [13, 20]</td><td rowspan=\"3\">6-hour unlabeled video</td><td>29.8</td><td>-</td><td>-</td><td>14.2</td><td>21.5</td><td>-</td><td>-</td><td>11.2</td></tr><tr><td>UGA [24, 20]</td><td>37.2</td><td>-</td><td>-</td><td>17.8</td><td>25.6</td><td>-</td><td>-</td><td>15.4</td></tr><tr><td>CycAs [20]</td><td>50.8</td><td>-</td><td>-</td><td>23.3</td><td>34.6</td><td>-</td><td>-</td><td>19.2</td></tr><tr><td>Ours</td><td>480-hour unlabeled videos</td><td>72.7</td><td>83.7</td><td>88.1</td><td>36.2</td><td>51.7</td><td>65.4</td><td>71.5</td><td>31.2</td></tr></tbody></table>", "caption": "Table 4: Comparison with other unsupervised methods.The first section separated by double lines include the unsupervised methods that use the training data in the testing domain, include supervised, unsupervised domain adaptation, and unsupervised ReID methods. The second section includes methods that do not use any testing domain data. Our method performs the best in this category.", "list_citation_info": ["[29] H. Yu, A. Wu, and W. Zheng. Unsupervised person re-identification by deep asymmetric metric embedding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(4):956\u2013973, 2020.", "[13] Yutian Lin, Xuanyi Dong, Liang Zheng, Yan Yan, and Yi Yang. A bottom-up clustering approach to unsupervised person re-identification. In AAAI Conference on Artificial Intelligence (AAAI), 2019.", "[12] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised tracklet person re-identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(7):1770\u20131782, 2020.", "[27] Fengxiang Yang, Zhun Zhong, Zhiming Luo, Yuanzheng Cai, Yaojin Lin, Shaozi Li, and Nicu Sebe. Joint noise-tolerant learning and meta camera shift adaptation for unsupervised person re-identification. arXiv preprint arXiv:2103.04618, 2021. To appear in CVPR 2021.", "[5] Yixiao Ge, Dapeng Chen, and Hongsheng Li. Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. In International Conference on Learning Representations, 2020.", "[28] Qize Yang, Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Patch-based discriminative feature learning for unsupervised person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.", "[24] Jinlin Wu, Yang Yang, Hao Liu, Shengcai Liao, Zhen Lei, and Stan Z Li. Unsupervised graph association for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, 2019.", "[37] Zhun Zhong, Liang Zheng, Shaozi Li, and Yi Yang. Generalizing a person retrieval model hetero-and homogeneously. In Proceedings of the European Conference on Computer Vision (ECCV), pages 172\u2013188, 2018.", "[30] Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong, and Jianhuang Lai. Unsupervised person re-identification by soft multilabel learning. In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[4] Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou, Honghui Shi, and Thomas S Huang. Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, 2019.", "[20] Zhongdao Wang, Jingwei Zhang, Liang Zheng, Yixuan Liu, Yifan Sun, Yali Li, and Shengjin Wang. Cycas: Self-supervised cycle association for learning re-identifiable descriptions. In Proceedings of the European conference on computer vision (ECCV), 2020.", "[26] Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wei Bian, and Yi Yang. Progressive learning for person re-identification with one example. IEEE Transactions on Image Processing, 28(6):2872\u20132881, June 2019.", "[38] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi Yang. Invariance matters: Exemplar memory for domain adaptive person re-identi\ufb01cation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[15] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei Jiang. Bag of tricks and a strong baseline for deep person re-identification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2019.", "[2] Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, and Jianbin Jiao. Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 994\u20131003, 2018.", "[31] Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo. Hierarchical clustering with hard-batch triplet loss for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.", "[39] Xiangping Zhu, Xiatian Zhu, Minxian Li, Vittorio Murino, and Shaogang Gong. Intra-camera supervised person re-identification: A new benchmark. In Proceedings of the IEEE International Conference on Computer Vision Workshops, 2019."]}]}