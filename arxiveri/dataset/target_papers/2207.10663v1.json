{"title": "Neural Pixel Composition: 3D-4D View Synthesis from Multi-Views", "abstract": "We present Neural Pixel Composition (NPC), a novel approach for continuous\n3D-4D view synthesis given only a discrete set of multi-view observations as\ninput. Existing state-of-the-art approaches require dense multi-view\nsupervision and an extensive computational budget. The proposed formulation\nreliably operates on sparse and wide-baseline multi-view imagery and can be\ntrained efficiently within a few seconds to 10 minutes for hi-res (12MP)\ncontent, i.e., 200-400X faster convergence than existing methods. Crucial to\nour approach are two core novelties: 1) a representation of a pixel that\ncontains color and depth information accumulated from multi-views for a\nparticular location and time along a line of sight, and 2) a multi-layer\nperceptron (MLP) that enables the composition of this rich information provided\nfor a pixel location to obtain the final color output. We experiment with a\nlarge variety of multi-view sequences, compare to existing approaches, and\nachieve better results in diverse and challenging settings. Finally, our\napproach enables dense 3D reconstruction from sparse multi-views, where COLMAP,\na state-of-the-art 3D reconstruction approach, struggles.", "authors": ["Aayush Bansal", "Michael Zollhoefer"], "published_date": "2022_07_21", "pdf_url": "http://arxiv.org/pdf/2207.10663v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>24 sequences</td><td></td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS [49] \\downarrow</td></tr><tr><td>LLFF [26]</td><td></td><td>15.187 \\pm 2.166</td><td>0.384 \\pm 0.082</td><td>0.602 \\pm 0.090</td></tr><tr><td>NeRF [27]</td><td></td><td>13.693 \\pm 2.050</td><td>0.317 \\pm 0.094</td><td>0.713 \\pm 0.089</td></tr><tr><td>DS-NeRF [6]</td><td>.</td><td>14.531 \\pm 2.603</td><td>0.316 \\pm 0.099</td><td>0.757 \\pm 0.040</td></tr><tr><td>DS-NeRF** [6]</td><td>.</td><td>15.346 \\pm 2.276</td><td>0.389 \\pm 0.076</td><td>0.716 \\pm 0.048</td></tr><tr><td>Naive Composition</td><td></td><td>15.480 \\pm 1.928</td><td>0.372 \\pm 0.061</td><td>0.665 \\pm 0.065</td></tr><tr><td>Naive Composition++</td><td></td><td>16.244 \\pm 2.186</td><td>0.442 \\pm 0.074</td><td>0.616 \\pm 0.063</td></tr><tr><td>Ours</td><td></td><td>17.946 \\pm 1.471</td><td>0.562 \\pm 0.077</td><td>0.534 \\pm 0.061</td></tr></tbody></table>", "caption": "Table 1: Sparse and Unconstrained Multi-Views: We evaluate on the 24 sparse and unconstrained multi-view sequences of the Open4D dataset [3]. We train NeRF [27] and DS-NeRF [6] models for each sequence. DS-NeRF [6] employs additional depth along with NeRF. We trained two versions of DS-NeRF. One where we use the same model as NeRF with additional depth supervision. The second version is DS-NeRF** with tuned hyperparameters. We also use the off-the-shelf LLFF model. We observe degenerate outputs using LLFF, NeRF and DS-NeRF, especially for unbounded scenes. However, our approach is able to reliably generate novel views in twenty times less time. Training a NeRF/DS-NeRF model takes roughly 420 minutes per sequence whereas our approach take 10 minutes (including pre-processing multi-view content). We also generate results using naive composition and obtain better results than prior work. We observe that the MLP allows us to do better composition than naive composition.", "list_citation_info": ["[26] Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph. (2019)", "[6] Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised nerf: Fewer views and faster training for free. In: CVPR (2022)", "[3] Bansal, A., Vo, M., Sheikh, Y., Ramanan, D., Narasimhan, S.: 4d visualization of dynamic events from unconstrained multi-view videos. In: CVPR (2020)", "[49] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018)", "[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)"]}, {"table": "<table><tbody><tr><td>12 sequences</td><td></td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><td>NeRF [27]</td><td></td><td></td><td></td><td></td></tr><tr><td>2 hours</td><td></td><td>21.151 \\pm 2.783</td><td>0.577 \\pm 0.157</td><td>0.662 \\pm 0.099</td></tr><tr><td>4 hours</td><td></td><td>21.469 \\pm 2.881</td><td>0.588 \\pm 0.153</td><td>0.628 \\pm 0.096</td></tr><tr><td>vanilla</td><td></td><td>21.625 \\pm 2.933</td><td>0.596 \\pm 0.150</td><td>0.605 \\pm 0.092</td></tr><tr><td>8 hours</td><td></td><td>21.674 \\pm 2.958</td><td>0.598 \\pm 0.149</td><td>0.599 \\pm 0.091</td></tr><tr><td>16 hours</td><td></td><td>21.734 \\pm 2.981</td><td>0.602 \\pm 0.148</td><td>0.586 \\pm 0.088</td></tr><tr><td>32 hours</td><td></td><td>21.741 \\pm 2.985</td><td>0.602 \\pm 0.147</td><td>0.584 \\pm 0.087</td></tr><tr><td>64 hours</td><td>.</td><td>21.741 \\pm 2.985</td><td>0.602 \\pm 0.147</td><td>0.584 \\pm 0.087</td></tr><tr><td>Naive Composition</td><td></td><td>16.008 \\pm 2.315</td><td>0.415 \\pm 0.142</td><td>0.427 \\pm 0.068</td></tr><tr><td>Naive Composition++</td><td></td><td>17.022 \\pm 2.483</td><td>0.460 \\pm 0.144</td><td>0.406 \\pm 0.066</td></tr><tr><td>Ours (10 minutes)</td><td></td><td></td><td></td><td></td></tr><tr><td>K=50,N=50</td><td></td><td>20.834 \\pm 2.784</td><td>0.594 \\pm 0.136</td><td>0.426 \\pm 0.075</td></tr><tr><td> K=100,N=100</td><td></td><td>20.953 \\pm 2.805</td><td>0.598 \\pm 0.136</td><td>0.460 \\pm 0.078</td></tr><tr><td>K=200,N=200</td><td></td><td>20.783 \\pm 2.749</td><td>0.593 \\pm 0.135</td><td>0.494 \\pm 0.081</td></tr><tr><td>K=ALL,N=200*</td><td></td><td>20.712 \\pm 2.656</td><td>0.591 \\pm 0.134</td><td>0.497 \\pm 0.077</td></tr><tr><td>Ours (50 minutes)</td><td></td><td></td><td></td><td></td></tr><tr><td>K=50,N=50</td><td></td><td>20.777 \\pm 2.809</td><td>0.591 \\pm 0.137</td><td>0.416 \\pm 0.075</td></tr><tr><td>K=100,N=100</td><td></td><td>21.006 \\pm 2.869</td><td>0.597 \\pm 0.136</td><td>0.448 \\pm 0.080</td></tr><tr><td>K=200,N=200</td><td></td><td>20.924 \\pm 2.847</td><td>0.592 \\pm 0.134</td><td>0.477 \\pm 0.082</td></tr><tr><td>K=ALL,N=200*</td><td></td><td>20.825 \\pm 2.708</td><td>0.589 \\pm 0.132</td><td>0.480 \\pm 0.079</td></tr><tr><td>Ours (250 minutes)</td><td></td><td></td><td></td><td></td></tr><tr><td>K=50,N=50</td><td></td><td>20.582 \\pm 2.751</td><td>0.585 \\pm 0.135</td><td>0.409 \\pm 0.076</td></tr><tr><td>K=100,N=100</td><td></td><td>20.916 \\pm 2.874</td><td>0.593 \\pm 0.135</td><td>0.433 \\pm 0.078</td></tr><tr><td>K=200,N=200</td><td></td><td>20.640 \\pm 3.234</td><td>0.582 \\pm 0.139</td><td>0.474 \\pm 0.095</td></tr><tr><td>K=ALL,N=200*</td><td></td><td>20.548 \\pm 3.104</td><td>0.580 \\pm 0.137</td><td>0.478 \\pm 0.092</td></tr></tbody></table>", "caption": "Table 2: Hi-Res (12MP) View Synthesis: We evaluate on 12 sequences from LLFF containing specular surfaces on original 4032\\times 3024 resolution. The details of these sequences are available in Appendix 0.A.2. We contrast the performance of our approach with different intervals of training a NeRF model. Performance saturates at 1M iterations after 32 hours of training. Our composition model converges quickly in a few minutes. Here, we show the results of our composition model trained for 10 epochs that takes around 10 minutes, 50 epochs that takes less than 1 hour. Training our model require 1 GB of GPU memory for training. We also show the results when the model is trained for 250 epochs. For each setting, we vary the number of stereo pairs (K) and number of 3D points (N). We observe that using a few stereo-pairs gives competitive and better results than using all the pairs. We posit that noise introduced by using more stereo pairs might be responsible for the lower performance. Finally, we study the benefit of using an MLP for composing per-pixel color and depth information. The MLP allows us to obtain better results than a naive composition (Fig. 4). We refer the reader to Figure 6 for visual comparisons. We observe that details become better for NeRF when trained for long. However, our approach captures more details in a few minutes as compared to 32 hours of training of a NeRF model. Consistent with the observation of Zhang et al. [49], PSNR may favor averaged/blurry results while LPIPS favors sharp results. ", "list_citation_info": ["[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)", "[49] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018)"]}, {"table": "<table><tbody><tr><th>8 sequences</th><td>PSNR\\uparrow</td><td>MCSSIM\\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><th>4032\\times3024</th><td></td><td></td><td></td></tr><tr><th>NeRF</th><td></td><td></td><td></td></tr><tr><th>vanilla</th><td>21.141 \\pm 3.528</td><td>0.735 \\pm 0.155</td><td>0.528 \\pm 0.157</td></tr><tr><th>2M iterations</th><td>21.457 \\pm 3.657</td><td>0.751 \\pm 0.155</td><td>0.498 \\pm 0.153</td></tr><tr><th>Naive Composition</th><td>16.624 \\pm 2.906</td><td>0.648 \\pm 0.197</td><td>0.342 \\pm 0.096</td></tr><tr><th>Naive Composition++</th><td>17.535 \\pm 2.698</td><td>0.688 \\pm 0.184</td><td>0.317 \\pm 0.107</td></tr><tr><th>Ours (10 minutes)</th><td></td><td></td><td></td></tr><tr><th>K=50,N=50</th><td>22.430 \\pm 4.748</td><td>0.795 \\pm 0.142</td><td>0.256 \\pm 0.108</td></tr><tr><th> K=100,N=100</th><td>22.868 \\pm 4.588</td><td>0.802 \\pm 0.140</td><td>0.269 \\pm 0.120</td></tr><tr><th>K=200,N=200</th><td>23.016 \\pm 4.698</td><td>0.803 \\pm 0.144</td><td>0.285 \\pm 0.132</td></tr><tr><th>K=ALL,N=200*</th><td>22.090 \\pm 4.263</td><td>0.786 \\pm 0.154</td><td>0.332 \\pm 0.145</td></tr><tr><th>Ours (50 minutes)</th><td></td><td></td><td></td></tr><tr><th>K=50,N=50</th><td>22.261 \\pm 4.812</td><td>0.791 \\pm 0.144</td><td>0.252 \\pm 0.105</td></tr><tr><th>K=100,N=100</th><td>22.739 \\pm 4.637</td><td>0.801 \\pm 0.142</td><td>0.258 \\pm 0.113</td></tr><tr><th>K=200,N=200</th><td>23.020 \\pm 4.690</td><td>0.805 \\pm 0.143</td><td>0.271 \\pm 0.123</td></tr><tr><th>K=ALL,N=200*</th><td>21.788 \\pm 4.243</td><td>0.780 \\pm 0.154</td><td>0.317 \\pm 0.137</td></tr><tr><th>resized to original resolution</th><td></td><td></td><td></td></tr><tr><th>NeRF [27]</th><td>22.009 \\pm 3.148</td><td>0.757 \\pm 0.156</td><td>0.487 \\pm 0.180</td></tr><tr><th>NeX [44]</th><td>22.292 \\pm 3.137</td><td>0.774 \\pm 0.152</td><td>0.423 \\pm 0.156</td></tr></tbody></table>", "caption": "Table 3: Shiny dataset: We study our approach on the 8 real sequences from the Shiny dataset [44]. NeRF is trained for 2M iterations taking approx 64 hours. We also add the results of 4\\times bi-linearly upsampled results from NeX [44] on these sequences. Our approach gets competitive performance in only a few minutes. ", "list_citation_info": ["[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)", "[44] Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: Real-time view synthesis with neural basis expansion. In: CVPR (2021)"]}, {"table": "<table><tbody><tr><th>8 sequences</th><td>PSNR\\uparrow</td><td>MCSSIM\\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><th>4032\\times3024</th><td></td><td></td><td></td></tr><tr><th>NeRF</th><td></td><td></td><td></td></tr><tr><th>vanilla</th><td>25.192 \\pm 3.681</td><td>0.881 \\pm 0.063</td><td>0.396 \\pm 0.084</td></tr><tr><th>2M iterations</th><td>25.666 \\pm 3.833</td><td>0.887 \\pm 0.062</td><td>0.372 \\pm 0.080</td></tr><tr><th>Naive Composition</th><td>17.147 \\pm 2.878</td><td>0.687 \\pm 0.134</td><td>0.528 \\pm 0.116</td></tr><tr><th>Naive Composition++</th><td>18.280 \\pm 2.852</td><td>0.732 \\pm 0.124</td><td>0.475 \\pm 0.118</td></tr><tr><th>Ours (10 minutes)</th><td></td><td></td><td></td></tr><tr><th>K=50,N=50</th><td>22.561 \\pm 3.361</td><td>0.848 \\pm 0.078</td><td>0.347 \\pm 0.085</td></tr><tr><th> K=100,N=100</th><td>22.951 \\pm 3.564</td><td>0.854 \\pm 0.077</td><td>0.361 \\pm 0.087</td></tr><tr><th>K=200,N=200</th><td>22.930 \\pm 3.612</td><td>0.854 \\pm 0.078</td><td>0.380 \\pm 0.096</td></tr><tr><th>K=ALL,N=200*</th><td>21.650 \\pm 2.605</td><td>0.841 \\pm 0.072</td><td>0.416 \\pm 0.079</td></tr><tr><th>Ours (50 minutes)</th><td></td><td></td><td></td></tr><tr><th>K=50,N=50</th><td>22.335 \\pm 3.316</td><td>0.839 \\pm 0.086</td><td>0.355 \\pm 0.099</td></tr><tr><th>K=100,N=100</th><td>23.020 \\pm 3.500</td><td>0.851 \\pm 0.079</td><td>0.356 \\pm 0.093</td></tr><tr><th>K=200,N=200</th><td>23.237 \\pm 3.673</td><td>0.853 \\pm 0.082</td><td>0.369 \\pm 0.105</td></tr><tr><th>K=ALL,N=200*</th><td>21.650 \\pm 2.605</td><td>0.841 \\pm 0.072</td><td>0.400 \\pm 0.090</td></tr><tr><th>resized to original resolution</th><td></td><td></td><td></td></tr><tr><th>SRN [39]</th><td>21.147 \\pm 3.140</td><td>0.821 \\pm 0.078</td><td>0.594 \\pm 0.113</td></tr><tr><th>LLFF [26]</th><td>23.334 \\pm 3.315</td><td>0.863 \\pm 0.064</td><td>0.431 \\pm 0.091</td></tr><tr><th>NeRF</th><td>25.076 \\pm 3.432</td><td>0.871 \\pm 0.062</td><td>0.439 \\pm 0.103</td></tr><tr><th>NeX</th><td>25.430 \\pm 3.503</td><td>0.881 \\pm 0.058</td><td>0.387 \\pm 0.077</td></tr></tbody></table>", "caption": "Table 4: Real forward-facing dataset: We study our approach on the original resolution of the 8 real sequences from Mildenhall et al. [26]. We also add the results of 4\\times bi-linearly upsampled results from NeX [44] on these sequences. Our approach underperform PSNR and SSIM but competitive LPIPS score. ", "list_citation_info": ["[39] Sitzmann, V., Zollh\u00f6fer, M., Wetzstein, G.: Scene representation networks: Continuous 3d-structure-aware neural scene representations. In: Neurips (2019)", "[44] Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: Real-time view synthesis with neural basis expansion. In: CVPR (2021)", "[26] Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph. (2019)"]}, {"table": "<table><thead><tr><th>5 sequences</th><th>PSNR\\uparrow</th><th>SSIM\\uparrow</th><th>LPIPS \\downarrow</th></tr></thead><tbody><tr><th>Naive Composition</th><td>13.723 \\pm 2.759</td><td>0.342 \\pm 0.110</td><td>0.665 \\pm 0.113</td></tr><tr><th>Open4D [3]</th><td>20.355 \\pm 4.425</td><td>0.626 \\pm 0.131</td><td>0.306 \\pm 0.079</td></tr><tr><th>Ours</th><td>21.458 \\pm 4.690</td><td>0.645 \\pm 0.145</td><td>0.431 \\pm 0.139</td></tr></tbody></table>", "caption": "Table 7: Unseen Temporal Sequences:  We study the compositional ability of our model in contrast to the more explicit Open4D. The model is trained with multi-views available for 300-400 time instances and evaluated on unseen 100 time instances. There are a total of 5297 frames used for evaluation. Our approach is able to generate results competitive to Open4D without any modification.", "list_citation_info": ["[3] Bansal, A., Vo, M., Sheikh, Y., Ramanan, D., Narasimhan, S.: 4d visualization of dynamic events from unconstrained multi-view videos. In: CVPR (2020)"]}, {"table": "<table><thead><tr><th>5 sequences</th><th>PSNR\\uparrow</th><th>SSIM\\uparrow</th><th>LPIPS \\downarrow</th></tr></thead><tbody><tr><th>Naive Composition</th><td>14.584 \\pm 3.364</td><td>0.374 \\pm 0.089</td><td>0.617 \\pm 0.064</td></tr><tr><th>Open4D [3]</th><td>16.681 \\pm 2.718</td><td>0.498 \\pm 0.071</td><td>0.477 \\pm 0.061</td></tr><tr><th>Ours (w/o T)</th><td>16.665 \\pm 2.365</td><td>0.519 \\pm 0.074</td><td>0.538 \\pm 0.071</td></tr><tr><th>Ours (w/ T)</th><td>16.797 \\pm 2.523</td><td>0.535 \\pm 0.080</td><td>0.522 \\pm 0.075</td></tr></tbody></table>", "caption": "Table 8: Held-Out Camera Views:  We contrast the performance of our approach with Open4D [3] to synthesize held-out camera views. There are a total of 2092 frames used for evaluation. Quantitatively, we achieve similar performance. Importantly, our approach does not require heuristics to compute a foreground and background image. Finally, we further improve performance by incorporating temporal information as an input to the model.", "list_citation_info": ["[3] Bansal, A., Vo, M., Sheikh, Y., Ramanan, D., Narasimhan, S.: 4d visualization of dynamic events from unconstrained multi-view videos. In: CVPR (2020)"]}, {"table": "<table><tbody><tr><th>24 sequences</th><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><th>Num-Epochs</th><td></td><td></td><td></td></tr><tr><th>1</th><td>18.181 \\pm 1.519</td><td>0.559 \\pm 0.079</td><td>0.533 \\pm 0.066</td></tr><tr><th>2</th><td>18.139 \\pm 1.378</td><td>0.562 \\pm 0.077</td><td>0.528 \\pm 0.062</td></tr><tr><th>3</th><td>18.016 \\pm 1.461</td><td>0.562 \\pm 0.077</td><td>0.527 \\pm 0.063</td></tr><tr><th>4</th><td>18.026 \\pm 1.420</td><td>0.563 \\pm 0.076</td><td>0.527 \\pm 0.063</td></tr><tr><th>5</th><td>18.115 \\pm 1.459</td><td>0.566 \\pm 0.076</td><td>0.523 \\pm 0.061</td></tr><tr><th>6</th><td>17.877 \\pm 1.409</td><td>0.561 \\pm 0.075</td><td>0.532 \\pm 0.061</td></tr><tr><th>7</th><td>17.951 \\pm 1.456</td><td>0.562 \\pm 0.076</td><td>0.531 \\pm 0.059</td></tr><tr><th>8</th><td>17.918 \\pm 1.511</td><td>0.562 \\pm 0.077</td><td>0.532 \\pm 0.061</td></tr><tr><th>9</th><td>17.951 \\pm 1.475</td><td>0.562 \\pm 0.076</td><td>0.531 \\pm 0.058</td></tr><tr><th>10</th><td>17.948 \\pm 1.472</td><td>0.562 \\pm 0.077</td><td>0.534 \\pm 0.061</td></tr><tr><th>NeRF [27]</th><td>13.693 \\pm 2.050</td><td>0.317 \\pm 0.094</td><td>0.713 \\pm 0.089</td></tr><tr><th>DS-NeRF [6]</th><td>14.531 \\pm 2.603</td><td>0.316 \\pm 0.099</td><td>0.757 \\pm 0.040</td></tr><tr><th>LLFF [26]</th><td>15.187 \\pm 2.166</td><td>0.384 \\pm 0.082</td><td>0.602 \\pm 0.090</td></tr></tbody></table>", "caption": "Table 9: Sparse and Unconstrained Multi-Views : We follow the evaluation criterion in Table 1. We observe that our model gets the best performance in the the first 10 seconds of training. We contrast the performance of NeRF and DS-NeRF which takes 420 minutes of training on a single NVIDIA V100 GPU. We also show the performance of LLFF which is an off-the-shelf method and does not require training.", "list_citation_info": ["[6] Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised nerf: Fewer views and faster training for free. In: CVPR (2022)", "[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)", "[26] Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph. (2019)"]}, {"table": "<table><tbody><tr><th>8 sequences</th><td>PSNR\\uparrow</td><td>MCSSIM\\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><th>Num-Epochs</th><td></td><td></td><td></td></tr><tr><th>1</th><td>22.184 \\pm 4.211</td><td>0.793 \\pm 0.142</td><td>0.268 \\pm 0.110</td></tr><tr><th>2</th><td>22.270 \\pm 4.321</td><td>0.795 \\pm 0.142</td><td>0.263 \\pm 0.108</td></tr><tr><th>3</th><td>22.316 \\pm 4.372</td><td>0.795 \\pm 0.141</td><td>0.261 \\pm 0.107</td></tr><tr><th>4</th><td>22.348 \\pm 4.379</td><td>0.796 \\pm 0.141</td><td>0.260 \\pm 0.107</td></tr><tr><th>5</th><td>22.234 \\pm 4.399</td><td>0.795 \\pm 0.141</td><td>0.259 \\pm 0.107</td></tr><tr><th>6</th><td>22.395 \\pm 4.542</td><td>0.795 \\pm 0.141</td><td>0.258 \\pm 0.107</td></tr><tr><th>7</th><td>22.375 \\pm 4.579</td><td>0.795 \\pm 0.142</td><td>0.258 \\pm 0.017</td></tr><tr><th>8</th><td>22.386 \\pm 4.625</td><td>0.795 \\pm 0.142</td><td>0.257 \\pm 0.107</td></tr><tr><th>9</th><td>22.430 \\pm 4.677</td><td>0.795 \\pm 0.142</td><td>0.257 \\pm 0.107</td></tr><tr><th>10</th><td>22.430 \\pm 4.740</td><td>0.795 \\pm 0.142</td><td>0.256 \\pm 0.107</td></tr><tr><th>NeRF [27]</th><td>22.009 \\pm 3.148</td><td>0.757 \\pm 0.156</td><td>0.487 \\pm 0.180</td></tr><tr><th>NeRF-2M</th><td>21.457 \\pm 3.657</td><td>0.751 \\pm 0.155</td><td>0.498 \\pm 0.153</td></tr><tr><th>NeX [44]</th><td>22.292 \\pm 3.137</td><td>0.774 \\pm 0.152</td><td>0.423 \\pm 0.156</td></tr></tbody></table>", "caption": "Table 13: Shiny dataset and (K=50,N=50): We use 50 stereo-pairs and 50 3D points. We follow the evaluation criterion in Table 3. We observe that our model gets close to the best performing model in the first 60 seconds of training. For reference, we also show the performance of NeRF models. We also show the performance of NeX models take 24-30 hours of training for one-fourth resolution. ", "list_citation_info": ["[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)", "[44] Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: Real-time view synthesis with neural basis expansion. In: CVPR (2021)"]}, {"table": "<table><tbody><tr><th>8 sequences</th><td>PSNR\\uparrow</td><td>MCSSIM\\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><th>Num-Epochs</th><td></td><td></td><td></td></tr><tr><th>1</th><td>22.519 \\pm 4.197</td><td>0.799 \\pm 0.142</td><td>0.287 \\pm 0.126</td></tr><tr><th>2</th><td>22.647 \\pm 4.264</td><td>0.801 \\pm 0.140</td><td>0.281 \\pm 0.123</td></tr><tr><th>3</th><td>22.665 \\pm 4.305</td><td>0.801 \\pm 0.140</td><td>0.279 \\pm 0.123</td></tr><tr><th>4</th><td>22.718 \\pm 4.347</td><td>0.801 \\pm 0.140</td><td>0.278 \\pm 0.123</td></tr><tr><th>5</th><td>22.745 \\pm 4.369</td><td>0.801 \\pm 0.141</td><td>0.277 \\pm 0.123</td></tr><tr><th>6</th><td>22.756 \\pm 4.437</td><td>0.801 \\pm 0.141</td><td>0.275 \\pm 0.123</td></tr><tr><th>7</th><td>22.812 \\pm 4.499</td><td>0.802 \\pm 0.141</td><td>0.273 \\pm 0.123</td></tr><tr><th>8</th><td>22.754 \\pm 4.401</td><td>0.802 \\pm 0.141</td><td>0.272 \\pm 0.123</td></tr><tr><th>9</th><td>22.843 \\pm 4.455</td><td>0.802 \\pm 0.141</td><td>0.270 \\pm 0.123</td></tr><tr><th>10</th><td>22.868 \\pm 4.588</td><td>0.802 \\pm 0.141</td><td>0.269 \\pm 0.123</td></tr><tr><th>NeRF [27]</th><td>22.009 \\pm 3.148</td><td>0.757 \\pm 0.156</td><td>0.487 \\pm 0.180</td></tr><tr><th>NeRF-2M</th><td>21.457 \\pm 3.657</td><td>0.751 \\pm 0.155</td><td>0.498 \\pm 0.153</td></tr><tr><th>NeX [44]</th><td>22.292 \\pm 3.137</td><td>0.774 \\pm 0.152</td><td>0.423 \\pm 0.156</td></tr></tbody></table>", "caption": "Table 14: Shiny dataset and (K=100,N=100): We use 100 stereo-pairs and 100 3D points. We follow the evaluation criterion in Table 3. We observe that our model gets close to the best performing model in the first 60 seconds of training. For reference, we also show the performance of NeRF models. We also show the performance of NeX models take 24-30 hours of training for one-fourth resolution. ", "list_citation_info": ["[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)", "[44] Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: Real-time view synthesis with neural basis expansion. In: CVPR (2021)"]}, {"table": "<table><tbody><tr><th>8 sequences</th><td>PSNR\\uparrow</td><td>MCSSIM\\uparrow</td><td>LPIPS \\downarrow</td></tr><tr><th>Num-Epochs</th><td></td><td></td><td></td></tr><tr><th>1</th><td>22.563 \\pm 4.269</td><td>0.799 \\pm 0.147</td><td>0.311 \\pm 0.141</td></tr><tr><th>2</th><td>22.734 \\pm 4.385</td><td>0.800 \\pm 0.146</td><td>0.303 \\pm 0.138</td></tr><tr><th>3</th><td>22.788 \\pm 4.413</td><td>0.801 \\pm 0.145</td><td>0.301 \\pm 0.137</td></tr><tr><th>4</th><td>22.838 \\pm 4.428</td><td>0.802 \\pm 0.145</td><td>0.298 \\pm 0.137</td></tr><tr><th>5</th><td>22.847 \\pm 4.467</td><td>0.802 \\pm 0.145</td><td>0.294 \\pm 0.135</td></tr><tr><th>6</th><td>22.878 \\pm 4.478</td><td>0.802 \\pm 0.145</td><td>0.293 \\pm 0.135</td></tr><tr><th>7</th><td>22.916 \\pm 4.543</td><td>0.802 \\pm 0.145</td><td>0.291 \\pm 0.134</td></tr><tr><th>8</th><td>22.934 \\pm 4.571</td><td>0.802 \\pm 0.145</td><td>0.289 \\pm 0.133</td></tr><tr><th>9</th><td>22.947 \\pm 4.603</td><td>0.803 \\pm 0.144</td><td>0.287 \\pm 0.133</td></tr><tr><th>10</th><td>23.016 \\pm 4.698</td><td>0.803 \\pm 0.144</td><td>0.285 \\pm 0.132</td></tr><tr><th>NeRF [27]</th><td>22.009 \\pm 3.148</td><td>0.757 \\pm 0.156</td><td>0.487 \\pm 0.180</td></tr><tr><th>NeRF-2M</th><td>21.457 \\pm 3.657</td><td>0.751 \\pm 0.155</td><td>0.498 \\pm 0.153</td></tr><tr><th>NeX [44]</th><td>22.292 \\pm 3.137</td><td>0.774 \\pm 0.152</td><td>0.423 \\pm 0.156</td></tr></tbody></table>", "caption": "Table 15: Shiny dataset and (K=200,N=200): We use 200 stereo-pairs and 200 3D points. We follow the evaluation criterion in Table 3. We observe that our model gets close to the best performing model in the first 60 seconds of training. For reference, we also show the performance of NeRF models. We also show the performance of NeX models take 24-30 hours of training for one-fourth resolution. ", "list_citation_info": ["[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)", "[44] Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: Real-time view synthesis with neural basis expansion. In: CVPR (2021)"]}], "citation_info_to_title": {"[27] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)": "Nerf: Representing scenes as neural radiance fields for view synthesis", "[44] Wizadwongsa, S., Phongthawee, P., Yenphraphai, J., Suwajanakorn, S.: Nex: Real-time view synthesis with neural basis expansion. In: CVPR (2021)": "Nex: Real-time view synthesis with neural basis expansion", "[26] Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph. (2019)": "Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines", "[49] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as a perceptual metric. In: CVPR (2018)": "The unreasonable effectiveness of deep features as a perceptual metric", "[3] Bansal, A., Vo, M., Sheikh, Y., Ramanan, D., Narasimhan, S.: 4d visualization of dynamic events from unconstrained multi-view videos. In: CVPR (2020)": "4D Visualization of Dynamic Events from Unconstrained Multi-View Videos", "[6] Deng, K., Liu, A., Zhu, J.Y., Ramanan, D.: Depth-supervised nerf: Fewer views and faster training for free. In: CVPR (2022)": "Depth-Supervised NeRF: Fewer Views and Faster Training for Free", "[39] Sitzmann, V., Zollh\u00f6fer, M., Wetzstein, G.: Scene representation networks: Continuous 3d-structure-aware neural scene representations. In: Neurips (2019)": "Scene representation networks: Continuous 3d-structure-aware neural scene representations"}, "source_title_to_arxiv_id": {"Depth-Supervised NeRF: Fewer Views and Faster Training for Free": "2107.02791"}}