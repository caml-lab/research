{"title": "DEVIANT: Depth EquiVarIAnt NeTwork for Monocular 3D Object Detection", "abstract": "Modern neural networks use building blocks such as convolutions that are\nequivariant to arbitrary 2D translations. However, these vanilla blocks are not\nequivariant to arbitrary 3D translations in the projective manifold. Even then,\nall monocular 3D detectors use vanilla blocks to obtain the 3D coordinates, a\ntask for which the vanilla blocks are not designed for. This paper takes the\nfirst step towards convolutions equivariant to arbitrary 3D translations in the\nprojective manifold. Since the depth is the hardest to estimate for monocular\ndetection, this paper proposes Depth EquiVarIAnt NeTwork (DEVIANT) built with\nexisting scale equivariant steerable blocks. As a result, DEVIANT is\nequivariant to the depth translations in the projective manifold whereas\nvanilla networks are not. The additional depth equivariance forces the DEVIANT\nto learn consistent depth estimates, and therefore, DEVIANT achieves\nstate-of-the-art monocular 3D detection results on KITTI and Waymo datasets in\nthe image-only category and performs competitively to methods using extra\ninformation. Moreover, DEVIANT works better than vanilla networks in\ncross-dataset evaluation. Code and models at\nhttps://github.com/abhi1kumar/DEVIANT", "authors": ["Abhinav Kumar", "Garrick Brazil", "Enrique Corona", "Armin Parchami", "Xiaoming Liu"], "published_date": "2022_07_21", "pdf_url": "http://arxiv.org/pdf/2207.10758v1", "list_table_and_caption": [{"table": "<table><tr><td>    </td><td colspan=\"3\">3D  </td><td colspan=\"2\">Proj. 2D  </td></tr><tr><td> Translation <p>\\relbar\\joinrel\\mathrel{\\RHD}</p>  </td><td>x-ax</td><td>y-ax</td><td>z-ax  </td><td>u-ax</td><td>v-ax  </td></tr><tr><td></td><td>(t_{x})</td><td>(t_{y})</td><td>(t_{z})  </td><td>(t_{u})</td><td>(t_{v})  </td></tr><tr><td>  Vanilla CNN</td><td>-</td><td>-</td><td>-  </td><td>\u2713</td><td>\u2713  </td></tr><tr><td> Log-polar [106]  </td><td>-</td><td>-</td><td>\u2713  </td><td>-</td><td>-  </td></tr><tr><td> DEVIANT  </td><td>-</td><td>-</td><td>\u2713  </td><td>\u2713</td><td>\u2713  </td></tr><tr><td> Ideal  </td><td>\u2713</td><td>\u2713</td><td>\u2713  </td><td>-</td><td>-  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Equivariance comparisons. [Key: Proj.= Projected, ax= axis]", "list_citation_info": ["[106] Zwicke, P., Kiss, I.: A new implementation of the mellin transform and its application to radar classification of ships. TPAMI (1983)"]}, {"table": "<table><tr><td>  Transformation <p>\\relbar\\joinrel\\mathrel{\\RHD}</p></td><td rowspan=\"2\">Translation</td><td rowspan=\"2\">Rotation</td><td rowspan=\"2\">Scale</td><td rowspan=\"2\">Flips</td><td rowspan=\"2\">Learned  </td></tr><tr><td> Manifold \\relbar\\joinrel\\mathrel{\\RHD}  </td></tr><tr><td rowspan=\"2\">  Euclidean</td><td rowspan=\"2\">Vanilla CNN[40]</td><td>Polar,</td><td>Log-polar[31],</td><td rowspan=\"2\">ChiralNets[96]</td><td rowspan=\"2\">Transformers[21]  </td></tr><tr><td>Steerable[91]</td><td>Steerable[29]</td></tr><tr><td> Spherical  </td><td>Spherical CNN[15]</td><td>-</td><td>-</td><td>-</td><td>-  </td></tr><tr><td> Hyperbolic  </td><td>Hyperbolic CNN[26]</td><td>-</td><td>-</td><td>-</td><td>-  </td></tr><tr><td> Projective  </td><td>Monocular Detector</td><td>-</td><td>-</td><td>-</td><td>-  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 2: Equivariances known in the literature.", "list_citation_info": ["[31] Henriques, J., Vedaldi, A.: Warped convolutions: Efficient invariance to spatial transformations. In: ICML (2017)", "[26] Ganea, O.E., B\u00e9cigneul, G., Hofmann, T.: Hyperbolic neural networks. In: NeurIPS (2017)", "[29] Ghosh, R., Gupta, A.: Scale steerable filters for locally scale-invariant convolutional neural networks. In: ICML Workshops (2019)", "[96] Yeh, R., Hu, Y.T., Schwing, A.: Chirality nets for human pose regression. NeurIPS (2019)", "[21] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)", "[15] Cohen, T., Geiger, M., K\u00f6hler, J., Welling, M.: Spherical CNNs. In: ICLR (2018)", "[91] Worrall, D., Garbin, S., Turmukhambetov, D., Brostow, G.: Harmonic networks: Deep translation and rotation equivariance. In: CVPR (2017)", "[40] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE (1998)"]}, {"table": "<table><tr><td rowspan=\"2\">  Method</td><td rowspan=\"2\">Extra  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  AutoShape [48]</td><td>CAD  </td><td>22.47</td><td>14.17</td><td>11.36  </td><td>30.66</td><td>20.08</td><td>15.59  </td></tr><tr><td> PCT [80]  </td><td>Depth  </td><td>21.00</td><td>13.37</td><td>11.31  </td><td>29.65</td><td>19.03</td><td>15.92  </td></tr><tr><td> DFR-Net [105]  </td><td>Depth  </td><td>19.40</td><td>13.63</td><td>10.35  </td><td>28.17</td><td>19.17</td><td>14.84  </td></tr><tr><td> MonoDistill [14]  </td><td>Depth  </td><td>22.97</td><td>16.03</td><td>13.60  </td><td>31.87</td><td>22.59</td><td>19.72  </td></tr><tr><td> PatchNet-C [69]  </td><td>LiDAR  </td><td>22.40</td><td>12.53</td><td>10.60  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> CaDDN [62]  </td><td>LiDAR  </td><td>19.17</td><td>13.41</td><td>11.46  </td><td>27.94</td><td>18.91</td><td>17.19  </td></tr><tr><td> DD3D [57]  </td><td>LiDAR  </td><td>23.22</td><td>16.34</td><td>14.20  </td><td>30.98</td><td>22.56</td><td>20.03  </td></tr><tr><td> MonoEF [103]  </td><td>Odometry  </td><td>21.29</td><td>13.87</td><td>11.71  </td><td>29.03</td><td>19.70</td><td>17.26  </td></tr><tr><td> Kinematic [5]  </td><td>Video  </td><td>19.07</td><td>12.72</td><td>9.17  </td><td>26.69</td><td>17.52</td><td>13.10  </td></tr><tr><td>  GrooMeD-NMS[36]</td><td>-  </td><td>18.10</td><td>12.32</td><td>9.65  </td><td>26.19</td><td>18.27</td><td>14.05  </td></tr><tr><td> MonoRCNN [67]  </td><td>-  </td><td>18.36</td><td>12.65</td><td>10.03  </td><td>25.48</td><td>18.11</td><td>14.10  </td></tr><tr><td> MonoDIS-M[68]  </td><td>-  </td><td>16.54</td><td>12.97</td><td>11.04  </td><td>24.45</td><td>19.25</td><td>16.87  </td></tr><tr><td> Ground-Aware [47]  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{21.65}}</td><td>13.25</td><td>9.91  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{29.81}}</td><td>17.98</td><td>13.08  </td></tr><tr><td> MonoFlex [100]  </td><td>-  </td><td>19.94</td><td>13.89</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{12.07}}  </td><td>28.23</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{19.75}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{16.89}}  </td></tr><tr><td> GUP Net [49]  </td><td>-  </td><td>20.11</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{14.20}}</td><td>11.77  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{21.88}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{14.46}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{11.89}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{29.65}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{20.44}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{17.43}}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 3: Results on KITTI Test carsat IoU{}_{3\\text{D}} \\geq\\!0.7. Previous results are from the leader-board or papers.We show 3 methods in each Extra category and 6 methods in the image-only category. [Key: Best, Second Best]", "list_citation_info": ["[103] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: MonoEF: Extrinsic parameter free monocular 3333D object detection. TPAMI (2021)", "[36] Kumar, A., Brazil, G., Liu, X.: GrooMeD-NMS: Grouped mathematically differentiable NMS for monocular 3333D object detection. In: CVPR (2021)", "[48] Liu, Z., Zhou, D., Lu, F., Fang, J., Zhang, L.: AutoShape: Real-time shape-aware monocular 3333D object detection. In: ICCV (2021)", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)", "[67] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposition for monocular 3333D object detection. In: ICCV (2021)", "[105] Zou, Z., Ye, X., Du, L., Cheng, X., Tan, X., Zhang, L., Feng, J., Xue, X., Ding, E.: The devil is in the task: Exploiting reciprocal appearance-localization features for monocular 3333D object detection. In: ICCV (2021)", "[68] Simonelli, A., Bul\u00f2, S., Porzi, L., Antequera, M., Kontschieder, P.: Disentangling monocular 3333D object detection: From single to multi-class recognition. TPAMI (2020)", "[69] Simonelli, A., Bul\u00f2, S., Porzi, L., Kontschieder, P., Ricci, E.: Are we missing confidence in Pseudo-LiDAR methods for monocular 3333D object detection? In: ICCV (2021)", "[47] Liu, Y., Yixuan, Y., Liu, M.: Ground-aware monocular 3333D object detection for autonomous driving. Robotics and Automation Letters (2021)", "[100] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3333D object detection. In: CVPR (2021)", "[5] Brazil, G., Pons-Moll, G., Liu, X., Schiele, B.: Kinematic 3333D object detection in monocular video. In: ECCV (2020)", "[14] Chong, Z., Ma, X., Zhang, H., Yue, Y., Li, H., Wang, Z., Ouyang, W.: MonoDistill: Learning spatial features for monocular 3333D object detection. In: ICLR (2022)", "[62] Reading, C., Harakeh, A., Chae, J., Waslander, S.: Categorical depth distribution network for monocular 3333D object detection. In: CVPR (2021)", "[80] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3333D object detection. In: NeurIPS (2021)", "[57] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is Pseudo-LiDAR needed for monocular 3333D object detection? In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"2\">  Method</td><td rowspan=\"2\">Extra  </td><td colspan=\"3\">Cyc AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">Ped AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  DDMP-3D[79]</td><td>Depth  </td><td>4.18</td><td>2.50</td><td>2.32  </td><td>4.93</td><td>3.55</td><td>3.01  </td></tr><tr><td> DFR-Net [105]  </td><td>Depth  </td><td>5.69</td><td>3.58</td><td>3.10  </td><td>6.09</td><td>3.62</td><td>3.39  </td></tr><tr><td> MonoDistill [14]  </td><td>Depth  </td><td>5.53</td><td>2.81</td><td>2.40  </td><td>12.79</td><td>8.17</td><td>7.45  </td></tr><tr><td> CaDDN [62]  </td><td>LiDAR  </td><td>7.00</td><td>3.41</td><td>3.30  </td><td>12.87</td><td>8.14</td><td>6.76  </td></tr><tr><td> DD3D [57]  </td><td>LiDAR  </td><td>2.39</td><td>1.52</td><td>1.31  </td><td>13.91</td><td>9.30</td><td>8.05  </td></tr><tr><td> MonoEF [103]  </td><td>Odometry  </td><td>1.80</td><td>0.92</td><td>0.71  </td><td>4.27</td><td>2.79</td><td>2.21  </td></tr><tr><td>  MonoDIS-M[68]</td><td>-  </td><td>1.17</td><td>0.54</td><td>0.48  </td><td>7.79</td><td>5.14</td><td>4.42  </td></tr><tr><td> MonoFlex [100]  </td><td>-  </td><td>3.39</td><td>2.10</td><td>1.67  </td><td>11.89</td><td>8.16</td><td>6.81  </td></tr><tr><td> GUP Net [49]  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{4.18}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.65}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.09}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{14.72}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{9.53}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{7.87}}  </td></tr><tr><td> DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{5.05}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{3.13}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{2.59}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{13.43}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{8.65}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{7.69}}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 4: Results on KITTI Test cyclists and pedestrians (Cyc/Ped) at IoU{}_{3\\text{D}}\\geq\\!0.5. Previous results are from the leader-board or papers. [Key: Best, Second Best]", "list_citation_info": ["[103] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: MonoEF: Extrinsic parameter free monocular 3333D object detection. TPAMI (2021)", "[79] Wang, L., Du, L., Ye, X., Fu, Y., Guo, G., Xue, X., Feng, J., Zhang, L.: Depth-conditioned dynamic message propagation for monocular 3333D object detection. In: CVPR (2021)", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)", "[68] Simonelli, A., Bul\u00f2, S., Porzi, L., Antequera, M., Kontschieder, P.: Disentangling monocular 3333D object detection: From single to multi-class recognition. TPAMI (2020)", "[100] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3333D object detection. In: CVPR (2021)", "[14] Chong, Z., Ma, X., Zhang, H., Yue, Y., Li, H., Wang, Z., Ouyang, W.: MonoDistill: Learning spatial features for monocular 3333D object detection. In: ICLR (2022)", "[62] Reading, C., Harakeh, A., Chae, J., Waslander, S.: Categorical depth distribution network for monocular 3333D object detection. In: CVPR (2021)", "[105] Zou, Z., Ye, X., Du, L., Cheng, X., Tan, X., Zhang, L., Feng, J., Xue, X., Ding, E.: The devil is in the task: Exploiting reciprocal appearance-localization features for monocular 3333D object detection. In: ICCV (2021)", "[57] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is Pseudo-LiDAR needed for monocular 3333D object detection? In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"3\">  Method</td><td rowspan=\"3\">Extra  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.5  </td></tr><tr><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  DDMP-3D[79]</td><td>Depth  </td><td>28.12</td><td>20.39</td><td>16.34  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> PCT [80]  </td><td>Depth  </td><td>38.39</td><td>27.53</td><td>24.44  </td><td>47.16</td><td>34.65</td><td>28.47  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> MonoDistill [14]  </td><td>Depth  </td><td>24.31</td><td>18.47</td><td>15.76  </td><td>33.09</td><td>25.40</td><td>22.16  </td><td>65.69</td><td>49.35</td><td>43.49  </td><td>71.45</td><td>53.11</td><td>46.94  </td></tr><tr><td> CaDDN [62]  </td><td>LiDAR  </td><td>23.57</td><td>16.31</td><td>13.84  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> PatchNet-C [69]  </td><td>LiDAR  </td><td>24.51</td><td>17.03</td><td>13.25  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> DD3D (DLA34) [57]  </td><td>LiDAR  </td><td>-</td><td>-</td><td>-  </td><td>33.5</td><td>26.0</td><td>22.6  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> DD3D{}^{-}\u200b(DLA34)[57]  </td><td>LiDAR  </td><td>-</td><td>-</td><td>-  </td><td>26.8</td><td>20.2</td><td>16.7  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> MonoEF [103]  </td><td>Odometry  </td><td>18.26</td><td>16.30</td><td>15.24  </td><td>26.07</td><td>25.21</td><td>21.61  </td><td>57.98</td><td>51.80</td><td>49.34  </td><td>63.40</td><td>61.13</td><td>53.22  </td></tr><tr><td> Kinematic [5]  </td><td>Video  </td><td>19.76</td><td>14.10</td><td>10.47  </td><td>27.83</td><td>19.72</td><td>15.10  </td><td>55.44</td><td>39.47</td><td>31.26  </td><td>61.79</td><td>44.68</td><td>34.56  </td></tr><tr><td>  MonoRCNN [67]</td><td>-  </td><td>16.61</td><td>13.19</td><td>10.65  </td><td>25.29</td><td>19.22</td><td>15.30  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> MonoDLE [53]  </td><td>-  </td><td>17.45</td><td>13.66</td><td>11.68  </td><td>24.97</td><td>19.33</td><td>17.01  </td><td>55.41</td><td>43.42</td><td>37.81  </td><td>60.73</td><td>46.87</td><td>41.89  </td></tr><tr><td> GrooMeD-NMS[36]  </td><td>-  </td><td>19.67</td><td>14.32</td><td>11.27  </td><td>27.38</td><td>19.75</td><td>15.92  </td><td>55.62</td><td>41.07</td><td>32.89  </td><td>61.83</td><td>44.98</td><td>36.29  </td></tr><tr><td> Ground-Aware [47]  </td><td>-  </td><td>23.63</td><td>16.16</td><td>12.06  </td><td>-</td><td>-</td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{60.92}}</td><td>42.18</td><td>32.02  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> MonoFlex [100]  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{23.64}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{17.51}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{14.83}}  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> GUP Net (Reported)\u200b[49]  </td><td>-  </td><td>22.76</td><td>16.46</td><td>13.72  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{31.07}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{22.94}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{19.75}}  </td><td>57.62</td><td>42.33</td><td>37.59  </td><td>61.78</td><td>47.06</td><td>40.88  </td></tr><tr><td> GUP Net (Retrained)\u200b[49]  </td><td>-  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>28.58</td><td>20.92</td><td>17.83  </td><td>58.95</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{43.99}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{38.07}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{64.60}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{47.76}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{42.97}}  </td></tr><tr><td> DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{24.63}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{16.54}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{14.52}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{32.60}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{23.04}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{19.99}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{61.00}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{46.00}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{40.18}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{65.28}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{49.63}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{43.50}}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 5: Results on KITTI Val cars.Comparison with bigger CNN backbones in Tab. 16. [Key: Best, Second Best, {}^{-}= No pretrain] ", "list_citation_info": ["[103] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: MonoEF: Extrinsic parameter free monocular 3333D object detection. TPAMI (2021)", "[36] Kumar, A., Brazil, G., Liu, X.: GrooMeD-NMS: Grouped mathematically differentiable NMS for monocular 3333D object detection. In: CVPR (2021)", "[79] Wang, L., Du, L., Ye, X., Fu, Y., Guo, G., Xue, X., Feng, J., Zhang, L.: Depth-conditioned dynamic message propagation for monocular 3333D object detection. In: CVPR (2021)", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)", "[67] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposition for monocular 3333D object detection. In: ICCV (2021)", "[69] Simonelli, A., Bul\u00f2, S., Porzi, L., Kontschieder, P., Ricci, E.: Are we missing confidence in Pseudo-LiDAR methods for monocular 3333D object detection? In: ICCV (2021)", "[47] Liu, Y., Yixuan, Y., Liu, M.: Ground-aware monocular 3333D object detection for autonomous driving. Robotics and Automation Letters (2021)", "[100] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3333D object detection. In: CVPR (2021)", "[5] Brazil, G., Pons-Moll, G., Liu, X., Schiele, B.: Kinematic 3333D object detection in monocular video. In: ECCV (2020)", "[14] Chong, Z., Ma, X., Zhang, H., Yue, Y., Li, H., Wang, Z., Ouyang, W.: MonoDistill: Learning spatial features for monocular 3333D object detection. In: ICLR (2022)", "[62] Reading, C., Harakeh, A., Chae, J., Waslander, S.: Categorical depth distribution network for monocular 3333D object detection. In: CVPR (2021)", "[80] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3333D object detection. In: NeurIPS (2021)", "[53] Ma, X., Zhang, Y., Xu, D., Zhou, D., Yi, S., Li, H., Ouyang, W.: Delving into localization errors for monocular 3333D object detection. In: CVPR (2021)", "[57] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is Pseudo-LiDAR needed for monocular 3333D object detection? In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"2\">  Method</td><td colspan=\"4\">KITTI Val  </td><td colspan=\"4\">nuScenes frontal Val  </td></tr><tr><td>0\\!-\\!20</td><td>20\\!-\\!40</td><td>40\\!-\\!\\infty</td><td>All  </td><td>0\\!-\\!20</td><td>20\\!-\\!40</td><td>40\\!-\\!\\infty</td><td>All  </td></tr><tr><td>  M3D-RPN[4]</td><td>0.56</td><td>1.33</td><td>2.73</td><td>1.26  </td><td>0.94</td><td>3.06</td><td>10.36</td><td>2.67  </td></tr><tr><td> MonoRCNN [67]  </td><td>0.46</td><td>1.27</td><td>2.59</td><td>1.14  </td><td>0.94</td><td>2.84</td><td>8.65</td><td>2.39  </td></tr><tr><td> GUP Net [49]  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.45}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{1.10}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{1.85}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.89}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.82}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{1.70}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{6.20}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{1.45}}  </td></tr><tr><td> DEVIANT  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.40}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{1.09}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{1.80}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.87}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.76}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{1.60}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{4.50}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{1.26}}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 6: Cross-dataset evaluation of the KITTI Val model on KITTI Val and nuScenes frontal Val cars with depth MAE (\\relbar\\joinrel\\mathrel{\\RHD}). [Key: Best, Second Best]", "list_citation_info": ["[67] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposition for monocular 3333D object detection. In: ICCV (2021)", "[4] Brazil, G., Liu, X.: M3333D-RPN: Monocular 3333D region proposal network for object detection. In: ICCV (2019)", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"3\">  Method</td><td>Scale</td><td>Scale  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.5  </td></tr><tr><td>Eqv</td><td>Aug  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td></td><td></td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  GUP Net\u200b[49]</td><td></td><td></td><td>20.82</td><td>14.15</td><td>12.44  </td><td>29.93</td><td>20.90</td><td>17.87  </td><td>\\mathbf{62.37}</td><td>44.40</td><td>39.61  </td><td>\\mathbf{66.81}</td><td>48.09</td><td>43.14  </td></tr><tr><td></td><td></td><td>\u2713  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>28.58</td><td>20.92</td><td>17.83  </td><td>58.95</td><td>43.99</td><td>38.07  </td><td>64.60</td><td>47.76</td><td>42.97  </td></tr><tr><td>  DEVIANT</td><td>\u2713</td><td></td><td>21.33</td><td>14.77</td><td>12.57  </td><td>28.79</td><td>20.28</td><td>17.59  </td><td>59.31</td><td>43.25</td><td>37.64  </td><td>63.94</td><td>47.02</td><td>41.12  </td></tr><tr><td></td><td>\u2713</td><td>\u2713  </td><td>\\mathbf{24.63}</td><td>\\mathbf{16.54}</td><td>\\mathbf{14.52}  </td><td>\\mathbf{32.60}</td><td>\\mathbf{23.04}</td><td>\\mathbf{19.99}  </td><td>61.00</td><td>\\mathbf{46.00}</td><td>\\mathbf{40.18}  </td><td>65.28</td><td>\\mathbf{49.63}</td><td>\\mathbf{43.50}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 7: Scale Augmentation vs Scale Equivariance on KITTI Val cars. [Key: Best, Eqv= Equivariance, Aug= Augmentation]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"3\">  Method</td><td rowspan=\"3\">Eqv  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.5  </td></tr><tr><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  DETR3D{}^{\\dagger}\u200b[85]</td><td>Learned  </td><td>1.94</td><td>1.26</td><td>1.09  </td><td>4.41</td><td>3.06</td><td>2.79  </td><td>20.09</td><td>13.80</td><td>12.78  </td><td>26.51</td><td>18.49</td><td>17.36  </td></tr><tr><td> GUP Net\u200b[49]  </td><td>2D  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>28.58</td><td>20.92</td><td>17.83  </td><td>58.95</td><td>43.99</td><td>38.07  </td><td>64.60</td><td>47.76</td><td>42.97  </td></tr><tr><td> DEVIANT  </td><td>2D+Depth  </td><td>\\mathbf{24.63}</td><td>\\mathbf{16.54}</td><td>\\mathbf{14.52}  </td><td>\\mathbf{32.60}</td><td>\\mathbf{23.04}</td><td>\\mathbf{19.99}  </td><td>\\mathbf{61.00}</td><td>\\mathbf{46.00}</td><td>\\mathbf{40.18}  </td><td>\\mathbf{65.28}</td><td>\\mathbf{49.63}</td><td>\\mathbf{43.50}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 8: Comparison of Equivariant Architectures on KITTI Val cars. [Key: Best, Eqv= Equivariance, {}^{\\dagger}= Retrained]", "list_citation_info": ["[85] Wang, Y., Guizilini, V., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: DETR3D: 3333D object detection from multi-view images via 3333D-to-2222D queries. In: CoRL (2021)", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"3\">   Method</td><td rowspan=\"3\">Extra  </td><td colspan=\"6\">IoU{}_{3\\text{D}}\\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}}\\geq 0.5  </td></tr><tr><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>   D4LCN [20]</td><td>Depth  </td><td>22.32</td><td>16.20</td><td>12.30  </td><td>31.53</td><td>22.58</td><td>17.87  </td><td>-</td><td>-</td><td>-  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td>  DCNN [97]</td><td>-  </td><td>21.66</td><td>15.49</td><td>12.90  </td><td>30.22</td><td>22.06</td><td>19.01  </td><td>57.54</td><td>43.12</td><td>38.80  </td><td>63.29</td><td>46.86</td><td>42.42  </td></tr><tr><td>  DEVIANT</td><td>-  </td><td>\\mathbf{24.63}</td><td>\\mathbf{16.54}</td><td>\\mathbf{14.52}  </td><td>\\mathbf{32.60}</td><td>\\mathbf{23.04}</td><td>\\mathbf{19.99}  </td><td>\\mathbf{61.00}</td><td>\\mathbf{46.00}</td><td>\\mathbf{40.18}  </td><td>\\mathbf{65.28}</td><td>\\mathbf{49.63}</td><td>\\mathbf{43.50}  </td></tr><tr><td>   </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 9: Comparison with Dilated Convolution on KITTI Val cars. [Key: Best]", "list_citation_info": ["[97] Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: ICLR (2015)", "[20] Ding, M., Huo, Y., Yi, H., Wang, Z., Shi, J., Lu, Z., Luo, P.: Learning depth-guided convolutions for monocular 3333D object detection. In: CVPR Workshops (2020)"]}, {"table": "<table><tr><td rowspan=\"3\">  Method</td><td colspan=\"6\">IoU \\geq 0.7  </td><td colspan=\"6\">IoU \\geq 0.5  </td></tr><tr><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{2\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{2\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  GUP Net [49]</td><td>21.10</td><td>15.48</td><td>12.88  </td><td>96.78</td><td>88.87</td><td>79.02  </td><td>58.95</td><td>43.99</td><td>38.07  </td><td>99.52</td><td>91.89</td><td>81.99  </td></tr><tr><td> DEVIANT (Ours)  </td><td>24.63</td><td>16.54</td><td>14.52  </td><td>96.68</td><td>88.66</td><td>78.87  </td><td>61.00</td><td>46.00</td><td>40.18  </td><td>97.12</td><td>91.77</td><td>81.93  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 10: 3D and 2D detection on KITTI Val cars.", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td colspan=\"2\">       Change from DEVIANT :  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.5  </td></tr><tr><td rowspan=\"2\">  Changed  </td><td rowspan=\"2\">From \\relbar\\joinrel\\relbar\\joinrel\\mathrel{\\RHD}To  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>     </td><td>SES\\relbar\\joinrel\\mathrel{\\RHD}Vanilla  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>28.58</td><td>20.92</td><td>17.83  </td><td>58.95</td><td>43.99</td><td>38.07  </td><td>64.60</td><td>47.76</td><td>42.97  </td></tr><tr><td>  Convolution  </td><td>SES\\relbar\\joinrel\\mathrel{\\RHD}Log-polar\u200b[106]  </td><td>9.19</td><td>6.77</td><td>5.78  </td><td>16.39</td><td>11.15</td><td>9.80  </td><td>40.51</td><td>27.62</td><td>23.90  </td><td>45.66</td><td>31.34</td><td>25.80  </td></tr><tr><td></td><td>SES\\relbar\\joinrel\\mathrel{\\RHD}Dilated\u200b[97]  </td><td>21.66</td><td>15.49</td><td>12.90  </td><td>30.22</td><td>22.06</td><td>19.01  </td><td>57.54</td><td>43.12</td><td>38.80  </td><td>63.29</td><td>46.86</td><td>42.42  </td></tr><tr><td></td><td>SES\\relbar\\joinrel\\mathrel{\\RHD}DISCO\u200b[72]  </td><td>20.21</td><td>13.84</td><td>11.46  </td><td>28.56</td><td>19.38</td><td>16.41  </td><td>55.22</td><td>39.76</td><td>35.37  </td><td>59.46</td><td>43.16</td><td>38.52  </td></tr><tr><td>  Downscale  </td><td>10\\%\\relbar\\joinrel\\mathrel{\\RHD}5\\%  </td><td>24.24</td><td>16.51</td><td>14.43  </td><td>31.94</td><td>22.86</td><td>19.82  </td><td>60.64</td><td>44.46</td><td>40.02  </td><td>64.68</td><td>49.30</td><td>43.49  </td></tr><tr><td>  \\alpha  </td><td>10\\%\\relbar\\joinrel\\mathrel{\\RHD}20\\%  </td><td>22.19</td><td>15.85</td><td>13.48  </td><td>31.15</td><td>23.01</td><td>19.90  </td><td>61.24</td><td>44.93</td><td>40.22  </td><td>67.46</td><td>50.10</td><td>43.83  </td></tr><tr><td>  BNP  </td><td>SE\\relbar\\joinrel\\mathrel{\\RHD} Vanilla  </td><td>24.39</td><td>16.20</td><td>14.36  </td><td>32.43</td><td>22.53</td><td>19.70  </td><td>62.81</td><td>46.14</td><td>40.38  </td><td>67.87</td><td>50.23</td><td>44.08  </td></tr><tr><td>  Scales  </td><td>3\\relbar\\joinrel\\mathrel{\\RHD}1  </td><td>23.20</td><td>16.29</td><td>13.63  </td><td>31.76</td><td>23.23</td><td>19.97  </td><td>61.90</td><td>46.66</td><td>40.61  </td><td>67.37</td><td>50.31</td><td>43.93  </td></tr><tr><td></td><td>3\\relbar\\joinrel\\mathrel{\\RHD}2  </td><td>24.15</td><td>16.48</td><td>14.55  </td><td>32.42</td><td>23.17</td><td>20.07  </td><td>61.05</td><td>46.34</td><td>40.46  </td><td>67.36</td><td>50.32</td><td>44.07  </td></tr><tr><td>  \u2014  </td><td>DEVIANT (best)  </td><td>\\mathbf{24.63}</td><td>\\mathbf{16.54}</td><td>\\mathbf{14.52}  </td><td>\\mathbf{32.60}</td><td>\\mathbf{23.04}</td><td>\\mathbf{19.99}  </td><td>\\mathbf{61.00}</td><td>\\mathbf{46.00}</td><td>\\mathbf{40.18}  </td><td>\\mathbf{65.28}</td><td>\\mathbf{49.63}</td><td>\\mathbf{43.50}  </td></tr><tr><td>   </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 11: Ablation studies on KITTI Val cars.", "list_citation_info": ["[72] Sosnovik, I., Moskalev, A., Smeulders, A.: DISCO: accurate discrete scale convolutions. In: BMVC (2021)", "[97] Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: ICLR (2015)", "[106] Zwicke, P., Kiss, I.: A new implementation of the mellin transform and its application to radar classification of ships. TPAMI (1983)"]}, {"table": "<table><tr><td>     </td><td></td><td></td><td></td><td colspan=\"4\">AP{}_{3\\text{D}} [%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"4\">APH{}_{3\\text{D}} [%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td rowspan=\"-2\">  IoU{}_{3\\text{D}}  </td><td rowspan=\"-2\">Difficulty  </td><td rowspan=\"-2\">Method  </td><td rowspan=\"-2\">Extra  </td><td>All</td><td>0-30</td><td>30-50</td><td>50-\\infty  </td><td>All</td><td>0-30</td><td>30-50</td><td>50-\\infty  </td></tr><tr><td>     </td><td></td><td>CaDDN[62]  </td><td>LiDAR  </td><td>5.03</td><td>14.54</td><td>1.47</td><td>0.10  </td><td>4.99</td><td>14.43</td><td>1.45</td><td>0.10  </td></tr><tr><td></td><td></td><td>PatchNet[50] in [80]  </td><td>Depth  </td><td>0.39</td><td>1.67</td><td>0.13</td><td>0.03  </td><td>0.39</td><td>1.63</td><td>0.12</td><td>0.03  </td></tr><tr><td></td><td></td><td>PCT [80]  </td><td>Depth  </td><td>0.89</td><td>3.18</td><td>0.27</td><td>0.07  </td><td>0.88</td><td>3.15</td><td>0.27</td><td>0.07  </td></tr><tr><td>  0.7  </td><td>Level_1  </td><td>M3D-RPN[4] in [62]  </td><td>-  </td><td>0.35</td><td>1.12</td><td>0.18</td><td>0.02  </td><td>0.34</td><td>1.10</td><td>0.18</td><td>0.02  </td></tr><tr><td></td><td></td><td>GUP Net (Retrained)[49]  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.28}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{6.15}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.81}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.03}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.27}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{6.11}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.80}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.03}}  </td></tr><tr><td></td><td></td><td>DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{2.69}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{6.95}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.99}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.02}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{2.67}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{6.90}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.98}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.02}}  </td></tr><tr><td>     </td><td></td><td>CaDDN[62]  </td><td>LiDAR  </td><td>4.49</td><td>14.50</td><td>1.42</td><td>0.09  </td><td>4.45</td><td>14.38</td><td>1.41</td><td>0.09  </td></tr><tr><td></td><td></td><td>PatchNet[50] in [80]  </td><td>Depth  </td><td>0.38</td><td>1.67</td><td>0.13</td><td>0.03  </td><td>0.36</td><td>1.63</td><td>0.11</td><td>0.03  </td></tr><tr><td></td><td></td><td>PCT [80]  </td><td>Depth  </td><td>0.66</td><td>3.18</td><td>0.27</td><td>0.07  </td><td>0.66</td><td>3.15</td><td>0.26</td><td>0.07  </td></tr><tr><td>  0.7  </td><td>Level_2  </td><td>M3D-RPN[4] in [62]  </td><td>-  </td><td>0.33</td><td>1.12</td><td>0.18</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.02}}  </td><td>0.33</td><td>1.10</td><td>0.17</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.02}}  </td></tr><tr><td></td><td></td><td>GUP Net (Retrained)[49]  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.14}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{6.13}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.78}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.02}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.12}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{6.08}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.77}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.02}}  </td></tr><tr><td></td><td></td><td>DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{2.52}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{6.93}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.95}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.02}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{2.50}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{6.87}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.94}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.02}}  </td></tr><tr><td>     </td><td></td><td>CaDDN[62]  </td><td>LiDAR  </td><td>17.54</td><td>45.00</td><td>9.24</td><td>0.64  </td><td>17.31</td><td>44.46</td><td>9.11</td><td>0.62  </td></tr><tr><td></td><td></td><td>PatchNet[50] in [80]  </td><td>Depth  </td><td>2.92</td><td>10.03</td><td>1.09</td><td>0.23  </td><td>2.74</td><td>9.75</td><td>0.96</td><td>0.18  </td></tr><tr><td></td><td></td><td>PCT [80]  </td><td>Depth  </td><td>4.20</td><td>14.70</td><td>1.78</td><td>0.39  </td><td>4.15</td><td>14.54</td><td>1.75</td><td>0.39  </td></tr><tr><td>  0.5  </td><td>Level_1  </td><td>M3D-RPN[4] in [62]  </td><td>-  </td><td>3.79</td><td>11.14</td><td>2.16</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.26}}  </td><td>3.63</td><td>10.70</td><td>2.09</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.21}}  </td></tr><tr><td></td><td></td><td>GUP Net (Retrained)[49]  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{10.02}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{24.78}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{4.84}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.22}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{9.94}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{24.59}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{4.78}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.22}}  </td></tr><tr><td></td><td></td><td>DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{10.98}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{26.85}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{5.13}}</td><td>0.18  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{10.89}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{26.64}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{5.08}}</td><td>0.18  </td></tr><tr><td>     </td><td></td><td>CaDDN[62]  </td><td>LiDAR  </td><td>16.51</td><td>44.87</td><td>8.99</td><td>0.58  </td><td>16.28</td><td>44.33</td><td>8.86</td><td>0.55  </td></tr><tr><td></td><td></td><td>PatchNet[50] in [80]  </td><td>Depth  </td><td>2.42</td><td>10.01</td><td>1.07</td><td>0.22  </td><td>2.28</td><td>9.73</td><td>0.97</td><td>0.16  </td></tr><tr><td></td><td></td><td>PCT [80]  </td><td>Depth  </td><td>4.03</td><td>14.67</td><td>1.74</td><td>0.36  </td><td>4.15</td><td>14.51</td><td>1.71</td><td>0.35  </td></tr><tr><td>  0.5  </td><td>Level_2  </td><td>M3D-RPN[4] in [62]  </td><td>-  </td><td>3.61</td><td>11.12</td><td>2.12</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.24}}  </td><td>3.46</td><td>10.67</td><td>2.04</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.20}}  </td></tr><tr><td></td><td></td><td>GUP Net (Retrained)[49]  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{9.39}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{24.69}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{4.67}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.19}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{9.31}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{24.50}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{4.62}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.19}}  </td></tr><tr><td></td><td></td><td>DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{10.29}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{26.75}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{4.95}}</td><td>0.16  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{10.20}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{26.54}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{4.90}}</td><td>0.16  </td></tr><tr><td>   </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 12: Results on Waymo Val vehicles. [Key: Best, Second Best]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)", "[4] Brazil, G., Liu, X.: M3333D-RPN: Monocular 3333D region proposal network for object detection. In: ICCV (2019)", "[62] Reading, C., Harakeh, A., Chae, J., Waslander, S.: Categorical depth distribution network for monocular 3333D object detection. In: CVPR (2021)", "[50] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking Pseudo-LiDAR representation. In: ECCV (2020)", "[80] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3333D object detection. In: NeurIPS (2021)"]}, {"table": "<table><tr><td rowspan=\"2\">  Method</td><td>Input  </td><td>#Conv</td><td rowspan=\"2\">Output</td><td>Output Constrained  </td></tr><tr><td>Frame  </td><td>Kernel</td><td>for Scales?  </td></tr><tr><td>  Vanilla CNN</td><td>1  </td><td>1</td><td>4D</td><td>\u2715  </td></tr><tr><td> Aware [4]  </td><td>1  </td><td>&gt;1</td><td>4D</td><td>\u2715  </td></tr><tr><td> Dilated CNN [97]  </td><td>1  </td><td>&gt;1</td><td>5D</td><td>Integer [92]  </td></tr><tr><td> DEVIANT  </td><td>1  </td><td>&gt;1</td><td>5D</td><td>Float  </td></tr><tr><td> guided[20]  </td><td>1 + Depth  </td><td>1</td><td>4D</td><td>Integer [92]  </td></tr><tr><td> Kinematic3D [5]  </td><td>&gt;1  </td><td>1</td><td>5D</td><td>\u2715  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 13: Comparison of Methods on the basis of inputs, convolution kernels, outputs and whether output are scale-constrained.", "list_citation_info": ["[5] Brazil, G., Pons-Moll, G., Liu, X., Schiele, B.: Kinematic 3333D object detection in monocular video. In: ECCV (2020)", "[20] Ding, M., Huo, Y., Yi, H., Wang, Z., Shi, J., Lu, Z., Luo, P.: Learning depth-guided convolutions for monocular 3333D object detection. In: CVPR Workshops (2020)", "[92] Worrall, D., Welling, M.: Deep scale-spaces: Equivariance over scale. In: NeurIPS (2019)", "[4] Brazil, G., Liu, X.: M3333D-RPN: Monocular 3333D region proposal network for object detection. In: ICCV (2019)", "[97] Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: ICLR (2015)"]}, {"table": "<table><tr><td rowspan=\"3\">  Method</td><td>Scale  </td><td rowspan=\"3\">Set  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.5  </td></tr><tr><td>Eqv  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td></td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td rowspan=\"3\">  GUP Net [49]</td><td></td><td>Train  </td><td>91.83</td><td>74.87</td><td>67.43  </td><td>95.19</td><td>80.95</td><td>73.55  </td><td>99.50</td><td>93.62</td><td>86.22  </td><td>99.56</td><td>93.88</td><td>86.46  </td></tr><tr><td></td><td>Val  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>28.58</td><td>20.92</td><td>17.83  </td><td>58.95</td><td>43.99</td><td>38.07  </td><td>64.60</td><td>47.76</td><td>42.97  </td></tr><tr><td></td><td>Gap  </td><td>70.73</td><td>\\mathbf{59.39}</td><td>54.55  </td><td>66.61</td><td>60.03</td><td>55.72  </td><td>40.55</td><td>49.63</td><td>\\mathbf{48.15}  </td><td>34.96</td><td>46.12</td><td>\\mathbf{43.49}  </td></tr><tr><td rowspan=\"3\">  DEVIANT</td><td rowspan=\"3\">\u2713  </td><td>Train  </td><td>91.09</td><td>76.19</td><td>67.16  </td><td>94.76</td><td>82.61</td><td>75.51  </td><td>99.37</td><td>93.56</td><td>88.57  </td><td>99.50</td><td>93.87</td><td>88.90  </td></tr><tr><td>Val  </td><td>24.63</td><td>16.54</td><td>14.52  </td><td>32.60</td><td>23.04</td><td>19.99  </td><td>61.00</td><td>46.00</td><td>40.18  </td><td>65.28</td><td>49.63</td><td>43.50  </td></tr><tr><td>Gap  </td><td>\\mathbf{66.46}</td><td>59.65</td><td>\\mathbf{52.64}  </td><td>\\mathbf{62.16}</td><td>\\mathbf{59.57}</td><td>\\mathbf{55.52}  </td><td>\\mathbf{38.37}</td><td>\\mathbf{47.56}</td><td>48.39  </td><td>\\mathbf{34.22}</td><td>\\mathbf{44.24}</td><td>45.40  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 14: Generalization gap (\\relbar\\joinrel\\mathrel{\\RHD}) on KITTI Val cars. Monocular detection has huge generalization gap between training and inference sets. [Key: Best]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"3\">  BackBone</td><td rowspan=\"3\">Method  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.5  </td></tr><tr><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  ResNet-18</td><td>GUP Net\u200b[49]  </td><td>18.86</td><td>13.20</td><td>11.01  </td><td>26.05</td><td>19.37</td><td>16.57  </td><td>54.90</td><td>40.65</td><td>34.98  </td><td>60.54</td><td>46.13</td><td>40.12  </td></tr><tr><td></td><td>DEVIANT  </td><td>20.27</td><td>14.21</td><td>12.56  </td><td>28.09</td><td>20.32</td><td>17.49  </td><td>55.75</td><td>42.41</td><td>36.97  </td><td>60.82</td><td>46.43</td><td>40.59  </td></tr><tr><td>  DLA-34</td><td>GUP Net\u200b[49]  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>28.58</td><td>20.92</td><td>17.83  </td><td>58.95</td><td>43.99</td><td>38.07  </td><td>64.60</td><td>47.76</td><td>42.97  </td></tr><tr><td></td><td>DEVIANT  </td><td>\\mathbf{24.63}</td><td>\\mathbf{16.54}</td><td>\\mathbf{14.52}  </td><td>\\mathbf{32.60}</td><td>\\mathbf{23.04}</td><td>\\mathbf{19.99}  </td><td>\\mathbf{61.00}</td><td>\\mathbf{46.00}</td><td>\\mathbf{40.18}  </td><td>\\mathbf{65.28}</td><td>\\mathbf{49.63}</td><td>\\mathbf{43.50}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 15: Comparison on multiple backbones on KITTI Val cars. [Key: Best]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"2\">  Method</td><td rowspan=\"2\">BackBone  </td><td>Param (\\relbar\\joinrel\\mathrel{\\RHD})  </td><td>Disk Size (\\relbar\\joinrel\\mathrel{\\RHD})  </td><td>Flops (\\relbar\\joinrel\\mathrel{\\RHD})  </td><td>Infer (\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}} IoU{}_{3\\text{D}}\\geq 0.7 (\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}} IoU{}_{3\\text{D}}\\geq 0.5 (\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>(M)  </td><td>(MB)  </td><td>(G)  </td><td>(ms)  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  GUP Net [49]</td><td>DLA-34  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{16}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{235}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{30}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{20}}  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>58.95</td><td>43.99</td><td>38.07  </td></tr><tr><td> GUP Net [49]  </td><td>DLA-102  </td><td>34  </td><td>583  </td><td>70  </td><td>25  </td><td>20.96</td><td>14.64</td><td>12.80  </td><td>57.06</td><td>41.78</td><td>37.26  </td></tr><tr><td> GUP Net [49]  </td><td>DLA-169  </td><td>54  </td><td>814  </td><td>114  </td><td>30  </td><td>21.76</td><td>15.35</td><td>12.72  </td><td>57.60</td><td>43.27</td><td>37.32  </td></tr><tr><td> DEVIANT  </td><td>SES-DLA-34  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{16}}  </td><td>236  </td><td>235  </td><td>40  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{24.63}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{16.54}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{14.52}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{61.00}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{46.00}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{40.18}}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 16: Results with bigger CNNs having similar flops on KITTI Val cars.[Key: Best]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"2\">  Method</td><td rowspan=\"2\">Extra  </td><td colspan=\"3\">Cyc AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">Ped AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  GrooMeD-NMS[36]</td><td>-  </td><td>0.00</td><td>0.00</td><td>0.00  </td><td>3.79</td><td>2.71</td><td>2.61  </td></tr><tr><td> MonoDIS [70]  </td><td>-  </td><td>1.52</td><td>0.73</td><td>0.71  </td><td>3.20</td><td>2.28</td><td>1.71  </td></tr><tr><td> MonoDIS-M[68]  </td><td>-  </td><td>2.70</td><td>1.50</td><td>1.30  </td><td>9.50</td><td>7.10</td><td>5.70  </td></tr><tr><td> GUP Net (Retrained) [49]  </td><td>-  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{4.41}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.17}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{2.03}}  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{9.37}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{6.84}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{5.73}}  </td></tr><tr><td> DEVIANT (Ours)  </td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{4.05}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{2.20}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{2.14}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{9.85}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{7.18}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{5.42}}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 17: Results on KITTI Val cyclists and pedestrians (Cyc/Ped) (IoU{}_{3\\text{D}}\\geq\\!0.5). [Key: Best, Second Best]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)", "[36] Kumar, A., Brazil, G., Liu, X.: GrooMeD-NMS: Grouped mathematically differentiable NMS for monocular 3333D object detection. In: CVPR (2021)", "[70] Simonelli, A., Bul\u00f2, S., Porzi, L., L\u00f3pez-Antequera, M., Kontschieder, P.: Disentangling monocular 3333D object detection. In: ICCV (2019)", "[68] Simonelli, A., Bul\u00f2, S., Porzi, L., Antequera, M., Kontschieder, P.: Disentangling monocular 3333D object detection: From single to multi-class recognition. TPAMI (2020)"]}, {"table": "<table><tr><td rowspan=\"2\">  Set </td><td rowspan=\"2\">Method   </td><td colspan=\"3\">AP{}_{3\\text{D}} IoU{}_{3\\text{D}}\\geq 0.7 (\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}} IoU{}_{3\\text{D}}\\geq 0.5 (\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>  Subset</td><td>GUP Net [49]  </td><td>17.22</td><td>11.43</td><td>9.91  </td><td>47.47</td><td>35.02</td><td>32.63  </td></tr><tr><td> (306)  </td><td>DEVIANT  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{20.17}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{12.49}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{10.93}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{49.81}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{36.93}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{34.32}}  </td></tr><tr><td>  KITTI Val</td><td>GUP Net [49]  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>58.95</td><td>43.99</td><td>38.07  </td></tr><tr><td> (3769)  </td><td>DEVIANT  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{24.63}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{16.54}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{14.52}}  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{61.00}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{46.00}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{40.18}}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 18: Stress Test with rotational and xy-translation ego movement on KITTI Val cars.[Key: Best]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"2\">  Method</td><td>Depth</td><td>Ground  </td><td colspan=\"3\">Back+ Foreground  </td><td colspan=\"3\">Foreground (Cars)  </td></tr><tr><td>at</td><td>Truth  </td><td>0\\!-\\!20</td><td>20\\!-\\!40</td><td>40\\!-\\!\\infty  </td><td>0\\!-\\!20</td><td>20\\!-\\!40</td><td>40\\!-\\!\\infty  </td></tr><tr><td>  GUP Net [49]</td><td>3D Center</td><td>3D Box  </td><td>-</td><td>-</td><td>-  </td><td>0.45</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{1.10}}</td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{1.85}}  </td></tr><tr><td> DEVIANT  </td><td>3D Center</td><td>3D Box  </td><td>-</td><td>-</td><td>-  </td><td>{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\pgfsys@color@rgb@stroke{0}{0}{1}\\pgfsys@color@rgb@fill{0}{0}{1}\\mathbf{0.40}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{1.09}}</td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{1.80}}  </td></tr><tr><td> BTS [41]  </td><td>Pixel</td><td>LiDAR  </td><td>0.48</td><td>1.30</td><td>1.83  </td><td>{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\mathbf{0.30}}</td><td>1.22</td><td>2.16  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 19: Comparison of Depth Estimates of monocular depth estimators and 3D object detectors on KITTI Val cars. Depth from a depth estimator BTS is not good for foreground objects (cars) beyond 20+ m range.[Key: Best, Second Best]", "list_citation_info": ["[41] Lee, J., Han, M., Ko, D., Suh, I.: From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326 (2019)", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"3\">  Method</td><td rowspan=\"3\">Run  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.7  </td><td colspan=\"6\">IoU{}_{3\\text{D}} \\geq 0.5  </td></tr><tr><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{3\\text{D}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td><td colspan=\"3\">AP{}_{\\text{BEV}|R_{40}}[%](\\relbar\\joinrel\\mathrel{\\RHD})  </td></tr><tr><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td><td>Easy</td><td>Mod</td><td>Hard  </td></tr><tr><td>    </td><td>1  </td><td>21.67</td><td>14.75</td><td>12.68  </td><td>28.72</td><td>20.88</td><td>17.79  </td><td>58.27</td><td>43.53</td><td>37.62  </td><td>63.67</td><td>47.37</td><td>42.55  </td></tr><tr><td></td><td>2  </td><td>21.26</td><td>14.94</td><td>12.49  </td><td>28.39</td><td>20.40</td><td>17.43  </td><td>59.20</td><td>43.55</td><td>37.63  </td><td>64.06</td><td>47.46</td><td>42.67  </td></tr><tr><td> GUP Net [49]  </td><td>3  </td><td>20.87</td><td>15.03</td><td>12.61  </td><td>28.66</td><td>20.56</td><td>17.48  </td><td>60.19</td><td>44.08</td><td>39.36  </td><td>65.26</td><td>49.44</td><td>43.17  </td></tr><tr><td></td><td>4  </td><td>21.10</td><td>15.48</td><td>12.88  </td><td>28.58</td><td>20.92</td><td>17.83  </td><td>58.95</td><td>43.99</td><td>38.07  </td><td>64.60</td><td>47.76</td><td>42.97  </td></tr><tr><td></td><td>5  </td><td>22.52</td><td>15.92</td><td>13.31  </td><td>30.77</td><td>22.40</td><td>19.36  </td><td>59.91</td><td>44.00</td><td>39.30  </td><td>64.94</td><td>48.01</td><td>43.08  </td></tr><tr><td></td><td>Avg  </td><td>\\mathbf{21.48}</td><td>\\mathbf{15.22}</td><td>\\mathbf{12.79}  </td><td>\\mathbf{29.02}</td><td>\\mathbf{21.03}</td><td>\\mathbf{17.98}  </td><td>\\mathbf{59.30}</td><td>\\mathbf{43.83}</td><td>\\mathbf{38.40}  </td><td>\\mathbf{64.51}</td><td>\\mathbf{48.01}</td><td>\\mathbf{42.89}  </td></tr><tr><td>    </td><td>1  </td><td>23.19</td><td>15.84</td><td>14.11  </td><td>29.82</td><td>21.93</td><td>19.16  </td><td>60.19</td><td>45.52</td><td>39.86  </td><td>66.32</td><td>49.39</td><td>43.38  </td></tr><tr><td></td><td>2  </td><td>23.33</td><td>16.12</td><td>13.54  </td><td>31.22</td><td>22.64</td><td>19.64  </td><td>61.59</td><td>46.33</td><td>40.35  </td><td>67.49</td><td>50.26</td><td>43.98  </td></tr><tr><td> DEVIANT  </td><td>3  </td><td>24.12</td><td>16.37</td><td>14.48  </td><td>31.58</td><td>22.52</td><td>19.65  </td><td>62.51</td><td>46.47</td><td>40.65  </td><td>67.33</td><td>50.24</td><td>44.16  </td></tr><tr><td></td><td>4  </td><td>24.63</td><td>16.54</td><td>14.52  </td><td>32.60</td><td>23.04</td><td>19.99  </td><td>61.00</td><td>46.00</td><td>40.18  </td><td>65.28</td><td>49.63</td><td>43.50  </td></tr><tr><td></td><td>5  </td><td>25.82</td><td>17.69</td><td>15.07  </td><td>33.63</td><td>23.84</td><td>20.60  </td><td>62.39</td><td>46.46</td><td>40.61  </td><td>67.55</td><td>50.51</td><td>45.80  </td></tr><tr><td></td><td>Avg  </td><td>\\mathbf{24.22}</td><td>\\mathbf{16.51}</td><td>\\mathbf{14.34}  </td><td>\\mathbf{31.77}</td><td>\\mathbf{22.79}</td><td>\\mathbf{19.81}  </td><td>\\mathbf{61.54}</td><td>\\mathbf{46.16}</td><td>\\mathbf{40.33}  </td><td>\\mathbf{66.79}</td><td>\\mathbf{50.01}</td><td>\\mathbf{44.16}  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 20: Five Different Runs on KITTI Val cars.[Key: Average]", "list_citation_info": ["[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)"]}, {"table": "<table><tr><td>  Method</td><td>Venue  </td><td>Multi-Dataset</td><td>Cross-Dataset</td><td>Multi-Backbone  </td></tr><tr><td>  GrooMeD-NMS[36]</td><td>CVPR21  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> MonoFlex [100]  </td><td>CVPR21  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> CaDDN[62]  </td><td>CVPR21  </td><td>\u2713</td><td>-</td><td>-  </td></tr><tr><td> MonoRCNN [67]  </td><td>ICCV21  </td><td>-</td><td>\u2713</td><td>-  </td></tr><tr><td> GUP Net [49]  </td><td>ICCV21  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> DD3D[57]  </td><td>ICCV21  </td><td>\u2713</td><td>-</td><td>\u2713  </td></tr><tr><td> PCT [80]  </td><td>NeurIPS21  </td><td>\u2713</td><td>-</td><td>\u2713  </td></tr><tr><td> MonoDistill [14]  </td><td>ICLR22  </td><td>-</td><td>-</td><td>-  </td></tr><tr><td> MonoDIS-M[68]  </td><td>TPAMI20  </td><td>\u2713</td><td>-</td><td>-  </td></tr><tr><td> MonoEF [103]  </td><td>TPAMI21  </td><td>\u2713</td><td>-</td><td>-  </td></tr><tr><td> \\mathbf{DEVIANT}  </td><td>-  </td><td>\u2713</td><td>\u2713</td><td>\u2713  </td></tr><tr><td>  </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 21: Experiments Comparison.", "list_citation_info": ["[103] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: MonoEF: Extrinsic parameter free monocular 3333D object detection. TPAMI (2021)", "[36] Kumar, A., Brazil, G., Liu, X.: GrooMeD-NMS: Grouped mathematically differentiable NMS for monocular 3333D object detection. In: CVPR (2021)", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)", "[67] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposition for monocular 3333D object detection. In: ICCV (2021)", "[68] Simonelli, A., Bul\u00f2, S., Porzi, L., Antequera, M., Kontschieder, P.: Disentangling monocular 3333D object detection: From single to multi-class recognition. TPAMI (2020)", "[100] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3333D object detection. In: CVPR (2021)", "[14] Chong, Z., Ma, X., Zhang, H., Yue, Y., Li, H., Wang, Z., Ouyang, W.: MonoDistill: Learning spatial features for monocular 3333D object detection. In: ICLR (2022)", "[62] Reading, C., Harakeh, A., Chae, J., Waslander, S.: Categorical depth distribution network for monocular 3333D object detection. In: CVPR (2021)", "[80] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3333D object detection. In: NeurIPS (2021)", "[57] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is Pseudo-LiDAR needed for monocular 3333D object detection? In: ICCV (2021)"]}], "citation_info_to_title": {"[91] Worrall, D., Garbin, S., Turmukhambetov, D., Brostow, G.: Harmonic networks: Deep translation and rotation equivariance. In: CVPR (2017)": "Harmonic networks: Deep translation and rotation equivariance", "[105] Zou, Z., Ye, X., Du, L., Cheng, X., Tan, X., Zhang, L., Feng, J., Xue, X., Ding, E.: The devil is in the task: Exploiting reciprocal appearance-localization features for monocular 3333D object detection. In: ICCV (2021)": "The devil is in the task: Exploiting reciprocal appearance-localization features for monocular 3D object detection", "[26] Ganea, O.E., B\u00e9cigneul, G., Hofmann, T.: Hyperbolic neural networks. In: NeurIPS (2017)": "Hyperbolic Neural Networks", "[79] Wang, L., Du, L., Ye, X., Fu, Y., Guo, G., Xue, X., Feng, J., Zhang, L.: Depth-conditioned dynamic message propagation for monocular 3333D object detection. In: CVPR (2021)": "Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection", "[41] Lee, J., Han, M., Ko, D., Suh, I.: From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326 (2019)": "From big to small: Multi-scale local planar guidance for monocular depth estimation", "[36] Kumar, A., Brazil, G., Liu, X.: GrooMeD-NMS: Grouped mathematically differentiable NMS for monocular 3333D object detection. In: CVPR (2021)": "GrooMeD-NMS: Grouped mathematically differentiable NMS for monocular 3D object detection", "[70] Simonelli, A., Bul\u00f2, S., Porzi, L., L\u00f3pez-Antequera, M., Kontschieder, P.: Disentangling monocular 3333D object detection. In: ICCV (2019)": "Disentangling Monocular 3D Object Detection", "[31] Henriques, J., Vedaldi, A.: Warped convolutions: Efficient invariance to spatial transformations. In: ICML (2017)": "Warped convolutions: Efficient invariance to spatial transformations", "[29] Ghosh, R., Gupta, A.: Scale steerable filters for locally scale-invariant convolutional neural networks. In: ICML Workshops (2019)": "Scale Steerable Filters for Locally Scale-Invariant Convolutional Neural Networks", "[49] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3333D object detection. In: ICCV (2021)": "Geometry Uncertainty Projection Network for Monocular 3D Object Detection", "[68] Simonelli, A., Bul\u00f2, S., Porzi, L., Antequera, M., Kontschieder, P.: Disentangling monocular 3333D object detection: From single to multi-class recognition. TPAMI (2020)": "Disentangling Monocular 3D Object Detection: From Single to Multi-Class Recognition", "[92] Worrall, D., Welling, M.: Deep scale-spaces: Equivariance over scale. In: NeurIPS (2019)": "Deep Scale-Spaces: Equivariance over Scale", "[21] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: ICLR (2021)": "An image is worth 16x16 words: Transformers for image recognition at scale", "[53] Ma, X., Zhang, Y., Xu, D., Zhou, D., Yi, S., Li, H., Ouyang, W.: Delving into localization errors for monocular 3333D object detection. In: CVPR (2021)": "Delving into localization errors for monocular 3D object detection", "[103] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: MonoEF: Extrinsic parameter free monocular 3333D object detection. TPAMI (2021)": "MonoEF: Extrinsic parameter free monocular 3D object detection", "[97] Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In: ICLR (2015)": "Multi-scale context aggregation by dilated convolutions", "[62] Reading, C., Harakeh, A., Chae, J., Waslander, S.: Categorical depth distribution network for monocular 3333D object detection. In: CVPR (2021)": "Categorical Depth Distribution Network for Monocular 3D Object Detection", "[14] Chong, Z., Ma, X., Zhang, H., Yue, Y., Li, H., Wang, Z., Ouyang, W.: MonoDistill: Learning spatial features for monocular 3333D object detection. In: ICLR (2022)": "MonoDistill: Learning Spatial Features for Monocular 3D Object Detection", "[4] Brazil, G., Liu, X.: M3333D-RPN: Monocular 3333D region proposal network for object detection. In: ICCV (2019)": "M3333D-RPN: Monocular 3333D region proposal network for object detection", "[50] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking Pseudo-LiDAR representation. In: ECCV (2020)": "Rethinking Pseudo-LiDAR representation", "[72] Sosnovik, I., Moskalev, A., Smeulders, A.: DISCO: accurate discrete scale convolutions. In: BMVC (2021)": "DISCO: Accurate Discrete Scale Convolutions", "[85] Wang, Y., Guizilini, V., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: DETR3D: 3333D object detection from multi-view images via 3333D-to-2222D queries. In: CoRL (2021)": "DETR3D: 3333D object detection from multi-view images via 3333D-to-2222D queries", "[69] Simonelli, A., Bul\u00f2, S., Porzi, L., Kontschieder, P., Ricci, E.: Are we missing confidence in Pseudo-LiDAR methods for monocular 3333D object detection? In: ICCV (2021)": "Are we missing confidence in Pseudo-LiDAR methods for monocular 3D object detection?", "[80] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3333D object detection. In: NeurIPS (2021)": "Progressive coordinate transforms for monocular 3D object detection", "[48] Liu, Z., Zhou, D., Lu, F., Fang, J., Zhang, L.: AutoShape: Real-time shape-aware monocular 3333D object detection. In: ICCV (2021)": "AutoShape: Real-time shape-aware monocular 3D object detection", "[96] Yeh, R., Hu, Y.T., Schwing, A.: Chirality nets for human pose regression. NeurIPS (2019)": "Chirality Nets for Human Pose Regression", "[15] Cohen, T., Geiger, M., K\u00f6hler, J., Welling, M.: Spherical CNNs. In: ICLR (2018)": "Spherical CNNs", "[100] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3333D object detection. In: CVPR (2021)": "Flexible Monocular 3D Object Detection", "[67] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposition for monocular 3333D object detection. In: ICCV (2021)": "Geometry-based distance decomposition for monocular 3D object detection", "[40] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. Proceedings of the IEEE (1998)": "Gradient-based learning applied to document recognition", "[47] Liu, Y., Yixuan, Y., Liu, M.: Ground-aware monocular 3333D object detection for autonomous driving. Robotics and Automation Letters (2021)": "Ground-aware Monocular 3D Object Detection for Autonomous Driving", "[57] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is Pseudo-LiDAR needed for monocular 3333D object detection? In: ICCV (2021)": "Is Pseudo-LiDAR needed for monocular 3D object detection?", "[20] Ding, M., Huo, Y., Yi, H., Wang, Z., Shi, J., Lu, Z., Luo, P.: Learning depth-guided convolutions for monocular 3333D object detection. In: CVPR Workshops (2020)": "Learning Depth-Guided Convolutions for Monocular 3D Object Detection", "[106] Zwicke, P., Kiss, I.: A new implementation of the mellin transform and its application to radar classification of ships. TPAMI (1983)": "A new implementation of the mellin transform and its application to radar classification of ships", "[5] Brazil, G., Pons-Moll, G., Liu, X., Schiele, B.: Kinematic 3333D object detection in monocular video. In: ECCV (2020)": "Kinematic 3D object detection in monocular video"}, "source_title_to_arxiv_id": {"Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection": "2103.16470", "Disentangling Monocular 3D Object Detection": "1905.12365", "Categorical Depth Distribution Network for Monocular 3D Object Detection": "2103.01100", "Progressive coordinate transforms for monocular 3D object detection": "2108.05793"}}