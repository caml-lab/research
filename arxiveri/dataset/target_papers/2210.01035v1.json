{"title": "Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning", "abstract": "Vision transformers have recently achieved competitive results across various\nvision tasks but still suffer from heavy computation costs when processing a\nlarge number of tokens. Many advanced approaches have been developed to reduce\nthe total number of tokens in large-scale vision transformers, especially for\nimage classification tasks. Typically, they select a small group of essential\ntokens according to their relevance with the class token, then fine-tune the\nweights of the vision transformer. Such fine-tuning is less practical for dense\nprediction due to the much heavier computation and GPU memory cost than image\nclassification. In this paper, we focus on a more challenging problem, i.e.,\naccelerating large-scale vision transformers for dense prediction without any\nadditional re-training or fine-tuning. In response to the fact that\nhigh-resolution representations are necessary for dense prediction, we present\ntwo non-parametric operators, a token clustering layer to decrease the number\nof tokens and a token reconstruction layer to increase the number of tokens.\nThe following steps are performed to achieve this: (i) we use the token\nclustering layer to cluster the neighboring tokens together, resulting in\nlow-resolution representations that maintain the spatial structures; (ii) we\napply the following transformer layers only to these low-resolution\nrepresentations or clustered tokens; and (iii) we use the token reconstruction\nlayer to re-create the high-resolution representations from the refined\nlow-resolution representations. The results obtained by our method are\npromising on five dense prediction tasks, including object detection, semantic\nsegmentation, panoptic segmentation, instance segmentation, and depth\nestimation.", "authors": ["Weicong Liang", "Yuhui Yuan", "Henghui Ding", "Xiao Luo", "Weihong Lin", "Ding Jia", "Zheng Zhang", "Chao Zhang", "Han Hu"], "published_date": "2022_10_03", "pdf_url": "http://arxiv.org/pdf/2210.01035v1", "list_table_and_caption": [{"table": "<table><tr><td>Dataset</td><td>Method</td><td>GFLOPs</td><td>FPS</td><td>\\delta&gt;1.25</td><td>\\delta&gt;1.25^{2}</td><td>\\delta&gt;1.25^{3}</td><td>AbsRel</td><td>SqRel</td><td>RMSE</td><td>RMSElog</td><td>SILog</td><td>log10</td></tr><tr><td rowspan=\"2\">KITTI</td><td>DPT</td><td>810</td><td>11.38</td><td>0.959</td><td>0.995</td><td>0.999</td><td>0.062</td><td>0.222</td><td>2.573</td><td>0.092</td><td>8.282</td><td>0.027</td></tr><tr><td>DPT+Ours</td><td>627</td><td>14.75</td><td>0.958</td><td>0.995</td><td>0.999</td><td>0.062</td><td>0.226</td><td>2.597</td><td>0.093</td><td>8.341</td><td>0.027</td></tr><tr><td rowspan=\"2\">NYUv2</td><td>DPT</td><td>560</td><td>17.58</td><td>0.904</td><td>0.988</td><td>0.998</td><td>0.110</td><td>0.054</td><td>0.357</td><td>0.129</td><td>9.522</td><td>0.045</td></tr><tr><td>DPT+Ours</td><td>404</td><td>24.03</td><td>0.900</td><td>0.987</td><td>0.998</td><td>0.113</td><td>0.056</td><td>0.363</td><td>0.132</td><td>9.532</td><td>0.046</td></tr></table>", "caption": "Table 7: Depth estimation results based on DPT [54] with ResNet-50+ViT-B/16.", "list_citation_info": ["[54] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, pages 12179\u201312188, 2021."]}, {"table": "<p>!MethodBackboneDataset\\alpha\\alpha+\\beta\\gamma\\mathsf{L}\\frac{\\mathsf{H}}{\\mathsf{P}}\\times\\frac{\\mathsf{W}}{\\mathsf{P}}\\mathsf{h}\\times\\mathsf{w}\\lambda\\kappa\\tau\\mathsf{k}Segmenter [63]ViT-L/16ADE20K102402440\\times 4028\\times 285\\times 555020Cityscapes1248\\times 4832\\times 32PASCAL-Context1430\\times 3015\\times 15DPT [54]R50+ViT-B/16KITTI21201276\\times 2228\\times 285\\times 55520NYUv2340\\times 3016\\times 167\\times 751050SWAG [61]ViT-H/14ImageNet-1K83203237\\times 3725\\times 257\\times 75120ViT-L/1682402432\\times 3222\\times 229\\times 95120</p><p>!MethodBackboneDataset\\alpha\\alpha+\\beta\\gamma\\mathsf{L}\\mathsf{K}\\times\\mathsf{K}\\mathsf{k}\\times\\mathsf{k}\\lambda\\kappa\\tau\\mathsf{k}Mask2Former [11]Swin-LCOCO (panoptic seg.)102222412\\times 128\\times 87\\times 752010ADE20K (semantic seg.)85\\times 5510010COCO (instance seg.)1211\\times 11510060SwinV2-L + HTC++ [46]SwinV2-LCOCO (object det.)122222432\\times 3223\\times 235\\times 553320</p><p>!MethodBackbonewith HeadDatasetInput resolutionSegmenter [63]ViT-L/16\u2713ADE20K640\\times 640Cityscapes768\\times 768PASCAL-Context480\\times 480DPT [54]R50+ViT-B/16\u2713KITTI1216\\times 352NYUv2640\\times 480SWAG [61]ViT-H/14\u2713ImageNet-1K518\\times 518ViT-L/16512\\times 512Mask2Former [11]Swin-L\u2717COCO (panoptic seg.)1152\\times 1152ADE20K (semantic seg.)1152\\times 1152COCO (instance seg.)1152\\times 1152SwinV2-L + HTC++ [46]SwinV2-L\u2717COCO (object det.)1024\\times 1024</p><table><tr><td>Dataset</td><td>Method</td><td>Parametric</td><td>Fine-Tuning</td><td>GFLOPs</td><td>mIoU</td></tr><tr><td rowspan=\"6\">ADE20K</td><td>Dynamic ViT (\\rho=0.7)</td><td>\u2713</td><td>\u2713</td><td>455.6</td><td>45.62</td></tr><tr><td>Dynamic ViT (\\rho=0.8)</td><td>\u2713</td><td>\u2713</td><td>513.3</td><td>47.89</td></tr><tr><td>Dynamic ViT (\\rho=0.9)</td><td>\u2713</td><td>\u2713</td><td>583.0</td><td>50.42</td></tr><tr><td>Ours (\\mathrm{h}\\times\\mathrm{w}=16\\times 16)</td><td>\u2717</td><td>\u2717</td><td>315.1</td><td>48.21</td></tr><tr><td>Ours (\\mathrm{h}\\times\\mathrm{w}=20\\times 20)</td><td>\u2717</td><td>\u2717</td><td>347.2</td><td>50.17</td></tr><tr><td>Ours (\\mathrm{h}\\times\\mathrm{w}=24\\times 24)</td><td>\u2717</td><td>\u2717</td><td>388.2</td><td>51.32</td></tr></table><p>!\\pbox22mmCluster methodquerykey-valueFFN#clustering layersClustered Attention [68]\u2713\u2717\u2717# MHSA layersACT [89]\u2713\u2717\u2717# MHSA layersSMRF [15]\u2713\u2713\u2717# MHSA layersOurs\u2713\u2713\u27131</p><p>!\\pbox22mmCluster methodFPSGFLOPsmIoUSegmenter+ViT-B/166.2659.051.82Segmenter+ViT-B/16+Ours(\\mathrm{h}\\times\\mathrm{w}=24\\times 24)9.1388.251.32Segmenter+ViT-B/16+Ours(\\mathrm{h}\\times\\mathrm{w}=28\\times 28)8.8438.951.56Segmenter+ViT-B/16+ACT(#query-hashes=16)5.8578.748.12Segmenter+ViT-B/16+ACT(#query-hashes=24)5.3614.751.38Segmenter+ViT-B/16+ACT(#query-hashes=32)5.0638.251.64</p>", "caption": "Table 8: Illustrating the hyper-parameter settings used for Segmenter, DPT, and SWAG.", "list_citation_info": ["[11] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.", "[68] Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. Fast transformers with clustered attention. NeurIPS, 33, 2020.", "[54] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, pages 12179\u201312188, 2021.", "[63] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. arXiv preprint arXiv:2105.05633, 2021.", "[46] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883, 2021.", "[61] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll\u00e1r, and Laurens van der Maaten. Revisiting weakly supervised pre-training of visual perception models, 2022.", "[15] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. NeurIPS, 33:6476\u20136489, 2020.", "[89] Minghang Zheng, Peng Gao, Renrui Zhang, Kunchang Li, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object detection with adaptive clustering transformer. arXiv preprint arXiv:2011.09315, 2020."]}], "citation_info_to_title": {"[63] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. arXiv preprint arXiv:2105.05633, 2021.": "Segmenter: Transformer for semantic segmentation", "[15] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering. NeurIPS, 33:6476\u20136489, 2020.": "Smyrf-efficient attention using asymmetric clustering", "[61] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll\u00e1r, and Laurens van der Maaten. Revisiting weakly supervised pre-training of visual perception models, 2022.": "Revisiting weakly supervised pre-training of visual perception models", "[46] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. arXiv preprint arXiv:2111.09883, 2021.": "Swin Transformer V2: Scaling up Capacity and Resolution", "[68] Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. Fast transformers with clustered attention. NeurIPS, 33, 2020.": "Fast transformers with clustered attention", "[89] Minghang Zheng, Peng Gao, Renrui Zhang, Kunchang Li, Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end object detection with adaptive clustering transformer. arXiv preprint arXiv:2011.09315, 2020.": "End-to-end object detection with adaptive clustering transformer", "[54] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, pages 12179\u201312188, 2021.": "Vision Transformers for Dense Prediction", "[11] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.": "Masked-attention mask transformer for universal image segmentation"}, "source_title_to_arxiv_id": {"Swin Transformer V2: Scaling up Capacity and Resolution": "2111.09883", "End-to-end object detection with adaptive clustering transformer": "2011.09315", "Vision Transformers for Dense Prediction": "2102.12122"}}