{"title": "HIER: Metric Learning Beyond Class Labels via Hierarchical Regularization", "abstract": "Supervision for metric learning has long been given in the form of\nequivalence between human-labeled classes. Although this type of supervision\nhas been a basis of metric learning for decades, we argue that it hinders\nfurther advances in the field. In this regard, we propose a new regularization\nmethod, dubbed HIER, to discover the latent semantic hierarchy of training\ndata, and to deploy the hierarchy to provide richer and more fine-grained\nsupervision than inter-class separability induced by common metric learning\nlosses.HIER achieves this goal with no annotation for the semantic hierarchy\nbut by learning hierarchical proxies in hyperbolic spaces. The hierarchical\nproxies are learnable parameters, and each of them is trained to serve as an\nancestor of a group of data or other proxies to approximate the semantic\nhierarchy among them. HIER deals with the proxies along with data in hyperbolic\nspace since the geometric properties of the space are well-suited to represent\ntheir hierarchical structure. The efficacy of HIER is evaluated on four\nstandard benchmarks, where it consistently improved the performance of\nconventional methods when integrated with them, and consequently achieved the\nbest records, surpassing even the existing hyperbolic metric learning\ntechnique, in almost all settings.", "authors": ["Sungyeon Kim", "Boseung Jeong", "Suha Kwak"], "published_date": "2022_12_29", "pdf_url": "http://arxiv.org/pdf/2212.14258v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Arch.</td><td colspan=\"3\">CUB</td><td colspan=\"3\">Cars</td><td colspan=\"3\">SOP</td><td colspan=\"3\">In-Shop</td></tr><tr><td><p>R@1</p></td><td><p>R@2</p></td><td><p>R@4</p></td><td><p>R@1</p></td><td><p>R@2</p></td><td><p>R@4</p></td><td><p>R@1</p></td><td><p>R@10</p></td><td><p>R@100</p></td><td><p>R@1</p></td><td><p>R@10</p></td><td><p>R@20</p></td></tr><tr><td colspan=\"14\">Backbone architecture: CNN</td></tr><tr><td><p>NSoftmax [57]</p></td><td><p>R{}^{128}</p></td><td><p>56.5</p></td><td><p>69.6</p></td><td><p>79.9</p></td><td><p>81.6</p></td><td><p>88.7</p></td><td><p>93.4</p></td><td><p>75.2</p></td><td><p>88.7</p></td><td><p>95.2</p></td><td><p>86.6</p></td><td><p>96.8</p></td><td><p>97.8</p></td></tr><tr><td><p>MIC [34]</p></td><td><p>R{}^{128}</p></td><td><p>66.1</p></td><td><p>76.8</p></td><td><p>85.6</p></td><td><p>82.6</p></td><td><p>89.1</p></td><td><p>93.2</p></td><td><p>77.2</p></td><td><p>89.4</p></td><td><p>94.6</p></td><td><p>88.2</p></td><td><p>97.0</p></td><td><p>-</p></td></tr><tr><td><p>XBM [52]</p></td><td><p>R{}^{128}</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>80.6</p></td><td><p>91.6</p></td><td><p>96.2</p></td><td><p>91.3</p></td><td><p>97.8</p></td><td><p>98.4</p></td></tr><tr><td><p>XBM [52]</p></td><td><p>B{}^{512}</p></td><td><p>65.8</p></td><td><p>75.9</p></td><td><p>84.0</p></td><td><p>82.0</p></td><td><p>88.7</p></td><td><p>93.1</p></td><td><p>79.5</p></td><td><p>90.8</p></td><td><p>96.1</p></td><td><p>89.9</p></td><td><p>97.6</p></td><td><p>98.4</p></td></tr><tr><td><p>HTL [15]</p></td><td><p>B{}^{512}</p></td><td><p>57.1</p></td><td><p>68.8</p></td><td><p>78.7</p></td><td><p>81.4</p></td><td><p>88.0</p></td><td><p>92.7</p></td><td><p>74.8</p></td><td><p>88.3</p></td><td><p>94.8</p></td><td><p>80.9</p></td><td><p>94.3</p></td><td><p>95.8</p></td></tr><tr><td><p>MS [50]</p></td><td><p>B{}^{512}</p></td><td><p>65.7</p></td><td><p>77.0</p></td><td><p>86.3</p></td><td><p>84.1</p></td><td><p>90.4</p></td><td><p>94.0</p></td><td><p>78.2</p></td><td><p>90.5</p></td><td><p>96.0</p></td><td><p>89.7</p></td><td><p>97.9</p></td><td><p>98.5</p></td></tr><tr><td><p>SoftTriple [32]</p></td><td><p>B{}^{512}</p></td><td><p>65.4</p></td><td><p>76.4</p></td><td><p>84.5</p></td><td><p>84.5</p></td><td><p>90.7</p></td><td><p>94.5</p></td><td><p>78.6</p></td><td><p>86.6</p></td><td><p>91.8</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>PA [20]</p></td><td><p>B{}^{512}</p></td><td><p>69.1</p></td><td><p>78.9</p></td><td><p>86.1</p></td><td><p>86.4</p></td><td><p>91.9</p></td><td><p>95.0</p></td><td><p>79.2</p></td><td><p>90.7</p></td><td><p>96.2</p></td><td><p>91.9</p></td><td><p>98.1</p></td><td><p>98.7</p></td></tr><tr><td><p>NSoftmax [57]</p></td><td><p>R{}^{512}</p></td><td><p>61.3</p></td><td><p>73.9</p></td><td><p>83.5</p></td><td><p>84.2</p></td><td><p>90.4</p></td><td><p>94.4</p></td><td><p>78.2</p></td><td><p>90.6</p></td><td><p>96.2</p></td><td><p>86.6</p></td><td><p>97.5</p></td><td><p>98.4</p></td></tr><tr><td><p>ProxyNCA++ [44]</p></td><td><p>R{}^{512}</p></td><td><p>69.0</p></td><td><p>79.8</p></td><td><p>87.3</p></td><td><p>86.5</p></td><td><p>92.5</p></td><td><p>95.7</p></td><td><p>80.7</p></td><td><p>92.0</p></td><td><p>96.7</p></td><td><p>90.4</p></td><td><p>98.1</p></td><td><p>98.8</p></td></tr><tr><td><p>ResNet-50 [17]</p></td><td><p>R{}^{2048}</p></td><td><p>41.2</p></td><td><p>53.8</p></td><td><p>66.3</p></td><td><p>41.4</p></td><td><p>53.6</p></td><td><p>66.1</p></td><td><p>50.6</p></td><td><p>66.7</p></td><td><p>80.7</p></td><td><p>25.8</p></td><td><p>49.1</p></td><td><p>56.4</p></td></tr><tr><td><p>NSoftmax [57]</p></td><td><p>R{}^{2048}</p></td><td><p>65.3</p></td><td><p>76.7</p></td><td><p>85.4</p></td><td><p>89.3</p></td><td><p>94.1</p></td><td><p>96.4</p></td><td><p>79.5</p></td><td><p>91.5</p></td><td><p>96.7</p></td><td><p>89.4</p></td><td><p>97.8</p></td><td><p>98.7</p></td></tr><tr><td><p>ProxyNCA++ [44]</p></td><td><p>R{}^{2048}</p></td><td><p>72.2</p></td><td><p>82.0</p></td><td><p>89.2</p></td><td><p>90.1</p></td><td><p>94.5</p></td><td><p>97.0</p></td><td><p>81.4</p></td><td><p>92.4</p></td><td><p>96.9</p></td><td><p>90.9</p></td><td><p>98.2</p></td><td><p>98.9</p></td></tr><tr><td colspan=\"14\">Backbone architecture: ViT</td></tr><tr><td><p>IRT{}_{\\text{R}} [12]</p></td><td><p>De{}^{128}</p></td><td><p>72.6</p></td><td><p>81.9</p></td><td><p>88.7</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>83.4</p></td><td><p>93.0</p></td><td><p>97.0</p></td><td><p>91.1</p></td><td><p>98.1</p></td><td><p>98.6</p></td></tr><tr><td><p>Hyp [13]</p></td><td><p>De{}^{128}</p></td><td><p>74.7</p></td><td><p>84.5</p></td><td><p>90.1</p></td><td><p>82.1</p></td><td><p>89.1</p></td><td><p>93.4</p></td><td><p>83.0</p></td><td><p>93.4</p></td><td><p>97.5</p></td><td><p>90.9</p></td><td><p>97.9</p></td><td><p>98.6</p></td></tr><tr><td>HIER (ours)</td><td>De{}^{128}</td><td>75.2</td><td>84.2</td><td>90.0</td><td>85.1</td><td>91.1</td><td>95.1</td><td>82.5</td><td>92.7</td><td>97.0</td><td>91.0</td><td>98.0</td><td>98.6</td></tr><tr><td><p>Hyp [13]</p></td><td><p>DN{}^{128}</p></td><td><p>78.3</p></td><td><p>86.0</p></td><td><p>91.2</p></td><td><p>86.0</p></td><td><p>91.9</p></td><td><p>95.2</p></td><td><p>84.6</p></td><td><p>94.1</p></td><td><p>97.7</p></td><td><p>92.6</p></td><td><p>98.4</p></td><td><p>99.0</p></td></tr><tr><td>HIER (ours)</td><td>DN{}^{128}</td><td>78.5</td><td>86.7</td><td>91.5</td><td>88.4</td><td>93.3</td><td>95.9</td><td>84.9</td><td>94.2</td><td>97.5</td><td>92.6</td><td>98.4</td><td>98.9</td></tr><tr><td><p>Hyp{}^{\\dagger} [13]</p></td><td>V{}^{128}</td><td><p>84.0</p></td><td><p>90.2</p></td><td><p>94.2</p></td><td><p>82.7</p></td><td><p>89.7</p></td><td><p>93.9</p></td><td><p>85.5</p></td><td><p>94.9</p></td><td><p>98.1</p></td><td><p>92.7</p></td><td><p>98.4</p></td><td><p>98.9</p></td></tr><tr><td>HIER{}^{\\dagger} (ours)</td><td>V{}^{128}</td><td>84.2</td><td>90.1</td><td>93.7</td><td>86.4</td><td>91.9</td><td>95.1</td><td>85.6</td><td>94.6</td><td>97.8</td><td>92.7</td><td>98.4</td><td>98.9</td></tr><tr><td><p>IRT{}_{\\text{R}} [12]</p></td><td>De{}^{384}</td><td><p>76.6</p></td><td><p>85.0</p></td><td><p>91.1</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>84.2</p></td><td><p>93.7</p></td><td><p>97.3</p></td><td><p>91.9</p></td><td><p>98.1</p></td><td><p>98.9</p></td></tr><tr><td><p>DeiT-S [45]</p></td><td>De{}^{384}</td><td><p>70.6</p></td><td><p>81.3</p></td><td><p>88.7</p></td><td><p>52.8</p></td><td><p>65.1</p></td><td><p>76.2</p></td><td><p>58.3</p></td><td><p>73.9</p></td><td><p>85.9</p></td><td><p>37.9</p></td><td><p>64.7</p></td><td><p>72.1</p></td></tr><tr><td><p>Hyp [13]</p></td><td>De{}^{384}</td><td><p>77.8</p></td><td><p>86.6</p></td><td><p>91.9</p></td><td><p>86.4</p></td><td><p>92.2</p></td><td><p>95.5</p></td><td><p>83.3</p></td><td><p>93.5</p></td><td><p>97.4</p></td><td><p>90.5</p></td><td><p>97.8</p></td><td><p>98.5</p></td></tr><tr><td>HIER (ours)</td><td>De{}^{384}</td><td>78.7</td><td>86.8</td><td>92.0</td><td>88.9</td><td>93.9</td><td>96.6</td><td>83.0</td><td>93.1</td><td>97.2</td><td>90.6</td><td>98.1</td><td>98.6</td></tr><tr><td><p>DINO [4]</p></td><td>DN{}^{384}</td><td><p>70.8</p></td><td><p>81.1</p></td><td><p>88.8</p></td><td><p>42.9</p></td><td><p>53.9</p></td><td><p>64.2</p></td><td><p>63.4</p></td><td><p>78.1</p></td><td><p>88.3</p></td><td><p>46.1</p></td><td><p>71.1</p></td><td><p>77.5</p></td></tr><tr><td><p>Hyp [13]</p></td><td>DN{}^{384}</td><td><p>80.9</p></td><td><p>87.6</p></td><td><p>92.4</p></td><td><p>89.2</p></td><td><p>94.1</p></td><td><p>96.7</p></td><td><p>85.1</p></td><td><p>94.4</p></td><td><p>97.8</p></td><td><p>92.4</p></td><td><p>98.4</p></td><td><p>98.9</p></td></tr><tr><td>HIER (ours)</td><td>DN{}^{384}</td><td>81.1</td><td>88.2</td><td>93.3</td><td>91.3</td><td>95.2</td><td>97.1</td><td>85.7</td><td>94.6</td><td>97.8</td><td>92.5</td><td>98.6</td><td>99.0</td></tr><tr><td><p>ViT-S{}^{\\dagger} [11]</p></td><td>V{}^{384}</td><td><p>83.1</p></td><td><p>90.4</p></td><td><p>94.4</p></td><td><p>47.8</p></td><td><p>60.2</p></td><td><p>72.2</p></td><td><p>62.1</p></td><td><p>77.7</p></td><td><p>89.0</p></td><td><p>43.2</p></td><td><p>70.2</p></td><td><p>76.7</p></td></tr><tr><td><p>Hyp{}^{\\dagger} [13]</p></td><td>V{}^{384}</td><td><p>85.6</p></td><td><p>91.4</p></td><td><p>94.8</p></td><td><p>86.5</p></td><td><p>92.1</p></td><td><p>95.3</p></td><td><p>85.9</p></td><td><p>94.9</p></td><td><p>98.1</p></td><td><p>92.5</p></td><td><p>98.3</p></td><td><p>98.8</p></td></tr><tr><td>HIER{}^{\\dagger} (ours)</td><td>V{}^{384}</td><td>85.7</td><td>91.3</td><td>94.4</td><td>88.3</td><td>93.2</td><td>96.1</td><td>86.1</td><td>95.0</td><td>98.0</td><td>92.8</td><td>98.4</td><td>99.0</td></tr></tbody></table>", "caption": "Table 1: Performance of metric learning methods on the four datasets. Their network architectures are denoted by abbreviations, R\u2013ResNet50 [17], B\u2013Inception with BatchNorm [18], De\u2013DeiT [45], DN\u2013DINO [4] and V\u2013ViT [11]. Superscripts denote their embedding dimensions. \\dagger denotes model pretrained on ImageNet-21k [10].", "list_citation_info": ["[32] Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric learning without triplet sampling. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.", "[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proc. IEEE International Conference on Computer Vision (ICCV), 2021.", "[18] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. International Conference on Machine Learning (ICML), 2015.", "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.", "[15] Weifeng Ge, Weilin Huang, Dengke Dong, and Matthew R. Scott. Deep metric learning with hierarchical triplet loss. In Proc. European Conference on Computer Vision (ECCV), 2018.", "[20] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[50] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss with general pair weighting for deep metric learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: a large-scale hierarchical image database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.", "[52] Xun Wang, Haozhi Zhang, Weilin Huang, and Matthew R Scott. Cross-batch memory for embedding learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6388\u20136397, 2020.", "[57] Andrew Zhai and Hao-Yu Wu. Classification is a strong baseline for deep metric learning. arXiv preprint arXiv:1811.12649, 2018.", "[44] Eu Wern Teh, Terrance DeVries, and Graham W Taylor. Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. In European Conference on Computer Vision (ECCV). Springer, 2020.", "[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proc. International Conference on Machine Learning (ICML), 2021.", "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proc. International Conference on Learning Representations (ICLR), 2021.", "[34] Karsten Roth, Biagio Brattoli, and Bjorn Ommer. Mic: Mining interclass characteristics for improved metric learning. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.", "[12] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herv\u00e9 J\u00e9gou. Training vision transformers for image retrieval. arXiv preprint arXiv:2102.05644, 2021.", "[13] Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, and Ivan Oseledets. Hyperbolic vision transformers: Combining improvements in metric learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022."]}, {"table": "<table><thead><tr><th>Methods</th><th>CUB</th><th>Cars</th><th>SOP</th><th>In-Shop</th></tr></thead><tbody><tr><th>PA</th><td><p>74.7</p></td><td><p>84.3</p></td><td><p>82.3</p></td><td><p>90.4</p></td></tr><tr><th>PA + HIER{}_{sph}</th><td><p>75.1</p></td><td><p>84.8</p></td><td><p>82.4</p></td><td><p>90.6</p></td></tr><tr><th>PA + HIER (ours)</th><td><p>75.2</p></td><td><p>85.1</p></td><td><p>82.5</p></td><td><p>91.0</p></td></tr><tr><th>MS</th><td><p>75.4</p></td><td><p>83.5</p></td><td><p>80.0</p></td><td><p>88.1</p></td></tr><tr><th>MS + HIER{}_{sph}</th><td><p>75.6</p></td><td><p>83.6</p></td><td><p>80.0</p></td><td><p>88.0</p></td></tr><tr><th>MS + HIER (ours)</th><td><p>75.8</p></td><td><p>84.3</p></td><td><p>80.0</p></td><td><p>88.2</p></td></tr></tbody></table>", "caption": "Table 2: Accuracy in Recall@1 of ours with two metric learning losses [20, 50], and their variants on the four datasets. The network architecture is DeiT [45] with 128 embedding dimensions. HIER{}_{sph} denotes HIER over spherical space.", "list_citation_info": ["[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proc. International Conference on Machine Learning (ICML), 2021.", "[20] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Methods</th><th colspan=\"3\">CUB</th><th colspan=\"3\">Cars</th><th colspan=\"3\">SOP</th><th colspan=\"3\">In-Shop</th></tr><tr><th><p>#Proxies</p></th><th><p>R@1</p></th><th><p>R@2</p></th><th><p>#Proxies</p></th><th><p>R@1</p></th><th><p>R@2</p></th><th><p>#Proxies</p></th><th><p>R@1</p></th><th><p>R@10</p></th><th><p>#Proxies</p></th><th><p>R@1</p></th><th><p>R@10</p></th></tr></thead><tbody><tr><th>SoftTriple [32]</th><td>1,000</td><td><p>72.7</p></td><td><p>82.7</p></td><td>980</td><td><p>83.2</p></td><td><p>90.2</p></td><td>113,180</td><td><p>80.9</p></td><td><p>91.3</p></td><td>39,970</td><td><p>88.5</p></td><td><p>97.3</p></td></tr><tr><th>PA + HIER (ours)</th><td><p>612</p></td><td>75.2</td><td>84.2</td><td><p>610</p></td><td>85.1</td><td>91.2</td><td><p>11,830</p></td><td>82.5</td><td>92.7</td><td><p>4,509</p></td><td>91.0</td><td>98.0</td></tr></tbody></table>", "caption": "Table 4: Comparison between methods using proxies in terms of performance and the number of proxies on the four benchmark datasets. In these experiments, the backbone network is initialized by weights of DeiT [45].", "list_citation_info": ["[32] Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric learning without triplet sampling. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.", "[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proc. International Conference on Machine Learning (ICML), 2021."]}], "citation_info_to_title": {"[34] Karsten Roth, Biagio Brattoli, and Bjorn Ommer. Mic: Mining interclass characteristics for improved metric learning. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.": "Mic: Mining interclass characteristics for improved metric learning", "[15] Weifeng Ge, Weilin Huang, Dengke Dong, and Matthew R. Scott. Deep metric learning with hierarchical triplet loss. In Proc. European Conference on Computer Vision (ECCV), 2018.": "Deep Metric Learning with Hierarchical Triplet Loss", "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proc. International Conference on Learning Representations (ICLR), 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[20] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "Proxy Anchor Loss for Deep Metric Learning", "[44] Eu Wern Teh, Terrance DeVries, and Graham W Taylor. Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis. In European Conference on Computer Vision (ECCV). Springer, 2020.": "Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis", "[12] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herv\u00e9 J\u00e9gou. Training vision transformers for image retrieval. arXiv preprint arXiv:2102.05644, 2021.": "Training Vision Transformers for Image Retrieval", "[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: a large-scale hierarchical image database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.": "ImageNet: a large-scale hierarchical image database", "[13] Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, and Ivan Oseledets. Hyperbolic vision transformers: Combining improvements in metric learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.": "Hyperbolic Vision Transformers: Combining Improvements in Metric Learning", "[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proc. IEEE International Conference on Computer Vision (ICCV), 2021.": "Emerging properties in self-supervised vision transformers", "[32] Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric learning without triplet sampling. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.": "Softtriple loss: Deep metric learning without triplet sampling", "[57] Andrew Zhai and Hao-Yu Wu. Classification is a strong baseline for deep metric learning. arXiv preprint arXiv:1811.12649, 2018.": "Classification is a strong baseline for deep metric learning", "[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proc. International Conference on Machine Learning (ICML), 2021.": "Training Data-Efficient Image Transformers & Distillation Through Attention", "[18] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proc. International Conference on Machine Learning (ICML), 2015.": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.": "Deep Residual Learning for Image Recognition", "[50] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R Scott. Multi-similarity loss with general pair weighting for deep metric learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Multi-similarity loss with general pair weighting for deep metric learning", "[52] Xun Wang, Haozhi Zhang, Weilin Huang, and Matthew R Scott. Cross-batch memory for embedding learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6388\u20136397, 2020.": "Cross-batch memory for embedding learning"}, "source_title_to_arxiv_id": {"Emerging properties in self-supervised vision transformers": "2104.14294"}}