{"title": "LAPFormer: A Light and Accurate Polyp Segmentation Transformer", "abstract": "Polyp segmentation is still known as a difficult problem due to the large\nvariety of polyp shapes, scanning and labeling modalities. This prevents deep\nlearning model to generalize well on unseen data. However, Transformer-based\napproach recently has achieved some remarkable results on performance with the\nability of extracting global context better than CNN-based architecture and yet\nlead to better generalization. To leverage this strength of Transformer, we\npropose a new model with encoder-decoder architecture named LAPFormer, which\nuses a hierarchical Transformer encoder to better extract global feature and\ncombine with our novel CNN (Convolutional Neural Network) decoder for capturing\nlocal appearance of the polyps. Our proposed decoder contains a progressive\nfeature fusion module designed for fusing feature from upper scales and lower\nscales and enable multi-scale features to be more correlative. Besides, we also\nuse feature refinement module and feature selection module for processing\nfeature. We test our model on five popular benchmark datasets for polyp\nsegmentation, including Kvasir, CVC-Clinic DB, CVC-ColonDB, CVC-T, and\nETIS-Larib", "authors": ["Mai Nguyen", "Tung Thanh Bui", "Quan Van Nguyen", "Thanh Tung Nguyen", "Toan Van Pham"], "published_date": "2022_10_10", "pdf_url": "http://arxiv.org/pdf/2210.04393v1", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">Kvasir</th><th colspan=\"2\">ClinicDB</th><th colspan=\"2\">ColonDB</th><th colspan=\"2\">CVC-T</th><th colspan=\"2\">ETIS</th></tr><tr><th>mDice</th><th>mIoU</th><th>mDice</th><th>mIoU</th><th>mDice</th><th>mIoU</th><th>mDice</th><th>mIoU</th><th>mDice</th><th>mIoU</th></tr></thead><tbody><tr><th>PraNet [26]</th><td>0.898</td><td>0.840</td><td>0.899</td><td>0.849</td><td>0.709</td><td>0.640</td><td>0.871</td><td>0.797</td><td>0.628</td><td>0.567</td></tr><tr><th>Polyp-PVT [30]</th><td>0.917</td><td>0.864</td><td>0.937</td><td>0.889</td><td>0.808</td><td>0.727</td><td>0.900</td><td>0.833</td><td>0.787</td><td>0.706</td></tr><tr><th>SANet [24]</th><td>0.904</td><td>0.847</td><td>0.916</td><td>0.859</td><td>0.753</td><td>0.670</td><td>0.888</td><td>0.815</td><td>0.750</td><td>0.654</td></tr><tr><th>MSNet [25]</th><td>0.907</td><td>0.862</td><td>0.921</td><td>0.879</td><td>0.755</td><td>0.678</td><td>0.869</td><td>0.807</td><td>0.719</td><td>0.664</td></tr><tr><th>TransFuse-L* [7]</th><td>0.920</td><td>0.870</td><td>0.942</td><td>0.897</td><td>0.781</td><td>0.706</td><td>0.894</td><td>0.826</td><td>0.737</td><td>0.663</td></tr><tr><th>SSFormer-L [29]</th><td>0.917</td><td>0.864</td><td>0.906</td><td>0.855</td><td>0.802</td><td>0.721</td><td>0.895</td><td>0.827</td><td>0.796</td><td>0.720</td></tr><tr><th>ColonFormer-L [8]</th><td>0.924</td><td>0.876</td><td>0.932</td><td>0.884</td><td>0.811</td><td>0.733</td><td>0.906</td><td>0.842</td><td>0.801</td><td>0.722</td></tr><tr><th>ColonFormer-XL [8]</th><td>0.920</td><td>0.870</td><td>0.923</td><td>0.875</td><td>0.814</td><td>0.735</td><td>0.905</td><td>0.840</td><td>0.795</td><td>0.715</td></tr><tr><th>LAPFormer-S</th><td>0.910</td><td>0.857</td><td>0.901</td><td>0.849</td><td>0.781</td><td>0.695</td><td>0.859</td><td>0.781</td><td>0.768</td><td>0.686</td></tr><tr><th>LAPFormer-M</th><td>0.913</td><td>0.863</td><td>0.911</td><td>0.861</td><td>0.800</td><td>0.722</td><td>0.856</td><td>0.790</td><td>0.784</td><td>0.709</td></tr><tr><th>LAPFormer-L</th><td>0.917</td><td>0.867</td><td>0.915</td><td>0.866</td><td>0.815</td><td>0.735</td><td>0.891</td><td>0.821</td><td>0.797</td><td>0.719</td></tr></tbody></table>", "caption": "Table III: Comparison with other approaches on 5 benchmark datasets", "list_citation_info": ["[24] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, \u201cShallow attention network for polyp segmentation,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 699\u2013708.", "[7] Y. Zhang, H. Liu, and Q. Hu, \u201cTransfuse: Fusing transformers and cnns for medical image segmentation,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 14\u201324.", "[26] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, \u201cPranet: Parallel reverse attention network for polyp segmentation,\u201d in International conference on medical image computing and computer-assisted intervention. Springer, 2020, pp. 263\u2013273.", "[8] N. T. Duc, N. T. Oanh, N. T. Thuy, T. M. Triet, and D. V. Sang, \u201cColonformer: An efficient transformer based method for colon polyp segmentation,\u201d arXiv preprint arXiv:2205.08473, 2022.", "[29] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, and S. Song, \u201cStepwise feature fusion: Local guides global,\u201d arXiv preprint arXiv:2203.03635, 2022.", "[30] B. Dong, W. Wang, D.-P. Fan, J. Li, H. Fu, and L. Shao, \u201cPolyp-pvt: Polyp segmentation with pyramid vision transformers,\u201d arXiv preprint arXiv:2108.06932, 2021.", "[25] X. Zhao, L. Zhang, and H. Lu, \u201cAutomatic polyp segmentation via multi-scale subtraction network,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 120\u2013130."]}, {"table": "<table><thead><tr><th>Methods</th><th>GFLOPs</th><th>Params (M)</th></tr></thead><tbody><tr><td>PraNet [26]</td><td>13.11</td><td>32.55</td></tr><tr><td>CaraNet [28]</td><td>21.69</td><td>46.64</td></tr><tr><td>TransUNet [31]</td><td>60.75</td><td>105.5</td></tr><tr><td>ColonFormer-S [8]</td><td>16.03</td><td>33.04</td></tr><tr><td>ColonFormer-L [8]</td><td>22.94</td><td>52.94</td></tr><tr><td>SSFormer-S [29]</td><td>17.54</td><td>29.31</td></tr><tr><td>SSFormer-L [29]</td><td>28.26</td><td>65.96</td></tr><tr><td>LAPFormer-S</td><td>10.54</td><td>16.3</td></tr><tr><td>LAPFormer-L</td><td>18.96</td><td>47.22</td></tr></tbody></table>", "caption": "Table IV: Number of Parameters and FLOPs of different methods", "list_citation_info": ["[8] N. T. Duc, N. T. Oanh, N. T. Thuy, T. M. Triet, and D. V. Sang, \u201cColonformer: An efficient transformer based method for colon polyp segmentation,\u201d arXiv preprint arXiv:2205.08473, 2022.", "[26] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, \u201cPranet: Parallel reverse attention network for polyp segmentation,\u201d in International conference on medical image computing and computer-assisted intervention. Springer, 2020, pp. 263\u2013273.", "[28] A. Lou, S. Guan, and M. Loew, \u201cCaranet: Context axial reverse attention network for segmentation of small medical objects,\u201d arXiv preprint arXiv:2108.07368, 2021.", "[29] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, and S. Song, \u201cStepwise feature fusion: Local guides global,\u201d arXiv preprint arXiv:2203.03635, 2022.", "[31] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou, \u201cTransunet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021."]}], "citation_info_to_title": {"[28] A. Lou, S. Guan, and M. Loew, \u201cCaranet: Context axial reverse attention network for segmentation of small medical objects,\u201d arXiv preprint arXiv:2108.07368, 2021.": "Caranet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects", "[31] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille, and Y. Zhou, \u201cTransunet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.": "Transunet: Transformers make strong encoders for medical image segmentation", "[7] Y. Zhang, H. Liu, and Q. Hu, \u201cTransfuse: Fusing transformers and cnns for medical image segmentation,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 14\u201324.": "Transfuse: Fusing transformers and cnns for medical image segmentation", "[24] J. Wei, Y. Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, \u201cShallow attention network for polyp segmentation,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 699\u2013708.": "Shallow attention network for polyp segmentation", "[29] J. Wang, Q. Huang, F. Tang, J. Meng, J. Su, and S. Song, \u201cStepwise feature fusion: Local guides global,\u201d arXiv preprint arXiv:2203.03635, 2022.": "Stepwise feature fusion: Local guides global", "[26] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao, \u201cPranet: Parallel reverse attention network for polyp segmentation,\u201d in International conference on medical image computing and computer-assisted intervention. Springer, 2020, pp. 263\u2013273.": "Pranet: Parallel reverse attention network for polyp segmentation", "[8] N. T. Duc, N. T. Oanh, N. T. Thuy, T. M. Triet, and D. V. Sang, \u201cColonformer: An efficient transformer based method for colon polyp segmentation,\u201d arXiv preprint arXiv:2205.08473, 2022.": "Colonformer: An efficient transformer based method for colon polyp segmentation", "[25] X. Zhao, L. Zhang, and H. Lu, \u201cAutomatic polyp segmentation via multi-scale subtraction network,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 120\u2013130.": "Automatic polyp segmentation via multi-scale subtraction network", "[30] B. Dong, W. Wang, D.-P. Fan, J. Li, H. Fu, and L. Shao, \u201cPolyp-pvt: Polyp segmentation with pyramid vision transformers,\u201d arXiv preprint arXiv:2108.06932, 2021.": "Polyp-pvt: Polyp Segmentation with Pyramid Vision Transformers"}, "source_title_to_arxiv_id": {"Transunet: Transformers make strong encoders for medical image segmentation": "2102.04306", "Polyp-pvt: Polyp Segmentation with Pyramid Vision Transformers": "2108.06932"}}