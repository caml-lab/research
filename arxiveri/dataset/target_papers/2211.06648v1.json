{"title": "DATa: Domain Adaptation-Aided Deep Table Detection Using Visual-Lexical Representations", "abstract": "Considerable research attention has been paid to table detection by\ndeveloping not only rule-based approaches reliant on hand-crafted heuristics\nbut also deep learning approaches. Although recent studies successfully perform\ntable detection with enhanced results, they often experience performance\ndegradation when they are used for transferred domains whose table layout\nfeatures might differ from the source domain in which the underlying model has\nbeen trained. To overcome this problem, we present DATa, a novel Domain\nAdaptation-aided deep Table detection method that guarantees satisfactory\nperformance in a specific target domain where few trusted labels are available.\nTo this end, we newly design lexical features and an augmented model used for\nre-training. More specifically, after pre-training one of state-of-the-art\nvision-based models as our backbone network, we re-train our augmented model,\nconsisting of the vision-based model and the multilayer perceptron (MLP)\narchitecture. Using new confidence scores acquired based on the trained MLP\narchitecture as well as an initial prediction of bounding boxes and their\nconfidence scores, we calculate each confidence score more accurately. To\nvalidate the superiority of DATa, we perform experimental evaluations by\nadopting a real-world benchmark dataset in a source domain and another dataset\nin our target domain consisting of materials science articles. Experimental\nresults demonstrate that the proposed DATa method substantially outperforms\ncompeting methods that only utilize visual representations in the target\ndomain. Such gains are possible owing to the capability of eliminating high\nfalse positives or false negatives according to the setting of a confidence\nscore threshold.", "authors": ["Hyebin Kwon", "Joungbin An", "Dongwoo Lee", "Won-Yong Shin"], "published_date": "2022_11_12", "pdf_url": "http://arxiv.org/pdf/2211.06648v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>Model</th><th>Precision</th><th>Recall</th><th>F_{1} Score</th></tr></thead><tbody><tr><td rowspan=\"6\">ICDAR 2013</td><td>YOLOv5 glenn_jocher_2020_4154370 </td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>CascadeTabNet prasad2020cascadetabnet </td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><td>DeCNT siddiqui2018decnt </td><td>0.996</td><td>0.996</td><td>0.996</td></tr><tr><td>DeepDeSRT schreiber2017deepdesrt </td><td>0.9615</td><td>0.9740</td><td>0.9677</td></tr><tr><td>TableNet paliwal2019tablenet </td><td>0.9628</td><td>0.9697</td><td>0.9662</td></tr><tr><td>CDeC-Net agarwal2021cdec </td><td>0.942</td><td>0.993</td><td>0.968</td></tr><tr><td></td><td>DETR-R50 detr2020eccv </td><td>0.967</td><td>0.971</td><td>0.969</td></tr><tr><td></td><td>Deformable DETR deformabledetr2021iclr </td><td>0.966</td><td>0.971</td><td>0.968</td></tr><tr><td rowspan=\"3\">ICDAR 2019</td><td>YOLOv5 glenn_jocher_2020_4154370 </td><td>0.980</td><td>0.975</td><td>0.978</td></tr><tr><td>CDec-Net agarwal2021cdec </td><td>0.930</td><td>0.971</td><td>0.950</td></tr><tr><td>TableRadar gao2019icdar </td><td>0.940</td><td>0.950</td><td>0.945</td></tr><tr><td></td><td>DETR-R50 detr2020eccv </td><td>0.946</td><td>0.971</td><td>0.958</td></tr><tr><td></td><td>Deformable DETR deformabledetr2021iclr </td><td>0.969</td><td>0.994</td><td>0.981</td></tr><tr><td rowspan=\"3\">Marmot</td><td>YOLOv5 glenn_jocher_2020_4154370 </td><td>0.955</td><td>0.940</td><td>0.947</td></tr><tr><td>DeCNT siddiqui2018decnt </td><td>0.946</td><td>0.849</td><td>0.895</td></tr><tr><td>CDeC-Net agarwal2021cdec </td><td>0.779</td><td>0.943</td><td>0.861</td></tr><tr><td></td><td>DETR-R50 detr2020eccv </td><td>0.914</td><td>0.945</td><td>0.929</td></tr><tr><td></td><td>Deformable DETR deformabledetr2021iclr </td><td>0.932</td><td>0.991</td><td>0.931</td></tr></tbody></table>", "caption": "Table 1: Performance comparison among state-of-the-art methods for table detection in terms of three performance metrics when three benchmark datasets are used.", "list_citation_info": ["(27) S. Schreiber, S. Agne, I. Wolf, A. Dengel, and S. Ahmed, DeepDeSRT: Deep learning for detection and structure recognition of tables in document images, in: Proceedings of the 14th International Conference on Document Analysis and Recognition (ICDAR), 2017, pp. 1162\u20131167.", "(26) D. Prasad, A. Gadpal, K. Kapadni, M. Visave, and K. Sultanpure, CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents, in: Proceedings of the CVPR Workshop on Text and Documents in the Deep Learning Era, 2020, pp. 1\u201310.", "(3) N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, in: Proceedings of the 16th European Conference on Computer Vision (ECCV), 2020, pp. 213\u2013229.", "(14) G. Jocher, A. Stoken, J. Borovec, NanoCode012, ChristopherSTAN, L. Changyu, Laughing, tkianai, A. Hogan, lorenzomammana, yxNONG, AlexWang1900, L. Diaconu, Marc, wanghaoyang0106, ml5ah, Doug, F. Ingham, Frederik, Guilhen, Hatovix, J. Poznanski, J. Fang, L. Y., changyu98, M. Wang, N. Gupta, O. Akhtar, PetrDvoracek, and P. Rai, ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements, Available: https://doi.org/10.5281/zenodo.4154370, 2020.", "(32) X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, Deformable DETR: Deformable transformers for end-to-end object detection, in: Proceedings of the 9th International Conference on Learning Representations (ICLR), 2021, pp. 1\u2013-16.", "(23) S. S. Paliwal, D. Vishwanath, R. Rahul, M. Sharma, and L. Vig, TableNet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images, in: Proceedings of the 15th International Conference on Document Analysis and Recognition (ICDAR), 2019, pp. 128\u2013133.", "(2) M. Agarwal, A. Mondal, and C. Jawahar, CDeC-Net: Composite deformable cascade network for table detection in document images, in: Proceedings of the 25th International Conference on Pattern Recognition (ICPR), 2021, pp. 9491-\u20139498.", "(8) L. Gao, Y. Huang, H. D\u00b4ejean, J.-L. Meunier, Q. Yan, Y. Fang, F. Kleber, and E. Lang, ICDAR 2019 competition on table detection and recognition (cTDaR), in: Proceedings of the 15th International Conference on Document Analysis and Recognition (ICDAR), 2019, pp. 1510\u20131515.", "(28) S. A. Siddiqui, M. I. Malik, S. Agne, A. Dengel, and S. Ahmed, DeCNT: Deep deformable CNN for table detection, IEEE Access, 6 (2018) 74151\u201374161."]}], "citation_info_to_title": {"(27) S. Schreiber, S. Agne, I. Wolf, A. Dengel, and S. Ahmed, DeepDeSRT: Deep learning for detection and structure recognition of tables in document images, in: Proceedings of the 14th International Conference on Document Analysis and Recognition (ICDAR), 2017, pp. 1162\u20131167.": "DeepDeSRT: Deep learning for detection and structure recognition of tables in document images", "(26) D. Prasad, A. Gadpal, K. Kapadni, M. Visave, and K. Sultanpure, CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents, in: Proceedings of the CVPR Workshop on Text and Documents in the Deep Learning Era, 2020, pp. 1\u201310.": "CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents", "(14) G. Jocher, A. Stoken, J. Borovec, NanoCode012, ChristopherSTAN, L. Changyu, Laughing, tkianai, A. Hogan, lorenzomammana, yxNONG, AlexWang1900, L. Diaconu, Marc, wanghaoyang0106, ml5ah, Doug, F. Ingham, Frederik, Guilhen, Hatovix, J. Poznanski, J. Fang, L. Y., changyu98, M. Wang, N. Gupta, O. Akhtar, PetrDvoracek, and P. Rai, ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements, Available: https://doi.org/10.5281/zenodo.4154370, 2020.": "ultralytics/yolov5: v31 - Bug Fixes and Performance Improvements", "(3) N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, in: Proceedings of the 16th European Conference on Computer Vision (ECCV), 2020, pp. 213\u2013229.": "End-to-end object detection with transformers", "(23) S. S. Paliwal, D. Vishwanath, R. Rahul, M. Sharma, and L. Vig, TableNet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images, in: Proceedings of the 15th International Conference on Document Analysis and Recognition (ICDAR), 2019, pp. 128\u2013133.": "TableNet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images", "(8) L. Gao, Y. Huang, H. D\u00b4ejean, J.-L. Meunier, Q. Yan, Y. Fang, F. Kleber, and E. Lang, ICDAR 2019 competition on table detection and recognition (cTDaR), in: Proceedings of the 15th International Conference on Document Analysis and Recognition (ICDAR), 2019, pp. 1510\u20131515.": "ICDAR 2019 competition on table detection and recognition (cTDaR)", "(32) X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, Deformable DETR: Deformable transformers for end-to-end object detection, in: Proceedings of the 9th International Conference on Learning Representations (ICLR), 2021, pp. 1\u2013-16.": "Deformable DETR: Deformable transformers for end-to-end object detection", "(28) S. A. Siddiqui, M. I. Malik, S. Agne, A. Dengel, and S. Ahmed, DeCNT: Deep deformable CNN for table detection, IEEE Access, 6 (2018) 74151\u201374161.": "DeCNT: Deep deformable CNN for table detection", "(2) M. Agarwal, A. Mondal, and C. Jawahar, CDeC-Net: Composite deformable cascade network for table detection in document images, in: Proceedings of the 25th International Conference on Pattern Recognition (ICPR), 2021, pp. 9491-\u20139498.": "CDeC-Net: Composite deformable cascade network for table detection in document images"}, "source_title_to_arxiv_id": {"CDeC-Net: Composite deformable cascade network for table detection in document images": "2008.10831"}}