{"title": "Surrogate-assisted Multi-objective Neural Architecture Search for Real-time Semantic Segmentation", "abstract": "The architectural advancements in deep neural networks have led to remarkable\nleap-forwards across a broad array of computer vision tasks. Instead of relying\non human expertise, neural architecture search (NAS) has emerged as a promising\navenue toward automating the design of architectures. While recent achievements\nin image classification have suggested opportunities, the promises of NAS have\nyet to be thoroughly assessed on more challenging tasks of semantic\nsegmentation. The main challenges of applying NAS to semantic segmentation\narise from two aspects: (i) high-resolution images to be processed; (ii)\nadditional requirement of real-time inference speed (i.e., real-time semantic\nsegmentation) for applications such as autonomous driving. To meet such\nchallenges, we propose a surrogate-assisted multi-objective method in this\npaper. Through a series of customized prediction models, our method effectively\ntransforms the original NAS task into an ordinary multi-objective optimization\nproblem. Followed by a hierarchical pre-screening criterion for in-fill\nselection, our method progressively achieves a set of efficient architectures\ntrading-off between segmentation accuracy and inference speed. Empirical\nevaluations on three benchmark datasets together with an application using\nHuawei Atlas 200 DK suggest that our method can identify architectures\nsignificantly outperforming existing state-of-the-art architectures designed\nboth manually by human experts and automatically by other NAS methods.", "authors": ["Zhichao Lu", "Ran Cheng", "Shihua Huang", "Haoming Zhang", "Changxiao Qiu", "Fan Yang"], "published_date": "2022_08_14", "pdf_url": "http://arxiv.org/pdf/2208.06820v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>  Model</th><td>FLOPs \\downarrow (G)</td><td>Speed \\uparrow (imgs / sec)</td><td>mIoU \\uparrow (%)</td></tr><tr><th>  FCN [64]</th><td>-</td><td>5.9</td><td>22.7</td></tr><tr><th>  DeepLab</th><td>-</td><td>8.1</td><td>26.9</td></tr><tr><th>  BiSeNet [14]</th><td>25.1</td><td>-</td><td>28.1</td></tr><tr><th>  BiSeNetV2 [61]</th><td>27.1</td><td>42.5</td><td>28.7</td></tr><tr><th>  ICNet [65]</th><td>-</td><td>35.7</td><td>29.1</td></tr><tr><th>  PSPNet50 [63]</th><td>-</td><td>6.6</td><td>32.6</td></tr><tr><th>  MoSegNet{}_{small}</th><td>12.0</td><td>92.1</td><td>30.5</td></tr><tr><th>  MoSegNet{}_{large}</th><td>14.3</td><td>78.3</td><td>34.2</td></tr></tbody></table>", "caption": "Table 3: Performance comparison on COCO-Stuff-10\\bm{K} [27]. Models are evaluated on network complexity (FLOPs), inference speed, and accuracy (mIoU). The best result in each section is in bold. ", "list_citation_info": ["[27] H. Caesar, J. Uijlings, and V. Ferrari, \u201cCoco-stuff: Thing and stuff classes in context,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.", "[65] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia, \u201cICNet for real-time semantic segmentation on high-resolution images,\u201d in European Conference on Computer Vision (ECCV), 2018.", "[63] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[64] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks for semantic segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.", "[14] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, \u201cBiSeNet: Bilateral segmentation network for real-time semantic segmentation,\u201d in European Conference on Computer Vision (ECCV), 2018.", "[61] C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, \u201cBisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation,\u201d International Journal of Computer Vision, vol. 129, no. 11, pp. 3051\u20133068, 2021."]}, {"table": "<table><tbody><tr><td>  Model</td><td>Method</td><td>COCO</td><td>FLOPs \\downarrow (G)</td><td>mIoU \\uparrow (%)</td></tr><tr><td>  BiSeNet [14]</td><td rowspan=\"5\">manual</td><td></td><td>16</td><td>71.0</td></tr><tr><td>  DeepLabv3+ [44]</td><td></td><td>101</td><td>79.0</td></tr><tr><td>  Res2Net [66]</td><td></td><td>101</td><td>80.2</td></tr><tr><td>  PSPNet [63]</td><td></td><td>-</td><td>82.4</td></tr><tr><td>  RefineNet [67]</td><td>\u2713</td><td>261</td><td>82.9</td></tr><tr><td>  Auto-DeepLab-S [18]</td><td rowspan=\"2\">gradient</td><td></td><td>43</td><td>71.7</td></tr><tr><td>  Auto-DeepLab-L [18]</td><td>\u2713</td><td>87</td><td>80.8</td></tr><tr><td>  MoSegNet{}_{small}</td><td rowspan=\"2\">EA</td><td></td><td>7.8</td><td>75.8</td></tr><tr><td>  MoSegNet{}_{large}</td><td>\u2713</td><td>9.3</td><td>82.2</td></tr></tbody></table>", "caption": "Table 4: Performance comparison on PASCAL VOC 2012 [13]. Models are evaluated on network complexity (FLOPs) and accuracy (mIoU). \u201cCOCO\u201d indicates additional pretraining on MS COCO dataset [57]. The best result in each section is in bold. ", "list_citation_info": ["[66] S. Gao, M. M. Cheng, K. Zhao, X. Y. Zhang, M. H. Yang, and P. H. S. Torr, \u201cRes2net: A new multi-scale backbone architecture,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2019.", "[57] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in European Conference on Computer Vision (ECCV), 2014.", "[44] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, \u201cEncoder-decoder with atrous separable convolution for semantic image segmentation,\u201d in European Conference on Computer Vision (ECCV), 2018.", "[18] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and L. Fei-Fei, \u201cAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[63] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[13] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, \u201cThe pascal visual object classes challenge: A retrospective,\u201d International journal of computer vision, vol. 111, no. 1, pp. 98\u2013136, 2015.", "[67] G. Lin, F. Liu, A. Milan, C. Shen, and I. Reid, \u201cRefinenet: Multi-path refinement networks for dense prediction,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 5, pp. 1228\u20131242, 2020.", "[14] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, \u201cBiSeNet: Bilateral segmentation network for real-time semantic segmentation,\u201d in European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>  Method</th><th>Pearson r</th><th>Spearman \\rho</th><th>Kendall \\tau</th><th>Time (sec.)</th></tr></thead><tbody><tr><th>  RBF</th><td>0.6199 (0.12)</td><td>0.5688 (0.10)</td><td>0.4692 (0.05)</td><td>0.02</td></tr><tr><th>  Kriging</th><td>0.1161 (0.18)</td><td>0.0211 (0.15)</td><td>0.0107 (0.10)</td><td>4.52</td></tr><tr><th>  SVR</th><td>0.5721 (0.11)</td><td>0.5597 (0.10)</td><td>0.4386 (0.05)</td><td>0.11</td></tr><tr><th>  DT</th><td>0.6621 (0.15)</td><td>0.5986 (0.10)</td><td>0.5377 (0.05)</td><td>0.01</td></tr><tr><th>  GB [56]</th><td>0.7409 (0.10)</td><td>0.7079 (0.08)</td><td>0.6259 (0.04)</td><td>0.61</td></tr><tr><th>  E2EPP [37]</th><td>0.7746 (0.10)</td><td>0.7302 (0.07)</td><td>0.6162 (0.04)</td><td>1.01</td></tr><tr><th>  RankNet (ours)</th><td>0.7988 (0.08)</td><td>0.7658 (0.07)</td><td>0.6873 (0.03)</td><td>3.82</td></tr></tbody></table>", "caption": "Table 5: Coefficients of correlation comparison. Each method is trained with 1,000 samples and evaluated on a held-out test set of 100 samples. The results are averaged over 31 runs with standard deviation shown in the parentheses. \u201cTime\u201d denotes the training time measured in seconds. The best result in each section is in bold. ", "list_citation_info": ["[56] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu, \u201cLightgbm: A highly efficient gradient boosting decision tree,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2017.", "[37] Y. Sun, H. Wang, B. Xue, Y. Jin, G. G. Yen, and M. Zhang, \u201cSurrogate-assisted evolutionary deep learning using an end-to-end random forest-based performance predictor,\u201d IEEE Transactions on Evolutionary Computation, vol. 24, no. 2, pp. 350\u2013364, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">  Backbone</th><th rowspan=\"2\">Optimizedfor</th><th rowspan=\"2\">ImageNetTop-1</th><td colspan=\"3\">Semantic Segmentation mIoU (%)</td></tr><tr><td>Cityscapes</td><td>VOC 2012</td><td>COCO-Stuff</td></tr><tr><th>  ProxylessNAS [40]</th><th>Cls.</th><th>74.6</th><td>73.4</td><td>74.2</td><td>28.2</td></tr><tr><th>  MobileNetV3 [68]</th><th>Cls.</th><th>75.2</th><td>75.2</td><td>73.8</td><td>28.5</td></tr><tr><th>  FBNetV2 [41]</th><th>Cls.</th><th>75.5</th><td>72.6</td><td>73.6</td><td>28.5</td></tr><tr><th>  MoSegNet{}_{small}</th><th>Seg.</th><th>(-1.7) 73.4</th><td>(+2.9) 75.9</td><td>(+1.9) 75.8</td><td>(+2.1) 30.5</td></tr></tbody></table>", "caption": "Table 7: Semantic segmentation performance comparison on backbone (encoder) models optimized for classification (Cls.) and segmentation (Seg.). The compared backbone models are of similar complexity in terms of inference speed. The averaged relative differences are shown in parentheses.", "list_citation_info": ["[40] H. Cai, L. Zhu, and S. Han, \u201cProxylessNAS: Direct neural architecture search on target task and hardware,\u201d in International Conference on Learning Representations (ICLR), 2019.", "[68] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, and H. Adam, \u201cSearching for mobilenetv3,\u201d in International Conference on Computer Vision (ICCV), 2019.", "[41] A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu, K. Chen et al., \u201cFBNetv2: Differentiable neural architecture search for spatial and channel dimensions,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}], "citation_info_to_title": {"[65] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia, \u201cICNet for real-time semantic segmentation on high-resolution images,\u201d in European Conference on Computer Vision (ECCV), 2018.": "ICNet for real-time semantic segmentation on high-resolution images", "[27] H. Caesar, J. Uijlings, and V. Ferrari, \u201cCoco-stuff: Thing and stuff classes in context,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.": "Coco-stuff: Thing and stuff classes in context", "[67] G. Lin, F. Liu, A. Milan, C. Shen, and I. Reid, \u201cRefinenet: Multi-path refinement networks for dense prediction,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 5, pp. 1228\u20131242, 2020.": "RefineNet: Multi-Path Refinement Networks for Dense Prediction", "[18] C. Liu, L.-C. Chen, F. Schroff, H. Adam, W. Hua, A. L. Yuille, and L. Fei-Fei, \u201cAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation", "[40] H. Cai, L. Zhu, and S. Han, \u201cProxylessNAS: Direct neural architecture search on target task and hardware,\u201d in International Conference on Learning Representations (ICLR), 2019.": "ProxylessNAS: Direct neural architecture search on target task and hardware", "[63] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.": "Pyramid scene parsing network", "[57] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in European Conference on Computer Vision (ECCV), 2014.": "Microsoft coco: Common objects in context", "[37] Y. Sun, H. Wang, B. Xue, Y. Jin, G. G. Yen, and M. Zhang, \u201cSurrogate-assisted evolutionary deep learning using an end-to-end random forest-based performance predictor,\u201d IEEE Transactions on Evolutionary Computation, vol. 24, no. 2, pp. 350\u2013364, 2020.": "Surrogate-assisted evolutionary deep learning using an end-to-end random forest-based performance predictor", "[14] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, \u201cBiSeNet: Bilateral segmentation network for real-time semantic segmentation,\u201d in European Conference on Computer Vision (ECCV), 2018.": "BiSeNet: Bilateral segmentation network for real-time semantic segmentation", "[68] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, Q. V. Le, and H. Adam, \u201cSearching for mobilenetv3,\u201d in International Conference on Computer Vision (ICCV), 2019.": "Searching for MobileNetV3", "[56] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu, \u201cLightgbm: A highly efficient gradient boosting decision tree,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2017.": "Lightgbm: A highly efficient gradient boosting decision tree", "[61] C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, \u201cBisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation,\u201d International Journal of Computer Vision, vol. 129, no. 11, pp. 3051\u20133068, 2021.": "Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation", "[64] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks for semantic segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.": "Fully convolutional networks for semantic segmentation", "[44] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, \u201cEncoder-decoder with atrous separable convolution for semantic image segmentation,\u201d in European Conference on Computer Vision (ECCV), 2018.": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "[13] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, \u201cThe pascal visual object classes challenge: A retrospective,\u201d International journal of computer vision, vol. 111, no. 1, pp. 98\u2013136, 2015.": "The Pascal Visual Object Classes Challenge: A Retrospective", "[66] S. Gao, M. M. Cheng, K. Zhao, X. Y. Zhang, M. H. Yang, and P. H. S. Torr, \u201cRes2net: A new multi-scale backbone architecture,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1\u20131, 2019.": "Res2net: A new multi-scale backbone architecture", "[41] A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu, K. Chen et al., \u201cFBNetv2: Differentiable neural architecture search for spatial and channel dimensions,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "FBNetv2: Differentiable neural architecture search for spatial and channel dimensions"}, "source_title_to_arxiv_id": {"ProxylessNAS: Direct neural architecture search on target task and hardware": "1812.00332"}}