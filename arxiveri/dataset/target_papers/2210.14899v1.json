{"title": "Learning a Task-specific Descriptor for Robust Matching of 3D Point Clouds", "abstract": "Existing learning-based point feature descriptors are usually task-agnostic,\nwhich pursue describing the individual 3D point clouds as accurate as possible.\nHowever, the matching task aims at describing the corresponding points\nconsistently across different 3D point clouds. Therefore these too accurate\nfeatures may play a counterproductive role due to the inconsistent point\nfeature representations of correspondences caused by the unpredictable noise,\npartiality, deformation, \\etc, in the local geometry. In this paper, we propose\nto learn a robust task-specific feature descriptor to consistently describe the\ncorrect point correspondence under interference. Born with an Encoder and a\nDynamic Fusion module, our method EDFNet develops from two aspects. First, we\naugment the matchability of correspondences by utilizing their repetitive local\nstructure. To this end, a special encoder is designed to exploit two input\npoint clouds jointly for each point descriptor. It not only captures the local\ngeometry of each point in the current point cloud by convolution, but also\nexploits the repetitive structure from paired point cloud by Transformer.\nSecond, we propose a dynamical fusion module to jointly use different scale\nfeatures. There is an inevitable struggle between robustness and\ndiscriminativeness of the single scale feature. Specifically, the small scale\nfeature is robust since little interference exists in this small receptive\nfield. But it is not sufficiently discriminative as there are many repetitive\nlocal structures within a point cloud. Thus the resultant descriptors will lead\nto many incorrect matches. In contrast, the large scale feature is more\ndiscriminative by integrating more neighborhood information. ...", "authors": ["Zhiyuan Zhang", "Yuchao Dai", "Bin Fan", "Jiadai Sun", "Mingyi He"], "published_date": "2022_10_26", "pdf_url": "http://arxiv.org/pdf/2210.14899v1", "list_table_and_caption": [{"table": "<table><tr><td>Methods</td><td>AVG(%)</td><td>STD</td><td>Dim.</td><td>Times(ms)</td></tr><tr><td>Spin [13]</td><td>22.7</td><td>11.4</td><td>153</td><td>0.093</td></tr><tr><td>FPFH [15]</td><td>35.9</td><td>13.4</td><td>33</td><td>0.022</td></tr><tr><td>SHOT [16]</td><td>23.8</td><td>10.9</td><td>352</td><td>0.201</td></tr><tr><td>USC [36]</td><td>40.0</td><td>12.5</td><td>1980</td><td>2.483</td></tr><tr><td>PointNet [17]</td><td>47.1</td><td>12.7</td><td>256</td><td>0.117</td></tr><tr><td>3DMatch [46]</td><td>59.6</td><td>8.8</td><td>512</td><td>2.151</td></tr><tr><td>CGF [58]</td><td>58.2</td><td>14.2</td><td>32</td><td>0.981</td></tr><tr><td>PPFNet [24]</td><td>62.3</td><td>10.8</td><td>64</td><td>1.510</td></tr><tr><td>PPF-FoldNet [25]</td><td>71.8</td><td>10.5</td><td>512</td><td>0.531</td></tr><tr><td>DirectReg [59]</td><td>74.6</td><td>9.4</td><td>512</td><td>0.531</td></tr><tr><td>CapsuleNet [60]</td><td>80.7</td><td>6.2</td><td>512</td><td>0.808</td></tr><tr><td>PerferctMatch [23]</td><td>94.7</td><td>2.7</td><td>32</td><td>3.678</td></tr><tr><td>FCGF [19]</td><td>95.2</td><td>2.9</td><td>32</td><td>0.013</td></tr><tr><td>D3Feat(rand) [4]</td><td>95.3</td><td>2.7</td><td>32</td><td>0.005</td></tr><tr><td>D3Feat(pred) [4]</td><td>95.8</td><td>2.9</td><td>32</td><td>0.005</td></tr><tr><td>LMVD [61]</td><td>97.5</td><td>2.8</td><td>32</td><td>-</td></tr><tr><td>SpinNet [62]</td><td>97.6</td><td>1.9</td><td>32</td><td>4.216</td></tr><tr><td>DGR [63]</td><td>97.1</td><td>2.7</td><td>32</td><td>0.842</td></tr><tr><td>\\cdashline1-5[2.2pt/1.2pt]EDFNet(rand)</td><td>95.8</td><td>2.7</td><td>32</td><td>0.016</td></tr><tr><td>EDFNet(pred)</td><td>97.5</td><td>2.7</td><td>32</td><td>0.016</td></tr></table>", "caption": "TABLE I: Feature matching recall at \\tau_{1}=10\\text{cm}, \\tau_{2}=5\\% on the 3DMatch dataset. AVG and STD indicate the average feature matching recall and its standard deviation. Dim. indicates the dimension of the final feature. Time consumption of per feature is tested on Intel Core i7-11700K @ 3.60GHz and Nvidia RTX3090.", "list_citation_info": ["[19] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional geometric features. In Proceedings of the IEEE International Conference on Computer Vision, pages 8958\u20138966, 2019.", "[4] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, and ChiewLan Tai. D3feat: Joint learning of dense detection and description of 3d local features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6358\u20136366, 2020.", "[60] Yongheng Zhao, Tolga Birdal, Haowen Deng, and Federico Tombari. 3d point capsule networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1009\u20131018, 2019.", "[62] Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, and Yulan Guo. Spinnet: Learning a general surface descriptor for 3d point cloud registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11753\u201311762, 2021.", "[24] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppfnet: Global context aware local features for robust 3d point matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 195\u2013205, 2018.", "[15] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast point feature histograms (fpfh) for 3d registration. In Proceedings of the International Conference on Robotics and Automation, pages 3212\u20133217, 2009.", "[58] Marc Khoury, Qian-Yi Zhou, and Vladlen Koltun. Learning compact geometric features. In Proceedings of the IEEE International Conference on Computer Vision, pages 153\u2013161, 2017.", "[23] Zan Gojcic, Caifa Zhou, Jan D. Wegner, and Andreas Wieser. The perfect match: 3d point cloud matching with smoothed densities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5545\u20135554, 2019.", "[25] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-foldnet: Unsuper- vised learning of rotation invariant 3d local descriptors. In Proceedings of the European Conference on Computer Vision, pages 620\u2013638, 2018.", "[63] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep global registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2511\u20132520, 2020.", "[46] Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew Fisher, Jianxiong Xiao, and Thomas A. Funkhouser. 3dmatch: Learning local geometric descriptors from RGB-D reconstructions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 199\u2013208, 2017.", "[59] Haowen Deng, Tolga Birdal, and Slobodan Ilic. 3d local features for direct pairwise registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3244\u20133253, 2019.", "[36] Federico Tombari, Samuele Salti, and Luigi di Stefano. Unique shape context for 3d data description. In Proceedings of the ACM workshop on 3D object retrieval, pages 57\u201362, 2010.", "[61] Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai. End-to-end learning local multi-view descriptors for 3d point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1916\u20131925, 2020.", "[13] Andrew E. Johnson and Martial Hebert. Using spin images for efficient object recognition in cluttered 3d scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(5):433\u2013449, 1999.", "[17] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652\u2013660, 2017.", "[16] Federico Tombari, Samuele Salti, and Luigi di Stefano. Unique signatures of histograms for local surface description. In Proceedings of the European Conference on Computer Vision, pages 356\u2013369, 2010."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"2\">TE(cm)</td><td colspan=\"2\">RE({}^{\\circ})</td><td rowspan=\"2\">Recall(%)</td></tr><tr><td>AVG</td><td>STD</td><td>AVG</td><td>STD</td></tr><tr><td>FGR [65]</td><td>42.8</td><td>19.7</td><td>1.89</td><td>1.11</td><td>2.74</td></tr><tr><td>3DFeat-Net [18]</td><td>21.4</td><td>17.2</td><td>0.44</td><td>0.36</td><td>45.72</td></tr><tr><td>FCGF [19]</td><td>10.5</td><td>1.37</td><td>0.30</td><td>0.29</td><td>97.34</td></tr><tr><td>D3Feat [4]</td><td>6.54</td><td>1.06</td><td>0.33</td><td>0.23</td><td>97.81</td></tr><tr><td>SpinNet [62]</td><td>9.13</td><td>1.42</td><td>0.73</td><td>0.41</td><td>97.50</td></tr><tr><td>EDFNet</td><td>5.12</td><td>1.01</td><td>0.31</td><td>0.20</td><td>98.14</td></tr><tr><td>DGR [63]</td><td>4.01</td><td>0.82</td><td>0.23</td><td>0.14</td><td>97.94</td></tr><tr><td>PointDSC [66]</td><td>3.84</td><td>0.83</td><td>0.22</td><td>0.12</td><td>98.70</td></tr><tr><td>EDFNet+DGR</td><td>3.82</td><td>0.77</td><td>0.17</td><td>0.14</td><td>98.92</td></tr></table>", "caption": "TABLE II: Registration on the KITTI dataset. AVG indicates the average result and STD indicates the standard deviation.", "list_citation_info": ["[4] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, and ChiewLan Tai. D3feat: Joint learning of dense detection and description of 3d local features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6358\u20136366, 2020.", "[19] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional geometric features. In Proceedings of the IEEE International Conference on Computer Vision, pages 8958\u20138966, 2019.", "[62] Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, and Yulan Guo. Spinnet: Learning a general surface descriptor for 3d point cloud registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11753\u201311762, 2021.", "[66] Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, Zeyu Hu, Hongbo Fu, and Chiew-Lan Tai. Pointdsc: Robust point cloud registration using deep spatial consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15859\u201315869, 2021.", "[18] Zi Jian Yew and Gim Hee Lee. 3dfeat-net: Weakly supervised local 3d features for point cloud registration. In Proceedings of the European Conference on Computer Vision, pages 630\u2013646, 2018.", "[65] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global registration. In Proceedings of the European Conference on Computer Vision, pages 766\u2013782, 2016.", "[63] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep global registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2511\u20132520, 2020."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td>voxel</td><td colspan=\"2\">Gazebo</td><td colspan=\"2\">Wood</td><td rowspan=\"2\">AVG(%)</td></tr><tr><td>size (cm)</td><td>Sum.</td><td>Wint.</td><td>Sum.</td><td>Aut.</td></tr><tr><td>PerfectMatch [23]</td><td>6.25</td><td>91.3</td><td>84.1</td><td>67.8</td><td>72.8</td><td>79.0</td></tr><tr><td>FCGF [19]</td><td>5.00</td><td>22.8</td><td>10.0</td><td>17.8</td><td>16.8</td><td>16.1</td></tr><tr><td>D3Feat(rand) [4]</td><td>5.00</td><td>45.7</td><td>23.9</td><td>13.0</td><td>22.4</td><td>26.2</td></tr><tr><td>D3Feat(pred) [4]</td><td>5.00</td><td>78.9</td><td>62.6</td><td>45.2</td><td>37.6</td><td>56.3</td></tr><tr><td>D3Feat(pred) [4]</td><td>6.25</td><td>85.9</td><td>63.0</td><td>49.6</td><td>48.0</td><td>61.6</td></tr><tr><td>LMVD [61]</td><td>-</td><td>85.3</td><td>72.0</td><td>84.0</td><td>78.3</td><td>79.9</td></tr><tr><td>SpinNet [62]</td><td>-</td><td>92.9</td><td>91.7</td><td>92.2</td><td>94.4</td><td>92.8</td></tr><tr><td>DGR [63]</td><td>5.00</td><td>74.2</td><td>66.7</td><td>70.1</td><td>64.6</td><td>67.5</td></tr><tr><td>\\cdashline1-7[2.2pt/1.2pt]EDFNet(rand)</td><td>5.00</td><td>60.3</td><td>52.7</td><td>40.1</td><td>38.4</td><td>49.3</td></tr><tr><td>EDFNet(pred)</td><td>5.00</td><td>80.4</td><td>67.3</td><td>72.0</td><td>69.8</td><td>70.7</td></tr><tr><td>EDFNet(pred)</td><td>6.25</td><td>88.4</td><td>82.7</td><td>84.3</td><td>79.1</td><td>80.5</td></tr></table>", "caption": "TABLE III: Feature matching recall comparison on the ETH dataset, where \\tau_{1}=10cm, \\tau_{2}=5\\%. The model is trained on the 3DMatch dataset. Some results are copied from [4, 62].", "list_citation_info": ["[4] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, and ChiewLan Tai. D3feat: Joint learning of dense detection and description of 3d local features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6358\u20136366, 2020.", "[19] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional geometric features. In Proceedings of the IEEE International Conference on Computer Vision, pages 8958\u20138966, 2019.", "[62] Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, and Yulan Guo. Spinnet: Learning a general surface descriptor for 3d point cloud registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11753\u201311762, 2021.", "[23] Zan Gojcic, Caifa Zhou, Jan D. Wegner, and Andreas Wieser. The perfect match: 3d point cloud matching with smoothed densities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5545\u20135554, 2019.", "[63] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep global registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2511\u20132520, 2020.", "[61] Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai. End-to-end learning local multi-view descriptors for 3d point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1916\u20131925, 2020."]}, {"table": "<table><tr><td>Methods</td><td>Long Dress</td><td>Loot</td><td>Red and Black</td><td>Soldier</td></tr><tr><td>D3Feat [4]</td><td>0.041</td><td>0.033</td><td>0.037</td><td>0.055</td></tr><tr><td>PointNet [17]</td><td>0.064</td><td>0.057</td><td>0.061</td><td>0.072</td></tr><tr><td>S-CorrNet3D [68]</td><td>0.014</td><td>0.012</td><td>0.012</td><td>0.023</td></tr><tr><td>U-CorrNet3D [68]</td><td>0.057</td><td>0.051</td><td>0.055</td><td>0.063</td></tr><tr><td>S-F3D [71]</td><td>0.019</td><td>0.017</td><td>0.018</td><td>0.025</td></tr><tr><td>U-F3D [71]</td><td>0.061</td><td>0.052</td><td>0.054</td><td>0.066</td></tr><tr><td>SpinNet [62]</td><td>0.022</td><td>0.019</td><td>0.021</td><td>0.029</td></tr><tr><td>KPConv [3]</td><td>0.034</td><td>0.027</td><td>0.031</td><td>0.047</td></tr><tr><td>DGCNN [1]</td><td>0.047</td><td>0.042</td><td>0.045</td><td>0.059</td></tr><tr><td>\\cdashline1-5[2.2pt/1.2pt]EDFNet</td><td>0.012</td><td>0.010</td><td>0.010</td><td>0.019</td></tr></table>", "caption": "TABLE IV: Average Chamfer distance comparison on the 8iVFB dataset [72] involving four point cloud sequences.", "list_citation_info": ["[4] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, and ChiewLan Tai. D3feat: Joint learning of dense detection and description of 3d local features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6358\u20136366, 2020.", "[62] Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, and Yulan Guo. Spinnet: Learning a general surface descriptor for 3d point cloud registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11753\u201311762, 2021.", "[71] Xingyu Liu, Charles R Qi, and Leonidas J Guibas. Flownet3d: Learning scene flow in 3d point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 529\u2013537, 2019.", "[68] Yiming Zeng, Yue Qian, Zhiyu Zhu, Junhui Hou, Hui Yuan, and Ying He. Corrnet3d: Unsupervised end-to-end learning of dense correspondence for 3d point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6052\u20136061, 2021.", "[1] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics, 38(5):1\u201312, 2019.", "[72] Eugene d\u2019Eon, Bob Harrison, Taos Myers, and Philip A Chou. 8i voxelized full bodies-a voxelized point cloud dataset. ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG) input document WG11M40059/WG1M74006, 7(8):11, 2017.", "[3] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 6410\u20136419, 2019.", "[17] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652\u2013660, 2017."]}, {"table": "<table><tr><td>Methods</td><td>5000</td><td>2500</td><td>1000</td><td>500</td><td>250</td></tr><tr><td colspan=\"6\">Feature Matching Recall (%)</td></tr><tr><td>PerferctMatch [23]</td><td>94.7</td><td>94.2</td><td>92.6</td><td>90.1</td><td>82.9</td></tr><tr><td>FCGF [19]</td><td>95.2</td><td>95.5</td><td>94.6</td><td>93.0</td><td>89.9</td></tr><tr><td>D3Feat(rand) [4]</td><td>95.3</td><td>95.1</td><td>94.2</td><td>93.6</td><td>90.8</td></tr><tr><td>D3Feat(pred) [4]</td><td>95.8</td><td>95.6</td><td>94.6</td><td>94.3</td><td>93.3</td></tr><tr><td>\\cdashline1-6[2.2pt/1.2pt]EDFNet(rand)</td><td>95.8</td><td>95.6</td><td>95.3</td><td>96.2</td><td>93.3</td></tr><tr><td>EDFNet(pred)</td><td>97.5</td><td>96.5</td><td>95.7</td><td>96.2</td><td>95.5</td></tr><tr><td colspan=\"6\">Registration Recall (%)</td></tr><tr><td>PerferctMatch [23]</td><td>80.3</td><td>77.5</td><td>73.4</td><td>64.8</td><td>50.9</td></tr><tr><td>FCGF [19]</td><td>87.3</td><td>85.8</td><td>85.8</td><td>81.0</td><td>73.0</td></tr><tr><td>D3Feat(rand) [4]</td><td>83.5</td><td>82.1</td><td>81.7</td><td>77.6</td><td>68.8</td></tr><tr><td>D3Feat(pred) [4]</td><td>82.2</td><td>84.4</td><td>84.9</td><td>82.5</td><td>79.3</td></tr><tr><td>\\cdashline1-6[2.2pt/1.2pt]EDFNet(rand)</td><td>87.2</td><td>85.9</td><td>85.5</td><td>81.3</td><td>73.3</td></tr><tr><td>EDFNet(pred)</td><td>87.5</td><td>86.2</td><td>87.9</td><td>87.9</td><td>86.3</td></tr><tr><td colspan=\"6\">Inlier Ratio (%)</td></tr><tr><td>PerferctMatch [23]</td><td>37.7</td><td>34.5</td><td>28.3</td><td>23.0</td><td>19.1</td></tr><tr><td>FCGF [19]</td><td>56.9</td><td>54.5</td><td>49.1</td><td>43.3</td><td>34.7</td></tr><tr><td>D3Feat(rand) [4]</td><td>40.6</td><td>38.3</td><td>33.3</td><td>28.6</td><td>23.5</td></tr><tr><td>D3Feat(pred) [4]</td><td>40.7</td><td>40.6</td><td>42.7</td><td>44.1</td><td>45.0</td></tr><tr><td>\\cdashline1-6[2.2pt/1.2pt]EDFNet(rand)</td><td>52.8</td><td>51.0</td><td>45.3</td><td>43.1</td><td>39.7</td></tr><tr><td>EDFNet(pred)</td><td>55.2</td><td>54.5</td><td>51.7</td><td>49.3</td><td>48.8</td></tr></table>", "caption": "TABLE V: Performance comparison under different numbers of selected points.", "list_citation_info": ["[23] Zan Gojcic, Caifa Zhou, Jan D. Wegner, and Andreas Wieser. The perfect match: 3d point cloud matching with smoothed densities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5545\u20135554, 2019.", "[4] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, and ChiewLan Tai. D3feat: Joint learning of dense detection and description of 3d local features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6358\u20136366, 2020.", "[19] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional geometric features. In Proceedings of the IEEE International Conference on Computer Vision, pages 8958\u20138966, 2019."]}], "citation_info_to_title": {"[58] Marc Khoury, Qian-Yi Zhou, and Vladlen Koltun. Learning compact geometric features. In Proceedings of the IEEE International Conference on Computer Vision, pages 153\u2013161, 2017.": "Learning compact geometric features", "[60] Yongheng Zhao, Tolga Birdal, Haowen Deng, and Federico Tombari. 3d point capsule networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1009\u20131018, 2019.": "3D Point Capsule Networks", "[16] Federico Tombari, Samuele Salti, and Luigi di Stefano. Unique signatures of histograms for local surface description. In Proceedings of the European Conference on Computer Vision, pages 356\u2013369, 2010.": "Unique Signatures of Histograms for Local Surface Description", "[71] Xingyu Liu, Charles R Qi, and Leonidas J Guibas. Flownet3d: Learning scene flow in 3d point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 529\u2013537, 2019.": "Flownet3d: Learning scene flow in 3d point clouds", "[63] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep global registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2511\u20132520, 2020.": "Deep global registration", "[4] Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, and ChiewLan Tai. D3feat: Joint learning of dense detection and description of 3d local features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6358\u20136366, 2020.": "D3feat: Joint learning of dense detection and description of 3d local features", "[68] Yiming Zeng, Yue Qian, Zhiyu Zhu, Junhui Hou, Hui Yuan, and Ying He. Corrnet3d: Unsupervised end-to-end learning of dense correspondence for 3d point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6052\u20136061, 2021.": "Corrnet3d: Unsupervised end-to-end learning of dense correspondence for 3d point clouds", "[62] Sheng Ao, Qingyong Hu, Bo Yang, Andrew Markham, and Yulan Guo. Spinnet: Learning a general surface descriptor for 3d point cloud registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11753\u201311762, 2021.": "Spinnet: Learning a general surface descriptor for 3d point cloud registration", "[59] Haowen Deng, Tolga Birdal, and Slobodan Ilic. 3d local features for direct pairwise registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3244\u20133253, 2019.": "3D Local Features for Direct Pairwise Registration", "[66] Xuyang Bai, Zixin Luo, Lei Zhou, Hongkai Chen, Lei Li, Zeyu Hu, Hongbo Fu, and Chiew-Lan Tai. Pointdsc: Robust point cloud registration using deep spatial consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15859\u201315869, 2021.": "PointDSC: Robust Point Cloud Registration Using Deep Spatial Consistency", "[36] Federico Tombari, Samuele Salti, and Luigi di Stefano. Unique shape context for 3d data description. In Proceedings of the ACM workshop on 3D object retrieval, pages 57\u201362, 2010.": "Unique Shape Context for 3D Data Description", "[1] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics, 38(5):1\u201312, 2019.": "Dynamic Graph CNN for Learning on Point Clouds", "[24] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppfnet: Global context aware local features for robust 3d point matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 195\u2013205, 2018.": "Ppfnet: Global context aware local features for robust 3d point matching", "[19] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional geometric features. In Proceedings of the IEEE International Conference on Computer Vision, pages 8958\u20138966, 2019.": "Fully Convolutional Geometric Features", "[46] Andy Zeng, Shuran Song, Matthias Nie\u00dfner, Matthew Fisher, Jianxiong Xiao, and Thomas A. Funkhouser. 3dmatch: Learning local geometric descriptors from RGB-D reconstructions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 199\u2013208, 2017.": "3dmatch: Learning local geometric descriptors from RGB-D reconstructions", "[13] Andrew E. Johnson and Martial Hebert. Using spin images for efficient object recognition in cluttered 3d scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(5):433\u2013449, 1999.": "Using Spin Images for Efficient Object Recognition in Cluttered 3D Scenes", "[3] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. In Proceedings of the IEEE International Conference on Computer Vision, pages 6410\u20136419, 2019.": "Kpconv: Flexible and deformable convolution for point clouds", "[65] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global registration. In Proceedings of the European Conference on Computer Vision, pages 766\u2013782, 2016.": "Fast global registration", "[23] Zan Gojcic, Caifa Zhou, Jan D. Wegner, and Andreas Wieser. The perfect match: 3d point cloud matching with smoothed densities. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5545\u20135554, 2019.": "The perfect match: 3d point cloud matching with smoothed densities", "[61] Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai. End-to-end learning local multi-view descriptors for 3d point clouds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1916\u20131925, 2020.": "End-to-end learning local multi-view descriptors for 3d point clouds", "[25] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-foldnet: Unsuper- vised learning of rotation invariant 3d local descriptors. In Proceedings of the European Conference on Computer Vision, pages 620\u2013638, 2018.": "Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors", "[18] Zi Jian Yew and Gim Hee Lee. 3dfeat-net: Weakly supervised local 3d features for point cloud registration. In Proceedings of the European Conference on Computer Vision, pages 630\u2013646, 2018.": "3dfeat-net: Weakly supervised local 3d features for point cloud registration", "[15] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast point feature histograms (fpfh) for 3d registration. In Proceedings of the International Conference on Robotics and Automation, pages 3212\u20133217, 2009.": "Fast Point Feature Histograms (FPFH) for 3D Registration", "[72] Eugene d\u2019Eon, Bob Harrison, Taos Myers, and Philip A Chou. 8i voxelized full bodies-a voxelized point cloud dataset. ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG) input document WG11M40059/WG1M74006, 7(8):11, 2017.": "8i Voxelized Full Bodies - A Voxelized Point Cloud Dataset", "[17] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652\u2013660, 2017.": "Pointnet: Deep learning on point sets for 3d classification and segmentation"}, "source_title_to_arxiv_id": {"Deep global registration": "2004.11540"}}