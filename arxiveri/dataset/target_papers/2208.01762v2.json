{"title": "Robust RGB-D Fusion for Saliency Detection", "abstract": "Efficiently exploiting multi-modal inputs for accurate RGB-D saliency\ndetection is a topic of high interest. Most existing works leverage cross-modal\ninteractions to fuse the two streams of RGB-D for intermediate features'\nenhancement. In this process, a practical aspect of the low quality of the\navailable depths has not been fully considered yet. In this work, we aim for\nRGB-D saliency detection that is robust to the low-quality depths which\nprimarily appear in two forms: inaccuracy due to noise and the misalignment to\nRGB. To this end, we propose a robust RGB-D fusion method that benefits from\n(1) layer-wise, and (2) trident spatial, attention mechanisms. On the one hand,\nlayer-wise attention (LWA) learns the trade-off between early and late fusion\nof RGB and depth features, depending upon the depth accuracy. On the other\nhand, trident spatial attention (TSA) aggregates the features from a wider\nspatial context to address the depth misalignment problem. The proposed LWA and\nTSA mechanisms allow us to efficiently exploit the multi-modal inputs for\nsaliency detection while being robust against low-quality depths. Our\nexperiments on five benchmark datasets demonstrate that the proposed fusion\nmethod performs consistently better than the state-of-the-art fusion\nalternatives.", "authors": ["Zongwei Wu", "Shriarulmozhivarman Gobichettipalayam", "Brahim Tamadazte", "Guillaume Allibert", "Danda Pani Paudel", "C\u00e9dric Demonceaux"], "published_date": "2022_08_02", "pdf_url": "http://arxiv.org/pdf/2208.01762v2", "list_table_and_caption": [{"table": "<table><tr><td>Dataset</td><td>Size\\downarrow</td><td colspan=\"4\">DES</td><td colspan=\"4\">NLPR</td><td colspan=\"4\">NJU2K</td><td colspan=\"4\">STERE</td><td colspan=\"4\">SIP</td></tr><tr><td>Metric</td><td>(\\Delta Mb)</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td></tr><tr><td>BBS [6]</td><td>460(+95)</td><td>.015</td><td>.946</td><td>.941</td><td>.976</td><td>.023</td><td>.920</td><td>.923</td><td>.953</td><td>.035</td><td>.924</td><td>.915</td><td>.944</td><td>.040</td><td>.913</td><td>.902</td><td>.935</td><td>.053</td><td>.904</td><td>.877</td><td>.912</td></tr><tr><td>DFM[56]</td><td>495(+130)</td><td>.015</td><td>.946</td><td>.941</td><td>.974</td><td>.022</td><td>.919</td><td>.926</td><td>.956</td><td>.034</td><td>.922</td><td>.917</td><td>.943</td><td>.041</td><td>.909</td><td>.902</td><td>.933</td><td>.049</td><td>.909</td><td>.885</td><td>.919</td></tr><tr><td>CDI[52]</td><td>520(+155)</td><td>.023</td><td>.919</td><td>.914</td><td>.950</td><td>.024</td><td>.918</td><td>.921</td><td>.952</td><td>.035</td><td>.927</td><td>.915</td><td>.944</td><td>.036</td><td>.918</td><td>.910</td><td>.941</td><td>.055</td><td>.900</td><td>.870</td><td>.911</td></tr><tr><td>DCF[12]</td><td>336(-29)</td><td>.015</td><td>.944</td><td>.938</td><td>.976</td><td>.020</td><td>.927</td><td>.931</td><td>.960</td><td>.030</td><td>.930</td><td>.924</td><td>.949</td><td>.038</td><td>.913</td><td>.904</td><td>.937</td><td>.044</td><td>.913</td><td>.891</td><td>.928</td></tr><tr><td>SPNet[62]</td><td>593(+228)</td><td>.016</td><td>.944</td><td>.936</td><td>.973</td><td>.022</td><td>.924</td><td>.925</td><td>.956</td><td>.032</td><td>.928</td><td>.919</td><td>.945</td><td>.038</td><td>.913</td><td>.904</td><td>.938</td><td>.048</td><td>.907</td><td>.884</td><td>.921</td></tr><tr><td>MobSal[45]</td><td>723(+358)</td><td>.015</td><td>.945</td><td>.940</td><td>.976</td><td>.024</td><td>.924</td><td>.923</td><td>.955</td><td>.033</td><td>.926</td><td>.915</td><td>.945</td><td>.038</td><td>.913</td><td>.902</td><td>.937</td><td>.042</td><td>.915</td><td>.892</td><td>.930</td></tr><tr><td>Ours</td><td>365</td><td>.015</td><td>.946</td><td>.941</td><td>.977</td><td>.020</td><td>.932</td><td>.931</td><td>.962</td><td>.029</td><td>.936</td><td>.926</td><td>.951</td><td>.035</td><td>.921</td><td>.911</td><td>.944</td><td>.042</td><td>.916</td><td>.893</td><td>.931</td></tr></table>", "caption": "Table 1: Quantitative comparison with different fusion designs. We replace our fusion module with five SOTA fusion modules and retrain the new networks with the same training setting. \\uparrow (\\downarrow) denotes that the higher (lower) is better.", "list_citation_info": ["[62] Tao Zhou, Huazhu Fu, Geng Chen, Yi Zhou, Deng-Ping Fan, and Ling Shao. Specificity-preserving RGB-D saliency detection. In Poceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[56] Wenbo Zhang, Ge-Peng Ji, Zhuo Wang, Keren Fu, and Qijun Zhao. Depth quality-inspired feature manipulation for efficient RGB-D salient object detection. In Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021.", "[52] Chen Zhang, Runmin Cong, Qinwei Lin, Lin Ma, Feng Li, Yao Zhao, and Sam Kwong. Cross-modality discrepant interaction network for RGB-D salient object detection. In Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021.", "[12] Wei Ji, Jingjing Li, Shuang Yu, Miao Zhang, Yongri Piao, Shunyu Yao, Qi Bi, Kai Ma, Yefeng Zheng, Huchuan Lu, et al. Calibrated RGB-D salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[45] Yu-Huan Wu, Yun Liu, Jun Xu, Jia-Wang Bian, Yu-Chao Gu, and Ming-Ming Cheng. Mobilesal: Extremely efficient rgb-d salient object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[6] Deng-Ping Fan, Yingjie Zhai, Ali Borji, Jufeng Yang, and Ling Shao. BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network. In Proceedings of the European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><tr><td></td><td></td><td colspan=\"16\">Benchmarks with challenging depth</td><td colspan=\"12\">Benchmarks with less noisy depth</td></tr><tr><td>Dataset</td><td>Size\\downarrow</td><td colspan=\"4\">NLPR</td><td colspan=\"4\">NJU2K</td><td colspan=\"4\">STERE</td><td colspan=\"4\">AvgMetric</td><td colspan=\"4\">DES</td><td colspan=\"4\">SIP</td><td colspan=\"4\">AvgMetric</td></tr><tr><td>Metric</td><td>(Mb)</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td><td>M\\downarrow</td><td>F\\uparrow</td><td>S\\uparrow</td><td>E\\uparrow</td></tr><tr><td>CPFP_{19} [59]</td><td>278</td><td>.036</td><td>.867</td><td>.888</td><td>.932</td><td>.053</td><td>.877</td><td>.878</td><td>.923</td><td>.051</td><td>.874</td><td>.879</td><td>.925</td><td>.049</td><td>.873</td><td>.880</td><td>.925</td><td>.038</td><td>.846</td><td>.872</td><td>.923</td><td>.064</td><td>.851</td><td>.850</td><td>.903</td><td>.060</td><td>.850</td><td>.852</td><td>.905</td></tr><tr><td>DMRA_{19} [33]</td><td>238</td><td>.031</td><td>.879</td><td>.899</td><td>.947</td><td>.051</td><td>.886</td><td>.886</td><td>.927</td><td>.047</td><td>.886</td><td>.886</td><td>.938</td><td>.045</td><td>.884</td><td>.888</td><td>.936</td><td>.030</td><td>.888</td><td>.900</td><td>.943</td><td>.085</td><td>.821</td><td>.806</td><td>.875</td><td>.078</td><td>.829</td><td>.817</td><td>.883</td></tr><tr><td>A2dele_{20} [34]</td><td>116</td><td>.029</td><td>.882</td><td>.898</td><td>.944</td><td>.051</td><td>.874</td><td>.871</td><td>.916</td><td>.044</td><td>.879</td><td>.878</td><td>.928</td><td>.043</td><td>.878</td><td>.879</td><td>.927</td><td>.029</td><td>.872</td><td>.886</td><td>.920</td><td>.070</td><td>.833</td><td>.828</td><td>.889</td><td>.060</td><td>.850</td><td>.852</td><td>.905</td></tr><tr><td>JLDCF_{20} [9]</td><td>548</td><td>.022</td><td>.916</td><td>.925</td><td>.962</td><td>.043</td><td>.903</td><td>.903</td><td>.944</td><td>.042</td><td>.901</td><td>.905</td><td>.946</td><td>.038</td><td>.904</td><td>.907</td><td>.948</td><td>.022</td><td>.919</td><td>.929</td><td>.968</td><td>.051</td><td>.885</td><td>.879</td><td>.923</td><td>.047</td><td>.889</td><td>.885</td><td>.928</td></tr><tr><td>CMMS_{20} [17]</td><td>546</td><td>.027</td><td>.896</td><td>.915</td><td>.949</td><td>.044</td><td>.897</td><td>.900</td><td>.936</td><td>.043</td><td>.893</td><td>.895</td><td>.939</td><td>.040</td><td>.894</td><td>.899</td><td>.939</td><td>.018</td><td>.930</td><td>.937</td><td>.976</td><td>.058</td><td>.877</td><td>.872</td><td>.911</td><td>.052</td><td>.883</td><td>.880</td><td>.918</td></tr><tr><td>CoNet_{20} [13]</td><td>162</td><td>.031</td><td>.887</td><td>.908</td><td>.945</td><td>.046</td><td>.893</td><td>.895</td><td>.937</td><td>.040</td><td>.905</td><td>.908</td><td>.949</td><td>.040</td><td>.898</td><td>.904</td><td>.945</td><td>.028</td><td>.896</td><td>.909</td><td>.945</td><td>.063</td><td>.867</td><td>.858</td><td>.913</td><td>.058</td><td>.870</td><td>.864</td><td>.917</td></tr><tr><td>DANet_{20} [60]</td><td>128</td><td>.028</td><td>.916</td><td>.915</td><td>.953</td><td>.045</td><td>.910</td><td>.899</td><td>.935</td><td>.043</td><td>.892</td><td>.901</td><td>.937</td><td>.041</td><td>.901</td><td>.902</td><td>.939</td><td>.023</td><td>.928</td><td>.924</td><td>.968</td><td>.054</td><td>.892</td><td>.875</td><td>.918</td><td>.050</td><td>.896</td><td>.881</td><td>.924</td></tr><tr><td>DASNet_{20} [57]</td><td>-</td><td>.021</td><td>.929</td><td>.929</td><td>-</td><td>.042</td><td>.911</td><td>.902</td><td>-</td><td>.037</td><td>.915</td><td>.910</td><td>-</td><td>.035</td><td>.916</td><td>.910</td><td>-</td><td>.023</td><td>.928</td><td>.908</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HDFNet_{20} [30]</td><td>308</td><td>.031</td><td>.839</td><td>.898</td><td>.942</td><td>.051</td><td>.847</td><td>.885</td><td>.920</td><td>.039</td><td>.863</td><td>.906</td><td>.937</td><td>.041</td><td>.854</td><td>.898</td><td>.933</td><td>.030</td><td>.843</td><td>.899</td><td>.944</td><td>.050</td><td>.904</td><td>.878</td><td>.920</td><td>.047</td><td>.896</td><td>.880</td><td>.923</td></tr><tr><td>BBSNet_{20} [6]</td><td>200</td><td>.023</td><td>.918</td><td>.930</td><td>.961</td><td>.035</td><td>.920</td><td>.921</td><td>.949</td><td>.041</td><td>.903</td><td>.908</td><td>.942</td><td>.036</td><td>.910</td><td>.915</td><td>.947</td><td>.021</td><td>.927</td><td>.933</td><td>.966</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DCF_{21} [12]</td><td>435</td><td>.021</td><td>.891</td><td>-</td><td>.957</td><td>.035</td><td>.902</td><td>-</td><td>.924</td><td>.039</td><td>.885</td><td>-</td><td>.927</td><td>.034</td><td>.890</td><td>-</td><td>.931</td><td>-</td><td>-</td><td>-</td><td>-</td><td>.051</td><td>.875</td><td>-</td><td>.920</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>D3Net_{21} [5]</td><td>518</td><td>.030</td><td>.897</td><td>.912</td><td>.953</td><td>.041</td><td>.900</td><td>.900</td><td>.950</td><td>.046</td><td>.891</td><td>.899</td><td>.938</td><td>.041</td><td>.894</td><td>.901</td><td>.943</td><td>.031</td><td>.885</td><td>.898</td><td>.946</td><td>.063</td><td>.861</td><td>.860</td><td>.909</td><td>.058</td><td>.864</td><td>.864</td><td>.913</td></tr><tr><td>DSA2F_{21} [39]</td><td>-</td><td>.024</td><td>.897</td><td>.918</td><td>.950</td><td>.039</td><td>.901</td><td>.903</td><td>.923</td><td>.036</td><td>.898</td><td>.904</td><td>.933</td><td>.034</td><td>.898</td><td>.906</td><td>.933</td><td>.021</td><td>.896</td><td>.920</td><td>.962</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TriTrans_{21} [26]</td><td>927</td><td>.020</td><td>.923</td><td>.928</td><td>.960</td><td>.030</td><td>.926</td><td>.920</td><td>.925</td><td>.033</td><td>.911</td><td>.908</td><td>.927</td><td>.030</td><td>.917</td><td>.914</td><td>.931</td><td>.014</td><td>.940</td><td>.943</td><td>.981</td><td>.043</td><td>.898</td><td>.886</td><td>.924</td><td>.039</td><td>-</td><td>.893</td><td>.931</td></tr><tr><td>CDINet_{21} [52]</td><td>217</td><td>.024</td><td>.916</td><td>.927</td><td>-</td><td>.035</td><td>.922</td><td>.919</td><td>-</td><td>.041</td><td>.903</td><td>.906</td><td>-</td><td>.036</td><td>.910</td><td>.913</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SPNet_{21}[62]</td><td>702</td><td>.021</td><td>.925</td><td>.927</td><td>.959</td><td>.028</td><td>.935</td><td>.925</td><td>.954</td><td>.037</td><td>.915</td><td>.907</td><td>.944</td><td>.031</td><td>.922</td><td>.915</td><td>.949</td><td>.014</td><td>.950</td><td>.945</td><td>.980</td><td>.043</td><td>.916</td><td>.894</td><td>.930</td><td>.039</td><td>.920</td><td>.900</td><td>.936</td></tr><tr><td>RFNet (ours)</td><td>364</td><td>.020</td><td>.932</td><td>.931</td><td>.962</td><td>.029</td><td>.936</td><td>.926</td><td>.951</td><td>.035</td><td>.921</td><td>.911</td><td>.944</td><td>.030</td><td>.927</td><td>.918</td><td>.948</td><td>.015</td><td>.946</td><td>.941</td><td>.977</td><td>.042</td><td>.916</td><td>.893</td><td>.931</td><td>.038</td><td>.919</td><td>.899</td><td>.936</td></tr></table>", "caption": "Table 2: Quantitative comparison with state-of-the-art models. \\uparrow (\\downarrow) denotes that the higher (lower) is better. The best and second best are highlighted in bold and underline, respectively. We further report the average metric (AvgMetric) for datasets with more challenging depths and with less noisy depths.", "list_citation_info": ["[62] Tao Zhou, Huazhu Fu, Geng Chen, Yi Zhou, Deng-Ping Fan, and Ling Shao. Specificity-preserving RGB-D saliency detection. In Poceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[60] Xiaoqi Zhao, Lihe Zhang, Youwei Pang, Huchuan Lu, and Lei Zhang. A single stream network for robust and real-time RGB-D salient object detection. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[33] Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, and Huchuan Lu. Depth-induced multi-scale recurrent attention network for saliency detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.", "[30] Youwei Pang, Lihe Zhang, Xiaoqi Zhao, and Huchuan Lu. Hierarchical dynamic filtering network for RGB-D salient object detection. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[26] Zhengyi Liu, Wang Yuan, Zhengzheng Tu, Yun Xiao, and Bin Tang. TriTransNet: RGB-D salient object detection with a triplet transformer embedding network. Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021.", "[9] Keren Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao. JL-DCF: Joint learning and densely-cooperative fusion framework for RGB-D salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[17] Chongyi Li, Runmin Cong, Yongri Piao, Qianqian Xu, and Chen Change Loy. RGB-D salient object detection with cross-modality modulation and selection. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[39] Peng Sun, Wenhu Zhang, Huanyu Wang, Songyuan Li, and Xi Li. Deep RGB-D saliency detection with depth-sensitive attention and automatic multi-modal fusion. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[57] Jiawei Zhao, Yifan Zhao, Jia Li, and Xiaowu Chen. Is depth really necessary for salient object detection? In Proceedings of the 28th ACM International Conference on Multimedia (ACM MM), 2020.", "[52] Chen Zhang, Runmin Cong, Qinwei Lin, Lin Ma, Feng Li, Yao Zhao, and Sam Kwong. Cross-modality discrepant interaction network for RGB-D salient object detection. In Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021.", "[5] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE Transactions on neural networks and learning systems (TNNLS), 32(5):2075\u20132089, 2021.", "[34] Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren, and Huchuan Lu. A2dele: Adaptive and attentive depth distiller for efficient RGB-D salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[13] Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan Lu. Accurate RGB-D salient object detection via collaborative learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.", "[12] Wei Ji, Jingjing Li, Shuang Yu, Miao Zhang, Yongri Piao, Shunyu Yao, Qi Bi, Kai Ma, Yefeng Zheng, Huchuan Lu, et al. Calibrated RGB-D salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2021.", "[59] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming Cheng, Xuan-Yi Li, and Le Zhang. Contrast prior and fluid pyramid integration for rgbd salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[6] Deng-Ping Fan, Yingjie Zhai, Ali Borji, Jufeng Yang, and Ling Shao. BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network. In Proceedings of the European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><tr><td rowspan=\"2\"><p>B</p></td><td colspan=\"4\">CRM</td><td><p>DFM</p></td><td><p>LWA</p></td><td><p>Size</p></td><td colspan=\"4\">Overall Metric</td></tr><tr><td><p>CA</p></td><td><p>TSA</p></td><td><p>SA</p></td><td><p>\\alpha,\\beta</p></td><td><p>([56])</p></td><td></td><td><p>Mb\\downarrow</p></td><td><p>M\\downarrow</p></td><td><p>F\\uparrow</p></td><td><p>S\\uparrow</p></td><td><p>E\\uparrow</p></td></tr><tr><td><p>\u2713</p></td><td></td><td></td><td></td><td></td><td></td><td></td><td>305</td><td><p>.039</p></td><td><p>.915</p></td><td><p>.904</p></td><td><p>.935</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td></td><td></td><td></td><td></td><td><p>336</p></td><td><p>.035</p></td><td><p>.918</p></td><td><p>.907</p></td><td><p>.940</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td></td><td></td><td></td><td><p>364</p></td><td><p>.035</p></td><td><p>.923</p></td><td><p>.910</p></td><td><p>.943</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>\u2713</p></td><td></td><td></td><td></td><td><p>363</p></td><td><p>.035</p></td><td><p>.920</p></td><td><p>.908</p></td><td><p>.941</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>\u2713</p></td><td></td><td></td><td><p>364</p></td><td><p>.034</p></td><td>.924</td><td><p>.910</p></td><td><p>.943</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>364</p></td><td><p>.035</p></td><td><p>.921</p></td><td><p>.908</p></td><td><p>.941</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>\u2713</p></td><td></td><td><p>\u2713</p></td><td><p>365</p></td><td>.033</td><td>.924</td><td>.911</td><td>.944</td></tr></table>", "caption": "Table 3: Ablation study on key components. B stands for the baseline performance where RGB-D features are merged through simple addition without any form of attention.", "list_citation_info": ["[56] Wenbo Zhang, Ge-Peng Ji, Zhuo Wang, Keren Fu, and Qijun Zhao. Depth quality-inspired feature manipulation for efficient RGB-D salient object detection. In Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021."]}], "citation_info_to_title": {"[45] Yu-Huan Wu, Yun Liu, Jun Xu, Jia-Wang Bian, Yu-Chao Gu, and Ming-Ming Cheng. Mobilesal: Extremely efficient rgb-d salient object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.": "Mobilesal: Extremely efficient RGB-D Salient Object Detection", "[52] Chen Zhang, Runmin Cong, Qinwei Lin, Lin Ma, Feng Li, Yao Zhao, and Sam Kwong. Cross-modality discrepant interaction network for RGB-D salient object detection. In Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021.": "Cross-modality discrepant interaction network for RGB-D salient object detection", "[6] Deng-Ping Fan, Yingjie Zhai, Ali Borji, Jufeng Yang, and Ling Shao. BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.": "BBS-Net: RGB-D salient object detection with a bifurcated backbone strategy network", "[30] Youwei Pang, Lihe Zhang, Xiaoqi Zhao, and Huchuan Lu. Hierarchical dynamic filtering network for RGB-D salient object detection. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.": "Hierarchical dynamic filtering network for RGB-D salient object detection", "[26] Zhengyi Liu, Wang Yuan, Zhengzheng Tu, Yun Xiao, and Bin Tang. TriTransNet: RGB-D salient object detection with a triplet transformer embedding network. Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021.": "TriTransNet: RGB-D salient object detection with a triplet transformer embedding network", "[33] Yongri Piao, Wei Ji, Jingjing Li, Miao Zhang, and Huchuan Lu. Depth-induced multi-scale recurrent attention network for saliency detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.": "Depth-Induced Multi-Scale Recurrent Attention Network for Saliency Detection", "[57] Jiawei Zhao, Yifan Zhao, Jia Li, and Xiaowu Chen. Is depth really necessary for salient object detection? In Proceedings of the 28th ACM International Conference on Multimedia (ACM MM), 2020.": "Is depth really necessary for salient object detection?", "[59] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming Cheng, Xuan-Yi Li, and Le Zhang. Contrast prior and fluid pyramid integration for rgbd salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection", "[5] Deng-Ping Fan, Zheng Lin, Zhao Zhang, Menglong Zhu, and Ming-Ming Cheng. Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks. IEEE Transactions on neural networks and learning systems (TNNLS), 32(5):2075\u20132089, 2021.": "Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks", "[62] Tao Zhou, Huazhu Fu, Geng Chen, Yi Zhou, Deng-Ping Fan, and Ling Shao. Specificity-preserving RGB-D saliency detection. In Poceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.": "Specificity-preserving RGB-D saliency detection", "[56] Wenbo Zhang, Ge-Peng Ji, Zhuo Wang, Keren Fu, and Qijun Zhao. Depth quality-inspired feature manipulation for efficient RGB-D salient object detection. In Poceedings of the 29th ACM International Conference on Multimedia (ACM MM), 2021.": "Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection", "[9] Keren Fu, Deng-Ping Fan, Ge-Peng Ji, and Qijun Zhao. JL-DCF: Joint learning and densely-cooperative fusion framework for RGB-D salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "JL-DCF: Joint learning and densely-cooperative fusion framework for RGB-D salient object detection", "[13] Wei Ji, Jingjing Li, Miao Zhang, Yongri Piao, and Huchuan Lu. Accurate RGB-D salient object detection via collaborative learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.": "Accurate RGB-D salient object detection via collaborative learning", "[60] Xiaoqi Zhao, Lihe Zhang, Youwei Pang, Huchuan Lu, and Lei Zhang. A single stream network for robust and real-time RGB-D salient object detection. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.": "A single stream network for robust and real-time RGB-D salient object detection", "[34] Yongri Piao, Zhengkun Rong, Miao Zhang, Weisong Ren, and Huchuan Lu. A2dele: Adaptive and attentive depth distiller for efficient RGB-D salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "A2dele: Adaptive and attentive depth distiller for efficient RGB-D salient object detection", "[17] Chongyi Li, Runmin Cong, Yongri Piao, Qianqian Xu, and Chen Change Loy. RGB-D salient object detection with cross-modality modulation and selection. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.": "RGB-D Salient Object Detection with Cross-Modality Modulation and Selection", "[12] Wei Ji, Jingjing Li, Shuang Yu, Miao Zhang, Yongri Piao, Shunyu Yao, Qi Bi, Kai Ma, Yefeng Zheng, Huchuan Lu, et al. Calibrated RGB-D salient object detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2021.": "Calibrated RGB-D salient object detection", "[39] Peng Sun, Wenhu Zhang, Huanyu Wang, Songyuan Li, and Xi Li. Deep RGB-D saliency detection with depth-sensitive attention and automatic multi-modal fusion. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2021.": "Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion"}, "source_title_to_arxiv_id": {"Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection": "2107.01779"}}