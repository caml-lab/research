{"title": "im2nerf: Image to Neural Radiance Field in the Wild", "abstract": "We propose im2nerf, a learning framework that predicts a continuous neural\nobject representation given a single input image in the wild, supervised by\nonly segmentation output from off-the-shelf recognition methods. The standard\napproach to constructing neural radiance fields takes advantage of multi-view\nconsistency and requires many calibrated views of a scene, a requirement that\ncannot be satisfied when learning on large-scale image data in the wild. We\ntake a step towards addressing this shortcoming by introducing a model that\nencodes the input image into a disentangled object representation that contains\na code for object shape, a code for object appearance, and an estimated camera\npose from which the object image is captured. Our model conditions a NeRF on\nthe predicted object representation and uses volume rendering to generate\nimages from novel views. We train the model end-to-end on a large collection of\ninput images. As the model is only provided with single-view images, the\nproblem is highly under-constrained. Therefore, in addition to using a\nreconstruction loss on the synthesized input view, we use an auxiliary\nadversarial loss on the novel rendered views. Furthermore, we leverage object\nsymmetry and cycle camera pose consistency. We conduct extensive quantitative\nand qualitative experiments on the ShapeNet dataset as well as qualitative\nexperiments on Open Images dataset. We show that in all cases, im2nerf achieves\nthe state-of-the-art performance for novel view synthesis from a single-view\nunposed image in the wild.", "authors": ["Lu Mi", "Abhijit Kundu", "David Ross", "Frank Dellaert", "Noah Snavely", "Alireza Fathi"], "published_date": "2022_09_08", "pdf_url": "http://arxiv.org/pdf/2209.04061v1", "list_table_and_caption": [{"table": "<table><thead><tr><th colspan=\"2\">Supervision</th><th colspan=\"2\">chair</th><th colspan=\"2\">car</th><th colspan=\"2\">airplane</th></tr><tr><th>Labeled</th><th>Unlabeled</th><th>PSNR</th><th>SSIM</th><th>PSNR</th><th>SSIM</th><th>PSNR</th><th>SSIM</th></tr></thead><tbody><tr><th>100%</th><th>0%</th><td>18.9</td><td>0.77</td><td>20.1</td><td>0.83</td><td>25.2</td><td>0.93</td></tr><tr><th>1%</th><th>99%</th><td>18.6</td><td>0.77</td><td>19.5</td><td>0.82</td><td>24.8</td><td>0.92</td></tr><tr><th>1%</th><th>0%</th><td>17.1</td><td>0.72</td><td>19.5</td><td>0.81</td><td>23.7</td><td>0.90</td></tr><tr><th>0%</th><th>100%</th><td>16.4</td><td>0.72</td><td>18.6</td><td>0.79</td><td>22.2</td><td>0.87</td></tr></tbody></table>", "caption": "Table 1: We perform an ablation study by training our model with different levels of supervision on chair, car, airplane in ShapeNet 3D-R2N2 [5]. This study demonstrates that using only 1\\% of pose ground truth under the weakly supervised case, and utilizing unlabeled images, leads to performance comparable to fully supervision (100\\%) of pose.", "list_citation_info": ["[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Proceedings of the European Conference on Computer Vision (ECCV), 2016."]}, {"table": "<table><thead><tr><th>model</th><th>HoloGAN</th><th>PrGAN</th><th>Shelf-Supervised</th><th>im2nerf(ours)</th></tr></thead><tbody><tr><th>chair</th><td>13.2</td><td>13.8</td><td>13.9</td><td>16.4</td></tr><tr><th>car</th><td>16.2</td><td>16.7</td><td>16.9</td><td>18.6</td></tr><tr><th>airplane</th><td>16.0</td><td>16.3</td><td>16.4</td><td>22.2</td></tr></tbody></table>", "caption": "Table 3: We compare our method quantitatively with baselines including Shelf-Supervised [48], HoloGAN [29], PrGAN [7] evaluated on the unsupervised condition for ShapeNet [5] chair, car, and airplane. We use the average PSNR of the synthesized novel view images as the comparison metric.", "list_citation_info": ["[48] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-supervised mesh prediction in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8843\u20138852, 2021.", "[7] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape induction from 2d views of multiple objects. In 2017 International Conference on 3D Vision (3DV), pages 402\u2013411. IEEE, 2017.", "[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.", "[29] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan: Unsupervised learning of 3d representations from natural images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7588\u20137597, 2019."]}, {"table": "<table><thead><tr><th></th><th>Category</th><th colspan=\"2\">Chair</th><th colspan=\"2\">Car</th><th colspan=\"2\">Airplane</th></tr><tr><th>View</th><th>Pose</th><th>PSNR</th><th>SSIM</th><th>PSNR</th><th>SSIM</th><th>PSNR</th><th>SSIM</th></tr></thead><tbody><tr><th>multi-</th><th>input</th><td>20.8</td><td>0.83</td><td>22.2</td><td>0.86</td><td>25.8</td><td>0.94</td></tr><tr><th rowspan=\"3\">single-</th><th>input</th><td>19.3</td><td>0.80</td><td>21.0</td><td>0.85</td><td>25.4</td><td>0.93</td></tr><tr><th>fully-supervised</th><td>19.0</td><td>0.77</td><td>20.1</td><td>0.83</td><td>25.2</td><td>0.93</td></tr><tr><th>weakly-supervised</th><td>18.6</td><td>0.77</td><td>19.5</td><td>0.82</td><td>24.8</td><td>0.92</td></tr><tr><th></th><th>unsupervised</th><td>16.4</td><td>0.72</td><td>17.2</td><td>0.74</td><td>22.2</td><td>0.87</td></tr></tbody></table>", "caption": "Table 4: Comparison between model with multi-view v.s. single-view supervision, and whether pose is directly given as input or predict from the single-view image input given different levels of supervision (fully-supervised 100% v.s. weakly-supervised 1% v.s. unsupervised 0%) on ShapeNet 3D-R2N2 dataset [5] chair, car and airplane.", "list_citation_info": ["[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Proceedings of the European Conference on Computer Vision (ECCV), 2016."]}], "citation_info_to_title": {"[7] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape induction from 2d views of multiple objects. In 2017 International Conference on 3D Vision (3DV), pages 402\u2013411. IEEE, 2017.": "3D Shape Induction from 2D Views of Multiple Objects", "[5] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.": "3D-R2N2: A Unified Approach for Single and Multi-View 3D Object Reconstruction", "[48] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-supervised mesh prediction in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8843\u20138852, 2021.": "Shelf-supervised mesh prediction in the wild", "[29] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan: Unsupervised learning of 3d representations from natural images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7588\u20137597, 2019.": "Hologan: Unsupervised learning of 3d representations from natural images"}, "source_title_to_arxiv_id": {"Shelf-supervised mesh prediction in the wild": "2102.06195"}}