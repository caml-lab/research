{"title": "Self-Supervised Visual Representation Learning via Residual Momentum", "abstract": "Self-supervised learning (SSL) approaches have shown promising capabilities\nin learning the representation from unlabeled data. Amongst them,\nmomentum-based frameworks have attracted significant attention. Despite being a\ngreat success, these momentum-based SSL frameworks suffer from a large gap in\nrepresentation between the online encoder (student) and the momentum encoder\n(teacher), which hinders performance on downstream tasks. This paper is the\nfirst to investigate and identify this invisible gap as a bottleneck that has\nbeen overlooked in the existing SSL frameworks, potentially preventing the\nmodels from learning good representation. To solve this problem, we propose\n\"residual momentum\" to directly reduce this gap to encourage the student to\nlearn the representation as close to that of the teacher as possible, narrow\nthe performance gap with the teacher, and significantly improve the existing\nSSL. Our method is straightforward, easy to implement, and can be easily\nplugged into other SSL frameworks. Extensive experimental results on numerous\nbenchmark datasets and diverse network architectures have demonstrated the\neffectiveness of our method over the state-of-the-art contrastive learning\nbaselines.", "authors": ["Trung X. Pham", "Axi Niu", "Zhang Kang", "Sultan Rizky Madjid", "Ji Woo Hong", "Daehyeok Kim", "Joshua Tian Jin Tee", "Chang D. Yoo"], "published_date": "2022_11_17", "pdf_url": "http://arxiv.org/pdf/2211.09861v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Inter-M</td><td>Intra-M</td><td>Top-1 (%)</td><td>Top-5 (%)</td><td>KNN-1 (%)</td></tr><tr><td>W-MSE [16]</td><td>-</td><td>-</td><td>88.67</td><td>99.68</td><td>-</td></tr><tr><td>SwAV [6]</td><td>-</td><td>-</td><td>89.17</td><td>99.68</td><td>-</td></tr><tr><td>SimCLR [8]</td><td>-</td><td>-</td><td>90.74</td><td>99.75</td><td>-</td></tr><tr><td>SimSiam [10]</td><td>-</td><td>-</td><td>90.51</td><td>99.72</td><td>-</td></tr><tr><td>VICReg [3]</td><td>-</td><td>-</td><td>92.07</td><td>99.74</td><td>-</td></tr><tr><td>NNCLR [15]</td><td>-</td><td>-</td><td>91.88</td><td>99.78</td><td>-</td></tr><tr><td>Barlow Twins [51]</td><td>-</td><td>-</td><td>92.10</td><td>99.73</td><td>-</td></tr><tr><td>ReSSL [54]</td><td>\u2713</td><td>-</td><td>90.63</td><td>99.62</td><td>-</td></tr><tr><td>DINO [7]</td><td>\u2713</td><td>-</td><td>89.52</td><td>99.71</td><td>-</td></tr><tr><td>BYOL [18]</td><td>\u2713</td><td>-</td><td>92.58</td><td>99.79</td><td>88.84</td></tr><tr><td>MoCo-v3 [11]</td><td>\u2713</td><td>-</td><td>93.10</td><td>99.80</td><td>89.12</td></tr><tr><td>Res-MoCo (ours)</td><td>-</td><td>\u2713</td><td>93.53 {}_{+0.43}</td><td>99.88 {}_{+0.08}</td><td>90.78 {}_{+1.66}</td></tr><tr><td>Res-MoCo (ours)</td><td>\u2713</td><td>\u2713</td><td>93.71 {}_{+0.61}</td><td>99.84 {}_{+0.04}</td><td>90.78 {}_{+1.66}</td></tr></tbody></table>", "caption": "Table 1: CIFAR-10. Comparison of MoCo-v3 and Res-MoCo. All methods are trained in full 1000 epochs using the same settings, such as 1 GPU, batch 256, and ResNet-18. We employ the results for other SSL methods from the official solo-learn library [12].", "list_citation_info": ["[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650\u20139660, 2021.", "[12] Victor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-supervised methods for visual representation learning. Journal of Machine Learning Research, 23(56):1\u20136, 2022.", "[51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. International Conference on Machine Learning, 2021.", "[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "[16] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-supervised representation learning. In International Conference on Machine Learning. PMLR, 2021.", "[3] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022.", "[15] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9588\u20139597, 2021.", "[54] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. ReSSL: Relational self-supervised learning with weak augmentation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.", "[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.", "[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021."]}, {"table": "<table><tbody><tr><td>Method</td><td>Inter-M</td><td>Intra-M</td><td>Top-1 (%)</td><td>Top-5 (%)</td><td>KNN-1 (%)</td></tr><tr><td>W-MSE [16]</td><td>-</td><td>-</td><td>61.33</td><td>87.26</td><td>-</td></tr><tr><td>SwAV [6]</td><td>-</td><td>-</td><td>64.88</td><td>88.78</td><td>-</td></tr><tr><td>SimCLR [8]</td><td>-</td><td>-</td><td>65.78</td><td>89.04</td><td>58.52</td></tr><tr><td>SimSiam [10]</td><td>-</td><td>-</td><td>66.04</td><td>89.97</td><td>59.01</td></tr><tr><td>VICReg [3]</td><td>-</td><td>-</td><td>68.54</td><td>90.83</td><td>-</td></tr><tr><td>NNCLR [15]</td><td>-</td><td>-</td><td>69.62</td><td>91.52</td><td>-</td></tr><tr><td>Barlow Twins [51]</td><td>-</td><td>-</td><td>70.84</td><td>92.04</td><td>62.35</td></tr><tr><td>ReSSL [54]</td><td>\u2713</td><td>-</td><td>65.92</td><td>89.91</td><td>59.05</td></tr><tr><td>DINO [7]</td><td>\u2713</td><td>-</td><td>66.76</td><td>88.63</td><td>56.58</td></tr><tr><td>BYOL [18]</td><td>\u2713</td><td>-</td><td>70.06</td><td>92.12</td><td>61.83</td></tr><tr><td>MoCo-v3 [11]</td><td>\u2713</td><td>-</td><td>68.83</td><td>90.07</td><td>60.75</td></tr><tr><td>Res-BYOL (ours)</td><td>-</td><td>\u2713</td><td>71.27 {}_{+1.21}</td><td>92.47 {}_{+0.35}</td><td>63.17 {}_{+1.34}</td></tr><tr><td>Res-BYOL (ours)</td><td>\u2713</td><td>\u2713</td><td>71.43 {}_{+1.37}</td><td>92.54 {}_{+0.42}</td><td>62.34 {}_{+0.54}</td></tr><tr><td>Res-MoCo (ours)</td><td>-</td><td>\u2713</td><td>70.56 {}_{+1.73}</td><td>92.47 {}_{+2.40}</td><td>63.40 {}_{+2.65}</td></tr><tr><td>Res-MoCo (ours)</td><td>\u2713</td><td>\u2713</td><td>71.65 {}_{+2.82}</td><td>92.32 {}_{+2.25}</td><td>64.27 {}_{+3.52}</td></tr></tbody></table>", "caption": "Table 2: CIFAR-100. Comparison of MoCo-v3 and Res-MoCo. All methods are trained in 1000 epochs using the same settings, such as 1 GPU, batch 256, and ResNet-18. We employ the results for the other SSL methods from the official solo-learn library [12].", "list_citation_info": ["[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650\u20139660, 2021.", "[12] Victor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-supervised methods for visual representation learning. Journal of Machine Learning Research, 23(56):1\u20136, 2022.", "[51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. International Conference on Machine Learning, 2021.", "[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "[16] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-supervised representation learning. In International Conference on Machine Learning. PMLR, 2021.", "[3] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022.", "[15] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9588\u20139597, 2021.", "[54] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. ReSSL: Relational self-supervised learning with weak augmentation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.", "[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.", "[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021."]}, {"table": "<table><tbody><tr><td>Method</td><td>Inter-M</td><td>Intra-M</td><td>Top-1 (%)</td><td>Top-5 (%)</td><td>KNN-1 (%)</td></tr><tr><td>SimCLR [8]</td><td>-</td><td>-</td><td>78.46</td><td>-</td><td>73.22</td></tr><tr><td>MoCo-v2 [9]</td><td>\u2713</td><td>-</td><td>79.98</td><td>-</td><td>74.56</td></tr><tr><td>BYOL [18]</td><td>\u2713</td><td>-</td><td>81.46</td><td>95.26</td><td>75.34</td></tr><tr><td>MoCo-v3 [11]</td><td>\u2713</td><td>-</td><td>81.26</td><td>95.51</td><td>75.28</td></tr><tr><td>MoCo-v3 w/o Mom.</td><td>-</td><td>-</td><td>78.71</td><td>94.32</td><td>73.24</td></tr><tr><td>Res-BYOL (ours)</td><td>\u2713</td><td>\u2713</td><td>82.58{}_{+1.12}</td><td>95.44{}_{+0.18}</td><td>75.76{}_{+0.42}</td></tr><tr><td>Res-MoCo (ours)</td><td>-</td><td>\u2713</td><td>81.66{}_{+0.40}</td><td>95.86{}_{+0.35}</td><td>76.24{}_{+0.96}</td></tr><tr><td>Res-MoCo (ours)</td><td>\u2713</td><td>\u2713</td><td>82.62{}_{+1.36}</td><td>95.82{}_{+0.31}</td><td>75.84{}_{+0.56}</td></tr></tbody></table>", "caption": "Table 3: ImageNet-100. Compare MoCo-v3 and Res-MoCo. Methods are trained for 1000 epochs with the same settings, 2 GPUs, batch 256, and ResNet-18. Mom. denotes \u201cmomentum\u201d.", "list_citation_info": ["[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning, moco v2. arXiv preprint arXiv:2003.04297, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Epoch</th><th>Inter-M</th><th>Intra-M</th><th>Architecture</th><th>Top 1</th><th>Top-5</th></tr></thead><tbody><tr><th>SimCLR [8]</th><th>200</th><td>-</td><td>-</td><td>ResNet-50</td><td>68.3</td><td>-</td></tr><tr><th>SwAV [6]</th><th>200</th><td>-</td><td>-</td><td>ResNet-50</td><td>69.1</td><td>-</td></tr><tr><th>MoCo-v2 [9]</th><th>200</th><td>\u2713</td><td>-</td><td>ResNet-50</td><td>69.9</td><td>-</td></tr><tr><th>SimSiam [10]</th><th>200</th><td>-</td><td>-</td><td>ResNet-50</td><td>70.0</td><td>-</td></tr><tr><th>BYOL [18]</th><th>200</th><td>\u2713</td><td>-</td><td>ResNet-50</td><td>70.6</td><td>-</td></tr><tr><th>MoCo-v3 [11]</th><th>200</th><td>\u2713</td><td>-</td><td>ResNet-50</td><td>71.0</td><td>90.0</td></tr><tr><th>Res-MoCo (ours)</th><th>200</th><td>\u2713</td><td>\u2713</td><td>ResNet-50</td><td>71.7 {}_{+0.7}</td><td>90.4 {}_{+0.4}</td></tr></tbody></table>", "caption": "Table 4: ImageNet-1K. Linear classification. Comparisons of MoCo-v3 [11] and Res-MoCo with the other approaches.", "list_citation_info": ["[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning, moco v2. arXiv preprint arXiv:2003.04297, 2020.", "[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.", "[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Inter-M</th><th>Intra-M</th><th>Arch.</th><th>Top-1</th><th>\\Delta_{top-1}</th><th>Top-5</th><th>\\Delta_{top-5}</th><th>KNN-1</th><th>\\Delta_{knn}</th></tr></thead><tbody><tr><td>MoCo-v3 [11]</td><td>\u2713</td><td>-</td><td>ViT-S</td><td>79.66</td><td>-</td><td>95.40</td><td>-</td><td>76.64</td><td>-</td></tr><tr><td>Res-MoCo (ours)</td><td>\u2713</td><td>\u2713</td><td>ViT-S</td><td>79.94</td><td>+ 0.28</td><td>95.78</td><td>+ 0.38</td><td>77.46</td><td>+ 0.82</td></tr><tr><td>MoCo-v3 [11]</td><td>\u2713</td><td>-</td><td>ResNet-18</td><td>81.26</td><td>-</td><td>95.51</td><td>-</td><td>75.28</td><td>-</td></tr><tr><td>Res-MoCo (ours)</td><td>\u2713</td><td>\u2713</td><td>ResNet-18</td><td>82.62</td><td>+ 1.36</td><td>95.82</td><td>+ 0.31</td><td>75.84</td><td>+ 0.56</td></tr><tr><td>MoCo-v3 [11]</td><td>\u2713</td><td>-</td><td>ResNet-34</td><td>83.40</td><td>-</td><td>95.70</td><td>-</td><td>78.82</td><td>-</td></tr><tr><td>Res-MoCo (ours)</td><td>\u2713</td><td>\u2713</td><td>ResNet-34</td><td>84.50</td><td>+ 1.10</td><td>96.06</td><td>+ 0.36</td><td>79.74</td><td>+ 0.92</td></tr><tr><td>MoCo-v3 [11]</td><td>\u2713</td><td>-</td><td>ResNet-50</td><td>83.80</td><td>-</td><td>95.88</td><td>-</td><td>72.80</td><td>-</td></tr><tr><td>Res-MoCo (ours)</td><td>\u2713</td><td>\u2713</td><td>ResNet-50</td><td>86.21</td><td>+ 2.41</td><td>96.82</td><td>+ 0.94</td><td>74.46</td><td>+ 1.66</td></tr></tbody></table>", "caption": "Table 5:  Different backbone architectures. Comparison of MoCo-v3 and Res-MoCo on ImageNet-100. All methods are trained for 1000 epochs using 2 GPUs, batch size 256.", "list_citation_info": ["[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Epoch</th><td></td><td>VOC07+12</td><td></td><td></td><td>COCO</td><td></td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td><td>AP</td><td>AP50</td><td>AP75</td></tr><tr><th>Random Init.</th><th>-</th><td>33.8</td><td>60.2</td><td>33.1</td><td>29.9</td><td>47.9</td><td>32.0</td></tr><tr><th>Pre-Train Sup.</th><th>200</th><td>54.2</td><td>81.6</td><td>59.8</td><td>38.2</td><td>58.2</td><td>41.2</td></tr><tr><th>MoCo [20]</th><th>200</th><td>55.9</td><td>81.5</td><td>62.6</td><td>38.5</td><td>58.3</td><td>41.6</td></tr><tr><th>SimCLR [8]</th><th>1000</th><td>56.3</td><td>81.9</td><td>62.5</td><td>37.9</td><td>57.7</td><td>40.9</td></tr><tr><th>MoCo-v2 [9]</th><th>800</th><td>57.4</td><td>82.5</td><td>64.0</td><td>39.3</td><td>58.9</td><td>42.5</td></tr><tr><th>Barlow Twins [51]</th><th>1000</th><td>56.8</td><td>82.6</td><td>63.4</td><td>39.2</td><td>59.0</td><td>42.5</td></tr><tr><th>VICReg [3]</th><th>1000</th><td>-</td><td>82.4</td><td>-</td><td>39.4</td><td>-</td><td>-</td></tr><tr><th>BYOL [18]</th><th>200</th><td>55.3</td><td>81.4</td><td>61.1</td><td>37.9</td><td>57.8</td><td>40.9</td></tr><tr><th>SwAV [6]</th><th>1000</th><td>56.1</td><td>82.6</td><td>62.7</td><td>38.4</td><td>58.6</td><td>41.3</td></tr><tr><th>MoCo-v3 [11]</th><th>200</th><td>56.2</td><td>82.4</td><td>62.9</td><td>39.1</td><td>58.8</td><td>42.2</td></tr><tr><th>Res-MoCo (ours)</th><th>200</th><td>56.5{}_{+0.3}</td><td>82.6{}_{+0.2}</td><td>63.0{}_{+0.1}</td><td>39.5{}_{+0.4}</td><td>59.2{}_{+0.4}</td><td>42.9{}_{+0.7}</td></tr></tbody></table>", "caption": "Table 6: Object Detection on VOC07+12 and COCO2017. Comparison of Res-MoCo and the existing methods using ResNet-50 pre-trained on ImageNet-1K for transfer learning. Results MoCo-v3 and Res-MoCo are run three times and taken average.", "list_citation_info": ["[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. International Conference on Machine Learning, 2021.", "[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning, moco v2. arXiv preprint arXiv:2003.04297, 2020.", "[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.", "[3] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022."]}, {"table": "<table><tbody><tr><th>Method</th><td>Top 1</td><td>Top-5</td><td>KNN-1</td></tr><tr><th>MoCo-v3 [11]</th><td>68.83</td><td>90.07</td><td>60.75</td></tr><tr><th>Res-MoCo w/ MSE (Eq. 7) [44]</th><td>68.73</td><td>90.92</td><td>60.61</td></tr><tr><th>Res-MoCo w/ CE (Eq. 6) [22]</th><td>69.72</td><td>91.85</td><td>63.14</td></tr><tr><th>Res-MoCo w/ Cosine</th><td>71.65</td><td>92.32</td><td>64.27</td></tr></tbody></table>", "caption": "Table 7: Choices for \\mathcal{L}_{\\text{Intra-M}}. Performance on CIFAR-100, trained for 1000 epochs when all models are fully converged.", "list_citation_info": ["[44] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.", "[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.", "[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Inter-M</th><th>Intra-M</th><th>Arch.</th><th>Top-1</th><th>\\Delta_{top-1}</th><th>Top-5</th><th>\\Delta_{top-5}</th><th>KNN-1</th><th>\\Delta_{knn}</th></tr></thead><tbody><tr><td>MoCo-v3 [11] \\tau=0.2</td><td>\u2713</td><td>-</td><td>ResNet-50</td><td>83.80</td><td>-</td><td>95.88</td><td>-</td><td>72.80</td><td>-</td></tr><tr><td>Res-MoCo (ours) \\tau=0.2</td><td>\u2713</td><td>\u2713</td><td>ResNet-50</td><td>86.21</td><td>+ 2.41</td><td>96.82</td><td>+ 0.94</td><td>74.46</td><td>+ 1.66</td></tr><tr><td>MoCo-v3 [11] \\tau=1.0</td><td>\u2713</td><td>-</td><td>ResNet-50</td><td>82.54</td><td>-</td><td>95.50</td><td>-</td><td>66.28</td><td>-</td></tr><tr><td>Res-MoCo \\tau=1.0</td><td>\u2713</td><td>\u2713</td><td>ResNet-50</td><td>84.64</td><td>+ 2.10</td><td>96.28</td><td>+ 0.78</td><td>71.12</td><td>+ 4.84</td></tr></tbody></table>", "caption": "Table 13: Different temperatures. Comparison of MoCo-v3 and Res-MoCo on ImageNet-100. All methods are trained for 1000 epochs using the same settings run on 2 GPUs, batch size 256. It shows that Res-MoCo outperforms MoCo-v3 for both temperature settings.", "list_citation_info": ["[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Epoch</th><th>Inter-M</th><th>Intra-M</th><th>Sim (%)</th><th>\\Delta_{sim}</th><th>Top-1</th><th>\\Delta_{top-1}</th><th>KNN-1</th><th>\\Delta_{knn}</th></tr></thead><tbody><tr><th>MoCo-v3 [11]</th><th>100</th><td>\u2713</td><td>-</td><td>90.74</td><td>-</td><td>59.76</td><td>-</td><td>55.88</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>100</th><td>\u2713</td><td>\u2713</td><td>94.04</td><td>+ 3.30</td><td>64.21</td><td>+ 4.45</td><td>61.89</td><td>+ 6.01</td></tr><tr><th>MoCo-v3 [11]</th><th>200</th><td>\u2713</td><td>-</td><td>90.15</td><td>-</td><td>60.87</td><td>-</td><td>56.70</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>200</th><td>\u2713</td><td>\u2713</td><td>93.01</td><td>+ 2.86</td><td>63.84</td><td>+ 2.97</td><td>61.96</td><td>+ 5.26</td></tr><tr><th>MoCo-v3 [11]</th><th>400</th><td>\u2713</td><td>-</td><td>88.48</td><td>-</td><td>64.16</td><td>-</td><td>58.60</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>400</th><td>\u2713</td><td>\u2713</td><td>93.90</td><td>+ 5.42</td><td>69.88</td><td>+ 5.72</td><td>66.21</td><td>+ 7.61</td></tr><tr><th>MoCo-v3 [11]</th><th>800</th><td>\u2713</td><td>-</td><td>96.88</td><td>-</td><td>80.15</td><td>-</td><td>68.57</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>800</th><td>\u2713</td><td>\u2713</td><td>98.41</td><td>+ 1.53</td><td>82.74</td><td>+ 2.59</td><td>74.52</td><td>+ 5.95</td></tr><tr><th>MoCo-v3 [11]</th><th>1000</th><td>\u2713</td><td>-</td><td>99.84</td><td>-</td><td>83.80</td><td>-</td><td>72.80</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>1000</th><td>\u2713</td><td>\u2713</td><td>99.95</td><td>+ 0.11</td><td>86.21</td><td>+ 2.41</td><td>74.46</td><td>+ 1.66</td></tr></tbody></table>", "caption": "Table 14: Similarity of the online and momentum version of each image itself on ImageNet-100, ResNet-50. During training, Res-MoCo shows a much higher similarity for the image and its momentum version, which strongly corresponds to the higher performance in both linear top-1 (%) and KNN accuracy (%). We monitor similarity (Sim) by taking the average cosine similarity of each training batch. ", "list_citation_info": ["[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Epoch</th><th>Inter-M</th><th>Intra-M</th><th>Sim (%)</th><th>\\Delta_{sim}</th><th>Top-1</th><th>\\Delta_{top-1}</th><th>KNN-1</th><th>\\Delta_{knn}</th></tr></thead><tbody><tr><th>MoCo-v3 [11]</th><th>100</th><td>\u2713</td><td>-</td><td>89.65</td><td>-</td><td>54.62</td><td>-</td><td>54.98</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>100</th><td>\u2713</td><td>\u2713</td><td>93.63</td><td>+ 3.98</td><td>60.21</td><td>+ 5.59</td><td>60.42</td><td>+ 5.54</td></tr><tr><th>MoCo-v3 [11]</th><th>200</th><td>\u2713</td><td>-</td><td>88.89</td><td>-</td><td>60.38</td><td>-</td><td>59.06</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>200</th><td>\u2713</td><td>\u2713</td><td>92.39</td><td>+ 3.50</td><td>64.08</td><td>+ 3.70</td><td>63.32</td><td>+ 4.26</td></tr><tr><th>MoCo-v3 [11]</th><th>400</th><td>\u2713</td><td>-</td><td>89.63</td><td>-</td><td>63.35</td><td>-</td><td>60.64</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>400</th><td>\u2713</td><td>\u2713</td><td>94.43</td><td>+ 4.8</td><td>69.98</td><td>+ 6.63</td><td>67.48</td><td>+ 6.84</td></tr><tr><th>MoCo-v3 [11]</th><th>800</th><td>\u2713</td><td>-</td><td>96.72</td><td>-</td><td>78.14</td><td>-</td><td>72.67</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>800</th><td>\u2713</td><td>\u2713</td><td>98.77</td><td>+ 2.05</td><td>80.41</td><td>+ 2.27</td><td>75.03</td><td>+ 2.36</td></tr><tr><th>MoCo-v3 [11]</th><th>1000</th><td>\u2713</td><td>-</td><td>99.76</td><td>-</td><td>81.26</td><td>-</td><td>75.28</td><td>-</td></tr><tr><th>Res-MoCo (ours)</th><th>1000</th><td>\u2713</td><td>\u2713</td><td>99.94</td><td>+ 0.17</td><td>82.62</td><td>+ 1.36</td><td>75.84</td><td>+ 0.56</td></tr></tbody></table>", "caption": "Table 15: Similarity of the student and teacher of each image itself on ImageNet-100, ResNet-18. During training, Res-MoCo shows a much higher similarity between teacher and student, which strongly corresponds to a performance boost in both linear top-1 (%) and KNN accuracy (%). We monitor similarity (sim) by taking the average cosine similarity of each training batch. ", "list_citation_info": ["[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Inter-M</th><th>Intra-M</th><th>Top-1 (%)</th><th>Top-5 (%)</th><th>KNN-1 (%)</th></tr></thead><tbody><tr><td>BYOL [18]</td><td>\u2713</td><td>-</td><td>70.06</td><td>92.12</td><td>61.83</td></tr><tr><td>ours</td><td>\u2713</td><td>\u2713</td><td>71.43</td><td>92.54</td><td>62.34</td></tr><tr><td>ReSSL [54]</td><td>\u2713</td><td>-</td><td>66.00</td><td>89.91</td><td>59.05</td></tr><tr><td>ours</td><td>\u2713</td><td>\u2713</td><td>66.30</td><td>90.17</td><td>59.32</td></tr><tr><td>SimSiam [10]</td><td>-</td><td>-</td><td>66.50</td><td>89.97</td><td>59.00</td></tr><tr><td></td><td>\u2713</td><td>-</td><td>66.33</td><td>89.71</td><td>58.59</td></tr><tr><td>ours</td><td>-</td><td>\u2713</td><td>67.28</td><td>90.49</td><td>59.66</td></tr><tr><td>ours</td><td>\u2713</td><td>\u2713</td><td>67.19</td><td>90.19</td><td>59.79</td></tr></tbody></table>", "caption": "Table 16: Linear accuracy for other momentum-based and momentum-free SSL frameworks. ", "list_citation_info": ["[54] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. ReSSL: Relational self-supervised learning with weak augmentation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021."]}], "citation_info_to_title": {"[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650\u20139660, 2021.": "Emerging properties in self-supervised vision transformers", "[6] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.": "Unsupervised learning of visual features by contrasting cluster assignments", "[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "Momentum contrast for unsupervised visual representation learning", "[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.": "A simple framework for contrastive learning of visual representations", "[18] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.": "Bootstrap your own latent-a new approach to self-supervised learning", "[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.": "Distilling the knowledge in a neural network", "[11] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.": "An empirical study of training self-supervised vision transformers", "[44] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.": "Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results", "[15] Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9588\u20139597, 2021.": "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations", "[3] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In International Conference on Learning Representations, 2022.": "VICReg: Variance-invariance-covariance regularization for self-supervised learning", "[51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. International Conference on Machine Learning, 2021.": "Barlow twins: Self-supervised learning via redundancy reduction", "[9] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning, moco v2. arXiv preprint arXiv:2003.04297, 2020.": "Improved Baselines with Momentum Contrastive Learning, Moco V2", "[16] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-supervised representation learning. In International Conference on Machine Learning. PMLR, 2021.": "Whitening for self-supervised representation learning", "[12] Victor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-supervised methods for visual representation learning. Journal of Machine Learning Research, 23(56):1\u20136, 2022.": "solo-learn: A library of self-supervised methods for visual representation learning", "[54] Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. ReSSL: Relational self-supervised learning with weak augmentation. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.": "ReSSL: Relational self-supervised learning with weak augmentation", "[10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.": "Exploring Simple Siamese Representation Learning"}, "source_title_to_arxiv_id": {"Emerging properties in self-supervised vision transformers": "2104.14294", "An empirical study of training self-supervised vision transformers": "2104.02057"}}