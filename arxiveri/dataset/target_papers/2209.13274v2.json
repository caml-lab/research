{"title": "Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and NeRF-realized Mapping", "abstract": "A spatial AI that can perform complex tasks through visual signals and\ncooperate with humans is highly anticipated. To achieve this, we need a visual\nSLAM that easily adapts to new scenes without pre-training and generates dense\nmaps for downstream tasks in real-time. None of the previous learning-based and\nnon-learning-based visual SLAMs satisfy all needs due to the intrinsic\nlimitations of their components. In this work, we develop a visual SLAM named\nOrbeez-SLAM, which successfully collaborates with implicit neural\nrepresentation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM\ncan work with the monocular camera since it only needs RGB inputs, making it\nwidely applicable to the real world. Results show that our SLAM is up to 800x\nfaster than the strong baseline with superior rendering outcomes. Code link:\nhttps://github.com/MarvinChung/Orbeez-SLAM.", "authors": ["Chi-Ming Chung", "Yang-Che Tseng", "Ya-Ching Hsu", "Xiang-Qian Shi", "Yun-Hung Hua", "Jia-Fong Yeh", "Wen-Chin Chen", "Yi-Ting Chen", "Winston H. Hsu"], "published_date": "2022_09_27", "pdf_url": "http://arxiv.org/pdf/2209.13274v2", "list_table_and_caption": [{"table": "<table><tr><td></td><td>fr1/desk</td><td>fr2/xyz</td><td>fr3/office</td></tr><tr><td>DI-Fusion[32]</td><td>4.4</td><td>2.3</td><td>15.6</td></tr><tr><td>iMAP[5]</td><td>4.9</td><td>2.0</td><td>5.8</td></tr><tr><td>iMAP{}^{*}[6]</td><td>7.2</td><td>2.1</td><td>9.0</td></tr><tr><td>NICE-SLAM[6]</td><td>2.7</td><td>1.8</td><td>3.0</td></tr><tr><td>Orbeez-SLAM (Ours)</td><td>1.9</td><td>0.3</td><td>1.0</td></tr><tr><td>BAD-SLAM[33]</td><td>1.7</td><td>1.1</td><td>1.7</td></tr><tr><td>Kintinuous[34]</td><td>3.7</td><td>2.9</td><td>3.0</td></tr><tr><td>ORB-SLAM2[1]</td><td>1.6</td><td>0.3</td><td>0.9</td></tr></table>", "caption": "TABLE I: Tracking Results on TUM RGB-D. ATE [cm] (\\downarrow) is used. Learning-based and traditional SLAMs are separated by the middle line. Results of DI-Fusion, iMAP, iMAP{}^{*}, NICE-SLAM, BAD-SLAM and Kintinuous are from [6]. The best out of 5 runs are reported.", "list_citation_info": ["[33] T. Sch\u00f6ps, T. Sattler, and M. Pollefeys, \u201cBad slam: Bundle adjusted direct rgb-d slam,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 134\u2013144.", "[1] R. Mur-Artal and J. D. Tard\u00f3s, \u201cORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras,\u201d IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255\u20131262, 2017.", "[32] J. Huang, S.-S. Huang, H. Song, and S.-M. Hu, \u201cDi-fusion: Online implicit 3d reconstruction with deep priors,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.", "[6] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, \u201cNice-slam: Neural implicit scalable encoding for slam,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.", "[34] T. Whelan, M. Kaess, M. Fallon, H. Johannsson, J. Leonard, and J. McDonald, \u201cKintinuous: Spatially extended kinectfusion,\u201d 2012.", "[5] E. Sucar, S. Liu, J. Ortiz, and A. Davison, \u201ciMAP: Implicit mapping and positioning in real-time,\u201d in Proceedings of the International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tr><td>Scene ID</td><td> 0000 </td><td> 0059 </td><td> 0106 </td><td> 0169 </td><td> 0181 </td><td> 0207 </td><td>Avg.</td></tr><tr><td>DI-Fusion  [32]</td><td>62.99</td><td>128.00</td><td>18.50</td><td>75.80</td><td>87.88</td><td>100.19</td><td>78.89</td></tr><tr><td>iMAP{}^{*} [5]</td><td>55.95</td><td>32.06</td><td>17.50</td><td>70.51</td><td>32.10</td><td>11.91</td><td>36.67</td></tr><tr><td>NICE-SLAM[6]</td><td>8.64</td><td>12.25</td><td>8.09</td><td>10.28</td><td>12.93</td><td>5.59</td><td>9.63</td></tr><tr><td>Orbeez-SLAM (Ours)</td><td>7.22</td><td>7.15</td><td>8.05</td><td>6.58</td><td>15.77</td><td>7.16</td><td>8.655</td></tr><tr><td>ORB-SLAM2 [1]</td><td>7.57</td><td>6.92</td><td>8.30</td><td>6.90</td><td>16.42</td><td>8.78</td><td>9.15</td></tr></table>", "caption": "TABLE II: Tracking Results on ScanNet. ATE [cm] (\\downarrow) is used. Results of iMAP{}^{*}, DI-Fusion and NICE-SLAM are from [6].", "list_citation_info": ["[32] J. Huang, S.-S. Huang, H. Song, and S.-M. Hu, \u201cDi-fusion: Online implicit 3d reconstruction with deep priors,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.", "[6] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, \u201cNice-slam: Neural implicit scalable encoding for slam,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.", "[1] R. Mur-Artal and J. D. Tard\u00f3s, \u201cORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras,\u201d IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255\u20131262, 2017.", "[5] E. Sucar, S. Liu, J. Ortiz, and A. Davison, \u201ciMAP: Implicit mapping and positioning in real-time,\u201d in Proceedings of the International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tr><td></td><td>fr1/desk</td><td>fr2/xyz</td><td>fr3/office</td></tr><tr><td># images</td><td>613</td><td>3669</td><td>2585</td></tr><tr><td>NICE-SLAM [6]</td><td>0.056</td><td>0.028</td><td>0.037</td></tr><tr><td>Orbeez-SLAM (Ours)</td><td>19.210</td><td>22.725</td><td>21.542</td></tr></table>", "caption": "TABLE IV: Runtime Comparison. Frame per second [fps] (\\uparrow) when running on TUM RGB-D. We show that Orbeez-SLAM is much faster than the SOTA NeRF-SLAM. ", "list_citation_info": ["[6] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, \u201cNice-slam: Neural implicit scalable encoding for slam,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022."]}], "citation_info_to_title": {"[34] T. Whelan, M. Kaess, M. Fallon, H. Johannsson, J. Leonard, and J. McDonald, \u201cKintinuous: Spatially extended kinectfusion,\u201d 2012.": "Kintinuous: Spatially extended kinectfusion", "[33] T. Sch\u00f6ps, T. Sattler, and M. Pollefeys, \u201cBad slam: Bundle adjusted direct rgb-d slam,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 134\u2013144.": "Bad slam: Bundle adjusted direct rgb-d slam", "[1] R. Mur-Artal and J. D. Tard\u00f3s, \u201cORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras,\u201d IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255\u20131262, 2017.": "ORB-SLAM2: an open-source SLAM system for monocular, stereo and RGB-D cameras", "[6] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys, \u201cNice-slam: Neural implicit scalable encoding for slam,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.": "Nice-slam: Neural Implicit Scalable Encoding for SLAM", "[5] E. Sucar, S. Liu, J. Ortiz, and A. Davison, \u201ciMAP: Implicit mapping and positioning in real-time,\u201d in Proceedings of the International Conference on Computer Vision (ICCV), 2021.": "iMAP: Implicit mapping and positioning in real-time", "[32] J. Huang, S.-S. Huang, H. Song, and S.-M. Hu, \u201cDi-fusion: Online implicit 3d reconstruction with deep priors,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.": "Di-fusion: Online Implicit 3D Reconstruction with Deep Priors"}, "source_title_to_arxiv_id": {"Di-fusion: Online Implicit 3D Reconstruction with Deep Priors": "2012.05551"}}