{"title": "Person Re-identification: A Retrospective on Domain Specific Open Challenges and Future Trends", "abstract": "Person re-identification (Re-ID) is one of the primary components of an\nautomated visual surveillance system. It aims to automatically identify/search\npersons in a multi-camera network having non-overlapping field-of-views. Owing\nto its potential in various applications and research significance, a plethora\nof deep learning based re-Id approaches have been proposed in the recent years.\nHowever, there exist several vision related challenges, e.g., occlusion, pose\nscale \\& viewpoint variance, background clutter, person misalignment and\ncross-domain generalization across camera modalities, which makes the problem\nof re-Id still far from being solved. Majority of the proposed approaches\ndirectly or indirectly aim to solve one or multiple of these existing\nchallenges. In this context, a comprehensive review of current re-ID approaches\nin solving theses challenges is needed to analyze and focus on particular\naspects for further advancements. At present, such a focused review does not\nexist and henceforth in this paper, we have presented a systematic\nchallenge-specific literature survey of 230+ papers between the years of\n2015-21. For the first time a survey of this type have been presented where the\nperson re-Id approaches are reviewed in such solution-oriented perspective.\nMoreover, we have presented several diversified prominent developing trends in\nthe respective research domain which will provide a visionary perspective\nregarding ongoing person re-Id research and eventually help to develop\npractical real world solutions.", "authors": ["Asmat Zahra", "Nazia Perwaiz", "Muhammad Shahzad", "Muhammad Moazam Fraz"], "published_date": "2022_02_26", "pdf_url": "http://arxiv.org/pdf/2202.13121v1", "list_table_and_caption": [{"table": "<table><tr><td><p>Sr.No</p></td><td><p>Dataset</p></td><td><p>Year</p></td><td><p>Environment</p></td><td><p>Identities</p></td><td><p>Cameras</p></td><td><p>Resolution</p></td><td><p>Label</p></td><td><p>BBoxes</p></td><td><p>Challenging Attributes</p></td></tr><tr><td><p>1</p></td><td><p>VIPeR [14]</p></td><td><p>2007</p></td><td><p>Campus</p></td><td><p>632</p></td><td><p>2</p></td><td><p>48\u00d7128</p></td><td><p>Hand</p></td><td><p>1,264</p></td><td><p>VV<sup>1</sup>, IV<sup>2</sup></p></td></tr><tr><td><p>2</p></td><td><p>PRID-2011 [15]</p></td><td><p>2011</p></td><td><p>Outdoor</p></td><td><p>200</p></td><td><p>2</p></td><td><p>64\u00d7128</p></td><td><p>DPM/ GMMCP/ Hand</p></td><td><p>40,000</p></td><td><p>IV, VV,,BC<sup>3</sup></p></td></tr><tr><td><p>3</p></td><td><p>CUHK01 [12]</p></td><td><p>2012</p></td><td><p>Campus</p></td><td><p>971</p></td><td><p>2</p></td><td><p>60\u00d7160</p></td><td><p>Hand</p></td><td><p>3,884</p></td><td><p>VV,OCC<sup>4</sup></p></td></tr><tr><td><p>4</p></td><td><p>CUHK03 [13]</p></td><td><p>2014</p></td><td><p>Campus</p></td><td><p>1,360</p></td><td><p>10</p></td><td><p>Vary</p></td><td><p>DPM/Hand</p></td><td><p>13,164</p></td><td><p>VV,OCC</p></td></tr><tr><td><p>5</p></td><td><p>iLIDS-Vid [16]</p></td><td><p>2014</p></td><td><p>Airport</p></td><td><p>300</p></td><td><p>2</p></td><td><p>Vary</p></td><td><p>Hand</p></td><td><p>42,495</p></td><td><p>VV,IV,BC,OCC</p></td></tr><tr><td><p>6</p></td><td><p>Market-1501 [10]</p></td><td><p>2015</p></td><td><p>Campus</p></td><td><p>1,501</p></td><td><p>6</p></td><td><p>64\u00d7128</p></td><td><p>DPM/Hand</p></td><td><p>32,688</p></td><td><p>VV, PV<sup>5</sup>, RES<sup>6</sup></p></td></tr><tr><td><p>7</p></td><td><p>MARS [17]</p></td><td><p>2016</p></td><td><p>Campus</p></td><td><p>1,261</p></td><td><p>6</p></td><td><p>256\u00d7128</p></td><td><p>DPM/ GMMCP</p></td><td><p>1,067,516</p></td><td><p>PV,IV,RES</p></td></tr><tr><td><p>8</p></td><td><p>DukeMTMC-ReID [11]</p></td><td><p>2017</p></td><td><p>Campus</p></td><td><p>1,404</p></td><td><p>8</p></td><td><p>Vary</p></td><td><p>Doppia/ Hand</p></td><td><p>36,411</p></td><td><p>VV,IV,BC,OCC</p></td></tr><tr><td><p>9</p></td><td><p>DukeMTMC-Video ReID</p></td><td><p>2017</p></td><td><p>Campus</p></td><td><p>1,404</p></td><td><p>8</p></td><td><p>Vary</p></td><td><p>Hand</p></td><td><p>36,411</p></td><td><p>VV,IV,BC,OCC</p></td></tr><tr><td><p>10</p></td><td><p>MSMT-17 [18]</p></td><td><p>2018</p></td><td><p>Campus</p></td><td><p>4,101</p></td><td><p>15</p></td><td><p>Vary</p></td><td><p>Faster RCNN</p></td><td><p>12,6441</p></td><td><p>VV,IV</p></td></tr></table><ul><li>1<p>Viewpoint Variation</p></li><li>2<p>Illumination Variation</p></li><li>3<p>Background Clutter</p></li><li>4<p>Occlusion</p></li><li>5<p>Pose Variation</p></li><li>6<p>Resolution</p></li><li>7<p>Deformable Part Model [19] (A Pedestrian detector)</p></li><li>8<p>GMMCP- Generalized Maximum Multi Clique problem [20] (A tracker)</p></li></ul>", "caption": "Table 4: Properties of each dataset.", "list_citation_info": ["[17] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, and Qi Tian. Mars: A video benchmark for large-scale person re-identification. In European Conference on Computer Vision, pages 868\u2013884. Springer, 2016.", "[11] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European conference on computer vision, pages 17\u201335. Springer, 2016.", "[14] Douglas Gray, Shane Brennan, and Hai Tao. Evaluating appearance models for recognition, reacquisition, and tracking. In Proc. IEEE international workshop on performance evaluation for tracking and surveillance (PETS), volume 3, pages 1\u20137. Citeseer, 2007.", "[20] Afshin Dehghan, Shayan Modiri Assari, and Mubarak Shah. Gmmcp tracker: Globally optimal generalized maximum multi clique problem for multiple object tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4091\u20134099, 2015.", "[12] Wei Li, Rui Zhao, and Xiaogang Wang. Human reidentification with transferred metric learning. In Asian conference on computer vision, pages 31\u201344. Springer, 2012.", "[10] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In Proceedings of the IEEE international conference on computer vision, pages 1116\u20131124, 2015.", "[13] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing neural network for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 152\u2013159, 2014.", "[16] Taiqing Wang, Shaogang Gong, Xiatian Zhu, and Shengjin Wang. Person re-identification by video ranking. In European conference on computer vision, pages 688\u2013703. Springer, 2014.", "[18] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain gap for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 79\u201388, 2018.", "[15] Martin Hirzer, Csaba Beleznai, Peter M Roth, and Horst Bischof. Person re-identification by descriptive and discriminative classification. In Scandinavian conference on Image analysis, pages 91\u2013102. Springer, 2011.", "[19] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9):1627\u20131645, 2009."]}, {"table": "<table><tr><td><p>SN</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>STRF, 2021, [70]</p></td><td><p>MARS</p></td><td><p>90.3/86.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VidReId</p></td><td><p>97.4/96.4</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDs</p></td><td><p>89.3/\u2013</p></td><td></td></tr><tr><td><p>2</p></td><td><p>STMN, 2021, [71]</p></td><td><p>MARS</p></td><td><p>90.5/84.5</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VidReId</p></td><td><p>97.0/95.9</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDs</p></td><td><p>82.1/69.2</p></td><td></td></tr><tr><td><p>3</p></td><td><p>PSTA, 2021, [67]</p></td><td><p>MARS</p></td><td><p>91.5/85.8</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VidReId</p></td><td><p>98.3/97.4</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDs</p></td><td><p>91.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>95.6/\u2013</p></td><td></td></tr><tr><td><p>4</p></td><td><p>SGPR, 2021, [59]</p></td><td><p>Market-1501</p></td><td><p>96.1/89.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>91.1/81.3</p></td><td></td></tr><tr><td></td><td></td><td><p>Occluded-ReId</p></td><td><p>78.5/72.9</p></td><td></td></tr><tr><td></td><td></td><td><p>Occluded-Duke</p></td><td><p>69.0/57.2</p></td><td></td></tr><tr><td><p>5</p></td><td><p>PE-PGFA, 2021, [60]</p></td><td><p>Occluded-ReId</p></td><td><p>81.0/71.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>Occluded-Duke</p></td><td><p>62.2/46.3</p></td><td></td></tr><tr><td></td><td></td><td><p>Partial-iLIDs</p></td><td><p>80.7/85.7</p></td><td></td></tr><tr><td><p>6</p></td><td><p>FPR, 2019, [56]</p></td><td><p>Market-1501</p></td><td><p>95.42/86.58</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.64/78.42</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>76.08/72.31</p></td><td></td></tr><tr><td><p>7</p></td><td><p>PGFA, 2019, [64]</p></td><td><p>Market-1501</p></td><td><p>91.2/76.8</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.6/65.5</p></td><td></td></tr><tr><td></td><td></td><td><p>Occluded-Duke</p></td><td><p>51.4/37.3</p></td><td></td></tr><tr><td><p>8</p></td><td><p>AMC-SWM, 2015, [55]</p></td><td><p>Partial-ReId</p></td><td><p>53.14/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>9</p></td><td><p>PISNet, 2020, [66]</p></td><td><p>Market-1501</p></td><td><p>95.6/87.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.8/78.7</p></td><td></td></tr><tr><td><p>10</p></td><td><p>GASM, 2020, [57]</p></td><td><p>Market-1501</p></td><td><p>95.3/84.7</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.3/74.4</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>79.5/52.5</p></td><td></td></tr><tr><td><p>11</p></td><td><p>PAT, 2021, [72]</p></td><td><p>Market-1501</p></td><td><p>95.4/88.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>64.5/53.6</p></td><td></td></tr><tr><td></td><td></td><td><p>Occluded-ReId</p></td><td><p>81.6/72.1</p></td><td></td></tr><tr><td></td><td></td><td><p>Partial-iLIDs</p></td><td><p>88.0/76.5</p></td><td></td></tr><tr><td><p>12</p></td><td><p>HOReID, 2020, [58]</p></td><td><p>Market-1501</p></td><td><p>94.2/84.9</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.9/75.6</p></td><td></td></tr><tr><td></td><td></td><td><p>Occluded-ReId</p></td><td><p>80.3/70.2</p></td><td></td></tr><tr><td></td><td></td><td><p>Occluded-Duke</p></td><td><p>55.1/43.8</p></td><td></td></tr><tr><td></td><td></td><td><p>Partial-iLIDs</p></td><td><p>72.6/85.3</p></td><td></td></tr><tr><td><p>13</p></td><td><p>VRSTC, 2019, [65]</p></td><td><p>MARS</p></td><td><p>88.5/82.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VidReId</p></td><td><p>95.0/93.5</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDs</p></td><td><p>83.4/\u2013</p></td><td></td></tr><tr><td><p>14</p></td><td><p>ATNet, 2019, [73]</p></td><td><p>Market-1501</p></td><td><p>45.1/24.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>55.7/25.6</p></td><td></td></tr><tr><td><p>15</p></td><td><p>IGOAS, 2021, [61]</p></td><td><p>Market-1501</p></td><td><p>93.4/84.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.9/75.1</p></td><td></td></tr><tr><td><p>16</p></td><td><p>SCAN, 2019, [68]</p></td><td><p>MARS</p></td><td><p>87.2/77.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDs</p></td><td><p>88.0/89.9</p></td><td></td></tr><tr><td><p>17</p></td><td><p>STAL, 2019, [69]</p></td><td><p>MARS</p></td><td><p>71.5/50.8</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDs</p></td><td><p>76.7/\u2013</p></td><td></td></tr><tr><td><p>18</p></td><td><p>SRFC, 2021, [62]</p></td><td><p>MARS</p></td><td><p>95.2/89.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>90.7/80.7</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>78.0/81.1</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>82.0/60.2</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>90.7/86.3</p></td><td></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VidReId</p></td><td><p>97.6/97.0</p></td><td></td></tr><tr><td><p>19</p></td><td><p>SORN, 2021, [63]</p></td><td><p>Market-1501</p></td><td><p>94.8/84.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.9/74.1</p></td><td></td></tr></table>", "caption": "Table 5: Results obtained on occlusion challenge against each dataset. Results in bold are the highest. ", "list_citation_info": ["[67] Yingquan Wang, Pingping Zhang, Shang Gao, Xia Geng, Hu Lu, and Dong Wang. Pyramid spatial-temporal aggregation for video-based person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12026\u201312035, 2021.", "[58] Guan\u2019an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang, Yang Yang, Shuliang Wang, Gang Yu, Erjin Zhou, and Jian Sun. High-order information matters: Learning relation and topology for occluded person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6449\u20136458, 2020.", "[63] Xiaokang Zhang, Yan Yan, Jing-Hao Xue, Yang Hua, and Hanzi Wang. Semantic-aware occlusion-robust network for occluded person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2020.", "[65] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Vrstc: Occlusion-free video person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7183\u20137192, 2019.", "[71] Chanho Eom, Geon Lee, Junghyup Lee, and Bumsub Ham. Video-based person re-identification with spatial and temporal memory networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12036\u201312045, 2021.", "[62] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Feature completion for occluded person re-identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[64] Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, and Yi Yang. Pose-guided feature alignment for occluded person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 542\u2013551, 2019.", "[73] Jiawei Liu, Zheng-Jun Zha, Di Chen, Richang Hong, and Meng Wang. Adaptive transfer network for cross-domain person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7202\u20137211, 2019.", "[59] Cheng Yan, Guansong Pang, Jile Jiao, Xiao Bai, Xuetao Feng, and Chunhua Shen. Occluded person re-identification with single-scale global representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11875\u201311884, 2021.", "[60] Jinrui Yang, Jiawei Zhang, Fufu Yu, Xinyang Jiang, Mengdan Zhang, Xing Sun, Ying-Cong Chen, and Wei-Shi Zheng. Learning to know where to see: A visibility-aware approach for occluded person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11885\u201311894, 2021.", "[72] Yulin Li, Jianfeng He, Tianzhu Zhang, Xiang Liu, Yongdong Zhang, and Feng Wu. Diverse part discovery: Occluded person re-identification with part-aware transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2898\u20132907, 2021.", "[61] Cairong Zhao, Xinbi Lv, Shuguang Dou, Shanshan Zhang, Jun Wu, and Liang Wang. Incremental generative occlusion adversarial suppression network for person reid. IEEE Transactions on Image Processing, 30:4212\u20134224, 2021.", "[68] Ruimao Zhang, Jingyu Li, Hongbin Sun, Yuying Ge, Ping Luo, Xiaogang Wang, and Liang Lin. Scan: Self-and-collaborative attention network for video person re-identification. IEEE Transactions on Image Processing, 28(10):4870\u20134882, 2019.", "[55] Wei-Shi Zheng, Xiang Li, Tao Xiang, Shengcai Liao, Jianhuang Lai, and Shaogang Gong. Partial person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 4678\u20134686, 2015.", "[69] Guangyi Chen, Jiwen Lu, Ming Yang, and Jie Zhou. Spatial-temporal attention-aware learning for video-based person re-identification. IEEE Transactions on Image Processing, 28(9):4192\u20134205, 2019.", "[70] Abhishek Aich, Meng Zheng, Srikrishna Karanam, Terrence Chen, Amit K Roy-Chowdhury, and Ziyan Wu. Spatio-temporal representation factorization for video-based person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 152\u2013162, 2021.", "[66] Shizhen Zhao, Changxin Gao, Jun Zhang, Hao Cheng, Chuchu Han, Xinyang Jiang, Xiaowei Guo, Wei-Shi Zheng, Nong Sang, and Xing Sun. Do not disturb me: Person re-identification under the interference of other pedestrians. In European Conference on Computer Vision, pages 647\u2013663. Springer, 2020.", "[56] Lingxiao He, Yinggang Wang, Wu Liu, He Zhao, Zhenan Sun, and Jiashi Feng. Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 8450\u20138459, 2019.", "[57] Lingxiao He and Wu Liu. Guided saliency feature learning for person re-identification in crowded scenes. In European Conference on Computer Vision, pages 357\u2013373. Springer, 2020."]}, {"table": "<table><tr><td><p>SN</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>PDC, 2017, [76]</p></td><td><p>Market-1501</p></td><td><p>84.14/63.41</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>88.7/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>78.29/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>51.27/\u2013</p></td><td></td></tr><tr><td><p>2</p></td><td><p>RGA, 2020, [107]</p></td><td><p>Market-1501</p></td><td><p>96.1/88.4</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>81.1/77.4</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>79.6/74.5</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>80.3/57.5</p></td><td></td></tr><tr><td><p>3</p></td><td><p>SIA+CIA, 2019, [74]</p></td><td><p>Market-1501</p></td><td><p>94.4/83.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.1/73.4</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>92.4/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>90.1/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>75.5/46.8</p></td><td></td></tr><tr><td><p>4</p></td><td><p>CAM+RAM, 2019, [105]</p></td><td><p>Market-1501</p></td><td><p>94.7/84.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>85.5/72.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (labeled)</p></td><td><p>70.1/66.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>66.6/64.2</p></td><td></td></tr><tr><td><p>5</p></td><td><p>AACN, 2018, [106]</p></td><td><p>Market-1501</p></td><td><p>88.69/82.96</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>76.84/59.25</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01(Detected)</p></td><td><p>88.07/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>81.86/81.61</p></td><td></td></tr><tr><td><p>6</p></td><td><p>PSE, 2018, [93]</p></td><td><p>Market-1501</p></td><td><p>90.3/84</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>85.2/79.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>76.7/71.8</p></td><td></td></tr><tr><td><p>7</p></td><td><p>PaMM, 2016, [78]</p></td><td><p>iLIDS</p></td><td><p>30.3/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>45.0/\u2013</p></td><td></td></tr><tr><td><p>8</p></td><td><p>SCSR, 2016, [100]</p></td><td><p>VIPeR</p></td><td><p>53.54/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>Market-1501</p></td><td><p>51.9/\u2013</p></td><td></td></tr><tr><td><p>9</p></td><td><p>SOMAset, 2018, [86]</p></td><td><p>Market-1501</p></td><td><p>81.29/56.98</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>85.9/\u2013</p></td><td></td></tr><tr><td><p>10</p></td><td><p>PPA, 2021, [87]</p></td><td><p>Market-1501</p></td><td><p>92.4/79.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>85.1/71.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>69.2/66.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>65.5/62.4</p></td><td></td></tr><tr><td><p>11</p></td><td><p>PIE, 2019, [94]</p></td><td><p>Market-1501</p></td><td><p>87.33/69.25</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>80.84/64.09</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>45.88/41.21</p></td><td></td></tr><tr><td><p>12</p></td><td><p>CAN, 2017, [108]</p></td><td><p>Market-1501</p></td><td><p>72.1/47.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01(Labeled)</p></td><td><p>87.2/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>77.6/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>69.2/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>54.1/\u2013</p></td><td></td></tr><tr><td><p>13</p></td><td><p>PaMM, 2018, [104]</p></td><td><p>MARS</p></td><td><p>66.3/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDs</p></td><td><p>57.3/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>79.4/\u2013</p></td><td></td></tr><tr><td><p>14</p></td><td><p>PAFAM, 2020, [98]</p></td><td><p>MARS</p></td><td><p>89.8/81.1</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>84.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>94.6/\u2013</p></td><td></td></tr><tr><td><p>15</p></td><td><p>FGSAM, 2020, [89]</p></td><td><p>Market-1501</p></td><td><p>91.5/85.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>85.9/74.1</p></td><td></td></tr><tr><td><p>16</p></td><td><p>HOReID, 2021, [90]</p></td><td><p>Market-1501</p></td><td><p>97.78/93.94</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>89.83/82.16</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>96.12/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>78.42/54.77</p></td><td></td></tr><tr><td><p>17</p></td><td><p>PBCNN, 2018, [80]</p></td><td><p>CUHK-03(Detected)</p></td><td><p>65.0/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>18</p></td><td><p>PCB, 2019, [103]</p></td><td><p>Market-1501</p></td><td><p>93.8/81.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>84.5/71.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>63.7/57.5</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>69.8/43.6</p></td><td></td></tr><tr><td><p>19</p></td><td><p>KPMM, 2021, [91]</p></td><td><p>Market-1501</p></td><td><p>93.1/84.7</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>71.3/84.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>97.3/96.4</p></td><td></td></tr><tr><td><p>20</p></td><td><p>PGR, 2019, [84]</p></td><td><p>Market-1501</p></td><td><p>93.87/77.21</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.63/65.98</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>92.15/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>89.61/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>60.02/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>66.02/37.87</p></td><td></td></tr><tr><td><p>21</p></td><td><p>PGR, 2019, [84]</p></td><td><p>Market-1501</p></td><td><p>93.87/77.21</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.63/65.98</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>92.15/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>89.61/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>60.02/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>66.02/37.87</p></td><td></td></tr><tr><td><p>22</p></td><td><p>FGPR, 2020, [88]</p></td><td><p>MARS</p></td><td><p>82.9/72.7</p></td><td><p>No</p></td></tr><tr><td><p>23</p></td><td><p>PA-HVPReid, 2021, [99]</p></td><td><p>MARS</p></td><td><p>89.9/79.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>87.9/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>95.9/\u2013</p></td><td></td></tr><tr><td><p>24</p></td><td><p>WSMTAL, 2017, [92]</p></td><td><p>Market-1501</p></td><td><p>56.6/31.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>39.7/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>24.2/\u2013</p></td><td></td></tr><tr><td><p>25</p></td><td><p>WSMTAL, 2017, [92]</p></td><td><p>Market-1501</p></td><td><p>56.6/31.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>39.7/\u2013</p></td><td></td></tr><tr><td><p>26</p></td><td><p>PReID-RS, 2015, [102]</p></td><td><p>CUHK-01</p></td><td><p>31.1/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>33.29/\u2013</p></td><td></td></tr><tr><td><p>27</p></td><td><p>DIFs, 2016, [85]</p></td><td><p>CUHK-01</p></td><td><p>39.46/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>29.35/\u2013</p></td><td></td></tr><tr><td><p>28</p></td><td><p>SNML, 2016, [81]</p></td><td><p>VIPeR</p></td><td><p>33.6/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>29</p></td><td><p>DL-ReID, 2016, [82]</p></td><td><p>CUHK-01(Detected)</p></td><td><p>57.02/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>55.89/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>50.67/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>40.51/\u2013</p></td><td></td></tr></table>", "caption": "Table 6: Results obtained on Pose-variation challenge against each dataset. Results in bold are the highest.", "list_citation_info": ["[90] Pingyu Wang, Zhicheng Zhao, Fei Su, Xingyu Zu, and Nikolaos V Boulgouris. Horeid: Deep high-order mapping enhances pose alignment for person re-identification. IEEE Transactions on Image Processing, 30:2908\u20132922, 2021.", "[106] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli Ouyang. Attention-aware compositional network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2119\u20132128, 2018.", "[98] Yiming Wu, Omar El Farouk Bourahla, Xi Li, Fei Wu, Qi Tian, and Xue Zhou. Adaptive graph representation learning for video person re-identification. IEEE Transactions on Image Processing, 29:8821\u20138830, 2020.", "[99] Xiaoqiang Hu, Dan Wei, Ziyang Wang, Jianglin Shen, and Hongjuan Ren. Hypergraph video pedestrian re-identification based on posture structure relationship and action constraints. Pattern Recognition, 111:107688, 2021.", "[103] Yifan Sun, Liang Zheng, Yali Li, Yi Yang, Qi Tian, and Shengjin Wang. Learning part-based convolutional features for person re-identification. IEEE transactions on pattern analysis and machine intelligence, 2019.", "[92] Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. Multi-type attributes driven multi-camera person re-identification. Pattern Recognition, 75:77\u201389, 2018.", "[74] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Interaction-and-aggregation network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9317\u20139326, 2019.", "[94] Liang Zheng, Yujia Huang, Huchuan Lu, and Yi Yang. Pose-invariant embedding for deep person re-identification. IEEE Transactions on Image Processing, 28(9):4500\u20134509, 2019.", "[84] Jianing Li, Shiliang Zhang, Qi Tian, Meng Wang, and Wen Gao. Pose-guided representation learning for person re-identification. IEEE transactions on pattern analysis and machine intelligence, 2019.", "[104] Yeong-Jun Cho and Kuk-Jin Yoon. Pamm: Pose-aware multi-shot matching for improving person re-identification. IEEE Transactions on Image Processing, 27(8):3739\u20133752, 2018.", "[78] Yeong-Jun Cho and Kuk-Jin Yoon. Improving person re-identification via pose-aware multi-shot matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1354\u20131362, 2016.", "[82] Jin Wang, Zheng Wang, Changxin Gao, Nong Sang, and Rui Huang. Deeplist: Learning deep features with adaptive listwise constraint for person reidentification. IEEE Transactions on Circuits and Systems for Video Technology, 27(3):513\u2013524, 2016.", "[76] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. Pose-driven deep convolutional model for person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 3960\u20133969, 2017.", "[88] Jiahang Yin, Ancong Wu, and Wei-Shi Zheng. Fine-grained person re-identification. International journal of computer vision, 128(6):1654\u20131672, 2020.", "[91] Yantao Shen, Tong Xiao, Shuai Yi, Dapeng Chen, Xiaogang Wang, and Hongsheng Li. Person re-identification with deep kronecker-product matching and group-shuffling random walk. IEEE transactions on pattern analysis and machine intelligence, 43(5):1649\u20131665, 2021.", "[93] M Saquib Sarfraz, Arne Schumann, Andreas Eberle, and Rainer Stiefelhagen. A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 420\u2013429, 2018.", "[86] Igor Barros Barbosa, Marco Cristani, Barbara Caputo, Aleksander Rognhaugen, and Theoharis Theoharis. Looking beyond appearances: Synthetic training data for deep cnns in re-identification. Computer Vision and Image Understanding, 167:50\u201362, 2018.", "[85] Shoubiao Tan, Feng Zheng, Li Liu, Jungong Han, and Ling Shao. Dense invariant feature-based support vector ranking for cross-camera person reidentification. IEEE Transactions on Circuits and Systems for Video Technology, 28(2):356\u2013363, 2016.", "[107] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, and Zhibo Chen. Relation-aware global attention for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3186\u20133195, 2020.", "[105] Wenjie Yang, Houjing Huang, Zhang Zhang, Xiaotang Chen, Kaiqi Huang, and Shu Zhang. Towards rich feature discovery with class activation maps augmentation for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1389\u20131398, 2019.", "[89] Qinqin Zhou, Bineng Zhong, Xiangyuan Lan, Gan Sun, Yulun Zhang, Baochang Zhang, and Rongrong Ji. Fine-grained spatial alignment model for person re-identification with focal triplet loss. IEEE Transactions on Image Processing, 29:7578\u20137589, 2020.", "[80] Yiqiang Chen, Stefan Duffner, Andrei Stoian, Jean-Yves Dufour, and Atilla Baskurt. Deep and low-level feature based attribute learning for person re-identification. Image and Vision Computing, 79:25\u201334, 2018.", "[108] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and Shuicheng Yan. End-to-end comparative attention networks for person re-identification. IEEE Transactions on Image Processing, 26(7):3492\u20133506, 2017.", "[100] Dapeng Chen, Zejian Yuan, Badong Chen, and Nanning Zheng. Similarity learning with spatial constraints for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1268\u20131277, 2016.", "[102] Le An, Mehran Kafai, Songfan Yang, and Bir Bhanu. Person reidentification with reference descriptor. IEEE Transactions on Circuits and Systems for Video Technology, 26(4):776\u2013787, 2015.", "[81] Niall McLaughlin, Jesus Martinez del Rincon, and Paul C Miller. Person reidentification using deep convnets with multitask learning. IEEE Transactions on Circuits and Systems for Video Technology, 27(3):525\u2013539, 2016.", "[87] Zhiyong Li, Jingyi Lv, Ying Chen, and Jin Yuan. Person re-identification with part prediction alignment. Computer Vision and Image Understanding, 205:103172, 2021."]}, {"table": "<table><tr><td><p>Sr.No</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>CAR, 2019, [115]</p></td><td><p>Market-1501</p></td><td><p>96.1/84.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>86.3/73.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>96.9/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>93.2/\u2013</p></td><td></td></tr><tr><td><p>2</p></td><td><p>PRGP-DNN, 2018, [110]</p></td><td><p>Market-1501</p></td><td><p>81.2/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01 (Detected)</p></td><td><p>80.2/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>92.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>51.9/\u2013</p></td><td></td></tr><tr><td><p>3</p></td><td><p>FANN, 2019, [116]</p></td><td><p>Market-1501</p></td><td><p>94.4/82.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>85.2/70.2</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01 (Labeled)</p></td><td><p>98.1/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01 (Detected)</p></td><td><p>81.2/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>92.3/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>70.2/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>58.4/\u2013</p></td><td></td></tr><tr><td><p>4</p></td><td><p>TEM-ReID, 2021, [117]</p></td><td><p>Market-1501</p></td><td><p>95.0/84.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.7/77.0</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>76.8/51.0</p></td><td></td></tr><tr><td><p>5</p></td><td><p>HOG\u2013SVM, 2017, [111]</p></td><td><p>iLIDS</p></td><td><p>73.8/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>6</p></td><td><p>SBSGAN, 2021, [120]</p></td><td><p>Market-1501</p></td><td><p>87.9/80</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>79.7/71.5</p></td><td></td></tr><tr><td><p>7</p></td><td><p>LSTS-NET, 2020, [113]</p></td><td><p>Market-1501</p></td><td><p>95.8/93.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>78.23/72.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>70.11/67.9</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>89.22/83.12</p></td><td></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReID</p></td><td><p>96.81/93.91</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>60.92/\u2013</p></td><td></td></tr><tr><td><p>8</p></td><td><p>LSTM-PDReID, 2020, [112]</p></td><td><p>Market-1501</p></td><td><p>94.48/85.09</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>80.9/64.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>91.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detecte)</p></td><td><p>89.4/\u2013</p></td><td></td></tr><tr><td><p>9</p></td><td><p>DSPL, 2018, [114]</p></td><td><p>Market-1501</p></td><td><p>87.05/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>81.33/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>73.16/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>56.32/\u2013</p></td><td></td></tr><tr><td><p>10</p></td><td><p>MEMF, 2021, [118]</p></td><td><p>Market-1501</p></td><td><p>96.11/89.45</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>82.89/59.8</p></td><td></td></tr><tr><td><p>11</p></td><td><p>WFCB-ReID, 2021, [119]</p></td><td><p>Market-1501</p></td><td><p>98.3/94.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>94.7/90.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>88.6/84.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>84.2/80.6</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>84.9/66.7</p></td><td></td></tr></table>", "caption": "Table 7: Results obtained on Background-clutter challenge against each dataset. Results in bold are the highest.", "list_citation_info": ["[113] Shuai Li, Wenfeng Song, Zheng Fang, Jiaying Shi, Aimin Hao, Qinping Zhao, and Hong Qin. Long-short temporal-spatial clues excited network for robust person re-identification. International Journal of Computer Vision, 128(12):2936\u20132961, 2020.", "[110] Maoqing Tian, Shuai Yi, Hongsheng Li, Shihua Li, Xuesen Zhang, Jianping Shi, Junjie Yan, and Xiaogang Wang. Eliminating background-bias for robust person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5794\u20135803, 2018.", "[117] Yiheng Liu, Wengang Zhou, Jianzhuang Liu, Guo-Jun Qi, Qi Tian, and Houqiang Li. An end-to-end foreground-aware network for person re-identification. IEEE Transactions on Image Processing, 30:2060\u20132071, 2021.", "[114] Sanping Zhou, Jinjun Wang, Deyu Meng, Xiaomeng Xin, Yubing Li, Yihong Gong, and Nanning Zheng. Deep self-paced learning for person re-identification. Pattern Recognition, 76:739\u2013751, 2018.", "[119] Xin Ning, Ke Gong, Weijun Li, Liping Zhang, Xiao Bai, and Shengwei Tian. Feature refinement and filter network for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2020.", "[111] Thi Thanh Thuy Pham, Thi-Lan Le, Hai Vu, Trung Kien Dao, et al. Fully-automated person re-identification in multi-camera surveillance system with a robust kernel descriptor and effective shadow removal method. Image and Vision Computing, 59:44\u201362, 2017.", "[112] Xiang Bai, Mingkun Yang, Tengteng Huang, Zhiyong Dou, Rui Yu, and Yongchao Xu. Deep-person: Learning discriminative deep features for person re-identification. Pattern Recognition, 98:107036, 2020.", "[115] Sanping Zhou, Fei Wang, Zeyi Huang, and Jinjun Wang. Discriminative feature learning with consistent attention regularization for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 8040\u20138049, 2019.", "[120] Yan Huang, Qiang Wu, Jingsong Xu, Yi Zhong, and Zhaoxiang Zhang. Unsupervised domain adaptation with background shift mitigating for person re-identification. International Journal of Computer Vision, 129(7):2244\u20132263, 2021.", "[118] Jia Sun, Yanfeng Li, Houjin Chen, Bin Zhang, and Jinlei Zhu. Memf: Multi-level-attention embedding and multi-layer-feature fusion model for person re-identification. Pattern Recognition, 116:107937, 2021.", "[116] Sanping Zhou, Jinjun Wang, Deyu Meng, Yudong Liang, Yihong Gong, and Nanning Zheng. Discriminative feature learning with foreground attention for person re-identification. IEEE Transactions on Image Processing, 28(9):4671\u20134684, 2019."]}, {"table": "<table><tr><td><p>Sr.No</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>STRF, 2021, [70]</p></td><td><p>MARS</p></td><td><p>90.3/86.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReID</p></td><td><p>97.4/96.4</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>89.3/\u2013</p></td><td></td></tr><tr><td><p>2</p></td><td><p>DenseIL, 2021, [142]</p></td><td><p>MARS</p></td><td><p>90.8/87.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReID</p></td><td><p>97.6/97.1</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>92.0/\u2013</p></td><td></td></tr><tr><td><p>3</p></td><td><p>ABD-Net, 2019, [47]</p></td><td><p>Market-1501</p></td><td><p>95.6/82.28</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>89.0/78.59</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>82.3/60.8</p></td><td></td></tr><tr><td><p>4</p></td><td><p>BAT-net, 2019, [143]</p></td><td><p>Market-1501</p></td><td><p>95.1/87.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.7/77.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>78.6/76.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>76.2/73.2</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>79.5/56.8</p></td><td></td></tr><tr><td><p>5</p></td><td><p>P2-Net, 2019, [144]</p></td><td><p>Market-1501</p></td><td><p>95.2/85.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.5/73.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>78.3/73.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>74.9/68.9</p></td><td></td></tr><tr><td><p>6</p></td><td><p>PAHR, 2017, [146]</p></td><td><p>Market-1501</p></td><td><p>81.0/63.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>85.4/90.9</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>48.7/\u2013</p></td><td></td></tr><tr><td><p>7</p></td><td><p>BBA+PWM, 2015, [140]</p></td><td><p>iLIDS</p></td><td><p>44.3/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>64.1/\u2013</p></td><td></td></tr><tr><td><p>8</p></td><td><p>AP3D, 2020, [152]</p></td><td><p>MARS</p></td><td><p>90.7/85.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReID</p></td><td><p>97.2/96.1</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>88.7/\u2013</p></td><td></td></tr><tr><td><p>9</p></td><td><p>ISP, 2020, [125]</p></td><td><p>Market-1501</p></td><td><p>95.3/88.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>89.6/80</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>76.5/74.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>75.2/71.4</p></td><td></td></tr><tr><td><p>10</p></td><td><p>PCB, 2018, [54]</p></td><td><p>Market-1501</p></td><td><p>93.8/81.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.3/69.2</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>63.7/57.5</p></td><td></td></tr><tr><td><p>11</p></td><td><p>MANCS, 2018, [145]</p></td><td><p>Market-1501</p></td><td><p>93.1/82.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>84.9/71.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>69.0/63.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>65.5/60.5</p></td><td></td></tr><tr><td><p>12</p></td><td><p>PABR, 2018, [123]</p></td><td><p>Market-1501</p></td><td><p>95.4/93.1</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>84.4/69.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01(Labeled)</p></td><td><p>80.7/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01(Detected)</p></td><td><p>90.4/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>91.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>88.0/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>84.7/75.9</p></td><td></td></tr><tr><td><p>13</p></td><td><p>VAPM, 2019, [126]</p></td><td><p>Market-1501</p></td><td><p>93.0/80.8</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.6/72.6</p></td><td></td></tr><tr><td><p>14</p></td><td><p>EANet, 2019, [153]</p></td><td><p>Market-1501</p></td><td><p>94.6/85.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.5/74.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>72.5/66.8</p></td><td></td></tr><tr><td><p>15</p></td><td><p>DSA-reID, 2019, [121]</p></td><td><p>Market-1501</p></td><td><p>95.7/87.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.2/74.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>90.4/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>78.9/75.2</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>78.2/73.1</p></td><td></td></tr><tr><td><p>16</p></td><td><p>DSR, 2018, [122]</p></td><td><p>Market-1501</p></td><td><p>83.58/64.25</p></td><td><p>No</p></td></tr><tr><td><p>17</p></td><td><p>HA-CNN, 2018, [50]</p></td><td><p>Market-1501</p></td><td><p>91.2/75.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>80.5/63.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>44.4/41</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>41.7/38.6</p></td><td></td></tr><tr><td><p>18</p></td><td><p>SPReID, 2018, [128]</p></td><td><p>Market-1501</p></td><td><p>94.63/90.96</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.96/84.99</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>96.22/\u2013</p></td><td></td></tr><tr><td><p>19</p></td><td><p>Spindle Net, 2017, [124]</p></td><td><p>Market-1501</p></td><td><p>76.9/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>79.9/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>88.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>66.3/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>67/\u2013</p></td><td></td></tr><tr><td><p>20</p></td><td><p>CDPM, 2020, [148]</p></td><td><p>Market-1501</p></td><td><p>92.2/86.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.2/77.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>81.4/77.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>78.8/73.3</p></td><td></td></tr><tr><td><p>21</p></td><td><p>BCD-Net, 2020, [149]</p></td><td><p>Market-1501</p></td><td><p>97/92.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>91.1/81.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>86.2/81.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>84.2/78.7</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>84.1/63.7</p></td><td></td></tr><tr><td><p>22</p></td><td><p>PCB, 2020, [150]</p></td><td><p>Market-1501</p></td><td><p>95.5/88.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>89.1/79.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>72.5/69.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>81.4/59.7</p></td><td></td></tr><tr><td><p>23</p></td><td><p>RMGL, 2020, [133]</p></td><td><p>Market-1501</p></td><td><p>96.2/90.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>90.7/81.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>20.8/18.7</p></td><td></td></tr><tr><td><p>24</p></td><td><p>ST2N, 2019, [134]</p></td><td><p>MARS</p></td><td><p>80.5/69.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>57.7/\u2013</p></td><td></td></tr><tr><td><p>25</p></td><td><p>MPN, 2020, [132]</p></td><td><p>Market-1501</p></td><td><p>96.3/89.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>91.5/82.0</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>85.0/81.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>83.4/79.1</p></td><td></td></tr><tr><td><p>26</p></td><td><p>ADP, 2018, [147]</p></td><td><p>Market-1501</p></td><td><p>94.99/86.47</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.04/74.57</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>96.43/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>93.58/\u2013</p></td><td></td></tr><tr><td><p>27</p></td><td><p>AlignedReID++ , 2019, [129]</p></td><td><p>Market-1501</p></td><td><p>92.8/89.4</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.2/82.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>67.9/70.7</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>69.8/43.7</p></td><td></td></tr><tr><td><p>28</p></td><td><p>APDR, 2020, [135]</p></td><td><p>Market-1501</p></td><td><p>94.4/90.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.3/83.2</p></td><td></td></tr><tr><td><p>29</p></td><td><p>3D-SAA+CMIL, 2021, [136]</p></td><td><p>MARS</p></td><td><p>81.3/72.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReId</p></td><td><p>82.8/81.0</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>54.7/\u2013</p></td><td></td></tr><tr><td><p>30</p></td><td><p>PGCN, 2021, [141]</p></td><td><p>Market-1501</p></td><td><p>98.0/94.8</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>91.1/85.2</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>86.7/83.6</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>87.7/72.7</p></td><td></td></tr><tr><td><p>31</p></td><td><p>TS-DTW, 2017, [137]</p></td><td><p>iLIDS</p></td><td><p>31.5/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>41.7/\u2013</p></td><td></td></tr><tr><td><p>32</p></td><td><p>DPML, 2017, [138]</p></td><td><p>CUHK-01</p></td><td><p>75.9/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>84.0/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>51.7/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>82.2/\u2013</p></td><td></td></tr><tr><td><p>33</p></td><td><p>PAN, 2019, [151]</p></td><td><p>Market-1501</p></td><td><p>82.81/63.35</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>71.59/51.51</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>36.86/35.03</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>36.29/34.0</p></td><td></td></tr></table>", "caption": "Table 8: Results obtained on Misalignment challenge against each dataset. Results in bold are the highest.", "list_citation_info": ["[151] Zhedong Zheng, Liang Zheng, and Yi Yang. Pedestrian alignment network for large-scale person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 29(10):3037\u20133045, 2018.", "[152] Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, and Xilin Chen. Appearance-preserving 3d convolution for video-based person re-identification. In European Conference on Computer Vision, pages 228\u2013243. Springer, 2020.", "[148] Kan Wang, Changxing Ding, Stephen J Maybank, and Dacheng Tao. Cdpm: convolutional deformable part models for semantically aligned person re-identification. IEEE Transactions on Image Processing, 29:3416\u20133428, 2019.", "[134] Ju Dai, Pingping Zhang, Dong Wang, Huchuan Lu, and Hongyu Wang. Video person re-identification by temporal residual learning. IEEE Transactions on Image Processing, 28(3):1366\u20131377, 2018.", "[146] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. Deeply-learned part-aligned representations for person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 3219\u20133228, 2017.", "[54] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline). In Proceedings of the European Conference on Computer Vision (ECCV), pages 480\u2013496, 2018.", "[122] Lingxiao He, Jian Liang, Haiqing Li, and Zhenan Sun. Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7073\u20137082, 2018.", "[50] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious attention network for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2285\u20132294, 2018.", "[133] Guanshuo Wang, Yufeng Yuan, Jiwei Li, Shiming Ge, and Xi Zhou. Receptive multi-granularity representation for person re-identification. IEEE Transactions on Image Processing, 29:6096\u20136109, 2020.", "[135] Shuzhao Li, Huimin Yu, and Roland Hu. Attributes-aided part detection and refinement for person re-identification. Pattern Recognition, 97:107016, 2020.", "[126] Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, and Jian Sun. Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 393\u2013402, 2019.", "[124] Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, and Xiaoou Tang. Spindle net: Person re-identification with human body region guided feature decomposition and fusion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1077\u20131085, 2017.", "[143] Pengfei Fang, Jieming Zhou, Soumava Kumar Roy, Lars Petersson, and Mehrtash Harandi. Bilinear attention networks for person retrieval. In Proceedings of the IEEE International Conference on Computer Vision, pages 8030\u20138039, 2019.", "[142] Tianyu He, Xin Jin, Xu Shen, Jianqiang Huang, Zhibo Chen, and Xian-Sheng Hua. Dense interaction learning for video-based person re-identification. arXiv preprint arXiv:2103.09013, 2021.", "[138] S\u0142awomir B\u0105k and Peter Carr. Deep deformable patch metric learning for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 28(10):2690\u20132702, 2017.", "[137] Xiaolong Ma, Xiatian Zhu, Shaogang Gong, Xudong Xie, Jianming Hu, Kin-Man Lam, and Yisheng Zhong. Person re-identification by unsupervised video matching. Pattern Recognition, 65:197\u2013210, 2017.", "[144] Jianyuan Guo, Yuhui Yuan, Lang Huang, Chao Zhang, Jin-Ge Yao, and Kai Han. Beyond human parts: Dual part-aligned representations for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3642\u20133651, 2019.", "[153] Houjing Huang, Wenjie Yang, Xiaotang Chen, Xin Zhao, Kaiqi Huang, Jinbin Lin, Guan Huang, and Dalong Du. Eanet: Enhancing alignment for cross-domain person re-identification. arXiv preprint arXiv:1812.11369, 2018.", "[129] Hao Luo, Wei Jiang, Xuan Zhang, Xing Fan, Jingjing Qian, and Chi Zhang. Alignedreid++: Dynamically matching local information for person re-identification. Pattern Recognition, 94:53\u201361, 2019.", "[132] Changxing Ding, Kan Wang, Pengfei Wang, and Dacheng Tao. Multi-task learning with coarse priors for robust part-aware person re-identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[147] Fan Yang, Ke Yan, Shijian Lu, Huizhu Jia, Xiaodong Xie, and Wen Gao. Attention driven person re-identification. Pattern Recognition, 86:143\u2013155, 2019.", "[128] Mahdi M Kalayeh, Emrah Basaran, Muhittin G\u00f6kmen, Mustafa E Kamasak, and Mubarak Shah. Human semantic parsing for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1062\u20131071, 2018.", "[149] Kan Wang, Pengfei Wang, Changxing Ding, and Dacheng Tao. Batch coherence-driven network for part-aware person re-identification. IEEE Transactions on Image Processing, 30:3405\u20133418, 2021.", "[136] Wei Shi, Hong Liu, and Mengyuan Liu. Image-to-video person re-identification using three-dimensional semantic appearance alignment and cross-modal interactive learning. Pattern Recognition, 122:108314, 2022.", "[141] Zhong Zhang, Haijia Zhang, Shuang Liu, Yuan Xie, and Tariq S Durrani. Part-guided graph convolution networks for person re-identification. Pattern Recognition, 120:108155, 2021.", "[121] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Densely semantically aligned person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 667\u2013676, 2019.", "[70] Abhishek Aich, Meng Zheng, Srikrishna Karanam, Terrence Chen, Amit K Roy-Chowdhury, and Ziyan Wu. Spatio-temporal representation factorization for video-based person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 152\u2013162, 2021.", "[140] Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin Wu, and Jingdong Wang. Person re-identification with correspondence structure learning. In Proceedings of the IEEE international conference on computer vision, pages 3200\u20133208, 2015.", "[123] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Kyoung Mu Lee. Part-aligned bilinear representations for person re-identification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 402\u2013419, 2018.", "[150] Zhizhong Zhang, Yuan Xie, Ding Li, Wensheng Zhang, and Qi Tian. Learning to align via wasserstein for person re-identification. IEEE Transactions on Image Processing, 29:7104\u20137116, 2020.", "[125] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao Wang. Identity-guided human semantic parsing for person re-identification. arXiv preprint arXiv:2007.13467, 2020.", "[47] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-net: Attentive but diverse person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 8351\u20138361, 2019.", "[145] Cheng Wang, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Mancs: A multi-task attentional network with curriculum sampling for person re-identification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 365\u2013381, 2018."]}, {"table": "<table><tr><td><p>Sr.No</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>BV-Person, 2021, [164]</p></td><td><p>Market-1501</p></td><td><p>96.0/89.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>90.5/80.6</p></td><td></td></tr><tr><td><p>2</p></td><td><p>OSNet, 2019, [157]</p></td><td><p>Market-1501</p></td><td><p>94.8/84.9</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>88.6/73.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>72.3/67.8</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>68.0/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>78.7/52.9</p></td><td></td></tr><tr><td><p>3</p></td><td><p>MSDL, 2017, [155]</p></td><td><p>CUHK-01</p></td><td><p>79.01/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>75.64/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>43.03/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>41.0/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>65.0/\u2013</p></td><td></td></tr><tr><td><p>4</p></td><td><p>DPFL, 2017, [154]</p></td><td><p>Market-1501</p></td><td><p>92.3/80.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>79.2/60.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>86.7/82.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>82/78.1</p></td><td></td></tr><tr><td><p>5</p></td><td><p>CTL, 2021, [158]</p></td><td><p>MARS</p></td><td><p>91.4/86.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>89.7/\u2013</p></td><td></td></tr><tr><td><p>6</p></td><td><p>CFPModel, 2019, [156]</p></td><td><p>Market-1501</p></td><td><p>95.7/88.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>89.0/79.0</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>78.9/76.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>78.9/74.8</p></td><td></td></tr><tr><td><p>7</p></td><td><p>AKA, 2019, [162]</p></td><td><p>Market-1501</p></td><td><p>49.7/24.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>47.6/31.1</p></td><td></td></tr><tr><td><p>8</p></td><td><p>STNs, 2018, [163]</p></td><td><p>CUHK-01</p></td><td><p>88.2/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>87.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>86.45/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>50.1/\u2013</p></td><td></td></tr><tr><td><p>9</p></td><td><p>MSTA, 2020, [166]</p></td><td><p>MARS</p></td><td><p>82.28/69.42</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>70.1/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>91.2/\u2013</p></td><td></td></tr><tr><td><p>10</p></td><td><p>DPRM, 2021, [167]</p></td><td><p>MARS</p></td><td><p>89.0/83.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReId</p></td><td><p>97.1/95.6</p></td><td></td></tr><tr><td><p>11</p></td><td><p>PyrAttNet, 2020, [168]</p></td><td><p>Market-1501</p></td><td><p>97.8/95.8</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>93.0/90.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>86.8/88.0</p></td><td></td></tr><tr><td><p>12</p></td><td><p>M3D-CNN, 2020, [159]</p></td><td><p>MARS</p></td><td><p>88.87/85.46</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReId</p></td><td><p>95.49/93.67</p></td><td></td></tr><tr><td><p>13</p></td><td><p>APNet, 2021, [169]</p></td><td><p>Market-1501</p></td><td><p>96.2/90.5</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>90.4/81.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>87.4/85.3</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>83.7/63.5</p></td><td></td></tr><tr><td><p>14</p></td><td><p>PFE, 2021, [170]</p></td><td><p>Market-1501</p></td><td><p>95.2/87.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>89.2/77.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>74.0/71.1</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>82.0/56.2</p></td><td></td></tr><tr><td><p>15</p></td><td><p>PREST, 2021, [172]</p></td><td><p>Market-1501</p></td><td><p>82.5/62.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>74.4/56.1</p></td><td></td></tr><tr><td><p>16</p></td><td><p>MuDeep, 2020, [165]</p></td><td><p>Market-1501</p></td><td><p>95.34/84.66</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.19/75.63</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>98.73/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>95.84/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>93.70/\u2013</p></td><td></td></tr><tr><td><p>17</p></td><td><p>OSNet, 2021, [160]</p></td><td><p>Market-1501</p></td><td><p>94.8/86.7</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.7/76.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>86.6/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>72.3/67.8</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>68.0/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>79.1/55.1</p></td><td></td></tr><tr><td><p>18</p></td><td><p>IALM, 2020, [161]</p></td><td><p>CUHK-01</p></td><td><p>68.44/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>56.32/\u2013</p></td><td></td></tr><tr><td><p>19</p></td><td><p>MOAN, 2020, [171]</p></td><td><p>Market-1501</p></td><td><p>97.45/96.42</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>93.81/92.82</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>90.07/90.32</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>81.53/58.02</p></td><td></td></tr></table>", "caption": "Table 9: Results obtained on Scale challenge against each dataset. Results in bold are the highest.", "list_citation_info": ["[156] Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao Yu, Feiyue Huang, and Rongrong Ji. Pyramidal person re-identification via multi-loss dynamic training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8514\u20138522, 2019.", "[168] Niki Martinel, Gian Luca Foresti, and Christian Micheloni. Deep pyramidal pooling with attention for person re-identification. IEEE Transactions on Image Processing, 29:7306\u20137316, 2020.", "[163] Yiluan Guo and Ngai-Man Cheung. Efficient and deep person re-identification using multi-level similarity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2335\u20132344, 2018.", "[169] Guangyi Chen, Tianpei Gu, Jiwen Lu, Jin-An Bao, and Jie Zhou. Person re-identification via attention pyramid. IEEE Transactions on Image Processing, 30:7663\u20137676, 2021.", "[155] Xuelin Qian, Yanwei Fu, Yu-Gang Jiang, Tao Xiang, and Xiangyang Xue. Multi-scale deep learning architectures for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 5399\u20135408, 2017.", "[170] Yingji Zhong, Yaowei Wang, and Shiliang Zhang. Progressive feature enhancement for person re-identification. IEEE Transactions on Image Processing, 30:8384\u20138395, 2021.", "[161] Cairong Zhao, Xuekuan Wang, Wangmeng Zuo, Fumin Shen, Ling Shao, and Duoqian Miao. Similarity learning with joint transfer constraints for person re-identification. Pattern Recognition, 97:107014, 2020.", "[162] Ancong Wu, Wei-Shi Zheng, Xiaowei Guo, and Jian-Huang Lai. Distilled person re-identification: Towards a more scalable system. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1187\u20131196, 2019.", "[160] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. Learning generalisable omni-scale representations for person re-identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[166] Wei Zhang, Xuanyu He, Xiaodong Yu, Weizhi Lu, Zhengjun Zha, and Qi Tian. A multi-scale spatial-temporal attention model for person re-identification in videos. IEEE Transactions on Image Processing, 29:3365\u20133373, 2019.", "[171] Yewen Huang, Sicheng Lian, Haifeng Hu, Dihu Chen, and Tao Su. Multiscale omnibearing attention networks for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 31(5):1790\u20131803, 2020.", "[154] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Person re-identification by deep learning multi-scale representations. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 2590\u20132600, 2017.", "[164] Cheng Yan, Guansong Pang, Lei Wang, Jile Jiao, Xuetao Feng, Chunhua Shen, and Jingjing Li. Bv-person: A large-scale dataset for bird-view person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10943\u201310952, 2021.", "[157] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. Omni-scale feature learning for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3702\u20133712, 2019.", "[158] Jiawei Liu, Zheng-Jun Zha, Wei Wu, Kecheng Zheng, and Qibin Sun. Spatial-temporal correlation and topology learning for person re-identification in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4370\u20134379, 2021.", "[159] Jianing Li, Shiliang Zhang, and Tiejun Huang. Multi-scale temporal cues learning for video person re-identification. IEEE Transactions on Image Processing, 29:4461\u20134473, 2020.", "[165] X Qian, Y Fu, T Xiang, YG Jiang, and X Xue. Leader-based multi-scale attention deep architecture for person re-identification. IEEE transactions on pattern analysis and machine intelligence, 42(2):371, 2020.", "[172] Hang Zhang, Huanhuan Cao, Xu Yang, Cheng Deng, and Dacheng Tao. Self-training with progressive representation enhancement for unsupervised cross-domain person re-identification. IEEE Transactions on Image Processing, 2021.", "[167] Xi Yang, Liangchen Liu, Nannan Wang, and Xinbo Gao. A two-stream dynamic pyramid representation model for video-based person re-identification. IEEE Transactions on Image Processing, 30:6266\u20136276, 2021."]}, {"table": "<table><tr><td><p>Sr.No</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>CAMEL, 2017, [201]</p></td><td><p>Market-1501</p></td><td><p>54.5/26.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>61.9/57.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>39.4/31.9</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>30.9/\u2013</p></td><td></td></tr><tr><td><p>2</p></td><td><p>LOMO+GOG, 2017, [108]</p></td><td><p>MARS</p></td><td><p>23.9/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>41.7/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>80.9/\u2013</p></td><td></td></tr><tr><td><p>3</p></td><td><p>DVDL, 2015, [174]</p></td><td><p>iLIDS</p></td><td><p>25.9/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>40.6/\u2013</p></td><td></td></tr><tr><td><p>4</p></td><td><p>VKD, 2020, [176]</p></td><td><p>MARS</p></td><td><p>88.74/82.22</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReId</p></td><td><p>95.01/93.41</p></td><td></td></tr><tr><td><p>5</p></td><td><p>GCL, 2021, [158]</p></td><td><p>Market-1501</p></td><td><p>90.5/75.4</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>81.9/67.6</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>54.4/29.7</p></td><td></td></tr><tr><td><p>6</p></td><td><p>PersonX, 2019, [173]</p></td><td><p>Market-1501</p></td><td><p>93.0/80.8</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.6/72.6</p></td><td></td></tr><tr><td><p>7</p></td><td><p>ECN, 2019, [200]</p></td><td><p>Market-1501</p></td><td><p>63.3/40.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>63.3/40.4</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>30.2/10.2</p></td><td></td></tr><tr><td><p>8</p></td><td><p>CASN, 2019, [203]</p></td><td><p>Market-1501</p></td><td><p>94.4/82.8</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.7/73.7</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>73.7/68</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>71.5/64.4</p></td><td></td></tr><tr><td><p>9</p></td><td><p>LOMO-XQDA, 2015, [175]</p></td><td><p>CUHK-03(Labeled)</p></td><td><p>52.25/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>46.25/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>40/\u2013</p></td><td></td></tr><tr><td><p>10</p></td><td><p>FC2, 2018, [184]</p></td><td><p>Market-1501</p></td><td><p>48.06/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>37.41/\u2013</p></td><td></td></tr><tr><td><p>11</p></td><td><p>ICV-ECCL, 2018, [181]</p></td><td><p>Market-1501</p></td><td><p>90.6/77.3</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>83.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>88.6/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>51.9/\u2013</p></td><td></td></tr><tr><td><p>12</p></td><td><p>BINet, 2021, [185]</p></td><td><p>Market-1501</p></td><td><p>95.3/88.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReId</p></td><td><p>91.4/81.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>73.6/72.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>72.3/69.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>76.1/52.8</p></td><td></td></tr><tr><td><p>13</p></td><td><p>SMC-ECD, 2018, [186]</p></td><td><p>Market-1501</p></td><td><p>80.31/59.68</p></td><td><p>No</p></td></tr><tr><td><p>14</p></td><td><p>DCIA, 2021, [170]</p></td><td><p>PRID-2011</p></td><td><p>32.5/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>64.78/\u2013</p></td><td></td></tr><tr><td><p>15</p></td><td><p>VRNNs, 2020, [188]</p></td><td><p>MARS</p></td><td><p>61.2/52.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>64.6/\u2013</p></td><td></td></tr><tr><td><p>16</p></td><td><p>MGN, 2020, [189]</p></td><td><p>Market-1501</p></td><td><p>95.8/88.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>90/79.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>78.8/74.4</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>84.0/62.4</p></td><td></td></tr><tr><td><p>17</p></td><td><p>UDA, 2020, [190]</p></td><td><p>Market-1501</p></td><td><p>73.3/38.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>56.1/30.6</p></td><td></td></tr><tr><td><p>18</p></td><td><p>VIH-ReID, 2015, [179]</p></td><td><p>VIPeR</p></td><td><p>21.4/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>19</p></td><td><p>CRAFT-MFA, 2018, [180]</p></td><td><p>Market-1501</p></td><td><p>77.0/50.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>74.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>84.3/72.41</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>50.3/\u2013</p></td><td></td></tr><tr><td><p>20</p></td><td><p>DECAMEL, 2020, [202]</p></td><td><p>Market-1501</p></td><td><p>60.24/32.44</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>65.81/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>38.27/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>30.34/11.13</p></td><td></td></tr><tr><td><p>21</p></td><td><p>CSPL, 2018, [177]</p></td><td><p>CUHK-01</p></td><td><p>72.02/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>70.2/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>66.8/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>51.3/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>69.20/\u2013</p></td><td></td></tr><tr><td><p>22</p></td><td><p>DAM, 2019, [191]</p></td><td><p>MARS</p></td><td><p>74.65/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>77.3/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>87.0/\u2013</p></td><td></td></tr><tr><td><p>23</p></td><td><p>GMDA-RC, 2018, [192]</p></td><td><p>CUHK-01</p></td><td><p>75.69/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>67.09/\u2013</p></td><td></td></tr><tr><td><p>24</p></td><td><p>VS-SSL, 2020, [193]</p></td><td><p>Market-1501</p></td><td><p>74.8/51.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>73.0/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>44.8/\u2013</p></td><td></td></tr><tr><td><p>25</p></td><td><p>CVDCA, 2016, [178]</p></td><td><p>CUHK-01</p></td><td><p>47.8/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>47.78/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>57.6/\u2013</p></td><td></td></tr><tr><td><p>26</p></td><td><p>VCFL, 2021, [204]</p></td><td><p>Market-1501</p></td><td><p>91.85/76.97</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>82.68/65.68</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>61.29/54.26</p></td><td></td></tr><tr><td><p>27</p></td><td><p>MPML, 2019, [194]</p></td><td><p>Market-1501</p></td><td><p>55.61/27.58</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>64.98/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>44.72/\u2013</p></td><td></td></tr><tr><td><p>28</p></td><td><p>AANet, 2021, [205]</p></td><td><p>Market-1501</p></td><td><p>96.1/87.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>90.2/79.5</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>82.7/76.7</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>77.2/70.5</p></td><td></td></tr><tr><td><p>29</p></td><td><p>AVA-ReID, 2020, [195]</p></td><td><p>Market-1501</p></td><td><p>88.6/73.1</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>86.7/83.8</p></td><td></td></tr><tr><td><p>30</p></td><td><p>CCM, 2021, [197]</p></td><td><p>DukeMTMC-VideoReID</p></td><td><p>66.0/42.3</p></td><td><p>No</p></td></tr><tr><td><p>31</p></td><td><p>RCN-ReID, 2019, [198]</p></td><td><p>MARS</p></td><td><p>48.0/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>65.0/\u2013</p></td><td></td></tr><tr><td><p>32</p></td><td><p>MLCPL, 2018, [199]</p></td><td><p>CUHK-01</p></td><td><p>34.05/\u2013</p></td><td><p>No</p></td></tr></table>", "caption": "Table 10: Results obtained on viewpoint variation challenge against each dataset. Results in bold are the highest.", "list_citation_info": ["[191] Jingke Meng, Ancong Wu, and Wei-Shi Zheng. Deep asymmetric video-based person re-identification. Pattern Recognition, 93:430\u2013441, 2019.", "[188] Lin Wu, Yang Wang, Hongzhi Yin, Meng Wang, and Ling Shao. Few-shot deep adversarial learning for video-based person re-identification. IEEE Transactions on Image Processing, 29:1233\u20131245, 2020.", "[194] Hai-Miao Hu, Wen Fang, Bo Li, and Qi Tian. An adaptive multi-projection metric learning for person re-identification across non-overlapping cameras. IEEE Transactions on Circuits and Systems for Video Technology, 29(9):2809\u20132821, 2019.", "[177] Ju Dai, Ying Zhang, Huchuan Lu, and Hongyu Wang. Cross-view semantic projection learning for person re-identification. Pattern Recognition, 75:63\u201376, 2018.", "[174] Srikrishna Karanam, Yang Li, and Richard J Radke. Person re-identification with discriminatively trained viewpoint invariant dictionaries. In Proceedings of the IEEE international conference on computer vision, pages 4516\u20134524, 2015.", "[185] Xiumei Chen, Xiangtao Zheng, and Xiaoqiang Lu. Bidirectional interaction network for person re-identification. IEEE Transactions on Image Processing, 30:1935\u20131948, 2021.", "[176] Angelo Porrello, Luca Bergamini, and Simone Calderara. Robust re-identification by multiple views knowledge distillation. In European Conference on Computer Vision, pages 93\u2013110. Springer, 2020.", "[201] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Cross-view asymmetric metric learning for unsupervised person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 994\u20131002, 2017.", "[189] Houjing Huang, Wenjie Yang, Jinbin Lin, Guan Huang, Jiamiao Xu, Guoli Wang, Xiaotang Chen, and Kaiqi Huang. Improve person re-identification with part awareness learning. IEEE Transactions on Image Processing, 29:7468\u20137481, 2020.", "[180] Ying-Cong Chen, Xiatian Zhu, Wei-Shi Zheng, and Jian-Huang Lai. Person re-identification by camera correlation aware feature augmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(2):392\u2013408, 2018.", "[195] Lin Wu, Richang Hong, Yang Wang, and Meng Wang. Cross-entropy adversarial view adaptation for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 30(7):2081\u20132092, 2020.", "[181] Zhanxiang Feng, Jianhuang Lai, and Xiaohua Xie. Learning view-specific deep networks for person re-identification. IEEE Transactions on Image Processing, 27(7):3472\u20133483, 2018.", "[175] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. Person re-identification by local maximal occurrence representation and metric learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2197\u20132206, 2015.", "[170] Yingji Zhong, Yaowei Wang, and Shiliang Zhang. Progressive feature enhancement for person re-identification. IEEE Transactions on Image Processing, 30:8384\u20138395, 2021.", "[184] Lin Wu, Yang Wang, Zongyuan Ge, Qichang Hu, and Xue Li. Structured deep hashing with convolutional neural networks for fast person re-identification. Computer Vision and Image Understanding, 167:63\u201373, 2018.", "[203] Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J Radke. Re-identification with consistent attentive siamese networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5735\u20135744, 2019.", "[198] Wei Zhang, Yimeng Li, Weizhi Lu, Xinshun Xu, Zhaowei Liu, and Xiangyang Ji. Learning intra-video difference for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 29(10):3028\u20133036, 2018.", "[197] Xueping Wang, Rameswar Panda, Min Liu, Yaonan Wang, and Amit K Roy-Chowdhury. Exploiting global camera network constraints for unsupervised video person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2020.", "[179] Z Wu, Y Li, and RJ Radke. Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features. IEEE transactions on pattern analysis and machine intelligence, 37(5):1095, 2015.", "[190] Yutian Lin, Yu Wu, Chenggang Yan, Mingliang Xu, and Yi Yang. Unsupervised person re-identification via cross-camera similarity exploration. IEEE Transactions on Image Processing, 29:5481\u20135490, 2020.", "[173] Xiaoxiao Sun and Liang Zheng. Dissecting person re-identification from the viewpoint of viewpoint. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 608\u2013617, 2019.", "[200] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi Yang. Invariance matters: Exemplar memory for domain adaptive person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 598\u2013607, 2019.", "[186] Alessandro Borgia, Yang Hua, Elyor Kodirov, and Neil M Robertson. Cross-view discriminative feature learning for person re-identification. IEEE Transactions on Image Processing, 27(11):5338\u20135349, 2018.", "[192] Cairong Zhao, Xuekuan Wang, Duoqian Miao, Hanli Wang, Weishi Zheng, Yong Xu, and David Zhang. Maximal granularity structure and generalized multi-view discriminant analysis for person re-identification. Pattern Recognition, 79:79\u201396, 2018.", "[205] Sicheng Lian, Weitao Jiang, and Haifeng Hu. Attention-aligned network for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2021.", "[108] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and Shuicheng Yan. End-to-end comparative attention networks for person re-identification. IEEE Transactions on Image Processing, 26(7):3492\u20133506, 2017.", "[158] Jiawei Liu, Zheng-Jun Zha, Wei Wu, Kecheng Zheng, and Qibin Sun. Spatial-temporal correlation and topology learning for person re-identification in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4370\u20134379, 2021.", "[204] Lei Zhang, Fangyi Liu, and David Zhang. Adversarial view confusion feature learning for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 31(4):1490\u20131502, 2021.", "[178] Ying-Cong Chen, Wei-Shi Zheng, Jian-Huang Lai, and Pong C Yuen. An asymmetric distance model for cross-view feature mapping in person reidentification. IEEE transactions on circuits and systems for video technology, 27(8):1661\u20131675, 2016.", "[193] Jieru Jia, Qiuqi Ruan, Yi Jin, Gaoyun An, and Shiming Ge. View-specific subspace learning and re-ranking for semi-supervised person re-identification. Pattern Recognition, 108:107568, 2020.", "[202] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Unsupervised person re-identification by deep asymmetric metric embedding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(4):956\u2013973, 2020.", "[199] Le An, Zhen Qin, Xiaojing Chen, and Songfan Yang. Multi-level common space learning for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 28(8):1777\u20131787, 2018."]}, {"table": "<table><tr><td><p>Sr.No</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>JUDEA, 2015, [208]</p></td><td><p>VIPeR</p></td><td><p>25.87/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>2</p></td><td><p>PRI, 2020, [210]</p></td><td><p>Market-1501</p></td><td><p>86.9/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.1/\u2013</p></td><td></td></tr><tr><td><p>3</p></td><td><p>INTACT, 2020, [217]</p></td><td><p>Market-1501</p></td><td><p>88.1/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>81.2/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>86.4/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>46.2/\u2013</p></td><td></td></tr><tr><td><p>4</p></td><td><p>DaRe, 2018, [207]</p></td><td><p>Market-1501</p></td><td><p>90.9/86.7</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>84.4/80.0</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>73.8/74.7</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>70.6/71.6</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>85.1/81.9</p></td><td></td></tr><tr><td><p>5</p></td><td><p>SLD2L, 2015, [209]</p></td><td><p>CUHK-01</p></td><td><p>24.48/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>16.86/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>33.33/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>22.6/\u2013</p></td><td></td></tr><tr><td><p>6</p></td><td><p>MCSLD2L, 2017, [212]</p></td><td><p>CUHK-01</p></td><td><p>27.86/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>20.79/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>38.04/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>26.30/\u2013</p></td><td></td></tr><tr><td><p>7</p></td><td><p>RKD, 2021, [213]</p></td><td><p>Market-1501</p></td><td><p>93.4/83.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>85.8/73.0</p></td><td></td></tr><tr><td><p>8</p></td><td><p>DI-REID, 2020, [218]</p></td><td><p>MSMT-17</p></td><td><p>75.5/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>9</p></td><td><p>JLCF, 2016, [216]</p></td><td><p>VIPeR</p></td><td><p>26.27/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>10</p></td><td><p>M3L, 2017, [215]</p></td><td><p>VIPeR</p></td><td><p>30.22/\u2013</p></td><td><p>No</p></td></tr></table>", "caption": "Table 11: Results obtained on low resolution and illumination variance challenge against each dataset. Results in bold are the highest.", "list_citation_info": ["[213] Zhanxiang Feng, Jianhuang Lai, and Xiaohua Xie. Resolution-aware knowledge distillation for efficient inference. IEEE Transactions on Image Processing, 30:6985\u20136996, 2021.", "[208] Xiang Li, Wei-Shi Zheng, Xiaojuan Wang, Tao Xiang, and Shaogang Gong. Multi-scale learning for low-resolution person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3765\u20133773, 2015.", "[216] Rahul Rama Varior, Gang Wang, Jiwen Lu, and Ting Liu. Learning invariant color features for person reidentification. IEEE Transactions on Image Processing, 25(7):3395\u20133410, 2016.", "[215] Xiaokai Liu, Xiaorui Ma, Jie Wang, and Hongyu Wang. M3l: Multi-modality mining for metric learning in person re-identification. Pattern Recognition, 76:650\u2013661, 2018.", "[212] XY Jing, X Zhu, F Wu, R Hu, X You, Y Wang, H Feng, and JY Yang. Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning. IEEE Transactions on Image Processing: a Publication of the IEEE Signal Processing Society, 26(3):1363\u20131378, 2017.", "[207] Yan Wang, Lequn Wang, Yurong You, Xu Zou, Vincent Chen, Serena Li, Gao Huang, Bharath Hariharan, and Kilian Q Weinberger. Resource aware person re-identification across multiple resolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8042\u20138051, 2018.", "[209] Xiao-Yuan Jing, Xiaoke Zhu, Fei Wu, Xinge You, Qinglong Liu, Dong Yue, Ruimin Hu, and Baowen Xu. Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 695\u2013704, 2015.", "[217] Zhiyi Cheng, Qi Dong, Shaogang Gong, and Xiatian Zhu. Inter-task association critic for cross-resolution person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2605\u20132615, 2020.", "[218] Yukun Huang, Zheng-Jun Zha, Xueyang Fu, Richang Hong, and Liang Li. Real-world person re-identification via degradation invariance learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14084\u201314094, 2020.", "[210] Ke Han, Yan Huang, Zerui Chen, Liang Wang, and Tieniu Tan. Prediction and recovery for adaptive low-resolution person re-identification. In European Conference on Computer Vision, pages 193\u2013209. Springer, 2020."]}, {"table": "<table><tr><td><p>Sr.No</p></td><td><p>Paper</p></td><td><p>Dataset</p></td><td><p>R1/mAP</p></td><td><p>Code availability</p></td></tr><tr><td><p>1</p></td><td><p>CCL-PDA-FA, 2021, [258]</p></td><td><p>Market-1501</p></td><td><p>94.2/83.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.5/70.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>66.6/36.3</p></td><td></td></tr><tr><td><p>2</p></td><td><p>OPLG, 2021, [233]</p></td><td><p>Market-1501</p></td><td><p>91.5/80.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.2/70.1</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>56.1/29.3</p></td><td></td></tr><tr><td><p>3</p></td><td><p>SCAL, 2019, [48]</p></td><td><p>Market-1501</p></td><td><p>95.8/89.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.9/79.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>74.8/72.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>71.1/68.6</p></td><td></td></tr><tr><td><p>4</p></td><td><p>NAS, 2019, [220]</p></td><td><p>Market-1501</p></td><td><p>95.4/94.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03 (labeled)</p></td><td><p>77.9/73</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>73.3/69.3</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>78.2/52.5</p></td><td></td></tr><tr><td><p>5</p></td><td><p>MHN, 2019, [274]</p></td><td><p>Market-1501</p></td><td><p>95.1/85.0</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>89.1/77.2</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>71.7/65.4</p></td><td></td></tr><tr><td><p>6</p></td><td><p>ASTPN, 2017, [280]</p></td><td><p>MARS</p></td><td><p>44.0/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>30.0/\u2013</p></td><td></td></tr><tr><td><p>7</p></td><td><p>DPM, 2015, [g2015scalable]</p></td><td><p>Market-1501</p></td><td><p>42.64/19.47</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>22.95/22.7</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>21.74/26.55</p></td><td></td></tr><tr><td><p>8</p></td><td><p>MEB-Net, 2020, [224]</p></td><td><p>Market-1501</p></td><td><p>89.9/76.0</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>79.6/66.1</p></td><td></td></tr><tr><td><p>9</p></td><td><p>GDS, 2020, [248]</p></td><td><p>Market-1501</p></td><td><p>81.1/61.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReID</p></td><td><p>73.1/55.1</p></td><td></td></tr><tr><td><p>10</p></td><td><p>DCML, 2020, [249]</p></td><td><p>Market-1501</p></td><td><p>88.2/72.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>79.3/63.5</p></td><td></td></tr><tr><td><p>11</p></td><td><p>NRMT, 2020, [120]</p></td><td><p>Market-1501</p></td><td><p>87.8/71.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>77.8/62.2</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>45.2/20.6</p></td><td></td></tr><tr><td><p>12</p></td><td><p>CBN, 2020, [225]</p></td><td><p>Market-1501</p></td><td><p>94.3/83.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>84.8/70.1</p></td><td></td></tr><tr><td><p>13</p></td><td><p>CD-ReID, 2020, [226]</p></td><td><p>Market-1501</p></td><td><p>88.1/71.5</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>79.5/65.2</p></td><td></td></tr><tr><td><p>14</p></td><td><p>DG-Net++, 2020, [227]</p></td><td><p>Market-1501</p></td><td><p>83.1/64.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>78.9/63.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>48.8/22.1</p></td><td></td></tr><tr><td><p>15</p></td><td><p>TLift, 2020, [268]</p></td><td><p>Market-1501</p></td><td><p>88.4/76.0</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.2/78.4</p></td><td></td></tr><tr><td><p>16</p></td><td><p>D-MMD, 2020, [250]</p></td><td><p>Market-1501</p></td><td><p>72.8/50.8</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>68.8/51.6</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>34.4/15.3</p></td><td></td></tr><tr><td><p>17</p></td><td><p>SSDAL, 2016, [245]</p></td><td><p>Market-1501</p></td><td><p>49.0/25.8</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>43.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>22.6/\u2013</p></td><td></td></tr><tr><td><p>18</p></td><td><p>MARS, 2016, [17]</p></td><td><p>MARS</p></td><td><p>68.3/49.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>53.0/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>77.3/\u2013</p></td><td></td></tr><tr><td><p>19</p></td><td><p>DMG-Net, 2021, [228]</p></td><td><p>Person-30K</p></td><td><p>84.23/72.19</p></td><td><p>No</p></td></tr><tr><td><p>20</p></td><td><p>RDSBN, 2021, [251]</p></td><td><p>Market-1501</p></td><td><p>94.8/86.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.1/68.9</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>64.7/34.9</p></td><td></td></tr><tr><td><p>21</p></td><td><p>MetaBIN, 2021, [229]</p></td><td><p>Market-1501</p></td><td><p>69.2/35.9</p></td><td><p>yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>55.2/33.1</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>59.9/68.6</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>81.3/87.0</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>81.0/72.4</p></td><td></td></tr><tr><td><p>22</p></td><td><p>RaMoE, 2021, [230]</p></td><td><p>Market-1501</p></td><td><p>82.0/56.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>73.6/56.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>36.6/35.5</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>63.4/72.2</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>34.1/13.5</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>88.4/92.3</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>56.9/66.8</p></td><td></td></tr><tr><td><p>23</p></td><td><p>LUPerson, 2021, [252]</p></td><td><p>Market-1501</p></td><td><p>97.0/92.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>91.9/84.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>81.9/79.6</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>86.6/68.8</p></td><td></td></tr><tr><td><p>24</p></td><td><p>LReID, 2021, [272]</p></td><td><p>Market-1501</p></td><td><p>87.0/74.8</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>80.1/68.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>56.6/50.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>54.1/27.9</p></td><td></td></tr><tr><td><p>25</p></td><td><p>IICS, 2021, [253]</p></td><td><p>Market-1501</p></td><td><p>89.5/72.9</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>80.0/64.4</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>56.4/26.9</p></td><td></td></tr><tr><td><p>26</p></td><td><p>DSCE, 2021, [254]</p></td><td><p>Market-1501</p></td><td><p>83.9/61.7</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>73.8/53.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>35.2/15.5</p></td><td></td></tr><tr><td><p>27</p></td><td><p>ADC-2OIB, 2021, [231]</p></td><td><p>Market-1501</p></td><td><p>94.8/87.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.4/74.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Labeled)</p></td><td><p>80.6/79.3</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03(Detected)</p></td><td><p>81.3/84.1</p></td><td></td></tr><tr><td><p>28</p></td><td><p>RLCC, 2021, [255]</p></td><td><p>Market-1501</p></td><td><p>90.8/77.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.2/69.2</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>56.5/27.9</p></td><td></td></tr><tr><td><p>29</p></td><td><p>UnrealPerson, 2021, [232]</p></td><td><p>Market-1501</p></td><td><p>93.0/80.2</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>88.3/75.2</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>68.2/34.8</p></td><td></td></tr><tr><td><p>30</p></td><td><p>M3L, 2021, [256]</p></td><td><p>Market-1501</p></td><td><p>75.9/50.2</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>69.2/51.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>33.1/32.1</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>36.9/14.7</p></td><td></td></tr><tr><td><p>31</p></td><td><p>GLT, 2021, [257]</p></td><td><p>Market-1501</p></td><td><p>92.2/79.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.0/69.2</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>59.5/27.7</p></td><td></td></tr><tr><td><p>32</p></td><td><p>SNR, 2020, [221]</p></td><td><p>Market-1501</p></td><td><p>85.5/65.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>78.2/61.6</p></td><td></td></tr><tr><td><p>33</p></td><td><p>AD-Cluster, 2020, [267]</p></td><td><p>Market-1501</p></td><td><p>86.7/68.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>72.6/54.1</p></td><td></td></tr><tr><td><p>34</p></td><td><p>ATNet, 2019, [73]</p></td><td><p>Market-1501</p></td><td><p>45.1/24.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>55.7/25.6</p></td><td></td></tr><tr><td><p>35</p></td><td><p>AANet, 2019, [275]</p></td><td><p>Market-1501</p></td><td><p>95.1/92.38</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>90.36/36.87</p></td><td></td></tr><tr><td><p>36</p></td><td><p>PatchNet, 2019, [243]</p></td><td><p>Market-1501</p></td><td><p>68.5/40.1</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>72.0/53.2</p></td><td></td></tr><tr><td><p>37</p></td><td><p>EANet, 2019, [153]</p></td><td><p>Market-1501</p></td><td><p>94.5/85.6</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.5/74.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>72.5/66.8</p></td><td></td></tr><tr><td><p>38</p></td><td><p>DG-Net, 2019, [219]</p></td><td><p>Market-1501</p></td><td><p>94.8/86.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>86.6/74.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>77.2/52.3</p></td><td></td></tr><tr><td><p>39</p></td><td><p>CV-MIML, 2019, [266]</p></td><td><p>DukeMTMC-VideoReId</p></td><td><p>78.05/59.53</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>66.88/55.16</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>60.0/56.01</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>72.0/70.78</p></td><td></td></tr><tr><td><p>40</p></td><td><p>SPGAN, 2018, [271]</p></td><td><p>Market-1501</p></td><td><p>58.1/26.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>46.9/26.4</p></td><td></td></tr><tr><td><p>41</p></td><td><p>DuATM, 2018, [276]</p></td><td><p>Market-1501</p></td><td><p>91.42/76.62</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>81.82/64.58</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>78.74/62.26</p></td><td></td></tr><tr><td><p>42</p></td><td><p>QDNet, 2017, [222]</p></td><td><p>CUHK-01</p></td><td><p>81.0/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>75.53/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>49.05/\u2013</p></td><td></td></tr><tr><td><p>43</p></td><td><p>DGD, 2016, [223]</p></td><td><p>CUHK-01</p></td><td><p>66.6/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>75.3/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>38.6/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>64.6/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>64.0/\u2013</p></td><td></td></tr><tr><td><p>44</p></td><td><p>UMDL, 2016, [247]</p></td><td><p>CUHK-01</p></td><td><p>27.1/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>31.5/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>49.3/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>PRID-2011</p></td><td><p>24.2/\u2013</p></td><td></td></tr><tr><td><p>45</p></td><td><p>SAL, 2015, [246]</p></td><td><p>CUHK-01</p></td><td><p>22.4/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>29.3/\u2013</p></td><td></td></tr><tr><td><p>46</p></td><td><p>4S-Net, 2020, [234]</p></td><td><p>Market-1501</p></td><td><p>91.6/75.7</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.4/77.3</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>71.4/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>84.8/\u2013</p></td><td></td></tr><tr><td><p>47</p></td><td><p>PL-Net, 2019, [244]</p></td><td><p>Market-1501</p></td><td><p>88.2/69.3</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>82.75/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>56.65/\u2013</p></td><td></td></tr><tr><td><p>48</p></td><td><p>kLDFA, 2016, [182]</p></td><td><p>CUHK-01</p></td><td><p>57.28/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>38.37/\u2013</p></td><td></td></tr><tr><td><p>49</p></td><td><p>CGAN-TM, 2020, [270]</p></td><td><p>Market-1501</p></td><td><p>64.43/31.33</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>54.85/32.85</p></td><td></td></tr><tr><td><p>50</p></td><td><p>GPP-ReID, 2021, [236]</p></td><td><p>Market-1501</p></td><td><p>90.6/78.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>81.3/67.9</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>53.5/24.6</p></td><td></td></tr><tr><td><p>51</p></td><td><p>CI-CNN, 2020, [237]</p></td><td><p>Market-1501</p></td><td><p>94.26/89.54</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.6/81.3</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>87.3/78.8</p></td><td></td></tr><tr><td><p>52</p></td><td><p>DRM, 2021, [238]</p></td><td><p>Market-1501</p></td><td><p>90.9/78.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>82.1/67.7</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>55.0/26.9</p></td><td></td></tr><tr><td><p>53</p></td><td><p>HCC-GCNs, 2021, [260]</p></td><td><p>Market-1501</p></td><td><p>91.2/78.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>81.2/67.5</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>57.4/28.4</p></td><td></td></tr><tr><td><p>54</p></td><td><p>IIA, 2020, [279]</p></td><td><p>Market-1501</p></td><td><p>98.2/96.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>94.4/91.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>84.93/86.58</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>80.14/82.72</p></td><td></td></tr><tr><td><p>55</p></td><td><p>VOLTA, 2020, [261]</p></td><td><p>MARS</p></td><td><p>66.7/51.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-VideoReId</p></td><td><p>89.2/85.9</p></td><td></td></tr><tr><td><p>56</p></td><td><p>MMFA-AAE, 2021, [239]</p></td><td><p>VIPeR</p></td><td><p>58.4/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>46.0/20.7</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>84.8/\u2013</p></td><td></td></tr><tr><td><p>57</p></td><td><p>SAL, 2020, [262]</p></td><td><p>Market-1501</p></td><td><p>68.7/41.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>70.8/51.4</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>31.0/35.7</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>28.7/34.4</p></td><td></td></tr><tr><td><p>58</p></td><td><p>PREST, 2021, [172]</p></td><td><p>Market-1501</p></td><td><p>82.5/62.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>74.4/56.1</p></td><td></td></tr><tr><td><p>59</p></td><td><p>MLOL, 2021, [263]</p></td><td><p>Market-1501</p></td><td><p>86.6/70.9</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>83.1/69.8</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>48.3/22.4</p></td><td></td></tr><tr><td><p>60</p></td><td><p>CCAN, 2020, [277]</p></td><td><p>Market-1501</p></td><td><p>94.6/87.0</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>87.2/76.8</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>75.2/72.9</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>73.0/70.7</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>76.3/53.6</p></td><td></td></tr><tr><td><p>61</p></td><td><p>JPIL, 2020, [259]</p></td><td><p>Market-1501</p></td><td><p>73.5/48.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>74.7/55.8</p></td><td></td></tr><tr><td><p>62</p></td><td><p>CDL, 2018, [131]</p></td><td><p>CUHK-01</p></td><td><p>78.17/\u2013</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>66.39/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>45.1/\u2013</p></td><td></td></tr><tr><td><p>63</p></td><td><p>DECAMEL, 2020, [202]</p></td><td><p>Market-1501</p></td><td><p>60.24/32.44</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>CUHK-01</p></td><td><p>65.81/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>38.27/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>90.34/11.13</p></td><td></td></tr><tr><td><p>64</p></td><td><p>UTAL, 2020, [264]</p></td><td><p>Market-1501</p></td><td><p>69.2/46.2</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>62.3/44.6</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>56.3/42.3</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>31.4/13.1</p></td><td></td></tr><tr><td></td><td></td><td><p>MARS</p></td><td><p>49.9/35.2</p></td><td></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>35.1/\u2013</p></td><td></td></tr><tr><td><p>65</p></td><td><p>HAN, 2019, [278]</p></td><td><p>Market-1501</p></td><td><p>94.2/83.4</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>80.6/64.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Labeled)</p></td><td><p>46.5/46.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03 (Detected)</p></td><td><p>47.5/45.5</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>60.1/32.6</p></td><td></td></tr><tr><td><p>66</p></td><td><p>SBSGAN, 2021, [120]</p></td><td><p>Market-1501</p></td><td><p>87.9/80.0</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>79.7/71.5</p></td><td></td></tr><tr><td><p>67</p></td><td><p>3D-GAT, 2021, [240]</p></td><td><p>Market-1501</p></td><td><p>94.1/81.5</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>85.5/71.2</p></td><td></td></tr><tr><td><p>68</p></td><td><p>pLMNN, 2018, [241]</p></td><td><p>CUHK-01</p></td><td><p>53.5/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>VIPeR</p></td><td><p>46.5/\u2013</p></td><td></td></tr><tr><td><p>69</p></td><td><p>SADL, 2020, [242]</p></td><td><p>Market-1501</p></td><td><p>60.7/26.6</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>50.2/28.1</p></td><td></td></tr><tr><td></td><td></td><td><p>CUHK-03</p></td><td><p>75.42/\u2013</p></td><td></td></tr><tr><td></td><td></td><td><p>MSMT-17</p></td><td><p>30.6/\u2013</p></td><td></td></tr><tr><td><p>70</p></td><td><p>UDAM, 2020, [273]</p></td><td><p>Market-1501</p></td><td><p>75.8/53.7</p></td><td><p>Yes</p></td></tr><tr><td></td><td></td><td><p>DukeMTMC-ReId</p></td><td><p>68.4/49.0</p></td><td></td></tr><tr><td><p>71</p></td><td><p>LADL, 2020, [265]</p></td><td><p>CUHK-01</p></td><td><p>57.67/\u2013</p></td><td><p>No</p></td></tr><tr><td><p>72</p></td><td><p>IVD-ReID, 2019, [198]</p></td><td><p>MARS</p></td><td><p>48.0/\u2013</p></td><td><p>No</p></td></tr><tr><td></td><td></td><td><p>iLIDS</p></td><td><p>65.0/\u2013</p></td><td></td></tr></table>", "caption": "Table 12: Results obtained on Cross-domain/Generalization challenge against each dataset. Results in bold are the highest.", "list_citation_info": ["[256] Yuyang Zhao, Zhun Zhong, Fengxiang Yang, Zhiming Luo, Yaojin Lin, Shaozi Li, and Nicu Sebe. Learning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6277\u20136286, 2021.", "[245] Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. Deep attributes driven multi-camera person re-identification. In European conference on computer vision, pages 475\u2013491. Springer, 2016.", "[221] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li Zhang. Style normalization and restitution for generalizable person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3143\u20133152, 2020.", "[263] Jia Sun, Yanfeng Li, Houjin Chen, Yahui Peng, and Jinlei Zhu. Unsupervised cross domain person re-identification by multi-loss optimization learning. IEEE Transactions on Image Processing, 30:2935\u20132946, 2021.", "[230] Yongxing Dai, Xiaotong Li, Jun Liu, Zekun Tong, and Ling-Yu Duan. Generalizable person re-identification with relevance-aware mixture of experts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16145\u201316154, 2021.", "[278] Wei Li, Xiatian Zhu, and Shaogang Gong. Scalable person re-identification by harmonious attention. International Journal of Computer Vision, 128(6):1635\u20131653, 2020.", "[275] Chiat-Pin Tay, Sharmili Roy, and Kim-Hui Yap. Aanet: Attribute attention network for person re-identifications. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7134\u20137143, 2019.", "[262] Kongzhu Jiang, Tianzhu Zhang, Yongdong Zhang, Feng Wu, and Yong Rui. Self-supervised agent learning for unsupervised cross-domain person re-identification. IEEE Transactions on Image Processing, 29:8549\u20138560, 2020.", "[268] Shengcai Liao and Ling Shao. Interpretable and generalizable person re-identification with query-adaptive convolution and temporal lifting. arXiv preprint arXiv:1904.10424, 2019.", "[264] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised tracklet person re-identification. IEEE transactions on pattern analysis and machine intelligence, 42(7):1770\u20131782, 2020.", "[225] Zijie Zhuang, Longhui Wei, Lingxi Xie, Tianyu Zhang, Hengheng Zhang, Haozhe Wu, Haizhou Ai, and Qi Tian. Rethinking the distribution gap of person re-identification with camera-based batch normalization. In European Conference on Computer Vision, pages 140\u2013157. Springer, 2020.", "[236] Hao Feng, Minghao Chen, Jinming Hu, Dong Shen, Haifeng Liu, and Deng Cai. Complementary pseudo labels for unsupervised domain adaptation on person re-identification. IEEE Transactions on Image Processing, 30:2898\u20132907, 2021.", "[250] Djebril Mekhazni, Amran Bhuiyan, George Ekladious, and Eric Granger. Unsupervised domain adaptation in the dissimilarity space for person re-identification. In European Conference on Computer Vision, pages 159\u2013174. Springer, 2020.", "[259] Yu Zhao, Qiaoyuan Shu, Keren Fu, Pengcheng Wei, and Jian Zhan. Joint patch and instance discrimination learning for unsupervised person re-identification. Image and Vision Computing, 103:104000, 2020.", "[222] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: a deep quadruplet network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 403\u2013412, 2017.", "[274] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-order attention network for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 371\u2013381, 2019.", "[248] Xin Jin, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Global distance-distributions separation for unsupervised person re-identification. In European Conference on Computer Vision, pages 735\u2013751. Springer, 2020.", "[255] Xiao Zhang, Yixiao Ge, Yu Qiao, and Hongsheng Li. Refining pseudo labels with clustering consensus over generations for unsupervised object re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3436\u20133445, 2021.", "[220] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi Yang. Auto-reid: Searching for a part-aware convnet for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3750\u20133759, 2019.", "[240] Hengheng Zhang, Ying Li, Zijie Zhuang, Lingxi Xie, and Qi Tian. 3d-gat: 3d-guided adversarial transform network for person re-identification in unseen domains. Pattern Recognition, 112:107799, 2021.", "[241] Zhicheng Zhao, Binlin Zhao, and Fei Su. Person re-identification via integrating patch-based metric learning and local salience learning. Pattern Recognition, 75:90\u201398, 2018.", "[48] Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, and Jie Zhou. Self-critical attention learning for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 9637\u20139646, 2019.", "[270] Yingzhi Tang, Xi Yang, Nannan Wang, Bin Song, and Xinbo Gao. Cgan-tm: A novel domain-to-domain transferring method for person re-identification. IEEE Transactions on Image Processing, 29:5641\u20135651, 2020.", "[242] Huafeng Li, Zhenyu Kuang, Zhengtao Yu, and Jiebo Luo. Structure alignment of attributes and visual features for cross-dataset person re-identification. Pattern Recognition, 106:107414, 2020.", "[265] Huafeng Li, Shuanglin Yan, Zhengtao Yu, and Dapeng Tao. Attribute-identity embedding and self-supervised learning for scalable person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 30(10):3472\u20133485, 2019.", "[73] Jiawei Liu, Zheng-Jun Zha, Di Chen, Richang Hong, and Meng Wang. Adaptive transfer network for cross-domain person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7202\u20137211, 2019.", "[131] Sheng Li, Ming Shao, and Yun Fu. Person re-identification by cross-view multi-level dictionary learning. IEEE transactions on pattern analysis and machine intelligence, 40(12):2963\u20132977, 2017.", "[153] Houjing Huang, Wenjie Yang, Xiaotang Chen, Xin Zhao, Kaiqi Huang, Jinbin Lin, Guan Huang, and Dalong Du. Eanet: Enhancing alignment for cross-domain person re-identification. arXiv preprint arXiv:1812.11369, 2018.", "[266] Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly supervised person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 760\u2013769, 2019.", "[271] Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, and Jianbin Jiao. Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 994\u20131003, 2018.", "[223] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang Wang. Learning deep feature representations with domain guided dropout for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1249\u20131258, 2016.", "[182] Shi-Zhe Chen, Chun-Chao Guo, and Jian-Huang Lai. Deep ranking for person re-identification via joint representation learning. IEEE Transactions on Image Processing, 25(5):2353\u20132367, 2016.", "[273] Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang Huang, and Xinggang Wang. Unsupervised domain adaptive re-identification: Theory and practice. Pattern Recognition, 102:107173, 2020.", "[120] Yan Huang, Qiang Wu, Jingsong Xu, Yi Zhong, and Zhaoxiang Zhang. Unsupervised domain adaptation with background shift mitigating for person re-identification. International Journal of Computer Vision, 129(7):2244\u20132263, 2021.", "[267] Yunpeng Zhai, Shijian Lu, Qixiang Ye, Xuebo Shan, Jie Chen, Rongrong Ji, and Yonghong Tian. Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9021\u20139030, 2020.", "[272] Nan Pu, Wei Chen, Yu Liu, Erwin M Bakker, and Michael S Lew. Lifelong person re-identification via adaptive knowledge accumulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7901\u20137910, 2021.", "[226] Chuanchen Luo, Chunfeng Song, and Zhaoxiang Zhang. Generalizing person re-identification by camera-aware invariance learning and cross-domain mixup. In European Conference on Computer Vision, volume 2, page 7. Springer, 2020.", "[238] Yongxing Dai, Jun Liu, Yan Bai, Zekun Tong, and Ling-Yu Duan. Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification. IEEE Transactions on Image Processing, 30:7815\u20137829, 2021.", "[243] Qize Yang, Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Patch-based discriminative feature learning for unsupervised person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3633\u20133642, 2019.", "[198] Wei Zhang, Yimeng Li, Weizhi Lu, Xinshun Xu, Zhaowei Liu, and Xiangyang Ji. Learning intra-video difference for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 29(10):3028\u20133036, 2018.", "[249] Guangyi Chen, Yuhao Lu, Jiwen Lu, and Jie Zhou. Deep credible metric learning for unsupervised domain adaptation person re-identification. In Proc. Eur. Conf. Comput. Vis, pages 643\u2013659. Springer, 2020.", "[17] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, and Qi Tian. Mars: A video benchmark for large-scale person re-identification. In European Conference on Computer Vision, pages 868\u2013884. Springer, 2016.", "[277] Jieming Zhou, Soumava Kumar Roy, Pengfei Fang, Mehrtash Harandi, and Lars Petersson. Cross-correlated attention networks for person re-identification. Image and Vision Computing, 100:103931, 2020.", "[232] Tianyu Zhang, Lingxi Xie, Longhui Wei, Zijie Zhuang, Yongfei Zhang, Bo Li, and Qi Tian. Unrealperson: An adaptive pipeline towards costless person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11506\u201311515, 2021.", "[244] Hantao Yao, Shiliang Zhang, Richang Hong, Yongdong Zhang, Changsheng Xu, and Qi Tian. Deep representation learning with part loss for person re-identification. IEEE Transactions on Image Processing, 28(6):2860\u20132871, 2019.", "[280] Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, and Pan Zhou. Jointly attentive spatial-temporal pooling networks for video-based person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 4733\u20134742, 2017.", "[227] Yang Zou, Xiaodong Yang, Zhiding Yu, BVK Kumar, and Jan Kautz. Joint disentangling and adaptation for cross-domain person re-identification. arXiv preprint arXiv:2007.10315, 2020.", "[258] Takashi Isobe, Dong Li, Lu Tian, Weihua Chen, Yi Shan, and Shengjin Wang. Towards discriminative representation learning for unsupervised person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8526\u20138536, 2021.", "[251] Zechen Bai, Zhigang Wang, Jian Wang, Di Hu, and Errui Ding. Unsupervised multi-source domain adaptation for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12914\u201312923, 2021.", "[257] Kecheng Zheng, Wu Liu, Lingxiao He, Tao Mei, Jiebo Luo, and Zheng-Jun Zha. Group-aware label transfer for domain adaptive person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5310\u20135319, 2021.", "[252] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsupervised pre-training for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14750\u201314759, 2021.", "[233] Yi Zheng, Shixiang Tang, Guolong Teng, Yixiao Ge, Kaijian Liu, Jing Qin, Donglian Qi, and Dapeng Chen. Online pseudo label generation by hierarchical cluster dynamics for adaptive person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8371\u20138381, 2021.", "[260] Yan Bai, Ce Wang, Yihang Lou, Jun Liu, and Ling-Yu Duan. Hierarchical connectivity-centered clustering for unsupervised domain adaptation on person re-identification. IEEE Transactions on Image Processing, 30:6715\u20136729, 2021.", "[279] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen, Jianmin Bao, Gang Hua, and Houqiang Li. Improving person re-identification with iterative impression aggregation. IEEE Transactions on Image Processing, 29:9559\u20139571, 2020.", "[261] Meng Liu, Leigang Qu, Liqiang Nie, Maofu Liu, Lingyu Duan, and Baoquan Chen. Iterative local-global collaboration learning towards one-shot video person re-identification. IEEE Transactions on Image Processing, 29:9360\u20139372, 2020.", "[228] Yan Bai, Jile Jiao, Wang Ce, Jun Liu, Yihang Lou, Xuetao Feng, and Ling-Yu Duan. Person30k: A dual-meta generalization network for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2123\u20132132, 2021.", "[172] Hang Zhang, Huanhuan Cao, Xu Yang, Cheng Deng, and Dacheng Tao. Self-training with progressive representation enhancement for unsupervised cross-domain person re-identification. IEEE Transactions on Image Processing, 2021.", "[224] Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Rongrong Ji, and Yonghong Tian. Multiple expert brainstorming for domain adaptive person re-identification. arXiv preprint arXiv:2007.01546, 2020.", "[229] Seokeon Choi, Taekyung Kim, Minki Jeong, Hyoungseob Park, and Changick Kim. Meta batch-instance normalization for generalizable person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3425\u20133435, 2021.", "[239] Shan Lin, Chang-Tsun Li, and Alex C Kot. Multi-domain adversarial feature generalization for person re-identification. IEEE Transactions on Image Processing, 30:1596\u20131607, 2020.", "[202] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Unsupervised person re-identification by deep asymmetric metric embedding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(4):956\u2013973, 2020.", "[253] Shiyu Xuan and Shiliang Zhang. Intra-inter camera similarity for unsupervised person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11926\u201311935, 2021.", "[246] Zhiyuan Shi, Timothy M Hospedales, and Tao Xiang. Transferring a semantic representation for person re-identification and search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4184\u20134193, 2015.", "[247] Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pontil, Shaogang Gong, Tiejun Huang, and Yonghong Tian. Unsupervised cross-dataset transfer learning for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1306\u20131315, 2016.", "[276] Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong, Alex C Kot, and Gang Wang. Dual attention matching network for context-aware feature sequence based person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5363\u20135372, 2018.", "[254] Fengxiang Yang, Zhun Zhong, Zhiming Luo, Yuanzheng Cai, Yaojin Lin, Shaozi Li, and Nicu Sebe. Joint noise-tolerant learning and meta camera shift adaptation for unsupervised person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4855\u20134864, 2021.", "[237] Wenfeng Song, Shuai Li, Tao Chang, Aimin Hao, Qinping Zhao, and Hong Qin. Context-interactive cnn for person re-identification. IEEE Transactions on Image Processing, 29:2860\u20132874, 2019.", "[234] Amena Khatun, Simon Denman, Sridha Sridharan, and Clinton Fookes. Joint identification\u2013verification for person re-identification: A four stream deep learning approach with improved quartet loss function. Computer Vision and Image Understanding, 197:102989, 2020.", "[231] Anguo Zhang, Yueming Gao, Yuzhen Niu, Wenxi Liu, and Yongcheng Zhou. Coarse-to-fine person re-identification with auxiliary-domain classification and second-order information bottleneck. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 598\u2013607, 2021.", "[219] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, and Jan Kautz. Joint discriminative and generative learning for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2138\u20132147, 2019."]}, {"table": "<table><tr><td>Sr.No</td><td>Dataset</td><td>SOTA R1-results</td><td>Paper cited</td><td>Challenge addressed</td><td>Venue</td></tr><tr><td>1</td><td>Market-1501</td><td>98.3</td><td>[119]</td><td>Background</td><td>TCSV-2020</td></tr><tr><td>2</td><td>DukeMTMC-ReID</td><td>94.7</td><td>[119]</td><td>Background</td><td>TCSV-2020</td></tr><tr><td>3</td><td>CUHK01</td><td>98.73</td><td>[165]</td><td>Scale</td><td>PAMI-2020</td></tr><tr><td>4</td><td>CUHK03</td><td>97.3</td><td>[91]</td><td>Pose</td><td>PAMI-2021</td></tr><tr><td>5</td><td>VIPeR</td><td>71.4</td><td>[234]</td><td>Generalization</td><td>CVIU-2020</td></tr><tr><td>6</td><td>MSMT-17</td><td>87.7</td><td>[141]</td><td>Misalignment</td><td>PR-2021</td></tr><tr><td>7</td><td>PRID-2011</td><td>95.9</td><td>[99]</td><td>Pose</td><td>PR-2021</td></tr><tr><td>8</td><td>MARS</td><td>91.5</td><td>[67]</td><td>Occlusion</td><td>ICCV-2021</td></tr><tr><td>9</td><td>DukeMTMC-Video ReID</td><td>98.3</td><td>[67]</td><td>Occlusion</td><td>ICCV-2021</td></tr><tr><td>10</td><td>iLIDS</td><td>92</td><td>[142](arxiv)</td><td>Misalignment</td><td>ICCV-2021</td></tr></table>", "caption": "Table 13: SOTA results obtained on each challenge against each dataset.", "list_citation_info": ["[142] Tianyu He, Xin Jin, Xu Shen, Jianqiang Huang, Zhibo Chen, and Xian-Sheng Hua. Dense interaction learning for video-based person re-identification. arXiv preprint arXiv:2103.09013, 2021.", "[67] Yingquan Wang, Pingping Zhang, Shang Gao, Xia Geng, Hu Lu, and Dong Wang. Pyramid spatial-temporal aggregation for video-based person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12026\u201312035, 2021.", "[99] Xiaoqiang Hu, Dan Wei, Ziyang Wang, Jianglin Shen, and Hongjuan Ren. Hypergraph video pedestrian re-identification based on posture structure relationship and action constraints. Pattern Recognition, 111:107688, 2021.", "[119] Xin Ning, Ke Gong, Weijun Li, Liping Zhang, Xiao Bai, and Shengwei Tian. Feature refinement and filter network for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2020.", "[91] Yantao Shen, Tong Xiao, Shuai Yi, Dapeng Chen, Xiaogang Wang, and Hongsheng Li. Person re-identification with deep kronecker-product matching and group-shuffling random walk. IEEE transactions on pattern analysis and machine intelligence, 43(5):1649\u20131665, 2021.", "[141] Zhong Zhang, Haijia Zhang, Shuang Liu, Yuan Xie, and Tariq S Durrani. Part-guided graph convolution networks for person re-identification. Pattern Recognition, 120:108155, 2021.", "[165] X Qian, Y Fu, T Xiang, YG Jiang, and X Xue. Leader-based multi-scale attention deep architecture for person re-identification. IEEE transactions on pattern analysis and machine intelligence, 42(2):371, 2020.", "[234] Amena Khatun, Simon Denman, Sridha Sridharan, and Clinton Fookes. Joint identification\u2013verification for person re-identification: A four stream deep learning approach with improved quartet loss function. Computer Vision and Image Understanding, 197:102989, 2020."]}], "citation_info_to_title": {"[73] Jiawei Liu, Zheng-Jun Zha, Di Chen, Richang Hong, and Meng Wang. Adaptive transfer network for cross-domain person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7202\u20137211, 2019.": "Adaptive transfer network for cross-domain person re-identification", "[191] Jingke Meng, Ancong Wu, and Wei-Shi Zheng. Deep asymmetric video-based person re-identification. Pattern Recognition, 93:430\u2013441, 2019.": "Deep asymmetric video-based person re-identification", "[158] Jiawei Liu, Zheng-Jun Zha, Wei Wu, Kecheng Zheng, and Qibin Sun. Spatial-temporal correlation and topology learning for person re-identification in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4370\u20134379, 2021.": "Spatial-temporal correlation and topology learning for person re-identification in videos", "[263] Jia Sun, Yanfeng Li, Houjin Chen, Yahui Peng, and Jinlei Zhu. Unsupervised cross domain person re-identification by multi-loss optimization learning. IEEE Transactions on Image Processing, 30:2935\u20132946, 2021.": "Unsupervised cross domain person re-identification by multi-loss optimization learning", "[242] Huafeng Li, Zhenyu Kuang, Zhengtao Yu, and Jiebo Luo. Structure alignment of attributes and visual features for cross-dataset person re-identification. Pattern Recognition, 106:107414, 2020.": "Structure Alignment of Attributes and Visual Features for Cross-Dataset Person Re-identification", "[111] Thi Thanh Thuy Pham, Thi-Lan Le, Hai Vu, Trung Kien Dao, et al. Fully-automated person re-identification in multi-camera surveillance system with a robust kernel descriptor and effective shadow removal method. Image and Vision Computing, 59:44\u201362, 2017.": "Fully-automated person re-identification in multi-camera surveillance system with a robust kernel descriptor and effective shadow removal method", "[204] Lei Zhang, Fangyi Liu, and David Zhang. Adversarial view confusion feature learning for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 31(4):1490\u20131502, 2021.": "Adversarial view confusion feature learning for person re-identification", "[57] Lingxiao He and Wu Liu. Guided saliency feature learning for person re-identification in crowded scenes. In European Conference on Computer Vision, pages 357\u2013373. Springer, 2020.": "Guided Saliency Feature Learning for Person Re-identification in Crowded Scenes", "[160] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. Learning generalisable omni-scale representations for person re-identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.": "Learning Generalisable Omni-Scale Representations for Person Re-Identification", "[138] S\u0142awomir B\u0105k and Peter Carr. Deep deformable patch metric learning for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 28(10):2690\u20132702, 2017.": "Deep deformable patch metric learning for person re-identification", "[103] Yifan Sun, Liang Zheng, Yali Li, Yi Yang, Qi Tian, and Shengjin Wang. Learning part-based convolutional features for person re-identification. IEEE transactions on pattern analysis and machine intelligence, 2019.": "Learning part-based convolutional features for person re-identification", "[258] Takashi Isobe, Dong Li, Lu Tian, Weihua Chen, Yi Shan, and Shengjin Wang. Towards discriminative representation learning for unsupervised person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8526\u20138536, 2021.": "Towards discriminative representation learning for unsupervised person re-identification", "[257] Kecheng Zheng, Wu Liu, Lingxiao He, Tao Mei, Jiebo Luo, and Zheng-Jun Zha. Group-aware label transfer for domain adaptive person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5310\u20135319, 2021.": "Group-aware label transfer for domain adaptive person re-identification", "[219] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, and Jan Kautz. Joint discriminative and generative learning for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2138\u20132147, 2019.": "Joint discriminative and generative learning for person re-identification", "[144] Jianyuan Guo, Yuhui Yuan, Lang Huang, Chao Zhang, Jin-Ge Yao, and Kai Han. Beyond human parts: Dual part-aligned representations for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3642\u20133651, 2019.": "Beyond human parts: Dual part-aligned representations for person re-identification", "[80] Yiqiang Chen, Stefan Duffner, Andrei Stoian, Jean-Yves Dufour, and Atilla Baskurt. Deep and low-level feature based attribute learning for person re-identification. Image and Vision Computing, 79:25\u201334, 2018.": "Deep and low-level feature based attribute learning for person re-identification", "[259] Yu Zhao, Qiaoyuan Shu, Keren Fu, Pengcheng Wei, and Jian Zhan. Joint patch and instance discrimination learning for unsupervised person re-identification. Image and Vision Computing, 103:104000, 2020.": "Joint patch and instance discrimination learning for unsupervised person re-identification", "[163] Yiluan Guo and Ngai-Man Cheung. Efficient and deep person re-identification using multi-level similarity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2335\u20132344, 2018.": "Efficient and deep person re-identification using multi-level similarity", "[243] Qize Yang, Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Patch-based discriminative feature learning for unsupervised person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3633\u20133642, 2019.": "Patch-based discriminative feature learning for unsupervised person re-identification", "[175] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. Person re-identification by local maximal occurrence representation and metric learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2197\u20132206, 2015.": "Person re-identification by local maximal occurrence representation and metric learning", "[168] Niki Martinel, Gian Luca Foresti, and Christian Micheloni. Deep pyramidal pooling with attention for person re-identification. IEEE Transactions on Image Processing, 29:7306\u20137316, 2020.": "Deep pyramidal pooling with attention for person re-identification", "[123] Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Kyoung Mu Lee. Part-aligned bilinear representations for person re-identification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 402\u2013419, 2018.": "Part-aligned bilinear representations for person re-identification", "[107] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, and Zhibo Chen. Relation-aware global attention for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3186\u20133195, 2020.": "Relation-aware global attention for person re-identification", "[212] XY Jing, X Zhu, F Wu, R Hu, X You, Y Wang, H Feng, and JY Yang. Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning. IEEE Transactions on Image Processing: a Publication of the IEEE Signal Processing Society, 26(3):1363\u20131378, 2017.": "Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning", "[56] Lingxiao He, Yinggang Wang, Wu Liu, He Zhao, Zhenan Sun, and Jiashi Feng. Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 8450\u20138459, 2019.": "Foreground-aware pyramid reconstruction for alignment-free occluded person re-identification", "[198] Wei Zhang, Yimeng Li, Weizhi Lu, Xinshun Xu, Zhaowei Liu, and Xiangyang Ji. Learning intra-video difference for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 29(10):3028\u20133036, 2018.": "Learning intra-video difference for person re-identification", "[166] Wei Zhang, Xuanyu He, Xiaodong Yu, Weizhi Lu, Zhengjun Zha, and Qi Tian. A multi-scale spatial-temporal attention model for person re-identification in videos. IEEE Transactions on Image Processing, 29:3365\u20133373, 2019.": "A multi-scale spatial-temporal attention model for person re-identification in videos", "[230] Yongxing Dai, Xiaotong Li, Jun Liu, Zekun Tong, and Ling-Yu Duan. Generalizable person re-identification with relevance-aware mixture of experts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16145\u201316154, 2021.": "Generalizable person re-identification with relevance-aware mixture of experts", "[102] Le An, Mehran Kafai, Songfan Yang, and Bir Bhanu. Person reidentification with reference descriptor. IEEE Transactions on Circuits and Systems for Video Technology, 26(4):776\u2013787, 2015.": "Person reidentification with reference descriptor", "[188] Lin Wu, Yang Wang, Hongzhi Yin, Meng Wang, and Ling Shao. Few-shot deep adversarial learning for video-based person re-identification. IEEE Transactions on Image Processing, 29:1233\u20131245, 2020.": "Few-shot deep adversarial learning for video-based person re-identification", "[154] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Person re-identification by deep learning multi-scale representations. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 2590\u20132600, 2017.": "Person re-identification by deep learning multi-scale representations", "[223] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang Wang. Learning deep feature representations with domain guided dropout for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1249\u20131258, 2016.": "Learning deep feature representations with domain guided dropout for person re-identification", "[13] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing neural network for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 152\u2013159, 2014.": "Deepreid: Deep Filter Pairing Neural Network for Person Re-identification", "[185] Xiumei Chen, Xiangtao Zheng, and Xiaoqiang Lu. Bidirectional interaction network for person re-identification. IEEE Transactions on Image Processing, 30:1935\u20131948, 2021.": "Bidirectional Interaction Network for Person Re-identification", "[135] Shuzhao Li, Huimin Yu, and Roland Hu. Attributes-aided part detection and refinement for person re-identification. Pattern Recognition, 97:107016, 2020.": "Attributes-aided part detection and refinement for person re-identification", "[132] Changxing Ding, Kan Wang, Pengfei Wang, and Dacheng Tao. Multi-task learning with coarse priors for robust part-aware person re-identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.": "Multi-task learning with coarse priors for robust part-aware person re-identification", "[253] Shiyu Xuan and Shiliang Zhang. Intra-inter camera similarity for unsupervised person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11926\u201311935, 2021.": "Intra-inter camera similarity for unsupervised person re-identification", "[225] Zijie Zhuang, Longhui Wei, Lingxi Xie, Tianyu Zhang, Hengheng Zhang, Haozhe Wu, Haizhou Ai, and Qi Tian. Rethinking the distribution gap of person re-identification with camera-based batch normalization. In European Conference on Computer Vision, pages 140\u2013157. Springer, 2020.": "Rethinking the distribution gap of person re-identification with camera-based batch normalization", "[71] Chanho Eom, Geon Lee, Junghyup Lee, and Bumsub Ham. Video-based person re-identification with spatial and temporal memory networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12036\u201312045, 2021.": "Video-based person re-identification with spatial and temporal memory networks", "[124] Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, and Xiaoou Tang. Spindle net: Person re-identification with human body region guided feature decomposition and fusion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1077\u20131085, 2017.": "Spindle net: Person re-identification with human body region guided feature decomposition and fusion", "[279] Dengpan Fu, Bo Xin, Jingdong Wang, Dongdong Chen, Jianmin Bao, Gang Hua, and Houqiang Li. Improving person re-identification with iterative impression aggregation. IEEE Transactions on Image Processing, 29:9559\u20139571, 2020.": "Improving person re-identification with iterative impression aggregation", "[151] Zhedong Zheng, Liang Zheng, and Yi Yang. Pedestrian alignment network for large-scale person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 29(10):3037\u20133045, 2018.": "Pedestrian alignment network for large-scale person re-identification", "[272] Nan Pu, Wei Chen, Yu Liu, Erwin M Bakker, and Michael S Lew. Lifelong person re-identification via adaptive knowledge accumulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7901\u20137910, 2021.": "Lifelong person re-identification via adaptive knowledge accumulation", "[106] Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang, and Wanli Ouyang. Attention-aware compositional network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2119\u20132128, 2018.": "Attention-aware Compositional Network for Person Re-identification", "[18] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain gap for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 79\u201388, 2018.": "Person Transfer GAN to Bridge Domain Gap for Person Re-Identification", "[100] Dapeng Chen, Zejian Yuan, Badong Chen, and Nanning Zheng. Similarity learning with spatial constraints for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1268\u20131277, 2016.": "Similarity Learning with Spatial Constraints for Person Re-identification", "[11] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European conference on computer vision, pages 17\u201335. Springer, 2016.": "Performance measures and a data set for multi-target, multi-camera tracking", "[155] Xuelin Qian, Yanwei Fu, Yu-Gang Jiang, Tao Xiang, and Xiangyang Xue. Multi-scale deep learning architectures for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 5399\u20135408, 2017.": "Multi-scale deep learning architectures for person re-identification", "[159] Jianing Li, Shiliang Zhang, and Tiejun Huang. Multi-scale temporal cues learning for video person re-identification. IEEE Transactions on Image Processing, 29:4461\u20134473, 2020.": "Multi-scale temporal cues learning for video person re-identification", "[195] Lin Wu, Richang Hong, Yang Wang, and Meng Wang. Cross-entropy adversarial view adaptation for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 30(7):2081\u20132092, 2020.": "Cross-entropy adversarial view adaptation for person re-identification", "[89] Qinqin Zhou, Bineng Zhong, Xiangyuan Lan, Gan Sun, Yulun Zhang, Baochang Zhang, and Rongrong Ji. Fine-grained spatial alignment model for person re-identification with focal triplet loss. IEEE Transactions on Image Processing, 29:7578\u20137589, 2020.": "Fine-grained spatial alignment model for person re-identification with focal triplet loss", "[161] Cairong Zhao, Xuekuan Wang, Wangmeng Zuo, Fumin Shen, Ling Shao, and Duoqian Miao. Similarity learning with joint transfer constraints for person re-identification. Pattern Recognition, 97:107014, 2020.": "Similarity Learning with Joint Transfer Constraints for Person Re-identification", "[181] Zhanxiang Feng, Jianhuang Lai, and Xiaohua Xie. Learning view-specific deep networks for person re-identification. IEEE Transactions on Image Processing, 27(7):3472\u20133483, 2018.": "Learning view-specific deep networks for person re-identification", "[266] Jingke Meng, Sheng Wu, and Wei-Shi Zheng. Weakly supervised person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 760\u2013769, 2019.": "Weakly supervised person re-identification", "[126] Yifan Sun, Qin Xu, Yali Li, Chi Zhang, Yikang Li, Shengjin Wang, and Jian Sun. Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 393\u2013402, 2019.": "Perceive where to focus: Learning visibility-aware part-level features for partial person re-identification", "[244] Hantao Yao, Shiliang Zhang, Richang Hong, Yongdong Zhang, Changsheng Xu, and Qi Tian. Deep representation learning with part loss for person re-identification. IEEE Transactions on Image Processing, 28(6):2860\u20132871, 2019.": "Deep representation learning with part loss for person re-identification", "[233] Yi Zheng, Shixiang Tang, Guolong Teng, Yixiao Ge, Kaijian Liu, Jing Qin, Donglian Qi, and Dapeng Chen. Online pseudo label generation by hierarchical cluster dynamics for adaptive person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8371\u20138381, 2021.": "Online Pseudo Label Generation by Hierarchical Cluster Dynamics for Adaptive Person Re-identification", "[87] Zhiyong Li, Jingyi Lv, Ying Chen, and Jin Yuan. Person re-identification with part prediction alignment. Computer Vision and Image Understanding, 205:103172, 2021.": "Person re-identification with part prediction alignment", "[108] Hao Liu, Jiashi Feng, Meibin Qi, Jianguo Jiang, and Shuicheng Yan. End-to-end comparative attention networks for person re-identification. IEEE Transactions on Image Processing, 26(7):3492\u20133506, 2017.": "End-to-end comparative attention networks for person re-identification", "[136] Wei Shi, Hong Liu, and Mengyuan Liu. Image-to-video person re-identification using three-dimensional semantic appearance alignment and cross-modal interactive learning. Pattern Recognition, 122:108314, 2022.": "Image-to-Video Person Re-Identification Using Three-Dimensional Semantic Appearance Alignment and Cross-Modal Interactive Learning", "[125] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao Wang. Identity-guided human semantic parsing for person re-identification. arXiv preprint arXiv:2007.13467, 2020.": "Identity-guided human semantic parsing for person re-identification", "[221] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li Zhang. Style normalization and restitution for generalizable person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3143\u20133152, 2020.": "Style normalization and restitution for generalizable person re-identification", "[85] Shoubiao Tan, Feng Zheng, Li Liu, Jungong Han, and Ling Shao. Dense invariant feature-based support vector ranking for cross-camera person reidentification. IEEE Transactions on Circuits and Systems for Video Technology, 28(2):356\u2013363, 2016.": "Dense invariant feature-based support vector ranking for cross-camera person reidentification", "[68] Ruimao Zhang, Jingyu Li, Hongbin Sun, Yuying Ge, Ping Luo, Xiaogang Wang, and Liang Lin. Scan: Self-and-collaborative attention network for video person re-identification. IEEE Transactions on Image Processing, 28(10):4870\u20134882, 2019.": "Scan: Self-and-collaborative attention network for video person re-identification", "[216] Rahul Rama Varior, Gang Wang, Jiwen Lu, and Ting Liu. Learning invariant color features for person reidentification. IEEE Transactions on Image Processing, 25(7):3395\u20133410, 2016.": "Learning Invariant Color Features for Person Reidentification", "[64] Jiaxu Miao, Yu Wu, Ping Liu, Yuhang Ding, and Yi Yang. Pose-guided feature alignment for occluded person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 542\u2013551, 2019.": "Pose-guided feature alignment for occluded person re-identification", "[220] Ruijie Quan, Xuanyi Dong, Yu Wu, Linchao Zhu, and Yi Yang. Auto-reid: Searching for a part-aware convnet for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3750\u20133759, 2019.": "Auto-reid: Searching for a part-aware convnet for person re-identification", "[200] Zhun Zhong, Liang Zheng, Zhiming Luo, Shaozi Li, and Yi Yang. Invariance matters: Exemplar memory for domain adaptive person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 598\u2013607, 2019.": "Invariance matters: Exemplar memory for domain adaptive person re-identification", "[218] Yukun Huang, Zheng-Jun Zha, Xueyang Fu, Richang Hong, and Liang Li. Real-world person re-identification via degradation invariance learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14084\u201314094, 2020.": "Real-world person re-identification via degradation invariance learning", "[67] Yingquan Wang, Pingping Zhang, Shang Gao, Xia Geng, Hu Lu, and Dong Wang. Pyramid spatial-temporal aggregation for video-based person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12026\u201312035, 2021.": "Pyramid spatial-temporal aggregation for video-based person re-identification", "[70] Abhishek Aich, Meng Zheng, Srikrishna Karanam, Terrence Chen, Amit K Roy-Chowdhury, and Ziyan Wu. Spatio-temporal representation factorization for video-based person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 152\u2013162, 2021.": "Spatio-temporal representation factorization for video-based person re-identification", "[271] Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, and Jianbin Jiao. Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 994\u20131003, 2018.": "Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification", "[232] Tianyu Zhang, Lingxi Xie, Longhui Wei, Zijie Zhuang, Yongfei Zhang, Bo Li, and Qi Tian. Unrealperson: An adaptive pipeline towards costless person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11506\u201311515, 2021.": "Unrealperson: An adaptive pipeline towards costless person re-identification", "[184] Lin Wu, Yang Wang, Zongyuan Ge, Qichang Hu, and Xue Li. Structured deep hashing with convolutional neural networks for fast person re-identification. Computer Vision and Image Understanding, 167:63\u201373, 2018.": "Structured deep hashing with convolutional neural networks for fast person re-identification", "[255] Xiao Zhang, Yixiao Ge, Yu Qiao, and Hongsheng Li. Refining pseudo labels with clustering consensus over generations for unsupervised object re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3436\u20133445, 2021.": "Refining pseudo labels with clustering consensus over generations for unsupervised object re-identification", "[50] Wei Li, Xiatian Zhu, and Shaogang Gong. Harmonious attention network for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2285\u20132294, 2018.": "Harmonious attention network for person re-identification", "[275] Chiat-Pin Tay, Sharmili Roy, and Kim-Hui Yap. Aanet: Attribute attention network for person re-identifications. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7134\u20137143, 2019.": "Aanet: Attribute attention network for person re-identifications", "[47] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd-net: Attentive but diverse person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 8351\u20138361, 2019.": "Abd-net: Attentive but diverse person re-identification", "[215] Xiaokai Liu, Xiaorui Ma, Jie Wang, and Hongyu Wang. M3l: Multi-modality mining for metric learning in person re-identification. Pattern Recognition, 76:650\u2013661, 2018.": "M3l: Multi-modality mining for metric learning in person re-identification", "[261] Meng Liu, Leigang Qu, Liqiang Nie, Maofu Liu, Lingyu Duan, and Baoquan Chen. Iterative local-global collaboration learning towards one-shot video person re-identification. IEEE Transactions on Image Processing, 29:9360\u20139372, 2020.": "Iterative Local-Global Collaboration Learning Towards One-Shot Video Person Re-Identification", "[227] Yang Zou, Xiaodong Yang, Zhiding Yu, BVK Kumar, and Jan Kautz. Joint disentangling and adaptation for cross-domain person re-identification. arXiv preprint arXiv:2007.10315, 2020.": "Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification", "[65] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Vrstc: Occlusion-free video person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7183\u20137192, 2019.": "VRSTC: Occlusion-free Video Person Re-identification", "[147] Fan Yang, Ke Yan, Shijian Lu, Huizhu Jia, Xiaodong Xie, and Wen Gao. Attention driven person re-identification. Pattern Recognition, 86:143\u2013155, 2019.": "Attention Driven Person Re-identification", "[149] Kan Wang, Pengfei Wang, Changxing Ding, and Dacheng Tao. Batch coherence-driven network for part-aware person re-identification. IEEE Transactions on Image Processing, 30:3405\u20133418, 2021.": "Batch coherence-driven network for part-aware person re-identification", "[69] Guangyi Chen, Jiwen Lu, Ming Yang, and Jie Zhou. Spatial-temporal attention-aware learning for video-based person re-identification. IEEE Transactions on Image Processing, 28(9):4192\u20134205, 2019.": "Spatial-temporal attention-aware learning for video-based person re-identification", "[262] Kongzhu Jiang, Tianzhu Zhang, Yongdong Zhang, Feng Wu, and Yong Rui. Self-supervised agent learning for unsupervised cross-domain person re-identification. IEEE Transactions on Image Processing, 29:8549\u20138560, 2020.": "Self-supervised agent learning for unsupervised cross-domain person re-identification", "[165] X Qian, Y Fu, T Xiang, YG Jiang, and X Xue. Leader-based multi-scale attention deep architecture for person re-identification. IEEE transactions on pattern analysis and machine intelligence, 42(2):371, 2020.": "Leader-based multi-scale attention deep architecture for person re-identification", "[267] Yunpeng Zhai, Shijian Lu, Qixiang Ye, Xuebo Shan, Jie Chen, Rongrong Ji, and Yonghong Tian. Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9021\u20139030, 2020.": "Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification", "[112] Xiang Bai, Mingkun Yang, Tengteng Huang, Zhiyong Dou, Rui Yu, and Yongchao Xu. Deep-person: Learning discriminative deep features for person re-identification. Pattern Recognition, 98:107036, 2020.": "Deep-person: Learning discriminative deep features for person re-identification", "[146] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. Deeply-learned part-aligned representations for person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 3219\u20133228, 2017.": "Deeply-learned part-aligned representations for person re-identification", "[169] Guangyi Chen, Tianpei Gu, Jiwen Lu, Jin-An Bao, and Jie Zhou. Person re-identification via attention pyramid. IEEE Transactions on Image Processing, 30:7663\u20137676, 2021.": "Person re-identification via attention pyramid", "[276] Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong, Alex C Kot, and Gang Wang. Dual attention matching network for context-aware feature sequence based person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5363\u20135372, 2018.": "Dual attention matching network for context-aware feature sequence based person re-identification", "[76] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. Pose-driven deep convolutional model for person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 3960\u20133969, 2017.": "Pose-driven deep convolutional model for person re-identification", "[86] Igor Barros Barbosa, Marco Cristani, Barbara Caputo, Aleksander Rognhaugen, and Theoharis Theoharis. Looking beyond appearances: Synthetic training data for deep cnns in re-identification. Computer Vision and Image Understanding, 167:50\u201362, 2018.": "Looking beyond appearances: Synthetic training data for deep cnns in re-identification", "[176] Angelo Porrello, Luca Bergamini, and Simone Calderara. Robust re-identification by multiple views knowledge distillation. In European Conference on Computer Vision, pages 93\u2013110. Springer, 2020.": "Robust re-identification by multiple views knowledge distillation", "[131] Sheng Li, Ming Shao, and Yun Fu. Person re-identification by cross-view multi-level dictionary learning. IEEE transactions on pattern analysis and machine intelligence, 40(12):2963\u20132977, 2017.": "Person re-identification by cross-view multi-level dictionary learning", "[246] Zhiyuan Shi, Timothy M Hospedales, and Tao Xiang. Transferring a semantic representation for person re-identification and search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4184\u20134193, 2015.": "Transferring a semantic representation for person re-identification and search", "[226] Chuanchen Luo, Chunfeng Song, and Zhaoxiang Zhang. Generalizing person re-identification by camera-aware invariance learning and cross-domain mixup. In European Conference on Computer Vision, volume 2, page 7. Springer, 2020.": "Generalizing person re-identification by camera-aware invariance learning and cross-domain mixup", "[228] Yan Bai, Jile Jiao, Wang Ce, Jun Liu, Yihang Lou, Xuetao Feng, and Ling-Yu Duan. Person30k: A dual-meta generalization network for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2123\u20132132, 2021.": "Person30k: A dual-meta generalization network for person re-identification", "[114] Sanping Zhou, Jinjun Wang, Deyu Meng, Xiaomeng Xin, Yubing Li, Yihong Gong, and Nanning Zheng. Deep self-paced learning for person re-identification. Pattern Recognition, 76:739\u2013751, 2018.": "Deep self-paced learning for person re-identification", "[249] Guangyi Chen, Yuhao Lu, Jiwen Lu, and Jie Zhou. Deep credible metric learning for unsupervised domain adaptation person re-identification. In Proc. Eur. Conf. Comput. Vis, pages 643\u2013659. Springer, 2020.": "Deep credible metric learning for unsupervised domain adaptation person re-identification", "[140] Yang Shen, Weiyao Lin, Junchi Yan, Mingliang Xu, Jianxin Wu, and Jingdong Wang. Person re-identification with correspondence structure learning. In Proceedings of the IEEE international conference on computer vision, pages 3200\u20133208, 2015.": "Person re-identification with correspondence structure learning", "[19] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9):1627\u20131645, 2009.": "Object detection with discriminatively trained part-based models", "[55] Wei-Shi Zheng, Xiang Li, Tao Xiang, Shengcai Liao, Jianhuang Lai, and Shaogang Gong. Partial person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 4678\u20134686, 2015.": "Partial person re-identification", "[172] Hang Zhang, Huanhuan Cao, Xu Yang, Cheng Deng, and Dacheng Tao. Self-training with progressive representation enhancement for unsupervised cross-domain person re-identification. IEEE Transactions on Image Processing, 2021.": "Self-training with progressive representation enhancement for unsupervised cross-domain person re-identification", "[141] Zhong Zhang, Haijia Zhang, Shuang Liu, Yuan Xie, and Tariq S Durrani. Part-guided graph convolution networks for person re-identification. Pattern Recognition, 120:108155, 2021.": "Part-guided graph convolution networks for person re-identification", "[207] Yan Wang, Lequn Wang, Yurong You, Xu Zou, Vincent Chen, Serena Li, Gao Huang, Bharath Hariharan, and Kilian Q Weinberger. Resource aware person re-identification across multiple resolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8042\u20138051, 2018.": "Resource aware person re-identification across multiple resolutions", "[98] Yiming Wu, Omar El Farouk Bourahla, Xi Li, Fei Wu, Qi Tian, and Xue Zhou. Adaptive graph representation learning for video person re-identification. IEEE Transactions on Image Processing, 29:8821\u20138830, 2020.": "Adaptive graph representation learning for video person re-identification", "[170] Yingji Zhong, Yaowei Wang, and Shiliang Zhang. Progressive feature enhancement for person re-identification. IEEE Transactions on Image Processing, 30:8384\u20138395, 2021.": "Progressive feature enhancement for person re-identification", "[178] Ying-Cong Chen, Wei-Shi Zheng, Jian-Huang Lai, and Pong C Yuen. An asymmetric distance model for cross-view feature mapping in person reidentification. IEEE transactions on circuits and systems for video technology, 27(8):1661\u20131675, 2016.": "An asymmetric distance model for cross-view feature mapping in person reidentification", "[20] Afshin Dehghan, Shayan Modiri Assari, and Mubarak Shah. Gmmcp tracker: Globally optimal generalized maximum multi clique problem for multiple object tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4091\u20134099, 2015.": "Gmmcp tracker: Globally optimal generalized maximum multi clique problem for multiple object tracking", "[94] Liang Zheng, Yujia Huang, Huchuan Lu, and Yi Yang. Pose-invariant embedding for deep person re-identification. IEEE Transactions on Image Processing, 28(9):4500\u20134509, 2019.": "Pose-Invariant Embedding for Deep Person Re-Identification", "[238] Yongxing Dai, Jun Liu, Yan Bai, Zekun Tong, and Ling-Yu Duan. Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification. IEEE Transactions on Image Processing, 30:7815\u20137829, 2021.": "Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification", "[54] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline). In Proceedings of the European Conference on Computer Vision (ECCV), pages 480\u2013496, 2018.": "Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)", "[197] Xueping Wang, Rameswar Panda, Min Liu, Yaonan Wang, and Amit K Roy-Chowdhury. Exploiting global camera network constraints for unsupervised video person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2020.": "Exploiting Global Camera Network Constraints for Unsupervised Video Person Re-identification", "[61] Cairong Zhao, Xinbi Lv, Shuguang Dou, Shanshan Zhang, Jun Wu, and Liang Wang. Incremental generative occlusion adversarial suppression network for person reid. IEEE Transactions on Image Processing, 30:4212\u20134224, 2021.": "Incremental Generative Occlusion Adversarial Suppression Network for Person ReID", "[115] Sanping Zhou, Fei Wang, Zeyi Huang, and Jinjun Wang. Discriminative feature learning with consistent attention regularization for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 8040\u20138049, 2019.": "Discriminative feature learning with consistent attention regularization for person re-identification", "[113] Shuai Li, Wenfeng Song, Zheng Fang, Jiaying Shi, Aimin Hao, Qinping Zhao, and Hong Qin. Long-short temporal-spatial clues excited network for robust person re-identification. International Journal of Computer Vision, 128(12):2936\u20132961, 2020.": "Long-short temporal-spatial clues excited network for robust person re-identification", "[117] Yiheng Liu, Wengang Zhou, Jianzhuang Liu, Guo-Jun Qi, Qi Tian, and Houqiang Li. An end-to-end foreground-aware network for person re-identification. IEEE Transactions on Image Processing, 30:2060\u20132071, 2021.": "An end-to-end foreground-aware network for person re-identification", "[16] Taiqing Wang, Shaogang Gong, Xiatian Zhu, and Shengjin Wang. Person re-identification by video ranking. In European conference on computer vision, pages 688\u2013703. Springer, 2014.": "Person re-identification by video ranking", "[250] Djebril Mekhazni, Amran Bhuiyan, George Ekladious, and Eric Granger. Unsupervised domain adaptation in the dissimilarity space for person re-identification. In European Conference on Computer Vision, pages 159\u2013174. Springer, 2020.": "Unsupervised domain adaptation in the dissimilarity space for person re-identification", "[237] Wenfeng Song, Shuai Li, Tao Chang, Aimin Hao, Qinping Zhao, and Hong Qin. Context-interactive cnn for person re-identification. IEEE Transactions on Image Processing, 29:2860\u20132874, 2019.": "Context-Interactive CNN for Person Re-Identification", "[78] Yeong-Jun Cho and Kuk-Jin Yoon. Improving person re-identification via pose-aware multi-shot matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1354\u20131362, 2016.": "Improving person re-identification via pose-aware multi-shot matching", "[120] Yan Huang, Qiang Wu, Jingsong Xu, Yi Zhong, and Zhaoxiang Zhang. Unsupervised domain adaptation with background shift mitigating for person re-identification. International Journal of Computer Vision, 129(7):2244\u20132263, 2021.": "Unsupervised domain adaptation with background shift mitigating for person re-identification", "[217] Zhiyi Cheng, Qi Dong, Shaogang Gong, and Xiatian Zhu. Inter-task association critic for cross-resolution person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2605\u20132615, 2020.": "Inter-task association critic for cross-resolution person re-identification", "[171] Yewen Huang, Sicheng Lian, Haifeng Hu, Dihu Chen, and Tao Su. Multiscale omnibearing attention networks for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 31(5):1790\u20131803, 2020.": "Multiscale Omnibearing Attention Networks for Person Re-identification", "[201] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Cross-view asymmetric metric learning for unsupervised person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 994\u20131002, 2017.": "Cross-view asymmetric metric learning for unsupervised person re-identification", "[104] Yeong-Jun Cho and Kuk-Jin Yoon. Pamm: Pose-aware multi-shot matching for improving person re-identification. IEEE Transactions on Image Processing, 27(8):3739\u20133752, 2018.": "Pamm: Pose-aware multi-shot matching for improving person re-identification", "[208] Xiang Li, Wei-Shi Zheng, Xiaojuan Wang, Tao Xiang, and Shaogang Gong. Multi-scale learning for low-resolution person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3765\u20133773, 2015.": "Multi-scale learning for low-resolution person re-identification", "[278] Wei Li, Xiatian Zhu, and Shaogang Gong. Scalable person re-identification by harmonious attention. International Journal of Computer Vision, 128(6):1635\u20131653, 2020.": "Scalable person re-identification by harmonious attention", "[180] Ying-Cong Chen, Xiatian Zhu, Wei-Shi Zheng, and Jian-Huang Lai. Person re-identification by camera correlation aware feature augmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(2):392\u2013408, 2018.": "Person re-identification by camera correlation aware feature augmentation", "[229] Seokeon Choi, Taekyung Kim, Minki Jeong, Hyoungseob Park, and Changick Kim. Meta batch-instance normalization for generalizable person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3425\u20133435, 2021.": "Meta batch-instance normalization for generalizable person re-identification", "[90] Pingyu Wang, Zhicheng Zhao, Fei Su, Xingyu Zu, and Nikolaos V Boulgouris. Horeid: Deep high-order mapping enhances pose alignment for person re-identification. IEEE Transactions on Image Processing, 30:2908\u20132922, 2021.": "Horeid: Deep high-order mapping enhances pose alignment for person re-identification", "[134] Ju Dai, Pingping Zhang, Dong Wang, Huchuan Lu, and Hongyu Wang. Video person re-identification by temporal residual learning. IEEE Transactions on Image Processing, 28(3):1366\u20131377, 2018.": "Video person re-identification by temporal residual learning", "[153] Houjing Huang, Wenjie Yang, Xiaotang Chen, Xin Zhao, Kaiqi Huang, Jinbin Lin, Guan Huang, and Dalong Du. Eanet: Enhancing alignment for cross-domain person re-identification. arXiv preprint arXiv:1812.11369, 2018.": "Eanet: Enhancing Alignment for Cross-Domain Person Re-Identification", "[273] Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang Huang, and Xinggang Wang. Unsupervised domain adaptive re-identification: Theory and practice. Pattern Recognition, 102:107173, 2020.": "Unsupervised Domain Adaptive Re-Identification: Theory and Practice", "[193] Jieru Jia, Qiuqi Ruan, Yi Jin, Gaoyun An, and Shiming Ge. View-specific subspace learning and re-ranking for semi-supervised person re-identification. Pattern Recognition, 108:107568, 2020.": "View-specific subspace learning and re-ranking for semi-supervised person re-identification", "[192] Cairong Zhao, Xuekuan Wang, Duoqian Miao, Hanli Wang, Weishi Zheng, Yong Xu, and David Zhang. Maximal granularity structure and generalized multi-view discriminant analysis for person re-identification. Pattern Recognition, 79:79\u201396, 2018.": "Maximal granularity structure and generalized multi-view discriminant analysis for person re-identification", "[60] Jinrui Yang, Jiawei Zhang, Fufu Yu, Xinyang Jiang, Mengdan Zhang, Xing Sun, Ying-Cong Chen, and Wei-Shi Zheng. Learning to know where to see: A visibility-aware approach for occluded person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11885\u201311894, 2021.": "Learning to know where to see: A visibility-aware approach for occluded person re-identification", "[15] Martin Hirzer, Csaba Beleznai, Peter M Roth, and Horst Bischof. Person re-identification by descriptive and discriminative classification. In Scandinavian conference on Image analysis, pages 91\u2013102. Springer, 2011.": "Person re-identification by descriptive and discriminative classification", "[74] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Interaction-and-aggregation network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9317\u20139326, 2019.": "Interaction-and-aggregation network for person re-identification", "[133] Guanshuo Wang, Yufeng Yuan, Jiwei Li, Shiming Ge, and Xi Zhou. Receptive multi-granularity representation for person re-identification. IEEE Transactions on Image Processing, 29:6096\u20136109, 2020.": "Receptive multi-granularity representation for person re-identification", "[222] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: a deep quadruplet network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 403\u2013412, 2017.": "Beyond triplet loss: a deep quadruplet network for person re-identification", "[251] Zechen Bai, Zhigang Wang, Jian Wang, Di Hu, and Errui Ding. Unsupervised multi-source domain adaptation for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12914\u201312923, 2021.": "Unsupervised Multi-Source Domain Adaptation for Person Re-Identification", "[268] Shengcai Liao and Ling Shao. Interpretable and generalizable person re-identification with query-adaptive convolution and temporal lifting. arXiv preprint arXiv:1904.10424, 2019.": "Interpretable and Generalizable Person Re-Identification with Query-Adaptive Convolution and Temporal Lifting", "[92] Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. Multi-type attributes driven multi-camera person re-identification. Pattern Recognition, 75:77\u201389, 2018.": "Multi-type attributes driven multi-camera person re-identification", "[239] Shan Lin, Chang-Tsun Li, and Alex C Kot. Multi-domain adversarial feature generalization for person re-identification. IEEE Transactions on Image Processing, 30:1596\u20131607, 2020.": "Multi-domain adversarial feature generalization for person re-identification", "[203] Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J Radke. Re-identification with consistent attentive siamese networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5735\u20135744, 2019.": "Re-identification with consistent attentive siamese networks", "[277] Jieming Zhou, Soumava Kumar Roy, Pengfei Fang, Mehrtash Harandi, and Lars Petersson. Cross-correlated attention networks for person re-identification. Image and Vision Computing, 100:103931, 2020.": "Cross-correlated attention networks for person re-identification", "[224] Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Rongrong Ji, and Yonghong Tian. Multiple expert brainstorming for domain adaptive person re-identification. arXiv preprint arXiv:2007.01546, 2020.": "Multiple Expert Brainstorming for Domain Adaptive Person Re-Identification", "[199] Le An, Zhen Qin, Xiaojing Chen, and Songfan Yang. Multi-level common space learning for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 28(8):1777\u20131787, 2018.": "Multi-level common space learning for person re-identification", "[152] Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang, and Xilin Chen. Appearance-preserving 3d convolution for video-based person re-identification. In European Conference on Computer Vision, pages 228\u2013243. Springer, 2020.": "Appearance-Preserving 3D Convolution for Video-Based Person Re-Identification", "[231] Anguo Zhang, Yueming Gao, Yuzhen Niu, Wenxi Liu, and Yongcheng Zhou. Coarse-to-fine person re-identification with auxiliary-domain classification and second-order information bottleneck. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 598\u2013607, 2021.": "Coarse-to-fine person re-identification with auxiliary-domain classification and second-order information bottleneck", "[248] Xin Jin, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Global distance-distributions separation for unsupervised person re-identification. In European Conference on Computer Vision, pages 735\u2013751. Springer, 2020.": "Global distance-distributions separation for unsupervised person re-identification", "[157] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. Omni-scale feature learning for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3702\u20133712, 2019.": "Omni-scale feature learning for person re-identification", "[129] Hao Luo, Wei Jiang, Xuan Zhang, Xing Fan, Jingjing Qian, and Chi Zhang. Alignedreid++: Dynamically matching local information for person re-identification. Pattern Recognition, 94:53\u201361, 2019.": "Alignedreid++: Dynamically matching local information for person re-identification", "[189] Houjing Huang, Wenjie Yang, Jinbin Lin, Guan Huang, Jiamiao Xu, Guoli Wang, Xiaotang Chen, and Kaiqi Huang. Improve person re-identification with part awareness learning. IEEE Transactions on Image Processing, 29:7468\u20137481, 2020.": "Improve person re-identification with part awareness learning", "[254] Fengxiang Yang, Zhun Zhong, Zhiming Luo, Yuanzheng Cai, Yaojin Lin, Shaozi Li, and Nicu Sebe. Joint noise-tolerant learning and meta camera shift adaptation for unsupervised person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4855\u20134864, 2021.": "Joint noise-tolerant learning and meta camera shift adaptation for unsupervised person re-identification", "[137] Xiaolong Ma, Xiatian Zhu, Shaogang Gong, Xudong Xie, Jianming Hu, Kin-Man Lam, and Yisheng Zhong. Person re-identification by unsupervised video matching. Pattern Recognition, 65:197\u2013210, 2017.": "Person re-identification by unsupervised video matching", "[62] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin Chen. Feature completion for occluded person re-identification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.": "Feature completion for occluded person re-identification", "[14] Douglas Gray, Shane Brennan, and Hai Tao. Evaluating appearance models for recognition, reacquisition, and tracking. In Proc. IEEE international workshop on performance evaluation for tracking and surveillance (PETS), volume 3, pages 1\u20137. Citeseer, 2007.": "Evaluating Appearance Models for Recognition, Reacquisition, and Tracking", "[177] Ju Dai, Ying Zhang, Huchuan Lu, and Hongyu Wang. Cross-view semantic projection learning for person re-identification. Pattern Recognition, 75:63\u201376, 2018.": "Cross-view semantic projection learning for person re-identification", "[93] M Saquib Sarfraz, Arne Schumann, Andreas Eberle, and Rainer Stiefelhagen. A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 420\u2013429, 2018.": "A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking", "[210] Ke Han, Yan Huang, Zerui Chen, Liang Wang, and Tieniu Tan. Prediction and recovery for adaptive low-resolution person re-identification. In European Conference on Computer Vision, pages 193\u2013209. Springer, 2020.": "Prediction and Recovery for Adaptive Low-Resolution Person Re-identification", "[247] Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pontil, Shaogang Gong, Tiejun Huang, and Yonghong Tian. Unsupervised cross-dataset transfer learning for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1306\u20131315, 2016.": "Unsupervised Cross-Dataset Transfer Learning for Person Re-Identification", "[241] Zhicheng Zhao, Binlin Zhao, and Fei Su. Person re-identification via integrating patch-based metric learning and local salience learning. Pattern Recognition, 75:90\u201398, 2018.": "Person re-identification via integrating patch-based metric learning and local salience learning", "[142] Tianyu He, Xin Jin, Xu Shen, Jianqiang Huang, Zhibo Chen, and Xian-Sheng Hua. Dense interaction learning for video-based person re-identification. arXiv preprint arXiv:2103.09013, 2021.": "Dense interaction learning for video-based person re-identification", "[156] Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao Yu, Feiyue Huang, and Rongrong Ji. Pyramidal person re-identification via multi-loss dynamic training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8514\u20138522, 2019.": "Pyramidal person re-identification via multi-loss dynamic training", "[88] Jiahang Yin, Ancong Wu, and Wei-Shi Zheng. Fine-grained person re-identification. International journal of computer vision, 128(6):1654\u20131672, 2020.": "Fine-grained person re-identification", "[48] Guangyi Chen, Chunze Lin, Liangliang Ren, Jiwen Lu, and Jie Zhou. Self-critical attention learning for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 9637\u20139646, 2019.": "Self-critical attention learning for person re-identification", "[121] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo Chen. Densely semantically aligned person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 667\u2013676, 2019.": "Densely semantically aligned person re-identification", "[105] Wenjie Yang, Houjing Huang, Zhang Zhang, Xiaotang Chen, Kaiqi Huang, and Shu Zhang. Towards rich feature discovery with class activation maps augmentation for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1389\u20131398, 2019.": "Towards rich feature discovery with class activation maps augmentation for person re-identification", "[122] Lingxiao He, Jian Liang, Haiqing Li, and Zhenan Sun. Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7073\u20137082, 2018.": "Deep spatial feature reconstruction for partial person re-identification: Alignment-free approach", "[118] Jia Sun, Yanfeng Li, Houjin Chen, Bin Zhang, and Jinlei Zhu. Memf: Multi-level-attention embedding and multi-layer-feature fusion model for person re-identification. Pattern Recognition, 116:107937, 2021.": "Memf: Multi-level-attention embedding and multi-layer-feature fusion model for person re-identification", "[256] Yuyang Zhao, Zhun Zhong, Fengxiang Yang, Zhiming Luo, Yaojin Lin, Shaozi Li, and Nicu Sebe. Learning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6277\u20136286, 2021.": "Learning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification", "[270] Yingzhi Tang, Xi Yang, Nannan Wang, Bin Song, and Xinbo Gao. Cgan-tm: A novel domain-to-domain transferring method for person re-identification. IEEE Transactions on Image Processing, 29:5641\u20135651, 2020.": "CGAN-TM: A Novel Domain-to-Domain Transferring Method for Person Re-Identification", "[245] Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. Deep attributes driven multi-camera person re-identification. In European conference on computer vision, pages 475\u2013491. Springer, 2016.": "Deep attributes driven multi-camera person re-identification", "[202] Hong-Xing Yu, Ancong Wu, and Wei-Shi Zheng. Unsupervised person re-identification by deep asymmetric metric embedding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(4):956\u2013973, 2020.": "Unsupervised person re-identification by deep asymmetric metric embedding", "[174] Srikrishna Karanam, Yang Li, and Richard J Radke. Person re-identification with discriminatively trained viewpoint invariant dictionaries. In Proceedings of the IEEE international conference on computer vision, pages 4516\u20134524, 2015.": "Person re-identification with discriminatively trained viewpoint invariant dictionaries", "[240] Hengheng Zhang, Ying Li, Zijie Zhuang, Lingxi Xie, and Qi Tian. 3d-gat: 3d-guided adversarial transform network for person re-identification in unseen domains. Pattern Recognition, 112:107799, 2021.": "3D-GAT: 3D-Guided Adversarial Transform Network for Person Re-Identification in Unseen Domains", "[194] Hai-Miao Hu, Wen Fang, Bo Li, and Qi Tian. An adaptive multi-projection metric learning for person re-identification across non-overlapping cameras. IEEE Transactions on Circuits and Systems for Video Technology, 29(9):2809\u20132821, 2019.": "An adaptive multi-projection metric learning for person re-identification across non-overlapping cameras", "[59] Cheng Yan, Guansong Pang, Jile Jiao, Xiao Bai, Xuetao Feng, and Chunhua Shen. Occluded person re-identification with single-scale global representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11875\u201311884, 2021.": "Occluded person re-identification with single-scale global representations", "[110] Maoqing Tian, Shuai Yi, Hongsheng Li, Shihua Li, Xuesen Zhang, Jianping Shi, Junjie Yan, and Xiaogang Wang. Eliminating background-bias for robust person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5794\u20135803, 2018.": "Eliminating background-bias for robust person re-identification", "[150] Zhizhong Zhang, Yuan Xie, Ding Li, Wensheng Zhang, and Qi Tian. Learning to align via wasserstein for person re-identification. IEEE Transactions on Image Processing, 29:7104\u20137116, 2020.": "Learning to Align via Wasserstein for Person Re-identification", "[190] Yutian Lin, Yu Wu, Chenggang Yan, Mingliang Xu, and Yi Yang. Unsupervised person re-identification via cross-camera similarity exploration. IEEE Transactions on Image Processing, 29:5481\u20135490, 2020.": "Unsupervised person re-identification via cross-camera similarity exploration", "[274] Binghui Chen, Weihong Deng, and Jiani Hu. Mixed high-order attention network for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 371\u2013381, 2019.": "Mixed High-Order Attention Network for Person Re-Identification", "[234] Amena Khatun, Simon Denman, Sridha Sridharan, and Clinton Fookes. Joint identification\u2013verification for person re-identification: A four stream deep learning approach with improved quartet loss function. Computer Vision and Image Understanding, 197:102989, 2020.": "Joint identification\u2013verification for person re-identification: A four stream deep learning approach with improved quartet loss function", "[236] Hao Feng, Minghao Chen, Jinming Hu, Dong Shen, Haifeng Liu, and Deng Cai. Complementary pseudo labels for unsupervised domain adaptation on person re-identification. IEEE Transactions on Image Processing, 30:2898\u20132907, 2021.": "Complementary pseudo labels for unsupervised domain adaptation on person re-identification", "[119] Xin Ning, Ke Gong, Weijun Li, Liping Zhang, Xiao Bai, and Shengwei Tian. Feature refinement and filter network for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2020.": "Feature refinement and filter network for person re-identification", "[167] Xi Yang, Liangchen Liu, Nannan Wang, and Xinbo Gao. A two-stream dynamic pyramid representation model for video-based person re-identification. IEEE Transactions on Image Processing, 30:6266\u20136276, 2021.": "A two-stream dynamic pyramid representation model for video-based person re-identification", "[280] Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, and Pan Zhou. Jointly attentive spatial-temporal pooling networks for video-based person re-identification. In Proceedings of the IEEE international conference on computer vision, pages 4733\u20134742, 2017.": "Jointly attentive spatial-temporal pooling networks for video-based person re-identification", "[81] Niall McLaughlin, Jesus Martinez del Rincon, and Paul C Miller. Person reidentification using deep convnets with multitask learning. IEEE Transactions on Circuits and Systems for Video Technology, 27(3):525\u2013539, 2016.": "Person reidentification using deep convnets with multitask learning", "[84] Jianing Li, Shiliang Zhang, Qi Tian, Meng Wang, and Wen Gao. Pose-guided representation learning for person re-identification. IEEE transactions on pattern analysis and machine intelligence, 2019.": "Pose-guided representation learning for person re-identification", "[265] Huafeng Li, Shuanglin Yan, Zhengtao Yu, and Dapeng Tao. Attribute-identity embedding and self-supervised learning for scalable person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 30(10):3472\u20133485, 2019.": "Attribute-identity embedding and self-supervised learning for scalable person re-identification", "[91] Yantao Shen, Tong Xiao, Shuai Yi, Dapeng Chen, Xiaogang Wang, and Hongsheng Li. Person re-identification with deep kronecker-product matching and group-shuffling random walk. IEEE transactions on pattern analysis and machine intelligence, 43(5):1649\u20131665, 2021.": "Person re-identification with deep kronecker-product matching and group-shuffling random walk", "[66] Shizhen Zhao, Changxin Gao, Jun Zhang, Hao Cheng, Chuchu Han, Xinyang Jiang, Xiaowei Guo, Wei-Shi Zheng, Nong Sang, and Xing Sun. Do not disturb me: Person re-identification under the interference of other pedestrians. In European Conference on Computer Vision, pages 647\u2013663. Springer, 2020.": "Do not disturb me: Person re-identification under the interference of other pedestrians", "[148] Kan Wang, Changxing Ding, Stephen J Maybank, and Dacheng Tao. Cdpm: convolutional deformable part models for semantically aligned person re-identification. IEEE Transactions on Image Processing, 29:3416\u20133428, 2019.": "Cdpm: Convolutional Deformable Part Models for Semantically Aligned Person Re-identification", "[213] Zhanxiang Feng, Jianhuang Lai, and Xiaohua Xie. Resolution-aware knowledge distillation for efficient inference. IEEE Transactions on Image Processing, 30:6985\u20136996, 2021.": "Resolution-aware knowledge distillation for efficient inference", "[143] Pengfei Fang, Jieming Zhou, Soumava Kumar Roy, Lars Petersson, and Mehrtash Harandi. Bilinear attention networks for person retrieval. In Proceedings of the IEEE International Conference on Computer Vision, pages 8030\u20138039, 2019.": "Bilinear attention networks for person retrieval", "[173] Xiaoxiao Sun and Liang Zheng. Dissecting person re-identification from the viewpoint of viewpoint. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 608\u2013617, 2019.": "Dissecting person re-identification from the viewpoint of viewpoint", "[10] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In Proceedings of the IEEE international conference on computer vision, pages 1116\u20131124, 2015.": "Scalable person re-identification: A benchmark", "[182] Shi-Zhe Chen, Chun-Chao Guo, and Jian-Huang Lai. Deep ranking for person re-identification via joint representation learning. IEEE Transactions on Image Processing, 25(5):2353\u20132367, 2016.": "Deep ranking for person re-identification via joint representation learning", "[72] Yulin Li, Jianfeng He, Tianzhu Zhang, Xiang Liu, Yongdong Zhang, and Feng Wu. Diverse part discovery: Occluded person re-identification with part-aware transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2898\u20132907, 2021.": "Diverse part discovery: Occluded person re-identification with part-aware transformer", "[82] Jin Wang, Zheng Wang, Changxin Gao, Nong Sang, and Rui Huang. Deeplist: Learning deep features with adaptive listwise constraint for person reidentification. IEEE Transactions on Circuits and Systems for Video Technology, 27(3):513\u2013524, 2016.": "Deeplist: Learning deep features with adaptive listwise constraint for person reidentification", "[12] Wei Li, Rui Zhao, and Xiaogang Wang. Human reidentification with transferred metric learning. In Asian conference on computer vision, pages 31\u201344. Springer, 2012.": "Human reidentification with transferred metric learning", "[17] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, and Qi Tian. Mars: A video benchmark for large-scale person re-identification. In European Conference on Computer Vision, pages 868\u2013884. Springer, 2016.": "Mars: A video benchmark for large-scale person re-identification", "[145] Cheng Wang, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Mancs: A multi-task attentional network with curriculum sampling for person re-identification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 365\u2013381, 2018.": "Mancs: A multi-task attentional network with curriculum sampling for person re-identification", "[63] Xiaokang Zhang, Yan Yan, Jing-Hao Xue, Yang Hua, and Hanzi Wang. Semantic-aware occlusion-robust network for occluded person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2020.": "Semantic-aware Occlusion-Robust Network for Occluded Person Re-identification", "[162] Ancong Wu, Wei-Shi Zheng, Xiaowei Guo, and Jian-Huang Lai. Distilled person re-identification: Towards a more scalable system. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1187\u20131196, 2019.": "Distilled Person Re-Identification: Towards a More Scalable System", "[179] Z Wu, Y Li, and RJ Radke. Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features. IEEE transactions on pattern analysis and machine intelligence, 37(5):1095, 2015.": "Viewpoint Invariant Human Re-identification in Camera Networks Using Pose Priors and Subject-discriminative Features", "[164] Cheng Yan, Guansong Pang, Lei Wang, Jile Jiao, Xuetao Feng, Chunhua Shen, and Jingjing Li. Bv-person: A large-scale dataset for bird-view person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10943\u201310952, 2021.": "Bv-person: A large-scale dataset for bird-view person re-identification", "[128] Mahdi M Kalayeh, Emrah Basaran, Muhittin G\u00f6kmen, Mustafa E Kamasak, and Mubarak Shah. Human semantic parsing for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1062\u20131071, 2018.": "Human Semantic Parsing for Person Re-identification", "[264] Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised tracklet person re-identification. IEEE transactions on pattern analysis and machine intelligence, 42(7):1770\u20131782, 2020.": "Unsupervised tracklet person re-identification", "[116] Sanping Zhou, Jinjun Wang, Deyu Meng, Yudong Liang, Yihong Gong, and Nanning Zheng. Discriminative feature learning with foreground attention for person re-identification. IEEE Transactions on Image Processing, 28(9):4671\u20134684, 2019.": "Discriminative feature learning with foreground attention for person re-identification", "[205] Sicheng Lian, Weitao Jiang, and Haifeng Hu. Attention-aligned network for person re-identification. IEEE Transactions on Circuits and Systems for Video Technology, 2021.": "Attention-aligned network for person re-identification", "[209] Xiao-Yuan Jing, Xiaoke Zhu, Fei Wu, Xinge You, Qinglong Liu, Dong Yue, Ruimin Hu, and Baowen Xu. Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 695\u2013704, 2015.": "Super-resolution person re-identification with semi-coupled low-rank discriminant dictionary learning", "[186] Alessandro Borgia, Yang Hua, Elyor Kodirov, and Neil M Robertson. Cross-view discriminative feature learning for person re-identification. IEEE Transactions on Image Processing, 27(11):5338\u20135349, 2018.": "Cross-view discriminative feature learning for person re-identification", "[260] Yan Bai, Ce Wang, Yihang Lou, Jun Liu, and Ling-Yu Duan. Hierarchical connectivity-centered clustering for unsupervised domain adaptation on person re-identification. IEEE Transactions on Image Processing, 30:6715\u20136729, 2021.": "Hierarchical connectivity-centered clustering for unsupervised domain adaptation on person re-identification", "[99] Xiaoqiang Hu, Dan Wei, Ziyang Wang, Jianglin Shen, and Hongjuan Ren. Hypergraph video pedestrian re-identification based on posture structure relationship and action constraints. Pattern Recognition, 111:107688, 2021.": "Hypergraph video pedestrian re-identification based on posture structure relationship and action constraints", "[252] Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang, Houqiang Li, and Dong Chen. Unsupervised pre-training for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14750\u201314759, 2021.": "Unsupervised pre-training for person re-identification", "[58] Guan\u2019an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang, Yang Yang, Shuliang Wang, Gang Yu, Erjin Zhou, and Jian Sun. High-order information matters: Learning relation and topology for occluded person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6449\u20136458, 2020.": "High-order information matters: Learning relation and topology for occluded person re-identification"}, "source_title_to_arxiv_id": {"Spatial-temporal correlation and topology learning for person re-identification in videos": "2104.08241", "Group-aware label transfer for domain adaptive person re-identification": "2103.12366", "Spatio-temporal representation factorization for video-based person re-identification": "2107.11878", "Feature completion for occluded person re-identification": "2106.12733"}}