{"title": "Boosting Binary Neural Networks via Dynamic Thresholds Learning", "abstract": "Developing lightweight Deep Convolutional Neural Networks (DCNNs) and Vision\nTransformers (ViTs) has become one of the focuses in vision research since the\nlow computational cost is essential for deploying vision models on edge\ndevices. Recently, researchers have explored highly computational efficient\nBinary Neural Networks (BNNs) by binarizing weights and activations of\nFull-precision Neural Networks. However, the binarization process leads to an\nenormous accuracy gap between BNN and its full-precision version. One of the\nprimary reasons is that the Sign function with predefined or learned static\nthresholds limits the representation capacity of binarized architectures since\nsingle-threshold binarization fails to utilize activation distributions. To\novercome this issue, we introduce the statistics of channel information into\nexplicit thresholds learning for the Sign Function dubbed DySign to generate\nvarious thresholds based on input distribution. Our DySign is a straightforward\nmethod to reduce information loss and boost the representative capacity of\nBNNs, which can be flexibly applied to both DCNNs and ViTs (i.e., DyBCNN and\nDyBinaryCCT) to achieve promising performance improvement. As shown in our\nextensive experiments. For DCNNs, DyBCNNs based on two backbones (MobileNetV1\nand ResNet18) achieve 71.2% and 67.4% top1-accuracy on ImageNet dataset,\noutperforming baselines by a large margin (i.e., 1.8% and 1.5% respectively).\nFor ViTs, DyBinaryCCT presents the superiority of the convolutional embedding\nlayer in fully binarized ViTs and achieves 56.1% on the ImageNet dataset, which\nis nearly 9% higher than the baseline.", "authors": ["Jiehua Zhang", "Xueyang Zhang", "Zhuo Su", "Zitong Yu", "Yanghe Feng", "Xin Lu", "Matti Pietik\u00e4inen", "Li Liu"], "published_date": "2022_11_04", "pdf_url": "http://arxiv.org/pdf/2211.02292v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Binary Method</td><td>W/A</td><td>BOPs(\\times 10^{9})</td><td>FLOPs(\\times 10^{8})</td><td>OPs(\\times 10^{8})</td><td>Acc Top-1 (%)</td></tr><tr><td>BNN[36]</td><td>1/1</td><td>1.70</td><td>1.20</td><td>1.47</td><td>42.2</td></tr><tr><td>ABC-Net[77]</td><td>1/1</td><td>-</td><td>-</td><td>-</td><td>42.7</td></tr><tr><td>XNOR-Net[37]</td><td>1/1</td><td>1.70</td><td>1.41</td><td>1.67</td><td>51.2</td></tr><tr><td>DoReFa[40]</td><td>1/2</td><td>-</td><td>-</td><td>-</td><td>53.4</td></tr><tr><td>Bi-RealNet-18[38]</td><td>1/1</td><td>1.68</td><td>1.39</td><td>1.63</td><td>56.4</td></tr><tr><td>XNOR++ [78]</td><td>1/1</td><td>-</td><td>-</td><td>-</td><td>57.1</td></tr><tr><td>IR-Net [46]</td><td>1/1</td><td>-</td><td>-</td><td>-</td><td>58.1</td></tr><tr><td>BONN[79]</td><td>1/1</td><td>-</td><td>-</td><td>-</td><td>59.3</td></tr><tr><td>NoisySupervision[80]</td><td>1/1</td><td>-</td><td>-</td><td>-</td><td>59.4</td></tr><tr><td>Bi-RealNet-34[38]</td><td>1/1</td><td>3.53</td><td>1.39</td><td>1.93</td><td>62.2</td></tr><tr><td>Real-to-Binary Net[55]</td><td>1/1</td><td>1.68</td><td>1.56</td><td>1.83</td><td>65.4</td></tr><tr><td>ReActNet-ResNet18[43]</td><td>1/1</td><td>1.68</td><td>1.39</td><td>1.63</td><td>65.9</td></tr><tr><td>ReActNet[43]</td><td>1/1</td><td>4.82</td><td>0.22</td><td>0.97</td><td>69.4</td></tr><tr><td>AdamBNN[81]</td><td>1/1</td><td>4.82</td><td>0.22</td><td>0.97</td><td>70.5</td></tr><tr><td>DyBCNN-ResNet18(ours)</td><td>1/1</td><td>1.68</td><td>1.43</td><td>1.98</td><td>67.4(\\uparrow 1.5)</td></tr><tr><td>DyBCNN(ours)</td><td>1/1</td><td>4.82</td><td>0.24</td><td>0.99</td><td>71.2(\\uparrow 1.8)</td></tr></tbody></table><ul><li>1<p>The W/A denote the number of bits in weight and activation quantization. The blue font denotes the comparing result with DyBCNN and ReActNet based on ResNet18. The red font denotes the comparing result based on MobileNetV1.</p></li></ul>", "caption": "TABLE I: Compare of the top-1 accuracy with SOTA methods.", "list_citation_info": ["[80] K. Han, Y. Wang, Y. Xu, C. Xu, E. Wu, and C. Xu, \u201cTraining binary neural networks through learning with noisy supervision,\u201d in ICML, 2020.", "[46] H. Qin, R. Gong, X. Liu, M. Shen, Z. Wei, F. Yu, and J. Song, \u201cForward and backward information retention for accurate binary neural networks,\u201d in CVPR, 2020.", "[37] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, \u201cXNOR-Net: imagenet classification using binary convolutional neural networks,\u201d in ECCV, 2016.", "[78] A. Bulat and G. Tzimiropoulos, \u201cXNOR-Net++: improved binary neural networks,\u201d arXiv:1909.13863, 2019.", "[40] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, \u201cDoReFa-Net: training low bitwidth convolutional neural networks with low bitwidth gradients,\u201d arXiv:1606.06160, 2016.", "[77] X. Lin, C. Zhao, and W. Pan, \u201cTowards accurate binary convolutional neural network,\u201d in NeurIPS, vol. 30, 2017.", "[43] Z. Liu, Z. Shen, M. Savvides, and K. T. Cheng, \u201cReActNet: towards precise binary neural network with generalized activation functions,\u201d ECCV, 2020.", "[79] J. Gu, J. Zhao, X. Jiang, B. Zhang, J. Liu, G. Guo, and R. Ji, \u201cBayesian optimized 1-bit cnns,\u201d in CVPR, 2019.", "[81] Z. Liu, Z. Shen, S. Li, K. Helwegen, D. Huang, and K.-T. Cheng, \u201cHow do adam and training strategies help bnns optimization,\u201d in ICML, 2021.", "[36] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio, \u201cBinarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1,\u201d arXiv:1602.02830, 2016.", "[38] Z. Liu, W. Luo, B. Wu, X. Yang, W. Liu, and K. Cheng, \u201cBi-Real net: binarizing deep network towards real-network performance,\u201d International Journal of Computer Vision, pp. 1\u201318, 2018.", "[55] B. Martinez, J. Yang, A. Bulat, and G. Tzimiropoulos, \u201cTraining binary neural networks with real-to-binary convolutions,\u201d in ICLR, 2020."]}], "citation_info_to_title": {"[81] Z. Liu, Z. Shen, S. Li, K. Helwegen, D. Huang, and K.-T. Cheng, \u201cHow do adam and training strategies help bnns optimization,\u201d in ICML, 2021.": "How do Adam and Training Strategies Help BNNs Optimization", "[36] M. Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio, \u201cBinarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1,\u201d arXiv:1602.02830, 2016.": "Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1", "[46] H. Qin, R. Gong, X. Liu, M. Shen, Z. Wei, F. Yu, and J. Song, \u201cForward and backward information retention for accurate binary neural networks,\u201d in CVPR, 2020.": "Forward and backward information retention for accurate binary neural networks", "[78] A. Bulat and G. Tzimiropoulos, \u201cXNOR-Net++: improved binary neural networks,\u201d arXiv:1909.13863, 2019.": "XNOR-Net++: Improved Binary Neural Networks", "[40] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, \u201cDoReFa-Net: training low bitwidth convolutional neural networks with low bitwidth gradients,\u201d arXiv:1606.06160, 2016.": "DoReFa-Net: training low bitwidth convolutional neural networks with low bitwidth gradients", "[77] X. Lin, C. Zhao, and W. Pan, \u201cTowards accurate binary convolutional neural network,\u201d in NeurIPS, vol. 30, 2017.": "Towards accurate binary convolutional neural network", "[43] Z. Liu, Z. Shen, M. Savvides, and K. T. Cheng, \u201cReActNet: towards precise binary neural network with generalized activation functions,\u201d ECCV, 2020.": "ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions", "[55] B. Martinez, J. Yang, A. Bulat, and G. Tzimiropoulos, \u201cTraining binary neural networks with real-to-binary convolutions,\u201d in ICLR, 2020.": "Training binary neural networks with real-to-binary convolutions", "[79] J. Gu, J. Zhao, X. Jiang, B. Zhang, J. Liu, G. Guo, and R. Ji, \u201cBayesian optimized 1-bit cnns,\u201d in CVPR, 2019.": "Bayesian Optimized 1-Bit CNNs", "[38] Z. Liu, W. Luo, B. Wu, X. Yang, W. Liu, and K. Cheng, \u201cBi-Real net: binarizing deep network towards real-network performance,\u201d International Journal of Computer Vision, pp. 1\u201318, 2018.": "Bi-Real net: binarizing deep network towards real-network performance", "[37] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, \u201cXNOR-Net: imagenet classification using binary convolutional neural networks,\u201d in ECCV, 2016.": "XNOR-Net: Imagenet Classification Using Binary Convolutional Neural Networks", "[80] K. Han, Y. Wang, Y. Xu, C. Xu, E. Wu, and C. Xu, \u201cTraining binary neural networks through learning with noisy supervision,\u201d in ICML, 2020.": "Training binary neural networks through learning with noisy supervision"}, "source_title_to_arxiv_id": {"Towards accurate binary convolutional neural network": "1711.11294"}}