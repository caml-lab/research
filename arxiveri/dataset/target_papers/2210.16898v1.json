{"title": "Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin Lesion Segmentation", "abstract": "Melanoma is caused by the abnormal growth of melanocytes in human skin. Like\nother cancers, this life-threatening skin cancer can be treated with early\ndiagnosis. To support a diagnosis by automatic skin lesion segmentation,\nseveral Fully Convolutional Network (FCN) approaches, specifically the U-Net\narchitecture, have been proposed. The U-Net model with a symmetrical\narchitecture has exhibited superior performance in the segmentation task.\nHowever, the locality restriction of the convolutional operation incorporated\nin the U-Net architecture limits its performance in capturing long-range\ndependency, which is crucial for the segmentation task in medical images. To\naddress this limitation, recently a Transformer based U-Net architecture that\nreplaces the CNN blocks with the Swin Transformer module has been proposed to\ncapture both local and global representation. In this paper, we propose\nAtt-SwinU-Net, an attention-based Swin U-Net extension, for medical image\nsegmentation. In our design, we seek to enhance the feature re-usability of the\nnetwork by carefully designing the skip connection path. We argue that the\nclassical concatenation operation utilized in the skip connection path can be\nfurther improved by incorporating an attention mechanism. By performing a\ncomprehensive ablation study on several skin lesion segmentation datasets, we\ndemonstrate the effectiveness of our proposed attention mechanism.", "authors": ["Ehsan Khodapanah Aghdam", "Reza Azad", "Maral Zarvani", "Dorit Merhof"], "published_date": "2022_10_30", "pdf_url": "http://arxiv.org/pdf/2210.16898v1", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Methods</th><th colspan=\"4\">ISIC 2017</th><th colspan=\"4\">ISIC 2018</th><th colspan=\"4\">PH{}^{2}</th></tr><tr><th>DSC</th><th>SE</th><th>SP</th><th>ACC</th><th>DSC</th><th>SE</th><th>SP</th><th>ACC</th><th>DSC</th><th>SE</th><th>SP</th><th>ACC</th></tr></thead><tbody><tr><th>U-Net [3]</th><td>0.8159</td><td>0.8172</td><td>0.9680</td><td>0.9164</td><td>0.8545</td><td>0.8800</td><td>0.9697</td><td>0.9404</td><td>0.8936</td><td>0.9125</td><td>0.9588</td><td>0.9233</td></tr><tr><th>Att U-Net [13]</th><td>0.8082</td><td>0.7998</td><td>0.9776</td><td>0.9145</td><td>0.8566</td><td>0.8674</td><td>0.9863</td><td>0.9376</td><td>0.9003</td><td>0.9205</td><td>0.9640</td><td>0.9276</td></tr><tr><th>TransUNet [14]</th><td>0.8123</td><td>0.8263</td><td>0.9577</td><td>0.9207</td><td>0.8499</td><td>0.8578</td><td>0.9653</td><td>0.9452</td><td>0.8840</td><td>0.9063</td><td>0.9427</td><td>0.9200</td></tr><tr><th>MCGU-Net [24]</th><td>0.8927</td><td>0.8502</td><td>0.9855</td><td>0.9570</td><td>0.895</td><td>0.848</td><td>0.986</td><td>0.955</td><td>0.9263</td><td>0.8322</td><td>0.9714</td><td>0.9537</td></tr><tr><th>MedT [25]</th><td>0.8037</td><td>0.8064</td><td>0.9546</td><td>0.9090</td><td>0.8389</td><td>0.8252</td><td>0.9637</td><td>0.9358</td><td>0.9122</td><td>0.8472</td><td>0.9657</td><td>0.9416</td></tr><tr><th>FAT-Net [26]</th><td>0.8500</td><td>0.8392</td><td>0.9725</td><td>0.9326</td><td>0.8903</td><td>0.9100</td><td>0.9699</td><td>0.9578</td><td>0.9440</td><td>0.9441</td><td>0.9741</td><td>0.9703</td></tr><tr><th>TMU-Net [18]</th><td>0.9164</td><td>0.9128</td><td>0.9789</td><td>0.9660</td><td>0.9059</td><td>0.9038</td><td>0.9746</td><td>0.9603</td><td>0.9414</td><td>0.9395</td><td>0.9756</td><td>0.9647</td></tr><tr><th>Swin\u2009U-Net [16]</th><td>0.9183</td><td>0.9142</td><td>0.9798</td><td>0.9701</td><td>0.8946</td><td>0.9056</td><td>0.9798</td><td>0.9645</td><td>0.9449</td><td>0.9410</td><td>0.9564</td><td>0.9678</td></tr><tr><th>TransNorm [15]</th><td>0.8933</td><td>0.8532</td><td>0.9859</td><td>0.9582</td><td>0.8951</td><td>0.8750</td><td>0.9790</td><td>0.9580</td><td>0.9437</td><td>0.9438</td><td>0.9810</td><td>0.9723</td></tr><tr><th>Proposed Method</th><td>0.9240</td><td>0.9246</td><td>0.9794</td><td>0.9656</td><td>0.9105</td><td>0.9089</td><td>0.9807</td><td>0.9668</td><td>0.9504</td><td>0.9439</td><td>0.9576</td><td>0.9685</td></tr></tbody></table>", "caption": "Table 1: Performance comparison of the proposed method against the SOTA approaches on skin lesion segmentation task.", "list_citation_info": ["[18] Azad Reza, Heidari Moein, Wu Yuli, and Merhof Dorit, \u201cContextual attention network: Transformer meets u-net,\u201d in MICCAI International Workshop on Machine Learning in Medical Imaging. Springer, 2022.", "[3] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234\u2013241.", "[26] Huisi Wu, Shihuai Chen, Guilian Chen, Wei Wang, Baiying Lei, and Zhenkun Wen, \u201cFat-net: Feature adaptive transformers for automated skin lesion segmentation,\u201d Medical Image Analysis, vol. 76, pp. 102327, 2022.", "[16] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang, \u201cSwin-unet: Unet-like pure transformer for medical image segmentation,\u201d arXiv preprint arXiv:2105.05537, 2021.", "[15] Reza Azad, Mohammad T AL-Antary, Moein Heidari, and Dorit Merhof, \u201cTransnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model,\u201d IEEE Access, 2022.", "[14] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou, \u201cTransunet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.", "[25] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel, \u201cMedical transformer: Gated axial-attention for medical image segmentation,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 36\u201346.", "[24] Maryam Asadi-Aghbolaghi, Reza Azad, Mahmood Fathy, and Sergio Escalera, \u201cMulti-level context gating of embedded collective knowledge for medical image segmentation,\u201d arXiv preprint arXiv:2003.05056, 2020.", "[13] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al., \u201cAttention u-net: Learning where to look for the pancreas,\u201d arXiv preprint arXiv:1804.03999, 2018."]}], "citation_info_to_title": {"[14] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou, \u201cTransunet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.": "Transunet: Transformers make strong encoders for medical image segmentation", "[13] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al., \u201cAttention u-net: Learning where to look for the pancreas,\u201d arXiv preprint arXiv:1804.03999, 2018.": "Attention U-Net: Learning Where to Look for the Pancreas", "[3] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234\u2013241.": "U-net: Convolutional networks for biomedical image segmentation", "[26] Huisi Wu, Shihuai Chen, Guilian Chen, Wei Wang, Baiying Lei, and Zhenkun Wen, \u201cFat-net: Feature adaptive transformers for automated skin lesion segmentation,\u201d Medical Image Analysis, vol. 76, pp. 102327, 2022.": "Fat-net: Feature Adaptive Transformers for Automated Skin Lesion Segmentation", "[25] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel, \u201cMedical transformer: Gated axial-attention for medical image segmentation,\u201d in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2021, pp. 36\u201346.": "Medical transformer: Gated axial-attention for medical image segmentation", "[24] Maryam Asadi-Aghbolaghi, Reza Azad, Mahmood Fathy, and Sergio Escalera, \u201cMulti-level context gating of embedded collective knowledge for medical image segmentation,\u201d arXiv preprint arXiv:2003.05056, 2020.": "Multi-level context gating of embedded collective knowledge for medical image segmentation", "[16] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang, \u201cSwin-unet: Unet-like pure transformer for medical image segmentation,\u201d arXiv preprint arXiv:2105.05537, 2021.": "Swin-unet: Unet-like pure transformer for medical image segmentation", "[18] Azad Reza, Heidari Moein, Wu Yuli, and Merhof Dorit, \u201cContextual attention network: Transformer meets u-net,\u201d in MICCAI International Workshop on Machine Learning in Medical Imaging. Springer, 2022.": "Contextual Attention Network: Transformer Meets U-Net", "[15] Reza Azad, Mohammad T AL-Antary, Moein Heidari, and Dorit Merhof, \u201cTransnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model,\u201d IEEE Access, 2022.": "Transnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model"}, "source_title_to_arxiv_id": {"Transunet: Transformers make strong encoders for medical image segmentation": "2102.04306", "Attention U-Net: Learning Where to Look for the Pancreas": "1804.03999", "Swin-unet: Unet-like pure transformer for medical image segmentation": "2105.05537", "Contextual Attention Network: Transformer Meets U-Net": "2203.01932", "Transnorm: Transformer provides a strong spatial normalization mechanism for a deep segmentation model": "2207.13415"}}