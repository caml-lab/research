{"title": "InstaFormer: Instance-Aware Image-to-Image Translation with Transformer", "abstract": "We present a novel Transformer-based network architecture for instance-aware\nimage-to-image translation, dubbed InstaFormer, to effectively integrate\nglobal- and instance-level information. By considering extracted content\nfeatures from an image as tokens, our networks discover global consensus of\ncontent features by considering context information through a self-attention\nmodule in Transformers. By augmenting such tokens with an instance-level\nfeature extracted from the content feature with respect to bounding box\ninformation, our framework is capable of learning an interaction between object\ninstances and the global image, thus boosting the instance-awareness. We\nreplace layer normalization (LayerNorm) in standard Transformers with adaptive\ninstance normalization (AdaIN) to enable a multi-modal translation with style\ncodes. In addition, to improve the instance-awareness and translation quality\nat object regions, we present an instance-level content contrastive loss\ndefined between input and translated image. We conduct experiments to\ndemonstrate the effectiveness of our InstaFormer over the latest methods and\nprovide extensive ablation studies.", "authors": ["Soohyun Kim", "Jongbeom Baek", "Jihye Park", "Gyeongnyeon Kim", "Seungryong Kim"], "published_date": "2022_03_30", "pdf_url": "http://arxiv.org/pdf/2203.16248v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"2\">CycleGAN [70]</td><td colspan=\"2\">UNIT [38]</td><td colspan=\"2\">MUNIT [23]</td><td colspan=\"2\">DRIT [35]</td><td colspan=\"2\">INIT [51]</td><td colspan=\"2\">DUNIT [3]</td><td colspan=\"2\">MGUIT [28]</td><td colspan=\"2\">InstaFormer</td></tr><tr><th></th><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td><td>CIS</td><td>IS</td></tr><tr><th>sunny\\rightarrownight</th><td>0.014</td><td>1.026</td><td>0.082</td><td>1.030</td><td>1.159</td><td>1.278</td><td>1.058</td><td>1.224</td><td>1.060</td><td>1.118</td><td>1.166</td><td>1.259</td><td>1.176</td><td>1.271</td><td>1.200</td><td>1.404</td></tr><tr><th>night\\rightarrowsunny</th><td>0.012</td><td>1.023</td><td>0.027</td><td>1.024</td><td>1.036</td><td>1.051</td><td>1.024</td><td>1.099</td><td>1.045</td><td>1.080</td><td>1.083</td><td>1.108</td><td>1.115</td><td>1.130</td><td>1.115</td><td>1.127</td></tr><tr><th>sunny\\rightarrowrainy</th><td>0.011</td><td>1.073</td><td>0.097</td><td>1.075</td><td>1.012</td><td>1.146</td><td>1.007</td><td>1.207</td><td>1.036</td><td>1.152</td><td>1.029</td><td>1.225</td><td>1.092</td><td>1.213</td><td>1.158</td><td>1.394</td></tr><tr><th>sunny\\rightarrowcloudy</th><td>0.014</td><td>1.097</td><td>0.081</td><td>1.134</td><td>1.008</td><td>1.095</td><td>1.025</td><td>1.104</td><td>1.040</td><td>1.142</td><td>1.033</td><td>1.149</td><td>1.052</td><td>1.218</td><td>1.130</td><td>1.257</td></tr><tr><th>cloudy\\rightarrowsunny</th><td>0.090</td><td>1.033</td><td>0.219</td><td>1.046</td><td>1.026</td><td>1.321</td><td>1.046</td><td>1.249</td><td>1.016</td><td>1.460</td><td>1.077</td><td>1.472</td><td>1.136</td><td>1.489</td><td>1.141</td><td>1.585</td></tr><tr><th>Average</th><td>0.025</td><td>1.057</td><td>0.087</td><td>1.055</td><td>1.032</td><td>1.166</td><td>1.031</td><td>1.164</td><td>1.043</td><td>1.179</td><td>1.079</td><td>1.223</td><td>1.112</td><td>1.254</td><td>1.149</td><td>1.353</td></tr></tbody></table>", "caption": "Table 1: Quantitative evaluation on INIT dataset [51].For evaluation, we perform bidirectional translation for each domain pair.We measure CIS [23] and IS [50](higher is better).Our results shows the best results in terms of CIS and IS.", "list_citation_info": ["[28] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsupervised image-to-image translation. In CVPR, pages 6558\u20136567, 2021.", "[3] Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based unsupervised image-to-image translation. In CVPR, pages 4787\u20134796, 2020.", "[50] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29:2234\u20132242, 2016.", "[23] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, pages 172\u2013189, 2018.", "[70] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, pages 2223\u20132232, 2017.", "[38] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In NeurIPS, pages 700\u2013708, 2017.", "[35] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In ECCV, pages 35\u201351, 2018.", "[51] Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, and Thomas S Huang. Towards instance-level image-to-image translation. In CVPR, pages 3683\u20133692, 2019."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Methods</th><th colspan=\"2\">sunny\\rightarrownight</th><th colspan=\"2\">night\\rightarrowsunny</th><th colspan=\"2\">Average</th></tr><tr><th>FID\\downarrow</th><th>SSIM\\uparrow</th><th>FID\\downarrow</th><th>SSIM\\uparrow</th><th>FID\\downarrow</th><th>SSIM\\uparrow</th></tr></thead><tbody><tr><th>CUT [45]</th><td>75.28</td><td>0.698</td><td>80.72</td><td>0.634</td><td>78.00</td><td>0.666</td></tr><tr><th>MUNIT [23]</th><td>100.32</td><td>0.703</td><td>98.04</td><td>0.631</td><td>99.18</td><td>0.680</td></tr><tr><th>DRIT [35]</th><td>79.59</td><td>0.312</td><td>99.33</td><td>0.266</td><td>89.46</td><td>0.289</td></tr><tr><th>MGUIT [28]</th><td>98.03</td><td>0.836</td><td>82.17</td><td>0.848</td><td>90.10</td><td>0.842</td></tr><tr><th>InstaFormer</th><td>84.72</td><td>0.872</td><td>71.65</td><td>0.818</td><td>79.05</td><td>0.845</td></tr></tbody></table>", "caption": "Table 2: Quantitative evaluation with FID [20] metric for data distribution and SSIM [58] index measured at each instance.", "list_citation_info": ["[28] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsupervised image-to-image translation. In CVPR, pages 6558\u20136567, 2021.", "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, pages 6626\u20136637, 2017.", "[23] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, pages 172\u2013189, 2018.", "[58] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600\u2013612, 2004.", "[45] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. arXiv preprint arXiv:2007.15651, 2020.", "[35] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In ECCV, pages 35\u201351, 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>Pers</th><th>Car</th><th>Truc.</th><th>Bic</th><th>mAP</th></tr></thead><tbody><tr><th>DT [26]</th><td>28.5</td><td>40.7</td><td>25.9</td><td>29.7</td><td>31.2</td></tr><tr><th>DAF [23]</th><td>39.2</td><td>40.2</td><td>25.7</td><td>48.9</td><td>38.5</td></tr><tr><th>DARL [34]</th><td>46.4</td><td>58.7</td><td>27.0</td><td>49.1</td><td>45.3</td></tr><tr><th>DAOD [49]</th><td>47.3</td><td>59.1</td><td>28.3</td><td>49.6</td><td>46.1</td></tr><tr><th>DUNIT [3]</th><td>60.7</td><td>65.1</td><td>32.7</td><td>57.7</td><td>54.1</td></tr><tr><th>MGUIT [28]</th><td>58.3</td><td>68.2</td><td>33.4</td><td>58.4</td><td>54.6</td></tr><tr><th>InstaFormer</th><td>61.8</td><td>69.5</td><td>35.3</td><td>55.3</td><td>55.5</td></tr></tbody></table>", "caption": "Table 3: Results for domain adaptive detection.We compare the per-class Average Precision for KITTI \\rightarrow CityScape.", "list_citation_info": ["[28] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsupervised image-to-image translation. In CVPR, pages 6558\u20136567, 2021.", "[3] Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based unsupervised image-to-image translation. In CVPR, pages 4787\u20134796, 2020.", "[49] Adrian Lopez Rodriguez and Krystian Mikolajczyk. Domain adaptation for object detection via style consistency. arXiv preprint arXiv:1911.10033, 2019.", "[26] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-domain weakly-supervised object detection through progressive domain adaptation. In CVPR, pages 5001\u20135009, 2018.", "[23] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, pages 172\u2013189, 2018.", "[34] Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim. Diversify and match: A domain adaptive representation learning paradigm for object detection. In CVPR, pages 12456\u201312465, 2019."]}, {"table": "<table><tbody><tr><td>Variants</td><td>#blocks</td><td>#heads</td><td>#params</td><td>FID\\downarrow</td><td>SSIM\\uparrow</td></tr><tr><td>Less blocks</td><td>3</td><td>4</td><td>37.776M</td><td>89.96</td><td>0.711</td></tr><tr><td>Ours</td><td>6</td><td>4</td><td>75.552M</td><td>84.72</td><td>0.872</td></tr><tr><td>More blocks</td><td>9</td><td>4</td><td>113.329M</td><td>85.28</td><td>0.879</td></tr><tr><td>Less heads</td><td>6</td><td>1</td><td>4.732M</td><td>89.17</td><td>0.738</td></tr><tr><td>Ours</td><td>6</td><td>4</td><td>75.552M</td><td>84.72</td><td>0.872</td></tr><tr><td>More heads</td><td>6</td><td>8</td><td>302.100M</td><td>81.92</td><td>0.873</td></tr><tr><td>CNN-based</td><td>6</td><td>-</td><td>7.081M</td><td>89.73</td><td>0.708</td></tr></tbody></table>", "caption": "Table 6: Effects of the number of blocks and heads in our InstaFormer architecture, providing quantitative evaluations with FID [20] and SSIM [58].The only setting that vary across model is the number of Transformer blocks or heads, and we keep the others constant for sunny\\rightarrownight on INIT dataset [51]. Larger models tend to have a higher parameter count, and better FID [20] and SSIM [58] metric scores.", "list_citation_info": ["[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, pages 6626\u20136637, 2017.", "[58] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600\u2013612, 2004.", "[51] Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, and Thomas S Huang. Towards instance-level image-to-image translation. In CVPR, pages 3683\u20133692, 2019."]}], "citation_info_to_title": {"[3] Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based unsupervised image-to-image translation. In CVPR, pages 4787\u20134796, 2020.": "Dunit: Detection-based unsupervised image-to-image translation", "[70] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, pages 2223\u20132232, 2017.": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "[38] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In NeurIPS, pages 700\u2013708, 2017.": "Unsupervised image-to-image translation networks", "[35] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In ECCV, pages 35\u201351, 2018.": "Diverse image-to-image translation via disentangled representations", "[28] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsupervised image-to-image translation. In CVPR, pages 6558\u20136567, 2021.": "Memory-guided unsupervised image-to-image translation", "[58] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600\u2013612, 2004.": "Image Quality Assessment: From Error Visibility to Structural Similarity", "[23] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, pages 172\u2013189, 2018.": "Multimodal unsupervised image-to-image translation", "[51] Zhiqiang Shen, Mingyang Huang, Jianping Shi, Xiangyang Xue, and Thomas S Huang. Towards instance-level image-to-image translation. In CVPR, pages 3683\u20133692, 2019.": "Towards instance-level image-to-image translation", "[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, pages 6626\u20136637, 2017.": "Gans trained by a two time-scale update rule converge to a local nash equilibrium", "[49] Adrian Lopez Rodriguez and Krystian Mikolajczyk. Domain adaptation for object detection via style consistency. arXiv preprint arXiv:1911.10033, 2019.": "Domain Adaptation for Object Detection via Style Consistency", "[26] Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-domain weakly-supervised object detection through progressive domain adaptation. In CVPR, pages 5001\u20135009, 2018.": "Cross-domain weakly-supervised object detection through progressive domain adaptation", "[45] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. arXiv preprint arXiv:2007.15651, 2020.": "Contrastive learning for unpaired image-to-image translation", "[50] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29:2234\u20132242, 2016.": "Improved Techniques for Training GANs", "[34] Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim. Diversify and match: A domain adaptive representation learning paradigm for object detection. In CVPR, pages 12456\u201312465, 2019.": "Diversify and match: A domain adaptive representation learning paradigm for object detection"}, "source_title_to_arxiv_id": {"Memory-guided unsupervised image-to-image translation": "2104.05170"}}