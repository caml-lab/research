{"title": "Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models", "abstract": "Generative models (e.g., GANs, diffusion models) learn the underlying data\ndistribution in an unsupervised manner. However, many applications of interest\nrequire sampling from a particular region of the output space or sampling\nevenly over a range of characteristics. For efficient sampling in these\nscenarios, we propose Generative Visual Prompt (PromptGen), a framework for\ndistributional control over pre-trained generative models by incorporating\nknowledge of other off-the-shelf models. PromptGen defines control as\nenergy-based models (EBMs) and samples images in a feed-forward manner by\napproximating the EBM with invertible neural networks, avoiding optimization at\ninference. Our experiments demonstrate how PromptGen can efficiently sample\nfrom several unconditional generative models (e.g., StyleGAN2, StyleNeRF,\ndiffusion autoencoder, NVAE) in a controlled or/and de-biased manner using\nvarious off-the-shelf models: (1) with the CLIP model as control, PromptGen can\nsample images guided by text, (2) with image classifiers as control, PromptGen\ncan de-bias generative models across a set of attributes or attribute\ncombinations, and (3) with inverse graphics models as control, PromptGen can\nsample images of the same identity in different poses. (4) Finally, PromptGen\nreveals that the CLIP model shows a \"reporting bias\" when used as control, and\nPromptGen can further de-bias this controlled distribution in an iterative\nmanner. The code is available at\nhttps://github.com/ChenWu98/Generative-Visual-Prompt.", "authors": ["Chen Henry Wu", "Saman Motamed", "Shaunak Srivastava", "Fernando De la Torre"], "published_date": "2022_09_14", "pdf_url": "http://arxiv.org/pdf/2209.06970v2", "list_table_and_caption": [{"table": "<p>StyleFlow Abdal2021StyleFlowAE   PPGM Nguyen2017PlugP  /GuidedPromptGenLACE nie2021controllable DDPM dhariwal2021diffusion Arbitrary control (e.g., CLIP)\u2717\u2713\u2713\u2713Low-dimensional latent space\u2713\u2713\u2717\u2713Stands alone at inference\u2713\u2717\u2717\u2713Feed-forward (i.e., no inference-time optim.)\u2713\u2717\u2717\u2713Iterative distributional control\u2717\u2717\u2717\u2713</p>", "caption": "Table 1: Comparison between methods. ", "list_citation_info": ["(52) Weili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with latent-space energy-based models. NeurIPS, 2021.", "(1) Rameen Abdal, Peihao Zhu, Niloy Jyoti Mitra, and Peter Wonka. StyleFlow: Attribute-conditioned exploration of StyleGAN-generated images using conditional continuous normalizing flows. TOG, 2021.", "(10) Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. NeurIPS, 2021.", "(49) Anh M Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play generative networks: Conditional iterative generation of images in latent space. CVPR, 2017."]}, {"table": "<p>FFHQ (binary attributes)\\mathbb{D}_{\\text{KL}}^{\\text{gender}}\\downarrow\\mathbb{D}_{\\text{KL}}^{\\text{eyeglasses}}\\downarrow\\mathbb{D}_{\\text{KL}}^{\\text{blond hair}}\\downarrow\\mathbb{D}_{\\text{KL}}^{\\text{age}+\\text{gender}}\\downarrow\\mathbb{D}_{\\text{KL}}^{\\text{age}+\\text{eyeglasses}}\\downarrow\\mathbb{D}_{\\text{KL}}^{\\text{gender}+\\text{eyeglasses}}\\downarrowFFHQ (real data)0.0150.186\u2013  \u20030.246  \u20030.355  \u20030.242StyleGAN2 Karras2020AnalyzingAI 0.0180.180\u2013  \u20030.279  \u20030.384  \u20030.250StyleFlow Abdal2021StyleFlowAE 0.0230.061\u2013  \u20030.214  \u20030.162  \u20030.121FairGen Tan2020ImprovingTF 4.21 \\times 10^{-4}7.07 \\times 10^{-4}\u2013\u20030.0373\u20030.0330  0.00185FairStyle Karakas2022FairStyleDS 3.20 \\times 10^{-7}0\u2013\u20030.0257\u20030.01570.000241PromptGen (ours)1.71 \\times 10^{-5}1.72 \\times 10^{-5}0.00080.0005580.0004150.000628</p>", "caption": "Table 2: Comparison with baselines for de-biasing binary attributes and their correlations. Following Karakas2022FairStyleDS , we use classifiers on CelebA Liu2015DeepLF . Baseline performances are copied from Karakas2022FairStyleDS . PromptGen has competitive performance, even in the cases where FairStyle achieves nearly perfect performance. ", "list_citation_info": ["(31) Cemre Karakas, Alara Dirik, Eyl\u00fcl Yal\u00e7\u0131nkaya, and Pinar Yanardag. FairStyle: Debiasing StyleGAN2 with style channel manipulations. ArXiv, 2022.", "(1) Rameen Abdal, Peihao Zhu, Niloy Jyoti Mitra, and Peter Wonka. StyleFlow: Attribute-conditioned exploration of StyleGAN-generated images using conditional continuous normalizing flows. TOG, 2021.", "(47) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. ICCV, 2015.", "(72) Shuhan Tan, Yujun Shen, and Bolei Zhou. Improving the fairness of deep generative models without retraining. ArXiv, 2020.", "(36) Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. CVPR, 2020."]}, {"table": "<p>StyleGAN2GRAFpi-GANGIRAFFEStyleNeRFDFGGAN-ControlPromptGenResolution1024{}^{2}256{}^{2}256{}^{2}256{}^{2}1024{}^{2}256{}^{2}512{}^{2}1024{}^{2}Pose\u2717\u2713\u2713\u2713\u2713\u2713\u2713\u2713FID\\downarrow371853581364Dist. w/ same ID\\downarrow\u2013\u2013\u2013\u2013\u20130.830.680.45Dist. w/ diff. ID\u2013\u2013\u2013\u2013\u20131.731.901.37</p>", "caption": "Table 3: Pose-controlled face generation. In this experiment, PromptGen uses StyleGAN2 as the pre-trained generative model. Results for GRAF SchwarzLN020 , pi-GAN Chan2021piGANPI , GIRAFFE Niemeyer2021GIRAFFERS , and StyleNeRF gu2022stylenerf  are from gu2022stylenerf ; results for DiscoFaceGAN (DFG) Deng2020DisentangledAC  and GAN-Control Shoshan2021GANControlEC  are from Shoshan2021GANControlEC .", "list_citation_info": ["(66) Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. GRAF: Generative radiance fields for 3D-aware image synthesis. NeurIPS, 2020.", "(53) Michael Niemeyer and Andreas Geiger. GIRAFFE: Representing scenes as compositional generative neural feature fields. CVPR, 2021.", "(69) Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, and G\u00e9rard Medioni. GAN-Control: Explicitly controllable GANs. ICCV, 2021.", "(21) Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. StyleNeRF: A style-based 3D aware generator for high-resolution image synthesis. ICLR, 2022.", "(9) Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. Disentangled and controllable face image generation via 3D imitative-contrastive learning. CVPR, 2020.", "(5) Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-GAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis. CVPR, 2021."]}], "citation_info_to_title": {"(36) Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. CVPR, 2020.": "Analyzing and improving the image quality of StyleGAN", "(66) Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. GRAF: Generative radiance fields for 3D-aware image synthesis. NeurIPS, 2020.": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis", "(10) Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. NeurIPS, 2021.": "Diffusion models beat GANs on image synthesis", "(31) Cemre Karakas, Alara Dirik, Eyl\u00fcl Yal\u00e7\u0131nkaya, and Pinar Yanardag. FairStyle: Debiasing StyleGAN2 with style channel manipulations. ArXiv, 2022.": "FairStyle: Debiasing StyleGAN2 with style channel manipulations", "(72) Shuhan Tan, Yujun Shen, and Bolei Zhou. Improving the fairness of deep generative models without retraining. ArXiv, 2020.": "Improving the fairness of deep generative models without retraining", "(69) Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, and G\u00e9rard Medioni. GAN-Control: Explicitly controllable GANs. ICCV, 2021.": "GAN-Control: Explicitly controllable GANs", "(1) Rameen Abdal, Peihao Zhu, Niloy Jyoti Mitra, and Peter Wonka. StyleFlow: Attribute-conditioned exploration of StyleGAN-generated images using conditional continuous normalizing flows. TOG, 2021.": "StyleFlow: Attribute-conditioned exploration of StyleGAN-generated images using conditional continuous normalizing flows", "(5) Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-GAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis. CVPR, 2021.": "pi-GAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis", "(9) Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. Disentangled and controllable face image generation via 3D imitative-contrastive learning. CVPR, 2020.": "Disentangled and Controllable Face Image Generation via 3D Imitative-Contrastive Learning", "(47) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. ICCV, 2015.": "Deep learning face attributes in the wild", "(49) Anh M Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play generative networks: Conditional iterative generation of images in latent space. CVPR, 2017.": "Plug & play generative networks: Conditional iterative generation of images in latent space", "(53) Michael Niemeyer and Andreas Geiger. GIRAFFE: Representing scenes as compositional generative neural feature fields. CVPR, 2021.": "GIRAFFE: Representing scenes as compositional generative neural feature fields", "(52) Weili Nie, Arash Vahdat, and Anima Anandkumar. Controllable and compositional generation with latent-space energy-based models. NeurIPS, 2021.": "Controllable and Compositional Generation with Latent-Space Energy-Based Models", "(21) Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. StyleNeRF: A style-based 3D aware generator for high-resolution image synthesis. ICLR, 2022.": "StyleNeRF: A style-based 3D aware generator for high-resolution image synthesis"}, "source_title_to_arxiv_id": {"StyleFlow: Attribute-conditioned exploration of StyleGAN-generated images using conditional continuous normalizing flows": "2008.02401"}}