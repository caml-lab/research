{"title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "abstract": "Vision transformer (ViT) recently has drawn great attention in computer\nvision due to its remarkable model capability. However, most prevailing ViT\nmodels suffer from huge number of parameters, restricting their applicability\non devices with limited resources. To alleviate this issue, we propose TinyViT,\na new family of tiny and efficient small vision transformers pretrained on\nlarge-scale datasets with our proposed fast distillation framework. The central\nidea is to transfer knowledge from large pretrained models to small ones, while\nenabling small models to get the dividends of massive pretraining data. More\nspecifically, we apply distillation during pretraining for knowledge transfer.\nThe logits of large teacher models are sparsified and stored in disk in advance\nto save the memory cost and computation overheads. The tiny student\ntransformers are automatically scaled down from a large pretrained model with\ncomputation and parameter constraints. Comprehensive experiments demonstrate\nthe efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k\nwith only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k\nwhile using 4.2 times fewer parameters. Moreover, increasing image resolutions,\nTinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using\nonly 11% parameters. Last but not the least, we demonstrate a good transfer\nability of TinyViT on various downstream tasks. Code and models are available\nat https://github.com/microsoft/Cream/tree/main/TinyViT.", "authors": ["Kan Wu", "Jinnian Zhang", "Houwen Peng", "Mengchen Liu", "Bin Xiao", "Jianlong Fu", "Lu Yuan"], "published_date": "2022_07_21", "pdf_url": "http://arxiv.org/pdf/2207.10666v1", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\"> #</td><td rowspan=\"2\">Model</td><td>Pretraining</td><td>IN-1k</td><td>IN-Real [5]</td><td>IN-V2 [54]</td></tr><tr><td>Dataset</td><td>Top-1(%)</td><td>Top-1(%)</td><td>Top-1(%)</td></tr><tr><td> 0</td><td rowspan=\"4\">Swin-T [43]</td><td>Train from scratch on IN-1k</td><td>81.2</td><td>86.7</td><td>69.7</td></tr><tr><td>1</td><td>Original IN-21k</td><td>81.9(+0.7)</td><td>87.0(+0.3)</td><td>70.6(+0.9)</td></tr><tr><td>2</td><td>Cleaned IN-21k</td><td>82.2(+1.0)</td><td>87.3(+0.6)</td><td>71.1(+1.4)</td></tr><tr><td>3</td><td>Original IN-21k w/ distillation</td><td>83.4(+2.2)</td><td>88.0(+1.3)</td><td>72.6(+2.9)</td></tr><tr><td>4</td><td rowspan=\"4\">TinyViT-21M (ours)</td><td>Train from scratch on IN-1k</td><td>83.1</td><td>88.1</td><td>73.1</td></tr><tr><td>5</td><td>Original IN-21k</td><td>83.8(+0.7)</td><td>88.4(+0.3)</td><td>73.8(+0.7)</td></tr><tr><td>6</td><td>Cleaned IN-21k</td><td>84.2(+1.1)</td><td>88.5(+0.4)</td><td>73.8(+0.7)</td></tr><tr><td>7</td><td>Original IN-21k w/ distillation</td><td>84.8(+1.7)</td><td>88.9(+0.8)</td><td>75.1(+2.0)</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Impact of hard samples. Models are pretrained on IN-21k and then finetuned on IN-1k.", "list_citation_info": ["[54] Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classifiers generalize to imagenet? In: ICML (2019)", "[43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)", "[5] Beyer, L., H\u00e9naff, O.J., Kolesnikov, A., Zhai, X., Oord, A.v.d.: Are we done with imagenet? arXiv (2020)"]}, {"table": "<table><tr><td rowspan=\"2\"> Model</td><td>#Params</td><td>Train on</td><td colspan=\"2\">Pretrain on IN-21k</td></tr><tr><td>(M)</td><td>IN-1k</td><td>w/o distill.</td><td>w/ distill.</td></tr><tr><td> DeiT-Ti [64]</td><td>5</td><td>72.2</td><td>73.0(+0.8)</td><td>74.4(+2.2)</td></tr><tr><td>DeiT-S [64]</td><td>22</td><td>79.9</td><td>80.5(+0.6)</td><td>82.0(+2.1)</td></tr><tr><td>Swin-T [43]</td><td>28</td><td>81.2</td><td>81.9(+0.7)</td><td>83.4(+2.2)</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 2: Ablation study on different pretraining strategies for Swin [43] and DeiT [64]. The performance on IN-1k is reported.", "list_citation_info": ["[64] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. PMLR (2021)", "[43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)"]}, {"table": "<table><tr><td rowspan=\"2\"> #</td><td>IN-21k</td><td>IN-1k</td><td>IN-Real</td><td>IN-V2</td><td>Training Time</td><td>Memory</td></tr><tr><td>Pretrained Teacher</td><td>Top-1(%)</td><td>Top-1(%)</td><td>Top-1(%)</td><td>(GPU Hours)</td><td>(GB)</td></tr><tr><td> 0</td><td>w/o distill.</td><td>83.8</td><td>88.4</td><td>73.8</td><td>3,360</td><td>0</td></tr><tr><td>1</td><td>BEiT-L (326M, 84.1) [4]</td><td>84.1</td><td>88.4</td><td>73.8</td><td>6,415 (1.9\\times)</td><td>3.9</td></tr><tr><td>2</td><td>Swin-L (229M, 84.4) [43]</td><td>84.2</td><td>88.6</td><td>73.9</td><td>5,804 (1.7\\times)</td><td>6.8</td></tr><tr><td>3</td><td>CLIP-ViT-L/14 (321M, 85.2) [52]</td><td>84.8</td><td>88.9</td><td>75.1</td><td>7,087 (2.1\\times)</td><td>2.7</td></tr><tr><td>4</td><td>Florence (682M, 86.2) [75]</td><td>84.8</td><td>89.0</td><td>74.8</td><td>7,942 (2.4\\times)</td><td>10.7</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 3:  Ablation study on different teacher models for pretraining distillation.Teacher performance are listed in the brackets: (the number of parameters, linear probe performance on IN-1k). We report the training time cost and memory consumption of teacher models on NVIDIA V100 GPUs without using our proposed fast pretraining distillation.", "list_citation_info": ["[4] Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. In: ICLR (2022)", "[75] Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., Zhang, P.: Florence: A new foundation model for computer vision. In: ArXiv (2021)", "[52] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)", "[43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)"]}, {"table": "<table><tr><td></td><td rowspan=\"2\">Model</td><td>Top-1</td><td>Top-5</td><td>#Params</td><td>MACs</td><td>Throughput</td><td rowspan=\"2\">Input</td><td rowspan=\"2\">Arch.</td></tr><tr><td></td><td>(%)</td><td>(%)</td><td>(M)</td><td>(G)</td><td>(images/s)</td></tr><tr><td rowspan=\"7\">5-10M #Params</td><td>MoblieViT-S [46]</td><td>78.4</td><td>-</td><td>6</td><td>1.8</td><td>2,661</td><td>256</td><td>Hybrid</td></tr><tr><td>ViTAS-DeiT-A [60]</td><td>75.5</td><td>92.4</td><td>6</td><td>1.3</td><td>3,504</td><td>224</td><td>Trans</td></tr><tr><td>GLiT-Tiny [9]</td><td>76.3</td><td>-</td><td>7</td><td>1.5</td><td>3,262</td><td>224</td><td>Trans</td></tr><tr><td>Mobile-Former-214M [14]</td><td>76.7</td><td>-</td><td>9</td><td>0.2</td><td>3,105</td><td>224</td><td>Hybrid</td></tr><tr><td>CrossViT-9 [10]</td><td>77.1</td><td>-</td><td>9</td><td>2.0</td><td>2,659</td><td>224</td><td>Trans</td></tr><tr><td>TinyViT-5M (ours)</td><td>79.1</td><td>94.8</td><td>5.4</td><td>1.3</td><td>3,060</td><td>224</td><td>Hybrid</td></tr><tr><td>TinyViT-5M<img/> (ours)</td><td>80.7</td><td>95.6</td><td>5.4</td><td>1.3</td><td>3,060</td><td>224</td><td>Hybrid</td></tr><tr><td rowspan=\"7\">11-20M</td><td>ResNet-18 [28]</td><td>70.3</td><td>86.7</td><td>12</td><td>1.8</td><td>8,714</td><td>224</td><td>CNN</td></tr><tr><td>PVT-Tiny [66]</td><td>75.1</td><td>-</td><td>13</td><td>1.9</td><td>2,791</td><td>224</td><td>Trans</td></tr><tr><td>ResT-Small [81]</td><td>79.6</td><td>94.9</td><td>14</td><td>2.1</td><td>2,037</td><td>224</td><td>Trans</td></tr><tr><td>LeViT-256 [24]</td><td>81.6</td><td>-</td><td>19</td><td>1.1</td><td>7,386</td><td>224</td><td>Hybrid</td></tr><tr><td>CoaT-Lite Small [71]</td><td>81.9</td><td>95.6</td><td>20</td><td>4.0</td><td>1,138</td><td>224</td><td>Trans</td></tr><tr><td>TinyViT-11M (ours)</td><td>81.5</td><td>95.8</td><td>11</td><td>2.0</td><td>2,468</td><td>224</td><td>Hybrid</td></tr><tr><td>TinyViT-11M<img/> (ours)</td><td>83.2</td><td>96.5</td><td>11</td><td>2.0</td><td>2,468</td><td>224</td><td>Hybrid</td></tr><tr><td rowspan=\"7\">&gt;20M</td><td>DeiT-S [64]</td><td>79.9</td><td>95.0</td><td>22</td><td>4.6</td><td>2,276</td><td>224</td><td>Trans</td></tr><tr><td>T2T-ViT-14 [74]</td><td>81.5</td><td>95.7</td><td>21</td><td>4.8</td><td>1,557</td><td>224</td><td>Trans</td></tr><tr><td>AutoFormer-S [11]</td><td>81.7</td><td>95.7</td><td>23</td><td>5.1</td><td>1,341</td><td>224</td><td>Trans</td></tr><tr><td>Swin-T [43]</td><td>81.2</td><td>95.5</td><td>28</td><td>4.5</td><td>1,393</td><td>224</td><td>Trans</td></tr><tr><td>CrossViT-15 [10]</td><td>82.3</td><td>-</td><td>28</td><td>6.1</td><td>1,306</td><td>224</td><td>Trans</td></tr><tr><td>EffNet-B5 [62]</td><td>83.6</td><td>96.7</td><td>30</td><td>9.9</td><td>330</td><td>456</td><td>CNN</td></tr><tr><td>TinyViT-21M (ours)</td><td>83.1</td><td>96.5</td><td>21</td><td>4.3</td><td>1,571</td><td>224</td><td>Hybrid</td></tr><tr><td></td><td>TinyViT-21M<img/> (ours)</td><td>84.8</td><td>97.3</td><td>21</td><td>4.3</td><td>1,571</td><td>224</td><td>Hybrid</td></tr><tr><td></td><td>TinyViT-21M<img/> \\uparrow384 (ours)</td><td>86.2</td><td>97.8</td><td>21</td><td>13.8</td><td>394</td><td>384</td><td>Hybrid</td></tr><tr><td></td><td>TinyViT-21M<img/> \\uparrow512 (ours)</td><td>86.5</td><td>97.9</td><td>21</td><td>27.0</td><td>167</td><td>512</td><td>Hybrid</td></tr></table>", "caption": "Table 4: TinyViT performance on IN-1k [18] with comparisons to state-of-the-art models. MACs (multiply\u2013accumulate operations) and Throughput are measured using the GitHub repository of [1, 24] and a V100 GPU. <img/>: pretrain on IN-21k with the proposed fast distillation; \\uparrow: finetune with higher resolution.", "list_citation_info": ["[18] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR (2009)", "[1] fvcore library. https://github.com/facebookresearch/fvcore/", "[66] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV (2021)", "[62] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: ICML (2019)", "[74] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token vit: Training vision transformers from scratch on imagenet. ICCV (2021)", "[46] Mehta, S., Rastegari, M.: Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. In: ICLR (2021)", "[28] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)", "[10] Chen, C.F., Fan, Q., Panda, R.: Crossvit: Cross-attention multi-scale vision transformer for image classification. ICCV (2021)", "[9] Chen, B., Li, P., Li, C., Li, B., Bai, L., Lin, C., Sun, M., Yan, J., Ouyang, W.: Glit: Neural architecture search for global and local image transformer. In: ICCV (2021)", "[64] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. PMLR (2021)", "[43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)", "[24] Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J\u00e9gou, H., Douze, M.: Levit: a vision transformer in convnet\u2019s clothing for faster inference. In: ICCV (2021)", "[11] Chen, M., Peng, H., Fu, J., Ling, H.: Autoformer: Searching transformers for visual recognition. In: ICCV (2021)", "[14] Chen, Y., Dai, X., Chen, D., Liu, M., Dong, X., Yuan, L., Liu, Z.: Mobile-former: Bridging mobilenet and transformer. In: CVPR (2022)", "[71] Xu, W., Xu, Y., Chang, T., Tu, Z.: Co-scale conv-attentional image transformers. In: ICCV (2021)", "[60] Su, X., You, S., Xie, J., Zheng, M., Wang, F., Qian, C., Zhang, C., Wang, X., Xu, C.: Vitas: Vision transformer architecture search. arXiv (2021)", "[81] Zhang, Q., bin Yang, Y.: Rest: An efficient transformer for visual recognition. In: NeurIPS (2021)"]}, {"table": "<table><tr><td> #</td><td>Backbone</td><td>#Params</td><td>IN-1k</td><td>AP</td><td>AP_{50}</td><td>AP_{75}</td><td>AP_{S}</td><td>AP_{M}</td><td>AP_{L}</td></tr><tr><td> 0</td><td>Swin-T [43]</td><td>28M</td><td>81.2</td><td>48.1</td><td>67.1</td><td>52.1</td><td>31.1</td><td>51.2</td><td>63.5</td></tr><tr><td>1</td><td>TinyViT-21M</td><td>21M</td><td>83.1</td><td>49.6 (+1.5)</td><td>68.5</td><td>54.2</td><td>32.3</td><td>53.2</td><td>64.8</td></tr><tr><td>2</td><td>TinyViT-21M<img/></td><td>21M</td><td>84.8</td><td>50.2 (+2.1)</td><td>69.4</td><td>54.4</td><td>32.9</td><td>53.9</td><td>65.2</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 6: Comparison on COCO [41] objectdetection using Cascade Mask R-CNN [7, 27] for 12 epochs.We report the number of parameters of the backbone.", "list_citation_info": ["[41] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)", "[43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)", "[7] Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: CVPR (2018)"]}, {"table": "<table><tr><td> </td><td>Block</td><td>Configuration</td><td>Output</td></tr><tr><td> Patch Embed</td><td>Stacked Conv</td><td>\\begin{bmatrix}\\text{kernel size }~{}3\\times 3,\\\\\\text{stride 2, padding 1}\\end{bmatrix}\\times 2</td><td>56\\times 56</td></tr><tr><td>Stage 1</td><td>MBConv [33]</td><td>\\begin{bmatrix}\\text{embed dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{1}}},\\\\\\text{expansion ratio }{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}\\gamma_{R}}\\end{bmatrix}\\times{\\color[rgb]{0.6015625,0.19921875,0.3984375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.19921875,0.3984375}\\pgfsys@color@rgb@stroke{0.6015625}{0.19921875}{0.3984375}\\pgfsys@color@rgb@fill{0.6015625}{0.19921875}{0.3984375}\\gamma_{N_{1}}}</td><td>56\\times 56</td></tr><tr><td>Downsampling</td><td>MBConv [33]</td><td>\\begin{bmatrix}\\text{embed dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{1}}},\\text{stride }2,\\\\\\text{hidden/output dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{2}}}\\end{bmatrix}\\times 1</td><td>28\\times 28</td></tr><tr><td>Stage 2</td><td>Transformer [65]</td><td>\\begin{bmatrix}&amp;\\text{embed dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{2}}},~{}\\text{head }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{2}}}/{\\color[rgb]{0.234375,0.46875,0.84765625}\\definecolor[named]{pgfstrokecolor}{rgb}{0.234375,0.46875,0.84765625}\\pgfsys@color@rgb@stroke{0.234375}{0.46875}{0.84765625}\\pgfsys@color@rgb@fill{0.234375}{0.46875}{0.84765625}\\gamma_{E}},&amp;\\\\&amp;\\text{window size }{\\color[rgb]{1,0.19921875,0.80078125}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.19921875,0.80078125}\\pgfsys@color@rgb@stroke{1}{0.19921875}{0.80078125}\\pgfsys@color@rgb@fill{1}{0.19921875}{0.80078125}\\gamma_{W_{2}}}\\times{\\color[rgb]{1,0.19921875,0.80078125}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.19921875,0.80078125}\\pgfsys@color@rgb@stroke{1}{0.19921875}{0.80078125}\\pgfsys@color@rgb@fill{1}{0.19921875}{0.80078125}\\gamma_{W_{2}}},&amp;\\\\&amp;\\text{mlp ratio }{\\color[rgb]{1,0.3984375,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.3984375,0}\\pgfsys@color@rgb@stroke{1}{0.3984375}{0}\\pgfsys@color@rgb@fill{1}{0.3984375}{0}\\gamma_{M}}&amp;\\end{bmatrix}\\times{\\color[rgb]{0.6015625,0.19921875,0.3984375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.19921875,0.3984375}\\pgfsys@color@rgb@stroke{0.6015625}{0.19921875}{0.3984375}\\pgfsys@color@rgb@fill{0.6015625}{0.19921875}{0.3984375}\\gamma_{N_{2}}}</td><td>28\\times 28</td></tr><tr><td>Downsampling</td><td>MBConv [33]</td><td>\\begin{bmatrix}\\text{embed dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{2}}},\\text{stride }2,\\\\\\text{hidden/output dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{3}}}\\end{bmatrix}\\times 1</td><td>14\\times 14</td></tr><tr><td>Stage 3</td><td>Transformer [65]</td><td>\\begin{bmatrix}&amp;\\text{embed dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{3}}},~{}\\text{head }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{3}}}/{\\color[rgb]{0.234375,0.46875,0.84765625}\\definecolor[named]{pgfstrokecolor}{rgb}{0.234375,0.46875,0.84765625}\\pgfsys@color@rgb@stroke{0.234375}{0.46875}{0.84765625}\\pgfsys@color@rgb@fill{0.234375}{0.46875}{0.84765625}\\gamma_{E}},&amp;\\\\&amp;\\text{window size }{\\color[rgb]{1,0.19921875,0.80078125}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.19921875,0.80078125}\\pgfsys@color@rgb@stroke{1}{0.19921875}{0.80078125}\\pgfsys@color@rgb@fill{1}{0.19921875}{0.80078125}\\gamma_{W_{3}}}\\times{\\color[rgb]{1,0.19921875,0.80078125}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.19921875,0.80078125}\\pgfsys@color@rgb@stroke{1}{0.19921875}{0.80078125}\\pgfsys@color@rgb@fill{1}{0.19921875}{0.80078125}\\gamma_{W_{3}}},&amp;\\\\&amp;\\text{mlp ratio }{\\color[rgb]{1,0.3984375,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.3984375,0}\\pgfsys@color@rgb@stroke{1}{0.3984375}{0}\\pgfsys@color@rgb@fill{1}{0.3984375}{0}\\gamma_{M}}&amp;\\end{bmatrix}\\times{\\color[rgb]{0.6015625,0.19921875,0.3984375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.19921875,0.3984375}\\pgfsys@color@rgb@stroke{0.6015625}{0.19921875}{0.3984375}\\pgfsys@color@rgb@fill{0.6015625}{0.19921875}{0.3984375}\\gamma_{N_{3}}}</td><td>14\\times 14</td></tr><tr><td>Downsampling</td><td>MBConv [33]</td><td>\\begin{bmatrix}\\text{embed dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{3}}},\\text{stride }2,\\\\\\text{hidden/output dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{4}}}\\end{bmatrix}\\times 1</td><td>7\\times 7</td></tr><tr><td>Stage 4</td><td>Transformer [65]</td><td>\\begin{bmatrix}&amp;\\text{embed dim }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{4}}},~{}\\text{head }{\\color[rgb]{0.3125,0.78515625,0.70703125}\\definecolor[named]{pgfstrokecolor}{rgb}{0.3125,0.78515625,0.70703125}\\pgfsys@color@rgb@stroke{0.3125}{0.78515625}{0.70703125}\\pgfsys@color@rgb@fill{0.3125}{0.78515625}{0.70703125}\\gamma_{D_{4}}}/{\\color[rgb]{0.234375,0.46875,0.84765625}\\definecolor[named]{pgfstrokecolor}{rgb}{0.234375,0.46875,0.84765625}\\pgfsys@color@rgb@stroke{0.234375}{0.46875}{0.84765625}\\pgfsys@color@rgb@fill{0.234375}{0.46875}{0.84765625}\\gamma_{E}},&amp;\\\\&amp;\\text{window size }{\\color[rgb]{1,0.19921875,0.80078125}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.19921875,0.80078125}\\pgfsys@color@rgb@stroke{1}{0.19921875}{0.80078125}\\pgfsys@color@rgb@fill{1}{0.19921875}{0.80078125}\\gamma_{W_{4}}}\\times{\\color[rgb]{1,0.19921875,0.80078125}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.19921875,0.80078125}\\pgfsys@color@rgb@stroke{1}{0.19921875}{0.80078125}\\pgfsys@color@rgb@fill{1}{0.19921875}{0.80078125}\\gamma_{W_{4}}},&amp;\\\\&amp;\\text{mlp ratio }{\\color[rgb]{1,0.3984375,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0.3984375,0}\\pgfsys@color@rgb@stroke{1}{0.3984375}{0}\\pgfsys@color@rgb@fill{1}{0.3984375}{0}\\gamma_{M}}&amp;\\end{bmatrix}\\times{\\color[rgb]{0.6015625,0.19921875,0.3984375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.6015625,0.19921875,0.3984375}\\pgfsys@color@rgb@stroke{0.6015625}{0.19921875}{0.3984375}\\pgfsys@color@rgb@fill{0.6015625}{0.19921875}{0.3984375}\\gamma_{N_{4}}}</td><td>7\\times 7</td></tr><tr><td>Classifier</td><td>AvgPool+LayerNorm+Linear</td><td>\\begin{bmatrix}\\text{output dim: the number of classes}\\end{bmatrix}</td><td></td></tr><tr><td> </td><td></td><td></td><td></td></tr></table>", "caption": "Table 7: An elastic base architecture of TinyViT.", "list_citation_info": ["[33] Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: ICCV (2019)", "[65] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)"]}], "citation_info_to_title": {"[81] Zhang, Q., bin Yang, Y.: Rest: An efficient transformer for visual recognition. In: NeurIPS (2021)": "Rest: An efficient transformer for visual recognition", "[10] Chen, C.F., Fan, Q., Panda, R.: Crossvit: Cross-attention multi-scale vision transformer for image classification. ICCV (2021)": "Crossvit: Cross-attention multi-scale vision transformer for image classification", "[41] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014)": "Microsoft COCO: Common Objects in Context", "[33] Howard, A., Sandler, M., Chu, G., Chen, L.C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al.: Searching for mobilenetv3. In: ICCV (2019)": "Searching for MobileNetV3", "[43] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)": "Swin transformer: Hierarchical vision transformer using shifted windows", "[18] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR (2009)": "Imagenet: A large-scale hierarchical image database", "[7] Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: CVPR (2018)": "Cascade R-CNN: Delving into High Quality Object Detection", "[65] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, \u0141., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)": "Attention is all you need", "[11] Chen, M., Peng, H., Fu, J., Ling, H.: Autoformer: Searching transformers for visual recognition. In: ICCV (2021)": "Autoformer: Searching Transformers for Visual Recognition", "[9] Chen, B., Li, P., Li, C., Li, B., Bai, L., Lin, C., Sun, M., Yan, J., Ouyang, W.: Glit: Neural architecture search for global and local image transformer. In: ICCV (2021)": "Glit: Neural Architecture Search for Global and Local Image Transformer", "[14] Chen, Y., Dai, X., Chen, D., Liu, M., Dong, X., Yuan, L., Liu, Z.: Mobile-former: Bridging mobilenet and transformer. In: CVPR (2022)": "Mobile-former: Bridging MobileNet and Transformer", "[60] Su, X., You, S., Xie, J., Zheng, M., Wang, F., Qian, C., Zhang, C., Wang, X., Xu, C.: Vitas: Vision transformer architecture search. arXiv (2021)": "Vitas: Vision transformer architecture search", "[46] Mehta, S., Rastegari, M.: Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. In: ICLR (2021)": "Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer", "[4] Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. In: ICLR (2022)": "Beit: Bert pre-training of image transformers", "[66] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: ICCV (2021)": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "[74] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token vit: Training vision transformers from scratch on imagenet. ICCV (2021)": "Tokens-to-token vit: Training vision transformers from scratch on imagenet", "[54] Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classifiers generalize to imagenet? In: ICML (2019)": "Do imagenet classifiers generalize to imagenet?", "[75] Yuan, L., Chen, D., Chen, Y.L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., Liu, C., Liu, M., Liu, Z., Lu, Y., Shi, Y., Wang, L., Wang, J., Xiao, B., Xiao, Z., Yang, J., Zeng, M., Zhou, L., Zhang, P.: Florence: A new foundation model for computer vision. In: ArXiv (2021)": "Florence: A new foundation model for computer vision", "[52] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)": "Learning transferable visual models from natural language supervision", "[5] Beyer, L., H\u00e9naff, O.J., Kolesnikov, A., Zhai, X., Oord, A.v.d.: Are we done with imagenet? arXiv (2020)": "Are we done with imagenet?", "[62] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: ICML (2019)": "Efficientnet: Rethinking Model Scaling for Convolutional Neural Networks", "[24] Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J\u00e9gou, H., Douze, M.: Levit: a vision transformer in convnet\u2019s clothing for faster inference. In: ICCV (2021)": "Levit: A Vision Transformer in ConvNets Clothing for Faster Inference", "[28] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)": "Deep Residual Learning for Image Recognition", "[71] Xu, W., Xu, Y., Chang, T., Tu, Z.: Co-scale conv-attentional image transformers. In: ICCV (2021)": "Co-scale conv-attentional image transformers", "[64] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. PMLR (2021)": "Training data-efficient image transformers & distillation through attention", "[1] fvcore library. https://github.com/facebookresearch/fvcore/": "fvcore library"}, "source_title_to_arxiv_id": {"Rest: An efficient transformer for visual recognition": "2105.13677", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Glit: Neural Architecture Search for Global and Local Image Transformer": "2107.02960", "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions": "2102.12122"}}