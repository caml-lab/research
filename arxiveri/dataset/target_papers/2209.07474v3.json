{"title": "On the Surprising Effectiveness of Transformers in Low-Labeled Video Recognition", "abstract": "Recently vision transformers have been shown to be competitive with\nconvolution-based methods (CNNs) broadly across multiple vision tasks. The less\nrestrictive inductive bias of transformers endows greater representational\ncapacity in comparison with CNNs. However, in the image classification setting\nthis flexibility comes with a trade-off with respect to sample efficiency,\nwhere transformers require ImageNet-scale training. This notion has carried\nover to video where transformers have not yet been explored for video\nclassification in the low-labeled or semi-supervised settings. Our work\nempirically explores the low data regime for video classification and discovers\nthat, surprisingly, transformers perform extremely well in the low-labeled\nvideo setting compared to CNNs. We specifically evaluate video vision\ntransformers across two contrasting video datasets (Kinetics-400 and\nSomethingSomething-V2) and perform thorough analysis and ablation studies to\nexplain this observation using the predominant features of video transformer\narchitectures. We even show that using just the labeled data, transformers\nsignificantly outperform complex semi-supervised CNN methods that leverage\nlarge-scale unlabeled data as well. Our experiments inform our recommendation\nthat semi-supervised learning video work should consider the use of video\ntransformers in the future.", "authors": ["Farrukh Rahman", "\u00d6mer Mubarek", "Zsolt Kira"], "published_date": "2022_09_15", "pdf_url": "http://arxiv.org/pdf/2209.07474v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Architecture</td><td colspan=\"4\">Top-1 Accuracy</td><td># Params (M)</td><td>Flops (G)</td></tr><tr><td></td><td>1%</td><td>5%</td><td>10%</td><td>100%</td><td></td><td></td></tr><tr><td>I3D-R50 [38]</td><td>13.73(-3.79)</td><td>33.49</td><td>44.37(-4.73)</td><td>73.5<sup>\u2021</sup> (-2.5)</td><td>28.1</td><td>28.5</td></tr><tr><td>I3D-NL-R50 [47]</td><td>13.86 (-2.14)</td><td>34.18 (-0.69)</td><td>44.7 (-4.4)</td><td>74<sup>\u2021</sup> (-2)</td><td>35.4</td><td>36.7</td></tr><tr><td>C2D-R50 [47]</td><td>18.13 (-1.66)</td><td>35.1 (-3.13)</td><td>45.77 (-3.33)</td><td>71.61<sup>\u2021</sup> (-4.39)</td><td>24.3</td><td>19.6</td></tr><tr><td>C2D-NL-R50 [47]</td><td>19.79</td><td>38.23</td><td>46.93 ( (-2.71)</td><td>-</td><td>31.7</td><td>27.8</td></tr><tr><td>R3D-50 from [64]</td><td>16<sup>\u2020</sup> (-2.13)</td><td>-</td><td>49.1<sup>\u2020</sup></td><td>76.0<sup>\u2020</sup></td><td>-</td><td>-</td></tr><tr><td>ViViT FE [14]</td><td>24.91 (+5.12)</td><td>41.98 (+3.75)</td><td>48.84 (-0.26)</td><td>78.4<sup>\u2020</sup> (+2.4)</td><td>115.1</td><td>284</td></tr><tr><td>ViViT ST [14]</td><td>23.99 (+4.2)</td><td>40.47 (2.24)</td><td>46.49 (-2.61)</td><td>79.9<sup>\u2020</sup> (+3.9)</td><td>88.9</td><td>455.2</td></tr><tr><td>Uniformer Small [13]</td><td>22.52 (+2.73)</td><td>47.71 (+9.48)</td><td>56.01 (+6.91)</td><td>80.8<sup>\u2020</sup> (+4.8)</td><td>21.4</td><td>167.2</td></tr><tr><td>MViTv2 Small [13]</td><td>22.17 (+2.38)</td><td>47.59 (+9.36)</td><td>56.96 (+7.86)</td><td>81.0 <sup>\u2020</sup> (+5)</td><td>34.5</td><td>64.5</td></tr><tr><td>Video Swin Small [12]</td><td>32.3 (+12.56)</td><td>52.37 (+14.14)</td><td>60.66 (+11.56)</td><td>80.6<sup>\u2020</sup> (+4.6)</td><td>49.8</td><td>166</td></tr></tbody></table>", "caption": "Table 1: Top 1% accuracy results on K400 for various low-labeled settings for CNN-based (top half) vs Transformer-based (bottom half).Differences are computed against the highest performing CNN-based model for each split in bold.Refer to supplemental for top-5 acc results. <sup>\u2020</sup>Result from respectively cited work. <sup>\u2021</sup>Result from [65].", "list_citation_info": ["[64] R. Qian, T. Meng, B. Gong, M.-H. Yang, H. Wang, S. Belongie, and Y. Cui, \u201cSpatiotemporal contrastive video representation learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6964\u20136974, 2021.", "[47] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794\u20137803, 2018.", "[12] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \u201cVideo swin transformer,\u201d arXiv preprint arXiv:2106.13230, 2021.", "[65] H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer, \u201cPyslowfast.\u201d https://github.com/facebookresearch/slowfast, 2020.", "[38] J. Carreira and A. Zisserman, \u201cQuo vadis, action recognition? a new model and the kinetics dataset,\u201d in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299\u20136308, 2017.", "[14] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid, \u201cVivit: A video vision transformer,\u201d in International Conference on Computer Vision (ICCV), 2021.", "[13] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, \u201cUniformer: Unified transformer for efficient spatiotemporal representation learning,\u201d arXiv preprint arXiv:2201.04676, 2022."]}, {"table": "<table><tbody><tr><td>Architecture</td><td colspan=\"4\">Top-1 Accuracy</td></tr><tr><td></td><td>1%</td><td>5%</td><td>10%</td><td>100%</td></tr><tr><td>I3D-R50 [38]</td><td>8.47 (-1.59)</td><td>24.3 (-2.5)</td><td>32.92(-3.25)</td><td>-</td></tr><tr><td>I3D-NL-R50 [47]</td><td>8.42(-1.64)</td><td>24.85(-1.95)</td><td>32.59(-3.58)</td><td>-</td></tr><tr><td>C2D-R50 [47]</td><td>7.16 (-2.9)</td><td>-</td><td>20.16 (-16.01)</td><td>-</td></tr><tr><td>C2D-NL-R50 [47]</td><td>7.31 (-2.75)</td><td>-</td><td>21.19 (-16.04)</td><td>-</td></tr><tr><td>SlowFast-R50 [41]</td><td>10.06</td><td>26.8</td><td>36.17</td><td>63<sup>\u2021</sup></td></tr><tr><td>Video Swin Base [12]</td><td>14.3 (+4.11)</td><td>30.0 (+3.2)</td><td>37.5(+1.33)</td><td>65.67 (+2.67)</td></tr><tr><td>Uniformer Small [13]</td><td>12.77 (+2.9)</td><td>36.14 (+9.34)</td><td>44.81 (+8.64)</td><td>63.5<sup>\u2020</sup>(+0.5)</td></tr><tr><td>MViTv2 Small [15]</td><td>16.88 (+6.82)</td><td>35.88 (+9.08)</td><td>41.94 (+5.77)</td><td>68.2<sup>\u2020</sup>(+5.2)</td></tr></tbody></table>", "caption": "Table 2: Top 1% accuracy on SSv2 for various low-labeled settings. CNN-based (top half) vs. Transformer-based (bottom half). Differences are computed against baseline CNN-based model in bold. Refer to supplemental for full results. <sup>\u2020</sup>Result obtained from respectively cited work. <sup>\u2021</sup>Result obtained from [65].", "list_citation_info": ["[47] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794\u20137803, 2018.", "[41] C. Feichtenhofer, H. Fan, J. Malik, and K. He, \u201cSlowfast networks for video recognition,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6202\u20136211, 2019.", "[12] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \u201cVideo swin transformer,\u201d arXiv preprint arXiv:2106.13230, 2021.", "[65] H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer, \u201cPyslowfast.\u201d https://github.com/facebookresearch/slowfast, 2020.", "[38] J. Carreira and A. Zisserman, \u201cQuo vadis, action recognition? a new model and the kinetics dataset,\u201d in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299\u20136308, 2017.", "[15] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer, \u201cImproved multiscale vision transformers for classification and detection,\u201d arXiv preprint arXiv:2112.01526, 2021.", "[13] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, \u201cUniformer: Unified transformer for efficient spatiotemporal representation learning,\u201d arXiv preprint arXiv:2201.04676, 2022."]}, {"table": "<table><tbody><tr><th>Architecture</th><td colspan=\"2\">Top-1 Accuracy</td></tr><tr><th></th><td>1%</td><td>10%</td></tr><tr><th>R3D-50 in [64]</th><td>3.2<sup>\u2020</sup></td><td>39.6<sup>\u2020</sup></td></tr><tr><th>Slow-R50 [41]</th><td>4.81</td><td>31.68</td></tr><tr><th>MViTv2 [15] Small</th><td>3.1</td><td>33.68</td></tr><tr><th>Uniformer [13]</th><td>4.9</td><td>27.17</td></tr><tr><th>VST Small [12]</th><td>2.29</td><td>14.11</td></tr></tbody></table>", "caption": "Table 4: CNN vs Transformer performance on K400 without IN1k pretraining. Absent any spatial pretraining CNNs are still more sample efficient on video compared to video transformers.", "list_citation_info": ["[64] R. Qian, T. Meng, B. Gong, M.-H. Yang, H. Wang, S. Belongie, and Y. Cui, \u201cSpatiotemporal contrastive video representation learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6964\u20136974, 2021.", "[41] C. Feichtenhofer, H. Fan, J. Malik, and K. He, \u201cSlowfast networks for video recognition,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6202\u20136211, 2019.", "[12] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \u201cVideo swin transformer,\u201d arXiv preprint arXiv:2106.13230, 2021.", "[15] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer, \u201cImproved multiscale vision transformers for classification and detection,\u201d arXiv preprint arXiv:2112.01526, 2021.", "[13] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, \u201cUniformer: Unified transformer for efficient spatiotemporal representation learning,\u201d arXiv preprint arXiv:2201.04676, 2022."]}, {"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td colspan=\"4\">Top-1 Accuracy</td></tr><tr><td></td><td></td><td>1%</td><td>5%</td><td>10%</td><td>100%</td></tr><tr><td>CMPL [67]</td><td>R3D-50</td><td>17.6<sup>\u2020</sup></td><td>-</td><td>58.4<sup>\u2020</sup></td><td>-</td></tr><tr><td>MVPL [18]</td><td>R3D-50</td><td>17.0<sup>\u2020</sup></td><td>-</td><td>58.2<sup>\u2020</sup></td><td>-</td></tr><tr><td>MVPL from [19]</td><td>R3D-18</td><td>9.8<sup>\u2020</sup></td><td>-</td><td>43.8<sup>\u2020</sup></td><td>-</td></tr><tr><td>ActorCutMix [68]</td><td>TSM-R3D-18</td><td>9.24<sup>\u2020</sup></td><td>-</td><td>-</td><td></td></tr><tr><td>LfTG [19]</td><td>R3D-18</td><td>9.8<sup>\u2020</sup></td><td>-</td><td>43.8<sup>\u2020</sup></td><td>-</td></tr><tr><td>TCL + ActorCutMix [68]</td><td>TSM-R3D-18</td><td>9.02<sup>\u2020</sup></td><td>31.45<sup>\u2020</sup></td><td>-</td><td>-</td></tr><tr><td>TCL [69]</td><td>TSM-R3D-18 [40]</td><td>7.69<sup>\u2020</sup></td><td>30.28<sup>\u2020</sup></td><td>-</td><td>-</td></tr><tr><td>TCL w/ Fine tune [69]</td><td>TSM-R3D-18 [40]</td><td>8.45<sup>\u2020</sup></td><td>31.5<sup>\u2020</sup></td><td>-</td><td>-</td></tr><tr><td>TCL w/ Pretrain &amp; Finetune [69]</td><td>TSM-R3D-18 [40]</td><td>11.56<sup>\u2020</sup></td><td>31.91<sup>\u2020</sup></td><td>-</td><td>-</td></tr><tr><td>Supervised</td><td>I3D-R50 [38]</td><td>13.73</td><td>33.49</td><td>44.37</td><td>73.5<sup>\u2021</sup></td></tr><tr><td>Supervised</td><td>I3D-NL-R50 [47]</td><td>13.86</td><td>34.18</td><td>44.7</td><td>74<sup>\u2021</sup></td></tr><tr><td>Supervised</td><td>C2D-R50 [47]</td><td>18.13</td><td>35.1</td><td>45.77</td><td>71.61<sup>\u2021</sup></td></tr><tr><td>Supervised</td><td>C2D-NL-R50 [47]</td><td>19.79</td><td>38.23</td><td>46.93</td><td>-</td></tr><tr><td>Supervised</td><td>R3D-50 from [64]</td><td>16<sup>\u2020</sup></td><td>-</td><td>49.1<sup>\u2020</sup></td><td>76.0<sup>\u2020</sup></td></tr><tr><td>Supervised</td><td>ViViT FE [14]</td><td>24.91</td><td>41.98</td><td>48.84</td><td>78.4<sup>\u2020</sup></td></tr><tr><td>Supervised</td><td>ViViT ST [14]</td><td>23.99</td><td>40.47</td><td>46.49</td><td>79.9<sup>\u2020</sup></td></tr><tr><td>Supervised</td><td>Uniformer Small [13]</td><td>22.52</td><td>47.71</td><td>56.01</td><td>80.8<sup>\u2020</sup></td></tr><tr><td>Supervised</td><td>MViTv2 Small [13]</td><td>22.17</td><td>47.59</td><td>56.96</td><td>81.0<sup>\u2020</sup></td></tr><tr><td>Supervised</td><td>Video Swin Small [12]</td><td>32.3</td><td>52.37</td><td>60.66</td><td>80.6<sup>\u2020</sup></td></tr></tbody></table>", "caption": "Table 6: Top 1% accuracy results on K400 for various low-labeled settings. Supervised indicates backbone was trained on just available labeled data.Swin Transformer using no video unlabeled data outperforms CNN based methods using labeled and unlabeled video data. Video Swin especially has significant performance gains on K400.<sup>\u2020</sup>Result from respectively cited work. <sup>\u2021</sup>Result from [65].", "list_citation_info": ["[67] Y. Xu, F. Wei, X. Sun, C. Yang, Y. Shen, B. Dai, B. Zhou, and S. Lin, \u201cCross-model pseudo-labeling for semi-supervised action recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2959\u20132968, 2022.", "[64] R. Qian, T. Meng, B. Gong, M.-H. Yang, H. Wang, S. Belongie, and Y. Cui, \u201cSpatiotemporal contrastive video representation learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6964\u20136974, 2021.", "[69] A. Singh, O. Chakraborty, A. Varshney, R. Panda, R. Feris, K. Saenko, A. Das, I. Madras, and I. Kharagpur, \u201cSemi-supervised action recognition with temporal contrastive learning,\u201d 2021.", "[47] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794\u20137803, 2018.", "[12] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \u201cVideo swin transformer,\u201d arXiv preprint arXiv:2106.13230, 2021.", "[19] J. Xiao, L. Jing, L. Zhang, J. He, Q. She, Z. Zhou, A. Yuille, and Y. Li, \u201cLearning from temporal gradient for semi-supervised action recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3252\u20133262, 2022.", "[65] H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer, \u201cPyslowfast.\u201d https://github.com/facebookresearch/slowfast, 2020.", "[68] Y. Zou, J. Choi, Q. Wang, and J.-B. Huang, \u201cLearning representational invariances for data-efficient action recognition,\u201d arXiv preprint arXiv:2103.16565, 2021.", "[18] B. Xiong, H. Fan, K. Grauman, and C. Feichtenhofer, \u201cMultiview pseudo-labeling for semi-supervised learning from video,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7209\u20137219, 2021.", "[40] J. Lin, C. Gan, and S. Han, \u201cTsm: Temporal shift module for efficient video understanding,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7083\u20137093, 2019.", "[38] J. Carreira and A. Zisserman, \u201cQuo vadis, action recognition? a new model and the kinetics dataset,\u201d in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299\u20136308, 2017.", "[14] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid, \u201cVivit: A video vision transformer,\u201d in International Conference on Computer Vision (ICCV), 2021.", "[13] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, \u201cUniformer: Unified transformer for efficient spatiotemporal representation learning,\u201d arXiv preprint arXiv:2201.04676, 2022."]}, {"table": "<table><tbody><tr><td>Architecture</td><td colspan=\"4\">Top-1 Accuracy</td><td colspan=\"4\">Top-5 Accuracy</td></tr><tr><td></td><td>1%</td><td>5%</td><td>10%</td><td>100%</td><td>1%</td><td>5%</td><td>10%</td><td>100%</td></tr><tr><td>I3D-R50 [38]</td><td>13.73(-3.79)</td><td>33.49</td><td>44.37(-4.73)</td><td>73.5<sup>\u2021</sup> (-2.5)</td><td>29.39</td><td>57</td><td>68.59</td><td>90.8<sup>\u2021</sup></td></tr><tr><td>I3D-NL-R50 [47]</td><td>13.86 (-2.14)</td><td>34.18 (-0.69)</td><td>44.7 (-4.4)</td><td>74<sup>\u2021</sup> (-2)</td><td>30.29</td><td>57.48</td><td>68.95</td><td>91.1<sup>\u2021</sup></td></tr><tr><td>C2D-R50 [47]</td><td>18.13 (-1.66)</td><td>35.1 (-3.13)</td><td>45.77 (-3.33)</td><td>71.61<sup>\u2021</sup> (-4.39)</td><td>36.34</td><td>58.91</td><td>69.39</td><td>87.8<sup>\u2021</sup></td></tr><tr><td>C2D-NL-R50 [47]</td><td>19.79</td><td>38.23</td><td>46.93 ( (-2.71)</td><td>-</td><td>39.35</td><td>61.96</td><td>70.88</td><td>-</td></tr><tr><td>R3D-50 from [64]</td><td>16<sup>\u2020</sup> (-2.13)</td><td>-</td><td>49.1<sup>\u2020</sup></td><td>76.0<sup>\u2020</sup></td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SLOWFAST-R3D-50 [41]</td><td>6.09 (-13.7)</td><td>-</td><td>36.07(-13.03)</td><td>-</td><td>15.86</td><td>-</td><td>59.7</td><td>-</td></tr><tr><td>ViViT FE [14]</td><td>24.91 (+5.12)</td><td>41.98 (+3.75)</td><td>48.84 (-0.26)</td><td>78.4<sup>\u2020</sup> (+2.4)</td><td>46.43</td><td>65.55</td><td>72.15</td><td>-</td></tr><tr><td>ViViT ST [14]</td><td>23.99 (+4.2)</td><td>40.47 (2.24)</td><td>46.49 (-2.61)</td><td>79.9<sup>\u2020</sup> (+3.9)</td><td>46.25</td><td>64.45</td><td>70.56</td><td>-</td></tr><tr><td>Uniformer Small [13]</td><td>22.52 (+2.73)</td><td>47.71 (+9.48)</td><td>56.01 (+6.91)</td><td>80.8<sup>\u2020</sup> (+4.8)</td><td>43.59</td><td>71.06</td><td>77.99</td><td>94.7<sup>\u2020</sup></td></tr><tr><td>MViTv2 Small [13]</td><td>22.17 (+2.38)</td><td>47.59 (+9.36)</td><td>56.96 (+7.86)</td><td>81.0 <sup>\u2020</sup> (+5)</td><td>43.17</td><td>70.94</td><td>79.57</td><td>94.6<sup>\u2020</sup></td></tr><tr><td>Video Swin Small [12]</td><td>32.3 (+12.56)</td><td>52.37 (+14.14)</td><td>60.66 (+11.56)</td><td>80.6<sup>\u2020</sup> (+4.6)</td><td>56.09</td><td>75.64</td><td>80.06</td><td>94.5<sup>\u2020</sup></td></tr></tbody></table>", "caption": "Table A1: Top 1% &amp; Top 5% accuracy results on K400 for various low-labeled settings for CNN-based (top half) vs Transformer-based (bottom half).Differences (for top-1) are computed against the highest performing CNN-based model for each split in bold.<sup>\u2020</sup>Result from respectively cited work. <sup>\u2021</sup>Result from [65].", "list_citation_info": ["[64] R. Qian, T. Meng, B. Gong, M.-H. Yang, H. Wang, S. Belongie, and Y. Cui, \u201cSpatiotemporal contrastive video representation learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6964\u20136974, 2021.", "[47] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794\u20137803, 2018.", "[41] C. Feichtenhofer, H. Fan, J. Malik, and K. He, \u201cSlowfast networks for video recognition,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6202\u20136211, 2019.", "[12] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \u201cVideo swin transformer,\u201d arXiv preprint arXiv:2106.13230, 2021.", "[65] H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer, \u201cPyslowfast.\u201d https://github.com/facebookresearch/slowfast, 2020.", "[38] J. Carreira and A. Zisserman, \u201cQuo vadis, action recognition? a new model and the kinetics dataset,\u201d in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299\u20136308, 2017.", "[14] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid, \u201cVivit: A video vision transformer,\u201d in International Conference on Computer Vision (ICCV), 2021.", "[13] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, \u201cUniformer: Unified transformer for efficient spatiotemporal representation learning,\u201d arXiv preprint arXiv:2201.04676, 2022."]}, {"table": "<table><tbody><tr><td>Architecture</td><td colspan=\"4\">Top-1 Accuracy</td><td colspan=\"4\">Top-5 Accuracy</td></tr><tr><td></td><td>1%</td><td>5%</td><td>10%</td><td>100%</td><td>1%</td><td>5%</td><td>10%</td><td>100%</td></tr><tr><td>I3D-R50 [38]</td><td>8.47 (-1.59)</td><td>24.3 (-2.5)</td><td>32.92(-3.25)</td><td>-</td><td>24.19</td><td>51.52</td><td>61.81</td><td>-</td></tr><tr><td>I3D-NL-R50 [47]</td><td>8.42(-1.64)</td><td>24.85(-1.95)</td><td>32.59(-3.58)</td><td>-</td><td>23.22</td><td>50.92</td><td>62.11</td><td>-</td></tr><tr><td>C2D-R50 [47]</td><td>7.16 (-2.9)</td><td>-</td><td>20.16 (-16.01)</td><td>-</td><td>20.16</td><td>-</td><td>47.05</td><td>-</td></tr><tr><td>C2D-NL-R50 [47]</td><td>7.31 (-2.75)</td><td>-</td><td>21.19 (-16.04)</td><td>-</td><td>20.62</td><td>-</td><td>49.3</td><td>-</td></tr><tr><td>SlowFast-R50 [41]</td><td>10.06</td><td>26.8</td><td>36.17</td><td>63<sup>\u2020</sup></td><td>25.49</td><td>53.88</td><td>64.18</td><td>88.5<sup>\u2021</sup></td></tr><tr><td>Video Swin Base [12]</td><td>14.3 (+4.11)</td><td>30.0 (+3.2)</td><td>37.5(+1.33)</td><td>65.67 (+2.67)</td><td>36.24</td><td>62.52</td><td>71.22</td><td>90.65</td></tr><tr><td>Uniformer Small [13]</td><td>12.77 (+2.9)</td><td>36.14 (+9.34)</td><td>44.81 (+8.64)</td><td>63.5<sup>\u2020</sup>(+0.5)</td><td>31.97</td><td>65.36</td><td>74.5</td><td>85.75</td></tr></tbody></table>", "caption": "Table A2: Top 1% accuracy on SSv2 for various low-labeled settings. CNN-based (top half) vs. Transformer-based (bottom half). Differences (for top-1) are computed against baseline CNN-based model in bold. <sup>\u2020</sup>Result obtained from respectively cited work. <sup>\u2021</sup>Result obtained from [65].", "list_citation_info": ["[47] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794\u20137803, 2018.", "[41] C. Feichtenhofer, H. Fan, J. Malik, and K. He, \u201cSlowfast networks for video recognition,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6202\u20136211, 2019.", "[12] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \u201cVideo swin transformer,\u201d arXiv preprint arXiv:2106.13230, 2021.", "[65] H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer, \u201cPyslowfast.\u201d https://github.com/facebookresearch/slowfast, 2020.", "[38] J. Carreira and A. Zisserman, \u201cQuo vadis, action recognition? a new model and the kinetics dataset,\u201d in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299\u20136308, 2017.", "[13] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, \u201cUniformer: Unified transformer for efficient spatiotemporal representation learning,\u201d arXiv preprint arXiv:2201.04676, 2022."]}], "citation_info_to_title": {"[67] Y. Xu, F. Wei, X. Sun, C. Yang, Y. Shen, B. Dai, B. Zhou, and S. Lin, \u201cCross-model pseudo-labeling for semi-supervised action recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2959\u20132968, 2022.": "Cross-model pseudo-labeling for semi-supervised action recognition", "[12] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \u201cVideo swin transformer,\u201d arXiv preprint arXiv:2106.13230, 2021.": "Video Swin Transformer", "[65] H. Fan, Y. Li, B. Xiong, W.-Y. Lo, and C. Feichtenhofer, \u201cPyslowfast.\u201d https://github.com/facebookresearch/slowfast, 2020.": "Pyslowfast", "[64] R. Qian, T. Meng, B. Gong, M.-H. Yang, H. Wang, S. Belongie, and Y. Cui, \u201cSpatiotemporal contrastive video representation learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6964\u20136974, 2021.": "Spatiotemporal contrastive video representation learning", "[13] K. Li, Y. Wang, P. Gao, G. Song, Y. Liu, H. Li, and Y. Qiao, \u201cUniformer: Unified transformer for efficient spatiotemporal representation learning,\u201d arXiv preprint arXiv:2201.04676, 2022.": "Uniformer: Unified transformer for efficient spatiotemporal representation learning", "[41] C. Feichtenhofer, H. Fan, J. Malik, and K. He, \u201cSlowfast networks for video recognition,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 6202\u20136211, 2019.": "Slowfast networks for video recognition", "[69] A. Singh, O. Chakraborty, A. Varshney, R. Panda, R. Feris, K. Saenko, A. Das, I. Madras, and I. Kharagpur, \u201cSemi-supervised action recognition with temporal contrastive learning,\u201d 2021.": "Semi-supervised action recognition with temporal contrastive learning", "[40] J. Lin, C. Gan, and S. Han, \u201cTsm: Temporal shift module for efficient video understanding,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7083\u20137093, 2019.": "Tsm: Temporal shift module for efficient video understanding", "[47] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7794\u20137803, 2018.": "Non-local neural networks", "[38] J. Carreira and A. Zisserman, \u201cQuo vadis, action recognition? a new model and the kinetics dataset,\u201d in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299\u20136308, 2017.": "Quo vadis, action recognition? a new model and the kinetics dataset", "[68] Y. Zou, J. Choi, Q. Wang, and J.-B. Huang, \u201cLearning representational invariances for data-efficient action recognition,\u201d arXiv preprint arXiv:2103.16565, 2021.": "Learning Representational Invariances for Data-Efficient Action Recognition", "[15] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer, \u201cImproved multiscale vision transformers for classification and detection,\u201d arXiv preprint arXiv:2112.01526, 2021.": "Improved Multiscale Vision Transformers for Classification and Detection", "[14] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid, \u201cVivit: A video vision transformer,\u201d in International Conference on Computer Vision (ICCV), 2021.": "Vivit: A video vision transformer", "[19] J. Xiao, L. Jing, L. Zhang, J. He, Q. She, Z. Zhou, A. Yuille, and Y. Li, \u201cLearning from temporal gradient for semi-supervised action recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3252\u20133262, 2022.": "Learning from temporal gradient for semi-supervised action recognition", "[18] B. Xiong, H. Fan, K. Grauman, and C. Feichtenhofer, \u201cMultiview pseudo-labeling for semi-supervised learning from video,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7209\u20137219, 2021.": "Multiview pseudo-labeling for semi-supervised learning from video"}, "source_title_to_arxiv_id": {"Video Swin Transformer": "2106.13230"}}