{"title": "Uncertainty-aware Contrastive Distillation for Incremental Semantic Segmentation", "abstract": "A fundamental and challenging problem in deep learning is catastrophic\nforgetting, i.e. the tendency of neural networks to fail to preserve the\nknowledge acquired from old tasks when learning new tasks. This problem has\nbeen widely investigated in the research community and several Incremental\nLearning (IL) approaches have been proposed in the past years. While earlier\nworks in computer vision have mostly focused on image classification and object\ndetection, more recently some IL approaches for semantic segmentation have been\nintroduced. These previous works showed that, despite its simplicity, knowledge\ndistillation can be effectively employed to alleviate catastrophic forgetting.\nIn this paper, we follow this research direction and, inspired by recent\nliterature on contrastive learning, we propose a novel distillation framework,\nUncertainty-aware Contrastive Distillation (\\method). In a nutshell, \\method~is\noperated by introducing a novel distillation loss that takes into account all\nthe images in a mini-batch, enforcing similarity between features associated to\nall the pixels from the same classes, and pulling apart those corresponding to\npixels from different classes. In order to mitigate catastrophic forgetting, we\ncontrast features of the new model with features extracted by a frozen model\nlearned at the previous incremental step. Our experimental results demonstrate\nthe advantage of the proposed distillation technique, which can be used in\nsynergy with previous IL approaches, and leads to state-of-art performance on\nthree commonly adopted benchmarks for incremental semantic segmentation. The\ncode is available at \\url{https://github.com/ygjwd12345/UCD}.", "authors": ["Guanglei Yang", "Enrico Fini", "Dan Xu", "Paolo Rota", "Mingli Ding", "Moin Nabi", "Xavier Alameda-Pineda", "Elisa Ricci"], "published_date": "2022_03_26", "pdf_url": "http://arxiv.org/pdf/2203.14098v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><td colspan=\"6\">19-1</td><td colspan=\"6\">15-5</td><td colspan=\"6\">15-1</td></tr><tr><td colspan=\"3\">Disjoint</td><td colspan=\"3\">Overlapped</td><td colspan=\"3\">Disjoint</td><td colspan=\"3\">Overlapped</td><td colspan=\"3\">Disjoint</td><td colspan=\"3\">Overlapped</td></tr><tr><td>1-19</td><td>20</td><td>all</td><td>1-19</td><td>20</td><td>all</td><td>1-15</td><td>16-20</td><td>all</td><td>1-15</td><td>16-20</td><td>all</td><td>1-15</td><td>16-20</td><td>all</td><td>1-15</td><td>16-20</td><td>all</td></tr><tr><th>FT</th><td>5.8</td><td>12.3</td><td>6.2</td><td>6.8</td><td>12.9</td><td>7.1</td><td>1.1</td><td>33.6</td><td>9.2</td><td>2.1</td><td>33.1</td><td>9.8</td><td>0.2</td><td>1.8</td><td>0.6</td><td>0.2</td><td>1.8</td><td>0.6</td></tr><tr><th>PI[62]</th><td>5.4</td><td>14.1</td><td>5.9</td><td>7.5</td><td>14.0</td><td>7.8</td><td>1.3</td><td>34.1</td><td>9.5</td><td>1.6</td><td>33.3</td><td>9.5</td><td>0.0</td><td>1.8</td><td>0.4</td><td>0.0</td><td>1.8</td><td>0.4</td></tr><tr><th>EWC[15]</th><td>23.2</td><td>16.0</td><td>22.9</td><td>26.9</td><td>14.0</td><td>26.3</td><td>26.7</td><td>37.7</td><td>29.4</td><td>24.3</td><td>35.5</td><td>27.1</td><td>0.3</td><td>4.3</td><td>1.3</td><td>0.3</td><td>4.3</td><td>1.3</td></tr><tr><th>RW[63]</th><td>19.4</td><td>15.7</td><td>19.2</td><td>23.3</td><td>14.2</td><td>22.9</td><td>17.9</td><td>36.9</td><td>22.7</td><td>16.6</td><td>34.9</td><td>21.2</td><td>0.2</td><td>5.4</td><td>1.5</td><td>0.0</td><td>5.2</td><td>1.3</td></tr><tr><th>LwF[14]</th><td>53.0</td><td>9.1</td><td>50.8</td><td>51.2</td><td>8.5</td><td>49.1</td><td>58.4</td><td>37.4</td><td>53.1</td><td>58.9</td><td>36.6</td><td>53.3</td><td>0.8</td><td>3.6</td><td>1.5</td><td>1.0</td><td>3.9</td><td>1.8</td></tr><tr><th>LwF-MC[16]</th><td>63.0</td><td>13.2</td><td>60.5</td><td>64.4</td><td>13.3</td><td>61.9</td><td>67.2</td><td>41.2</td><td>60.7</td><td>58.1</td><td>35.0</td><td>52.3</td><td>4.5</td><td>7.0</td><td>5.2</td><td>6.4</td><td>8.4</td><td>6.9</td></tr><tr><th>ILT[19]</th><td>69.1</td><td>16.4</td><td>66.4</td><td>67.1</td><td>12.3</td><td>64.4</td><td>63.2</td><td>39.5</td><td>57.3</td><td>66.3</td><td>40.6</td><td>59.9</td><td>3.7</td><td>5.7</td><td>4.2</td><td>4.9</td><td>7.8</td><td>5.7</td></tr><tr><th>ILT{}^{\\dagger}[50]</th><td>69.4</td><td>16.5</td><td>66.7</td><td>67.4</td><td>12.4</td><td>64.7</td><td>63.3</td><td>39.6</td><td>57.4</td><td>66.4</td><td>40.8</td><td>60.0</td><td>4.4</td><td>6.4</td><td>4.9</td><td>5.5</td><td>8.0</td><td>6.1</td></tr><tr><th>SDR[51]</th><td>69.9</td><td>37.3</td><td>68.4</td><td>69.1</td><td>32.6</td><td>67.4</td><td>73.5</td><td>47.3</td><td>67.2</td><td>75.4</td><td>52.6</td><td>69.9</td><td>59.2</td><td>12.9</td><td>48.1</td><td>44.7</td><td>21.8</td><td>39.2</td></tr><tr><th>RECALL{}^{\\sharp}[41]</th><td>65.2</td><td>50.1</td><td>65.8</td><td>67.9</td><td>53.5</td><td>68.4</td><td>66.3</td><td>49.8</td><td>63.5</td><td>66.6</td><td>50.9</td><td>64.0</td><td>66.0</td><td>44.9</td><td>62.1</td><td>65.7</td><td>47.8</td><td>62.7</td></tr><tr><th>UCD</th><td>73.4</td><td>33.7</td><td>71.5</td><td>71.4</td><td>47.3</td><td>70.0</td><td>71.9</td><td>49.5</td><td>66.2</td><td>77.5</td><td>53.1</td><td>71.3</td><td>53.1</td><td>13.0</td><td>42.9</td><td>49.0</td><td>19.5</td><td>41.9</td></tr><tr><th>MiB[23]</th><td>69.6</td><td>25.6</td><td>67.4</td><td>70.2</td><td>22.1</td><td>67.8</td><td>71.8</td><td>43.3</td><td>64.7</td><td>75.5</td><td>49.4</td><td>69.0</td><td>46.2</td><td>12.9</td><td>37.9</td><td>35.1</td><td>13.5</td><td>29.7</td></tr><tr><th>MiB+SDR[51]</th><td>70.8</td><td>31.4</td><td>68.9</td><td>71.3</td><td>23.4</td><td>69.0</td><td>74.6</td><td>44.1</td><td>67.3</td><td>76.3</td><td>50.2</td><td>70.1</td><td>59.4</td><td>14.3</td><td>48.7</td><td>47.3</td><td>14.7</td><td>39.5</td></tr><tr><th>MiB+UCD</th><td>74.3</td><td>28.4</td><td>72.0</td><td>73.7</td><td>34.0</td><td>71.7</td><td>73.0</td><td>46.2</td><td>66.3</td><td>78.5</td><td>50.7</td><td>71.5</td><td>53.3</td><td>14.4</td><td>43.5</td><td>51.9</td><td>13.1</td><td>42.2</td></tr><tr><th>PLOP{}^{*} [29]</th><td>75.1</td><td>38.2</td><td>73.2</td><td>75.0</td><td>39.1</td><td>73.2</td><td>66.5</td><td>39.6</td><td>59.8</td><td>74.7</td><td>49.8</td><td>68.5</td><td>49.0</td><td>13.8</td><td>40.2</td><td>65.2</td><td>22.4</td><td>54.5</td></tr><tr><th>PLOP+ UCD</th><td>75.7</td><td>31.8</td><td>73.5</td><td>75.9</td><td>39.5</td><td>74.0</td><td>67.0</td><td>39.3</td><td>60.1</td><td>75.0</td><td>51.8</td><td>69.2</td><td>50.8</td><td>13.3</td><td>41.4</td><td>66.3</td><td>21.6</td><td>55.1</td></tr><tr><th>Joint</th><td>77.4</td><td>78.0</td><td>77.4</td><td>77.4</td><td>78.0</td><td>77.4</td><td>79.1</td><td>72.6</td><td>77.4</td><td>79.1</td><td>72.6</td><td>77.4</td><td>79.1</td><td>72.6</td><td>77.4</td><td>79.1</td><td>72.6</td><td>77.4</td></tr></tbody></table>", "caption": "TABLE I: Mean IoU on the Pascal-VOC 2012 dataset for different incremental class learning scenarios. * means results come from re-implementation. {\\dagger} means a updated version.\\sharp means the conditional GAN model pretrained on ImageNet [61] is used.UDC means uncertainty-aware contrastive distillation. Best among table in bold, best among part in underlined.", "list_citation_info": ["[51] \u2014\u2014, \u201cContinual semantic segmentation via repulsion-attraction of sparse and disentangled latent representations,\u201d CVPR, 2021.", "[50] U. Michieli and P. Zanuttigh, \u201cKnowledge distillation for incremental learning in semantic segmentation,\u201d CVIU, 2021.", "[41] A. Maracani, U. Michieli, M. Toldo, and P. Zanuttigh, \u201cRecall: Replay-based continual learning in semantic segmentation,\u201d ICCV, 2021.", "[62] F. Zenke, B. Poole, and S. Ganguli, \u201cContinual learning through synaptic intelligence,\u201d PLMR, 2017.", "[29] A. Douillard, Y. Chen, A. Dapogny, and M. Cord, \u201cPlop: Learning without forgetting for continual semantic segmentation,\u201d CVPR, 2021.", "[63] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, \u201cRiemannian walk for incremental learning: Understanding forgetting and intransigence,\u201d in ECCV, 2018.", "[19] U. Michieli and P. Zanuttigh, \u201cIncremental learning techniques for semantic segmentation,\u201d in ICCV Workshops, 2019.", "[14] Z. Li and D. Hoiem, \u201cLearning without forgetting,\u201d TPAMI, 2017.", "[23] F. Cermelli, M. Mancini, S. R. Bulo, E. Ricci, and B. Caputo, \u201cModeling the background for incremental learning in semantic segmentation,\u201d in CVPR, 2020.", "[61] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in CVPR, 2009.", "[16] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \u201cicarl: Incremental classifier and representation learning,\u201d in CVPR, 2017.", "[15] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., \u201cOvercoming catastrophic forgetting in neural networks,\u201d Proceedings of the national academy of sciences, 2017."]}, {"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><td colspan=\"6\">10-10</td><td colspan=\"6\">10-1</td></tr><tr><td colspan=\"3\">Disjoint</td><td colspan=\"3\">Overlapped</td><td colspan=\"3\">Disjoint</td><td colspan=\"3\">Overlapped</td></tr><tr><td>1-10</td><td>11-20</td><td>all</td><td>1-10</td><td>11-20</td><td>all</td><td>1-10</td><td>11-20</td><td>all</td><td>1-10</td><td>11-20</td><td>all</td></tr><tr><th>FT</th><td>7.7</td><td>60.8</td><td>33.0</td><td>7.8</td><td>58.9</td><td>32.1</td><td>6.3</td><td>2.0</td><td>4.3</td><td>6.3</td><td>2.8</td><td>4.7</td></tr><tr><th>LwF[14]</th><td>63.1</td><td>61.1</td><td>62.2</td><td>70.7</td><td>63.4</td><td>67.2</td><td>6.7</td><td>6.5</td><td>6.6</td><td>16.6</td><td>14.9</td><td>15.8</td></tr><tr><th>LwF-MC[16]</th><td>52.4</td><td>42.5</td><td>47.7</td><td>53.9</td><td>43.0</td><td>48.7</td><td>6.9</td><td>1.7</td><td>4.4</td><td>11.2</td><td>2.5</td><td>7.1</td></tr><tr><th>ILT[19]</th><td>67.7</td><td>61.3</td><td>64.7</td><td>70.3</td><td>61.9</td><td>66.3</td><td>14.1</td><td>0.6</td><td>7.5</td><td>16.5</td><td>1.0</td><td>9.1</td></tr><tr><th>RECALL{}^{\\sharp}[41]</th><td>62.6</td><td>56.1</td><td>60.8</td><td>65.0</td><td>58.4</td><td>63.1</td><td>58.3</td><td>46.0</td><td>53.9</td><td>59.5</td><td>46.7</td><td>54.8</td></tr><tr><th>MiB[23]</th><td>66.9</td><td>57.5</td><td>62.4</td><td>70.4</td><td>63.7</td><td>67.2</td><td>14.9</td><td>9.5</td><td>12.3</td><td>15.1</td><td>14.8</td><td>15.0</td></tr><tr><th>MiB+SDR[51]</th><td>67.5</td><td>57.9</td><td>62.9</td><td>70.5</td><td>63.9</td><td>67.4</td><td>25.5</td><td>15.7</td><td>20.8</td><td>26.3</td><td>19.7</td><td>23.2</td></tr><tr><th>MiB+UCD</th><td>65.0</td><td>58.7</td><td>61.9</td><td>71.8</td><td>65.2</td><td>68.5</td><td>33.1</td><td>26.1</td><td>30.6</td><td>33.7</td><td>26.5</td><td>31.1</td></tr><tr><th>PLOP{}^{*} [29]</th><td>61.8</td><td>53.1</td><td>57.5</td><td>65.0</td><td>58.8</td><td>61.9</td><td>39.5</td><td>13.9</td><td>26.7</td><td>40.0</td><td>21.7</td><td>30.8</td></tr><tr><th>PLOP+ UCD</th><td>63.0</td><td>55.8</td><td>59.4</td><td>71.5</td><td>58.9</td><td>65.2</td><td>42.6</td><td>15.0</td><td>28.8</td><td>42.3</td><td>28.3</td><td>35.3</td></tr><tr><th>Joint</th><td>78.6</td><td>76.0</td><td>77.4</td><td>78.6</td><td>76.0</td><td>77.4</td><td>78.6</td><td>76.0</td><td>77.4</td><td>78.6</td><td>76.0</td><td>77.4</td></tr></tbody></table>", "caption": "TABLE II: Mean IoU on the Pascal-VOC 2012 dataset for more incremental class learning scenarios. * means results come from re-implementation.\\sharp means the conditional GAN model pretrained on ImageNet [61] is used.UDC means uncertainty-aware contrastive distillation.Best among table in bold, best among part in underlined.", "list_citation_info": ["[51] \u2014\u2014, \u201cContinual semantic segmentation via repulsion-attraction of sparse and disentangled latent representations,\u201d CVPR, 2021.", "[41] A. Maracani, U. Michieli, M. Toldo, and P. Zanuttigh, \u201cRecall: Replay-based continual learning in semantic segmentation,\u201d ICCV, 2021.", "[29] A. Douillard, Y. Chen, A. Dapogny, and M. Cord, \u201cPlop: Learning without forgetting for continual semantic segmentation,\u201d CVPR, 2021.", "[19] U. Michieli and P. Zanuttigh, \u201cIncremental learning techniques for semantic segmentation,\u201d in ICCV Workshops, 2019.", "[14] Z. Li and D. Hoiem, \u201cLearning without forgetting,\u201d TPAMI, 2017.", "[23] F. Cermelli, M. Mancini, S. R. Bulo, E. Ricci, and B. Caputo, \u201cModeling the background for incremental learning in semantic segmentation,\u201d in CVPR, 2020.", "[61] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in CVPR, 2009.", "[16] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \u201cicarl: Incremental classifier and representation learning,\u201d in CVPR, 2017."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">100-50</td><td colspan=\"7\">100-10</td><td colspan=\"4\">50-50</td></tr><tr><td>1-100</td><td>101-150</td><td>all</td><td>1-100</td><td>100-110</td><td>110-120</td><td>120-130</td><td>130-140</td><td>140-150</td><td>all</td><td>1-50</td><td>51-100</td><td>101-150</td><td>all</td></tr><tr><td>FT</td><td>0.0</td><td>24.9</td><td>8.3</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>16.6</td><td>1.1</td><td>0.0</td><td>0.0</td><td>22.0</td><td>7.3</td></tr><tr><td>LwF [14]</td><td>21.1</td><td>25.6</td><td>22.6</td><td>0.1</td><td>0.0</td><td>0.4</td><td>2.6</td><td>4.6</td><td>16.9</td><td>1.7</td><td>5.7</td><td>12.9</td><td>22.8</td><td>13.9</td></tr><tr><td>LwF-MC [16]</td><td>34.2</td><td>10.5</td><td>26.3</td><td>18.7</td><td>2.5</td><td>8.7</td><td>4.1</td><td>6.5</td><td>5.1</td><td>14.3</td><td>27.8</td><td>7.0</td><td>10.4</td><td>15.1</td></tr><tr><td>ILT [19]</td><td>22.9</td><td>18.9</td><td>21.6</td><td>0.3</td><td>0.0</td><td>1.0</td><td>2.1</td><td>4.6</td><td>10.7</td><td>1.4</td><td>8.4</td><td>9.7</td><td>14.3</td><td>10.8</td></tr><tr><td>Inc.Seg [49]</td><td>36.6</td><td>0.4</td><td>24.6</td><td>32.4</td><td>0.0</td><td>0.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>21.7</td><td>40.2</td><td>1.3</td><td>0.3</td><td>14.1</td></tr><tr><td>SDR [51]</td><td>37.4</td><td>24.8</td><td>33.2</td><td>28.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>21.7</td><td>40.9</td><td>-</td><td>-</td><td>29.5</td></tr><tr><td>UCD</td><td>40.4</td><td>27.3</td><td>36.0</td><td>28.6</td><td>13.0</td><td>13.1</td><td>9.2</td><td>10.7</td><td>16.1</td><td>23.2</td><td>39.3</td><td>25.3</td><td>19.1</td><td>27.9</td></tr><tr><td>MiB [23]</td><td>37.9</td><td>27.9</td><td>34.6</td><td>31.8</td><td>10.4</td><td>14.8</td><td>12.8</td><td>13.6</td><td>18.7</td><td>25.9</td><td>35.5</td><td>22.2</td><td>23.6</td><td>27.0</td></tr><tr><td>MiB + SDR [51]</td><td>37.5</td><td>25.5</td><td>33.5</td><td>28.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>23.2</td><td>42.9</td><td>-</td><td>-</td><td>31.3</td></tr><tr><td>MiB + UCD</td><td>40.5</td><td>28.1</td><td>36.4</td><td>33.4</td><td>15.2</td><td>15.3</td><td>10.8</td><td>12.5</td><td>18.8</td><td>27.1</td><td>40.2</td><td>25.9</td><td>19.5</td><td>28.5</td></tr><tr><td>PLOP{}^{*} [29]</td><td>29.8</td><td>4.2</td><td>22.2</td><td>32.1</td><td>1.9</td><td>10.0</td><td>0.8</td><td>1.2</td><td>0.1</td><td>22.3</td><td>19.2</td><td>0.4</td><td>0.4</td><td>6.6</td></tr><tr><td>PLOP + UCD</td><td>33.2</td><td>4.7</td><td>23.7</td><td>35.7</td><td>2.1</td><td>11.1</td><td>0.9</td><td>1.4</td><td>0.1</td><td>24.8</td><td>21.3</td><td>0.4</td><td>0.4</td><td>7.4</td></tr><tr><td>Joint</td><td>44.3</td><td>28.2</td><td>38.9</td><td>44.3</td><td>26.1</td><td>42.8</td><td>26.7</td><td>28.1</td><td>17.3</td><td>38.9</td><td>51.1</td><td>38.3</td><td>28.2</td><td>38.9</td></tr></tbody></table>", "caption": "TABLE III: Mean IoU on the ADE20K dataset for different incremental class learning disjoint setting.* means results come from re-implementation. UDC means uncertainty-aware contrastive distillation. Best among table in bold, best among part in underlined.", "list_citation_info": ["[51] \u2014\u2014, \u201cContinual semantic segmentation via repulsion-attraction of sparse and disentangled latent representations,\u201d CVPR, 2021.", "[49] S. Yan, J. Zhou, J. Xie, S. Zhang, and X. He, \u201cAn em framework for online incremental learning of semantic segmentation,\u201d arXiv, 2021.", "[29] A. Douillard, Y. Chen, A. Dapogny, and M. Cord, \u201cPlop: Learning without forgetting for continual semantic segmentation,\u201d CVPR, 2021.", "[19] U. Michieli and P. Zanuttigh, \u201cIncremental learning techniques for semantic segmentation,\u201d in ICCV Workshops, 2019.", "[14] Z. Li and D. Hoiem, \u201cLearning without forgetting,\u201d TPAMI, 2017.", "[23] F. Cermelli, M. Mancini, S. R. Bulo, E. Ricci, and B. Caputo, \u201cModeling the background for incremental learning in semantic segmentation,\u201d in CVPR, 2020.", "[16] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \u201cicarl: Incremental classifier and representation learning,\u201d in CVPR, 2017."]}, {"table": "<table><thead><tr><th rowspan=\"2\">method</th><th colspan=\"3\">100-50</th><th colspan=\"3\">100-10</th><th colspan=\"3\">50-50</th></tr><tr><th>1-100</th><th>101-150</th><th>mIoU</th><th>1-100</th><th>101-150</th><th>mIoU</th><th>1-50</th><th>51-150</th><th>mIoU</th></tr></thead><tbody><tr><th>ILT{}^{*}[19]</th><td>18.29</td><td>14.40</td><td>17.00</td><td>0.11</td><td>3.06</td><td>1.09</td><td>3.53</td><td>12.85</td><td>9.70</td></tr><tr><th>MiB{}^{*}[23]</th><td>40.52</td><td>17.17</td><td>32.79</td><td>38.21</td><td>11.12</td><td>29.24</td><td>45.57</td><td>21.01</td><td>29.31</td></tr><tr><th>PLOP [29]</th><td>41.87</td><td>14.89</td><td>32.94</td><td>40.48</td><td>13.61</td><td>31.59</td><td>48.83</td><td>20.99</td><td>30.40</td></tr><tr><th>PLOP+UCD</th><td>42.12</td><td>15.84</td><td>33.31</td><td>40.80</td><td>15.23</td><td>32.29</td><td>47.12</td><td>24.12</td><td>31.79</td></tr></tbody></table>", "caption": "TABLE IV: Mean IoU on the ADE20K dataset for different incremental class learning overlapped setting. * means results come from PLOP. UDC means uncertainty-aware contrastive distillation. Best among table in bold.", "list_citation_info": ["[19] U. Michieli and P. Zanuttigh, \u201cIncremental learning techniques for semantic segmentation,\u201d in ICCV Workshops, 2019.", "[23] F. Cermelli, M. Mancini, S. R. Bulo, E. Ricci, and B. Caputo, \u201cModeling the background for incremental learning in semantic segmentation,\u201d in CVPR, 2020.", "[29] A. Douillard, Y. Chen, A. Dapogny, and M. Cord, \u201cPlop: Learning without forgetting for continual semantic segmentation,\u201d CVPR, 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">method</th><td colspan=\"3\">13-6</td><td colspan=\"3\">13-1</td></tr><tr><td>1-13</td><td>14-19</td><td>mIoU</td><td>1-13</td><td>14-19</td><td>mIoU</td></tr><tr><th>FT</th><td>0.0</td><td>33.4</td><td>10.6</td><td>0.0</td><td>7.4</td><td>2.3</td></tr><tr><th>ILT [19]</th><td>52.3</td><td>18.3</td><td>41.5</td><td>4.2</td><td>9.1</td><td>5.8</td></tr><tr><th>PLOP [29]</th><td>53.2</td><td>10.1</td><td>39.6</td><td>52.4</td><td>15.1</td><td>40.6</td></tr><tr><th>PLOP+UCD</th><td>53.4</td><td>10.0</td><td>39.7</td><td>52.7</td><td>15.3</td><td>40.9</td></tr><tr><th>MiB [23]</th><td>52.8</td><td>17.9</td><td>41.8</td><td>51.6</td><td>22.9</td><td>42.5</td></tr><tr><th>MiB+UCD</th><td>53.0</td><td>18.6</td><td>42.1</td><td>52.2</td><td>23.4</td><td>43.1</td></tr><tr><th>Joint</th><td>53.5</td><td>54.2</td><td>53.7</td><td>53.5</td><td>54.2</td><td>53.7</td></tr></tbody></table>", "caption": "TABLE V: Mean IoU on the Cityscapes dataset for different incremental class learning scenarios. UDC means uncertainty-aware contrastive distillation. Best among table in bold, best among part in underlined.", "list_citation_info": ["[19] U. Michieli and P. Zanuttigh, \u201cIncremental learning techniques for semantic segmentation,\u201d in ICCV Workshops, 2019.", "[23] F. Cermelli, M. Mancini, S. R. Bulo, E. Ricci, and B. Caputo, \u201cModeling the background for incremental learning in semantic segmentation,\u201d in CVPR, 2020.", "[29] A. Douillard, Y. Chen, A. Dapogny, and M. Cord, \u201cPlop: Learning without forgetting for continual semantic segmentation,\u201d CVPR, 2021."]}], "citation_info_to_title": {"[61] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in CVPR, 2009.": "Imagenet: A large-scale hierarchical image database", "[29] A. Douillard, Y. Chen, A. Dapogny, and M. Cord, \u201cPlop: Learning without forgetting for continual semantic segmentation,\u201d CVPR, 2021.": "Plop: Learning without forgetting for continual semantic segmentation", "[16] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \u201cicarl: Incremental classifier and representation learning,\u201d in CVPR, 2017.": "icarl: Incremental classifier and representation learning", "[23] F. Cermelli, M. Mancini, S. R. Bulo, E. Ricci, and B. Caputo, \u201cModeling the background for incremental learning in semantic segmentation,\u201d in CVPR, 2020.": "Modeling the background for incremental learning in semantic segmentation", "[49] S. Yan, J. Zhou, J. Xie, S. Zhang, and X. He, \u201cAn em framework for online incremental learning of semantic segmentation,\u201d arXiv, 2021.": "An EM Framework for Online Incremental Learning of Semantic Segmentation", "[50] U. Michieli and P. Zanuttigh, \u201cKnowledge distillation for incremental learning in semantic segmentation,\u201d CVIU, 2021.": "Knowledge distillation for incremental learning in semantic segmentation", "[63] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, \u201cRiemannian walk for incremental learning: Understanding forgetting and intransigence,\u201d in ECCV, 2018.": "Riemannian walk for incremental learning: Understanding forgetting and intransigence", "[51] \u2014\u2014, \u201cContinual semantic segmentation via repulsion-attraction of sparse and disentangled latent representations,\u201d CVPR, 2021.": "Continual semantic segmentation via repulsion-attraction of sparse and disentangled latent representations", "[14] Z. Li and D. Hoiem, \u201cLearning without forgetting,\u201d TPAMI, 2017.": "Learning without forgetting", "[19] U. Michieli and P. Zanuttigh, \u201cIncremental learning techniques for semantic segmentation,\u201d in ICCV Workshops, 2019.": "Incremental learning techniques for semantic segmentation", "[62] F. Zenke, B. Poole, and S. Ganguli, \u201cContinual learning through synaptic intelligence,\u201d PLMR, 2017.": "Continual learning through synaptic intelligence", "[15] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., \u201cOvercoming catastrophic forgetting in neural networks,\u201d Proceedings of the national academy of sciences, 2017.": "Overcoming catastrophic forgetting in neural networks", "[41] A. Maracani, U. Michieli, M. Toldo, and P. Zanuttigh, \u201cRecall: Replay-based continual learning in semantic segmentation,\u201d ICCV, 2021.": "Recall: Replay-based continual learning in semantic segmentation"}, "source_title_to_arxiv_id": {"Plop: Learning without forgetting for continual semantic segmentation": "2011.11390"}}