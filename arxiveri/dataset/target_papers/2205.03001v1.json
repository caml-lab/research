{"title": "Revisiting Pretraining for Semi-Supervised Learning in the Low-Label Regime", "abstract": "Semi-supervised learning (SSL) addresses the lack of labeled data by\nexploiting large unlabeled data through pseudolabeling. However, in the\nextremely low-label regime, pseudo labels could be incorrect, a.k.a. the\nconfirmation bias, and the pseudo labels will in turn harm the network\ntraining. Recent studies combined finetuning (FT) from pretrained weights with\nSSL to mitigate the challenges and claimed superior results in the low-label\nregime. In this work, we first show that the better pretrained weights brought\nin by FT account for the state-of-the-art performance, and importantly that\nthey are universally helpful to off-the-shelf semi-supervised learners. We\nfurther argue that direct finetuning from pretrained weights is suboptimal due\nto covariate shift and propose a contrastive target pretraining step to adapt\nmodel weights towards target dataset. We carried out extensive experiments on\nboth classification and segmentation tasks by doing target pretraining then\nfollowed by semi-supervised finetuning. The promising results validate the\nefficacy of target pretraining for SSL, in particular in the low-label regime.", "authors": ["Xun Xu", "Jingyi Liao", "Lile Cai", "Manh Cuong Nguyen", "Kangkang Lu", "Wanyue Zhang", "Yasin Yazici", "Chuan Sheng Foo"], "published_date": "2022_05_06", "pdf_url": "http://arxiv.org/pdf/2205.03001v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td></td><td></td><td colspan=\"3\">CIFAR-10</td><td colspan=\"2\">CIFAR-100</td><td>SVHN</td></tr><tr><td>Pretrain</td><td>Trg. Pre.</td><td>SSL</td><td>#40</td><td>#250</td><td>#1000</td><td>#400</td><td>#2500</td><td>#40</td></tr><tr><td>-</td><td>-</td><td>MT [2]</td><td>27.34</td><td>47.29</td><td>65.48</td><td>10.36</td><td>37.49</td><td>27.73</td></tr><tr><td>-</td><td>-</td><td>VAT [19]</td><td>31.81</td><td>57.02</td><td>75.61</td><td>12.85</td><td>32.32</td><td>15.21</td></tr><tr><td>-</td><td>-</td><td>-</td><td>18.41</td><td>29.76</td><td>44.96</td><td>8.67</td><td>21.69</td><td>10.47</td></tr><tr><td>-</td><td>BYOL</td><td>-</td><td>49.44 (+31.03)</td><td>66.33 (+36.57)</td><td>75.15 (+30.19)</td><td>15.18 (+6.51)</td><td>35.20 (+13.51)</td><td>18.25 (+7.78)</td></tr><tr><td>IN Sup</td><td>-</td><td>-</td><td>49.69</td><td>77.38</td><td>85.56</td><td>30.85</td><td>57.36</td><td>16.92</td></tr><tr><td>IN Sup</td><td>BYOL</td><td>-</td><td>65.13 (+15.44)</td><td>82.07 (+4.69)</td><td>87.64 (+2.08)</td><td>34.63 (+3.78)</td><td>58.65 (1.29)</td><td>35.82 (+18.90)</td></tr><tr><td>-</td><td>-</td><td>SelfTuning [6]</td><td>27.89</td><td>45.84</td><td>61.06</td><td>9.17</td><td>23.72</td><td>13.49</td></tr><tr><td>-</td><td>BYOL</td><td>SelfTuning</td><td>49.76 (+21.87)</td><td>73.70 (+27.86)</td><td>82.11 (+21.05)</td><td>19.05 (+9.88)</td><td>41.40 (+17.68)</td><td>22.23 (+8.74)</td></tr><tr><td>IN Sup</td><td>-</td><td>SelfTuning</td><td>48.83</td><td>82.56</td><td>89.86</td><td>35.56</td><td>62.86</td><td>45.84</td></tr><tr><td>IN Sup</td><td>BYOL</td><td>SelfTuning</td><td>62.57 (+13.74)</td><td>84.71 (+2.15)</td><td>90.83 (+0.97)</td><td>38.36 (+2.80)</td><td>67.31 (4.45)</td><td>46.95 (+1.11)</td></tr><tr><td>-</td><td>-</td><td>FixMatch [3]</td><td>60.75</td><td>91.32</td><td>93.01</td><td>29.85</td><td>59.12</td><td>32.76</td></tr><tr><td>-</td><td>BYOL</td><td>FixMatch</td><td>75.51 (+14.76)</td><td>91.55 (+0.23)</td><td>94.19 (+1.18)</td><td>40.14 (+10.29)</td><td>63.34 (+4.22)</td><td>36.94 (+4.18)</td></tr><tr><td>IN Sup</td><td>-</td><td>FixMatch</td><td>75.28</td><td>94.22</td><td>95.82</td><td>44.34</td><td>71.81</td><td>79.89</td></tr><tr><td>IN Sup</td><td>BYOL</td><td>FixMatch</td><td>90.06 (+14.78)</td><td>94.99 (+0.77)</td><td>95.48 (-0.34)</td><td>46.23 (+1.89)</td><td>72.19 (+0.38)</td><td>92.97 (+13.08)</td></tr></tbody></table>", "caption": "TABLE I: Evaluation of the impact of further contrastive pretraining by semi-supervised finetuning on classification tasks. IN Sup indicates supervise pretrained on ImageNet. The number in parenthesis (\\pmX) is the improvement brought by target pretraining. ", "list_citation_info": ["[19] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, \u201cVirtual adversarial training: a regularization method for supervised and semi-supervised learning,\u201d IEEE transactions on pattern analysis and machine intelligence, 2018.", "[6] X. Wang, J. Gao, M. Long, and J. Wang, \u201cSelf-tuning for data-efficient deep learning,\u201d in International Conference on Machine Learning, 2021.", "[3] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk, A. Kurakin, H. Zhang, and C. Raffel, \u201cFixmatch: Simplifying semi-supervised learning with consistency and confidence,\u201d in Advances in neural information processing systems, 2020.", "[2] A. Tarvainen and H. Valpola, \u201cMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,\u201d in Advances in neural information processing systems, 2017."]}, {"table": "<table><tbody><tr><th></th><th></th><th></th><td colspan=\"3\">PascalVOC</td><td colspan=\"2\">CityScapes</td></tr><tr><th>Pretrain</th><th>Trg.Pre.</th><th>SSL</th><td>1%</td><td>2%</td><td>5%</td><td>#100</td><td>#372</td></tr><tr><th>IN Sup</th><th>-</th><th>CutMix [7]</th><td>53.79</td><td>64.81</td><td>66.48</td><td>57.89</td><td>65.64</td></tr><tr><th>IN Sup</th><th>-</th><th>ClassMix [24]</th><td>40.64</td><td>52.84</td><td>59.82</td><td>-</td><td>-</td></tr><tr><th>IN Sup</th><th>-</th><th>CLMB [25]</th><td>-</td><td>63.40</td><td>69.10</td><td>64.90</td><td>70.00</td></tr><tr><th>IN PixPro</th><th>-</th><th>-</th><td>38.20</td><td>49.95</td><td>60.35</td><td>52.99</td><td>63.05</td></tr><tr><th>IN PixPro</th><th>PixPro</th><th>-</th><td>41.35</td><td>52.97</td><td>63.11</td><td>53.26</td><td>63.26</td></tr><tr><th>IN PixPro</th><th>-</th><th>CutMix</th><td>49.58</td><td>61.63</td><td>67.61</td><td>58.30</td><td>68.21</td></tr><tr><th>IN PixPro</th><th>PixPro</th><th>CutMix</th><td>52.20</td><td>61.25</td><td>68.59</td><td>59.20</td><td>68.02</td></tr></tbody></table>", "caption": "TABLE II: Evaluation of Semi-supervised finetuning with different model initial weights. IN PixPro indicates weights pretrained with PixPro on ImageNet.", "list_citation_info": ["[24] V. Olsson, W. Tranheden, J. Pinto, and L. Svensson, \u201cClassmix: Segmentation-based data augmentation for semi-supervised learning,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021.", "[25] I. Alonso, A. Sabater, D. Ferstl, L. Montesano, and A. C. Murillo, \u201cSemi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank,\u201d 2021.", "[7] G. French, T. Aila, S. Laine, M. Mackiewicz, and G. Finlayson, \u201cSemi-supervised semantic segmentation needs strong, high-dimensional perturbations,\u201d 2019."]}, {"table": "<table><thead><tr><th>#Labeled</th><th>Pretrain</th><th>Trg. Pre.</th><th>mIoU</th></tr></thead><tbody><tr><th rowspan=\"2\">10</th><td>IN Sup</td><td>-</td><td>73.80</td></tr><tr><td>IN Sup</td><td>BYOL</td><td>77.74</td></tr><tr><th rowspan=\"2\">20</th><td>IN Sup</td><td>-</td><td>82.93</td></tr><tr><td>IN Sup</td><td>BYOL</td><td>82.96</td></tr><tr><th rowspan=\"2\">50</th><td>IN Sup</td><td>-</td><td>85.99 (74.57)</td></tr><tr><td>IN Sup</td><td>BYOL</td><td>86.02</td></tr></tbody></table>", "caption": "TABLE III: Target pretraining on skin lesion segmentation dataset (ISIC2017). Result in () was reported in [7].", "list_citation_info": ["[7] G. French, T. Aila, S. Laine, M. Mackiewicz, and G. Finlayson, \u201cSemi-supervised semantic segmentation needs strong, high-dimensional perturbations,\u201d 2019."]}, {"table": "<table><tbody><tr><td></td><th></th><th colspan=\"4\">Semi-Supervised Finetune</th></tr><tr><th>Pretrain</th><th>Trg. Pre.</th><th>-</th><th>FixMatch [3]</th><th>UDA [28]</th><th>MT [2]</th></tr><tr><td>IN Sup</td><td>-</td><td>49.69</td><td>75.28</td><td>62.94</td><td>48.98</td></tr><tr><td>IN Sup</td><td>BYOL [10]</td><td>65.13</td><td>90.06</td><td>78.43</td><td>61.55</td></tr><tr><td>IN Sup</td><td>MoCo v2 [44]</td><td>71.13</td><td>90.37</td><td>87.04</td><td>69.02</td></tr></tbody></table>", "caption": "TABLE V: Additional contrastive learning and semi-supervised finetuning methods.", "list_citation_info": ["[10] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar et al., \u201cBootstrap your own latent: A new approach to self-supervised learning,\u201d in Advances in neural information processing systems, 2020.", "[44] X. Chen, H. Fan, R. Girshick, and K. He, \u201cImproved baselines with momentum contrastive learning,\u201d arXiv preprint arXiv:2003.04297, 2020.", "[28] Y. Ganin and V. Lempitsky, \u201cUnsupervised domain adaptation by backpropagation,\u201d in International conference on machine learning. PMLR, 2015.", "[3] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk, A. Kurakin, H. Zhang, and C. Raffel, \u201cFixmatch: Simplifying semi-supervised learning with consistency and confidence,\u201d in Advances in neural information processing systems, 2020.", "[2] A. Tarvainen and H. Valpola, \u201cMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,\u201d in Advances in neural information processing systems, 2017."]}], "citation_info_to_title": {"[44] X. Chen, H. Fan, R. Girshick, and K. He, \u201cImproved baselines with momentum contrastive learning,\u201d arXiv preprint arXiv:2003.04297, 2020.": "Improved Baselines with Momentum Contrastive Learning", "[6] X. Wang, J. Gao, M. Long, and J. Wang, \u201cSelf-tuning for data-efficient deep learning,\u201d in International Conference on Machine Learning, 2021.": "Self-tuning for data-efficient deep learning", "[3] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk, A. Kurakin, H. Zhang, and C. Raffel, \u201cFixmatch: Simplifying semi-supervised learning with consistency and confidence,\u201d in Advances in neural information processing systems, 2020.": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence", "[28] Y. Ganin and V. Lempitsky, \u201cUnsupervised domain adaptation by backpropagation,\u201d in International conference on machine learning. PMLR, 2015.": "Unsupervised domain adaptation by backpropagation", "[10] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G. Azar et al., \u201cBootstrap your own latent: A new approach to self-supervised learning,\u201d in Advances in neural information processing systems, 2020.": "Bootstrap your own latent: A new approach to self-supervised learning", "[24] V. Olsson, W. Tranheden, J. Pinto, and L. Svensson, \u201cClassmix: Segmentation-based data augmentation for semi-supervised learning,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021.": "Classmix: Segmentation-based data augmentation for semi-supervised learning", "[2] A. Tarvainen and H. Valpola, \u201cMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,\u201d in Advances in neural information processing systems, 2017.": "Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results", "[25] I. Alonso, A. Sabater, D. Ferstl, L. Montesano, and A. C. Murillo, \u201cSemi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank,\u201d 2021.": "Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank", "[7] G. French, T. Aila, S. Laine, M. Mackiewicz, and G. Finlayson, \u201cSemi-supervised semantic segmentation needs strong, high-dimensional perturbations,\u201d 2019.": "Semi-supervised semantic segmentation needs strong, high-dimensional perturbations", "[19] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, \u201cVirtual adversarial training: a regularization method for supervised and semi-supervised learning,\u201d IEEE transactions on pattern analysis and machine intelligence, 2018.": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning"}, "source_title_to_arxiv_id": {"Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank": "2104.13415"}}