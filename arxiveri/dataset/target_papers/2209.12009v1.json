{"title": "Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild", "abstract": "In this work, we tackle the challenging task of jointly tracking hand object\npose and reconstructing their shapes from depth point cloud sequences in the\nwild, given the initial poses at frame 0. We for the first time propose a point\ncloud based hand joint tracking network, HandTrackNet, to estimate the\ninter-frame hand joint motion. Our HandTrackNet proposes a novel hand pose\ncanonicalization module to ease the tracking task, yielding accurate and robust\nhand joint tracking. Our pipeline then reconstructs the full hand via\nconverting the predicted hand joints into a template-based parametric hand\nmodel MANO. For object tracking, we devise a simple yet effective module that\nestimates the object SDF from the first frame and performs optimization-based\ntracking. Finally, a joint optimization step is adopted to perform joint hand\nand object reasoning, which alleviates the occlusion-induced ambiguity and\nfurther refines the hand pose. During training, the whole pipeline only sees\npurely synthetic data, which are synthesized with sufficient variations and by\ndepth simulation for the ease of generalization. The whole pipeline is\npertinent to the generalization gaps and thus directly transferable to real\nin-the-wild data. We evaluate our method on two real hand object interaction\ndatasets, e.g. HO3D and DexYCB, without any finetuning. Our experiments\ndemonstrate that the proposed method significantly outperforms the previous\nstate-of-the-art depth-based hand and object pose estimation and tracking\nmethods, running at a frame rate of 9 FPS.", "authors": ["Jiayi Chen", "Mi Yan", "Jiazhao Zhang", "Yinzhen Xu", "Xiaolong Li", "Yijia Weng", "Li Yi", "Shuran Song", "He Wang"], "published_date": "2022_09_24", "pdf_url": "http://arxiv.org/pdf/2209.12009v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"3\">Simulated Validation</td><td colspan=\"6\">Real World Testing</td></tr><tr><th></th><td colspan=\"3\">SimGrasp</td><td colspan=\"3\">HO3D</td><td colspan=\"3\">DexYCB</td></tr><tr><th></th><td>MPJPE (cm)</td><td>PD (cm)</td><td>DD (cm)</td><td>MPJPE (cm)</td><td>PD (cm)</td><td>DD (cm)</td><td>MPJPE (cm)</td><td>PD (cm)</td><td>DD (cm)</td></tr><tr><th>Forth oikonomidis2011efficient </th><td>2.51</td><td>-</td><td>-</td><td>4.04</td><td>-</td><td>-</td><td>4.19</td><td>-</td><td>-</td></tr><tr><th>HandFoldingNet cheng2021handfoldingnet </th><td>1.22</td><td>-</td><td>-</td><td>2.93</td><td>-</td><td>-</td><td>3.78</td><td>-</td><td>-</td></tr><tr><th>A2J xiong2019a2j </th><td>0.95</td><td>-</td><td>-</td><td>4.03</td><td>-</td><td>-</td><td>3.52</td><td>-</td><td>-</td></tr><tr><th>VirtualView virtulview </th><td>0.91</td><td>-</td><td>-</td><td>2.73</td><td>-</td><td>-</td><td>3.05</td><td>-</td><td>-</td></tr><tr><th>HandTrackNet</th><td>0.84</td><td>-</td><td>-</td><td>2.11</td><td>-</td><td>-</td><td>2.75</td><td>-</td><td>-</td></tr><tr><th>Ours w/o hand-obj opt.</th><td>0.96</td><td>1.44</td><td>1.18</td><td>2.34</td><td>1.42</td><td>1.54</td><td>2.99</td><td>1.82</td><td>1.81</td></tr><tr><th>Ours (w/ hand-obj opt.)</th><td>0.90</td><td>1.31</td><td>1.11</td><td>2.06</td><td>1.08</td><td>1.21</td><td>2.69</td><td>1.48</td><td>1.45</td></tr></tbody></table>", "caption": "Table 1: Hand pose tracking. All the methods are trained and validated on SimGrasp, and directly tested on real world dataset. \u2019-\u2019 denotes the method doesn\u2019t reconstruct the hand shape thus can\u2019t be evaluated for those metrics. ", "list_citation_info": ["[98] Xiong, F., Zhang, B., Xiao, Y., Cao, Z., Yu, T., Zhou, J.T., Yuan, J.: A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 793\u2013802 (2019)", "[13] Cheng, J., Wan, Y., Zuo, D., Ma, C., Gu, J., Tan, P., Wang, H., Deng, X., Zhang, Y.: Efficient virtual view selection for 3d hand pose estimation. arXiv preprint arXiv:2203.15458 (2022)", "[14] Cheng, W., Park, J.H., Ko, J.H.: Handfoldingnet: A 3d hand pose estimation network using multiscale-feature guided folding of a 2d hand skeleton. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11260\u201311269 (2021)", "[62] Oikonomidis, I., Kyriazis, N., Argyros, A.A.: Efficient model-based 3D tracking of hand articulations using kinect. In: BmVC. vol. 1, p. 3 (2011)"]}, {"table": "<table><tbody><tr><td></td><td></td><td colspan=\"3\">Simulated Validation</td><td colspan=\"8\">Real World Testing</td></tr><tr><td></td><td></td><td colspan=\"3\">SimGrasp</td><td colspan=\"3\">HO3D</td><td colspan=\"5\">DexYCB</td></tr><tr><td></td><td></td><td>bottle{}^{\\dagger}</td><td>car</td><td>bowl{}^{\\dagger}</td><td>bottle</td><td>box</td><td>others*</td><td>bottle</td><td>bowl{}^{\\dagger}</td><td>box</td><td>can{}^{\\dagger}</td><td>others*</td></tr><tr><td></td><td>5{}^{\\circ}5cm(\\%) \\uparrow</td><td>84.6</td><td>87.9</td><td>56.7</td><td>15.7</td><td>67.3</td><td>-</td><td>16.9</td><td>23.5</td><td>23.6</td><td>22.2</td><td>-</td></tr><tr><td>CAPTRA</td><td>10{}^{\\circ}10cm(\\%) \\uparrow</td><td>95.5</td><td>96.4</td><td>80.5</td><td>48.5</td><td>91.9</td><td>-</td><td>42.3</td><td>54.1</td><td>46.5</td><td>55.0</td><td>-</td></tr><tr><td>weng2021captra </td><td>CD(cm) \\downarrow</td><td>1.84</td><td>2.21</td><td>2.17</td><td>2.84</td><td>2.22</td><td>-</td><td>3.97</td><td>4.97</td><td>3.83</td><td>4.93</td><td>-</td></tr><tr><td></td><td>5{}^{\\circ}5cm(\\%) \\uparrow</td><td>80.1</td><td>76.0</td><td>49.1</td><td>43.7</td><td>54.5</td><td>57.0</td><td>38.5</td><td>33.3</td><td>31.2</td><td>35.7</td><td>36.8</td></tr><tr><td>Ours</td><td>10{}^{\\circ}10cm(\\%) \\uparrow</td><td>90.1</td><td>88.1</td><td>66.5</td><td>68.9</td><td>84.6</td><td>82.3</td><td>64.1</td><td>58.4</td><td>52.0</td><td>59.0</td><td>58.5</td></tr><tr><td></td><td>CD(cm) \\downarrow</td><td>2.15</td><td>2.28</td><td>2.17</td><td>2.06</td><td>1.74</td><td>1.20</td><td>2.46</td><td>4.71</td><td>2.82</td><td>2.32</td><td>2.30</td></tr></tbody></table>", "caption": "Table 2: Object pose tracking. \u2019{}^{\\dagger}\u2019 denotes that the object category has symmetry and its rotation is evaluated by the error of symmetric axis direction. \u2019*\u2019 denotes that the ground truth object mesh is given. See the supplementary for the instances contained in each category of HO3D and DexYCB.", "list_citation_info": ["[95] Weng, Y., Wang, H., Zhou, Q., Qin, Y., Duan, Y., Fan, Q., Chen, B., Su, H., Guibas, L.J.: Captra: Category-level pose tracking for rigid and articulated objects from point clouds. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13209\u201313218 (2021)"]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">bottle</th><th colspan=\"3\">box</th><th colspan=\"3\">others*</th><th>Speed</th></tr><tr><th></th><th>5{}^{\\circ}5cm</th><th>10{}^{\\circ}10cm</th><th>CD</th><th>5{}^{\\circ}5cm</th><th>10{}^{\\circ}10cm</th><th>CD</th><th>5{}^{\\circ}5cm</th><th>10{}^{\\circ}10cm</th><th>CD</th><th>(FPS)</th></tr></thead><tbody><tr><td>Adam kingma2014adam </td><td>38.9</td><td>55.9</td><td>2.30</td><td>35.0</td><td>65.1</td><td>2.07</td><td>49.8</td><td>75.9</td><td>1.44</td><td>10.4</td></tr><tr><td>DeepLM huang2021deeplm </td><td>38.1</td><td>59.5</td><td>2.28</td><td>48.2</td><td>78.5</td><td>1.74</td><td>34.6</td><td>69.6</td><td>1.68</td><td>7.4</td></tr><tr><td>Ours</td><td>43.7</td><td>68.9</td><td>2.06</td><td>54.5</td><td>84.6</td><td>1.74</td><td>57.0</td><td>82.3</td><td>1.20</td><td>29.1</td></tr><tr><td>Adam*</td><td>51.3</td><td>72.0</td><td>2.06</td><td>52.8</td><td>84.3</td><td>1.69</td><td>52.1</td><td>78.9</td><td>1.63</td><td>4.2</td></tr><tr><td>DeepLM*</td><td>40.7</td><td>61.5</td><td>2.07</td><td>50.8</td><td>82.8</td><td>1.69</td><td>33.5</td><td>67.4</td><td>1.76</td><td>5.9</td></tr><tr><td>Ours*</td><td>49.0</td><td>72.0</td><td>2.04</td><td>56.2</td><td>87.1</td><td>1.75</td><td>57.9</td><td>83.3</td><td>1.21</td><td>5.3</td></tr></tbody></table>", "caption": "Table 3: Comparison of Different Optimizer.\u2019*\u2019 means using more iterations (and particle numbers).", "list_citation_info": ["[38] Huang, J., Huang, S., Sun, M.: Deeplm: Large-scale nonlinear least squares on deep learning frameworks using stochastic domain decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10308\u201310317 (2021)", "[44] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)"]}, {"table": "<table><thead><tr><th>AUC(20-50mm)\\uparrow</th><th>A2Jxiong2019a2j </th><th>HandFoldingNetcheng2021handfoldingnet </th><th>VirtualViewvirtulview </th><th>FORTHoikonomidis2011efficient </th><th>Ours</th></tr></thead><tbody><tr><th>HO3D</th><td>0.44</td><td>0.75</td><td>0.74</td><td>0.58</td><td>0.88</td></tr><tr><th>DexYCB</th><td>0.59</td><td>0.57</td><td>0.70</td><td>0.55</td><td>0.77</td></tr></tbody></table>", "caption": "Table 7: AUC metric for hand pose tracking. We show the Area Under the Curve (AUC) for the percentage of correct 3D key points (PCK) with thresholds ranging from 20mm to 50mm.", "list_citation_info": ["[98] Xiong, F., Zhang, B., Xiao, Y., Cao, Z., Yu, T., Zhou, J.T., Yuan, J.: A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 793\u2013802 (2019)", "[13] Cheng, J., Wan, Y., Zuo, D., Ma, C., Gu, J., Tan, P., Wang, H., Deng, X., Zhang, Y.: Efficient virtual view selection for 3d hand pose estimation. arXiv preprint arXiv:2203.15458 (2022)", "[14] Cheng, W., Park, J.H., Ko, J.H.: Handfoldingnet: A 3d hand pose estimation network using multiscale-feature guided folding of a 2d hand skeleton. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11260\u201311269 (2021)", "[62] Oikonomidis, I., Kyriazis, N., Argyros, A.A.: Efficient model-based 3D tracking of hand articulations using kinect. In: BmVC. vol. 1, p. 3 (2011)"]}], "citation_info_to_title": {"[14] Cheng, W., Park, J.H., Ko, J.H.: Handfoldingnet: A 3d hand pose estimation network using multiscale-feature guided folding of a 2d hand skeleton. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11260\u201311269 (2021)": "Handfoldingnet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton", "[13] Cheng, J., Wan, Y., Zuo, D., Ma, C., Gu, J., Tan, P., Wang, H., Deng, X., Zhang, Y.: Efficient virtual view selection for 3d hand pose estimation. arXiv preprint arXiv:2203.15458 (2022)": "Efficient virtual view selection for 3D hand pose estimation", "[98] Xiong, F., Zhang, B., Xiao, Y., Cao, Z., Yu, T., Zhou, J.T., Yuan, J.: A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 793\u2013802 (2019)": "A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image", "[38] Huang, J., Huang, S., Sun, M.: Deeplm: Large-scale nonlinear least squares on deep learning frameworks using stochastic domain decomposition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10308\u201310317 (2021)": "Deeplm: Large-scale nonlinear least squares on deep learning frameworks using stochastic domain decomposition", "[44] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)": "Adam: A method for stochastic optimization", "[95] Weng, Y., Wang, H., Zhou, Q., Qin, Y., Duan, Y., Fan, Q., Chen, B., Su, H., Guibas, L.J.: Captra: Category-level pose tracking for rigid and articulated objects from point clouds. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13209\u201313218 (2021)": "Captra: Category-level pose tracking for rigid and articulated objects from point clouds", "[62] Oikonomidis, I., Kyriazis, N., Argyros, A.A.: Efficient model-based 3D tracking of hand articulations using kinect. In: BmVC. vol. 1, p. 3 (2011)": "Efficient model-based 3D tracking of hand articulations using kinect"}, "source_title_to_arxiv_id": {"Captra: Category-level pose tracking for rigid and articulated objects from point clouds": "2104.03437"}}