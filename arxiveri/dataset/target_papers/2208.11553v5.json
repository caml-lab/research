{"title": "Improving video retrieval using multilingual knowledge transfer", "abstract": "Video retrieval has seen tremendous progress with the development of\nvision-language models. However, further improving these models require\nadditional labelled data which is a huge manual effort. In this paper, we\npropose a framework MKTVR, that utilizes knowledge transfer from a multilingual\nmodel to boost the performance of video retrieval. We first use\nstate-of-the-art machine translation models to construct pseudo ground-truth\nmultilingual video-text pairs. We then use this data to learn a video-text\nrepresentation where English and non-English text queries are represented in a\ncommon embedding space based on pretrained multilingual models. We evaluate our\nproposed approach on four English video retrieval datasets such as MSRVTT,\nMSVD, DiDeMo and Charades. Experimental results demonstrate that our approach\nachieves state-of-the-art results on all datasets outperforming previous\nmodels. Finally, we also evaluate our model on a multilingual video-retrieval\ndataset encompassing six languages and show that our model outperforms previous\nmultilingual video retrieval models in a zero-shot setting.", "authors": ["Avinash Madasu", "Estelle Aflalo", "Gabriela Ben Melech Stan", "Shao-Yen Tseng", "Gedas Bertasius", "Vasudev Lal"], "published_date": "2022_08_24", "pdf_url": "http://arxiv.org/pdf/2208.11553v5", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th></th><td colspan=\"5\">Text-to-Video Retrieval</td><td colspan=\"5\">Video-to-Text Retrieval</td></tr><tr><th>Type</th><th>Model</th><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td><td>R@1</td><td>R@5</td><td>R@10</td><td>MdR</td><td>MnR</td></tr><tr><th rowspan=\"13\">Others</th><th>JsFusion (Yu, Kim, and Kim 2018)</th><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HT (Miech et al. 2019)</th><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HERO (Li et al. 2020)</th><td>20.5</td><td>46.8</td><td>60.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CE (Liu et al. 2019)</th><td>20.9</td><td>48.8</td><td>62.4</td><td>6.0</td><td>28.2</td><td>20.6</td><td>50.3</td><td>64.0</td><td>5.3</td><td>25.1</td></tr><tr><th>ClipBERT (Lei et al. 2021)</th><td>22.0</td><td>46.8</td><td>59.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SupportSET (Patrick et al. 2020)</th><td>27.4</td><td>56.3</td><td>67.7</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>VideoCLIP (Xu et al. 2021)</th><td>30.9</td><td>55.4</td><td>66.8</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>FrozenInTime (Bain et al. 2021)</th><td>31</td><td>59.5</td><td>70.5</td><td>3.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CLIP (Radford et al. 2021)</th><td>31.2</td><td>53.7</td><td>2.6</td><td>4.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HIT (Liu et al. 2021)</th><td>30.7</td><td>60.9</td><td>73.2</td><td>2.6</td><td>-</td><td>32.1</td><td>62.7</td><td>74.1</td><td>3.0</td><td>-</td></tr><tr><th>AlignPrompt (Li et al. 2022)</th><td>33.9</td><td>60.7</td><td>73.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>All-in-one (Wang et al. 2022a)</th><td>34.4</td><td>65.4</td><td>75.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MDMMT (Dzabraev et al. 2021)</th><td>38.9</td><td>69.0</td><td>79.7</td><td>2.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th rowspan=\"7\">CLIP based</th><th>CLIP4Clip (Luo et al. 2021)</th><td>44.5</td><td>71.4</td><td>81.6</td><td>-</td><td>15.3</td><td>43.1</td><td>70.5</td><td>81.2</td><td>2.0</td><td>12.4</td></tr><tr><th>VCM (Cao et al. 2022)</th><td>43.8</td><td>71.0</td><td>80.9</td><td>2.0</td><td>14.3</td><td>45.1</td><td>72.3</td><td>82.3</td><td>2.0</td><td>10.7</td></tr><tr><th>MCQ (Ge et al. 2022a)</th><td>44.9</td><td>71.9</td><td>80.3</td><td>2.0</td><td>15.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MILES (Ge et al. 2022b)</th><td>44.3</td><td>71.1</td><td>80.8</td><td>2.0</td><td>14.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CAMoE (Cheng et al. 2021)</th><td>44.6</td><td>72.6</td><td>81.8</td><td>2.0</td><td>13.3</td><td>45.1</td><td>72.4</td><td>83.1</td><td>2.0</td><td>10.0</td></tr><tr><th>CLIP2Video (Fang et al. 2021)</th><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td><td>14.6</td><td>43.5</td><td>72.3</td><td>82.1</td><td>2.0</td><td>10.2</td></tr><tr><th>CLIP2TV (Gao et al. 2021)</th><td>46.1</td><td>72.5</td><td>82.9</td><td>2.0</td><td>15.2</td><td>43.9</td><td>70.9</td><td>82.2</td><td>2.0</td><td>12.0</td></tr><tr><th><p>Ours</p></th><th>MKTVR</th><td>46.6</td><td>72.6</td><td>82.2</td><td>2.0</td><td>13.9</td><td>45.5</td><td>73.4</td><td>84.7</td><td>2.0</td><td>8.07</td></tr></tbody></table>", "caption": "Table 1: Text-to-video and video-to-text retrieval results on MSR-VTT dataset 9k split. Recall at rank 1 (R@1)\\uparrow, rank 5 (R@5)\\uparrow, rank 10 (R@10)\\uparrow, Median Rank (MdR)\\downarrow and Mean Rank (MnR)\\downarrow are reported. Results of other methods taken from mentioned references. Our model surpasses previous state-of-the-art performance. In video-to-text retrieval, our model achieved 1.6 points boost in performance.", "list_citation_info": ["Cao et al. (2022) Cao, S.; Wang, B.; Zhang, W.; and Ma, L. 2022. Visual Consensus Modeling for Video-Text Retrieval.", "Bain et al. (2021) Bain, M.; Nagrani, A.; Varol, G.; and Zisserman, A. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1728\u20131738.", "Fang et al. (2021) Fang, H.; Xiong, P.; Xu, L.; and Chen, Y. 2021. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097.", "Xu et al. (2021) Xu, H.; Ghosh, G.; Huang, P.-Y.; Okhonko, D.; Aghajanyan, A.; Metze, F.; Zettlemoyer, L.; and Feichtenhofer, C. 2021. VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 6787\u20136800.", "Yu, Kim, and Kim (2018) Yu, Y.; Kim, J.; and Kim, G. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), 471\u2013487.", "Li et al. (2020) Li, L.; Chen, Y.-C.; Cheng, Y.; Gan, Z.; Yu, L.; and Liu, J. 2020. HERO: Hierarchical Encoder for Video+ Language Omni-representation Pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2046\u20132065.", "Cheng et al. (2021) Cheng, X.; Lin, H.; Wu, X.; Yang, F.; and Shen, D. 2021. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290.", "Liu et al. (2021) Liu, S.; Fan, H.; Qian, S.; Chen, Y.; Ding, W.; and Wang, Z. 2021. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 11915\u201311925.", "Miech et al. (2019) Miech, A.; Zhukov, D.; Alayrac, J.-B.; Tapaswi, M.; Laptev, I.; and Sivic, J. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2630\u20132640.", "Patrick et al. (2020) Patrick, M.; Huang, P.-Y.; Asano, Y.; Metze, F.; Hauptmann, A. G.; Henriques, J. F.; and Vedaldi, A. 2020. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations.", "Ge et al. (2022b) Ge, Y.; Ge, Y.; Liu, X.; Wang, A. J.; Wu, J.; Shan, Y.; Qie, X.; and Luo, P. 2022b. MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval. arXiv preprint arXiv:2204.12408.", "Li et al. (2022) Li, D.; Li, J.; Li, H.; Niebles, J. C.; and Hoi, S. C. 2022. Align and Prompt: Video-and-Language Pre-training with Entity Prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4953\u20134963.", "Ge et al. (2022a) Ge, Y.; Ge, Y.; Liu, X.; Li, D.; Shan, Y.; Qie, X.; and Luo, P. 2022a. Bridging Video-Text Retrieval With Multiple Choice Questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16167\u201316176.", "Lei et al. (2021) Lei, J.; Li, L.; Zhou, L.; Gan, Z.; Berg, T. L.; Bansal, M.; and Liu, J. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7331\u20137341.", "Wang et al. (2022a) Wang, A. J.; Ge, Y.; Yan, R.; Ge, Y.; Lin, X.; Cai, G.; Wu, J.; Shan, Y.; Qie, X.; and Shou, M. Z. 2022a. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303.", "Luo et al. (2021) Luo, H.; Ji, L.; Zhong, M.; Chen, Y.; Lei, W.; Duan, N.; and Li, T. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860.", "Liu et al. (2019) Liu, Y.; Albanie, S.; Nagrani, A.; and Zisserman, A. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Dzabraev et al. (2021) Dzabraev, M.; Kalashnikov, M.; Komkov, S.; and Petiushko, A. 2021. Mdmmt: Multidomain multimodal transformer for video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3354\u20133363.", "Gao et al. (2021) Gao, Z.; Liu, J.; Chen, S.; Chang, D.; Zhang, H.; and Yuan, J. 2021. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610.", "Radford et al. (2021) Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 8748\u20138763. PMLR."]}, {"table": "<table><thead><tr><th>Model</th><th>R@1 (\\uparrow)</th><th>R@5 (\\uparrow)</th><th>R@10(\\uparrow)</th><th>MdR (\\downarrow)</th></tr></thead><tbody><tr><th>HowTo100M (Miech et al. 2019)</th><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td></tr><tr><th>ActBERT (Zhu and Yang 2020)</th><td>8.6</td><td>23.4</td><td>33.1</td><td>36.0</td></tr><tr><th>NoiseE (Amrani et al. 2021)</th><td>17.4</td><td>41.6</td><td>53.6</td><td>8.0</td></tr><tr><th>ClipBERT (Lei et al. 2021)</th><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><th>CLIP4clip- (Luo et al. 2021)</th><td>42.1</td><td>71.9</td><td>81.4</td><td>2.0</td></tr><tr><th>Singularity (Lei, Berg, and Bansal 2022)</th><td>42.7</td><td>69.5</td><td>78.1</td><td>2.0</td></tr><tr><th>MKTVR</th><td>44.8</td><td>72.0</td><td>82.5</td><td>2.0</td></tr></tbody></table>", "caption": "Table 2: Text-to-video retrieval results on MSR-VTT - 7k split. Recall at rank-1 (R@1), rank-5 (R@5), rank-10 (R@10), Median Rank (MdR) are reported. Results of other methods taken from mentioned references.", "list_citation_info": ["Lei, Berg, and Bansal (2022) Lei, J.; Berg, T. L.; and Bansal, M. 2022. Revealing Single Frame Bias for Video-and-Language Learning. arXiv preprint arXiv:2206.03428.", "Luo et al. (2021) Luo, H.; Ji, L.; Zhong, M.; Chen, Y.; Lei, W.; Duan, N.; and Li, T. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860.", "Miech et al. (2019) Miech, A.; Zhukov, D.; Alayrac, J.-B.; Tapaswi, M.; Laptev, I.; and Sivic, J. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2630\u20132640.", "Zhu and Yang (2020) Zhu, L.; and Yang, Y. 2020. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 8746\u20138755.", "Lei et al. (2021) Lei, J.; Li, L.; Zhou, L.; Gan, Z.; Berg, T. L.; Bansal, M.; and Liu, J. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7331\u20137341.", "Amrani et al. (2021) Amrani, E.; Ben-Ari, R.; Rotman, D.; and Bronstein, A. 2021. Noise estimation using density estimation for self-supervised multimodal learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 6644\u20136652."]}, {"table": "<table><thead><tr><th>Model</th><th>R@1 (\\uparrow)</th><th>R@5 (\\uparrow)</th><th>R@10(\\uparrow)</th><th>MdR (\\downarrow)</th></tr></thead><tbody><tr><th>VSE (Fang et al. 2021)</th><td>12.3</td><td>30.1</td><td>42.3</td><td>14.0</td></tr><tr><th>CE (Liu et al. 2019)</th><td>19.8</td><td>49.0</td><td>63.8</td><td>6.0</td></tr><tr><th>SSML (Amrani et al. 2021)</th><td>20.3</td><td>49.0</td><td>63.3</td><td>6.0</td></tr><tr><th>SUPPORT-SET (Patrick et al. 2020)</th><td>28.4</td><td>60.0</td><td>72.9</td><td>4.0</td></tr><tr><th>FROZEN (Bain et al. 2021)</th><td>33.7</td><td>64.7</td><td>76.3</td><td>3.0</td></tr><tr><th>CLIP (Radford et al. 2021)</th><td>37.0</td><td>64.1</td><td>73.8</td><td>3.0</td></tr><tr><th>CLIP4Clip (Luo et al. 2021)</th><td>46.2</td><td>76.1</td><td>84.6</td><td>2.0</td></tr><tr><th>CLIP2Video (Fang et al. 2021)</th><td>47.0</td><td>76.8</td><td>85.9</td><td>2.0</td></tr><tr><th>MKTVR</th><td>47.2</td><td>77.3</td><td>86.2</td><td>2.0</td></tr></tbody></table>", "caption": "Table 3: Text-to-video retrieval results on MSVD dataset. Recall at rank-1 (R@1), rank-5 (R@5), rank-10 (R@10), Median Rank (MdR) are reported. Results of other methods taken from mentioned references.", "list_citation_info": ["Luo et al. (2021) Luo, H.; Ji, L.; Zhong, M.; Chen, Y.; Lei, W.; Duan, N.; and Li, T. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860.", "Bain et al. (2021) Bain, M.; Nagrani, A.; Varol, G.; and Zisserman, A. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1728\u20131738.", "Liu et al. (2019) Liu, Y.; Albanie, S.; Nagrani, A.; and Zisserman, A. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Patrick et al. (2020) Patrick, M.; Huang, P.-Y.; Asano, Y.; Metze, F.; Hauptmann, A. G.; Henriques, J. F.; and Vedaldi, A. 2020. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations.", "Radford et al. (2021) Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 8748\u20138763. PMLR.", "Fang et al. (2021) Fang, H.; Xiong, P.; Xu, L.; and Chen, Y. 2021. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097.", "Amrani et al. (2021) Amrani, E.; Ben-Ari, R.; Rotman, D.; and Bronstein, A. 2021. Noise estimation using density estimation for self-supervised multimodal learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 6644\u20136652."]}, {"table": "<table><thead><tr><th>Model</th><th>R@1 (\\uparrow)</th><th>R@5 (\\uparrow)</th><th>R@10 (\\uparrow)</th><th>MdR (\\downarrow)</th></tr></thead><tbody><tr><th>S2VT (Venugopalan et al. 2015)</th><td>11.9</td><td>33.6</td><td>-</td><td>13.0</td></tr><tr><th>FSE (Zhang, Hu, and Sha 2018)</th><td>13.9</td><td>36</td><td>-</td><td>11.0</td></tr><tr><th>CE (Liu et al. 2019)</th><td>16.1</td><td>41.1</td><td>-</td><td>8.3</td></tr><tr><th>ClipBERT (Lei et al. 2021)</th><td>20.4</td><td>48.0</td><td>60.8</td><td>6.0</td></tr><tr><th>FrozenInTime (Bain et al. 2021)</th><td>31.0</td><td>59.8</td><td>72.4</td><td>3.0</td></tr><tr><th>OA-Trans (Wang et al. 2022b)</th><td>34.8</td><td>64.4</td><td>75.1</td><td>3.0</td></tr><tr><th>CLIP4clip (Luo et al. 2021)</th><td>43.4</td><td>70.2</td><td>80.6</td><td>2.0</td></tr><tr><th>CLIP2TV (Gao et al. 2021)</th><td>43.9</td><td>70.5</td><td>79.8</td><td>2.0</td></tr><tr><th>TS2-Net (Liu et al. 2022)</th><td>41.8</td><td>71.6</td><td>82.0</td><td>2.0</td></tr><tr><th>ECLIPSE (Lin et al. 2022)</th><td>44.2</td><td>70.0</td><td>80.2</td><td>2.0</td></tr><tr><th>MKTVR</th><td>44.4</td><td>74.3</td><td>83.1</td><td>2.0</td></tr></tbody></table>", "caption": "Table 4: Text-to-video retrieval result on DiDeMo dataset. Recall at rank-1 (R@1), rank-5 (R@5), rank-10 (R@10), Median Rank (MdR) are reported. Results of other methods taken from mentioned references.", "list_citation_info": ["Wang et al. (2022b) Wang, J.; Ge, Y.; Cai, G.; Yan, R.; Lin, X.; Shan, Y.; Qie, X.; and Shou, M. Z. 2022b. Object-aware Video-language Pre-training for Retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3313\u20133322.", "Luo et al. (2021) Luo, H.; Ji, L.; Zhong, M.; Chen, Y.; Lei, W.; Duan, N.; and Li, T. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860.", "Bain et al. (2021) Bain, M.; Nagrani, A.; Varol, G.; and Zisserman, A. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1728\u20131738.", "Venugopalan et al. (2015) Venugopalan, S.; Rohrbach, M.; Donahue, J.; Mooney, R.; Darrell, T.; and Saenko, K. 2015. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision, 4534\u20134542.", "Zhang, Hu, and Sha (2018) Zhang, B.; Hu, H.; and Sha, F. 2018. Cross-modal and hierarchical modeling of video and text. In Proceedings of the european conference on computer vision (ECCV), 374\u2013390.", "Liu et al. (2019) Liu, Y.; Albanie, S.; Nagrani, A.; and Zisserman, A. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.", "Gao et al. (2021) Gao, Z.; Liu, J.; Chen, S.; Chang, D.; Zhang, H.; and Yuan, J. 2021. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610.", "Liu et al. (2022) Liu, Y.; Xiong, P.; Xu, L.; Cao, S.; and Jin, Q. 2022. TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval. arXiv preprint arXiv:2207.07852.", "Lin et al. (2022) Lin, Y.-B.; Lei, J.; Bansal, M.; and Bertasius, G. 2022. ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. arXiv preprint arXiv:2204.02874.", "Lei et al. (2021) Lei, J.; Li, L.; Zhou, L.; Gan, Z.; Berg, T. L.; Bansal, M.; and Liu, J. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7331\u20137341."]}, {"table": "<table><thead><tr><th>Model</th><th>R@1 (\\uparrow)</th><th>R@5 (\\uparrow)</th><th>R@10 (\\uparrow)</th><th>MdR</th><th>MnR</th></tr></thead><tbody><tr><th>ClipBERT (Lei et al. 2021)</th><td>6.7</td><td>17.3</td><td>25.2</td><td>32.0</td><td>149.7</td></tr><tr><th>FrozenInTime (Bain et al. 2021)</th><td>11.9</td><td>28.3</td><td>35.1</td><td>17.0</td><td>103.8</td></tr><tr><th>CLIP4clip (Luo et al. 2021)</th><td>13.9</td><td>30.4</td><td>37.1</td><td>14.0</td><td>98.0</td></tr><tr><th>ECLIPSE (Lin et al. 2022)</th><td>15.7</td><td>32.9</td><td>42.4</td><td>16.0</td><td>84.9</td></tr><tr><th>MKTVR</th><td>16.6</td><td>37.5</td><td>50.0</td><td>10.0</td><td>52.7</td></tr></tbody></table>", "caption": "Table 5: Text-to-video retrieval result on charades dataset. Recall at rank-1 (R@1), rank-5 (R@5), rank-10 (R@10), Median Rank (MdR) are reported. Results reported are taken from (Lin et al. 2022).", "list_citation_info": ["Bain et al. (2021) Bain, M.; Nagrani, A.; Varol, G.; and Zisserman, A. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1728\u20131738.", "Lin et al. (2022) Lin, Y.-B.; Lei, J.; Bansal, M.; and Bertasius, G. 2022. ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. arXiv preprint arXiv:2204.02874.", "Lei et al. (2021) Lei, J.; Li, L.; Zhou, L.; Gan, Z.; Berg, T. L.; Bansal, M.; and Liu, J. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7331\u20137341.", "Luo et al. (2021) Luo, H.; Ji, L.; Zhong, M.; Chen, Y.; Lei, W.; Duan, N.; and Li, T. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860."]}, {"table": "<table><thead><tr><th>Model</th><th>de</th><th>cs</th><th>zh</th><th>ru</th><th>sw</th><th>es</th></tr></thead><tbody><tr><td>m-BERT (zero-shot)</td><td>11.1</td><td>8.2</td><td>6.9</td><td>7.9</td><td>1.4</td><td>12</td></tr><tr><td>m-BERT MMP (zero-shot)</td><td>15</td><td>11.2</td><td>8.4</td><td>11</td><td>3.4</td><td>15.1</td></tr><tr><td>XLM-R (zero-shot)</td><td>16.3</td><td>16</td><td>14.9</td><td>15.4</td><td>7.7</td><td>17.3</td></tr><tr><td>XLM-MMP (zero-shot)</td><td>19.4</td><td>19.3</td><td>18.2</td><td>19.1</td><td>8.4</td><td>20.4</td></tr><tr><td>m-BERT (fine-tune)</td><td>18.2</td><td>16.9</td><td>16.2</td><td>16.5</td><td>13</td><td>18.5</td></tr><tr><td>XLM- R + MMP (fine-tune)</td><td>21.1</td><td>20.7</td><td>20</td><td>20.5</td><td>14.4</td><td>21.9</td></tr><tr><td>MKTVR - fr (zero-shot)</td><td>27.4</td><td>28.2</td><td>24.1</td><td>26.6</td><td>22.5</td><td>26.5</td></tr></tbody></table>", "caption": "Table 6: Text-to-video retrieval (R@1 metric) results on MSR-VTT - multilingual (Huang et al. 2021). Results of other methods taken from (Huang et al. 2021). Our model is trained on Charades dataset and using only french language and evaluated in a zero-shot setting on MSRVTT multilingual dataset. In zero-shot evaluation on other languages, our model significantly outperforms previous models trained in both zero-shot and fine-tuning setting.", "list_citation_info": ["Huang et al. (2021) Huang, P.-Y.; Patrick, M.; Hu, J.; Neubig, G.; Metze, F.; and Hauptmann, A. G. 2021. Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2443\u20132459."]}], "citation_info_to_title": {"Zhu and Yang (2020) Zhu, L.; and Yang, Y. 2020. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 8746\u20138755.": "Actbert: Learning global-local video-text representations", "Lei, Berg, and Bansal (2022) Lei, J.; Berg, T. L.; and Bansal, M. 2022. Revealing Single Frame Bias for Video-and-Language Learning. arXiv preprint arXiv:2206.03428.": "Revealing Single Frame Bias for Video-and-Language Learning", "Dzabraev et al. (2021) Dzabraev, M.; Kalashnikov, M.; Komkov, S.; and Petiushko, A. 2021. Mdmmt: Multidomain multimodal transformer for video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3354\u20133363.": "Mdmmt: Multidomain Multimodal Transformer for Video Retrieval", "Radford et al. (2021) Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 8748\u20138763. PMLR.": "Learning transferable visual models from natural language supervision", "Bain et al. (2021) Bain, M.; Nagrani, A.; Varol, G.; and Zisserman, A. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1728\u20131738.": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "Li et al. (2022) Li, D.; Li, J.; Li, H.; Niebles, J. C.; and Hoi, S. C. 2022. Align and Prompt: Video-and-Language Pre-training with Entity Prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4953\u20134963.": "Align and Prompt: Video-and-Language Pre-training with Entity Prompts", "Ge et al. (2022b) Ge, Y.; Ge, Y.; Liu, X.; Wang, A. J.; Wu, J.; Shan, Y.; Qie, X.; and Luo, P. 2022b. MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval. arXiv preprint arXiv:2204.12408.": "MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval", "Cao et al. (2022) Cao, S.; Wang, B.; Zhang, W.; and Ma, L. 2022. Visual Consensus Modeling for Video-Text Retrieval.": "Visual Consensus Modeling for Video-Text Retrieval", "Wang et al. (2022a) Wang, A. J.; Ge, Y.; Yan, R.; Ge, Y.; Lin, X.; Cai, G.; Wu, J.; Shan, Y.; Qie, X.; and Shou, M. Z. 2022a. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303.": "All in one: Exploring unified video-language pre-training", "Fang et al. (2021) Fang, H.; Xiong, P.; Xu, L.; and Chen, Y. 2021. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097.": "Clip2video: Mastering video-text retrieval via image clip", "Liu et al. (2019) Liu, Y.; Albanie, S.; Nagrani, A.; and Zisserman, A. 2019. Use what you have: Video retrieval using representations from collaborative experts. arXiv preprint arXiv:1907.13487.": "Use what you have: Video retrieval using representations from collaborative experts", "Gao et al. (2021) Gao, Z.; Liu, J.; Chen, S.; Chang, D.; Zhang, H.; and Yuan, J. 2021. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610.": "Clip2tv: An Empirical Study on Transformer-Based Methods for Video-Text Retrieval", "Liu et al. (2022) Liu, Y.; Xiong, P.; Xu, L.; Cao, S.; and Jin, Q. 2022. TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval. arXiv preprint arXiv:2207.07852.": "TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval", "Lei et al. (2021) Lei, J.; Li, L.; Zhou, L.; Gan, Z.; Berg, T. L.; Bansal, M.; and Liu, J. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7331\u20137341.": "Less is more: Clipbert for video-and-language learning via sparse sampling", "Liu et al. (2021) Liu, S.; Fan, H.; Qian, S.; Chen, Y.; Ding, W.; and Wang, Z. 2021. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 11915\u201311925.": "Hit: Hierarchical transformer with momentum contrast for video-text retrieval", "Li et al. (2020) Li, L.; Chen, Y.-C.; Cheng, Y.; Gan, Z.; Yu, L.; and Liu, J. 2020. HERO: Hierarchical Encoder for Video+ Language Omni-representation Pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2046\u20132065.": "HERO: Hierarchical Encoder for Video+ Language Omni-representation Pre-training", "Ge et al. (2022a) Ge, Y.; Ge, Y.; Liu, X.; Li, D.; Shan, Y.; Qie, X.; and Luo, P. 2022a. Bridging Video-Text Retrieval With Multiple Choice Questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16167\u201316176.": "Bridging Video-Text Retrieval With Multiple Choice Questions", "Miech et al. (2019) Miech, A.; Zhukov, D.; Alayrac, J.-B.; Tapaswi, M.; Laptev, I.; and Sivic, J. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2630\u20132640.": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips", "Amrani et al. (2021) Amrani, E.; Ben-Ari, R.; Rotman, D.; and Bronstein, A. 2021. Noise estimation using density estimation for self-supervised multimodal learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 6644\u20136652.": "Noise estimation using density estimation for self-supervised multimodal learning", "Lin et al. (2022) Lin, Y.-B.; Lei, J.; Bansal, M.; and Bertasius, G. 2022. ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound. arXiv preprint arXiv:2204.02874.": "ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound", "Xu et al. (2021) Xu, H.; Ghosh, G.; Huang, P.-Y.; Okhonko, D.; Aghajanyan, A.; Metze, F.; Zettlemoyer, L.; and Feichtenhofer, C. 2021. VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 6787\u20136800.": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding", "Wang et al. (2022b) Wang, J.; Ge, Y.; Cai, G.; Yan, R.; Lin, X.; Shan, Y.; Qie, X.; and Shou, M. Z. 2022b. Object-aware Video-language Pre-training for Retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3313\u20133322.": "Object-aware Video-language Pre-training for Retrieval", "Cheng et al. (2021) Cheng, X.; Lin, H.; Wu, X.; Yang, F.; and Shen, D. 2021. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290.": "Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss", "Zhang, Hu, and Sha (2018) Zhang, B.; Hu, H.; and Sha, F. 2018. Cross-modal and hierarchical modeling of video and text. In Proceedings of the european conference on computer vision (ECCV), 374\u2013390.": "Cross-modal and hierarchical modeling of video and text", "Venugopalan et al. (2015) Venugopalan, S.; Rohrbach, M.; Donahue, J.; Mooney, R.; Darrell, T.; and Saenko, K. 2015. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision, 4534\u20134542.": "Sequence to sequence-video to text", "Patrick et al. (2020) Patrick, M.; Huang, P.-Y.; Asano, Y.; Metze, F.; Hauptmann, A. G.; Henriques, J. F.; and Vedaldi, A. 2020. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations.": "Support-set bottlenecks for video-text representation learning", "Yu, Kim, and Kim (2018) Yu, Y.; Kim, J.; and Kim, G. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), 471\u2013487.": "A joint sequence fusion model for video question answering and retrieval", "Luo et al. (2021) Luo, H.; Ji, L.; Zhong, M.; Chen, Y.; Lei, W.; Duan, N.; and Li, T. 2021. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860.": "Clip4clip: An Empirical Study of Clip for End to End Video Clip Retrieval", "Huang et al. (2021) Huang, P.-Y.; Patrick, M.; Hu, J.; Neubig, G.; Metze, F.; and Hauptmann, A. G. 2021. Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2443\u20132459.": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models"}, "source_title_to_arxiv_id": {"Frozen in time: A joint video and image encoder for end-to-end retrieval": "2104.00650", "Align and Prompt: Video-and-Language Pre-training with Entity Prompts": "2112.09583", "TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval": "2207.07852", "Clip4clip: An Empirical Study of Clip for End to End Video Clip Retrieval": "2104.08860"}}