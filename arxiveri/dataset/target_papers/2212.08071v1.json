{"title": "MAViL: Masked Audio-Video Learners", "abstract": "We present Masked Audio-Video Learners (MAViL) to train audio-visual\nrepresentations. Our approach learns with three complementary forms of\nself-supervision: (1) reconstruction of masked audio and video input data, (2)\nintra- and inter-modal contrastive learning with masking, and (3) self-training\nby reconstructing joint audio-video contextualized features learned from the\nfirst two objectives. Pre-training with MAViL not only enables the model to\nperform well in audio-visual classification and retrieval tasks but also\nimproves representations of each modality in isolation, without using\ninformation from the other modality for fine-tuning or inference. Empirically,\nMAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%\naccuracy). For the first time, a self-supervised audio-visual model outperforms\nones that use external supervision on these benchmarks. Code will be available\nsoon.", "authors": ["Po-Yao Huang", "Vasu Sharma", "Hu Xu", "Chaitanya Ryali", "Haoqi Fan", "Yanghao Li", "Shang-Wen Li", "Gargi Ghosh", "Jitendra Malik", "Christoph Feichtenhofer"], "published_date": "2022_12_15", "pdf_url": "http://arxiv.org/pdf/2212.08071v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td></td><td colspan=\"3\">AS-20K</td><td colspan=\"3\">AS-2M</td></tr><tr><th>Method</th><td>PT</td><td>A</td><td>V</td><td>A+V</td><td>A</td><td>V</td><td>A+V</td></tr><tr><th colspan=\"8\">Audio-only Models</th></tr><tr><th>PANNs (Kong et al., 2020)</th><td>-</td><td>27.8</td><td>-</td><td>-</td><td>43.9</td><td>-</td><td>-</td></tr><tr><th>AST (Gong et al., 2021)</th><td>SL</td><td>34.7</td><td>-</td><td>-</td><td>45.9</td><td>-</td><td>-</td></tr><tr><th>HTS-AT (Chen et al., 2022)</th><td>SL</td><td>-</td><td>-</td><td>-</td><td>47.1</td><td>-</td><td>-</td></tr><tr><th>PaSST (Koutini et al., 2021)</th><td>SL</td><td>-</td><td>-</td><td>-</td><td>47.1</td><td>-</td><td>-</td></tr><tr><th>Data2vec (Baevski et al., 2022)</th><td>SSL</td><td>34.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SSAST (Gong et al., 2022a)</th><td>SSL</td><td>31.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MAE-AST (Baade et al., 2022)</th><td>SSL</td><td>30.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Aud-MAE(Huang et al., 2022a)</th><td>SSL</td><td>37.0</td><td>-</td><td>-</td><td>47.3</td><td>-</td><td>-</td></tr><tr><th colspan=\"8\">Audio-Video Models</th></tr><tr><th>G-Blend (Wang et al., 2020)</th><td>-</td><td>29.1</td><td>22.1</td><td>37.8</td><td>32.4</td><td>18.8</td><td>41.8</td></tr><tr><th>Perceiver (Jaegle et al., 2021)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>38.4</td><td>25.8</td><td>44.2</td></tr><tr><th>CAV-MAE<sup>\\dagger</sup> (Gong et al., 2022b)</th><td>SSL</td><td>37.7</td><td>19.8</td><td>42.0</td><td>46.6</td><td>26.2</td><td>51.5</td></tr><tr><th>Attn AV (Fayek &amp; Kumar, 2020)</th><td>SL</td><td>-</td><td>-</td><td>-</td><td>38.4</td><td>25.7</td><td>46.2</td></tr><tr><th>MBT <sup>*</sup>(Nagrani et al., 2021)</th><td>SL</td><td>31.3</td><td>27.7</td><td>43.9</td><td>44.3</td><td>32.3</td><td>52.1</td></tr><tr><th colspan=\"8\">Our Audio-Video Models</th></tr><tr><th>MAViL-Stage1</th><td>SSL</td><td>39.0</td><td>22.2</td><td>42.5</td><td>47.4</td><td>28.1</td><td>51.8</td></tr><tr><th>MAViL</th><td>SSL</td><td>40.7</td><td>24.1</td><td>44.3</td><td>48.4</td><td>30.2</td><td>53.1</td></tr></tbody></table>", "caption": "Table 6: Comparison to prior works on AudioSet (AS). mAP\\uparrow is reported for audio (A), video (V) and audio+video (A+V) classification. MAViL uses ViT-B encoders for each modality. Models are pre-trained on AS-2M and fine-tuned on either AS-20K or AS-2M.PT: pre-training type; SL: (ImageNet) supervised learning; SSL: self-supervised learning;<sup>\\dagger</sup>:concurrent work.<sup>*</sup>We de-emphasize the model with non-standard training/testing split.We underline the best model and bold the best self-supervised model.Table 7-9 follow the same notations. ", "list_citation_info": ["Gong et al. (2022b) Gong, Y., Rouditchenko, A., Liu, A. H., Harwath, D., Karlinsky, L., Kuehne, H., and Glass, J. Contrastive audio-visual masked autoencoder. arXiv preprint arXiv:2210.07839, 2022b.", "Fayek & Kumar (2020) Fayek, H. M. and Kumar, A. Large scale audiovisual learning of sounds with weakly labeled data. In IJCAI, 2020.", "Gong et al. (2022a) Gong, Y., Lai, C.-I., Chung, Y.-A., and Glass, J. R. SSAST: Self-Supervised Audio Spectrogram Transformer. In AAAI, 2022a.", "Baevski et al. (2022) Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M. Data2vec: A general framework for self-supervised learning in speech, vision and language. In ICML, 2022.", "Nagrani et al. (2021) Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., and Sun, C. Attention bottlenecks for multimodal fusion. In NeurIPS, 2021.", "Huang et al. (2022a) Huang, P.-Y., Xu, H., Li, J., Baevski, A., Auli, M., Galuba, W., Metze, F., Feichtenhofer, C., et al. Masked autoencoders that listen. In NeurIPS, 2022a.", "Kong et al. (2020) Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., and Plumbley, M. D. PANNs: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:2880\u20132894, 2020.", "Chen et al. (2022) Chen, K., Du, X., Zhu, B., Ma, Z., Berg-Kirkpatrick, T., and Dubnov, S. HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection. In ICASSP, 2022.", "Gong et al. (2021) Gong, Y., Chung, Y.-A., and Glass, J. AST: Audio Spectrogram Transformer. In Interspeech, 2021.", "Koutini et al. (2021) Koutini, K., Schl\u00fcter, J., Eghbal-zadeh, H., and Widmer, G. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021.", "Wang et al. (2020) Wang, W., Tran, D., and Feiszli, M. What makes training multi-modal classification networks hard? In CVPR, 2020.", "Baade et al. (2022) Baade, A., Peng, P., and Harwath, D. MAE-AST: Masked autoencoding audio spectrogram transformer. In Interspeech, 2022.", "Jaegle et al. (2021) Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In ICML, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><td>PT</td><td>A</td><td>V</td><td>A+V</td></tr><tr><th>CAV-MAE<sup>\\dagger</sup>(Gong et al., 2022b)</th><td>SSL</td><td>59.5</td><td>47.0</td><td>65.5</td></tr><tr><th>AV-SlowFast (Xiao et al., 2020)</th><td>SL</td><td>50.1</td><td>-</td><td>-</td></tr><tr><th>VGGSound (Chen et al., 2020)</th><td>SL</td><td>48.8</td><td>-</td><td>-</td></tr><tr><th>MBT (Nagrani et al., 2021)</th><td>SL</td><td>52.3</td><td>51.2</td><td>64.1</td></tr><tr><th>MAViL-Stage1</th><td>SSL</td><td>59.9</td><td>48.3</td><td>63.8</td></tr><tr><th>MAViL</th><td>SSL</td><td>60.8</td><td>50.9</td><td>67.1</td></tr></tbody></table>", "caption": "Table 7: VGGSound results (Accuracy\\uparrow).", "list_citation_info": ["Chen et al. (2020) Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. VGGSound: A large-scale audio-visual dataset. In ICASSP, 2020.", "Xiao et al. (2020) Xiao, F., Lee, Y. J., Grauman, K., Malik, J., and Feichtenhofer, C. Audiovisual slowfast networks for video recognition. arXiv preprint arXiv:2001.08740, 2020.", "Gong et al. (2022b) Gong, Y., Rouditchenko, A., Liu, A. H., Harwath, D., Karlinsky, L., Kuehne, H., and Glass, J. Contrastive audio-visual masked autoencoder. arXiv preprint arXiv:2210.07839, 2022b.", "Nagrani et al. (2021) Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., and Sun, C. Attention bottlenecks for multimodal fusion. In NeurIPS, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>PT</th><th>ESC-50</th><th>SPC-1</th></tr></thead><tbody><tr><th>AST (Gong et al., 2021)</th><td>SL</td><td>88.7</td><td>95.5</td></tr><tr><th>SS-AST (Gong et al., 2022a)</th><td>SSL</td><td>88.8</td><td>96.0</td></tr><tr><th>MAE-AST (Baade et al., 2022)</th><td>SSL</td><td>90.0</td><td>95.8</td></tr><tr><th>Aud-MAE (Huang et al., 2022a)</th><td>SSL</td><td>94.1</td><td>96.9</td></tr><tr><th>MAViL</th><td>SSL</td><td>94.3</td><td>97.4</td></tr></tbody></table>", "caption": "Table 8: Comparison on audio-only tasks (Accuracy\\uparrow).", "list_citation_info": ["Gong et al. (2022a) Gong, Y., Lai, C.-I., Chung, Y.-A., and Glass, J. R. SSAST: Self-Supervised Audio Spectrogram Transformer. In AAAI, 2022a.", "Baade et al. (2022) Baade, A., Peng, P., and Harwath, D. MAE-AST: Masked autoencoding audio spectrogram transformer. In Interspeech, 2022.", "Huang et al. (2022a) Huang, P.-Y., Xu, H., Li, J., Baevski, A., Auli, M., Galuba, W., Metze, F., Feichtenhofer, C., et al. Masked autoencoders that listen. In NeurIPS, 2022a.", "Gong et al. (2021) Gong, Y., Chung, Y.-A., and Glass, J. AST: Audio Spectrogram Transformer. In Interspeech, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>PT data</th><th>MSR-VTT</th><th>YouCook</th></tr></thead><tbody><tr><th>AVLNet (Rouditchenko et al., 2021)</th><td>HT100M</td><td>20.1</td><td>30.7</td></tr><tr><th>TVLT  (Tang et al., 2022)</th><td>HT100M</td><td>22.6</td><td>31.8</td></tr><tr><th>MAViL</th><td>AS-2M</td><td>22.2</td><td>31.3</td></tr><tr><th>MAViL</th><td>HT-100M</td><td>23.5</td><td>33.0</td></tr></tbody></table>", "caption": "Table 9: Comparison on audio-to-video retrieval tasks in VTT and YouCook. (Recall@1\\uparrow)", "list_citation_info": ["Rouditchenko et al. (2021) Rouditchenko, A., Boggust, A., Harwath, D., Chen, B., Joshi, D., Thomas, S., Audhkhasi, K., Kuehne, H., Panda, R., Feris, R., et al. AVLnet: Learning audio-visual language representations from instructional videos. In Interspeech, 2021.", "Tang et al. (2022) Tang, Z., Cho, J., Nie, Y., and Bansal, M. TVLT: Textless Vision-Language Transformer. In NeurIPS, 2022."]}, {"table": "<table><tbody><tr><th></th><th>Pre-training</th><td colspan=\"5\">Fine-tuning</td></tr><tr><th>Configuration</th><th>AS-2M PT</th><td>AS-2M</td><td>AS-20K</td><td>VGGSound</td><td>ESC</td><td>SPC</td></tr><tr><th>Optimizer</th><th colspan=\"6\">AdamW (Loshchilov &amp; Hutter, 2019)</th></tr><tr><th>Optimizer momentum</th><th colspan=\"6\">\\beta_{1}=0.9, \\beta_{2}=0.95</th></tr><tr><th>Weight decay</th><th colspan=\"6\">0.00001</th></tr><tr><th>Base learning rate</th><th>0.0002</th><td>0.0002<sup>\\dagger</sup></td><td>0.001</td><td>0.0002</td><td>0.001</td><td>0.001</td></tr><tr><th>Learning rate schedule</th><th colspan=\"6\">half-cycle cosine decay (Loshchilov &amp; Hutter, 2017)</th></tr><tr><th>Minimum learning rate</th><th colspan=\"6\">0.000001</th></tr><tr><th>Gradient clipping</th><th colspan=\"6\">None</th></tr><tr><th>Warm-up epochs</th><th>4</th><td>20</td><td>4</td><td>4</td><td>4</td><td>1</td></tr><tr><th>Epochs</th><th>20</th><td>100</td><td>60</td><td>60</td><td>60</td><td>10</td></tr><tr><th>Batch size</th><th>512</th><td>512</td><td>64</td><td>256</td><td>64</td><td>256</td></tr><tr><th>GPUs</th><th>64</th><td>64</td><td>8</td><td>32</td><td>4</td><td>4</td></tr><tr><th>Weighted sampling</th><th>False</th><td>True</td><td>False</td><td>True</td><td>False</td><td>False<sup>*</sup></td></tr><tr><th>Weighted sampling size</th><th>-</th><td>200,000</td><td>-</td><td>200,000</td><td>-</td><td>-</td></tr><tr><th>Augmentation</th><th>R</th><td>R</td><td>R</td><td>R+N</td><td>R</td><td>R+N</td></tr><tr><th>SpecAug (Park et al., 2019a) (time/frequency)</th><th>-</th><td>192/48</td><td>192/48</td><td>192/48</td><td>96/24</td><td>48/48</td></tr><tr><th>Drop path (Larsson et al., 2017)</th><th>0.0</th><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td><td>0.1</td></tr><tr><th>Mixup (Zhang et al., 2018)</th><th>0.0</th><td>0.5</td><td>0.5</td><td>0.5</td><td>0.0</td><td>0.5</td></tr><tr><th>Multilabel</th><th>n/a</th><td>True</td><td>True</td><td>False</td><td>False</td><td>False</td></tr><tr><th>Loss Function</th><th>MSE</th><td>BCE</td><td>BCE</td><td>BCE</td><td>CE</td><td>BCE</td></tr><tr><th>Dataset Mean for Normalization</th><th>-4.268</th><td>-4.268</td><td>-4.268</td><td>-5.189</td><td>-6.627</td><td>-6.702</td></tr><tr><th>Dataset Std for Normalization</th><th>4.569</th><td>4.569</td><td>4.569</td><td>3.260</td><td>5.359</td><td>5.448</td></tr></tbody></table>", "caption": "Table 10: Pre-training (PT) and Fine-tuning (FT) hyperparameters. For augmentation, R: sampling random starting points with cyclic rolling in time; N: adding random noise (signal-to-noise ratio (SNR): 20dB) to spectrograms. For loss functions, BCE: binary cross entropy loss (for multi-label datasets or when using mixup); CE: cross-entropy loss, MSE: mean square error loss.<sup>*</sup>: We repeat and balance each class to 50% of the size of the unknown class.<sup>\\dagger</sup>: For ViT-S, We use a learning rate of 0.0005 on AS-2M FT and 0.002 on AS-20K FT as we find larger learning rates work better for ViT-S encoder in the ablation study.", "list_citation_info": ["Park et al. (2019a) Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. Specaugment: A simple data augmentation method for automatic speech recognition. ArXiv, abs/1904.08779, 2019a.", "Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR, 2019.", "Larsson et al. (2017) Larsson, G., Maire, M., and Shakhnarovich, G. FractalNet: Ultra-deep neural networks without residuals. In ICLR, 2017.", "Zhang et al. (2018) Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. In ICLR, 2018.", "Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017."]}], "citation_info_to_title": {"Larsson et al. (2017) Larsson, G., Maire, M., and Shakhnarovich, G. FractalNet: Ultra-deep neural networks without residuals. In ICLR, 2017.": "FractalNet: Ultra-deep neural networks without residuals", "Wang et al. (2020) Wang, W., Tran, D., and Feiszli, M. What makes training multi-modal classification networks hard? In CVPR, 2020.": "What makes training multi-modal classification networks hard?", "Chen et al. (2020) Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. VGGSound: A large-scale audio-visual dataset. In ICASSP, 2020.": "VGGSound: A large-scale audio-visual dataset", "Koutini et al. (2021) Koutini, K., Schl\u00fcter, J., Eghbal-zadeh, H., and Widmer, G. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021.": "Efficient training of audio transformers with patchout", "Jaegle et al. (2021) Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General perception with iterative attention. In ICML, 2021.": "Perceiver: General perception with iterative attention", "Nagrani et al. (2021) Nagrani, A., Yang, S., Arnab, A., Jansen, A., Schmid, C., and Sun, C. Attention bottlenecks for multimodal fusion. In NeurIPS, 2021.": "Attention Bottlenecks for Multimodal Fusion", "Rouditchenko et al. (2021) Rouditchenko, A., Boggust, A., Harwath, D., Chen, B., Joshi, D., Thomas, S., Audhkhasi, K., Kuehne, H., Panda, R., Feris, R., et al. AVLnet: Learning audio-visual language representations from instructional videos. In Interspeech, 2021.": "AVLnet: Learning audio-visual language representations from instructional videos", "Loshchilov & Hutter (2019) Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR, 2019.": "Decoupled weight decay regularization", "Gong et al. (2022a) Gong, Y., Lai, C.-I., Chung, Y.-A., and Glass, J. R. SSAST: Self-Supervised Audio Spectrogram Transformer. In AAAI, 2022a.": "SSAST: Self-Supervised Audio Spectrogram Transformer", "Gong et al. (2021) Gong, Y., Chung, Y.-A., and Glass, J. AST: Audio Spectrogram Transformer. In Interspeech, 2021.": "AST: Audio Spectrogram Transformer", "Kong et al. (2020) Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., and Plumbley, M. D. PANNs: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:2880\u20132894, 2020.": "PANNs: Large-scale pretrained audio neural networks for audio pattern recognition", "Loshchilov & Hutter (2017) Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017.": "SGDR: Stochastic gradient descent with warm restarts", "Baevski et al. (2022) Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M. Data2vec: A general framework for self-supervised learning in speech, vision and language. In ICML, 2022.": "Data2vec: A general framework for self-supervised learning in speech, vision and language", "Huang et al. (2022a) Huang, P.-Y., Xu, H., Li, J., Baevski, A., Auli, M., Galuba, W., Metze, F., Feichtenhofer, C., et al. Masked autoencoders that listen. In NeurIPS, 2022a.": "Masked autoencoders that listen", "Zhang et al. (2018) Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. mixup: Beyond empirical risk minimization. In ICLR, 2018.": "mixup: Beyond Empirical Risk Minimization", "Xiao et al. (2020) Xiao, F., Lee, Y. J., Grauman, K., Malik, J., and Feichtenhofer, C. Audiovisual slowfast networks for video recognition. arXiv preprint arXiv:2001.08740, 2020.": "Audiovisual Slowfast Networks for Video Recognition", "Chen et al. (2022) Chen, K., Du, X., Zhu, B., Ma, Z., Berg-Kirkpatrick, T., and Dubnov, S. HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection. In ICASSP, 2022.": "HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection", "Tang et al. (2022) Tang, Z., Cho, J., Nie, Y., and Bansal, M. TVLT: Textless Vision-Language Transformer. In NeurIPS, 2022.": "TVLT: Textless Vision-Language Transformer", "Baade et al. (2022) Baade, A., Peng, P., and Harwath, D. MAE-AST: Masked autoencoding audio spectrogram transformer. In Interspeech, 2022.": "MAE-AST: Masked autoencoding audio spectrogram transformer", "Gong et al. (2022b) Gong, Y., Rouditchenko, A., Liu, A. H., Harwath, D., Karlinsky, L., Kuehne, H., and Glass, J. Contrastive audio-visual masked autoencoder. arXiv preprint arXiv:2210.07839, 2022b.": "Contrastive audio-visual masked autoencoder", "Fayek & Kumar (2020) Fayek, H. M. and Kumar, A. Large scale audiovisual learning of sounds with weakly labeled data. In IJCAI, 2020.": "Large scale audiovisual learning of sounds with weakly labeled data", "Park et al. (2019a) Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. Specaugment: A simple data augmentation method for automatic speech recognition. ArXiv, abs/1904.08779, 2019a.": "Specaugment: A simple data augmentation method for automatic speech recognition"}, "source_title_to_arxiv_id": {"VGGSound: A large-scale audio-visual dataset": "2004.14368", "SSAST: Self-Supervised Audio Spectrogram Transformer": "2110.09784", "AST: Audio Spectrogram Transformer": "2104.01778", "Masked autoencoders that listen": "2207.06405", "HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection": "2202.00874", "MAE-AST: Masked autoencoding audio spectrogram transformer": "2203.16691", "Contrastive audio-visual masked autoencoder": "2210.07839"}}