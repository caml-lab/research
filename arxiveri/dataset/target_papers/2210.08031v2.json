{"title": "Neural Attentive Circuits", "abstract": "Recent work has seen the development of general purpose neural architectures\nthat can be trained to perform tasks across diverse data modalities. General\npurpose models typically make few assumptions about the underlying\ndata-structure and are known to perform well in the large-data regime. At the\nsame time, there has been growing interest in modular neural architectures that\nrepresent the data using sparsely interacting modules. These models can be more\nrobust out-of-distribution, computationally efficient, and capable of\nsample-efficient adaptation to new data. However, they tend to make\ndomain-specific assumptions about the data, and present challenges in how\nmodule behavior (i.e., parameterization) and connectivity (i.e., their layout)\ncan be jointly learned. In this work, we introduce a general purpose, yet\nmodular neural architecture called Neural Attentive Circuits (NACs) that\njointly learns the parameterization and a sparse connectivity of neural modules\nwithout using domain knowledge. NACs are best understood as the combination of\ntwo systems that are jointly trained end-to-end: one that determines the module\nconfiguration and the other that executes it on an input. We demonstrate\nqualitatively that NACs learn diverse and meaningful module configurations on\nthe NLVR2 dataset without additional supervision. Quantitatively, we show that\nby incorporating modularity in this way, NACs improve upon a strong non-modular\nbaseline in terms of low-shot adaptation on CIFAR and CUBs dataset by about\n10%, and OOD robustness on Tiny ImageNet-R by about 2.5%. Further, we find that\nNACs can achieve an 8x speedup at inference time while losing less than 3%\nperformance. Finally, we find NACs to yield competitive results on diverse data\nmodalities spanning point-cloud classification, symbolic processing and\ntext-classification from ASCII bytes, thereby confirming its general purpose\nnature.", "authors": ["Nasim Rahaman", "Martin Weiss", "Francesco Locatello", "Chris Pal", "Yoshua Bengio", "Bernhard Sch\u00f6lkopf", "Li Erran Li", "Nicolas Ballas"], "published_date": "2022_10_14", "pdf_url": "http://arxiv.org/pdf/2210.08031v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>Test Accuracy</td></tr><tr><th>Hierarchical Perceiver (No MAE) (Carreira et al., 2022)</th><td>75.9%</td></tr><tr><th>Perceiver IO (Carreira et al., 2022)</th><td>77.4%</td></tr><tr><th>Hierarchical Perceiver (with MAE) (Carreira et al., 2022)</th><td>80.6%</td></tr><tr><th>NAC (ours) (No MAE)</th><td>83.0%</td></tr></tbody></table>", "caption": "Table 2: Point Cloud Classification.  In this experiment, we compare against results reported in Carreira et al. (2022). In that work, the authors were able to improve over Perceiver IO results through a masked auto-encoding (MAE) pre-training step. We find that, without pre-training, NACs with a Scale-Free sparse prior graph are able to achieve superior test accuracy. ", "list_citation_info": ["Carreira et al. [2022] Joao Carreira, Skanda Koppula, Daniel Zoran, Adria Recasens, Catalin Ionescu, Olivier Henaff, Evan Shelhamer, Relja Arandjelovic, Matt Botvinick, Oriol Vinyals, Karen Simonyan, Andrew Zisserman, and Andrew Jaegle. Hierarchical perceiver, 2022. URL https://arxiv.org/abs/2202.10890."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">Test Accuracy</th></tr><tr><th>Method</th><th>ListOps (Nangia and Bowman, 2018)</th><th>Text Classification (Tay et al., 2021)</th></tr></thead><tbody><tr><th>Full Attention Transformers (Tay et al., 2021)</th><td>37.13%</td><td>65.35%</td></tr><tr><th>Linformer (Tay et al., 2021)</th><td>37.38%</td><td>56.12%</td></tr><tr><th>Perceiver IO (our impl.)</th><td>39.70%</td><td>66.50%</td></tr><tr><th>NAC (ours)</th><td>41.40%</td><td>68.18%</td></tr></tbody></table>", "caption": "Table 3: ListOps and Text Classification Results. In this experiment, we train a Perceiver IO and a NAC from scratch on two tasks from the the Long Range Arena (Tay et al., 2021): ListOps symbolic processing and text classification from ASCII bytes. We observe that NAC excels in processing long-range dependencies in this setting.", "list_citation_info": ["Tay et al. [2021] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k.", "Nangia and Bowman [2018] Nikita Nangia and Samuel R. Bowman. Listops: A diagnostic dataset for latent tree learning, 2018. URL https://arxiv.org/abs/1804.06028."]}, {"table": "<table><tbody><tr><td>Hyperparameter</td><td>Neural Attentive Circuit</td><td>Perciever IO</td></tr><tr><td>Batch size</td><td>1024</td><td>1024</td></tr><tr><td>Number of epochs</td><td>400</td><td>400</td></tr><tr><td>Augmentation pipeline</td><td>Standard [Lee et al., 2021, Touvron et al., 2021]</td><td>Standard [Lee et al., 2021, Touvron et al., 2021]</td></tr><tr><td>Weight decay</td><td>0.05</td><td>0.05</td></tr><tr><td>Optimizer</td><td>AdamW</td><td>AdamW</td></tr><tr><td>Learning rate scheduler</td><td>Cosine</td><td>Cosine</td></tr><tr><td>Base peak learning rate (for batch size 512)</td><td>0.0003</td><td>0.0005</td></tr><tr><td>Base min learning rate (for batch size 512)</td><td>1e-5</td><td>1e-5</td></tr><tr><td>Warmup epochs</td><td>25</td><td>25</td></tr><tr><td>Warmup from learning rate</td><td>1e-6</td><td>1e-6</td></tr><tr><td>Dimension of state (d_{model})</td><td>384</td><td>384</td></tr><tr><td>Layers (L)</td><td>8</td><td>8</td></tr><tr><td>Layers that do not share weights</td><td>8</td><td>8</td></tr><tr><td>Processor modules (NACs) or latents (PIOs) (U_{p})</td><td>320</td><td>320</td></tr><tr><td>Attention heads</td><td>6</td><td>6</td></tr><tr><td>Read-in (NACs) or cross-attention (PIOs) heads</td><td>1</td><td>1</td></tr><tr><td>Activation function</td><td>GEGLU</td><td>GEGLU</td></tr><tr><td>FFN hidden units</td><td>1536</td><td>1536</td></tr><tr><td>Output modules (U_{o})</td><td>64</td><td>N/A</td></tr><tr><td>Signature dimension (d_{sig})</td><td>64</td><td>N/A</td></tr><tr><td>Code dimension (d_{code})</td><td>384</td><td>N/A</td></tr><tr><td>Sampling temperature (\\tau)</td><td>0.5</td><td>N/A</td></tr><tr><td>Kernel bandwidth (\\epsilon)</td><td>1.0</td><td>N/A</td></tr><tr><td>Modulation \\alpha at initialization</td><td>0.1</td><td>N/A</td></tr></tbody></table>", "caption": "Table 4: Hyperparameters for training on Tiny-ImageNet.", "list_citation_info": ["Lee et al. [2021] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets, 2021. URL https://arxiv.org/abs/2112.13492."]}, {"table": "<table><tbody><tr><td>Hyperparameter</td><td>Neural Attentive Circuit</td><td>Perciever IO</td></tr><tr><td>Batch size</td><td>1024</td><td>1024</td></tr><tr><td>Number of epochs</td><td>110</td><td>110</td></tr><tr><td>CutMix</td><td>\u2713</td><td>\u2713</td></tr><tr><td>MixUp</td><td>\u2713</td><td>\u2713</td></tr><tr><td>RandAugment</td><td>\u2713</td><td>\u2713</td></tr><tr><td>Augmentation pipeline</td><td>Standard [Lee et al., 2021, Touvron et al., 2021]</td><td>Custom [Jaegle et al., 2021a]</td></tr><tr><td>Weight decay</td><td>0.05</td><td>0.1</td></tr><tr><td>Optimizer</td><td>AdamW</td><td>LAMB</td></tr><tr><td>Learning rate scheduler</td><td>Cosine</td><td>Custom[Jaegle et al., 2021a]</td></tr><tr><td>Base peak learning rate (for batch size 512)</td><td>0.0003</td><td>0.001</td></tr><tr><td>Base min learning rate (for batch size 512)</td><td>1e-5</td><td>0</td></tr><tr><td>Warmup epochs</td><td>25</td><td>N/A</td></tr><tr><td>Warmup from learning rate</td><td>1e-6</td><td>N/A</td></tr><tr><td>Dimension of state (d_{model})</td><td>512</td><td>1024</td></tr><tr><td>Layers (L)</td><td>8</td><td>48</td></tr><tr><td>Layers that do not share weights</td><td>8</td><td>6</td></tr><tr><td>Processor modules (NACs) or latents (PIOs) (U_{p})</td><td>960</td><td>512</td></tr><tr><td>Attention heads</td><td>8</td><td>8</td></tr><tr><td>Read-in (NACs) or cross-attention (PIOs) heads</td><td>1</td><td>1</td></tr><tr><td>Activation function</td><td>GEGLU</td><td>GELU</td></tr><tr><td>FFN hidden units</td><td>1024</td><td>1024</td></tr><tr><td>Output modules (U_{o})</td><td>64</td><td>N/A</td></tr><tr><td>Signature dimension (d_{sig})</td><td>64</td><td>N/A</td></tr><tr><td>Code dimension (d_{code})</td><td>512</td><td>N/A</td></tr><tr><td>Sampling temperature (\\tau)</td><td>0.5</td><td>N/A</td></tr><tr><td>Kernel bandwidth (\\epsilon)</td><td>1.0</td><td>N/A</td></tr><tr><td>Modulation \\alpha at initialization</td><td>0.1</td><td>N/A</td></tr></tbody></table>", "caption": "Table 5: Hyperparameters for training on ImageNet. Note that we do not train the Perceiver IO, but use a pre-trained model. Nevertheless, we gather available hyperparameters from Jaegle et al. [2021a] and the official code-release for the reader\u2019s convenience.", "list_citation_info": ["Lee et al. [2021] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets, 2021. URL https://arxiv.org/abs/2112.13492.", "Jaegle et al. [2021a] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo\u0101o Carreira. Perceiver io: A general architecture for structured inputs and outputs, 2021a. URL https://arxiv.org/abs/2107.14795."]}], "citation_info_to_title": {"Nangia and Bowman [2018] Nikita Nangia and Samuel R. Bowman. Listops: A diagnostic dataset for latent tree learning, 2018. URL https://arxiv.org/abs/1804.06028.": "Listops: A diagnostic dataset for latent tree learning", "Jaegle et al. [2021a] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo\u0101o Carreira. Perceiver io: A general architecture for structured inputs and outputs, 2021a. URL https://arxiv.org/abs/2107.14795.": "Perceiver io: A general architecture for structured inputs and outputs", "Carreira et al. [2022] Joao Carreira, Skanda Koppula, Daniel Zoran, Adria Recasens, Catalin Ionescu, Olivier Henaff, Evan Shelhamer, Relja Arandjelovic, Matt Botvinick, Oriol Vinyals, Karen Simonyan, Andrew Zisserman, and Andrew Jaegle. Hierarchical perceiver, 2022. URL https://arxiv.org/abs/2202.10890.": "Hierarchical perceiver", "Tay et al. [2021] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k.": "Long Range Arena: A Benchmark for Efficient Transformers", "Lee et al. [2021] Seung Hoon Lee, Seunghyun Lee, and Byung Cheol Song. Vision transformer for small-size datasets, 2021. URL https://arxiv.org/abs/2112.13492.": "Vision Transformer for Small-Size Datasets"}, "source_title_to_arxiv_id": {"Hierarchical perceiver": "1911.08708"}}