{"title": "SIGMA: Semantic-complete Graph Matching for Domain Adaptive Object Detection", "abstract": "Domain Adaptive Object Detection (DAOD) leverages a labeled domain to learn\nan object detector generalizing to a novel domain free of annotations. Recent\nadvances align class-conditional distributions by narrowing down cross-domain\nprototypes (class centers). Though great success,they ignore the significant\nwithin-class variance and the domain-mismatched semantics within the training\nbatch, leading to a sub-optimal adaptation. To overcome these challenges, we\npropose a novel SemantIc-complete Graph MAtching (SIGMA) framework for DAOD,\nwhich completes mismatched semantics and reformulates the adaptation with graph\nmatching. Specifically, we design a Graph-embedded Semantic Completion module\n(GSC) that completes mismatched semantics through generating hallucination\ngraph nodes in missing categories. Then, we establish cross-image graphs to\nmodel class-conditional distributions and learn a graph-guided memory bank for\nbetter semantic completion in turn. After representing the source and target\ndata as graphs, we reformulate the adaptation as a graph matching problem,\ni.e., finding well-matched node pairs across graphs to reduce the domain gap,\nwhich is solved with a novel Bipartite Graph Matching adaptor (BGM). In a\nnutshell, we utilize graph nodes to establish semantic-aware node affinity and\nleverage graph edges as quadratic constraints in a structure-aware matching\nloss, achieving fine-grained adaptation with a node-to-node graph matching.\nExtensive experiments verify that SIGMA outperforms existing works\nsignificantly. Our code is available at\nhttps://github.com/CityU-AIM-Group/SIGMA.", "authors": ["Wuyang Li", "Xinyu Liu", "Yixuan Yuan"], "published_date": "2022_03_12", "pdf_url": "http://arxiv.org/pdf/2203.06398v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td>person</td><td>rider</td><td>car</td><td>truck</td><td>bus</td><td>train</td><td>motor</td><td>bike</td><td>mAP</td><td>SO/ GAIN</td></tr><tr><td>CFFA [47]{}_{CVPR^{\\prime}20}</td><td rowspan=\"9\">VGG-16</td><td>34.0</td><td>46.9</td><td>52.1</td><td>30.8</td><td>43.2</td><td>29.9</td><td>34.7</td><td>37.4</td><td>38.6</td><td>20.8/ 17.8</td></tr><tr><td>EPM [15]{}_{ECCV^{\\prime}20}</td><td>41.9</td><td>38.7</td><td>56.7</td><td>22.6</td><td>41.5</td><td>26.8</td><td>24.6</td><td>35.5</td><td>36.0</td><td>18.4/ 17.6</td></tr><tr><td>RPNPA [46]{}_{CVPR^{\\prime}21}</td><td>33.6</td><td>43.8</td><td>49.6</td><td>32.9</td><td>45.5</td><td>46.0</td><td>35.7</td><td>36.8</td><td>40.5</td><td>20.8/ 19.7</td></tr><tr><td>UMT [8]{}_{CVPR^{\\prime}21}</td><td>33.0</td><td>46.7</td><td>48.6</td><td>34.1</td><td>56.5</td><td>46.8</td><td>30.4</td><td>37.4</td><td>41.7</td><td>21.8/ 19.9</td></tr><tr><td>MeGA [38]{}_{CVPR^{\\prime}21}</td><td>37.7</td><td>49.0</td><td>52.4</td><td>25.4</td><td>49.2</td><td>46.9</td><td>34.5</td><td>39.0</td><td>41.8</td><td>24.4/ 17.4</td></tr><tr><td>ICCR-VDD [40]{}_{ICCV^{\\prime}21}</td><td>33.4</td><td>44.0</td><td>51.7</td><td>33.9</td><td>52.0</td><td>34.7</td><td>34.2</td><td>36.8</td><td>40.0</td><td>22.8/ 17.2</td></tr><tr><td>KTNet [36]{}_{ICCV^{\\prime}21}</td><td>46.4</td><td>43.2</td><td>60.6</td><td>25.8</td><td>41.2</td><td>40.4</td><td>30.7</td><td>38.8</td><td>40.9</td><td>18.4/ 22.5</td></tr><tr><td>SSAL [24]{}_{NeurIPS^{\\prime}21}</td><td>45.1</td><td>47.4</td><td>59.4</td><td>24.5</td><td>50.0</td><td>25.7</td><td>26.0</td><td>38.7</td><td>39.6</td><td>20.4/ 19.2</td></tr><tr><td>SIGMA (ours)</td><td>46.9</td><td>48.4</td><td>63.7</td><td>27.1</td><td>50.7</td><td>35.9</td><td>34.7</td><td>41.4</td><td>43.5</td><td>18.4/ 25.1</td></tr><tr><td>GPA [41]{}_{CVPR^{\\prime}20}</td><td rowspan=\"6\">ResNet-50</td><td>32.9</td><td>46.7</td><td>54.1</td><td>24.7</td><td>45.7</td><td>41.1</td><td>32.4</td><td>38.7</td><td>39.5</td><td>22.8/ 16.7</td></tr><tr><td>EPM [15]{}_{ECCV^{\\prime}20}</td><td>39.9</td><td>38.1</td><td>57.3</td><td>28.7</td><td>50.7</td><td>37.2</td><td>30.2</td><td>34.2</td><td>39.5</td><td>24.2/ 15.3</td></tr><tr><td>DIDN [21]{}_{ICCV^{\\prime}21}</td><td>38.3</td><td>44.4</td><td>51.8</td><td>28.7</td><td>53.3</td><td>34.7</td><td>32.4</td><td>40.4</td><td>40.5</td><td>28.6/ 11.9</td></tr><tr><td>DSS [39]{}_{CVPR^{\\prime}21}</td><td>42.9</td><td>51.2</td><td>53.6</td><td>33.6</td><td>49.2</td><td>18.9</td><td>36.2</td><td>41.8</td><td>40.9</td><td>22.8/ 18.1</td></tr><tr><td>SDA [29]{}_{ICCV^{\\prime}21}</td><td>38.8</td><td>45.9</td><td>57.2</td><td>29.9</td><td>50.2</td><td>51.9</td><td>31.9</td><td>40.9</td><td>43.3</td><td>22.8/ 20.5</td></tr><tr><td>SIGMA (ours)</td><td>44.0</td><td>43.9</td><td>60.3</td><td>31.6</td><td>50.4</td><td>51.5</td><td>31.7</td><td>40.6</td><td>44.2</td><td>24.2/ 20.0</td></tr></tbody></table>", "caption": "Table 1: Results on Cityscapes\\toFoggy Cityscapes (%) with VGG-16 and ResNet-50 backbone networks. SO represents the source only results and GAIN indicates the adaptation gains compared with the source only model.", "list_citation_info": ["[47] Yangtao Zheng, Di Huang, Songtao Liu, and Yunhong Wang. Cross-domain object detection through coarse-to-fine feature adaptation. In CVPR, pages 13766\u201313775, 2020.", "[15] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang. Every pixel matters: Center-aware feature alignment for domain adaptive object detector. In ECCV, pages 733\u2013748, 2020.", "[8] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Unbiased mean teacher for cross-domain object detection. In CVPR, pages 4091\u20134101, June 2021.", "[36] Kun Tian, Chenghao Zhang, Ying Wang, Shiming Xiang, and Chunhong Pan. Knowledge mining and transferring for domain adaptive object detection. In ICCV, pages 9133\u20139142, October 2021.", "[40] Aming Wu, Rui Liu, Yahong Han, Linchao Zhu, and Yi Yang. Vector-decomposed disentanglement for domain-invariant object detection. ICCV, 2021.", "[38] Vibashan VS, Vikram Gupta, Poojan Oza, Vishwanath A. Sindagi, and Vishal M. Patel. Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection. In CVPR, pages 4516\u20134526, June 2021.", "[41] Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wenjun Zhang. Cross-domain detection via graph-induced prototype alignment. In CVPR, pages 12355\u201312364, 2020.", "[24] Muhammad Akhtar Munir, Muhammad Haris Khan, M Saquib Sarfraz, and Mohsen Ali. Synergizing between self-training and adversarial learning for domain adaptive object detection. 2021.", "[29] Farzaneh Rezaeianaran, Rakshith Shetty, Rahaf Aljundi, Daniel Olmeda Reino, Shanshan Zhang, and Bernt Schiele. Seeking similarities over differences: Similarity-based domain alignment for adaptive object detection. In ICCV, pages 9204\u20139213, 2021.", "[39] Yu Wang, Rui Zhang, Shuo Zhang, Miao Li, Yangyang Xia, Xishan Zhang, and Shaoli Liu. Domain-specific suppression for adaptive object detection. In CVPR, pages 9603\u20139612, June 2021.", "[21] Chuang Lin, Zehuan Yuan, Sicheng Zhao, Peize Sun, Changhu Wang, and Jianfei Cai. Domain-invariant disentangled network for generalizable object detection. In ICCV, pages 8771\u20138780, October 2021.", "[46] Yixin Zhang, Zilei Wang, and Yushi Mao. Rpn prototype alignment for domain adaptive object detector. In CVPR, pages 12425\u201312434, June 2021."]}, {"table": "<table><thead><tr><th><p>Method</p></th><th>S\\toC</th><th>SO/GAIN</th><th>K\\toC</th><th>SO/GAIN</th></tr></thead><tbody><tr><th><p>EPM [15]{}_{ECCV^{\\prime}20}</p></th><td>49.0</td><td>39.8/ 9.2</td><td>43.2</td><td>34.4/ 8.8</td></tr><tr><th><p>DSS [39]{}_{CVPR^{\\prime}21}</p></th><td>44.5</td><td>34.7/ 9.8</td><td>42.7</td><td>34.6/ 8.1</td></tr><tr><th><p>MEGA [38]{}_{CVPR^{\\prime}21}</p></th><td>44.8</td><td>34.3/ 10.5</td><td>43.0</td><td>30.2/ 12.8</td></tr><tr><th><p>RPNPA [46]{}_{CVPR^{\\prime}21}</p></th><td>45.7</td><td>34.6/ 11.1</td><td>-</td><td>-</td></tr><tr><th><p>UMT [8]{}_{CVPR^{\\prime}21}</p></th><td>43.1</td><td>34.3/ 8.8</td><td>-</td><td>-</td></tr><tr><th><p>KTNet [36]{}_{ICCV^{\\prime}21}</p></th><td>50.7</td><td>39.8/ 10.9</td><td>45.6</td><td>34.4/ 11.2</td></tr><tr><th><p>SSAL [24]{}_{NeurIPS^{\\prime}21}</p></th><td>51.8</td><td>38.0/ 13.8</td><td>45.6</td><td>34.9/ 10.7</td></tr><tr><th><p>SIGMA (ours)</p></th><td>53.7</td><td>39.8/ 13.9</td><td>45.8</td><td>34.4/ 11.4</td></tr></tbody></table>", "caption": "Table 2: Comparison results (%) on Sim10K\\toCityscapes (S\\toC) and KITTI\\toCityscapes (K\\toC) with VGG-16 backbone. ", "list_citation_info": ["[8] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Unbiased mean teacher for cross-domain object detection. In CVPR, pages 4091\u20134101, June 2021.", "[15] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang. Every pixel matters: Center-aware feature alignment for domain adaptive object detector. In ECCV, pages 733\u2013748, 2020.", "[36] Kun Tian, Chenghao Zhang, Ying Wang, Shiming Xiang, and Chunhong Pan. Knowledge mining and transferring for domain adaptive object detection. In ICCV, pages 9133\u20139142, October 2021.", "[38] Vibashan VS, Vikram Gupta, Poojan Oza, Vishwanath A. Sindagi, and Vishal M. Patel. Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection. In CVPR, pages 4516\u20134526, June 2021.", "[24] Muhammad Akhtar Munir, Muhammad Haris Khan, M Saquib Sarfraz, and Mohsen Ali. Synergizing between self-training and adversarial learning for domain adaptive object detection. 2021.", "[39] Yu Wang, Rui Zhang, Shuo Zhang, Miao Li, Yangyang Xia, Xishan Zhang, and Shaoli Liu. Domain-specific suppression for adaptive object detection. In CVPR, pages 9603\u20139612, June 2021.", "[46] Yixin Zhang, Zilei Wang, and Yushi Mao. Rpn prototype alignment for domain adaptive object detector. In CVPR, pages 12425\u201312434, June 2021."]}, {"table": "<table><thead><tr><th><p>Method</p></th><th><p>w/o</p></th><th><p>prsn</p></th><th><p>rider</p></th><th><p>car</p></th><th><p>truc</p></th><th><p>bus</p></th><th><p>train</p></th><th><p>moto</p></th><th><p>bike</p></th><th>mAP</th></tr><tr><th><p>GA [15]</p></th><th><p>-</p></th><th><p>40.3</p></th><th><p>41.5</p></th><th><p>54.2</p></th><th><p>26.7</p></th><th><p>42.1</p></th><th><p>15.4</p></th><th><p>27.1</p></th><th><p>35.1</p></th><th>35.3</th></tr></thead><tbody><tr><th rowspan=\"4\">+GSC</th><th><p>DNC</p></th><td><p>45.2</p></td><td><p>46.2</p></td><td><p>57.2</p></td><td><p>29.1</p></td><td><p>46.5</p></td><td><p>31.2</p></td><td><p>29.2</p></td><td><p>38.7</p></td><td>40.4</td></tr><tr><th><p>GMB</p></th><td><p>43.5</p></td><td><p>43.8</p></td><td><p>57.4</p></td><td>29.4</td><td><p>48.3</p></td><td><p>30.4</p></td><td><p>31.4</p></td><td><p>41.1</p></td><td>41.0</td></tr><tr><th><p>ND</p></th><td><p>44.1</p></td><td><p>45.2</p></td><td><p>56.7</p></td><td><p>28.0</p></td><td><p>45.9</p></td><td><p>23.9</p></td><td><p>32.8</p></td><td><p>38.7</p></td><td>39.4</td></tr><tr><th><p>-</p></th><td><p>45.8</p></td><td><p>47.6</p></td><td><p>58.9</p></td><td><p>27.3</p></td><td><p>48.6</p></td><td><p>33.8</p></td><td><p>32.7</p></td><td><p>39.3</p></td><td>41.8</td></tr><tr><th></th><th><p>CGI</p></th><td><p>44.4</p></td><td><p>48.0</p></td><td><p>58.8</p></td><td><p>28.4</p></td><td><p>50.3</p></td><td>40.5</td><td><p>31.7</p></td><td><p>40.8</p></td><td>42.8</td></tr><tr><th><p>+GSC</p></th><th><p>SNA</p></th><td><p>46.0</p></td><td><p>46.9</p></td><td><p>58.8</p></td><td><p>28.6</p></td><td><p>48.2</p></td><td><p>40.4</p></td><td><p>33.1</p></td><td><p>39.5</p></td><td>42.6</td></tr><tr><th><p>+BGM</p></th><th><p>SML</p></th><td><p>46.1</p></td><td>49.9</td><td><p>59.1</p></td><td><p>26.2</p></td><td>52.5</td><td><p>27.1</p></td><td><p>34.6</p></td><td><p>41.3</p></td><td>42.2</td></tr><tr><th></th><th><p>-</p></th><td>46.9</td><td><p>48.4</p></td><td>63.7</td><td><p>27.1</p></td><td><p>50.7</p></td><td><p>35.9</p></td><td>34.7</td><td>41.4</td><td>43.5</td></tr></tbody></table>", "caption": "Table 3: Ablation studies on Cityscapes\\toFoggy Cityscapes (%).", "list_citation_info": ["[15] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang. Every pixel matters: Center-aware feature alignment for domain adaptive object detector. In ECCV, pages 733\u2013748, 2020."]}, {"table": "<table><thead><tr><th>Global Discriminator [15]</th></tr></thead><tbody><tr><td>Gradient Reversal Layer (GRL)</td></tr><tr><td>Conv 256 \\times 3 \\times 3, stride 1 \\to GroupNorm \\to ReLU</td></tr><tr><td>Conv 256 \\times 3 \\times 3, stride 1 \\to GroupNorm \\to ReLU</td></tr><tr><td>Conv 256 \\times 3 \\times 3, stride 1 \\to GroupNorm \\to ReLU</td></tr><tr><td>Conv 256 \\times 3 \\times 3, stride 1 \\to GroupNorm \\to ReLU</td></tr><tr><td>Conv 1 \\times 3 \\times 3, stride 1</td></tr><tr><td>Node Discriminator (ours)</td></tr><tr><td>Gradient Reversal Layer (GRL)</td></tr><tr><td>Fc 256 \\to LayerNorm \\to ReLU</td></tr><tr><td>Fc 256 \\to LayerNorm \\to ReLU</td></tr><tr><td>Fc 256 \\to LayerNorm \\to ReLU</td></tr><tr><td>Fc 1 \\to LayerNorm \\to ReLU</td></tr></tbody></table>", "caption": "Table 9: Architectures of the adversarial alignment modules.", "list_citation_info": ["[15] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang. Every pixel matters: Center-aware feature alignment for domain adaptive object detector. In ECCV, pages 733\u2013748, 2020."]}], "citation_info_to_title": {"[41] Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, and Wenjun Zhang. Cross-domain detection via graph-induced prototype alignment. In CVPR, pages 12355\u201312364, 2020.": "Cross-domain detection via graph-induced prototype alignment", "[39] Yu Wang, Rui Zhang, Shuo Zhang, Miao Li, Yangyang Xia, Xishan Zhang, and Shaoli Liu. Domain-specific suppression for adaptive object detection. In CVPR, pages 9603\u20139612, June 2021.": "Domain-specific suppression for adaptive object detection", "[21] Chuang Lin, Zehuan Yuan, Sicheng Zhao, Peize Sun, Changhu Wang, and Jianfei Cai. Domain-invariant disentangled network for generalizable object detection. In ICCV, pages 8771\u20138780, October 2021.": "Domain-invariant disentangled network for generalizable object detection", "[8] Jinhong Deng, Wen Li, Yuhua Chen, and Lixin Duan. Unbiased mean teacher for cross-domain object detection. In CVPR, pages 4091\u20134101, June 2021.": "Unbiased Mean Teacher for Cross-Domain Object Detection", "[29] Farzaneh Rezaeianaran, Rakshith Shetty, Rahaf Aljundi, Daniel Olmeda Reino, Shanshan Zhang, and Bernt Schiele. Seeking similarities over differences: Similarity-based domain alignment for adaptive object detection. In ICCV, pages 9204\u20139213, 2021.": "Seeking similarities over differences: Similarity-based domain alignment for adaptive object detection", "[47] Yangtao Zheng, Di Huang, Songtao Liu, and Yunhong Wang. Cross-domain object detection through coarse-to-fine feature adaptation. In CVPR, pages 13766\u201313775, 2020.": "Cross-domain object detection through coarse-to-fine feature adaptation", "[46] Yixin Zhang, Zilei Wang, and Yushi Mao. Rpn prototype alignment for domain adaptive object detector. In CVPR, pages 12425\u201312434, June 2021.": "Rpn prototype alignment for domain adaptive object detector", "[36] Kun Tian, Chenghao Zhang, Ying Wang, Shiming Xiang, and Chunhong Pan. Knowledge mining and transferring for domain adaptive object detection. In ICCV, pages 9133\u20139142, October 2021.": "Knowledge Mining and Transferring for Domain Adaptive Object Detection", "[24] Muhammad Akhtar Munir, Muhammad Haris Khan, M Saquib Sarfraz, and Mohsen Ali. Synergizing between self-training and adversarial learning for domain adaptive object detection. 2021.": "Synergizing between self-training and adversarial learning for domain adaptive object detection", "[40] Aming Wu, Rui Liu, Yahong Han, Linchao Zhu, and Yi Yang. Vector-decomposed disentanglement for domain-invariant object detection. ICCV, 2021.": "Vector-Decomposed Disentanglement for Domain-Invariant Object Detection", "[38] Vibashan VS, Vikram Gupta, Poojan Oza, Vishwanath A. Sindagi, and Vishal M. Patel. Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection. In CVPR, pages 4516\u20134526, June 2021.": "Mega-cda: Memory guided attention for category-aware unsupervised domain adaptive object detection", "[15] Cheng-Chun Hsu, Yi-Hsuan Tsai, Yen-Yu Lin, and Ming-Hsuan Yang. Every pixel matters: Center-aware feature alignment for domain adaptive object detector. In ECCV, pages 733\u2013748, 2020.": "Every pixel matters: Center-aware feature alignment for domain adaptive object detector"}, "source_title_to_arxiv_id": {"Domain-specific suppression for adaptive object detection": "2105.03570", "Synergizing between self-training and adversarial learning for domain adaptive object detection": "2110.00249"}}