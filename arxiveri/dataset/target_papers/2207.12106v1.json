{"title": "Black-box Few-shot Knowledge Distillation", "abstract": "Knowledge distillation (KD) is an efficient approach to transfer the\nknowledge from a large \"teacher\" network to a smaller \"student\" network.\nTraditional KD methods require lots of labeled training samples and a white-box\nteacher (parameters are accessible) to train a good student. However, these\nresources are not always available in real-world applications. The distillation\nprocess often happens at an external party side where we do not have access to\nmuch data, and the teacher does not disclose its parameters due to security and\nprivacy concerns. To overcome these challenges, we propose a black-box few-shot\nKD method to train the student with few unlabeled training samples and a\nblack-box teacher. Our main idea is to expand the training set by generating a\ndiverse set of out-of-distribution synthetic images using MixUp and a\nconditional variational auto-encoder. These synthetic images along with their\nlabels obtained from the teacher are used to train the student. We conduct\nextensive experiments to show that our method significantly outperforms recent\nSOTA few/zero-shot KD methods on image classification tasks. The code and\nmodels are available at: https://github.com/nphdang/FS-BBT", "authors": ["Dang Nguyen", "Sunil Gupta", "Kien Do", "Svetha Venkatesh"], "published_date": "2022_07_25", "pdf_url": "http://arxiv.org/pdf/2207.12106v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Dataset</td><td>Method</td><td>Teacher</td><td>Model</td><td>N</td><td>Accuracy</td></tr><tr><td rowspan=\"6\">MNIST</td><td>Student-Alone</td><td>-</td><td>-</td><td>2,000</td><td>95.97%</td></tr><tr><td>Standard-KD</td><td>99.18%</td><td>Black</td><td>2,000</td><td>95.99%</td></tr><tr><td>FSKD [3]</td><td>99.29%</td><td>White</td><td>2,000</td><td>80.43%</td></tr><tr><td>BBKD<sup>\\star</sup> [37]</td><td>99.29%</td><td>Black</td><td>2,000</td><td>98.74%</td></tr><tr><td>FS-BBT (Ours)</td><td>99.18%</td><td>Black</td><td>2,000</td><td>98.42%</td></tr><tr><td>FS-BBT<sup>\\star</sup> (Ours)</td><td>99.18%</td><td>Black</td><td>2,000</td><td>98.91%</td></tr><tr><td rowspan=\"7\">Fashion-MNIST</td><td>Student-Alone</td><td>-</td><td>-</td><td>2,000</td><td>81.37%</td></tr><tr><td>Standard-KD</td><td>90.15%</td><td>Black</td><td>2,000</td><td>83.87%</td></tr><tr><td>FSKD [3]</td><td>90.80%</td><td>White</td><td>2,000</td><td>68.64%</td></tr><tr><td>WaGe [20]</td><td>92.00%</td><td>White</td><td>1,000</td><td>85.18%</td></tr><tr><td>BBKD<sup>\\star</sup> [37]</td><td>90.80%</td><td>Black</td><td>2,000</td><td>80.90%</td></tr><tr><td>FS-BBT (Ours)</td><td>90.15%</td><td>Black</td><td>2,000</td><td>84.73%</td></tr><tr><td>FS-BBT<sup>\\star</sup> (Ours)</td><td>90.15%</td><td>Black</td><td>2,000</td><td>86.53%</td></tr></tbody></table>", "caption": "Table 1: Classification results onMNIST and Fashion-MNIST. \u201cTeacher\u201d indicates the accuracy of theteacher network on the test set. \u201cModel\u201d indicates whether theteacher network is a black-box model. \u201cN\u201d shows thenumber of original images used by each method. \u201cAccuracy\u201d is theaccuracy of the student network on the test set. The results ofFSKD, WaGe, and BBKD<sup>\\star</sup> are obtainedfrom [20, 37]. \u201c\\star\u201d meansthe BBKD<sup>\\star</sup> and FS-BBT<sup>\\star</sup>methods use the same architecture (LeNet5) for both teacher and studentnetworks.", "list_citation_info": ["[20] Kong, S., Guo, T., You, S., Xu, C.: Learning student networks with few data. In: AAAI. vol. 34, pp. 4469\u20134476 (2020)", "[3] Akisato, K., Zoubin, G., Koh, T., Tomoharu, I., Naonori, U.: Few-shot learning of neural networks from scratch by pseudo example optimization. In: British Machine Vision Conference (BMVC). p. 105 (2018)", "[37] Wang, D., Li, Y., Wang, L., Gong, B.: Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model. In: CVPR. pp. 1498\u20131507 (2020)"]}, {"table": "<table><tbody><tr><td>Dataset</td><td>Method</td><td>Teacher</td><td>Model</td><td>N</td><td>Accuracy</td></tr><tr><td rowspan=\"7\">CIFAR-10</td><td>Student-Alone</td><td>-</td><td>-</td><td>2,000</td><td>54.59%</td></tr><tr><td>Standard-KD</td><td>84.07%</td><td>Black</td><td>2,000</td><td>58.96%</td></tr><tr><td>FSKD [3]</td><td>83.07%</td><td>White</td><td>2,000</td><td>40.58%</td></tr><tr><td>WaGe [20]</td><td>89.00%</td><td>White</td><td>5,000</td><td>73.08%</td></tr><tr><td>BBKD<sup>\\star</sup> [37]</td><td>83.07%</td><td>Black</td><td>2,000</td><td>74.60%</td></tr><tr><td>FS-BBT (Ours)</td><td>84.07%</td><td>Black</td><td>2,000</td><td>74.10%</td></tr><tr><td>FS-BBT<sup>\\star</sup> (Ours)</td><td>84.07%</td><td>Black</td><td>2,000</td><td>76.17%</td></tr><tr><td rowspan=\"5\">CIFAR-100</td><td>Student-Alone</td><td>-</td><td>-</td><td>5,000</td><td>32.85%</td></tr><tr><td>Standard-KD</td><td>69.08%</td><td>Black</td><td>5,000</td><td>36.79%</td></tr><tr><td>WaGe [20]</td><td>47.00%</td><td>White</td><td>5,000</td><td>20.32%</td></tr><tr><td>BBKD<sup>\\dagger</sup> [37]</td><td>69.08%</td><td>Black</td><td>5,000</td><td>53.41%</td></tr><tr><td>FS-BBT (Ours)</td><td>69.08%</td><td>Black</td><td>5,000</td><td>56.28%</td></tr></tbody></table>", "caption": "Table 2: Classification resultson CIFAR-10 and CIFAR-100. \u201cN\u201d shows the number of originalimages used by each method. The results of FSKD, WaGe, andBBKD<sup>\\star</sup> are obtained from [20, 37].\u201c\\star\u201d means the BBKD<sup>\\star</sup> andFS-BBT<sup>\\star</sup> methods use thesame architecture (AlexNet) for both teacher and student networks.\u201c\\dagger\u201d means the result is based on our own implementation.", "list_citation_info": ["[20] Kong, S., Guo, T., You, S., Xu, C.: Learning student networks with few data. In: AAAI. vol. 34, pp. 4469\u20134476 (2020)", "[3] Akisato, K., Zoubin, G., Koh, T., Tomoharu, I., Naonori, U.: Few-shot learning of neural networks from scratch by pseudo example optimization. In: British Machine Vision Conference (BMVC). p. 105 (2018)", "[37] Wang, D., Li, Y., Wang, L., Gong, B.: Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model. In: CVPR. pp. 1498\u20131507 (2020)"]}, {"table": "<table><thead><tr><th>Dataset</th><th>Method</th><th>Teacher</th><th>Model</th><th>N</th><th>Accuracy</th></tr></thead><tbody><tr><td rowspan=\"5\">Tiny-ImageNet</td><td>Student-Alone (full)</td><td>-</td><td>-</td><td>100,000</td><td>48.81%</td></tr><tr><td>Student-Alone</td><td>-</td><td>-</td><td>10,000</td><td>23.19%</td></tr><tr><td>Standard-KD</td><td>52.02%</td><td>Black</td><td>10,000</td><td>35.81%</td></tr><tr><td>BBKD<sup>\\dagger</sup> [37]</td><td>52.02%</td><td>Black</td><td>10,000</td><td>40.01%</td></tr><tr><td>FS-BBT (Ours)</td><td>52.02%</td><td>Black</td><td>10,000</td><td>43.29%</td></tr></tbody></table>", "caption": "Table 3: Classification resultson Tiny-ImageNet. \u201cN\u201d shows the number of original images usedby each method. \u201c\\dagger\u201d means the result is based on ourown implementation.", "list_citation_info": ["[37] Wang, D., Li, Y., Wang, L., Gong, B.: Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model. In: CVPR. pp. 1498\u20131507 (2020)"]}, {"table": "<table><thead><tr><th>Method</th><th>Model</th><th>MNIST</th><th>Fashion-MNIST</th><th>CIFAR-10</th></tr></thead><tbody><tr><th>Meta-KD [24]</th><th>White</th><td>92.47%</td><td>-</td><td>-</td></tr><tr><th>ZSKD [28]</th><th>White</th><td>98.77%</td><td>79.62%</td><td>69.56%</td></tr><tr><th>DAFL [8]</th><th>White</th><td>98.20%</td><td>-</td><td>66.38%</td></tr><tr><th>DFKD [38]</th><th>White</th><td>99.08%</td><td>-</td><td>73.91%</td></tr><tr><th>ZSDB3KD [39]</th><th>Black</th><td>96.54%</td><td>72.31%</td><td>59.46%</td></tr><tr><th>FS-BBT (Ours)</th><th>Black</th><td>98.91%</td><td>86.53%</td><td>76.17%</td></tr></tbody></table>", "caption": "Table 4: Classification comparison withzero-shot KD methods. The results of baselines are obtained from [39].", "list_citation_info": ["[8] Chen, H., Wang, Y., Xu, C., Yang, Z., Liu, C., Shi, B., Xu, C., Xu, C., Tian, Q.: Data-free learning of student networks. In: ICCV. pp. 3514\u20133522 (2019)", "[39] Wang, Z.: Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model. In: ICML (2021)", "[38] Wang, Z.: Data-free knowledge distillation with soft targeted transfer set synthesis. In: AAAI. vol. 35, pp. 10245\u201310253 (2021)", "[28] Nayak, K., Mopuri, R., Shaj, V., Radhakrishnan, B., Chakraborty, A.: Zero-shot knowledge distillation in deep networks. In: ICML. pp. 4743\u20134751 (2019)", "[24] Lopes, R.G., Fenu, S., Starner, T.: Data-free knowledge distillation for deep neural networks. arXiv preprint arXiv:1710.07535 (2017)"]}], "citation_info_to_title": {"[3] Akisato, K., Zoubin, G., Koh, T., Tomoharu, I., Naonori, U.: Few-shot learning of neural networks from scratch by pseudo example optimization. In: British Machine Vision Conference (BMVC). p. 105 (2018)": "Few-shot learning of neural networks from scratch by pseudo example optimization", "[28] Nayak, K., Mopuri, R., Shaj, V., Radhakrishnan, B., Chakraborty, A.: Zero-shot knowledge distillation in deep networks. In: ICML. pp. 4743\u20134751 (2019)": "Zero-shot knowledge distillation in deep networks", "[20] Kong, S., Guo, T., You, S., Xu, C.: Learning student networks with few data. In: AAAI. vol. 34, pp. 4469\u20134476 (2020)": "Learning student networks with few data", "[39] Wang, Z.: Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model. In: ICML (2021)": "Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model", "[24] Lopes, R.G., Fenu, S., Starner, T.: Data-free knowledge distillation for deep neural networks. arXiv preprint arXiv:1710.07535 (2017)": "Data-free knowledge distillation for deep neural networks", "[38] Wang, Z.: Data-free knowledge distillation with soft targeted transfer set synthesis. In: AAAI. vol. 35, pp. 10245\u201310253 (2021)": "Data-free knowledge distillation with soft targeted transfer set synthesis", "[37] Wang, D., Li, Y., Wang, L., Gong, B.: Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model. In: CVPR. pp. 1498\u20131507 (2020)": "Neural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model", "[8] Chen, H., Wang, Y., Xu, C., Yang, Z., Liu, C., Shi, B., Xu, C., Xu, C., Tian, Q.: Data-free learning of student networks. In: ICCV. pp. 3514\u20133522 (2019)": "Data-free learning of student networks"}, "source_title_to_arxiv_id": {"Data-free knowledge distillation with soft targeted transfer set synthesis": "2104.04868"}}