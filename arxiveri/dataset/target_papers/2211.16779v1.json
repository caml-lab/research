{"title": "Attention-based Depth Distillation with 3D-Aware Positional Encoding for Monocular 3D Object Detection", "abstract": "Monocular 3D object detection is a low-cost but challenging task, as it\nrequires generating accurate 3D localization solely from a single image input.\nRecent developed depth-assisted methods show promising results by using\nexplicit depth maps as intermediate features, which are either precomputed by\nmonocular depth estimation networks or jointly evaluated with 3D object\ndetection. However, inevitable errors from estimated depth priors may lead to\nmisaligned semantic information and 3D localization, hence resulting in feature\nsmearing and suboptimal predictions. To mitigate this issue, we propose ADD, an\nAttention-based Depth knowledge Distillation framework with 3D-aware positional\nencoding. Unlike previous knowledge distillation frameworks that adopt stereo-\nor LiDAR-based teachers, we build up our teacher with identical architecture as\nthe student but with extra ground-truth depth as input. Credit to our teacher\ndesign, our framework is seamless, domain-gap free, easily implementable, and\nis compatible with object-wise ground-truth depth. Specifically, we leverage\nintermediate features and responses for knowledge distillation. Considering\nlong-range 3D dependencies, we propose \\emph{3D-aware self-attention} and\n\\emph{target-aware cross-attention} modules for student adaptation. Extensive\nexperiments are performed to verify the effectiveness of our framework on the\nchallenging KITTI 3D object detection benchmark. We implement our framework on\nthree representative monocular detectors, and we achieve state-of-the-art\nperformance with no additional inference computational cost relative to\nbaseline models. Our code is available at https://github.com/rockywind/ADD.", "authors": ["Zizhang Wu", "Yunzhe Wu", "Jian Pu", "Xianzhi Li", "Xiaoquan Wang"], "published_date": "2022_11_30", "pdf_url": "http://arxiv.org/pdf/2211.16779v1", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">AP_{BEV}@IoU=0.7(Car test)</td><td colspan=\"3\">AP_{BEV}@IoU=0.7(Car val)</td><td colspan=\"3\">AP_{3D}@IoU=0.7(Car val)</td></tr><tr><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td></tr><tr><td>MonoDETR (Zhang et al. 2022)</td><td>33.60</td><td>22.11</td><td>18.60</td><td>37.86</td><td>26.95</td><td>22.80</td><td>28.84</td><td>20.61</td><td>16.38</td></tr><tr><td>MonoDETR\\mathbf{+ADD}</td><td>35.20</td><td>23.58</td><td>20.08</td><td>40.38</td><td>29.07</td><td>25.05</td><td>30.71</td><td>21.94</td><td>18.42</td></tr><tr><td>Improvement</td><td>+1.60</td><td>+1.47</td><td>+1.48</td><td>+2.52</td><td>+2.12</td><td>+2.25</td><td>+1.87</td><td>+1.33</td><td>+2.04</td></tr><tr><td>CaDDN (Reading et al. 2021)</td><td>27.94</td><td>18.91</td><td>17.19</td><td>30.28</td><td>21.53</td><td>18.96</td><td>23.57</td><td>16.31</td><td>13.84</td></tr><tr><td>CaDDN\\mathbf{+ADD}</td><td>30.11</td><td>20.80</td><td>18.04</td><td>34.14</td><td>23.49</td><td>21.24</td><td>25.30</td><td>16.64</td><td>14.90</td></tr><tr><td>Improvement</td><td>+2.17</td><td>+1.89</td><td>+0.85</td><td>+3.86</td><td>+1.96</td><td>+2.28</td><td>+1.73</td><td>+0.33</td><td>+1.06</td></tr><tr><td>PatchNet (Ma et al. 2020)</td><td>22.97</td><td>16.86</td><td>14.97</td><td>41.49</td><td>23.60</td><td>19.93</td><td>31.60</td><td>16.80</td><td>13.80</td></tr><tr><td>PatchNet\\mathbf{+ADD}</td><td>28.15</td><td>17.38</td><td>15.06</td><td>42.15</td><td>24.75</td><td>20.26</td><td>32.21</td><td>16.92</td><td>13.87</td></tr><tr><td>Improvement</td><td>+5.18</td><td>+0.52</td><td>+0.09</td><td>+0.66</td><td>+1.15</td><td>+0.33</td><td>+0.61</td><td>+0.12</td><td>+0.07</td></tr></table>", "caption": "Table 1: Quantitative comparisons of the Car category on the KITTI validation and testing splits. The results are evaluated using 40 recall positions.Our improvements relative to baseline models are listed in red. ", "list_citation_info": ["Zhang et al. (2022) Zhang, R.; Qiu, H.; Wang, T.; Xu, X.; Guo, Z.; Qiao, Y.; Gao, P.; and Li, H. 2022. Monodetr: Depth-aware transformer for monocular 3d object detection. arXiv preprint arXiv:2203.13310.", "Reading et al. (2021) Reading, C.; Harakeh, A.; Chae, J.; and Waslander, S. L. 2021. Categorical depth distribution network for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8555\u20138564.", "Ma et al. (2020) Ma, X.; Liu, S.; Xia, Z.; Zhang, H.; Zeng, X.; and Ouyang, W. 2020. Rethinking pseudo-lidar representation. In European Conference on Computer Vision, 311\u2013327. Springer."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Reference</td><td rowspan=\"2\">Extra Data</td><td colspan=\"3\">AP_{BEV}(Car test)</td><td colspan=\"3\">AP_{3D}(Car test)</td></tr><tr><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td></tr><tr><td>PatchNet (Ma et al. 2020)</td><td>ECCV 2020</td><td>LiDAR</td><td>22.97</td><td>16.86</td><td>14.97</td><td>15.68</td><td>11.12</td><td>10.17</td></tr><tr><td>D4LCN (Ding et al. 2020)</td><td>CVPR 2020</td><td>LiDAR</td><td>22.51</td><td>16.02</td><td>12.55</td><td>16.65</td><td>11.72</td><td>9.51</td></tr><tr><td>DDMP-3D (Wang et al. 2021)</td><td>CVPR 2021</td><td>LiDAR</td><td>28.08</td><td>17.89</td><td>13.44</td><td>19.71</td><td>12.78</td><td>9.80</td></tr><tr><td>CaDDN (Reading et al. 2021)</td><td>CVPR 2021</td><td>LiDAR</td><td>27.94</td><td>18.91</td><td>17.19</td><td>19.17</td><td>13.41</td><td>11.46</td></tr><tr><td>MonoDTR (Huang et al. 2022)</td><td>CVPR 2022</td><td>LiDAR</td><td>28.59</td><td>20.38</td><td>17.14</td><td>21.99</td><td>15.39</td><td>12.73</td></tr><tr><td>Kinematic3D (Brazil et al. 2020)</td><td>ECCV 2020</td><td>Temporal</td><td>26.69</td><td>17.52</td><td>13.10</td><td>19.07</td><td>12.72</td><td>9.17</td></tr><tr><td>MonoDLE (Ma et al. 2021)</td><td>CVPR 2021</td><td>None</td><td>24.79</td><td>18.89</td><td>16.00</td><td>17.23</td><td>12.26</td><td>10.29</td></tr><tr><td>MonoRUn (Chen et al. 2021a)</td><td>CVPR 2021</td><td>None</td><td>27.94</td><td>17.34</td><td>15.24</td><td>19.65</td><td>12.30</td><td>10.58</td></tr><tr><td>GrooMed-NMS (Kumar, Brazil, and Liu 2021)</td><td>CVPR 2021</td><td>None</td><td>26.19</td><td>18.27</td><td>14.05</td><td>18.10</td><td>12.32</td><td>9.65</td></tr><tr><td>MonoRCNN (Shi et al. 2021)</td><td>ICCV 2021</td><td>None</td><td>25.48</td><td>18.11</td><td>14.10</td><td>18.36</td><td>12.65</td><td>10.03</td></tr><tr><td>MonoEF  (Zhou et al. 2021)</td><td>CVPR 2021</td><td>None</td><td>29.03</td><td>19.70</td><td>17.26</td><td>21.29</td><td>13.87</td><td>11.71</td></tr><tr><td>MonoFlex (Zhang, Lu, and Zhou 2021)</td><td>CVPR 2021</td><td>None</td><td>28.23</td><td>19.75</td><td>16.89</td><td>19.94</td><td>12.89</td><td>12.07</td></tr><tr><td>MonoJSG (Lian, Li, and Chen 2022)</td><td>CVPR 2022</td><td>None</td><td>\\mathbf{32.59}</td><td>21.26</td><td>18.18</td><td>\\mathbf{24.69}</td><td>16.14</td><td>13.64</td></tr><tr><td>MonoCon (Liu, Xue, and Wu 2022)</td><td>AAAI 2022</td><td>None</td><td>31.12</td><td>\\mathbf{22.10}</td><td>\\mathbf{19.00}</td><td>22.50</td><td>\\mathbf{16.46}</td><td>13.95</td></tr><tr><td>\\mathbf{Ours}</td><td>-</td><td>None</td><td>35.20</td><td>23.58</td><td>20.08</td><td>25.61</td><td>16.81</td><td>\\mathbf{13.79}</td></tr><tr><td rowspan=\"3\">Improvement</td><td>-</td><td>v.s. LiDAR</td><td>+6.61</td><td>+3.2</td><td>+2.89</td><td>+3.62</td><td>+1.42</td><td>+1.06</td></tr><tr><td>-</td><td>v.s. Temporal</td><td>+8.51</td><td>+6.06</td><td>+2.89</td><td>+6.54</td><td>+4.09</td><td>+4.62</td></tr><tr><td>-</td><td>v.s. None</td><td>+2.61</td><td>+1.48</td><td>+1.08</td><td>+0.92</td><td>+0.35</td><td>-0.16</td></tr></table>", "caption": "Table 2: Quantitative comparisons of the Car category on the KITTI test split. The best results are listed in blue and the second in \\mathbf{bold}. Our improvements relative to the best of each \u2018Extra Data\u2019 category are listed in red.", "list_citation_info": ["Lian, Li, and Chen (2022) Lian, Q.; Li, P.; and Chen, X. 2022. MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1070\u20131079.", "Ma et al. (2021) Ma, X.; Zhang, Y.; Xu, D.; Zhou, D.; Yi, S.; Li, H.; and Ouyang, W. 2021. Delving into localization errors for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4721\u20134730.", "Shi et al. (2021) Shi, X.; Ye, Q.; Chen, X.; Chen, C.; Chen, Z.; and Kim, T.-K. 2021. Geometry-based distance decomposition for monocular 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 15172\u201315181.", "Ma et al. (2020) Ma, X.; Liu, S.; Xia, Z.; Zhang, H.; Zeng, X.; and Ouyang, W. 2020. Rethinking pseudo-lidar representation. In European Conference on Computer Vision, 311\u2013327. Springer.", "Reading et al. (2021) Reading, C.; Harakeh, A.; Chae, J.; and Waslander, S. L. 2021. Categorical depth distribution network for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8555\u20138564.", "Zhou et al. (2021) Zhou, Y.; He, Y.; Zhu, H.; Wang, C.; Li, H.; and Jiang, Q. 2021. Monocular 3d object detection: An extrinsic parameter free approach. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7556\u20137566.", "Huang et al. (2022) Huang, K.-C.; Wu, T.-H.; Su, H.-T.; and Hsu, W. H. 2022. MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer. In CVPR.", "Liu, Xue, and Wu (2022) Liu, X.; Xue, N.; and Wu, T. 2022. Learning auxiliary monocular contexts helps monocular 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 1810\u20131818.", "Zhang, Lu, and Zhou (2021) Zhang, Y.; Lu, J.; and Zhou, J. 2021. Objects are Different: Flexible Monocular 3D Object Detection. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 3288\u20133297.", "Brazil et al. (2020) Brazil, G.; Pons-Moll, G.; Liu, X.; and Schiele, B. 2020. Kinematic 3d object detection in monocular video. In European Conference on Computer Vision, 135\u2013152. Springer.", "Chen et al. (2021a) Chen, H.; Huang, Y.; Tian, W.; Gao, Z.; and Xiong, L. 2021a. Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10379\u201310388.", "Kumar, Brazil, and Liu (2021) Kumar, A.; Brazil, G.; and Liu, X. 2021. Groomed-nms: Grouped mathematically differentiable nms for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8973\u20138983.", "Ding et al. (2020) Ding, M.; Huo, Y.; Yi, H.; Wang, Z.; Shi, J.; Lu, Z.; and Luo, P. 2020. Learning depth-guided convolutions for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 1000\u20131001.", "Wang et al. (2021) Wang, L.; Du, L.; Ye, X.; Fu, Y.; Guo, G.; Xue, X.; Feng, J.; and Zhang, L. 2021. Depth-conditioned dynamic message propagation for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 454\u2013463."]}, {"table": "<table><tr><td></td><td rowspan=\"2\">#</td><td rowspan=\"2\">F</td><td rowspan=\"2\">+SA</td><td rowspan=\"2\">R</td><td rowspan=\"2\">+CA</td><td colspan=\"3\">AP_{3D}@IoU=0.7(Car val)</td><td colspan=\"3\">AP_{BEV}@IoU=0.7(Car val)</td></tr><tr><td></td><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td></tr><tr><td colspan=\"6\">Teacher(with object-wise depth GT)</td><td>41.43</td><td>28.15</td><td>22.84</td><td>51.99</td><td>36.44</td><td>30.33</td></tr><tr><td colspan=\"6\">MonoDETR{}^{*}(Zhang et al. 2022)</td><td>27.63</td><td>19.93</td><td>16.70</td><td>36.64</td><td>26.32</td><td>22.35</td></tr><tr><td rowspan=\"2\">Feature-level</td><td>(a)</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>28.92</td><td>19.86</td><td>17.21</td><td>38.08</td><td>26.71</td><td>22.84</td></tr><tr><td>(b)</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>29.50</td><td>20.90</td><td>17.51</td><td>38.62</td><td>27.03</td><td>23.06</td></tr><tr><td rowspan=\"2\">Object-level</td><td>(c)</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>29.00</td><td>19.97</td><td>16.58</td><td>37.93</td><td>26.66</td><td>22.66</td></tr><tr><td>(d)</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>29.41</td><td>21.18</td><td>17.77</td><td>38.69</td><td>27.27</td><td>23.32</td></tr><tr><td></td><td>(e)</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>27.40</td><td>19.73</td><td>16.37</td><td>36.94</td><td>26.68</td><td>21.89</td></tr><tr><td>Feature-level \\&amp;</td><td>(f)</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>28.51</td><td>20.63</td><td>17.28</td><td>38.02</td><td>26.81</td><td>22.96</td></tr><tr><td>Object-level</td><td>(g)</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>29.20</td><td>20.69</td><td>17.25</td><td>37.95</td><td>26.63</td><td>22.73</td></tr><tr><td></td><td>(h)</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>30.71</td><td>21.94</td><td>18.42</td><td>40.38</td><td>29.07</td><td>25.05</td></tr><tr><td colspan=\"6\">Improvement</td><td>+3.08</td><td>+2.01</td><td>+1.72</td><td>+3.74</td><td>+2.75</td><td>+2.70</td></tr></table>", "caption": "Table 4: Ablation studies on the KITTI validation split.F denotes distillation on multi-level intermediate features, and +SA denotes adopting our 3D-aware self-attention module for feature-level adaptation.R denotes distillation on network responses, and +CA denotes adopting our target-aware cross-attention module for response-level adaptation.Our improvements are listed in red.{}^{*}The results for MonoDETR (Huang et al. 2022) are obtained by training the officially publicized code with our computational environment.", "list_citation_info": ["Zhang et al. (2022) Zhang, R.; Qiu, H.; Wang, T.; Xu, X.; Guo, Z.; Qiao, Y.; Gao, P.; and Li, H. 2022. Monodetr: Depth-aware transformer for monocular 3d object detection. arXiv preprint arXiv:2203.13310.", "Huang et al. (2022) Huang, K.-C.; Wu, T.-H.; Su, H.-T.; and Hsu, W. H. 2022. MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer. In CVPR."]}], "citation_info_to_title": {"Zhang et al. (2022) Zhang, R.; Qiu, H.; Wang, T.; Xu, X.; Guo, Z.; Qiao, Y.; Gao, P.; and Li, H. 2022. Monodetr: Depth-aware transformer for monocular 3d object detection. arXiv preprint arXiv:2203.13310.": "Monodetr: Depth-aware transformer for monocular 3d object detection", "Ma et al. (2021) Ma, X.; Zhang, Y.; Xu, D.; Zhou, D.; Yi, S.; Li, H.; and Ouyang, W. 2021. Delving into localization errors for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 4721\u20134730.": "Delving into localization errors for monocular 3d object detection", "Shi et al. (2021) Shi, X.; Ye, Q.; Chen, X.; Chen, C.; Chen, Z.; and Kim, T.-K. 2021. Geometry-based distance decomposition for monocular 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 15172\u201315181.": "Geometry-Based Distance Decomposition for Monocular 3D Object Detection", "Zhang, Lu, and Zhou (2021) Zhang, Y.; Lu, J.; and Zhou, J. 2021. Objects are Different: Flexible Monocular 3D Object Detection. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 3288\u20133297.": "Flexible Monocular 3D Object Detection", "Ma et al. (2020) Ma, X.; Liu, S.; Xia, Z.; Zhang, H.; Zeng, X.; and Ouyang, W. 2020. Rethinking pseudo-lidar representation. In European Conference on Computer Vision, 311\u2013327. Springer.": "Rethinking pseudo-lidar representation", "Brazil et al. (2020) Brazil, G.; Pons-Moll, G.; Liu, X.; and Schiele, B. 2020. Kinematic 3d object detection in monocular video. In European Conference on Computer Vision, 135\u2013152. Springer.": "Kinematic 3D Object Detection in Monocular Video", "Huang et al. (2022) Huang, K.-C.; Wu, T.-H.; Su, H.-T.; and Hsu, W. H. 2022. MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer. In CVPR.": "MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer", "Liu, Xue, and Wu (2022) Liu, X.; Xue, N.; and Wu, T. 2022. Learning auxiliary monocular contexts helps monocular 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 1810\u20131818.": "Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection", "Reading et al. (2021) Reading, C.; Harakeh, A.; Chae, J.; and Waslander, S. L. 2021. Categorical depth distribution network for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8555\u20138564.": "Categorical depth distribution network for monocular 3d object detection", "Ding et al. (2020) Ding, M.; Huo, Y.; Yi, H.; Wang, Z.; Shi, J.; Lu, Z.; and Luo, P. 2020. Learning depth-guided convolutions for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 1000\u20131001.": "Learning Depth-Guided Convolutions for Monocular 3D Object Detection", "Chen et al. (2021a) Chen, H.; Huang, Y.; Tian, W.; Gao, Z.; and Xiong, L. 2021a. Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10379\u201310388.": "Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation", "Lian, Li, and Chen (2022) Lian, Q.; Li, P.; and Chen, X. 2022. MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1070\u20131079.": "MonoJSG: Joint Semantic and Geometric Cost Volume for Monocular 3D Object Detection", "Wang et al. (2021) Wang, L.; Du, L.; Ye, X.; Fu, Y.; Guo, G.; Xue, X.; Feng, J.; and Zhang, L. 2021. Depth-conditioned dynamic message propagation for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 454\u2013463.": "Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection", "Zhou et al. (2021) Zhou, Y.; He, Y.; Zhu, H.; Wang, C.; Li, H.; and Jiang, Q. 2021. Monocular 3d object detection: An extrinsic parameter free approach. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7556\u20137566.": "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach", "Kumar, Brazil, and Liu (2021) Kumar, A.; Brazil, G.; and Liu, X. 2021. Groomed-nms: Grouped mathematically differentiable nms for monocular 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8973\u20138983.": "Groomed-nms: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection"}, "source_title_to_arxiv_id": {"Categorical depth distribution network for monocular 3d object detection": "2103.01100", "Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection": "2103.16470"}}