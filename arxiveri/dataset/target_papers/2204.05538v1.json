{"title": "NightLab: A Dual-level Architecture with Hardness Detection for Segmentation at Night", "abstract": "The semantic segmentation of nighttime scenes is a challenging problem that\nis key to impactful applications like self-driving cars. Yet, it has received\nlittle attention compared to its daytime counterpart. In this paper, we propose\nNightLab, a novel nighttime segmentation framework that leverages multiple deep\nlearning models imbued with night-aware features to yield State-of-The-Art\n(SoTA) performance on multiple night segmentation benchmarks. Notably, NightLab\ncontains models at two levels of granularity, i.e. image and regional, and each\nlevel is composed of light adaptation and segmentation modules. Given a\nnighttime image, the image level model provides an initial segmentation\nestimate while, in parallel, a hardness detection module identifies regions and\ntheir surrounding context that need further analysis. A regional level model\nfocuses on these difficult regions to provide a significantly improved\nsegmentation. All the models in NightLab are trained end-to-end using a set of\nproposed night-aware losses without handcrafted heuristics. Extensive\nexperiments on the NightCity and BDD100K datasets show NightLab achieves SoTA\nperformance compared to concurrent methods.", "authors": ["Xueqing Deng", "Peng Wang", "Xiaochen Lian", "Shawn Newsam"], "published_date": "2022_04_12", "pdf_url": "http://arxiv.org/pdf/2204.05538v1", "list_table_and_caption": [{"table": "<table><thead><tr><th colspan=\"5\">(a) NightCity+</th></tr></thead><tbody><tr><th>Network</th><td>Backbone</td><td>Resolution</td><td>NightCity+</td><td>w/Citys</td></tr><tr><th>*NightCity[44]</th><td>Res101</td><td>512x1024</td><td>51.5</td><td>53.9</td></tr><tr><th>PSPNet[60]</th><td>Res101</td><td>512x1024</td><td>54.75</td><td>56.89</td></tr><tr><th>PSPNet[60]</th><td>Res101</td><td>1024x2048</td><td>55.64</td><td>57.52</td></tr><tr><th>DeeplabV3+[6]</th><td>Res101</td><td>512x1024</td><td>54.21</td><td>58.29</td></tr><tr><th>DeeplabV3+[6]</th><td>Res101</td><td>1024x2048</td><td>54.47</td><td>59.03</td></tr><tr><th>UPerNet[26]</th><td>Swin-Base</td><td>512x1024</td><td>57.71</td><td>59.35</td></tr><tr><th>HRNetV2[49]</th><td>HRNet-W48</td><td>1024x2048</td><td>55.89</td><td>58.49</td></tr><tr><th>DANet[14]</th><td>Res101</td><td>1024x2048</td><td>55.98</td><td>57.72</td></tr><tr><th>UPer-Swin[57]</th><td>Res101</td><td>1024x2048</td><td>55.81</td><td>56.98</td></tr><tr><th>UPer-ViT [13]</th><td>ViT</td><td>1024x2048</td><td>57.13</td><td>58.07</td></tr><tr><th>UPer-Swin[26]</th><td>Swin-Base</td><td>1024x2048</td><td>58.25</td><td>59.67</td></tr><tr><th>*NightLab-HDM</th><td>Swin-Base</td><td>512x1024</td><td>59.84</td><td>61.07</td></tr><tr><th>NightLab (DeeplabV3+)</th><td>Res101</td><td>1024x2048</td><td>56.21</td><td>60.41</td></tr><tr><th>NightLab-Baseline</th><td>Swin-Base</td><td>1024x2048</td><td>59.25</td><td>60.37</td></tr><tr><th>NightLab-RDN</th><td>Swin-Base</td><td>1024x2048</td><td>60.27</td><td>62.11</td></tr><tr><th>NightLab-HDM</th><td>Swin-Base</td><td>1024x2048</td><td>60.73</td><td>62.82</td></tr><tr><th colspan=\"5\">(b) BDD100K-Night</th></tr><tr><th>Network</th><td>Backbone</td><td>Resolution</td><td>Night</td><td>w/100K</td></tr><tr><th>PSPNet[60]</th><td>Res101</td><td>720x1280</td><td>29.96</td><td>46.24</td></tr><tr><th>HRNetV2[49]</th><td>HRNet-W48</td><td>720x1280</td><td>29.86</td><td>44.32</td></tr><tr><th>DANet[14]</th><td>Res101</td><td>720x1280</td><td>29.46</td><td>42.64</td></tr><tr><th>DeeplabV3+[6]</th><td>Res101</td><td>720x1280</td><td>30.11</td><td>43.44</td></tr><tr><th>UPerNet[57]</th><td>Res101</td><td>720x1280</td><td>30.88</td><td>47.68</td></tr><tr><th>UPer-ViT [13]</th><td>ViT</td><td>720x1280</td><td>30.74</td><td>47.81</td></tr><tr><th>UPer-Swin[26]</th><td>Swin-Base</td><td>720x1280</td><td>31.74</td><td>48.04</td></tr><tr><th>NightLab (DeeplabV3+)</th><td>Res101</td><td>720x1280</td><td>31.27</td><td>45.11</td></tr><tr><th>NightLab-Baseline</th><td>Swin-Base</td><td>720x1280</td><td>32.37</td><td>48.52</td></tr><tr><th>NightLab-RDN</th><td>Swin-Base</td><td>720x1280</td><td>34.13</td><td>49.81</td></tr><tr><th>NightLab-HDM</th><td>Swin-Base</td><td>720x1280</td><td>35.41</td><td>50.42</td></tr></tbody></table>", "caption": "Table 1: Comparisons to SoTA semantic segmentation networks on NightCity+ and BDD-Night with metric of mIoU. The results of first column after \u201cResolution\u201d are models train with only night images, and the results of the column after trained with daytime data augmentation, i.e. with Cityscapes to NightCity, and BDD100K day images to BDD100K-Night. Here, for NightCity, lines with * denotes evaluation is done over the original NightCity val set since we do not have models in NightCity [44].", "list_citation_info": ["[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.", "[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.", "[14] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual Attention Network for Scene Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.", "[44] Xin Tan, Yiheng Zhang, Ying Cao, Lizhuang Ma, and Rynson WH Lau. Night-time scene parsing with a large real dataset. IEEE Transactions on Image Processing (TIP), 2021.", "[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid Scene Parsing Network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[49] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017.", "[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tbody><tr><th>Method</th><td>Adaptation Approach</td><td>Network</td><td>Nightcity+</td><td>Nightcity+ and Citys</td><td>BDD100K-Night</td><td>BDD100K</td></tr><tr><th>NightCity[44]</th><td>Exposure-Aware</td><td>Res101</td><td>51.8</td><td>53.9</td><td>-</td><td>-</td></tr><tr><th>UPerNet [26]</th><td>Segmentation</td><td>UPerNet-Swin</td><td>57.71</td><td>59.35</td><td>31.74</td><td>48.52</td></tr><tr><th>Pix2PixHD[21]</th><td>Image Translation</td><td>UPerNet-Swin</td><td>-</td><td>43.38</td><td>-</td><td>38.67</td></tr><tr><th>CycleGAN[64]</th><td>Image Translation</td><td>UPerNet-Swin</td><td>-</td><td>44.07</td><td>-</td><td>39.64</td></tr><tr><th>SingleHDR[25]</th><td>Image Enhancement</td><td>UPerNet-Swin</td><td>57.07</td><td>58.88</td><td>31.64</td><td>48.32</td></tr><tr><th>DANNet[54]</th><td>Network Adaptation</td><td>UPerNet-Swin</td><td>-</td><td>58.69</td><td>-</td><td>48.25</td></tr><tr><th>AdaptSeg[46]</th><td>Network Adaptation</td><td>UPerNet-Swin</td><td>-</td><td>58.29</td><td>-</td><td>48.32</td></tr><tr><th>NightLab-B</th><td>Segmentation</td><td>UPerNet-Swin-DeformConv</td><td>59.25</td><td>60.37</td><td>32.37</td><td>48.52</td></tr><tr><th>NightLab-RDN</th><td>Dual-level segmentation</td><td>UPerNet-Swin-DeformConv</td><td>60.27</td><td>62.11</td><td>34.13</td><td>49.81</td></tr><tr><th>NightLab-HDM</th><td>Dual-level segmentation</td><td>UPerNet-Swin-DeformConv</td><td>60.73</td><td>62.82</td><td>35.41</td><td>50.24</td></tr></tbody></table>", "caption": "Table 2: Comparison study of adaptation approaches. mIoU(%) are reported.", "list_citation_info": ["[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[44] Xin Tan, Yiheng Zhang, Ying Cao, Lizhuang Ma, and Rynson WH Lau. Night-time scene parsing with a large real dataset. IEEE Transactions on Image Processing (TIP), 2021.", "[64] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.", "[25] Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Single-image HDR reconstruction by learning to reverse the camera pipeline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[46] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.", "[54] Xinyi Wu, Zhenyao Wu, Hao Guo, Lili Ju, and Song Wang. Dannet: A one-stage domain adaptation network for unsupervised nighttime semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Backbone</th><th>FPN</th><th>Seg head</th><th>Nightcity+</th><th>+Citys</th></tr></thead><tbody><tr><td>UPerNet[57]</td><td>Res101</td><td>Conv2D</td><td>Conv2D</td><td>55.81</td><td>59.03</td></tr><tr><td>NightLab-B</td><td>Res101</td><td>Conv2D</td><td>DefConv[10]</td><td>56.31</td><td>59.33</td></tr><tr><td>NightLab-B</td><td>Res101</td><td>DefConv[10]</td><td>DefConv[10]</td><td>56.54</td><td>59.85</td></tr><tr><td>UPer-Swin[26]</td><td>Swin-Base</td><td>Conv2D</td><td>Conv2D</td><td>58.25</td><td>59.67</td></tr><tr><td>NightLab-B</td><td>Swin-Base</td><td>Conv2D</td><td>DefConv[10]</td><td>58.68</td><td>59.99</td></tr><tr><td>NightLab-B</td><td>Swin-Base</td><td>DefConv[10]</td><td>DefConv[10]</td><td>59.25</td><td>60.37</td></tr></tbody></table>", "caption": "Table 4: Ablation study on our proposed baseline architectures adding deformable convolution (DefConv [10]) to enrich contextual features for multiscale objects. Results are reported on NightCity+ val set.", "list_citation_info": ["[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[10] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017.", "[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), 2018."]}, {"table": "<table><thead><tr><th>Method</th><th>mIoU(%)</th></tr></thead><tbody><tr><td>UPerNet-Swin[[26]]</td><td>59.67</td></tr><tr><td>NightLab-B</td><td>60.37</td></tr><tr><td>NightLab-B + \\Phi^{R}_{seg} w/ RDN proposed hard regions</td><td>61.51</td></tr><tr><td>NightLab-B + \\Phi^{R}_{seg} w/ HDM proposed hard regions</td><td>62.31</td></tr><tr><td>NightLab-B + \\Phi_{light}^{I}</td><td>60.74</td></tr><tr><td>NightLab-B + \\Phi_{light}^{I} + \\Phi^{R}_{seg} w/ RDN proposed hard regions</td><td>61.87</td></tr><tr><td>NightLab-B + \\Phi_{light}^{I} + \\Phi^{R}_{seg} w/ HDM proposed hard regions</td><td>62.51</td></tr><tr><td>NightLab-B + \\Phi_{light}^{I} + \\Phi^{R}_{seg} w/ RDN proposed hard regions + \\Phi_{light}^{R}</td><td>62.11</td></tr><tr><td>NightLab-B + \\Phi_{light}^{I} + \\Phi^{R}_{seg} w/ HDM proposed hard regions + \\Phi_{light}^{R}</td><td>62.82</td></tr></tbody></table>", "caption": "Table 5: Ablation study on NightLab model variants. Models are trained jointly with NightCity+ and Cityscapes, evaluated on NightCity+ val set. NightLab-B represents our proposed baseline segmentation architecture. ", "list_citation_info": ["[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}], "citation_info_to_title": {"[64] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks", "[25] Yu-Lun Liu, Wei-Sheng Lai, Yu-Sheng Chen, Yi-Lung Kao, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Single-image HDR reconstruction by learning to reverse the camera pipeline. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "Single-image HDR reconstruction by learning to reverse the camera pipeline", "[54] Xinyi Wu, Zhenyao Wu, Hao Guo, Lili Ju, and Song Wang. Dannet: A one-stage domain adaptation network for unsupervised nighttime semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.": "Dannet: A one-stage domain adaptation network for unsupervised nighttime semantic segmentation", "[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid Scene Parsing Network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.": "Pyramid Scene Parsing Network", "[21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.": "Image-to-image translation with conditional adversarial networks", "[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation", "[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.": "Unified Perceptual Parsing for Scene Understanding", "[46] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.": "Learning to adapt structured output space for semantic segmentation", "[49] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017.": "Deep high-resolution representation learning for visual recognition", "[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[14] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual Attention Network for Scene Segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Dual Attention Network for Scene Segmentation", "[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[10] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017.": "Deformable Convolutional Networks", "[44] Xin Tan, Yiheng Zhang, Ying Cao, Lizhuang Ma, and Rynson WH Lau. Night-time scene parsing with a large real dataset. IEEE Transactions on Image Processing (TIP), 2021.": "Night-time scene parsing with a large real dataset"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}