{"title": "The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning", "abstract": "Vision transformers have recently set off a new wave in the field of medical\nimage analysis due to their remarkable performance on various computer vision\ntasks. However, recent hybrid-/transformer-based approaches mainly focus on the\nbenefits of transformers in capturing long-range dependency while ignoring the\nissues of their daunting computational complexity, high training costs, and\nredundant dependency. In this paper, we propose to employ adaptive pruning to\ntransformers for medical image segmentation and propose a lightweight and\neffective hybrid network APFormer. To our best knowledge, this is the first\nwork on transformer pruning for medical image analysis tasks. The key features\nof APFormer mainly are self-supervised self-attention (SSA) to improve the\nconvergence of dependency establishment, Gaussian-prior relative position\nembedding (GRPE) to foster the learning of position information, and adaptive\npruning to eliminate redundant computations and perception information.\nSpecifically, SSA and GRPE consider the well-converged dependency distribution\nand the Gaussian heatmap distribution separately as the prior knowledge of\nself-attention and position embedding to ease the training of transformers and\nlay a solid foundation for the following pruning operation. Then, adaptive\ntransformer pruning, both query-wise and dependency-wise, is performed by\nadjusting the gate control parameters for both complexity reduction and\nperformance improvement. Extensive experiments on two widely-used datasets\ndemonstrate the prominent segmentation performance of APFormer against the\nstate-of-the-art methods with much fewer parameters and lower GFLOPs. More\nimportantly, we prove, through ablation studies, that adaptive pruning can work\nas a plug-n-play module for performance improvement on other\nhybrid-/transformer-based methods. Code is available at\nhttps://github.com/xianlin7/APFormer.", "authors": ["Xian Lin", "Li Yu", "Kwang-Ting Cheng", "Zengqiang Yan"], "published_date": "2022_06_29", "pdf_url": "http://arxiv.org/pdf/2206.14413v2", "list_table_and_caption": [{"table": "<table><tr><td>Model Type</td><td>Method</td><td>Year</td><td>Dice (%)</td><td>IoU (%)</td><td>ACC (%)</td><td>SE (%)</td><td>SP (%)</td><td>Params (M)</td><td>GFLOPs</td></tr><tr><td rowspan=\"4\">CNNs</td><td>Att-UNet[8]</td><td>2019</td><td>85.66</td><td>77.64</td><td>93.76</td><td>86.00</td><td>98.26</td><td>45</td><td>84</td></tr><tr><td>CPFNet[30]</td><td>2020</td><td>87.69</td><td>79.88</td><td>94.96</td><td>89.53</td><td>96.55</td><td>43</td><td>16</td></tr><tr><td>DAGAN[31]</td><td>2020</td><td>88.07</td><td>81.13</td><td>93.24</td><td>90.72</td><td>95.88</td><td>54</td><td>62</td></tr><tr><td>CKDNet[32]</td><td>2021</td><td>87.79</td><td>80.41</td><td>94.92</td><td>90.55</td><td>97.01</td><td>51</td><td>44</td></tr><tr><td rowspan=\"4\">Transformer</td><td>SETR-PUP{}^{*}[5]</td><td>2021</td><td>88.03</td><td>80.53</td><td>95.51</td><td>91.51</td><td>96.52</td><td>39</td><td>40</td></tr><tr><td>TransUnet{}^{*}[9]</td><td>2021</td><td>88.88</td><td>81.85</td><td>95.94</td><td>90.08</td><td>97.89</td><td>105</td><td>25</td></tr><tr><td>FAT-Net[29]</td><td>2022</td><td>89.03</td><td>82.02</td><td>95.78</td><td>91.00</td><td>96.99</td><td>30</td><td>23</td></tr><tr><td>APFormer</td><td>2022</td><td>90.70</td><td>84.34</td><td>96.50</td><td>90.93</td><td>97.81</td><td>2.6</td><td>4.1</td></tr></table><ul><li>\u2022<p>* represents the re-implemented results based on released code</p></li></ul>", "caption": "TABLE I: Comparison results against the state-of-the-art approaches on the ISIC 2018 dataset.", "list_citation_info": ["[30] S. Feng et al., \u201cCPFNet: Context pyramid fusion network for medical image segmentation,\u201d IEEE Trans. Med. Imag., vol. 39, no. 10, pp. 3008-3018, 2020.", "[9] J. Chen et al., \u201cTransUNet: Transformers make strong encoders for medical image segmentation,\u201d 2021, arXiv:2102.04306.", "[29] H. Huisi et al., \u201cFAT-Net: Feature adaptive transformers for automated skin lesion segmentation,\u201d Med. Image Anal., vol. 76, pp. 102327, 2022.", "[5] S. Zheng et al., \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proc. CVPR, 2021, pp. 6881-6890.", "[8] J. Schlemper et al., \u201cAttention gated networks: Learning to leverage salient regions in medical images,\u201d Med. Image Anal., vol. 53, pp. 197-207, 2019.", "[31] B. Lei et al., \u201cSkin lesion segmentation via generative adversarial networks with dual discriminators,\u201d Med. Image Anal., vol. 64, pp. 101716, 2020.", "[32] Q. Jin, H. Cui, C. Sun, Z. Meng, and R. San, \u201cCascade knowledge diffusion network for skin lesion diagnosis and segmentation,\u201d Appl. Soft Comput., vol. 99, pp. 106881, 2021."]}, {"table": "<table><tr><td>Model Type</td><td>Method</td><td>Avg.</td><td>Aotra</td><td>Gall</td><td>L-Kid</td><td>R-Kid</td><td>Liver</td><td>Pancreas</td><td>Spleen</td><td>Stomach</td><td>P (M)</td><td>GFLOPs</td></tr><tr><td rowspan=\"4\">CNNs</td><td>R50 U-Net[9]</td><td>74.68</td><td>87.74</td><td>63.66</td><td>80.60</td><td>78.19</td><td>93.74</td><td>56.90</td><td>85.87</td><td>74.16</td><td>-</td><td>-</td></tr><tr><td>R50 Att-UNet [9]</td><td>75.57</td><td>55.92</td><td>63.91</td><td>79.20</td><td>72.71</td><td>93.56</td><td>49.37</td><td>87.19</td><td>74.95</td><td>-</td><td>-</td></tr><tr><td>U-Net[6]</td><td>76.85</td><td>89.07</td><td>69.72</td><td>77.77</td><td>68.60</td><td>93.43</td><td>53.98</td><td>86.67</td><td>75.58</td><td>40</td><td>89</td></tr><tr><td>Att-UNet[8]</td><td>77.77</td><td>89.55</td><td>68.88</td><td>77.98</td><td>71.11</td><td>93.57</td><td>58.04</td><td>87.30</td><td>75.75</td><td>45</td><td>84</td></tr><tr><td rowspan=\"4\"> 3DTransformer </td><td>CoTr{}^{\\star}[14]</td><td>72.60</td><td>83.27</td><td>60.41</td><td>79.58</td><td>73.01</td><td>91.93</td><td>45.07</td><td>82.84</td><td>64.67</td><td>42</td><td>377</td></tr><tr><td>UNETR{}^{\\star}[33]</td><td>79.56</td><td>89.99</td><td>60.56</td><td>85.66</td><td>84.80</td><td>94.46</td><td>59.25</td><td>87.81</td><td>73.99</td><td>92</td><td>86</td></tr><tr><td>nnFormer{}^{\\star}[34]</td><td>86.57</td><td>92.04</td><td>70.17</td><td>86.57</td><td>86.25</td><td>96.84</td><td>83.35</td><td>90.51</td><td>86.83</td><td>159</td><td>158</td></tr><tr><td>D-Former{}^{\\star}[23]</td><td>88.83</td><td>92.12</td><td>80.09</td><td>92.60</td><td>91.91</td><td>96.99</td><td>76.67</td><td>93.78</td><td>86.44</td><td>44</td><td>54</td></tr><tr><td rowspan=\"7\"> 2DTransformer </td><td>TransUnet{}^{\\diamond}[9]</td><td>77.48</td><td>87.23</td><td>63.13</td><td>81.87</td><td>77.02</td><td>94.08</td><td>55.86</td><td>85.08</td><td>75.62</td><td>105</td><td>25</td></tr><tr><td>Swin-UNet{}^{\\diamond}[35]</td><td>79.13</td><td>85.47</td><td>66.53</td><td>83.28</td><td>79.61</td><td>94.29</td><td>56.58</td><td>90.66</td><td>76.60</td><td>41</td><td>12</td></tr><tr><td>TransClaw U-Net[39]</td><td>78.09</td><td>85.87</td><td>61.38</td><td>84.83</td><td>79.36</td><td>94.28</td><td>57.65</td><td>87.74</td><td>73.55</td><td>-</td><td>-</td></tr><tr><td>LeVit-Unet-384{}^{\\diamond}[36]</td><td>78.53</td><td>87.33</td><td>62.23</td><td>84.61</td><td>80.25</td><td>93.11</td><td>59.07</td><td>88.86</td><td>72.76</td><td>52</td><td>26</td></tr><tr><td>MT-UNet[37]</td><td>78.59</td><td>87.92</td><td>64.99</td><td>81.47</td><td>77.29</td><td>93.06</td><td>59.46</td><td>87.75</td><td>76.81</td><td>-</td><td>-</td></tr><tr><td>MISSFormer[22]</td><td>81.96</td><td>86.99</td><td>68.65</td><td>85.21</td><td>82.00</td><td>94.41</td><td>65.67</td><td>91.92</td><td>80.81</td><td>-</td><td>-</td></tr><tr><td>CA-GANformer{}^{\\diamond}[38]</td><td>82.55</td><td>89.05</td><td>67.48</td><td>86.05</td><td>82.17</td><td>95.61</td><td>67.49</td><td>91.00</td><td>81.55</td><td>-</td><td>-</td></tr><tr><td></td><td>APFormer</td><td>83.53</td><td>90.84</td><td>64.36</td><td>90.54</td><td>85.99</td><td>94.93</td><td>72.16</td><td>91.88</td><td>77.55</td><td>2.6</td><td>3.9</td></tr></table><ul><li>\u2022<p>\\diamond represents the model needs pre-train on the large-scale dataset and \\star represents the model needs to train more than 1000 rounds.</p></li></ul>", "caption": "TABLE II: Comparison results against the state-of-the-art approaches on the Synapse dataset. Gall, L-Kid, R-Kid, and P are the abbreviations of gallbladder, left kidney, right kidney, and parameters.", "list_citation_info": ["[6] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: Convolutional networks for biomedical image segmentation,\u201d in Proc. MICCAI, 2015, pp. 234-241.", "[9] J. Chen et al., \u201cTransUNet: Transformers make strong encoders for medical image segmentation,\u201d 2021, arXiv:2102.04306.", "[34] H. Zhou et al., \u201cnnFormer: Interleaved transformer for volumetric segmentation,\u201d 2021, arXiv:2109.03201.", "[35] H. Cao et al., \u201cSwin-UNet: UNet-like pure transformer for medical image segmentation,\u201d 2021, arXiv:2105.05537.", "[8] J. Schlemper et al., \u201cAttention gated networks: Learning to leverage salient regions in medical images,\u201d Med. Image Anal., vol. 53, pp. 197-207, 2019.", "[23] Y. Wu et al., \u201cD-Former: A U-shaped dilated transformer for 3D medical image segmentation,\u201d 2022, arXiv:2201.00462.", "[14] Y. Xie, J. Zhang, C. Shen, and Y. Xia, \u201cCoTr: Efficiently bridging CNN and transformer for 3D medical image segmentation,\u201d 2021, arXiv:2103.03024.", "[37] X. Huang, Z. Deng, D. Li, and X. Yuan, \u201cMISSFormer: An effective medical image segmentation transformer,\u201d 2021, arXiv:2109.07162.", "[33] A. Hatamizadeh et al., \u201cUNETR: Transformers for 3D medical image segmentation,\u201d in Proc. WACV, 2022, pp. 574-584.", "[36] G. Xu, X. Wu, X. Zhang, and X. He, \u201cLeViT-UNet: Make faster encoders with transformer for medical image segmentation,\u201d 2021, arXiv:2107.08623.", "[38] P. Tschandl, C. Rosendahl, and H. Kittler, \u201cThe HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions,\u201d Sci. Data, vol. 5, no. 1, pp. 1-9, 2018.", "[22] X. Huang, Z. Deng, D. Li, and X. Yuan, \u201cMISSFormer: An effective medical image segmentation transformer,\u201d 2021, arXiv:2109.07162.", "[39] Y. Chang, H. Menghan, Z. Guangtao, and Z. Xiao-Ping, \u201cTransClaw U-Net: Claw U-Net with transformers for medical image segmentation,\u201d 2021, arXiv:2107.05188."]}], "citation_info_to_title": {"[30] S. Feng et al., \u201cCPFNet: Context pyramid fusion network for medical image segmentation,\u201d IEEE Trans. Med. Imag., vol. 39, no. 10, pp. 3008-3018, 2020.": "CPFNet: Context pyramid fusion network for medical image segmentation", "[5] S. Zheng et al., \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proc. CVPR, 2021, pp. 6881-6890.": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers", "[9] J. Chen et al., \u201cTransUNet: Transformers make strong encoders for medical image segmentation,\u201d 2021, arXiv:2102.04306.": "TransUNet: Transformers make strong encoders for medical image segmentation", "[38] P. Tschandl, C. Rosendahl, and H. Kittler, \u201cThe HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions,\u201d Sci. Data, vol. 5, no. 1, pp. 1-9, 2018.": "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions", "[31] B. Lei et al., \u201cSkin lesion segmentation via generative adversarial networks with dual discriminators,\u201d Med. Image Anal., vol. 64, pp. 101716, 2020.": "Skin lesion segmentation via generative adversarial networks with dual discriminators", "[23] Y. Wu et al., \u201cD-Former: A U-shaped dilated transformer for 3D medical image segmentation,\u201d 2022, arXiv:2201.00462.": "D-Former: A U-shaped dilated transformer for 3D medical image segmentation", "[6] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: Convolutional networks for biomedical image segmentation,\u201d in Proc. MICCAI, 2015, pp. 234-241.": "U-Net: Convolutional networks for biomedical image segmentation", "[14] Y. Xie, J. Zhang, C. Shen, and Y. Xia, \u201cCoTr: Efficiently bridging CNN and transformer for 3D medical image segmentation,\u201d 2021, arXiv:2103.03024.": "CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation", "[8] J. Schlemper et al., \u201cAttention gated networks: Learning to leverage salient regions in medical images,\u201d Med. Image Anal., vol. 53, pp. 197-207, 2019.": "Attention gated networks: Learning to leverage salient regions in medical images", "[36] G. Xu, X. Wu, X. Zhang, and X. He, \u201cLeViT-UNet: Make faster encoders with transformer for medical image segmentation,\u201d 2021, arXiv:2107.08623.": "LeViT-UNet: Make faster encoders with transformer for medical image segmentation", "[39] Y. Chang, H. Menghan, Z. Guangtao, and Z. Xiao-Ping, \u201cTransClaw U-Net: Claw U-Net with transformers for medical image segmentation,\u201d 2021, arXiv:2107.05188.": "TransClaw U-Net: Claw U-Net with transformers for medical image segmentation", "[35] H. Cao et al., \u201cSwin-UNet: UNet-like pure transformer for medical image segmentation,\u201d 2021, arXiv:2105.05537.": "Swin-UNet: UNet-like pure transformer for medical image segmentation", "[34] H. Zhou et al., \u201cnnFormer: Interleaved transformer for volumetric segmentation,\u201d 2021, arXiv:2109.03201.": "nnFormer: Interleaved transformer for volumetric segmentation", "[33] A. Hatamizadeh et al., \u201cUNETR: Transformers for 3D medical image segmentation,\u201d in Proc. WACV, 2022, pp. 574-584.": "UNETR: Transformers for 3D Medical Image Segmentation", "[22] X. Huang, Z. Deng, D. Li, and X. Yuan, \u201cMISSFormer: An effective medical image segmentation transformer,\u201d 2021, arXiv:2109.07162.": "MISSFormer: An effective medical image segmentation transformer", "[37] X. Huang, Z. Deng, D. Li, and X. Yuan, \u201cMISSFormer: An effective medical image segmentation transformer,\u201d 2021, arXiv:2109.07162.": "MISSFormer: An effective medical image segmentation transformer", "[29] H. Huisi et al., \u201cFAT-Net: Feature adaptive transformers for automated skin lesion segmentation,\u201d Med. Image Anal., vol. 76, pp. 102327, 2022.": "FAT-Net: Feature adaptive transformers for automated skin lesion segmentation", "[32] Q. Jin, H. Cui, C. Sun, Z. Meng, and R. San, \u201cCascade knowledge diffusion network for skin lesion diagnosis and segmentation,\u201d Appl. Soft Comput., vol. 99, pp. 106881, 2021.": "Cascade knowledge diffusion network for skin lesion diagnosis and segmentation"}, "source_title_to_arxiv_id": {"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers": "2012.15840", "TransUNet: Transformers make strong encoders for medical image segmentation": "2102.04306", "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions": "1803.10417", "TransClaw U-Net: Claw U-Net with transformers for medical image segmentation": "2107.05188", "Swin-UNet: UNet-like pure transformer for medical image segmentation": "2105.05537", "nnFormer: Interleaved transformer for volumetric segmentation": "2109.03201"}}