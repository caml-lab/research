{"title": "Controllable and Guided Face Synthesis for Unconstrained Face Recognition", "abstract": "Although significant advances have been made in face recognition (FR), FR in\nunconstrained environments remains challenging due to the domain gap between\nthe semi-constrained training datasets and unconstrained testing scenarios. To\naddress this problem, we propose a controllable face synthesis model (CFSM)\nthat can mimic the distribution of target datasets in a style latent space.\nCFSM learns a linear subspace with orthogonal bases in the style latent space\nwith precise control over the diversity and degree of synthesis. Furthermore,\nthe pre-trained synthesis model can be guided by the FR model, making the\nresulting images more beneficial for FR model training. Besides, target dataset\ndistributions are characterized by the learned orthogonal bases, which can be\nutilized to measure the distributional similarity among face datasets. Our\napproach yields significant performance gains on unconstrained benchmarks, such\nas IJB-B, IJB-C, TinyFace and IJB-S (+5.76% Rank1).", "authors": ["Feng Liu", "Minchul Kim", "Anil Jain", "Xiaoming Liu"], "published_date": "2022_07_20", "pdf_url": "http://arxiv.org/pdf/2207.10180v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Train Data, #labeled(+#unlabeled)</th><th rowspan=\"2\">Backbone</th><td colspan=\"3\">Verification</td><td colspan=\"2\">Identification</td></tr><tr><td>1e-5</td><td>1e-4</td><td>1e-3</td><td>Rank1</td><td>Rank5</td></tr><tr><th>VGGFace2 [7]</th><th>VGGFace2, 3.3M</th><th>SE-ResNet-50</th><td>70.50</td><td>83.10</td><td>90.80</td><td>90.20</td><td>94.6</td></tr><tr><th>AFRN [35]</th><th>VGGFace2-*, 3.1M</th><th>ResNet-101</th><td>77.10</td><td>88.50</td><td>94.90</td><td>97.30</td><td>97.60</td></tr><tr><th>ArcFace [12]</th><th>MS1MV2, 5.8M</th><th>ResNet-50</th><td>84.28</td><td>91.66</td><td>94.81</td><td>92.95</td><td>95.60</td></tr><tr><th>MagFace [53]</th><th>MS1MV2, 5.8M</th><th>ResNet-50</th><td>83.87</td><td>91.47</td><td>94.67</td><td>-</td><td>-</td></tr><tr><th>Shi et al. [69]</th><th>cleaned MS1MV2, 3.9M(+70K)</th><th>ResNet-50</th><td>88.19</td><td>92.78</td><td>95.86</td><td>95.86</td><td>96.72</td></tr><tr><th>ArcFace</th><th>cleaned MS1MV2, 3.9M</th><th>ResNet-50</th><td>87.26</td><td>94.01</td><td>95.95</td><td>94.61</td><td>96.52</td></tr><tr><th>ArcFace+Ours</th><th>cleaned MS1MV2, 3.9M(+70K)</th><th>ResNet-50</th><td>90.95</td><td>94.61</td><td>96.21</td><td>94.96</td><td>96.84</td></tr></tbody></table>", "caption": "Table 1:  Comparison with state-of-the-art methods on the IJB-B benchmark. \u2003\u2003\u2003\u2018*\u2019 denotes a subset of data selected by the authors.", "list_citation_info": ["[7] Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: VGGface2: A dataset for recognising faces across pose and age. In: FG (2018)", "[12] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: ArcFace: Additive angular margin loss for deep face recognition. In: CVPR (2019)", "[53] Meng, Q., Zhao, S., Huang, Z., Zhou, F.: MagFace: A universal representation for face recognition and quality assessment. In: CVPR (2021)", "[35] Kang, B.N., Kim, Y., Jun, B., Kim, D.: Attentional feature-pair relation networks for accurate face recognition. In: ICCV (2019)", "[69] Shi, Y., Jain, A.K.: Boosting unconstrained face recognition with auxiliary unlabeled data. In: CVPRW (2021)"]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Train Data, #labeled(+#unlabeled)</th><th rowspan=\"2\">Backbone</th><td colspan=\"3\">Verification</td><td colspan=\"2\">Identification</td></tr><tr><td>1e-6</td><td>1e-5</td><td>1e-4</td><td>Rank1</td><td>Rank5</td></tr><tr><th>VGGFace2 [7]</th><th>VGGFace2, 3.3M</th><th>SE-ResNet-50</th><td>-</td><td>76.80</td><td>86.20</td><td>91.40</td><td>95.10</td></tr><tr><th>AFRN [35]</th><th>VGGFace2-*, 3.1M</th><th>ResNet-101</th><td>-</td><td>88.30</td><td>93.00</td><td>95.70</td><td>97.60</td></tr><tr><th>PFE [68]</th><th>MS1M-*, 4.4M</th><th>ResNet-64</th><td>-</td><td>89.64</td><td>93.25</td><td>95.49</td><td>97.17</td></tr><tr><th>DUL [8]</th><th>MS1M-*, 3.6M</th><th>ResNet-64</th><td>-</td><td>90.23</td><td>94.20</td><td>95.70</td><td>97.60</td></tr><tr><th>ArcFace [12]</th><th>MS1MV2, 5.8M</th><th>ResNet-50</th><td>80.52</td><td>88.36</td><td>92.52</td><td>93.26</td><td>95.33</td></tr><tr><th>MagFace [53]</th><th>MS1MV2, 5.8M</th><th>ResNet-50</th><td>81.69</td><td>88.95</td><td>93.34</td><td>-</td><td>-</td></tr><tr><th>Shi et al. [69]</th><th>cleaned MS1MV2, 3.9M(+70K)</th><th>ResNet-50</th><td>87.92</td><td>91.86</td><td>94.66</td><td>95.61</td><td>97.13</td></tr><tr><th>ArcFace</th><th>cleaned MS1MV2, 3.9M</th><th>ResNet-50</th><td>87.24</td><td>93.32</td><td>95.61</td><td>95.89</td><td>97.08</td></tr><tr><th>ArcFace+ours</th><th>cleaned MS1MV2, 3.9M(+70K)</th><th>ResNet-50</th><td>89.34</td><td>94.06</td><td>95.90</td><td>96.31</td><td>97.48</td></tr></tbody></table>", "caption": "Table 2:  Comparison with state-of-the-art methods on the IJB-C benchmark.", "list_citation_info": ["[7] Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: VGGface2: A dataset for recognising faces across pose and age. In: FG (2018)", "[8] Chang, J., Lan, Z., Cheng, C., Wei, Y.: Data uncertainty learning in face recognition. In: CVPR (2020)", "[12] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: ArcFace: Additive angular margin loss for deep face recognition. In: CVPR (2019)", "[53] Meng, Q., Zhao, S., Huang, Z., Zhou, F.: MagFace: A universal representation for face recognition and quality assessment. In: CVPR (2021)", "[68] Shi, Y., Jain, A.K.: Probabilistic face embeddings. In: ICCV (2019)", "[35] Kang, B.N., Kim, Y., Jun, B., Kim, D.: Attentional feature-pair relation networks for accurate face recognition. In: ICCV (2019)", "[69] Shi, Y., Jain, A.K.: Boosting unconstrained face recognition with auxiliary unlabeled data. In: CVPRW (2021)"]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">LabeledTrain Data</td><td rowspan=\"2\">Backbone</td><td colspan=\"4\">IJB-S V2S</td><td colspan=\"4\">IJB-S V2B</td><td colspan=\"4\">IJB-S V2V</td><td colspan=\"2\">TinyFace</td></tr><tr><td>Rank1</td><td>Rank5</td><td>1\\%</td><td>10\\%</td><td>Rank1</td><td>Rank5</td><td>1\\%</td><td>10\\%</td><td>Rank1</td><td>Rank5</td><td>1\\%</td><td>10\\%</td><td>Rank1</td><td>Rank5</td></tr><tr><td>C-FAN [20]</td><td>MS1M-*</td><td>ResNet-64</td><td>50.82</td><td>61.16</td><td>16.44</td><td>24.19</td><td>53.04</td><td>62.67</td><td>27.40</td><td>29.70</td><td>10.05</td><td>17.55</td><td>0.11</td><td>0.68</td><td>-</td><td>-</td></tr><tr><td>MARN [21]</td><td>MS1M-*</td><td>ResNet-64</td><td>58.14</td><td>64.11</td><td>21.47</td><td>-</td><td>59.26</td><td>65.93</td><td>32.07</td><td>-</td><td>22.25</td><td>34.16</td><td>0.19</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PFE [68]</td><td>MS1M-*</td><td>ResNet-64</td><td>50.16</td><td>58.33</td><td>31.88</td><td>35.33</td><td>53.60</td><td>61.75</td><td>35.99</td><td>39.82</td><td>9.20</td><td>20.82</td><td>0.84</td><td>2.83</td><td>-</td><td>-</td></tr><tr><td>ArcFace [12]</td><td>MS1MV2</td><td>ResNet-50</td><td>50.39</td><td>60.42</td><td>32.39</td><td>42.99</td><td>52.25</td><td>61.19</td><td>34.87</td><td>43.50</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Shi et al. [69]</td><td>MS1MV2-*</td><td>ResNet-50</td><td>59.29</td><td>66.91</td><td>39.92</td><td>50.49</td><td>60.58</td><td>67.70</td><td>32.39</td><td>44.32</td><td>17.35</td><td>28.34</td><td>1.16</td><td>5.37</td><td>-</td><td>-</td></tr><tr><td>ArcFace [12]</td><td>MS1MV2-*</td><td>ResNet-50</td><td>58.78</td><td>66.40</td><td>40.99</td><td>50.45</td><td>60.66</td><td>67.43</td><td>43.12</td><td>51.38</td><td>14.81</td><td>26.72</td><td>2.51</td><td>5.72</td><td>62.21</td><td>66.85</td></tr><tr><td>ArcFace+Ours*</td><td>MS1MV2-*</td><td>ResNet-50</td><td>61.69</td><td>68.33</td><td>43.99</td><td>53.34</td><td>62.20</td><td>69.50</td><td>44.38</td><td>53.49</td><td>18.14</td><td>31.34</td><td>2.09</td><td>4.51</td><td>62.39</td><td>67.36</td></tr><tr><td>ArcFace+Ours</td><td>MS1MV2-*</td><td>ResNet-50</td><td>63.86</td><td>69.95</td><td>47.86</td><td>56.44</td><td>65.95</td><td>71.16</td><td>47.28</td><td>57.24</td><td>21.38</td><td>35.11</td><td>2.96</td><td>7.41</td><td>63.01</td><td>68.21</td></tr><tr><td>AdaFace [43]</td><td>WebFace12M</td><td>IResNet-100</td><td>71.35</td><td>76.24</td><td>59.40</td><td>66.34</td><td>71.93</td><td>76.56</td><td>59.37</td><td>66.68</td><td>36.71</td><td>50.03</td><td>4.62</td><td>11.84</td><td>72.29</td><td>74.97</td></tr><tr><td>AdaFace+Ours</td><td>WebFace12M</td><td>IResNet-100</td><td>72.54</td><td>77.59</td><td>60.94</td><td>66.02</td><td>72.65</td><td>78.18</td><td>60.26</td><td>65.88</td><td>39.14</td><td>50.91</td><td>5.05</td><td>13.17</td><td>73.87</td><td>76.77</td></tr></tbody></table>", "caption": "Table 3:  Comparison with state-of-the-art methods on three protocols of the IJB-S and TinyFace benchmark. The performance is reported in terms of rank retrieval (closed-set) and TAR@FAR (open-set). It is worth noting that MARN [21] is a multi-mode aggregation method and is fine-tuned on UMDFaceVideo [3], a video dataset.", "list_citation_info": ["[3] Bansal, A., Castillo, C., Ranjan, R., Chellappa, R.: The do\u2019s and don\u2019ts for CNN-based face verification. In: ICCVW (2017)", "[20] Gong, S., Shi, Y., Kalka, N.D., Jain, A.K.: Video face recognition: Component-wise feature aggregation network. In: ICB (2019)", "[43] Kim, M., Jain, A.K., Liu, X.: AdaFace: Quality adaptive margin for face recognition. In: CVPR (2022)", "[12] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: ArcFace: Additive angular margin loss for deep face recognition. In: CVPR (2019)", "[21] Gong, S., Shi, Y., Jain, A.: Low quality video face recognition: Multi-mode aggregation recurrent network. In: ICCVW (2019)", "[68] Shi, Y., Jain, A.K.: Probabilistic face embeddings. In: ICCV (2019)", "[69] Shi, Y., Jain, A.K.: Boosting unconstrained face recognition with auxiliary unlabeled data. In: CVPRW (2021)"]}], "citation_info_to_title": {"[8] Chang, J., Lan, Z., Cheng, C., Wei, Y.: Data uncertainty learning in face recognition. In: CVPR (2020)": "Data Uncertainty Learning in Face Recognition", "[7] Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: VGGface2: A dataset for recognising faces across pose and age. In: FG (2018)": "VGGface2: A dataset for recognising faces across pose and age", "[43] Kim, M., Jain, A.K., Liu, X.: AdaFace: Quality adaptive margin for face recognition. In: CVPR (2022)": "AdaFace: Quality Adaptive Margin for Face Recognition", "[21] Gong, S., Shi, Y., Jain, A.: Low quality video face recognition: Multi-mode aggregation recurrent network. In: ICCVW (2019)": "Low quality video face recognition: Multi-mode aggregation recurrent network", "[35] Kang, B.N., Kim, Y., Jun, B., Kim, D.: Attentional feature-pair relation networks for accurate face recognition. In: ICCV (2019)": "Attentional feature-pair relation networks for accurate face recognition", "[20] Gong, S., Shi, Y., Kalka, N.D., Jain, A.K.: Video face recognition: Component-wise feature aggregation network. In: ICB (2019)": "Video Face Recognition: Component-wise Feature Aggregation Network", "[69] Shi, Y., Jain, A.K.: Boosting unconstrained face recognition with auxiliary unlabeled data. In: CVPRW (2021)": "Boosting Unconstrained Face Recognition with Auxiliary Unlabeled Data", "[12] Deng, J., Guo, J., Xue, N., Zafeiriou, S.: ArcFace: Additive angular margin loss for deep face recognition. In: CVPR (2019)": "ArcFace: Additive angular margin loss for deep face recognition", "[3] Bansal, A., Castillo, C., Ranjan, R., Chellappa, R.: The do\u2019s and don\u2019ts for CNN-based face verification. In: ICCVW (2017)": "The dos and donts for CNN-based face verification", "[68] Shi, Y., Jain, A.K.: Probabilistic face embeddings. In: ICCV (2019)": "Probabilistic Face Embeddings", "[53] Meng, Q., Zhao, S., Huang, Z., Zhou, F.: MagFace: A universal representation for face recognition and quality assessment. In: CVPR (2021)": "MagFace: A universal representation for face recognition and quality assessment"}, "source_title_to_arxiv_id": {"VGGface2: A dataset for recognising faces across pose and age": "1710.08092", "ArcFace: Additive angular margin loss for deep face recognition": "1801.07698"}}