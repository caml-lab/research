{"title": "Dressing Avatars: Deep Photorealistic Appearance for Physically Simulated Clothing", "abstract": "Despite recent progress in developing animatable full-body avatars, realistic\nmodeling of clothing - one of the core aspects of human self-expression -\nremains an open challenge. State-of-the-art physical simulation methods can\ngenerate realistically behaving clothing geometry at interactive rates.\nModeling photorealistic appearance, however, usually requires physically-based\nrendering which is too expensive for interactive applications. On the other\nhand, data-driven deep appearance models are capable of efficiently producing\nrealistic appearance, but struggle at synthesizing geometry of highly dynamic\nclothing and handling challenging body-clothing configurations. To this end, we\nintroduce pose-driven avatars with explicit modeling of clothing that exhibit\nboth photorealistic appearance learned from real-world data and realistic\nclothing dynamics. The key idea is to introduce a neural clothing appearance\nmodel that operates on top of explicit geometry: at training time we use\nhigh-fidelity tracking, whereas at animation time we rely on physically\nsimulated geometry. Our core contribution is a physically-inspired appearance\nnetwork, capable of generating photorealistic appearance with view-dependent\nand dynamic shadowing effects even for unseen body-clothing configurations. We\nconduct a thorough evaluation of our model and demonstrate diverse animation\nresults on several subjects and different types of clothing. Unlike previous\nwork on photorealistic full-body avatars, our approach can produce much richer\ndynamics and more realistic deformations even for many examples of loose\nclothing. We also demonstrate that our formulation naturally allows clothing to\nbe used with avatars of different people while staying fully animatable, thus\nenabling, for the first time, photorealistic avatars with novel clothing.", "authors": ["Donglai Xiang", "Timur Bagautdinov", "Tuur Stuyck", "Fabian Prada", "Javier Romero", "Weipeng Xu", "Shunsuke Saito", "Jingfan Guo", "Breannan Smith", "Takaaki Shiratori", "Yaser Sheikh", "Jessica Hodgins", "Chenglei Wu"], "published_date": "2022_06_30", "pdf_url": "http://arxiv.org/pdf/2206.15470v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td><p>Method</p></td><td><p>MSE\\downarrow</p></td><td><p>SSIM\\uparrow</p></td></tr><tr><td><p>Network components: mean texture</p></td><td><p>316.48</p></td><td><p>0.67</p></td></tr><tr><td><p>Network components: mean texture + shadow</p></td><td><p>260.44</p></td><td><p>0.68</p></td></tr><tr><td><p>Network components: view-independent + shadow</p></td><td><p>189.29</p></td><td><p>0.73</p></td></tr><tr><td><p>Network components: view-independent + view-dependent</p></td><td><p>177.27</p></td><td><p>0.75</p></td></tr><tr><td><p>Our full model (network components: view-independent + view-dependent + shadow)</p></td><td>155.31</td><td>0.76</td></tr><tr><td><p>Geometry input conditioning: raw vertices</p></td><td><p>206.10</p></td><td><p>0.72</p></td></tr><tr><td><p>Geometry input conditioning: unposed normal</p></td><td><p>160.38</p></td><td>0.76</td></tr><tr><td><p>Geometry input conditioning: unposed vertices</p></td><td><p>161.08</p></td><td>0.76</td></tr><tr><td><p>Our full model (geometry input conditioning: normal)</p></td><td>155.31</td><td>0.76</td></tr><tr><td><p>W/o photometric alignment for training data (test on data w/o photometric alignment)</p></td><td><p>597.71</p></td><td><p>0.44</p></td></tr><tr><td><p>W/o photometric alignment for training data (test on data w/ photometric alignment)</p></td><td><p>483.73</p></td><td><p>0.46</p></td></tr><tr><td><p>Our full model (w/ photometric alignment for training data)</p></td><td>155.31</td><td>0.76</td></tr><tr><td><p>Previous method: Dynamic Neural Garments (Zhanget al., 2021a)</p></td><td><p>326.58</p></td><td><p>0.57</p></td></tr><tr><td><p>Previous method: Clothing Codec Avatars (Xiang et al., 2021) (texture only)</p></td><td><p>261.28</p></td><td><p>0.64</p></td></tr><tr><td><p>Previous method: Clothing Codec Avatars (Xiang et al., 2021) (geometry + texture)</p></td><td><p>379.87</p></td><td><p>0.58</p></td></tr><tr><td><p>Our full model</p></td><td>155.31</td><td>0.76</td></tr></tbody></table>", "caption": "Table 1. Quantitative evaluation of the clothing appearance model applied on tracked clothing geometry. Mean Squared Error (MSE, the lower the better) and Structural Similarity Index Measure (SSIM, the higher the better) are reported. We conduct ablation studies on the effectiveness of each network component, type of input conditioning, and photometric alignment for the training data. We also compare with similar modules in the previous work.", "list_citation_info": ["Zhang et al. (2021a) Meng Zhang, Duygu Ceylan, Tuanfeng Y. Wang, and Niloy J. Mitra. 2021a. Dynamic Neural Garments. ACM Transactions on Graphics (TOG) 40, 6 (2021).", "Xiang et al. (2021) Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. 2021. Modeling clothing as a separate layer for an animatable human avatar. ACM Transactions on Graphics (TOG) 40, 6 (2021)."]}], "citation_info_to_title": {"Xiang et al. (2021) Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins, and Chenglei Wu. 2021. Modeling clothing as a separate layer for an animatable human avatar. ACM Transactions on Graphics (TOG) 40, 6 (2021).": "Modeling clothing as a separate layer for an animatable human avatar", "Zhang et al. (2021a) Meng Zhang, Duygu Ceylan, Tuanfeng Y. Wang, and Niloy J. Mitra. 2021a. Dynamic Neural Garments. ACM Transactions on Graphics (TOG) 40, 6 (2021).": "Dynamic Neural Garments"}, "source_title_to_arxiv_id": {"Modeling clothing as a separate layer for an animatable human avatar": "2106.14879"}}