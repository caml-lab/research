{"title": "SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation", "abstract": "Recent advances in image captioning have focused on scaling the data and\nmodel size, substantially increasing the cost of pre-training and finetuning.\nAs an alternative to large models, we present SmallCap, which generates a\ncaption conditioned on an input image and related captions retrieved from a\ndatastore. Our model is lightweight and fast to train, as the only learned\nparameters are in newly introduced cross-attention layers between a pre-trained\nCLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without\nadditional finetuning and can exploit large-scale data in a training-free\nfashion since the contents of the datastore can be readily replaced. Our\nexperiments show that SmallCap, trained only on COCO, has competitive\nperformance on this benchmark, and also transfers to other domains without\nretraining, solely through retrieval from target-domain data. Further\nimprovement is achieved through the training-free exploitation of diverse\nhuman-labeled and web data, which proves to be effective for a range of\ndomains, including the nocaps benchmark, designed to test generalization to\nunseen visual concepts.", "authors": ["Rita Ramos", "Bruno Martins", "Desmond Elliott", "Yova Kementchedjhieva"], "published_date": "2022_09_30", "pdf_url": "http://arxiv.org/pdf/2209.15323v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><th>|\\theta|</th><td>B@4</td><td>M</td><td>CIDEr</td><td>S</td></tr><tr><th colspan=\"6\">Large Models with V&amp;L pre-training</th></tr><tr><th>LEMON{}_{\\text{Huge}} [11]</th><th>675</th><td>41.5</td><td>30.8</td><td>139.1</td><td>24.1</td></tr><tr><th>SimVLM{}_{\\text{Huge}} [39]</th><th>632</th><td>40.6</td><td>33.7</td><td>143.3</td><td>25.4</td></tr><tr><th>OSCAR{}_{\\text{Large}} [19]</th><th>338</th><td>37.4</td><td>30.7</td><td>127.8</td><td>23.5</td></tr><tr><th>BLIP{}_{{\\text{CapFilt-L}}} [18]</th><th>224</th><td>39.7</td><td>-</td><td>133.3</td><td>-</td></tr><tr><th colspan=\"6\">Lightweight models</th></tr><tr><th>I-tuning{}_{\\text{Large}} [22]</th><th>95</th><td>34.8</td><td>29.3</td><td>119.4</td><td>22.4</td></tr><tr><th>CaMEL [4]</th><th>76</th><td>39.1</td><td>29.4</td><td>125.7</td><td>22.2</td></tr><tr><th>I-tuning{}_{\\text{Medium}} [22]</th><th>44</th><td>35.5</td><td>28.8</td><td>120.0</td><td>22.0</td></tr><tr><th>ClipCap [24]</th><th>43</th><td>33.5</td><td>27.5</td><td>113.1</td><td>21.1</td></tr><tr><th>I-tuning{}_{\\text{Base}} [22]</th><th>14</th><td>34.8</td><td>28.3</td><td>116.7</td><td>21.8</td></tr><tr><th>SmallCap</th><th>7</th><td>37.0</td><td>27.9</td><td>119.7</td><td>21.3</td></tr><tr><th>-SmallerCap</th><th>3.6</th><td>36.7</td><td>27.8</td><td>119.1</td><td>21.1</td></tr><tr><th>-SmallestCap</th><th>1.8</th><td>36.0</td><td>27.4</td><td>117.4</td><td>21.0</td></tr></tbody></table>", "caption": "Table 1: Results on the COCO test split with cross-entropy training.|\\theta|: number of trainable parameters in the model (in millions).", "list_citation_info": ["[24] Ron Mokady, Amir Hertz, and Amit H. Bermano. Clipcap: Clip prefix for image captioning, 2021.", "[11] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980\u201317989, 2022.", "[18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022.", "[22] Ziyang Luo, Yadong Xi, Rongsheng Zhang, and Jing Ma. I-tuning: Tuning language models with image for caption generation. arXiv preprint arXiv:2202.06574, 2022.", "[4] Manuele Barraco, Matteo Stefanini, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, and Rita Cucchiara. CaMEL: Mean Teacher Learning for Image Captioning. In International Conference on Pattern Recognition, 2022.", "[19] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.", "[39] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021."]}, {"table": "<table><tbody><tr><td></td><td>Flickr30K</td><td>VizWiz</td><td>MSR-VTT</td></tr><tr><td>ClipCap</td><td>41.2</td><td>28.3</td><td>12.5</td></tr><tr><td>CaMEL</td><td>55.2</td><td>37.6</td><td>20.7</td></tr><tr><td>SmallCap</td><td>60.6</td><td>55.0</td><td>28.4</td></tr><tr><td colspan=\"4\">Pre-training &amp; finetuning</td></tr><tr><td>SOTA</td><td>79.6 [23]</td><td>120.8 [37]</td><td>75.9 [37]</td></tr></tbody></table>", "caption": "Table 4: Comparison to other models on test data without training or finetuning. Flickr30K and VizWiz results with In-domain + Web, and MSR-VTT result with In-domain + Human-labeled. On the bottom, we include SOTA results from large-scale pre-trained models that were finetuned on these datasets.", "list_citation_info": ["[37] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.", "[23] Ziyang Luo, Yadong Xi, Rongsheng Zhang, and Jing Ma. Vc-gpt: Visual conditioned gpt for end-to-end generative vision-and-language pre-training. arXiv preprint arXiv:2201.12723, 2022."]}, {"table": "<table><thead><tr><th>Datastore</th><th>Size</th></tr></thead><tbody><tr><th>Web  [18]</th><td>12M</td></tr><tr><th>Human-Labeled</th><td>2.1M</td></tr><tr><th>  COCO [6]</th><td>566K</td></tr><tr><th>  Flickr [45]</th><td>145K</td></tr><tr><th>  VizWiz [10]</th><td>117K</td></tr><tr><th>  MSR-VTT [42]</th><td>130K</td></tr><tr><th>  VATEX [38]</th><td>349K</td></tr><tr><th>  TGIF [20]</th><td>125K</td></tr><tr><th>  Clotho [8]</th><td>14K</td></tr><tr><th>  LN Ade20k[27]</th><td>19K</td></tr><tr><th>  LN COCO[27]</th><td>121K</td></tr><tr><th>  LN Flick30k[27]</th><td>28K</td></tr><tr><th>  LN Open Images [27]</th><td>496K</td></tr></tbody></table>", "caption": "Table 7: Data used in the datastore for the conducted experiments in Section 3.2 along with size in terms of image-caption pairs. LN stands for Localized Narratives [27].", "list_citation_info": ["[38] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4581\u20134591, 2019.", "[10] Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya. Captioning images taken by people who are blind. In European Conference on Computer Vision, pages 417\u2013434. Springer, 2020.", "[27] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In European conference on computer vision, pages 647\u2013664. Springer, 2020.", "[20] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: A new dataset and benchmark on animated gif description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4641\u20134650, 2016.", "[8] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736\u2013740. IEEE, 2020.", "[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.", "[18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022.", "[45] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.", "[42] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016."]}], "citation_info_to_title": {"[45] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "[19] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "[24] Ron Mokady, Amir Hertz, and Amit H. Bermano. Clipcap: Clip prefix for image captioning, 2021.": "Clipcap: Clip Prefix for Image Captioning", "[37] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.": "Git: A generative image-to-text transformer for vision and language", "[27] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In European conference on computer vision, pages 647\u2013664. Springer, 2020.": "Connecting vision and language with localized narratives", "[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.": "Microsoft COCO Captions: Data Collection and Evaluation Server", "[42] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language", "[10] Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya. Captioning images taken by people who are blind. In European Conference on Computer Vision, pages 417\u2013434. Springer, 2020.": "Captioning images taken by people who are blind", "[18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022.": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "[20] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: A new dataset and benchmark on animated gif description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4641\u20134650, 2016.": "TGIF: A New Dataset and Benchmark on Animated GIF Description", "[11] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980\u201317989, 2022.": "Scaling up vision-language pre-training for image captioning", "[38] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4581\u20134591, 2019.": "Vatex: A large-scale, high-quality multilingual dataset for video-and-language research", "[39] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.": "Simvlm: Simple visual language model pretraining with weak supervision", "[23] Ziyang Luo, Yadong Xi, Rongsheng Zhang, and Jing Ma. Vc-gpt: Visual conditioned gpt for end-to-end generative vision-and-language pre-training. arXiv preprint arXiv:2201.12723, 2022.": "Vc-gpt: Visual conditioned gpt for end-to-end generative vision-and-language pre-training", "[22] Ziyang Luo, Yadong Xi, Rongsheng Zhang, and Jing Ma. I-tuning: Tuning language models with image for caption generation. arXiv preprint arXiv:2202.06574, 2022.": "I-tuning: Tuning language models with image for caption generation", "[8] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736\u2013740. IEEE, 2020.": "Clotho: An audio captioning dataset", "[4] Manuele Barraco, Matteo Stefanini, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, and Rita Cucchiara. CaMEL: Mean Teacher Learning for Image Captioning. In International Conference on Pattern Recognition, 2022.": "CaMEL: Mean Teacher Learning for Image Captioning"}, "source_title_to_arxiv_id": {"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation": "2201.12086"}}