{"title": "Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification", "abstract": "Most methods tackle zero-shot video classification by aligning\nvisual-semantic representations within seen classes, which limits\ngeneralization to unseen classes. To enhance model generalizability, this paper\npresents an end-to-end framework that preserves alignment and uniformity\nproperties for representations on both seen and unseen classes. Specifically,\nwe formulate a supervised contrastive loss to simultaneously align\nvisual-semantic features (i.e., alignment) and encourage the learned features\nto distribute uniformly (i.e., uniformity). Unlike existing methods that only\nconsider the alignment, we propose uniformity to preserve maximal-info of\nexisting features, which improves the probability that unobserved features fall\naround observed data. Further, we synthesize features of unseen classes by\nproposing a class generator that interpolates and extrapolates the features of\nseen classes. Besides, we introduce two metrics, closeness and dispersion, to\nquantify the two properties and serve as new measurements of model\ngeneralizability. Experiments show that our method significantly outperforms\nSoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is\navailable.", "authors": ["Shi Pu", "Kaili Zhao", "Mao Zheng"], "published_date": "2022_03_29", "pdf_url": "http://arxiv.org/pdf/2203.15381v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Methods</th><th>\u2005ET</th><th>\u2005AUL</th><th>\u2005ERF</th><th>\u2005UCG</th><th>\u2005SLR</th></tr></thead><tbody><tr><td>SoTA [3]</td><td>\\times</td><td>\\times</td><td>\\checkmark</td><td>\\times</td><td>\\checkmark</td></tr><tr><td>MUFI [35]</td><td>\\times</td><td>\\times</td><td>\\times</td><td>\\times</td><td>\\times</td></tr><tr><td>ER [6]</td><td>\\checkmark</td><td>\\times</td><td>\\times</td><td>\\times</td><td>\\times</td></tr><tr><td>AURL (ours)</td><td>\\checkmark</td><td>\\checkmark</td><td>\\checkmark</td><td>\\checkmark</td><td>\\checkmark</td></tr></tbody></table>", "caption": "Table 1: Comparisons between AURL and alternative methods.", "list_citation_info": ["[6] Shizhe Chen and Dong Huang. Elaborative rehearsal for zero-shot action recognition. In ICCV, 2021.", "[3] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In CVPR, 2020.", "[35] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xiao-Ping Zhang, Dong Wu, and Tao Mei. Boosting video representation learning with multi-faceted integration. In CVPR, 2021."]}, {"table": "<table><thead><tr><th> Method </th><th>Pre-training</th><th>Videoclips</th><th>Testsplits</th><th>UCFtop-1</th><th>UCFtop-5</th><th>HMDBtop-1</th><th>HMDBtop-5</th></tr></thead><tbody><tr><td></td><td></td><td>1</td><td>10</td><td>43.0</td><td>68.2</td><td>27.0</td><td>54.4</td></tr><tr><td rowspan=\"-2\">SoTA</td><td>\u2713</td><td>1</td><td>10</td><td>45.6</td><td>73.1</td><td>28.1</td><td>51.8</td></tr><tr><td>AURL</td><td></td><td>1</td><td>10</td><td>55.1</td><td>79.3</td><td>34.3</td><td>65.1</td></tr><tr><td></td><td></td><td>25</td><td>10</td><td>48.0</td><td>74.2</td><td>31.2</td><td>58.3</td></tr><tr><td rowspan=\"-2\">SoTA</td><td>\\checkmark</td><td>25</td><td>10</td><td>49.2</td><td>77.0</td><td>32.6</td><td>57.1</td></tr><tr><td>AURL</td><td></td><td>25</td><td>10</td><td>58.0</td><td>82.0</td><td>39.0</td><td>69.5</td></tr><tr><td></td><td></td><td>1</td><td>1</td><td>35.1</td><td>56.4</td><td>21.3</td><td>42.2</td></tr><tr><td rowspan=\"-2\">SoTA</td><td>\u2713</td><td>1</td><td>1</td><td>36.8</td><td>61.7</td><td>23.0</td><td>41.3</td></tr><tr><td>AURL</td><td></td><td>1</td><td>1</td><td>44.4</td><td>70.0</td><td>27.4</td><td>53.2</td></tr><tr><td></td><td></td><td>25</td><td>1</td><td>37.6</td><td>62.5</td><td>26.9</td><td>49.8</td></tr><tr><td rowspan=\"-2\">SoTA</td><td>\\checkmark</td><td>25</td><td>1</td><td>39.8</td><td>65.6</td><td>27.2</td><td>47.4</td></tr><tr><td>AURL</td><td></td><td>25</td><td>1</td><td>46.8</td><td>73.1</td><td>31.7</td><td>58.9</td></tr></tbody></table>", "caption": "Table 3: Comparisons with the closest SoTA [3] on both UCF and HMDB datasets.Red numbers indicate the best.", "list_citation_info": ["[3] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In CVPR, 2020."]}, {"table": "<table><thead><tr><th>    Method</th><th>Testsplits</th><th>Traindataset</th><th>UCFtop-1</th><th>Traindataset</th><th>HMDBtop-1</th></tr></thead><tbody><tr><th>    SoTA{}^{\\star} [3]</th><th>1</th><td>Kinetics</td><td>37.6</td><td>Kinetics</td><td>26.9</td></tr><tr><th>    AURL{}^{\\star}</th><th>1</th><td>Kinetics</td><td>46.8</td><td>Kinetics</td><td>31.7</td></tr><tr><th>    Obj2act [16]</th><th>3</th><td>-</td><td>30.3</td><td>-</td><td>15.6</td></tr><tr><th>    SAOE [27]</th><th>3</th><td>-</td><td>32.8</td><td>-</td><td>-</td></tr><tr><th>    OPCL [13]</th><th>3</th><td>-</td><td>36.3</td><td>-</td><td>-</td></tr><tr><th>    MUFI[35]</th><th>3</th><td>Kinetics+</td><td>56.3</td><td>Kinetics+</td><td>31.0</td></tr><tr><th>    AURL</th><th>3</th><td>Kinetics</td><td>60.9</td><td>Kinetics</td><td>40.4</td></tr><tr><th>    TARN [2]</th><th>30</th><td>UCF</td><td>23.2</td><td>HMDB</td><td>19.5</td></tr><tr><th>    Act2Vec [14]</th><th>-</th><td>UCF</td><td>22.1</td><td>HMDB</td><td>23.5</td></tr><tr><th>    SAOE[27]</th><th>10</th><td>-</td><td>40.4</td><td>-</td><td>-</td></tr><tr><th>    PSGNN [13]</th><th>50</th><td>UCF</td><td>43.0</td><td>HMDB</td><td>32.6</td></tr><tr><th>    OPCL [13]</th><th>10</th><td>-</td><td>47.3</td><td>-</td><td>-</td></tr><tr><th>    SoTA{}^{\\star}[3]</th><th>10</th><td>Kinetics</td><td>48.0</td><td>Kinetics</td><td>32.7</td></tr><tr><th>    DASZL [19]</th><th>10</th><td>-</td><td>48.9</td><td>-</td><td>-</td></tr><tr><th>    ER [6]</th><th>50</th><td>UCF</td><td>51.8</td><td>HMDB</td><td>35.3</td></tr><tr><th>    AURL{}^{\\star}</th><th>10</th><td>Kinetics</td><td>58.0</td><td>Kinetics</td><td>39.0</td></tr></tbody></table>", "caption": "Table 4: Comparisons with SoTA alternatives on both UCF and HMDB datasets.Results of alternatives were obtained from original papers, and the higher, the better.Red and blue numbers indicate the best and second best.\\star means using \\tau=0.05 in Eq.10.", "list_citation_info": ["[14] Meera Hahn, Andrew Silva, and James M Rehg. Action2vec: A crossmodal embedding approach to action learning. arXiv:1901.00484, 2019.", "[6] Shizhe Chen and Dong Huang. Elaborative rehearsal for zero-shot action recognition. In ICCV, 2021.", "[35] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xiao-Ping Zhang, Dong Wu, and Tao Mei. Boosting video representation learning with multi-faceted integration. In CVPR, 2021.", "[13] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Learning to model relationships for zero-shot video classification. TPAMI, pages 3476\u20133491, 2020.", "[16] Mihir Jain, Jan C Van Gemert, Thomas Mensink, and Cees GM Snoek. Objects2action: Classifying and localizing actions without any video example. In ICCV, 2015.", "[27] Pascal Mettes and Cees GM Snoek. Spatial-aware object embeddings for zero-shot localization and classification of actions. In ICCV, 2017.", "[2] Mina Bishay, Georgios Zoumpourlis, and Ioannis Patras. Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition. arXiv:1907.09021, 2019.", "[3] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In CVPR, 2020.", "[19] Tae Soo Kim, Jonathan Jones, Michael Peven, Zihao Xiao, Jin Bai, Yi Zhang, Weichao Qiu, Alan Yuille, and Gregory D Hager. Daszl: Dynamic action signatures for zero-shot learning. In AAAI, 2021."]}], "citation_info_to_title": {"[35] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xiao-Ping Zhang, Dong Wu, and Tao Mei. Boosting video representation learning with multi-faceted integration. In CVPR, 2021.": "Boosting video representation learning with multi-faceted integration", "[19] Tae Soo Kim, Jonathan Jones, Michael Peven, Zihao Xiao, Jin Bai, Yi Zhang, Weichao Qiu, Alan Yuille, and Gregory D Hager. Daszl: Dynamic action signatures for zero-shot learning. In AAAI, 2021.": "Daszl: Dynamic Action Signatures for Zero-Shot Learning", "[3] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In CVPR, 2020.": "Rethinking zero-shot video classification: End-to-end training for realistic applications", "[13] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Learning to model relationships for zero-shot video classification. TPAMI, pages 3476\u20133491, 2020.": "Learning to Model Relationships for Zero-Shot Video Classification", "[6] Shizhe Chen and Dong Huang. Elaborative rehearsal for zero-shot action recognition. In ICCV, 2021.": "Elaborative rehearsal for zero-shot action recognition", "[2] Mina Bishay, Georgios Zoumpourlis, and Ioannis Patras. Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition. arXiv:1907.09021, 2019.": "Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition", "[14] Meera Hahn, Andrew Silva, and James M Rehg. Action2vec: A crossmodal embedding approach to action learning. arXiv:1901.00484, 2019.": "Action2vec: A crossmodal embedding approach to action learning", "[16] Mihir Jain, Jan C Van Gemert, Thomas Mensink, and Cees GM Snoek. Objects2action: Classifying and localizing actions without any video example. In ICCV, 2015.": "Objects2action: Classifying and Localizing Actions without Any Video Example", "[27] Pascal Mettes and Cees GM Snoek. Spatial-aware object embeddings for zero-shot localization and classification of actions. In ICCV, 2017.": "Spatial-aware Object Embeddings for Zero-shot Localization and Classification of Actions"}, "source_title_to_arxiv_id": {"Elaborative rehearsal for zero-shot action recognition": "2108.02833"}}