{"title": "Weakly-Supervised Camouflaged Object Detection with Scribble Annotations", "abstract": "Existing camouflaged object detection (COD) methods rely heavily on\nlarge-scale datasets with pixel-wise annotations. However, due to the ambiguous\nboundary, annotating camouflage objects pixel-wisely is very time-consuming and\nlabor-intensive, taking ~60mins to label one image. In this paper, we propose\nthe first weakly-supervised COD method, using scribble annotations as\nsupervision. To achieve this, we first relabel 4,040 images in existing\ncamouflaged object datasets with scribbles, which takes ~10s to label one\nimage. As scribble annotations only describe the primary structure of objects\nwithout details, for the network to learn to localize the boundaries of\ncamouflaged objects, we propose a novel consistency loss composed of two parts:\na cross-view loss to attain reliable consistency over different images, and an\ninside-view loss to maintain consistency inside a single prediction map.\nBesides, we observe that humans use semantic information to segment regions\nnear the boundaries of camouflaged objects. Hence, we further propose a\nfeature-guided loss, which includes visual features directly extracted from\nimages and semantically significant features captured by the model. Finally, we\npropose a novel network for COD via scribble learning on structural information\nand semantic relations. Our network has two novel modules: the local-context\ncontrasted (LCC) module, which mimics visual inhibition to enhance image\ncontrast/sharpness and expand the scribbles into potential camouflaged regions,\nand the logical semantic relation (LSR) module, which analyzes the semantic\nrelation to determine the regions representing the camouflaged object.\nExperimental results show that our model outperforms relevant SOTA methods on\nthree COD benchmarks with an average improvement of 11.0% on MAE, 3.2% on\nS-measure, 2.5% on E-measure, and 4.4% on weighted F-measure.", "authors": ["Ruozhen He", "Qihua Dong", "Jiaying Lin", "Rynson W. H. Lau"], "published_date": "2022_07_28", "pdf_url": "http://arxiv.org/pdf/2207.14083v2", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th></th><th colspan=\"4\">CAMO[29]</th><th colspan=\"4\">CHAMELEON[30]</th><th colspan=\"4\">COD10K[4]</th></tr><tr><th>Methods</th><th>Sup.</th><th>MAE\\downarrow</th><th>S{}_{m}\\uparrow</th><th>E{}_{m}\\uparrow</th><th>F{}_{\\beta}^{w}\\uparrow</th><th>MAE\\downarrow</th><th>S{}_{m}\\uparrow</th><th>E{}_{m}\\uparrow</th><th>F{}_{\\beta}^{w}\\uparrow</th><th>MAE\\downarrow</th><th>S{}_{m}\\uparrow</th><th>E{}_{m}\\uparrow</th><th>F{}_{\\beta}^{w}\\uparrow</th></tr></thead><tbody><tr><th>NLDF[34]</th><th>F</th><td>0.123</td><td>0.665</td><td>0.664</td><td>0.495</td><td>0.063</td><td>0.798</td><td>0.809</td><td>0.652</td><td>0.059</td><td>0.701</td><td>0.709</td><td>0.473</td></tr><tr><th>PiCANet[35]</th><th>F</th><td>0.125</td><td>0.701</td><td>0.716</td><td>0.510</td><td>0.085</td><td>0.765</td><td>0.778</td><td>0.552</td><td>0.081</td><td>0.696</td><td>0.712</td><td>0.415</td></tr><tr><th>CPD[36]</th><th>F</th><td>0.113</td><td>0.716</td><td>0.723</td><td>0.556</td><td>0.048</td><td>0.857</td><td>0.874</td><td>0.731</td><td>0.053</td><td>0.750</td><td>0.776</td><td>0.531</td></tr><tr><th>EGNet[37]</th><th>F</th><td>0.109</td><td>0.732</td><td>0.800</td><td>0.604</td><td>0.065</td><td>0.797</td><td>0.860</td><td>0.649</td><td>0.061</td><td>0.736</td><td>0.810</td><td>0.517</td></tr><tr><th>PoolNet[38]</th><th>F</th><td>0.105</td><td>0.730</td><td>0.746</td><td>0.575</td><td>0.054</td><td>0.845</td><td>0.863</td><td>0.690</td><td>0.056</td><td>0.740</td><td>0.776</td><td>0.506</td></tr><tr><th>SCRN[41]</th><th>F</th><td>0.090</td><td>0.779</td><td>0.797</td><td>0.643</td><td>0.042</td><td>0.876</td><td>0.889</td><td>0.741</td><td>0.047</td><td>0.789</td><td>0.817</td><td>0.575</td></tr><tr><th>F3Net[39]</th><th>F</th><td>0.109</td><td>0.711</td><td>0.741</td><td>0.564</td><td>0.047</td><td>0.848</td><td>0.894</td><td>0.744</td><td>0.051</td><td>0.739</td><td>0.795</td><td>0.544</td></tr><tr><th>CSNet[42]</th><th>F</th><td>0.092</td><td>0.771</td><td>0.795</td><td>0.641</td><td>0.047</td><td>0.856</td><td>0.868</td><td>0.718</td><td>0.047</td><td>0.778</td><td>0.809</td><td>0.569</td></tr><tr><th>ITSD[40]</th><th>F</th><td>0.102</td><td>0.750</td><td>0.779</td><td>0.610</td><td>0.057</td><td>0.814</td><td>0.844</td><td>0.662</td><td>0.051</td><td>0.767</td><td>0.808</td><td>0.557</td></tr><tr><th>MINet[43]</th><th>F</th><td>0.090</td><td>0.748</td><td>0.791</td><td>0.637</td><td>0.036</td><td>0.855</td><td>0.914</td><td>0.771</td><td>0.042</td><td>0.770</td><td>0.832</td><td>0.608</td></tr><tr><th>PraNet[2]</th><th>F</th><td>0.094</td><td>0.769</td><td>0.825</td><td>0.663</td><td>0.044</td><td>0.860</td><td>0.907</td><td>0.763</td><td>0.045</td><td>0.789</td><td>0.861</td><td>0.629</td></tr><tr><th>UCNet[44]</th><th>F</th><td>0.094</td><td>0.739</td><td>0.787</td><td>0.640</td><td>0.036</td><td>0.880</td><td>0.930</td><td>0.817</td><td>0.042</td><td>0.776</td><td>0.857</td><td>0.633</td></tr><tr><th>SINet[4]</th><th>F</th><td>0.092</td><td>0.745</td><td>0.804</td><td>0.644</td><td>0.034</td><td>0.872</td><td>0.936</td><td>0.806</td><td>0.043</td><td>0.776</td><td>0.864</td><td>0.631</td></tr><tr><th>SLSR[45]</th><th>F</th><td>0.080</td><td>0.787</td><td>0.838</td><td>0.696</td><td>0.030</td><td>0.890</td><td>0.935</td><td>0.822</td><td>0.037</td><td>0.804</td><td>0.880</td><td>0.673</td></tr><tr><th>MGL-R[20]</th><th>F</th><td>0.088</td><td>0.775</td><td>0.812</td><td>0.673</td><td>0.031</td><td>0.893</td><td>0.917</td><td>0.812</td><td>0.035</td><td>0.814</td><td>0.851</td><td>0.666</td></tr><tr><th>PFNet[22]</th><th>F</th><td>0.085</td><td>0.782</td><td>0.841</td><td>0.695</td><td>0.033</td><td>0.882</td><td>0.931</td><td>0.810</td><td>0.040</td><td>0.800</td><td>0.877</td><td>0.660</td></tr><tr><th>UJSC[21]</th><th>F</th><td>0.073</td><td>0.800</td><td>0.859</td><td>0.728</td><td>0.030</td><td>0.891</td><td>0.945</td><td>0.833</td><td>0.035</td><td>0.809</td><td>0.884</td><td>0.684</td></tr><tr><th>C2FNet[46]</th><th>F</th><td>0.080</td><td>0.796</td><td>0.854</td><td>0.719</td><td>0.032</td><td>0.888</td><td>0.935</td><td>0.828</td><td>0.036</td><td>0.813</td><td>0.890</td><td>0.686</td></tr><tr><th>UGTR[5]</th><th>F</th><td>0.086</td><td>0.784</td><td>0.822</td><td>0.684</td><td>0.031</td><td>0.888</td><td>0.910</td><td>0.794</td><td>0.036</td><td>0.817</td><td>0.852</td><td>0.666</td></tr><tr><th>USPS[47]</th><th>U</th><td>0.207</td><td>0.568</td><td>0.641</td><td>0.399</td><td>0.188</td><td>0.573</td><td>0.631</td><td>0.380</td><td>0.196</td><td>0.519</td><td>0.536</td><td>0.265</td></tr><tr><th>DUSD[48]</th><th>U</th><td>0.166</td><td>0.551</td><td>0.594</td><td>0.308</td><td>0.129</td><td>0.578</td><td>0.634</td><td>0.316</td><td>0.107</td><td>0.580</td><td>0.646</td><td>0.276</td></tr><tr><th>SS[6]</th><th>W</th><td>0.118</td><td>0.696</td><td>0.786</td><td>0.562</td><td>0.067</td><td>0.782</td><td>0.860</td><td>0.654</td><td>0.071</td><td>0.684</td><td>0.770</td><td>0.461</td></tr><tr><th>SCWSSOD[7]</th><th>W</th><td>0.102</td><td>0.713</td><td>0.795</td><td>0.618</td><td>0.053</td><td>0.792</td><td>0.881</td><td>0.714</td><td>0.055</td><td>0.710</td><td>0.805</td><td>0.546</td></tr><tr><th>Ours</th><th>W</th><td>0.092</td><td>0.735</td><td>0.815</td><td>0.641</td><td>0.046</td><td>0.818</td><td>0.897</td><td>0.744</td><td>0.049</td><td>0.733</td><td>0.832</td><td>0.576</td></tr></tbody></table>", "caption": "Table 1: Quantitative comparisons with state-of-the-arts on three benchmarks. \u201cF\u201d, \u201cU\u201d, and \u201cW\u201d denote fully-supervised, unsupervised, and weakly-supervised, respectively. ", "list_citation_info": ["[44] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8582\u20138591, 2020.", "[35] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3089\u20133098, 2018.", "[4] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2777\u20132787, 2020.", "[2] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse attention network for polyp segmentation. In International conference on medical image computing and computer-assisted intervention, pages 263\u2013273. Springer, 2020.", "[43] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9413\u20139422, 2020.", "[29] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-Triet Tran, and Akihiro Sugimoto. Anabranch network for camouflaged object segmentation. Computer Vision and Image Understanding, 184:45\u201356, 2019.", "[40] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for accurate and fast saliency detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9141\u20139150, 2020.", "[38] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3917\u20133926, 2019.", "[39] Jun Wei, Shuhui Wang, and Qingming Huang. F33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTnet: fusion, feedback and focus for salient object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12321\u201312328, 2020.", "[7] Siyue Yu, Bingfeng Zhang, Jimin Xiao, and Eng Gee Lim. Structure-consistent weakly supervised salient object detection with local saliency coherence. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). AAAI Palo Alto, CA, USA, 2021.", "[42] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In European Conference on Computer Vision, pages 702\u2013721. Springer, 2020.", "[5] Fan Yang, Qiang Zhai, Xin Li, Rui Huang, Ao Luo, Hong Cheng, and Deng-Ping Fan. Uncertainty-guided transformer reasoning for camouflaged object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4146\u20134155, 2021.", "[22] Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, and Deng-Ping Fan. Camouflaged object segmentation with distraction mining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8772\u20138781, 2021.", "[37] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for salient object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8779\u20138788, 2019.", "[48] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, and Richard Hartley. Deep unsupervised saliency detection: A multiple noisy labeling perspective. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9029\u20139038, 2018.", "[6] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-supervised salient object detection via scribble annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12546\u201312555, 2020.", "[20] Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, and Deng-Ping Fan. Mutual graph learning for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12997\u201313007, 2021.", "[34] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep features for salient object detection. In Proceedings of the IEEE Conference on computer vision and pattern recognition, pages 6609\u20136617, 2017.", "[21] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang, and Yuchao Dai. Uncertainty-aware joint salient object and camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10071\u201310081, 2021.", "[46] Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, and Nian Liu. Context-aware cross-level fusion network for camouflaged object detection. arXiv preprint arXiv:2105.12555, 2021.", "[47] Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou, and Thomas Brox. Deepusps: Deep robust unsupervised saliency prediction via self-supervision. Advances in Neural Information Processing Systems, 32, 2019.", "[41] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7264\u20137273, 2019.", "[45] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, and Deng-Ping Fan. Simultaneously localize, segment and rank the camouflaged objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11591\u201311601, 2021.", "[36] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3907\u20133916, 2019.", "[30] Przemys\u0142aw Skurowski, Hassan Abdulameer, J B\u0142aszczyk, Tomasz Depta, Adam Kornacki, and P Kozie\u0142. Animal camouflage analysis: Chameleon database. Unpublished manuscript, 2(6):7, 2018."]}, {"table": "<table><tbody><tr><th></th><th></th><td colspan=\"4\">CAMO[29]</td><td colspan=\"4\">CHAMELEON[30]</td><td colspan=\"4\">COD10K[4]</td></tr><tr><th>Basic setting</th><th>Loss</th><td>MAE\\downarrow</td><td>S{}_{m}\\uparrow</td><td>E{}_{m}\\uparrow</td><td>F{}_{\\beta}^{w}\\uparrow</td><td>MAE\\downarrow</td><td>S{}_{m}\\uparrow</td><td>E{}_{m}\\uparrow</td><td>F{}_{\\beta}^{w}\\uparrow</td><td>MAE\\downarrow</td><td>S{}_{m}\\uparrow</td><td>E{}_{m}\\uparrow</td><td>F{}_{\\beta}^{w}\\uparrow</td></tr><tr><th>w/ pce</th><th>Baseline</th><td>0.215</td><td>0.612</td><td>0.633</td><td>0.387</td><td>0.171</td><td>0.652</td><td>0.676</td><td>0.413</td><td>0.224</td><td>0.525</td><td>0.536</td><td>0.227</td></tr><tr><th>w/ ft, iv</th><th>w/o cv</th><td>0.105</td><td>0.721</td><td>0.786</td><td>0.600</td><td>0.060</td><td>0.789</td><td>0.882</td><td>0.682</td><td>0.058</td><td>0.709</td><td>0.805</td><td>0.518</td></tr><tr><th></th><th>w/ cv(R)</th><td>0.097</td><td>0.727</td><td>0.807</td><td>0.629</td><td>0.049</td><td>0.808</td><td>0.905</td><td>0.727</td><td>0.051</td><td>0.723</td><td>0.825</td><td>0.562</td></tr><tr><th></th><th>w/ cv(R, F)</th><td>0.094</td><td>0.730</td><td>0.812</td><td>0.638</td><td>0.047</td><td>0.808</td><td>0.887</td><td>0.730</td><td>0.050</td><td>0.727</td><td>0.818</td><td>0.571</td></tr><tr><th></th><th>w/ cv(R, F, T)</th><td>0.094</td><td>0.730</td><td>0.808</td><td>0.637</td><td>0.047</td><td>0.806</td><td>0.885</td><td>0.736</td><td>0.048</td><td>0.727</td><td>0.816</td><td>0.572</td></tr><tr><th></th><th>w/ cv(R, F, T, C)</th><td>0.092</td><td>0.735</td><td>0.815</td><td>0.641</td><td>0.046</td><td>0.818</td><td>0.897</td><td>0.744</td><td>0.049</td><td>0.733</td><td>0.832</td><td>0.576</td></tr><tr><th>w/ ft</th><th>w/ cv\u2019</th><td>0.095</td><td>0.723</td><td>0.801</td><td>0.624</td><td>0.045</td><td>0.808</td><td>0.896</td><td>0.734</td><td>0.049</td><td>0.72</td><td>0.815</td><td>0.561</td></tr><tr><th></th><th>w/ cv, iv</th><td>0.095</td><td>0.726</td><td>0.804</td><td>0.632</td><td>0.043</td><td>0.814</td><td>0.907</td><td>0.740</td><td>0.048</td><td>0.729</td><td>0.821</td><td>0.574</td></tr><tr><th></th><th>w/ cv, iv</th><td>0.092</td><td>0.735</td><td>0.815</td><td>0.641</td><td>0.046</td><td>0.818</td><td>0.897</td><td>0.744</td><td>0.049</td><td>0.733</td><td>0.832</td><td>0.576</td></tr><tr><th>w/ cs</th><th>w/ ca</th><td>0.095</td><td>0.727</td><td>0.807</td><td>0.631</td><td>0.045</td><td>0.815</td><td>0.900</td><td>0.737</td><td>0.051</td><td>0.729</td><td>0.820</td><td>0.573</td></tr><tr><th></th><th>w/ ca, ss</th><td>0.092</td><td>0.735</td><td>0.815</td><td>0.641</td><td>0.046</td><td>0.818</td><td>0.897</td><td>0.744</td><td>0.049</td><td>0.733</td><td>0.832</td><td>0.576</td></tr><tr><th>w/ pce</th><th>w/ cs</th><td>0.096</td><td>0.731</td><td>0.821</td><td>0.641</td><td>0.050</td><td>0.805</td><td>0.899</td><td>0.723</td><td>0.051</td><td>0.726</td><td>0.835</td><td>0.567</td></tr><tr><th></th><th>w/ ft</th><td>0.107</td><td>0.720</td><td>0.785</td><td>0.592</td><td>0.063</td><td>0.794</td><td>0.872</td><td>0.674</td><td>0.058</td><td>0.713</td><td>0.803</td><td>0.519</td></tr><tr><th></th><th>w/ cs, ft</th><td>0.092</td><td>0.735</td><td>0.815</td><td>0.641</td><td>0.046</td><td>0.818</td><td>0.897</td><td>0.744</td><td>0.049</td><td>0.733</td><td>0.832</td><td>0.576</td></tr><tr><th></th><th></th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 2: The ablation study results of our loss functions. Groups correspond to ablations on transformations in cross-view consistency, on consistency loss, on feature loss, on all losses. Here, pce stands for partial cross entropy; ft, cs stands for feature-guided loss and consistency loss (cs=cv+iv, ft=ca+ss); cv, iv stands for cross-view and inside-view consistency loss and cv\u2019 means cross-view consistency without reliability bias; R,F,T,C are resizing, flipping.", "list_citation_info": ["[4] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2777\u20132787, 2020.", "[30] Przemys\u0142aw Skurowski, Hassan Abdulameer, J B\u0142aszczyk, Tomasz Depta, Adam Kornacki, and P Kozie\u0142. Animal camouflage analysis: Chameleon database. Unpublished manuscript, 2(6):7, 2018.", "[29] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-Triet Tran, and Akihiro Sugimoto. Anabranch network for camouflaged object segmentation. Computer Vision and Image Understanding, 184:45\u201356, 2019."]}, {"table": "<table><thead><tr><th>Methods</th><th>BB</th><th>AGE</th><th>LCC</th><th>LSR</th><th>MAE\\downarrow</th><th>S{}_{m}\\uparrow</th><th>E{}_{m}\\uparrow</th><th>F{}_{\\beta}^{w}\\uparrow</th></tr></thead><tbody><tr><td>Ablation I</td><td>\\surd</td><td></td><td></td><td></td><td>0.104</td><td>0.701</td><td>0.774</td><td>0.598</td></tr><tr><td>Ablation II</td><td>\\surd</td><td>\\surd</td><td></td><td></td><td>0.100</td><td>0.716</td><td>0.799</td><td>0.615</td></tr><tr><td>Ablation III</td><td>\\surd</td><td>\\surd</td><td>\\surd</td><td></td><td>0.099</td><td>0.721</td><td>0.806</td><td>0.626</td></tr><tr><td>Ablation IV</td><td>\\surd</td><td>\\surd</td><td></td><td>\\surd</td><td>0.098</td><td>0.713</td><td>0.783</td><td>0.612</td></tr><tr><td>Ours</td><td>\\surd</td><td>\\surd</td><td>\\surd</td><td>\\surd</td><td>0.092</td><td>0.735</td><td>0.815</td><td>0.641</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 3: The ablation study results of components on CAMO[29].", "list_citation_info": ["[29] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-Triet Tran, and Akihiro Sugimoto. Anabranch network for camouflaged object segmentation. Computer Vision and Image Understanding, 184:45\u201356, 2019."]}], "citation_info_to_title": {"[35] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3089\u20133098, 2018.": "Picanet: Learning pixel-wise contextual attention for saliency detection", "[47] Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou, and Thomas Brox. Deepusps: Deep robust unsupervised saliency prediction via self-supervision. Advances in Neural Information Processing Systems, 32, 2019.": "Deepusps: Deep Robust Unsupervised Saliency Prediction via Self-Supervision", "[2] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse attention network for polyp segmentation. In International conference on medical image computing and computer-assisted intervention, pages 263\u2013273. Springer, 2020.": "Pranet: Parallel reverse attention network for polyp segmentation", "[30] Przemys\u0142aw Skurowski, Hassan Abdulameer, J B\u0142aszczyk, Tomasz Depta, Adam Kornacki, and P Kozie\u0142. Animal camouflage analysis: Chameleon database. Unpublished manuscript, 2(6):7, 2018.": "Animal camouflage analysis: Chameleon database", "[7] Siyue Yu, Bingfeng Zhang, Jimin Xiao, and Eng Gee Lim. Structure-consistent weakly supervised salient object detection with local saliency coherence. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). AAAI Palo Alto, CA, USA, 2021.": "Structure-Consistent Weakly Supervised Salient Object Detection with Local Saliency Coherence", "[34] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep features for salient object detection. In Proceedings of the IEEE Conference on computer vision and pattern recognition, pages 6609\u20136617, 2017.": "Non-local deep features for salient object detection", "[45] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, and Deng-Ping Fan. Simultaneously localize, segment and rank the camouflaged objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11591\u201311601, 2021.": "Simultaneously localize, segment and rank the camouflaged objects", "[20] Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, and Deng-Ping Fan. Mutual graph learning for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12997\u201313007, 2021.": "Mutual Graph Learning for Camouflaged Object Detection", "[41] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 7264\u20137273, 2019.": "Stacked cross refinement network for edge-aware salient object detection", "[5] Fan Yang, Qiang Zhai, Xin Li, Rui Huang, Ao Luo, Hong Cheng, and Deng-Ping Fan. Uncertainty-guided transformer reasoning for camouflaged object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4146\u20134155, 2021.": "Uncertainty-guided transformer reasoning for camouflaged object detection", "[38] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3917\u20133926, 2019.": "A simple pooling-based design for real-time salient object detection", "[4] Deng-Ping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2777\u20132787, 2020.": "Camouflaged object detection", "[37] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for salient object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8779\u20138788, 2019.": "Egnet: Edge guidance network for salient object detection", "[40] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for accurate and fast saliency detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9141\u20139150, 2020.": "Interactive two-stream decoder for accurate and fast saliency detection", "[36] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3907\u20133916, 2019.": "Cascaded partial decoder for fast and accurate salient object detection", "[39] Jun Wei, Shuhui Wang, and Qingming Huang. F33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTnet: fusion, feedback and focus for salient object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12321\u201312328, 2020.": "F33{}^{3}start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPTnet: Fusion, Feedback and Focus for Salient Object Detection", "[21] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang, and Yuchao Dai. Uncertainty-aware joint salient object and camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10071\u201310081, 2021.": "Uncertainty-aware joint salient object and camouflaged object detection", "[48] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi, and Richard Hartley. Deep unsupervised saliency detection: A multiple noisy labeling perspective. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9029\u20139038, 2018.": "Deep unsupervised saliency detection: A multiple noisy labeling perspective", "[6] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-supervised salient object detection via scribble annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12546\u201312555, 2020.": "Weakly-supervised salient object detection via scribble annotations", "[46] Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, and Nian Liu. Context-aware cross-level fusion network for camouflaged object detection. arXiv preprint arXiv:2105.12555, 2021.": "Context-aware cross-level fusion network for camouflaged object detection", "[43] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9413\u20139422, 2020.": "Multi-scale interactive network for salient object detection", "[44] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8582\u20138591, 2020.": "UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders", "[42] Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In European Conference on Computer Vision, pages 702\u2013721. Springer, 2020.": "Highly efficient salient object detection with 100k parameters", "[22] Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, and Deng-Ping Fan. Camouflaged object segmentation with distraction mining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8772\u20138781, 2021.": "Camouflaged Object Segmentation with Distraction Mining", "[29] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-Triet Tran, and Akihiro Sugimoto. Anabranch network for camouflaged object segmentation. Computer Vision and Image Understanding, 184:45\u201356, 2019.": "Anabranch network for camouflaged object segmentation"}, "source_title_to_arxiv_id": {"Simultaneously localize, segment and rank the camouflaged objects": "2103.04011", "Uncertainty-aware joint salient object and camouflaged object detection": "2104.02628", "Anabranch network for camouflaged object segmentation": "2105.09451"}}