{"title": "Students taught by multimodal teachers are superior action recognizers", "abstract": "The focal point of egocentric video understanding is modelling hand-object\ninteractions. Standard models -- CNNs, Vision Transformers, etc. -- which\nreceive RGB frames as input perform well, however, their performance improves\nfurther by employing additional modalities such as object detections, optical\nflow, audio, etc. as input. The added complexity of the required\nmodality-specific modules, on the other hand, makes these models impractical\nfor deployment. The goal of this work is to retain the performance of such\nmultimodal approaches, while using only the RGB images as input at inference\ntime. Our approach is based on multimodal knowledge distillation, featuring a\nmultimodal teacher (in the current experiments trained only using object\ndetections, optical flow and RGB frames) and a unimodal student (using only RGB\nframes as input). We present preliminary results which demonstrate that the\nresulting model -- distilled from a multimodal teacher -- significantly\noutperforms the baseline RGB model (trained without knowledge distillation), as\nwell as an omnivorous version of itself (trained on all modalities jointly), in\nboth standard and compositional action recognition.", "authors": ["Gorjan Radevski", "Dusan Grujicic", "Matthew Blaschko", "Marie-Francine Moens", "Tinne Tuytelaars"], "published_date": "2022_10_09", "pdf_url": "http://arxiv.org/pdf/2210.04331v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td></td><td colspan=\"2\">Something-Something</td><td colspan=\"2\">Something-Else</td></tr><tr><td>Method</td><td>Modalities at inference</td><td>Top 1</td><td>Top 5</td><td>Top 1</td><td>Top 5</td></tr><tr><td>Baseline [1]</td><td>RGB frames</td><td>59.6</td><td>85.6</td><td>51.7</td><td>78.1</td></tr><tr><td>Baseline [15]</td><td>Obj. detections</td><td>44.2</td><td>73.6</td><td>39.5</td><td>66.3</td></tr><tr><td>Baseline [1]</td><td>Optical flow</td><td>50.2</td><td>79.6</td><td>49.7</td><td>78.3</td></tr><tr><td>Teacher [1, 15]</td><td>RGB Frames &amp; Obj. detections</td><td>62.6</td><td>87.6</td><td>57.2</td><td>82.7</td></tr><tr><td>Student</td><td>RGB frames</td><td>62.1<sub>+2.5</sub></td><td>88.0<sub>+2.4</sub></td><td>57.2<sub>+5.5</sub></td><td>83.5<sub>+5.4</sub></td></tr><tr><td>Teacher [1]</td><td>RGB Frames &amp; Optical flow</td><td>64.0</td><td>89.0</td><td>61.5</td><td>86.2</td></tr><tr><td>Student</td><td>RGB frames</td><td>62.3<sub>+2.7</sub></td><td>88.8<sub>+3.2</sub></td><td>57.6<sub>+5.9</sub></td><td>84.1<sub>+6.0</sub></td></tr><tr><td>Teacher [1, 15]</td><td>RGB Frames &amp; Optical flow &amp; Obj. detections</td><td>65.7</td><td>90.2</td><td>63.2</td><td>87.3</td></tr><tr><td>Omnivore [1, 6]</td><td>RGB frames</td><td>62.5</td><td>88.1</td><td>56.8</td><td>83.3</td></tr><tr><td>Student</td><td>RGB frames</td><td>63.9<sub>+4.3</sub></td><td>89.2<sub>+3.6</sub></td><td>59.4<sub>+7.7</sub></td><td>85.4<sub>+7.3</sub></td></tr></tbody></table>", "caption": "Table 1: Something-Something [7] &amp; Something-Else [13] (compositional split) action recognition accuracy. Improvement over RGB frames baseline [1] in red.", "list_citation_info": ["[7] Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al.: The\" something something\" video database for learning and evaluating visual common sense. In: Proceedings of the IEEE international conference on computer vision. pp. 5842\u20135850 (2017)", "[13] Materzynska, J., Xiao, T., Herzig, R., Xu, H., Wang, X., Darrell, T.: Something-else: Compositional action recognition with spatial-temporal interaction networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1049\u20131059 (2020)", "[1] Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for video understanding? In: ICML. vol. 2, p. 4 (2021)", "[15] Radevski, G., Moens, M.F., Tuytelaars, T.: Revisiting spatio-temporal layouts for compositional action recognition. arXiv preprint arXiv:2111.01936 (2021)"]}], "citation_info_to_title": {"[1] Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for video understanding? In: ICML. vol. 2, p. 4 (2021)": "Is space-time attention all you need for video understanding?", "[15] Radevski, G., Moens, M.F., Tuytelaars, T.: Revisiting spatio-temporal layouts for compositional action recognition. arXiv preprint arXiv:2111.01936 (2021)": "Revisiting spatio-temporal layouts for compositional action recognition", "[13] Materzynska, J., Xiao, T., Herzig, R., Xu, H., Wang, X., Darrell, T.: Something-else: Compositional action recognition with spatial-temporal interaction networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1049\u20131059 (2020)": "Something-else: Compositional action recognition with spatial-temporal interaction networks", "[7] Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al.: The\" something something\" video database for learning and evaluating visual common sense. In: Proceedings of the IEEE international conference on computer vision. pp. 5842\u20135850 (2017)": "The something something video database for learning and evaluating visual common sense"}, "source_title_to_arxiv_id": {"Revisiting spatio-temporal layouts for compositional action recognition": "2111.01936"}}