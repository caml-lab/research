{"title": "Understanding and Improving Visual Prompting: A Label-Mapping Perspective", "abstract": "We revisit and advance visual prompting (VP), an input prompting technique\nfor vision tasks. VP can reprogram a fixed, pre-trained source model to\naccomplish downstream tasks in the target domain by simply incorporating\nuniversal prompts (in terms of input perturbation patterns) into downstream\ndata points. Yet, it remains elusive why VP stays effective even given a\nruleless label mapping (LM) between the source classes and the target classes.\nInspired by the above, we ask: How is LM interrelated with VP? And how to\nexploit such a relationship to improve its accuracy on target tasks? We peer\ninto the influence of LM on VP and provide an affirmative answer that a better\n'quality' of LM (assessed by mapping precision and explanation) can\nconsistently improve the effectiveness of VP. This is in contrast to the prior\nart where the factor of LM was missing. To optimize LM, we propose a new VP\nframework, termed ILM-VP (iterative label mapping-based visual prompting),\nwhich automatically re-maps the source labels to the target labels and\nprogressively improves the target task accuracy of VP. Further, when using a\ncontrastive language-image pretrained (CLIP) model, we propose to integrate an\nLM process to assist the text prompt selection of CLIP and to improve the\ntarget task accuracy. Extensive experiments demonstrate that our proposal\nsignificantly outperforms state-of-the-art VP methods. As highlighted below, we\nshow that when reprogramming an ImageNet-pretrained ResNet-18 to 13 target\ntasks, our method outperforms baselines by a substantial margin, e.g., 7.9% and\n6.7% accuracy improvements in transfer learning to the target Flowers102 and\nCIFAR100 datasets. Besides, our proposal on CLIP-based VP provides 13.7% and\n7.1% accuracy improvements on Flowers102 and DTD respectively. Our code is\navailable at https://github.com/OPTML-Group/ILM-VP.", "authors": ["Aochuan Chen", "Yuguang Yao", "Pin-Yu Chen", "Yihua Zhang", "Sijia Liu"], "published_date": "2022_11_21", "pdf_url": "http://arxiv.org/pdf/2211.11635v5", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><th>VP+TP</th><td colspan=\"2\">Ours (VP+TP+LM)</td></tr><tr><th>Acc(%)</th><td>Acc(%)</td><td>Examples of context prompt template \\to target label</td></tr><tr><th>Flowers102</th><th>70.0</th><td>83.7</td><td>a close-up photo of a {} \\to buttercup</td></tr><tr><th>DTD</th><th>56.8</th><td>63.9</td><td>graffiti of a {} \\to blotchy</td></tr><tr><th>UCF101</th><th>66.0</th><td>70.6</td><td>a {} in a video game \\to baseball pitch</td></tr><tr><th>Food101</th><th>78.9</th><td>79.1</td><td>a photo of the dirty {} \\to crab cake</td></tr><tr><th>SVHN</th><th>89.9</th><td>91.2</td><td>a photo of a {} \\to 7</td></tr><tr><th>EuroSAT</th><th>96.4</th><td>96.9</td><td>a pixelated photo of a {} \\to river</td></tr><tr><th>StanfordCars</th><th>57.2</th><td>57.6</td><td>the toy {} \\to 2011 audi s6 sedan</td></tr><tr><th>SUN397</th><th>60.5</th><td>61.2</td><td>a photo of a large {} \\to archive</td></tr><tr><th>CIFAR10</th><th>93.9</th><td>94.4</td><td>a pixelated photo of a {} \\to ship</td></tr></tbody></table>", "caption": "Table II: Results of our CLIP-based prompt learning \u2018VP+TP+LM\u2019 and the baseline \u2018VP+TP\u2019 [2] (restricted to using text prompt template \u201cThis is a photo of a {}\") over 9 target datasets. In each cell, the target task accuracy (%) is shown along with examples of LM in the text domain. Our method with higher accuracy than SOTA is marked in bold.", "list_citation_info": ["[2] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola, \u201cExploring visual prompts for adapting large-scale models,\u201d arXiv preprint arXiv:2203.17274, vol. 1, no. 3, pp. 4, 2022."]}], "citation_info_to_title": {"[2] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola, \u201cExploring visual prompts for adapting large-scale models,\u201d arXiv preprint arXiv:2203.17274, vol. 1, no. 3, pp. 4, 2022.": "Exploring visual prompts for adapting large-scale models"}, "source_title_to_arxiv_id": {"Exploring visual prompts for adapting large-scale models": "2203.17274"}}