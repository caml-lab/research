{"title": "Team Yao at Factify 2022: Utilizing Pre-trained Models and Co-attention Networks for Multi-Modal Fact Verification", "abstract": "In recent years, social media has enabled users to get exposed to a myriad of\nmisinformation and disinformation; thus, misinformation has attracted a great\ndeal of attention in research fields and as a social issue. To address the\nproblem, we propose a framework, Pre-CoFact, composed of two pre-trained models\nfor extracting features from text and images, and multiple co-attention\nnetworks for fusing the same modality but different sources and different\nmodalities. Besides, we adopt the ensemble method by using different\npre-trained models in Pre-CoFact to achieve better performance. We further\nillustrate the effectiveness from the ablation study and examine different\npre-trained models for comparison. Our team, Yao, won the fifth prize\n(F1-score: 74.585\\%) in the Factify challenge hosted by De-Factify @ AAAI 2022,\nwhich demonstrates that our model achieved competitive performance without\nusing auxiliary tasks or extra information. The source code of our work is\npublicly available at\nhttps://github.com/wywyWang/Multi-Modal-Fact-Verification-2021", "authors": ["Wei-Yao Wang", "Wen-Chih Peng"], "published_date": "2022_01_26", "pdf_url": "http://arxiv.org/pdf/2201.11664v1", "list_table_and_caption": [{"table": "<table><tr><td>Model</td><td>DINO [23]</td><td>XLM-RoBERTa [27]</td><td>RoBERTa [18]</td><td>Pre-CoFact (Ours)</td></tr><tr><td>Weighted F1 (%)</td><td>73.94 (-4.52)</td><td>74.11 (-4.35)</td><td>77.53 (-0.93)</td><td>78.46</td></tr></table>", "caption": "Table 2: Variant pre-trained models in terms of validation score. Pre-CoFact uses DeiT and DeBERTa as pre-trained models. DINO is replaced DeiT by DINO [23]. XLM-RoBERTa and RoBERTa are replaced DeBERTa by XLM-RoBERTa [27] and RoBERTa [18], respectively.", "list_citation_info": ["Liu et al. [2019] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized BERT pretraining approach, CoRR abs/1907.11692 (2019).", "Conneau et al. [2020] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott, L. Zettlemoyer, V. Stoyanov, Unsupervised cross-lingual representation learning at scale, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 2020, pp. 8440\u20138451.", "Caron et al. [2021] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, A. Joulin, Emerging properties in self-supervised vision transformers, CoRR abs/2104.14294 (2021)."]}], "citation_info_to_title": {"Caron et al. [2021] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, A. Joulin, Emerging properties in self-supervised vision transformers, CoRR abs/2104.14294 (2021).": "Emerging properties in self-supervised vision transformers", "Liu et al. [2019] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized BERT pretraining approach, CoRR abs/1907.11692 (2019).": "Roberta: A robustly optimized BERT pretraining approach", "Conneau et al. [2020] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott, L. Zettlemoyer, V. Stoyanov, Unsupervised cross-lingual representation learning at scale, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, 2020, pp. 8440\u20138451.": "Unsupervised cross-lingual representation learning at scale"}, "source_title_to_arxiv_id": {"Emerging properties in self-supervised vision transformers": "2104.14294"}}