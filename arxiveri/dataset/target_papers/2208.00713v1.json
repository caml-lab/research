{"title": "TransDeepLab: Convolution-Free Transformer-based DeepLab v3+ for Medical Image Segmentation", "abstract": "Convolutional neural networks (CNNs) have been the de facto standard in a\ndiverse set of computer vision tasks for many years. Especially, deep neural\nnetworks based on seminal architectures such as U-shaped models with\nskip-connections or atrous convolution with pyramid pooling have been tailored\nto a wide range of medical image analysis tasks. The main advantage of such\narchitectures is that they are prone to detaining versatile local features.\nHowever, as a general consensus, CNNs fail to capture long-range dependencies\nand spatial correlations due to the intrinsic property of confined receptive\nfield size of convolution operations. Alternatively, Transformer, profiting\nfrom global information modelling that stems from the self-attention mechanism,\nhas recently attained remarkable performance in natural language processing and\ncomputer vision. Nevertheless, previous studies prove that both local and\nglobal features are critical for a deep model in dense prediction, such as\nsegmenting complicated structures with disparate shapes and configurations. To\nthis end, this paper proposes TransDeepLab, a novel DeepLab-like pure\nTransformer for medical image segmentation. Specifically, we exploit\nhierarchical Swin-Transformer with shifted windows to extend the DeepLabv3 and\nmodel the Atrous Spatial Pyramid Pooling (ASPP) module. A thorough search of\nthe relevant literature yielded that we are the first to model the seminal\nDeepLab model with a pure Transformer-based model. Extensive experiments on\nvarious medical image segmentation tasks verify that our approach performs\nsuperior or on par with most contemporary works on an amalgamation of Vision\nTransformer and CNN-based methods, along with a significant reduction of model\ncomplexity. The codes and trained models are publicly available at\nhttps://github.com/rezazad68/transdeeplab", "authors": ["Reza Azad", "Moein Heidari", "Moein Shariatnia", "Ehsan Khodapanah Aghdam", "Sanaz Karimijafarbigloo", "Ehsan Adeli", "Dorit Merhof"], "published_date": "2022_08_01", "pdf_url": "http://arxiv.org/pdf/2208.00713v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Methods</th><th>DSC \\uparrow</th><th>HD \\downarrow</th><th>Aorta</th><th>Gallbladder</th><th>Kidney(L)</th><th>Kidney(R)</th><th>Liver</th><th>Pancreas</th><th>Spleen</th><th>Stomach</th></tr></thead><tbody><tr><th>V-Net [28]</th><td>68.81</td><td>-</td><td>75.34</td><td>51.87</td><td>77.10</td><td>80.75</td><td>87.84</td><td>40.05</td><td>80.56</td><td>56.98</td></tr><tr><th>R50 U-Net [9]</th><td>74.68</td><td>36.87</td><td>87.74</td><td>63.66</td><td>80.60</td><td>78.19</td><td>93.74</td><td>56.90</td><td>85.87</td><td>74.16</td></tr><tr><th>U-Net [31]</th><td>76.85</td><td>39.70</td><td>89.07</td><td>69.72</td><td>77.77</td><td>68.60</td><td>93.43</td><td>53.98</td><td>86.67</td><td>75.58</td></tr><tr><th>R50 Att-UNet [9]</th><td>75.57</td><td>36.97</td><td>55.92</td><td>63.91</td><td>79.20</td><td>72.71</td><td>93.56</td><td>49.37</td><td>87.19</td><td>74.95</td></tr><tr><th>Att-UNet [29]</th><td>77.77</td><td>36.02</td><td>89.55</td><td>68.88</td><td>77.98</td><td>71.11</td><td>93.57</td><td>58.04</td><td>87.30</td><td>75.75</td></tr><tr><th>R50 ViT [9]</th><td>71.29</td><td>32.87</td><td>73.73</td><td>55.13</td><td>75.80</td><td>72.20</td><td>91.51</td><td>45.99</td><td>81.99</td><td>73.95</td></tr><tr><th>TransUnet [9]</th><td>77.48</td><td>31.69</td><td>87.23</td><td>63.13</td><td>81.87</td><td>77.02</td><td>94.08</td><td>55.86</td><td>85.08</td><td>75.62</td></tr><tr><th>SwinUnet [8]</th><td>79.13</td><td>21.55</td><td>85.47</td><td>66.53</td><td>83.28</td><td>79.61</td><td>94.29</td><td>56.58</td><td>90.66</td><td>76.60</td></tr><tr><th>DeepLabv3+ (CNN) [11]</th><td>77.63</td><td>39.95</td><td>88.04</td><td>66.51</td><td>82.76</td><td>74.21</td><td>91.23</td><td>58.32</td><td>87.43</td><td>73.53</td></tr><tr><th>Proposed Method</th><td>80.16</td><td>21.25</td><td>86.04</td><td>69.16</td><td>84.08</td><td>79.88</td><td>93.53</td><td>61.19</td><td>89.00</td><td>78.40</td></tr></tbody></table>", "caption": "Table 1: Comparison results of the proposed method on the Synapse dataset.", "list_citation_info": ["[8] Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M.: Swin-unet: Unet-like pure transformer for medical image segmentation. arXiv preprint arXiv:2105.05537 (2021)", "[31] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234\u2013241. Springer (2015)", "[29] Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018)", "[28] Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 2016 fourth international conference on 3D vision (3DV). pp. 565\u2013571. IEEE (2016)", "[11] Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence 40(4), 834\u2013848 (2017)", "[9] Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021)"]}], "citation_info_to_title": {"[29] Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning where to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018)": "Attention U-Net: Learning Where to Look for the Pancreas", "[8] Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M.: Swin-unet: Unet-like pure transformer for medical image segmentation. arXiv preprint arXiv:2105.05537 (2021)": "Swin-UNet: UNet-like Pure Transformer for Medical Image Segmentation", "[9] Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021)": "Transunet: Transformers make strong encoders for medical image segmentation", "[11] Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence 40(4), 834\u2013848 (2017)": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs", "[28] Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 2016 fourth international conference on 3D vision (3DV). pp. 565\u2013571. IEEE (2016)": "V-net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "[31] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234\u2013241. Springer (2015)": "U-net: Convolutional networks for biomedical image segmentation"}, "source_title_to_arxiv_id": {"Attention U-Net: Learning Where to Look for the Pancreas": "1804.03999", "Swin-UNet: UNet-like Pure Transformer for Medical Image Segmentation": "2105.05537", "Transunet: Transformers make strong encoders for medical image segmentation": "2102.04306"}}