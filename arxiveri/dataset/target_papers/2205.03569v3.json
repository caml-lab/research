{"title": "Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement", "abstract": "Compressed video action recognition has recently drawn growing attention,\nsince it remarkably reduces the storage and computational cost via replacing\nraw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,\nmotion vectors and residuals). However, this task severely suffers from the\ncoarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB\nand motion modalities. To address the two issues above, this paper proposes a\nnovel framework, namely Attentive Cross-modal Interaction Network with Motion\nEnhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for\nthe RGB modality and the other for the motion modality. Particularly, the\nmotion stream employs a multi-scale block embedded with a denoising module to\nenhance representation learning. The interaction between the two streams is\nthen strengthened by introducing the Selective Motion Complement (SMC) and\nCross-Modality Augment (CMA) modules, where SMC complements the RGB modality\nwith spatio-temporally attentive local motion features and CMA further combines\nthe two modalities with selective feature augmentation. Extensive experiments\non the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the\neffectiveness and efficiency of MEACI-Net.", "authors": ["Bing Li", "Jiaxin Chen", "Dongming Zhang", "Xiuguo Bao", "Di Huang"], "published_date": "2022_05_07", "pdf_url": "http://arxiv.org/pdf/2205.03569v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td colspan=\"2\">Methods</td><td>Reference</td><td>Input Size [MB]</td><td>GFLOPs</td><td>Optical flow</td><td>HMDB-51</td><td>UCF-101</td><td>Kinetics-400</td></tr><tr><td rowspan=\"10\">Raw video based</td><td>TSN</td><td>Wang et al. (2016)</td><td>10.5</td><td>1600</td><td>Train &amp; Test</td><td>68.5</td><td>94.0</td><td>69.1</td></tr><tr><td>I3D-RGB</td><td>Carreira andZisserman (2017)</td><td>-</td><td>-</td><td>No</td><td>74.8</td><td>95.6</td><td>71.1</td></tr><tr><td>I3D+Flow</td><td>Carreira andZisserman (2017)</td><td>-</td><td>-</td><td>Train &amp; Test</td><td>80.7</td><td>98.0</td><td>63.4</td></tr><tr><td>ARTNet</td><td>Wang et al. (2018)</td><td>25</td><td>5875</td><td>No</td><td>70.9</td><td>94.3</td><td>70.7</td></tr><tr><td>R(2+1)D+Flow</td><td>Tran et al. (2018)</td><td>13.4</td><td>3040</td><td>Train &amp; Test</td><td>78.7</td><td>97.3</td><td>72.0</td></tr><tr><td>TSM</td><td>Lin et al. (2019)</td><td>12</td><td>1950</td><td>No</td><td>73.2</td><td>96.0</td><td>74.1</td></tr><tr><td>STM</td><td>Jiang et al. (2019)</td><td>12</td><td>2010</td><td>No</td><td>72.2</td><td>96.2</td><td>73.7</td></tr><tr><td>SlowFast</td><td>Feichtenhofer et al. (2019)</td><td>30</td><td>1971</td><td>No</td><td>79.3</td><td>96.8</td><td>75.6</td></tr><tr><td>TEA</td><td>Li et al. (2020b)</td><td>12</td><td>2100</td><td>No</td><td>73.3</td><td>96.9</td><td>76.1</td></tr><tr><td>TDN</td><td>Wang et al. (2020)</td><td>18</td><td>3240</td><td>No</td><td>76.3</td><td>97.4</td><td>76.6</td></tr><tr><td rowspan=\"13\">Compressed video based</td><td>EMV-CNN</td><td>Zhang et al. (2016)</td><td>6.5</td><td>-</td><td>Train</td><td>51.2</td><td>86.4</td><td>-</td></tr><tr><td>DTMV-CNN</td><td>Zhang et al. (2018)</td><td>6.5</td><td>-</td><td>Train</td><td>55.3</td><td>87.5</td><td>-</td></tr><tr><td>CoViAR</td><td>Wu et al. (2018)</td><td>6.8</td><td>3615</td><td>No</td><td>59.1</td><td>90.4</td><td>-</td></tr><tr><td>CoViAR + Flow</td><td>Wu et al. (2018)</td><td>11.0</td><td>3970</td><td>Train &amp; Test</td><td>70.2</td><td>94.9</td><td>-</td></tr><tr><td>CoViAR + PWC-Net</td><td>Sun et al. (2018)</td><td>6.8</td><td>-</td><td>Train</td><td>62.2</td><td>90.6</td><td>-</td></tr><tr><td>Refined-MV</td><td>Cao et al. (2019)</td><td>-</td><td>-</td><td>No</td><td>59.7</td><td>89.9</td><td>-</td></tr><tr><td>TTP</td><td>Huo et al. (2019)</td><td>6.8</td><td>1050</td><td>No</td><td>58.2</td><td>87.2</td><td>-</td></tr><tr><td>IP TSN</td><td>Huang et al. (2019)</td><td>6.8</td><td>3400</td><td>Train</td><td>69.1</td><td>93.4</td><td>-</td></tr><tr><td>DMC-Net (ResNet-18)</td><td>Shou et al. (2019)</td><td>-</td><td>-</td><td>Train</td><td>62.8</td><td>90.9</td><td>-</td></tr><tr><td>DMC-Net (I3D)</td><td>Shou et al. (2019)</td><td>-</td><td>401</td><td>Train</td><td>71.8</td><td>92.3</td><td>-</td></tr><tr><td>MFCD-Net</td><td>Battash et al. (2020)</td><td>0.4</td><td>1300</td><td>No</td><td>66.9</td><td>93.2</td><td>68.3</td></tr><tr><td>SIFP-Net</td><td>Li et al. (2020a)</td><td>8.1</td><td>1971</td><td>No</td><td>72.3</td><td>94.0</td><td>-</td></tr><tr><td>MEACI-Net (1-clip)</td><td>Ours</td><td>0.2</td><td>89</td><td>No</td><td>74.0</td><td>96.1</td><td>70.4</td></tr><tr><td></td><td>MEACI-Net (3-clip)</td><td>Ours</td><td>0.7</td><td>268</td><td>No</td><td>74.4</td><td>96.4</td><td>71.5</td></tr></tbody></table>", "caption": "Table 1: Comparison with the state-of-the-art approaches in the top-1 accuracy (%) on the HMDB-51, UCF-101 and Kinetics-400 datasets. \u2018MB\u2019 is short for MegaByte. \u2018-\u2019 indicates that the corresponding result is NOT publicly available.", "list_citation_info": ["Wang et al. [2016] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, pages 20\u201336, 2016.", "Lin et al. [2019] Ji Lin, Chuang Gan, and Song Han. TSM: temporal shift module for efficient video understanding. In ICCV, pages 7082\u20137092, 2019.", "Zhang et al. [2016] Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, and Hanli Wang. Real-time action recognition with enhanced motion vector cnns. In CVPR, pages 2718\u20132726, 2016.", "Sun et al. [2018] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In CVPR, pages 8934\u20138943, 2018.", "Wang et al. [2018] Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In CVPR, pages 1430\u20131439, 2018.", "Wang et al. [2020] Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efficient action recognition. arXiv preprint arXiv:2012.10071, 2020.", "Wu et al. [2018] Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R. Manmatha, Alexander J. Smola, and Philipp Kr\u00e4henb\u00fchl. Compressed video action recognition. In CVPR, pages 6026\u20136035, 2018.", "Huo et al. [2019] Yuqi Huo, Xiaoli Xu, Yao Lu, Yulei Niu, Zhiwu Lu, and Ji-Rong Wen. Mobile video action recognition. arXiv preprint arXiv:1908.10155, 2019.", "Battash et al. [2020] Barak Battash, Haim Barad, Hanlin Tang, and Amit Bleiweiss. Mimic the raw domain: Accelerating action recognition in the compressed domain. In CVPR Workshops, pages 684\u2013685, 2020.", "Tran et al. [2018] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, pages 6450\u20136459, 2018.", "Zhang et al. [2018] Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, and Hanli Wang. Real-time action recognition with deeply transferred motion vector cnns. IEEE Transactions on Image Processing, 27(5):2326\u20132339, 2018.", "Huang et al. [2019] Shiyuan Huang, Xudong Lin, Svebor Karaman, and Shih-Fu Chang. Flow-distilled ip two-stream networks for compressed video actionrecognition. arXiv preprint arXiv:1912.04462, 2019.", "Carreira and Zisserman [2017] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724\u20134733, 2017.", "Shou et al. [2019] Zheng Shou, Xudong Lin, Yannis Kalantidis, Laura Sevilla-Lara, Marcus Rohrbach, Shih-Fu Chang, and Zhicheng Yan. Dmc-net: Generating discriminative motion cues for fast compressed video action recognition. In CVPR, pages 1268\u20131277, 2019.", "Jiang et al. [2019] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. In CVPR, pages 2000\u20132009, 2019.", "Feichtenhofer et al. [2019] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In CVPR, pages 6202\u20136211, 2019.", "Cao et al. [2019] Haoyuan Cao, Shining Yu, and Jiashi Feng. Compressed video action recognition with refined motion vector. arXiv preprint arXiv:1910.02533, 2019.", "Li et al. [2020a] Jiapeng Li, Ping Wei, Yongchi Zhang, and Nanning Zheng. A slow-i-fast-p architecture for compressed video action recognition. In ACM MM, pages 2039\u20132047, 2020.", "Li et al. [2020b] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages 909\u2013918, 2020."]}], "citation_info_to_title": {"Carreira and Zisserman [2017] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, pages 4724\u20134733, 2017.": "Quo vadis, action recognition?", "Cao et al. [2019] Haoyuan Cao, Shining Yu, and Jiashi Feng. Compressed video action recognition with refined motion vector. arXiv preprint arXiv:1910.02533, 2019.": "Compressed Video Action Recognition with Refined Motion Vector", "Huang et al. [2019] Shiyuan Huang, Xudong Lin, Svebor Karaman, and Shih-Fu Chang. Flow-distilled ip two-stream networks for compressed video actionrecognition. arXiv preprint arXiv:1912.04462, 2019.": "Flow-Distilled IP Two-Stream Networks for Compressed Video Action Recognition", "Zhang et al. [2016] Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, and Hanli Wang. Real-time action recognition with enhanced motion vector cnns. In CVPR, pages 2718\u20132726, 2016.": "Real-time action recognition with enhanced motion vector cnns", "Huo et al. [2019] Yuqi Huo, Xiaoli Xu, Yao Lu, Yulei Niu, Zhiwu Lu, and Ji-Rong Wen. Mobile video action recognition. arXiv preprint arXiv:1908.10155, 2019.": "Mobile Video Action Recognition", "Li et al. [2020b] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In CVPR, pages 909\u2013918, 2020.": "Tea: Temporal excitation and aggregation for action recognition", "Tran et al. [2018] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In CVPR, pages 6450\u20136459, 2018.": "A closer look at spatiotemporal convolutions for action recognition", "Lin et al. [2019] Ji Lin, Chuang Gan, and Song Han. TSM: temporal shift module for efficient video understanding. In ICCV, pages 7082\u20137092, 2019.": "TSM: Temporal Shift Module for Efficient Video Understanding", "Zhang et al. [2018] Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, and Hanli Wang. Real-time action recognition with deeply transferred motion vector cnns. IEEE Transactions on Image Processing, 27(5):2326\u20132339, 2018.": "Real-time action recognition with deeply transferred motion vector CNNs", "Li et al. [2020a] Jiapeng Li, Ping Wei, Yongchi Zhang, and Nanning Zheng. A slow-i-fast-p architecture for compressed video action recognition. In ACM MM, pages 2039\u20132047, 2020.": "A slow-i-fast-p architecture for compressed video action recognition", "Jiang et al. [2019] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. In CVPR, pages 2000\u20132009, 2019.": "STM: Spatiotemporal and Motion Encoding for Action Recognition", "Wang et al. [2016] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, pages 20\u201336, 2016.": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition", "Feichtenhofer et al. [2019] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In CVPR, pages 6202\u20136211, 2019.": "Slowfast networks for video recognition", "Wang et al. [2020] Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efficient action recognition. arXiv preprint arXiv:2012.10071, 2020.": "TDN: Temporal Difference Networks for Efficient Action Recognition", "Shou et al. [2019] Zheng Shou, Xudong Lin, Yannis Kalantidis, Laura Sevilla-Lara, Marcus Rohrbach, Shih-Fu Chang, and Zhicheng Yan. Dmc-net: Generating discriminative motion cues for fast compressed video action recognition. In CVPR, pages 1268\u20131277, 2019.": "DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition", "Wang et al. [2018] Limin Wang, Wei Li, Wen Li, and Luc Van Gool. Appearance-and-relation networks for video classification. In CVPR, pages 1430\u20131439, 2018.": "Appearance-and-relation networks for video classification", "Wu et al. [2018] Chao-Yuan Wu, Manzil Zaheer, Hexiang Hu, R. Manmatha, Alexander J. Smola, and Philipp Kr\u00e4henb\u00fchl. Compressed video action recognition. In CVPR, pages 6026\u20136035, 2018.": "Compressed video action recognition", "Battash et al. [2020] Barak Battash, Haim Barad, Hanlin Tang, and Amit Bleiweiss. Mimic the raw domain: Accelerating action recognition in the compressed domain. In CVPR Workshops, pages 684\u2013685, 2020.": "Mimic the raw domain: Accelerating action recognition in the compressed domain", "Sun et al. [2018] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In CVPR, pages 8934\u20138943, 2018.": "Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume"}, "source_title_to_arxiv_id": {"TDN: Temporal Difference Networks for Efficient Action Recognition": "2012.10071"}}