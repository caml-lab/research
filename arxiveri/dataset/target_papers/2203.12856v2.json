{"title": "Beyond Fixation: Dynamic Window Visual Transformer", "abstract": "Recently, a surge of interest in visual transformers is to reduce the\ncomputational cost by limiting the calculation of self-attention to a local\nwindow. Most current work uses a fixed single-scale window for modeling by\ndefault, ignoring the impact of window size on model performance. However, this\nmay limit the modeling potential of these window-based models for multi-scale\ninformation. In this paper, we propose a novel method, named Dynamic Window\nVision Transformer (DW-ViT). The dynamic window strategy proposed by DW-ViT\ngoes beyond the model that employs a fixed single window setting. To the best\nof our knowledge, we are the first to use dynamic multi-scale windows to\nexplore the upper limit of the effect of window settings on model performance.\nIn DW-ViT, multi-scale information is obtained by assigning windows of\ndifferent sizes to different head groups of window multi-head self-attention.\nThen, the information is dynamically fused by assigning different weights to\nthe multi-scale window branches. We conducted a detailed performance evaluation\non three datasets, ImageNet-1K, ADE20K, and COCO. Compared with related\nstate-of-the-art (SoTA) methods, DW-ViT obtains the best performance.\nSpecifically, compared with the current SoTA Swin Transformers\n\\cite{liu2021swin}, DW-ViT has achieved consistent and substantial improvements\non all three datasets with similar parameters and computational costs. In\naddition, DW-ViT exhibits good scalability and can be easily inserted into any\nwindow-based visual transformers.", "authors": ["Pengzhen Ren", "Changlin Li", "Guangrun Wang", "Yun Xiao", "Qing Du", "Xiaodan Liang", "Xiaojun Chang"], "published_date": "2022_03_24", "pdf_url": "http://arxiv.org/pdf/2203.12856v2", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td> #param. (M) </td><td> FLOPs (G) </td><td>  Top-1 (%)  </td></tr><tr><td colspan=\"4\">ConvNet</td></tr><tr><td>ResNet50[19]</td><td>26</td><td>4.1</td><td>76.6</td></tr><tr><td>ResNet101[19]</td><td>45</td><td>7.9</td><td>78.2</td></tr><tr><td>X50-32x4d[58]</td><td>25</td><td>4.3</td><td>77.9</td></tr><tr><td>X101-32x4d[58]</td><td>44</td><td>8.0</td><td>78.7</td></tr><tr><td>RegNetY-4G [38]</td><td>21</td><td>4.0</td><td>80.0</td></tr><tr><td>RegNetY-8G [38]</td><td>39</td><td>8.0</td><td>81.7</td></tr><tr><td>RegNetY-16G [38]</td><td>84</td><td>16</td><td>82.9</td></tr><tr><td colspan=\"4\">Transformer</td></tr><tr><td>DeiT-Small/16 [49]</td><td>22</td><td>4.6</td><td>79.9</td></tr><tr><td>CrossViT-S [4]</td><td>27</td><td>5.6</td><td>81.0</td></tr><tr><td>T2T-ViT-14 [62]</td><td>22</td><td>5.2</td><td>81.5</td></tr><tr><td>TNT-S [16]</td><td>24</td><td>5.2</td><td>81.3</td></tr><tr><td>CoaT Mini [59]</td><td>10</td><td>6.8</td><td>80.8</td></tr><tr><td>PVT-Small [54]</td><td>25</td><td>3.8</td><td>79.8</td></tr><tr><td>CPVT-GAP [62]</td><td>23</td><td>4.6</td><td>81.5</td></tr><tr><td>CrossFormer-S{}^{\\dagger} [55]</td><td>28</td><td>4.5</td><td>81.5</td></tr><tr><td>Swin-T [34]</td><td>28</td><td>4.5</td><td>81.3</td></tr><tr><td>DW-T</td><td>30</td><td>5.2</td><td>82.0</td></tr><tr><td>ViT-Base/16 [11]</td><td>87</td><td>17.6</td><td>77.9</td></tr><tr><td>DeiT-Base/16 [49]</td><td>87</td><td>17.6</td><td>81.8</td></tr><tr><td>T2T-ViT-24 [62]</td><td>64</td><td>14.1</td><td>82.3</td></tr><tr><td>CrossViT-B [4]</td><td>105</td><td>21.2</td><td>82.2</td></tr><tr><td>TNT-B [16]</td><td>66</td><td>14.1</td><td>82.8</td></tr><tr><td>CPVT-B [8]</td><td>88</td><td>17.6</td><td>82.3</td></tr><tr><td>PVT-Large [54]</td><td>61</td><td>9.8</td><td>81.7</td></tr><tr><td>Swin-B [34]</td><td>88</td><td>15.4</td><td>83.3</td></tr><tr><td>DW-B</td><td>91</td><td>17.0</td><td>83.8</td></tr></table>", "caption": "Table 2: Performance comparison on ImageNet-1K. All models are trained and evaluated at 224\\times 224 resolution. \\text{CrossFormer-S}^{\\dagger} shows the performance in the case of single-scale embedding.", "list_citation_info": ["[58] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492\u20131500, 2017.", "[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. arXiv preprint arXiv:2103.14899, 2021.", "[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.", "[8] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021.", "[16] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.", "[55] Wenxiao Wang, Lu Yao, Long Chen, Deng Cai, Xiaofei He, and Wei Liu. Crossformer: A versatile vision transformer based on cross-scale attention. arXiv preprint arXiv:2108.00154, 2021.", "[59] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. arXiv preprint arXiv:2104.06399, 2021.", "[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.", "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.", "[62] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.", "[38] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428\u201310436, 2020.", "[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021."]}, {"table": "<table><tr><td>Backbone</td><td>Method</td><td> #param.(M) </td><td> FLOPs(G) </td><td> mIoU </td><td> +MS </td></tr><tr><td>ResNet-101 [19]</td><td>DANet [36]</td><td>69</td><td>1119</td><td>45.3</td><td>-</td></tr><tr><td>ResNet-101</td><td>OCRNet [63]</td><td>56</td><td>923</td><td>44.1</td><td>-</td></tr><tr><td>ResNet-101</td><td>DLab.v3+ [6]</td><td>63</td><td>1021</td><td>44.1</td><td>-</td></tr><tr><td>ResNet-101</td><td>ACNet[14]</td><td>-</td><td>-</td><td>45.9</td><td>-</td></tr><tr><td>ResNet-101</td><td>DNL[61]</td><td>69</td><td>1249</td><td>46.0</td><td>-</td></tr><tr><td>ResNet-101</td><td>UperNet [57]</td><td>86</td><td>1029</td><td>44.9</td><td>-</td></tr><tr><td>HRNet-w48 [41]</td><td>DLab.v3+ [6]</td><td>71</td><td>664</td><td>45.7</td><td></td></tr><tr><td>ResNeSt-101[64]</td><td>DLab.v3+[6]</td><td>66</td><td>1051</td><td>46.9</td><td>-</td></tr><tr><td>ResNeSt-200[64]</td><td>DLab.v3+[6]</td><td>88</td><td>1381</td><td>48.4</td><td>-</td></tr><tr><td>PVT-S [54]</td><td>S-FPN [26]</td><td>28</td><td>-</td><td>39.8</td><td></td></tr><tr><td>PVT-M</td><td>S-FPN</td><td>48</td><td>219</td><td>41.6</td><td>-</td></tr><tr><td>PVT-L</td><td>S-FPN</td><td>65</td><td>283</td><td>42.1</td><td>-</td></tr><tr><td>CAT-S [31]</td><td>S-FPN</td><td>41</td><td>214</td><td>42.8</td><td>-</td></tr><tr><td>CAT-B</td><td>S-FPN</td><td>55</td><td>276</td><td>44.9</td><td>-</td></tr><tr><td>Swin-T[34]</td><td>UperNet[57]</td><td>60</td><td>945</td><td>44.5</td><td>45.8</td></tr><tr><td>Swin-B[34]</td><td>UperNet[57]</td><td>121</td><td>1188</td><td>48.1</td><td>49.7</td></tr><tr><td>DW-T</td><td>UperNet[57]</td><td>61</td><td>953</td><td>45.7</td><td>46.9</td></tr><tr><td>DW-B</td><td>UperNet[57]</td><td>125</td><td>1200</td><td>48.7</td><td>50.3</td></tr></table>", "caption": "Table 3: Performance comparison on the ADE20K [66] val. The single-scale and multi-scale evaluation results are presented in the last two columns. The FLOPs (G) are calculated at an input resolution of 1024\\times 1024.", "list_citation_info": ["[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418\u2013434, 2018.", "[41] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5693\u20135703, 2019.", "[61] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In European Conference on Computer Vision, pages 191\u2013207. Springer, 2020.", "[64] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi-Li Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R. Manmatha, Mu Li, and Alex Smola. Resnest: Split-attention networks. ArXiv, abs/2004.08955, 2020.", "[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.", "[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[14] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive context network for scene parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6748\u20136757, 2019.", "[26] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6399\u20136408, 2019.", "[63] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16, pages 173\u2013190. Springer, 2020.", "[36] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks for multimodal reasoning and matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 299\u2013307, 2017.", "[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.", "[31] Hezheng Lin, Xing Cheng, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan Wang, Qing Song, and Wei Yuan. Cat: Cross attention in vision transformer. arXiv preprint arXiv:2106.05786, 2021.", "[66] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2018.", "[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021."]}, {"table": "<table><tr><td>Method</td><td> #param. (M) </td><td> FLOPs (G) </td><td>AP{}^{\\text{box}}</td><td>AP{}_{50}^{\\text{box}}</td><td>AP{}_{75}^{\\text{box}}</td><td>AP{}^{\\text{mask}}</td><td>AP{}_{50}^{\\text{mask}}</td><td>AP{}_{75}^{\\text{mask}}</td></tr><tr><td colspan=\"9\">Mask R-CNN [18]</td></tr><tr><td>ResNet50 [19]</td><td>44</td><td>260</td><td> 41.0 </td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><td>PVT-Small [54]</td><td>44</td><td>245</td><td>43.0</td><td>65.3</td><td>46.9</td><td>39.9</td><td>62.5</td><td>42.8</td></tr><tr><td>ViL-Small [65]</td><td>45</td><td>174</td><td>43.4</td><td>64.9</td><td>47.0</td><td>39.6</td><td>62.1</td><td>42.4</td></tr><tr><td>Swin-T [34]</td><td>48</td><td>264</td><td>46.0</td><td>68.2</td><td>50.2</td><td>41.6</td><td>65.1</td><td>44.8</td></tr><tr><td>DW-T</td><td>49</td><td>275</td><td>46.7</td><td>69.1</td><td>51.4</td><td>42.4</td><td>66.2</td><td>45.6</td></tr><tr><td>ResNeXt101-64x4d [58]</td><td>102</td><td>493</td><td>44.4</td><td>64.9</td><td>48.8</td><td>39.7</td><td>61.9</td><td>42.6</td></tr><tr><td>PVT-Large [54]</td><td>81</td><td>364</td><td>44.5</td><td>66.0</td><td>48.3</td><td>40.7</td><td>63.4</td><td>43.7</td></tr><tr><td>ViL-Base [65]</td><td>76.1</td><td>365</td><td>45.7</td><td>67.2</td><td>49.9</td><td>41.3</td><td>64.4</td><td>44.5</td></tr><tr><td>Swin-Base [34]</td><td>107</td><td>496</td><td>48.5</td><td>69.8</td><td>53.2</td><td>43.4</td><td>66.8</td><td>46.9</td></tr><tr><td>DW-B</td><td>111</td><td>505</td><td>49.2</td><td>70.6</td><td>54.0</td><td>44.0</td><td>68.0</td><td>47.7</td></tr><tr><td colspan=\"9\">Cascade Mask R-CNN [2, 18]</td></tr><tr><td>DeiT-S{}^{\\dagger}[49]</td><td>80</td><td>889</td><td> 48.0 </td><td> 67.2 </td><td> 51.7 </td><td>41.4</td><td>64.2</td><td>44.3</td></tr><tr><td>ResNet50[19]</td><td>82</td><td>739</td><td> 46.3 </td><td> 64.3 </td><td> 50.5 </td><td>40.1</td><td>61.7</td><td>43.4</td></tr><tr><td>Swin-T[34]</td><td>86</td><td>745</td><td> 50.5 </td><td> 69.3 </td><td> 54.9 </td><td>43.7</td><td>66.6</td><td>47.1</td></tr><tr><td>DW-T</td><td>87</td><td>754</td><td>51.5</td><td>70.5</td><td>55.9</td><td>44.7</td><td>67.8</td><td>48.5</td></tr><tr><td>X101-64 [58]</td><td>140</td><td>972</td><td> 48.3 </td><td> 66.4 </td><td> 52.3 </td><td>41.7</td><td>64.0</td><td>45.1</td></tr><tr><td>Swin-B [34]</td><td>145</td><td>982</td><td> 51.9 </td><td> 70.9 </td><td> 56.5 </td><td>45.0</td><td>68.4</td><td>48.7</td></tr><tr><td>DW-B</td><td>149</td><td>992</td><td>52.9</td><td>71.6</td><td>57.5</td><td>45.7</td><td>69.0</td><td>50.0</td></tr></table>", "caption": "Table 4: Performance comparison of object detection and instance segmentation on the COCO2017 val dataset. Two object detection frameworks are used: Mask R-CNN [18] and Cascade Mask R-CNN [2]. The FLOPs (G) are calculated at an input resolution of 1280\\times 800. {}^{\\dagger} indicates that additional deconvolution layers are used to generate hierarchical features.", "list_citation_info": ["[58] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492\u20131500, 2017.", "[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154\u20136162, 2018.", "[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.", "[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.", "[18] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.", "[65] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. ArXiv, abs/2103.15358, 2021.", "[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021."]}, {"table": "<table><tr><td>Method</td><td>Window</td><td> #param. (M) </td><td> FLOPs (G) </td><td> Top-1 (%) </td></tr><tr><td>Swin-T</td><td> 71114172123 </td><td> 28.2928.3128.3428.3528.3628.36 </td><td> 4.494.694.895.065.345.49 </td><td> 74.3175.1875.8376.3176.2876.24 </td></tr><tr><td>DW-T</td><td>DMSW</td><td></td><td></td><td></td></tr><tr><td> MSW-MSA([7,14,21]) </td><td> 1-\u2713 </td><td> 29.0528.3329.77 </td><td> 5.185.075.18 </td><td> 73.4376.1076.68 </td></tr></table>", "caption": "Table 5: Performance comparison of Swin and DW-ViT on ImageNet-1K [10] under different window and module settings.", "list_citation_info": ["[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009."]}], "citation_info_to_title": {"[64] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi-Li Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R. Manmatha, Mu Li, and Alex Smola. Resnest: Split-attention networks. ArXiv, abs/2004.08955, 2020.": "Resnest: Split-attention networks", "[57] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418\u2013434, 2018.": "Unified perceptual parsing for scene understanding", "[18] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.": "Mask R-CNN", "[31] Hezheng Lin, Xing Cheng, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan Wang, Qing Song, and Wei Yuan. Cat: Cross attention in vision transformer. arXiv preprint arXiv:2106.05786, 2021.": "Cat: Cross attention in vision transformer", "[36] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. Dual attention networks for multimodal reasoning and matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 299\u2013307, 2017.": "Dual Attention Networks for Multimodal Reasoning and Matching", "[38] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428\u201310436, 2020.": "Designing network design spaces", "[61] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In European Conference on Computer Vision, pages 191\u2013207. Springer, 2020.": "Disentangled non-local neural networks", "[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154\u20136162, 2018.": "Cascade r-cnn: Delving into high quality object detection", "[26] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6399\u20136408, 2019.": "Panoptic feature pyramid networks", "[16] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021.": "Transformer in transformer", "[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. arXiv preprint arXiv:2103.14899, 2021.": "Crossvit: Cross-attention multi-scale vision transformer for image classification", "[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.": "Training Data-Efficient Image Transformers & Distillation Through Attention", "[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.": "Deep residual learning for image recognition", "[59] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. arXiv preprint arXiv:2104.06399, 2021.": "Co-scale conv-attentional image transformers", "[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[41] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5693\u20135703, 2019.": "Deep high-resolution representation learning for human pose estimation", "[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "[55] Wenxiao Wang, Lu Yao, Long Chen, Deng Cai, Xiaofei He, and Wei Liu. Crossformer: A versatile vision transformer based on cross-scale attention. arXiv preprint arXiv:2108.00154, 2021.": "Crossformer: A versatile vision transformer based on cross-scale attention", "[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.": "Imagenet: A large-scale hierarchical image database", "[62] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet", "[65] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. ArXiv, abs/2103.15358, 2021.": "Multi-scale vision longformer: A new vision transformer for high-resolution image encoding", "[58] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492\u20131500, 2017.": "Aggregated residual transformations for deep neural networks", "[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[8] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021.": "Conditional positional encodings for vision transformers", "[14] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive context network for scene parsing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6748\u20136757, 2019.": "Adaptive context network for scene parsing", "[66] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302\u2013321, 2018.": "Semantic understanding of scenes through the ade20k dataset", "[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "[63] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI 16, pages 173\u2013190. Springer, 2020.": "Object-contextual representations for semantic segmentation"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions": "2102.12122"}}