{"title": "HuMMan: Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling", "abstract": "4D human sensing and modeling are fundamental tasks in vision and graphics\nwith numerous applications. With the advances of new sensors and algorithms,\nthere is an increasing demand for more versatile datasets. In this work, we\ncontribute HuMMan, a large-scale multi-modal 4D human dataset with 1000 human\nsubjects, 400k sequences and 60M frames. HuMMan has several appealing\nproperties: 1) multi-modal data and annotations including color images, point\nclouds, keypoints, SMPL parameters, and textured meshes; 2) popular mobile\ndevice is included in the sensor suite; 3) a set of 500 actions, designed to\ncover fundamental movements; 4) multiple tasks such as action recognition, pose\nestimation, parametric human recovery, and textured mesh reconstruction are\nsupported and evaluated. Extensive experiments on HuMMan voice the need for\nfurther study on challenges such as fine-grained action recognition, dynamic\nhuman mesh reconstruction, point cloud-based parametric human recovery, and\ncross-device domain gaps.", "authors": ["Zhongang Cai", "Daxuan Ren", "Ailing Zeng", "Zhengyu Lin", "Tao Yu", "Wenjia Wang", "Xiangyu Fan", "Yang Gao", "Yifan Yu", "Liang Pan", "Fangzhou Hong", "Mingyuan Zhang", "Chen Change Loy", "Lei Yang", "Ziwei Liu"], "published_date": "2022_04_28", "pdf_url": "http://arxiv.org/pdf/2204.13686v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Dataset</th><td rowspan=\"2\">#Subj</td><td rowspan=\"2\">#Act</td><td rowspan=\"2\">#Seq</td><td rowspan=\"2\">#Frame</td><td rowspan=\"2\">Video</td><td rowspan=\"2\">Mobile</td><td colspan=\"8\">Modalities</td></tr><tr><td>RGB</td><td>D/PC</td><td>Act</td><td>K2D</td><td>K3D</td><td>Param</td><td>Mesh</td><td>Txtr</td></tr><tr><th>UCF101 [91]</th><td>-</td><td>101</td><td>13k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AVA [22]</th><td>-</td><td>80</td><td>437</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>FineGym [88]</th><td>-</td><td>530</td><td>32k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HAA500 [15]</th><td>-</td><td>500</td><td>10k</td><td>591k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SYSU 3DHOI [30]</th><td>40</td><td>12</td><td>480</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>NTU RGB+D [87]</th><td>40</td><td>60</td><td>56k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>NTU RGB+D 120 [58]</th><td>106</td><td>120</td><td>114k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>NTU RGB+D X [97]</th><td>106</td><td>120</td><td>113k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th>MPII [3]</th><td>-</td><td>410</td><td>-</td><td>24k</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>COCO [56]</th><td>-</td><td>-</td><td>-</td><td>104k</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PoseTrack [2]</th><td>-</td><td>-</td><td>&gt;1.35k</td><td>&gt;46k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Human3.6M [32]</th><td>11</td><td>17</td><td>839</td><td>3.6M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CMU Panoptic [38]</th><td>8</td><td>5</td><td>65</td><td>154M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MPI-INF-3DHP [68]</th><td>8</td><td>8</td><td>16</td><td>1.3M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>3DPW [99]</th><td>7</td><td>-</td><td>60</td><td>51k</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th>AMASS [65]</th><td>344</td><td>-</td><td>&gt;11k</td><td>&gt;16.88M</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th>AIST++ [52]</th><td>30</td><td>-</td><td>1.40k</td><td>10.1M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th>CAPE [63]</th><td>15</td><td>-</td><td>&gt;600</td><td>&gt;140k</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td></tr><tr><th>BUFF [113]</th><td>6</td><td>3</td><td>&gt;30</td><td>&gt;13.6k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>DFAUST [6]</th><td>10</td><td>&gt;10</td><td>&gt;100</td><td>&gt;40k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>HUMBI [109]</th><td>772</td><td>-</td><td>-</td><td>\\sim26M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>ZJU LightStage [82]</th><td>6</td><td>6</td><td>9</td><td>&gt;1k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>THuman2.0 [107]</th><td>200</td><td>-</td><td>-</td><td>&gt;500</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>HuMMan (ours)</th><td>1000</td><td>500</td><td>400k</td><td>60M</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody></table>", "caption": "Table 1: Comparisons of HuMMan with published datasets. HuMMan has a competitive scale in terms of the number of subjects (#Subj), actions (#Act), sequences (#Seq) and frames (#Frame). Moreover, HuMMan features multiple modalities and supports multiple tasks. Video: sequential data, not limited to RGB sequences; Mobile: mobile device in the sensor suite; D/PC: depth image or point cloud, only genuine point cloud collected from depth sensors are considered; Act: action label; K2D: 2D keypoints; K3D: 3D keypoints; Param: statistical model (e.g. SMPL) parameters; Txtr: texture. -: not applicable or not reported.", "list_citation_info": ["[88] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2616\u20132625, 2020.", "[58] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding. IEEE transactions on pattern analysis and machine intelligence, 42(10):2684\u20132701, 2019.", "[68] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 2017 international conference on 3D vision (3DV), pages 506\u2013516. IEEE, 2017.", "[63] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learning to dress 3d people in generative clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6469\u20136478, 2020.", "[22] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6047\u20136056, 2018.", "[38] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: A massively multiview system for social motion capture. In Proceedings of the IEEE International Conference on Computer Vision, pages 3334\u20133342, 2015.", "[30] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly learning heterogeneous features for rgb-d activity recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5344\u20135352, 2015.", "[32] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):1325\u20131339, 2013.", "[107] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021), June 2021.", "[82] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.", "[52] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13401\u201313412, 2021.", "[2] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5167\u20135176, 2018.", "[15] Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13465\u201313474, 2021.", "[3] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pages 3686\u20133693, 2014.", "[56] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.", "[91] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.", "[6] Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J Black. Dynamic faust: Registering human bodies in motion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6233\u20136242, 2017.", "[87] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1010\u20131019, 2016.", "[99] Timo von Marcard, Roberto Henschel, Michael J Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In Proceedings of the European Conference on Computer Vision (ECCV), pages 601\u2013617, 2018.", "[113] Chao Zhang, Sergi Pujades, Michael J Black, and Gerard Pons-Moll. Detailed, accurate, human shape estimation from clothed 3d scan sequences. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4191\u20134200, 2017.", "[109] Zhixuan Yu, J. S. Yoon, I. Lee, Prashanth Venkatesh, Jaesik Park, J. Yu, and H. Park. Humbi: A large multiview dataset of human body expressions. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2987\u20132997, 2020.", "[97] Neel Trivedi, Anirudh Thatipelli, and Ravi Kiran Sarvadevabhatla. Ntu-x: An enhanced large-scale dataset for improving pose-based recognition of subtle human actions. arXiv preprint arXiv:2101.11529, 2021.", "[65] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5442\u20135451, 2019."]}, {"table": "<table><thead><tr><th>Train</th><th>Test</th><th>MPJPE \\downarrow</th><th>PA \\downarrow</th></tr><tr><th colspan=\"4\">FCN [66]</th></tr></thead><tbody><tr><td>HuMMan</td><td>HuMMan</td><td>78.5</td><td>46.3</td></tr><tr><td>H36M</td><td>AIST++</td><td>133.9</td><td>73.1</td></tr><tr><td>HuMMan</td><td>AIST++</td><td>116.4</td><td>67.2</td></tr><tr><td colspan=\"4\">Video3D [80]</td></tr><tr><td>HuMMan</td><td>HuMMan</td><td>73.1</td><td>43.5</td></tr><tr><td>H36M</td><td>AIST++</td><td>128.5</td><td>72.0</td></tr><tr><td>HuMMan</td><td>AIST++</td><td>109.2</td><td>63.5</td></tr></tbody></table>", "caption": "Table 3: 3D Keypoint Detection. PA: PA-MPJPE", "list_citation_info": ["[80] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7753\u20137762, 2019.", "[66] Julieta Martinez, Rayat Hossain, Javier Romero, and James J Little. A simple yet effective baseline for 3d human pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2640\u20132649, 2017."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Dataset</th><td rowspan=\"2\">#Subj</td><td rowspan=\"2\">#Act</td><td rowspan=\"2\">#Seq</td><td rowspan=\"2\">#Frame</td><td rowspan=\"2\">Video</td><td rowspan=\"2\">Mobile</td><td colspan=\"8\">Modalities</td></tr><tr><td>RGB</td><td>D/PC</td><td>Act</td><td>K2D</td><td>K3D</td><td>Param</td><td>Mesh</td><td>Txtr</td></tr><tr><th colspan=\"15\">Action Recognition</th></tr><tr><th>HMDB51 [49]</th><td>-</td><td>51</td><td>7k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>UCF101 [91]</th><td>-</td><td>101</td><td>13k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Sports1M [42]</th><td>-</td><td>487</td><td>1M</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AVA [22]</th><td>-</td><td>80</td><td>437</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Kinectics 700 [11]</th><td>-</td><td>700</td><td>650k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HACS [117]</th><td>-</td><td>200</td><td>1.55M</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Moments-In-Time [71]</th><td>-</td><td>339</td><td>1M</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>FineGym [88]</th><td>-</td><td>530</td><td>32k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>HAA500 [15]</th><td>-</td><td>500</td><td>10k</td><td>591k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MSR-Action3D [53]</th><td>10</td><td>20</td><td>567</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Northwestern-UCLA [101]</th><td>10</td><td>10</td><td>1.47k</td><td>&gt;23k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SYSU 3DHOI [30]</th><td>40</td><td>12</td><td>65</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>NTU RGB+D [87]</th><td>40</td><td>60</td><td>56k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>NTU RGB+D 120 [58]</th><td>106</td><td>120</td><td>114k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>NTU RGB+D X [97]</th><td>106</td><td>120</td><td>113k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th colspan=\"15\">2D/3D Keypoint Detection and 3D Parametric Human Recovery</th></tr><tr><th>J-HMDB [35]</th><td>-</td><td>21</td><td>928</td><td>33.18k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Penn Action [114]</th><td>-</td><td>15</td><td>2.32k</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MPII [3]</th><td>-</td><td>410</td><td>-</td><td>24k</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>COCO [56]</th><td>-</td><td>-</td><td>-</td><td>104k</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PoseTrack [2]</th><td>-</td><td>-</td><td>&gt;1.35k</td><td>&gt;46k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Human3.6M [32]</th><td>11</td><td>17</td><td>839</td><td>3.6M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CMU Panoptic [38]</th><td>8</td><td>5</td><td>65</td><td>154M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MPI-INF-3DHP [68]</th><td>8</td><td>8</td><td>16</td><td>1.3M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>TotalCapture [98]</th><td>5</td><td>5</td><td>60</td><td>1.89M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>3DPW [99]</th><td>7</td><td>-</td><td>60</td><td>51k</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th>AMASS [65]</th><td>344</td><td>-</td><td>&gt;11k</td><td>&gt;16.88M</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th>Mirrored-Human [16]</th><td>-</td><td>56</td><td>56</td><td>&gt;1.5M</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th>AIST++ [52]</th><td>30</td><td>-</td><td>1.40k</td><td>10.1M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td></tr><tr><th colspan=\"15\">Mesh Reconstruction</th></tr><tr><th>ZJU LightStage [82]</th><td>6</td><td>6</td><td>9</td><td>&gt;1k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>CAPE [63]</th><td>15</td><td>-</td><td>&gt;600</td><td>&gt;140k</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td></tr><tr><th>BUFF [113]</th><td>6</td><td>3</td><td>&gt;30</td><td>&gt;13.6k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>DFAUST [6]</th><td>10</td><td>&gt;10</td><td>&gt;100</td><td>&gt;40k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>People Snapshot [1]</th><td>9</td><td>-</td><td>24</td><td>15k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>LiveCap [25]</th><td>7</td><td>11</td><td>11</td><td>36k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>DynaCap [24]</th><td>4</td><td>5</td><td>5</td><td>35k</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>DeepCap [26]</th><td>4</td><td>17</td><td>17</td><td>26k</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>\u2713</td><td>\u2713</td></tr><tr><th>HUMBI [109]</th><td>772</td><td>-</td><td>-</td><td>\\sim26M</td><td>\u2713</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>THuman [119]</th><td>200</td><td>-</td><td>-</td><td>&gt;6k</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>THuman2.0 [107]</th><td>200</td><td>-</td><td>-</td><td>&gt;500</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th colspan=\"15\">Multi-task</th></tr><tr><th>HuMMan (ours)</th><td>1000</td><td>500</td><td>400k</td><td>60M</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody></table><br/>", "caption": "Table 6: A more complete comparison of HuMMan with published datasets. Subj: subjects; Act: actions; Seq: sequences; Video: sequential data, not limited to RGB sequences; Mobile: mobile device in the sensor suite; D/PC: depth image or point cloud, only genuine point cloud collected from depth sensors are considered; Act: action label; K2D: 2D keypoints; K3D: 3D keypoints; Param: statistical model (e.g. SMPL) parameters; Txtr: texture. -: not applicable or not reported", "list_citation_info": ["[119] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. Deephuman: 3d human reconstruction from a single image. In The IEEE International Conference on Computer Vision (ICCV), October 2019.", "[16] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021.", "[88] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2616\u20132625, 2020.", "[114] Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In Proceedings of the IEEE International Conference on Computer Vision, pages 2248\u20132255, 2013.", "[58] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding. IEEE transactions on pattern analysis and machine intelligence, 42(10):2684\u20132701, 2019.", "[22] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6047\u20136056, 2018.", "[68] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 2017 international conference on 3D vision (3DV), pages 506\u2013516. IEEE, 2017.", "[63] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learning to dress 3d people in generative clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6469\u20136478, 2020.", "[26] Marc Habermann, Weipeng Xu, Michael Zollhofer, Gerard Pons-Moll, and Christian Theobalt. Deepcap: Monocular human performance capture using weak supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5052\u20135063, 2020.", "[38] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: A massively multiview system for social motion capture. In Proceedings of the IEEE International Conference on Computer Vision, pages 3334\u20133342, 2015.", "[98] Matthew Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, and John P Collomosse. Total capture: 3d human pose estimation fusing video and inertial sensors. In BMVC, volume 2, pages 1\u201313, 2017.", "[30] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly learning heterogeneous features for rgb-d activity recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5344\u20135352, 2015.", "[32] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):1325\u20131339, 2013.", "[101] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-Chun Zhu. Cross-view action modeling, learning and recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2649\u20132656, 2014.", "[11] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019.", "[107] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021), June 2021.", "[82] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.", "[52] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13401\u201313412, 2021.", "[24] Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Real-time deep dynamic characters. ACM Transactions on Graphics (TOG), 40(4):1\u201316, 2021.", "[2] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5167\u20135176, 2018.", "[3] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pages 3686\u20133693, 2014.", "[15] Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13465\u201313474, 2021.", "[56] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.", "[91] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.", "[1] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Video based reconstruction of 3d people models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8387\u20138397, 2018.", "[6] Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J Black. Dynamic faust: Registering human bodies in motion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6233\u20136242, 2017.", "[87] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1010\u20131019, 2016.", "[99] Timo von Marcard, Roberto Henschel, Michael J Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In Proceedings of the European Conference on Computer Vision (ECCV), pages 601\u2013617, 2018.", "[117] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8668\u20138678, 2019.", "[71] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE transactions on pattern analysis and machine intelligence, 42(2):502\u2013508, 2019.", "[113] Chao Zhang, Sergi Pujades, Michael J Black, and Gerard Pons-Moll. Detailed, accurate, human shape estimation from clothed 3d scan sequences. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4191\u20134200, 2017.", "[42] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732, 2014.", "[109] Zhixuan Yu, J. S. Yoon, I. Lee, Prashanth Venkatesh, Jaesik Park, J. Yu, and H. Park. Humbi: A large multiview dataset of human body expressions. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2987\u20132997, 2020.", "[49] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pages 2556\u20132563. IEEE, 2011.", "[97] Neel Trivedi, Anirudh Thatipelli, and Ravi Kiran Sarvadevabhatla. Ntu-x: An enhanced large-scale dataset for improving pose-based recognition of subtle human actions. arXiv preprint arXiv:2101.11529, 2021.", "[35] Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, and Michael J Black. Towards understanding action recognition. In Proceedings of the IEEE international conference on computer vision, pages 3192\u20133199, 2013.", "[25] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Livecap: Real-time human performance capture from monocular video. ACM Transactions On Graphics (TOG), 38(2):1\u201317, 2019.", "[65] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5442\u20135451, 2019.", "[53] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Action recognition based on a bag of 3d points. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops, pages 9\u201314. IEEE, 2010."]}, {"table": "<table><thead><tr><th>Method</th><th>AP{}^{50}\\uparrow</th><th>AP{}^{75}\\uparrow</th></tr></thead><tbody><tr><th>CPN [12]</th><td>0.86</td><td>0.93</td></tr><tr><th>HRNet [92]</th><td>0.91</td><td>0.97</td></tr><tr><th>Lite-HRNet [106]</th><td>0.87</td><td>0.93</td></tr></tbody></table>", "caption": "Table 7: 2D Keypoint Detection under Protocol 1. Input image is resized to 384\\times288", "list_citation_info": ["[92] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5693\u20135703, 2019.", "[106] Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang, Nong Sang, and Jingdong Wang. Lite-hrnet: A lightweight high-resolution network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10440\u201310450, 2021.", "[12] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gan g Yu, and Jian Sun. Cascaded pyramid network for multi-person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7103\u20137112, 2018."]}], "citation_info_to_title": {"[109] Zhixuan Yu, J. S. Yoon, I. Lee, Prashanth Venkatesh, Jaesik Park, J. Yu, and H. Park. Humbi: A large multiview dataset of human body expressions. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2987\u20132997, 2020.": "Humbi: A large multiview dataset of human body expressions", "[113] Chao Zhang, Sergi Pujades, Michael J Black, and Gerard Pons-Moll. Detailed, accurate, human shape estimation from clothed 3d scan sequences. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4191\u20134200, 2017.": "Detailed, accurate, human shape estimation from clothed 3d scan sequences", "[91] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.": "UCF101: A Dataset of 101 Human Actions Classes from Videos in the Wild", "[24] Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Real-time deep dynamic characters. ACM Transactions on Graphics (TOG), 40(4):1\u201316, 2021.": "Real-time deep dynamic characters", "[114] Weiyu Zhang, Menglong Zhu, and Konstantinos G Derpanis. From actemes to action: A strongly-supervised representation for detailed action understanding. In Proceedings of the IEEE International Conference on Computer Vision, pages 2248\u20132255, 2013.": "From actemes to action: A strongly-supervised representation for detailed action understanding", "[66] Julieta Martinez, Rayat Hossain, Javier Romero, and James J Little. A simple yet effective baseline for 3d human pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2640\u20132649, 2017.": "A simple yet effective baseline for 3d human pose estimation", "[58] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding. IEEE transactions on pattern analysis and machine intelligence, 42(10):2684\u20132701, 2019.": "NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding", "[3] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pages 3686\u20133693, 2014.": "2D Human Pose Estimation: New Benchmark and State of the Art Analysis", "[99] Timo von Marcard, Roberto Henschel, Michael J Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and a moving camera. In Proceedings of the European Conference on Computer Vision (ECCV), pages 601\u2013617, 2018.": "Recovering accurate 3d human pose in the wild using imus and a moving camera", "[42] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725\u20131732, 2014.": "Large-scale video classification with convolutional neural networks", "[53] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Action recognition based on a bag of 3d points. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops, pages 9\u201314. IEEE, 2010.": "Action recognition based on a bag of 3d points", "[56] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.": "Microsoft COCO: Common Objects in Context", "[68] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using improved cnn supervision. In 2017 international conference on 3D vision (3DV), pages 506\u2013516. IEEE, 2017.": "Monocular 3D Human Pose Estimation in the Wild Using Improved CNN Supervision", "[32] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):1325\u20131339, 2013.": "Human36m: Large scale datasets and predictive methods for 3d human sensing in natural environments", "[1] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Video based reconstruction of 3d people models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8387\u20138397, 2018.": "Video based reconstruction of 3d people models", "[22] Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6047\u20136056, 2018.": "Ava: A video dataset of spatio-temporally localized atomic visual actions", "[88] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2616\u20132625, 2020.": "Finegym: A hierarchical video dataset for fine-grained action understanding", "[6] Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J Black. Dynamic faust: Registering human bodies in motion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6233\u20136242, 2017.": "Dynamic Faust: Registering Human Bodies in Motion", "[65] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5442\u20135451, 2019.": "Amass: Archive of motion capture as surface shapes", "[12] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gan g Yu, and Jian Sun. Cascaded pyramid network for multi-person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7103\u20137112, 2018.": "Cascaded pyramid network for multi-person pose estimation", "[11] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019.": "A short note on the kinetics-700 human action dataset", "[15] Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13465\u201313474, 2021.": "Haa500: Human-centric atomic action dataset with curated videos", "[80] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7753\u20137762, 2019.": "3D Human Pose Estimation in Video with Temporal Convolutions and Semi-Supervised Training", "[2] Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estimation and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5167\u20135176, 2018.": "Posetrack: A benchmark for human pose estimation and tracking", "[38] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: A massively multiview system for social motion capture. In Proceedings of the IEEE International Conference on Computer Vision, pages 3334\u20133342, 2015.": "Panoptic studio: A massively multiview system for social motion capture", "[63] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J Black. Learning to dress 3d people in generative clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6469\u20136478, 2020.": "Learning to Dress 3D People in Generative Clothing", "[82] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In CVPR, 2021.": "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans", "[26] Marc Habermann, Weipeng Xu, Michael Zollhofer, Gerard Pons-Moll, and Christian Theobalt. Deepcap: Monocular human performance capture using weak supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5052\u20135063, 2020.": "Deepcap: Monocular human performance capture using weak supervision", "[119] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. Deephuman: 3d human reconstruction from a single image. In The IEEE International Conference on Computer Vision (ICCV), October 2019.": "Deephuman: 3D Human Reconstruction from a Single Image", "[52] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13401\u201313412, 2021.": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++", "[30] Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, and Jianguo Zhang. Jointly learning heterogeneous features for rgb-d activity recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5344\u20135352, 2015.": "Jointly learning heterogeneous features for RGB-D activity recognition", "[71] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. IEEE transactions on pattern analysis and machine intelligence, 42(2):502\u2013508, 2019.": "Moments in Time Dataset: One Million Videos for Event Understanding", "[49] Hildegard Kuehne, Hueihan Jhuang, Est\u00edbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pages 2556\u20132563. IEEE, 2011.": "HMDB: A Large Video Database for Human Motion Recognition", "[98] Matthew Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, and John P Collomosse. Total capture: 3d human pose estimation fusing video and inertial sensors. In BMVC, volume 2, pages 1\u201313, 2017.": "Total capture: 3D human pose estimation fusing video and inertial sensors", "[106] Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang, Nong Sang, and Jingdong Wang. Lite-hrnet: A lightweight high-resolution network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10440\u201310450, 2021.": "Lite-hrnet: A lightweight high-resolution network", "[97] Neel Trivedi, Anirudh Thatipelli, and Ravi Kiran Sarvadevabhatla. Ntu-x: An enhanced large-scale dataset for improving pose-based recognition of subtle human actions. arXiv preprint arXiv:2101.11529, 2021.": "NTU-X: An Enhanced Large-Scale Dataset for Improving Pose-Based Recognition of Subtle Human Actions", "[35] Hueihan Jhuang, Juergen Gall, Silvia Zuffi, Cordelia Schmid, and Michael J Black. Towards understanding action recognition. In Proceedings of the IEEE international conference on computer vision, pages 3192\u20133199, 2013.": "Towards understanding action recognition", "[101] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-Chun Zhu. Cross-view action modeling, learning and recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2649\u20132656, 2014.": "Cross-view action modeling, learning and recognition", "[117] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8668\u20138678, 2019.": "Hacs: Human action clips and segments dataset for recognition and temporal localization", "[87] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1010\u20131019, 2016.": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis", "[107] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR2021), June 2021.": "Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors", "[92] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5693\u20135703, 2019.": "Deep high-resolution representation learning for human pose estimation", "[16] Qi Fang, Qing Shuai, Junting Dong, Hujun Bao, and Xiaowei Zhou. Reconstructing 3d human pose by watching humans in the mirror. In CVPR, 2021.": "Reconstructing 3D Human Pose by Watching Humans in the Mirror", "[25] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Livecap: Real-time human performance capture from monocular video. ACM Transactions On Graphics (TOG), 38(2):1\u201317, 2019.": "Livecap: Real-time human performance capture from monocular video"}, "source_title_to_arxiv_id": {"Real-time deep dynamic characters": "2105.01794", "Haa500: Human-centric atomic action dataset with curated videos": "2009.05224", "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++": "2101.08779", "Lite-hrnet: A lightweight high-resolution network": "2104.06403", "Reconstructing 3D Human Pose by Watching Humans in the Mirror": "2104.00340"}}