{"title": "Efficient Image Captioning for Edge Devices", "abstract": "Recent years have witnessed the rapid progress of image captioning. However,\nthe demands for large memory storage and heavy computational burden prevent\nthese captioning models from being deployed on mobile devices. The main\nobstacles lie in the heavyweight visual feature extractors (i.e., object\ndetectors) and complicated cross-modal fusion networks. To this end, we propose\nLightCap, a lightweight image captioner for resource-limited devices. The core\ndesign is built on the recent CLIP model for efficient image captioning. To be\nspecific, on the one hand, we leverage the CLIP model to extract the compact\ngrid features without relying on the time-consuming object detectors. On the\nother hand, we transfer the image-text retrieval design of CLIP to image\ncaptioning scenarios by devising a novel visual concept extractor and a\ncross-modal modulator. We further optimize the cross-modal fusion model and\nparallel prediction heads via sequential and ensemble distillations. With the\ncarefully designed architecture, our model merely contains 40M parameters,\nsaving the model size by more than 75% and the FLOPs by more than 98% in\ncomparison with the current state-of-the-art methods. In spite of the low\ncapacity, our model still exhibits state-of-the-art performance on prevalent\ndatasets, e.g., 136.6 CIDEr on COCO Karpathy test split. Testing on the\nsmartphone with only a single CPU, the proposed LightCap exhibits a fast\ninference speed of 188ms per image, which is ready for practical applications.", "authors": ["Ning Wang", "Jiangrong Xie", "Hang Luo", "Qinglin Cheng", "Jihao Wu", "Mingbo Jia", "Linlin Li"], "published_date": "2022_12_18", "pdf_url": "http://arxiv.org/pdf/2212.08985v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"4\">Cross-Entropy</td><td colspan=\"4\">CIDEr Optimization</td></tr><tr><td>B@4</td><td>M</td><td>C</td><td>S</td><td>B@4</td><td>M</td><td>C</td><td>S</td></tr><tr><th>w/o Pre-training</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>BUTD (Anderson et al. 2018)</th><td>36.2</td><td>27.0</td><td>113.5</td><td>20.3</td><td>36.3</td><td>27.7</td><td>120.1</td><td>21.4</td></tr><tr><th>LBPF (Qin et al. 2019)</th><td>37.4</td><td>28.1</td><td>116.4</td><td>21.2</td><td>38.3</td><td>28.5</td><td>127.6</td><td>22.0</td></tr><tr><th>AoANet (Huang et al. 2019)</th><td>37.2</td><td>28.4</td><td>119.8</td><td>21.3</td><td>38.9</td><td>29.2</td><td>129.8</td><td>22.4</td></tr><tr><th>X-LAN (Pan et al. 2020)</th><td>38.2</td><td>28.8</td><td>122.0</td><td>21.9</td><td>39.5</td><td>29.5</td><td>132.0</td><td>23.4</td></tr><tr><th>RSTNet (Zhang et al. 2021b)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>39.3</td><td>29.4</td><td>133.3</td><td>23.0</td></tr><tr><th>DLCT (Luo et al. 2021)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>39.8</td><td>29.5</td><td>133.8</td><td>23.0</td></tr><tr><th>Normal model design</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>\\text{VLP}_{\\text{B}} (Zhou et al. 2020)</th><td>36.5</td><td>28.4</td><td>116.9</td><td>21.2</td><td>39.5</td><td>29.3</td><td>129.3</td><td>23.2</td></tr><tr><th>\\text{Oscar}_{\\text{B}} (Li et al. 2020b)</th><td>36.5</td><td>30.3</td><td>123.7</td><td>23.1</td><td>40.5</td><td>29.7</td><td>137.6</td><td>22.8</td></tr><tr><th>\\text{UNIMO}_{\\text{B}} (Li et al. 2020a)</th><td>38.8</td><td>-</td><td>124.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ViTCAP (Fang et al. 2021a)</th><td>36.3</td><td>29.3</td><td>125.2</td><td>22.6</td><td>41.2</td><td>30.1</td><td>138.1</td><td>24.1</td></tr><tr><th>\\text{VinVL}_{\\text{B}} (Zhang et al. 2021a)</th><td>38.2</td><td>30.3</td><td>129.3</td><td>23.6</td><td>40.9</td><td>30.9</td><td>140.4</td><td>25.1</td></tr><tr><th>\\text{LEMON}_{\\text{B}} (Hu et al. 2021a)</th><td>40.3</td><td>30.2</td><td>133.3</td><td>23.3</td><td>41.6</td><td>31.0</td><td>142.7</td><td>25.1</td></tr><tr><th>\\text{BLIP}_{\\text{B}} (Li et al. 2022)</th><td>39.7</td><td>-</td><td>133.3</td><td>23.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Light model design</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>E2E-VLP (Xu et al. 2021)</th><td>36.2</td><td>-</td><td>117.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MiniVLM (Wang et al. 2020a)</th><td>35.6</td><td>28.6</td><td>119.8</td><td>21.6</td><td>39.2</td><td>29.7</td><td>131.7</td><td>23.5</td></tr><tr><th>DistillVLM (Fang et al. 2021b)</th><td>35.6</td><td>28.7</td><td>120.8</td><td>22.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>LightCap (Ours)</th><td>37.4</td><td>29.9</td><td>125.8</td><td>22.6</td><td>40.1</td><td>29.9</td><td>136.6</td><td>24.2</td></tr></tbody></table>", "caption": "Table 5: Performance comparisons on the COCO Karpathy test split (Lin et al. 2014).", "list_citation_info": ["Qin et al. (2019) Qin, Y.; Du, J.; Zhang, Y.; and Lu, H. 2019. Look back and predict forward in image captioning. In CVPR.", "Pan et al. (2020) Pan, Y.; Yao, T.; Li, Y.; and Mei, T. 2020. X-linear attention networks for image captioning. In CVPR.", "Luo et al. (2021) Luo, Y.; Ji, J.; Sun, X.; Cao, L.; Wu, Y.; Huang, F.; Lin, C.-W.; and Ji, R. 2021. Dual-level collaborative transformer for image captioning. In AAAI.", "Zhang et al. (2021b) Zhang, X.; Sun, X.; Luo, Y.; Ji, J.; Zhou, Y.; Wu, Y.; Huang, F.; and Ji, R. 2021b. RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words. In CVPR.", "Huang et al. (2019) Huang, L.; Wang, W.; Chen, J.; and Wei, X.-Y. 2019. Attention on attention for image captioning. In ICCV.", "Hu et al. (2021a) Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2021a. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233.", "Wang et al. (2020a) Wang, J.; Hu, X.; Zhang, P.; Li, X.; Wang, L.; Zhang, L.; Gao, J.; and Liu, Z. 2020a. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946.", "Zhang et al. (2021a) Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021a. Vinvl: Revisiting visual representations in vision-language models. In CVPR.", "Zhou et al. (2020) Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; and Gao, J. 2020. Unified vision-language pre-training for image captioning and vqa. In AAAI.", "Li et al. (2020a) Li, W.; Gao, C.; Niu, G.; Xiao, X.; Liu, H.; Liu, J.; Wu, H.; and Wang, H. 2020a. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409.", "Fang et al. (2021b) Fang, Z.; Wang, J.; Hu, X.; Wang, L.; Yang, Y.; and Liu, Z. 2021b. Compressing visual-linguistic model via knowledge distillation. arXiv preprint arXiv:2104.02096.", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.", "Li et al. (2020b) Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020b. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV.", "Li et al. (2022) Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv preprint arXiv:2201.12086.", "Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV.", "Xu et al. (2021) Xu, H.; Yan, M.; Li, C.; Bi, B.; Huang, S.; Xiao, W.; and Huang, F. 2021. E2E-VLP: End-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804.", "Fang et al. (2021a) Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2021a. Injecting semantic concepts into end-to-end image captioning. arXiv preprint arXiv:2112.05230."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">  Method</th><td colspan=\"2\">Out-of-domain</td><td colspan=\"2\">Overall</td></tr><tr><td>C</td><td>S</td><td>C</td><td>S</td></tr><tr><th>  BUTD (Anderson et al. 2018)</th><td>31.3</td><td>8.3</td><td>55.3</td><td>10.1</td></tr><tr><th>  BUTD (Anderson et al. 2018) + CBS</th><td>66.4</td><td>9.7</td><td>73.1</td><td>11.1</td></tr><tr><th>  \\text{Oscar}_{\\text{B}} (Li et al. 2020b)</th><td>45.3</td><td>9.7</td><td>63.8</td><td>11.2</td></tr><tr><th>  \\text{Oscar}_{\\text{B}} (Li et al. 2020b) + CBS</th><td>77.6</td><td>10.6</td><td>81.1</td><td>11.7</td></tr><tr><th>  \\text{VIVO}_{\\text{B}} (Hu et al. 2021b)</th><td>71.1</td><td>10.6</td><td>81.5</td><td>12.2</td></tr><tr><th>  \\text{VIVO}_{\\text{B}} (Hu et al. 2021b) + CBS</th><td>87.5</td><td>11.5</td><td>88.3</td><td>12.4</td></tr><tr><th>  \\text{VinVL}_{\\text{B}} (Zhang et al. 2021a) + CBS</th><td>87.4</td><td>11.6</td><td>90.9</td><td>12.8</td></tr><tr><th>  ViTCAP (Fang et al. 2021a)</th><td>78.1</td><td>11.9</td><td>89.2</td><td>12.7</td></tr><tr><th>  ViTCAP (Fang et al. 2021a) + CBS</th><td>95.4</td><td>12.7</td><td>93.8</td><td>13.0</td></tr><tr><th>   \\text{SimVLM}_{\\text{B}} (Wang et al. 2021) (w/ pre-train)</th><td>-</td><td>-</td><td>94.8</td><td>13.1</td></tr><tr><th>  \\text{LEMON}_{\\text{B}} (Hu et al. 2021a)</th><td>62.6</td><td>10.6</td><td>79.0</td><td>12.3</td></tr><tr><th>   \\text{LEMON}_{\\text{B}} (Hu et al. 2021a) (w/ pre-train)</th><td>107.9</td><td>13.1</td><td>106.8</td><td>14.1</td></tr><tr><th>  \\text{BLIP}_{\\text{B}} (Li et al. 2022) (w/ pre-train)</th><td>111.5</td><td>14.2</td><td>109.6</td><td>14.7</td></tr><tr><th>  Human Performance</th><td>95.7</td><td>14.0</td><td>87.1</td><td>14.2</td></tr><tr><th>  LightCap (Ours)</th><td>76.5</td><td>11.2</td><td>85.1</td><td>12.3</td></tr><tr><th>  LightCap (Ours) + CBS</th><td>90.5</td><td>11.5</td><td>90.8</td><td>12.8</td></tr></tbody></table>", "caption": "Table 6: Performance comparisons on the nocaps validation split (Agrawal et al. 2019). We report the results of both without and with constrained beam search (CBS) decoding.", "list_citation_info": ["Hu et al. (2021a) Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2021a. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233.", "Zhang et al. (2021a) Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021a. Vinvl: Revisiting visual representations in vision-language models. In CVPR.", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.", "Li et al. (2020b) Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020b. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV.", "Hu et al. (2021b) Hu, X.; Yin, X.; Lin, K.; Wang, L.; Zhang, L.; Gao, J.; and Liu, Z. 2021b. Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training. In AAAI.", "Li et al. (2022) Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv preprint arXiv:2201.12086.", "Wang et al. (2021) Wang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.", "Agrawal et al. (2019) Agrawal, H.; Desai, K.; Wang, Y.; Chen, X.; Jain, R.; Johnson, M.; Batra, D.; Parikh, D.; Lee, S.; and Anderson, P. 2019. Nocaps: Novel object captioning at scale. In ICCV.", "Fang et al. (2021a) Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2021a. Injecting semantic concepts into end-to-end image captioning. arXiv preprint arXiv:2112.05230."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">  Method</th><td colspan=\"2\">In-domain</td><td colspan=\"2\">Near-domain</td><td colspan=\"2\">Out-of-domain</td><td colspan=\"2\">Overall</td></tr><tr><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td></tr><tr><th>  BUTD (Anderson et al. 2018)</th><td>78.1</td><td>11.6</td><td>57.7</td><td>10.3</td><td>31.3</td><td>8.3</td><td>55.3</td><td>10.1</td></tr><tr><th>  BUTD (Anderson et al. 2018) + CBS</th><td>80.0</td><td>12.0</td><td>73.6</td><td>11.3</td><td>66.4</td><td>9.7</td><td>73.1</td><td>11.1</td></tr><tr><th>  \\text{Oscar}_{\\text{B}} (Li et al. 2020b)</th><td>79.6</td><td>12.3</td><td>66.1</td><td>11.5</td><td>45.3</td><td>9.7</td><td>63.8</td><td>11.2</td></tr><tr><th>  \\text{Oscar}_{\\text{B}} (Li et al. 2020b) + CBS</th><td>83.4</td><td>12.0</td><td>81.6</td><td>12.0</td><td>77.6</td><td>10.6</td><td>81.1</td><td>11.7</td></tr><tr><th>  \\text{VIVO}_{\\text{B}} (Hu et al. 2021b)</th><td>88.8</td><td>12.9</td><td>83.2</td><td>12.6</td><td>71.1</td><td>10.6</td><td>81.5</td><td>12.2</td></tr><tr><th>  \\text{VIVO}_{\\text{B}} (Hu et al. 2021b) + CBS</th><td>92.2</td><td>12.9</td><td>87.8</td><td>12.6</td><td>87.5</td><td>11.5</td><td>88.3</td><td>12.4</td></tr><tr><th>  \\text{VinVL}_{\\text{B}} (Zhang et al. 2021a) + CBS</th><td>96.8</td><td>13.5</td><td>90.7</td><td>13.1</td><td>87.4</td><td>11.6</td><td>90.9</td><td>12.8</td></tr><tr><th>  ViTCAP (Fang et al. 2021a)</th><td>99.3</td><td>13.2</td><td>90.4</td><td>12.9</td><td>78.1</td><td>11.9</td><td>89.2</td><td>12.7</td></tr><tr><th>  ViTCAP (Fang et al. 2021a) + CBS</th><td>98.7</td><td>13.3</td><td>92.3</td><td>13.3</td><td>95.4</td><td>12.7</td><td>93.8</td><td>13.0</td></tr><tr><th>   \\text{SimVLM}_{\\text{B}} (Wang et al. 2021) (w/ pre-train)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.8</td><td>13.1</td></tr><tr><th>  \\text{LEMON}_{\\text{B}} (Hu et al. 2021a)</th><td>91.4</td><td>13.3</td><td>81.4</td><td>12.5</td><td>62.6</td><td>10.6</td><td>79.0</td><td>12.3</td></tr><tr><th>   \\text{LEMON}_{\\text{B}} (Hu et al. 2021a) (w/ pre-train)</th><td>107.7</td><td>14.7</td><td>106.2</td><td>14.3</td><td>107.9</td><td>13.1</td><td>106.8</td><td>14.1</td></tr><tr><th>  \\text{BLIP}_{\\text{B}} (Li et al. 2022) (w/ pre-train)</th><td>111.8</td><td>14.9</td><td>108.6</td><td>14.8</td><td>111.5</td><td>14.2</td><td>109.6</td><td>14.7</td></tr><tr><th>  Human Performance</th><td>84.4</td><td>14.3</td><td>85.0</td><td>14.3</td><td>95.7</td><td>14.0</td><td>87.1</td><td>14.2</td></tr><tr><th>  LightCap (Ours)</th><td>95.4</td><td>13.2</td><td>85.5</td><td>12.3</td><td>76.5</td><td>11.2</td><td>85.1</td><td>12.3</td></tr><tr><th>  LightCap (Ours) + CBS</th><td>95.8</td><td>13.4</td><td>88.7</td><td>12.8</td><td>90.5</td><td>11.5</td><td>90.8</td><td>12.8</td></tr></tbody></table>", "caption": "Table 8: Performance comparisons on the Nocaps validation split (Agrawal et al. 2019), where C and S denote CIDEr and SPICE scores. We compare our method with previous state-of-the-art approaches at \u201cin-domain\u201d, \u201cnear-domain\u201d, and \u201cout-of-domain\u201d. We report the results of both without and with constrained beam search (CBS) decoding.", "list_citation_info": ["Hu et al. (2021a) Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2021a. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233.", "Zhang et al. (2021a) Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021a. Vinvl: Revisiting visual representations in vision-language models. In CVPR.", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.", "Li et al. (2020b) Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020b. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV.", "Hu et al. (2021b) Hu, X.; Yin, X.; Lin, K.; Wang, L.; Zhang, L.; Gao, J.; and Liu, Z. 2021b. Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training. In AAAI.", "Li et al. (2022) Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv preprint arXiv:2201.12086.", "Wang et al. (2021) Wang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.", "Agrawal et al. (2019) Agrawal, H.; Desai, K.; Wang, Y.; Chen, X.; Jain, R.; Johnson, M.; Batra, D.; Parikh, D.; Lee, S.; and Anderson, P. 2019. Nocaps: Novel object captioning at scale. In ICCV.", "Fang et al. (2021a) Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2021a. Injecting semantic concepts into end-to-end image captioning. arXiv preprint arXiv:2112.05230."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">  Method</td><td colspan=\"2\">Model Architecture</td><td>Pre-training</td><td colspan=\"4\">Cross-Entropy</td></tr><tr><td>Image Encoder</td><td>Fusion Model</td><td>Data</td><td>B@4</td><td>M</td><td>C</td><td>S</td></tr><tr><td>  Normal model design</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>  \\text{VLP}_{\\text{B}} (Zhou et al. 2020)</td><td>\\text{F-RCNN}_{\\text{101}}</td><td>\\text{BERT}_{\\text{base}}</td><td>4M</td><td>36.5</td><td>28.4</td><td>116.9</td><td>21.2</td></tr><tr><td>  \\text{Oscar}_{\\text{B}} (Li et al. 2020b)</td><td>\\text{F-RCNN}_{\\text{101}}</td><td>\\text{BERT}_{\\text{base}}</td><td>7M</td><td>36.5</td><td>30.3</td><td>123.7</td><td>23.1</td></tr><tr><td>  \\text{UNIMO}_{\\text{B}} (Li et al. 2020a)</td><td>\\text{F-RCNN}_{\\text{101}}</td><td>\\text{BERT}_{\\text{base}}</td><td>9M</td><td>38.8</td><td>-</td><td>124.4</td><td>-</td></tr><tr><td>  ViTCAP (Fang et al. 2021a)</td><td>\\text{ViT}_{\\text{B}}</td><td>\\text{BERT}_{\\text{base}}</td><td>10M</td><td>36.3</td><td>29.3</td><td>125.2</td><td>22.6</td></tr><tr><td>  \\text{VinVL}_{\\text{B}} (Zhang et al. 2021a)</td><td>\\text{ResNeXt}_{\\text{152}}</td><td>\\text{BERT}_{\\text{base}}</td><td>9M</td><td>38.2</td><td>30.3</td><td>129.3</td><td>23.6</td></tr><tr><td>  \\text{LEMON}_{\\text{B}} (Hu et al. 2021a)</td><td>\\text{ResNeXt}_{\\text{152}}</td><td>\\text{BERT}_{\\text{base}}</td><td>200M</td><td>40.3</td><td>30.2</td><td>133.3</td><td>23.3</td></tr><tr><td>  \\text{BLIP}_{\\text{B}} (Li et al. 2022)</td><td>\\text{ViT}_{\\text{B}}</td><td>\\text{BERT}_{\\text{base}}</td><td>129M</td><td>39.7</td><td>-</td><td>133.3</td><td>23.3</td></tr><tr><td>  \\text{SimVLM}_{\\text{B}} (Wang et al. 2021)</td><td>ResNet&amp;\\text{ViT}_{\\text{B}}</td><td>Transformer</td><td>1.8B</td><td>39.0</td><td>32.9</td><td>134.8</td><td>24.0</td></tr><tr><td>  Light model design</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>  E2E-VLP (Xu et al. 2021)</td><td>\\text{ResNet}_{\\text{50}}</td><td>Transformer</td><td>6M</td><td>36.2</td><td>-</td><td>117.3</td><td>-</td></tr><tr><td>  MiniVLM (Wang et al. 2020a)</td><td>Eff-DET</td><td>MiniLM</td><td>14M</td><td>35.6</td><td>28.6</td><td>119.8</td><td>21.6</td></tr><tr><td>  DistillVLM (Fang et al. 2021b)</td><td>Eff-DET</td><td>MiniLM</td><td>7M</td><td>35.6</td><td>28.7</td><td>120.8</td><td>22.1</td></tr><tr><td>  LightCap (Ours)</td><td>\\text{ResNet}_{\\text{50}}</td><td>\\text{TinyBERT}_{\\text{4}}</td><td>6M</td><td>37.4</td><td>29.9</td><td>125.8</td><td>22.6</td></tr></tbody></table>", "caption": "Table 9:  Performance comparisons on the COCO-caption Karpathy test split (Lin et al. 2014), where B@4, M, C, S denote BLEU@4, METEOR, CIDEr, and SPICE scores.", "list_citation_info": ["Wang et al. (2020a) Wang, J.; Hu, X.; Zhang, P.; Li, X.; Wang, L.; Zhang, L.; Gao, J.; and Liu, Z. 2020a. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946.", "Hu et al. (2021a) Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2021a. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233.", "Zhang et al. (2021a) Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021a. Vinvl: Revisiting visual representations in vision-language models. In CVPR.", "Zhou et al. (2020) Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; and Gao, J. 2020. Unified vision-language pre-training for image captioning and vqa. In AAAI.", "Li et al. (2020a) Li, W.; Gao, C.; Niu, G.; Xiao, X.; Liu, H.; Liu, J.; Wu, H.; and Wang, H. 2020a. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409.", "Fang et al. (2021b) Fang, Z.; Wang, J.; Hu, X.; Wang, L.; Yang, Y.; and Liu, Z. 2021b. Compressing visual-linguistic model via knowledge distillation. arXiv preprint arXiv:2104.02096.", "Li et al. (2020b) Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020b. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV.", "Li et al. (2022) Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv preprint arXiv:2201.12086.", "Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV.", "Xu et al. (2021) Xu, H.; Yan, M.; Li, C.; Bi, B.; Huang, S.; Xiao, W.; and Huang, F. 2021. E2E-VLP: End-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804.", "Wang et al. (2021) Wang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.", "Fang et al. (2021a) Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2021a. Injecting semantic concepts into end-to-end image captioning. arXiv preprint arXiv:2112.05230."]}], "citation_info_to_title": {"Wang et al. (2021) Wang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.": "Simvlm: Simple Visual Language Model Pretraining with Weak Supervision", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.": "Bottom-up and top-down attention for image captioning and visual question answering", "Xu et al. (2021) Xu, H.; Yan, M.; Li, C.; Bi, B.; Huang, S.; Xiao, W.; and Huang, F. 2021. E2E-VLP: End-to-end vision-language pre-training enhanced by visual learning. arXiv preprint arXiv:2106.01804.": "E2E-VLP: End-to-end vision-language pre-training enhanced by visual learning", "Qin et al. (2019) Qin, Y.; Du, J.; Zhang, Y.; and Lu, H. 2019. Look back and predict forward in image captioning. In CVPR.": "Look back and predict forward in image captioning", "Zhang et al. (2021a) Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021a. Vinvl: Revisiting visual representations in vision-language models. In CVPR.": "Vinvl: Revisiting visual representations in vision-language models", "Hu et al. (2021a) Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2021a. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233.": "Scaling up vision-language pre-training for image captioning", "Huang et al. (2019) Huang, L.; Wang, W.; Chen, J.; and Wei, X.-Y. 2019. Attention on attention for image captioning. In ICCV.": "Attention on attention for image captioning", "Wang et al. (2020a) Wang, J.; Hu, X.; Zhang, P.; Li, X.; Wang, L.; Zhang, L.; Gao, J.; and Liu, Z. 2020a. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946.": "Minivlm: A smaller and faster vision-language model", "Fang et al. (2021a) Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2021a. Injecting semantic concepts into end-to-end image captioning. arXiv preprint arXiv:2112.05230.": "Injecting semantic concepts into end-to-end image captioning", "Hu et al. (2021b) Hu, X.; Yin, X.; Lin, K.; Wang, L.; Zhang, L.; Gao, J.; and Liu, Z. 2021b. Vivo: Surpassing human performance in novel object captioning with visual vocabulary pre-training. In AAAI.": "Vivo: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-training", "Li et al. (2020a) Li, W.; Gao, C.; Niu, G.; Xiao, X.; Liu, H.; Liu, J.; Wu, H.; and Wang, H. 2020a. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409.": "Unimo: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning", "Zhang et al. (2021b) Zhang, X.; Sun, X.; Luo, Y.; Ji, J.; Zhou, Y.; Wu, Y.; Huang, F.; and Ji, R. 2021b. RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words. In CVPR.": "RSTNet: Captioning with Adaptive Attention on Visual and Non-Visual Words", "Zhou et al. (2020) Zhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J.; and Gao, J. 2020. Unified vision-language pre-training for image captioning and vqa. In AAAI.": "Unified Vision-Language Pre-Training for Image Captioning and VQA", "Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV.": "Microsoft COCO: Common Objects in Context", "Li et al. (2022) Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv preprint arXiv:2201.12086.": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "Fang et al. (2021b) Fang, Z.; Wang, J.; Hu, X.; Wang, L.; Yang, Y.; and Liu, Z. 2021b. Compressing visual-linguistic model via knowledge distillation. arXiv preprint arXiv:2104.02096.": "Compressing visual-linguistic model via knowledge distillation", "Li et al. (2020b) Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020b. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV.": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "Pan et al. (2020) Pan, Y.; Yao, T.; Li, Y.; and Mei, T. 2020. X-linear attention networks for image captioning. In CVPR.": "X-linear attention networks for image captioning", "Agrawal et al. (2019) Agrawal, H.; Desai, K.; Wang, Y.; Chen, X.; Jain, R.; Johnson, M.; Batra, D.; Parikh, D.; Lee, S.; and Anderson, P. 2019. Nocaps: Novel object captioning at scale. In ICCV.": "Nocaps: Novel Object Captioning at Scale", "Luo et al. (2021) Luo, Y.; Ji, J.; Sun, X.; Cao, L.; Wu, Y.; Huang, F.; Lin, C.-W.; and Ji, R. 2021. Dual-level collaborative transformer for image captioning. In AAAI.": "Dual-level collaborative transformer for image captioning"}, "source_title_to_arxiv_id": {"Injecting semantic concepts into end-to-end image captioning": "2112.05230", "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation": "2201.12086", "Compressing visual-linguistic model via knowledge distillation": "2104.02096"}}