{"title": "Multi-scale Attention Network for Single Image Super-Resolution", "abstract": "By exploiting large kernel decomposition and attention mechanisms,\nconvolutional neural networks (CNN) can compete with transformer-based methods\nin many high-level computer vision tasks. However, due to the advantage of\nlong-range modeling, the transformers with self-attention still dominate the\nlow-level vision, including the super-resolution task. In this paper, we\npropose a CNN-based multi-scale attention network (MAN), which consists of\nmulti-scale large kernel attention (MLKA) and a gated spatial attention unit\n(GSAU), to improve the performance of convolutional SR networks. Within our\nMLKA, we rectify LKA with multi-scale and gate schemes to obtain the abundant\nattention map at various granularity levels, therefore jointly aggregating\nglobal and local information and avoiding the potential blocking artifacts. In\nGSAU, we integrate gate mechanism and spatial attention to remove the\nunnecessary linear layer and aggregate informative spatial context. To confirm\nthe effectiveness of our designs, we evaluate MAN with multiple complexities by\nsimply stacking different numbers of MLKA and GSAU. Experimental results\nillustrate that our MAN can achieve varied trade-offs between state-of-the-art\nperformance and computations. Code is available at\nhttps://github.com/icandle/MAN.", "authors": ["Yan Wang", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "published_date": "2022_09_28", "pdf_url": "http://arxiv.org/pdf/2209.14145v2", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">#Params</td><td rowspan=\"2\">#MAdds</td><td>Set5</td><td>Set14</td><td>B100</td><td>U100</td></tr><tr><td>PSNR</td><td>PSNR</td><td>PSNR</td><td>PSNR</td></tr><tr><td>MLP</td><td>854K</td><td>48.0G</td><td>32.31</td><td>28.73</td><td>27.65</td><td>26.26</td></tr><tr><td>SG</td><td>768K</td><td>43.1G</td><td>32.28</td><td>28.74</td><td>27.66</td><td>26.28</td></tr><tr><td>CFF</td><td>1140K</td><td>64.3G</td><td>32.35</td><td>28.76</td><td>27.67</td><td>26.34</td></tr><tr><td>GSAU</td><td>840K</td><td>47.1G</td><td>32.33</td><td>28.76</td><td>27.67</td><td>26.31</td></tr></table>", "caption": "Table 3: Ablation study on GSAU. The MLP [44], Simple-Gate (SG) [23], CFF [24], and GSAU are tested on MAN-light for \\times 4 SR.", "list_citation_info": ["[44] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in 9th International Conference on Learning Representations, Virtual Event, Austria, 2021.", "[23] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun, \u201cSimple baselines for image restoration,\u201d arXiv preprint arXiv:2204.04676, 2022.", "[24] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao, \u201cPVT v2: Improved baselines with pyramid vision transformer,\u201d Comput. Vis. Media, vol. 8, no. 3, pp. 415\u2013424, 2022."]}], "citation_info_to_title": {"[23] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun, \u201cSimple baselines for image restoration,\u201d arXiv preprint arXiv:2204.04676, 2022.": "Simple baselines for image restoration", "[44] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in 9th International Conference on Learning Representations, Virtual Event, Austria, 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[24] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao, \u201cPVT v2: Improved baselines with pyramid vision transformer,\u201d Comput. Vis. Media, vol. 8, no. 3, pp. 415\u2013424, 2022.": "PVT v2: Improved baselines with pyramid vision transformer"}, "source_title_to_arxiv_id": {"PVT v2: Improved baselines with pyramid vision transformer": "2106.13797"}}