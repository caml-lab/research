{"title": "Unifying Short and Long-Term Tracking with Graph Hierarchies", "abstract": "Tracking objects over long videos effectively means solving a spectrum of\nproblems, from short-term association for un-occluded objects to long-term\nassociation for objects that are occluded and then reappear in the scene.\nMethods tackling these two tasks are often disjoint and crafted for specific\nscenarios, and top-performing approaches are often a mix of techniques, which\nyields engineering-heavy solutions that lack generality. In this work, we\nquestion the need for hybrid approaches and introduce SUSHI, a unified and\nscalable multi-object tracker. Our approach processes long clips by splitting\nthem into a hierarchy of subclips, which enables high scalability. We leverage\ngraph neural networks to process all levels of the hierarchy, which makes our\nmodel unified across temporal scales and highly general. As a result, we obtain\nsignificant improvements over state-of-the-art on four diverse datasets. Our\ncode and models are available at bit.ly/sushi-mot.", "authors": ["Orcun Cetintas", "Guillem Bras\u00f3", "Laura Leal-Taix\u00e9"], "published_date": "2022_12_06", "pdf_url": "http://arxiv.org/pdf/2212.03038v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Det Ref.</td><td>IDF1 \\uparrow</td><td>HOTA \\uparrow</td><td>MOTA \\uparrow</td><td>ID Sw. \\downarrow</td></tr><tr><td colspan=\"6\">MOT17 - Public</td></tr><tr><td>Tracktor [3]</td><td>Tracktor</td><td>55.1</td><td>44.8</td><td>56.3</td><td>1987</td></tr><tr><td>LPT [27]</td><td>Tracktor</td><td>57.7</td><td>\u2013</td><td>57.3</td><td>1424</td></tr><tr><td>MPNTrack [5]</td><td>Tracktor</td><td>61.7</td><td>49.0</td><td>58.8</td><td>1185</td></tr><tr><td>Lif_T [17]</td><td>Tracktor</td><td>65.6</td><td>51.3</td><td>60.5</td><td>1189</td></tr><tr><td>ApLift [18]</td><td>Tracktor</td><td>65.6</td><td>51.1</td><td>60.5</td><td>1709</td></tr><tr><td>GMT [15]</td><td>Tracktor</td><td>65.9</td><td>51.2</td><td>60.2</td><td>1675</td></tr><tr><td>LPC_MOT [9]</td><td>Tracktor</td><td>66.8</td><td>51.5</td><td>59.0</td><td>1122</td></tr><tr><td>SUSHI (Ours)</td><td>Tracktor</td><td>68.6</td><td>53.0</td><td>62.2</td><td>1062</td></tr><tr><td colspan=\"6\">MOT17 - Private</td></tr><tr><td>QDTrack [36]</td><td>\u2717</td><td>66.3</td><td>53.9</td><td>68.7</td><td>3378</td></tr><tr><td>TrackFormer [32]</td><td>\u2717</td><td>68.0</td><td>57.3</td><td>74.1</td><td>2829</td></tr><tr><td>MOTR [75]</td><td>\u2717</td><td>68.6</td><td>57.8</td><td>73.4</td><td>2439</td></tr><tr><td>PermaTrack [55]</td><td>\u2717</td><td>68.9</td><td>55.5</td><td>73.8</td><td>3699</td></tr><tr><td>MeMOT [7]</td><td>\u2717</td><td>69.0</td><td>56.9</td><td>72.5</td><td>2724</td></tr><tr><td>GTR [81]</td><td>\u2717</td><td>71.5</td><td>59.1</td><td>75.3</td><td>2859</td></tr><tr><td>FairMOT [78]</td><td>\u2717</td><td>72.3</td><td>59.3</td><td>73.7</td><td>3303</td></tr><tr><td>GRTU [58]</td><td>\u2717</td><td>75.0</td><td>62.0</td><td>74.9</td><td>1812</td></tr><tr><td>CorrTracker [57]</td><td>\u2717</td><td>73.6</td><td>60.7</td><td>76.5</td><td>3369</td></tr><tr><td>Unicorn [70]</td><td>\u2717</td><td>75.5</td><td>61.7</td><td>77.2</td><td>5379</td></tr><tr><td>ByteTrack{}^{\\dagger} [77]</td><td>\u2717</td><td>77.1</td><td>62.8</td><td>78.9</td><td>2363</td></tr><tr><td>ByteTrack [77]</td><td>\u2717</td><td>77.3</td><td>63.1</td><td>80.3</td><td>2196</td></tr><tr><td>SUSHI (Ours)</td><td>\u2717</td><td>80.5</td><td>65.2</td><td>80.7</td><td>1335</td></tr></tbody></table>", "caption": "Table 2: Test set results on MOT17 benchmark. Det. Ref. denotes the public detection refinement strategy. As ByteTrack (gray) uses different thresholds for test set sequences and interpolation, we also report scores by disabling these as ByteTrack{}^{\\dagger} (black).", "list_citation_info": ["[3] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In ICCV, pages 941\u2013951, 2019.", "[17] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swoboda. Lifted disjoint paths with application in multiple object tracking. In IEEE Int. Conf. Mach. Learn., pages 4364\u20134375. PMLR, 2020.", "[70] Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, and Huchuan Lu. Towards grand unification of object tracking. In European Conference on Computer Vision, pages 733\u2013751. Springer, 2022.", "[57] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object tracking with correlation learning. In CVPR, pages 3876\u20133886, 2021.", "[27] Shuai Li, Yu Kong, and Hamid Rezatofighi. Learning of global objective for network flow in multi-object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8855\u20138865, 2022.", "[5] Guillem Braso and Laura Leal-Taixe. Learning a neural solver for multiple object tracking. In CVPR, 2020.", "[18] Andrea Hornakova, Timo Kaiser, Paul Swoboda, Michal Rolinek, Bodo Rosenhahn, and Roberto Henschel. Making higher order mot scalable: An efficient approximate solver for lifted disjoint paths. In ICCV, pages 6330\u20136340, October 2021.", "[36] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In CVPR, pages 164\u2013173, June 2021.", "[77] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021.", "[78] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. IJCV, 129(11):3069\u20133087, 2021.", "[15] Jiawei He, Zehao Huang, Naiyan Wang, and Zhaoxiang Zhang. Learnable graph matching: Incorporating graph partitioning with deep feature learning for multiple object tracking. In CVPR, pages 5299\u20135309, 2021.", "[9] Peng Dai, Renliang Weng, Wongun Choi, Changshui Zhang, Zhangping He, and Wei Ding. Learning a proposal classifier for multiple object tracking. In CVPR, pages 2443\u20132452, June 2021.", "[32] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.", "[75] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages 659\u2013675. Springer, 2022.", "[55] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In ICCV, pages 10860\u201310869, October 2021.", "[81] Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Global tracking transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8771\u20138780, 2022.", "[58] Shuai Wang, Hao Sheng, Yang Zhang, Yubin Wu, and Zhang Xiong. A general recurrent tracking framework without real data. In ICCV, pages 13219\u201313228, October 2021.", "[7] Jiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano Soatto. Memot: Multi-object tracking with memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8090\u20138100, 2022."]}, {"table": "<table><tbody><tr><td>Method</td><td>Det Ref.</td><td>IDF1 \\uparrow</td><td>HOTA \\uparrow</td><td>MOTA \\uparrow</td><td>ID Sw. \\downarrow</td></tr><tr><td colspan=\"6\">MOT20 - Public</td></tr><tr><td>Tracktor [3]</td><td>Tracktor</td><td>52.7</td><td>42.1</td><td>52.6</td><td>1648</td></tr><tr><td>LPT [27]</td><td>Tracktor</td><td>53.5</td><td>\u2013</td><td>57.9</td><td>1827</td></tr><tr><td>ApLift [18]</td><td>Tracktor</td><td>56.5</td><td>46.6</td><td>58.9</td><td>2241</td></tr><tr><td>MPNTrack [5]</td><td>Tracktor</td><td>59.1</td><td>46.8</td><td>57.6</td><td>1210</td></tr><tr><td>LPC_MOT [9]</td><td>Tracktor</td><td>62.5</td><td>49.0</td><td>56.3</td><td>1562</td></tr><tr><td>SUSHI (Ours)</td><td>Tracktor</td><td>68.2</td><td>53.4</td><td>61.6</td><td>1078</td></tr><tr><td colspan=\"6\">MOT20 - Private</td></tr><tr><td>TrackFormer [32]</td><td>\u2717</td><td>65.7</td><td>54.7</td><td>68.6</td><td>1532</td></tr><tr><td>MeMOT [7]</td><td>\u2717</td><td>66.1</td><td>54.1</td><td>63.7</td><td>1938</td></tr><tr><td>FairMOT [78]</td><td>\u2717</td><td>67.3</td><td>54.6</td><td>61.8</td><td>5243</td></tr><tr><td>GSDT [59]</td><td>\u2717</td><td>67.5</td><td>53.6</td><td>67.1</td><td>3131</td></tr><tr><td>CorrTracker [57]</td><td>\u2717</td><td>69.1</td><td>\u2013</td><td>65.2</td><td>5183</td></tr><tr><td>ByteTrack{}^{\\dagger} [77]</td><td>\u2717</td><td>74.5</td><td>60.4</td><td>74.2</td><td>925</td></tr><tr><td>ByteTrack [77]</td><td>\u2717</td><td>75.2</td><td>61.3</td><td>77.8</td><td>1223</td></tr><tr><td>SUSHI (Ours)</td><td>\u2717</td><td>77.7</td><td>63.1</td><td>74.3</td><td>704</td></tr></tbody></table>", "caption": "Table 3: Test set results on MOT20 benchmark. Det. Ref. denotes the public detection refinement strategy. As ByteTrack (gray) uses different thresholds for test set sequences and interpolation, we also report scores by disabling these as ByteTrack{}^{\\dagger} (black).", "list_citation_info": ["[3] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In ICCV, pages 941\u2013951, 2019.", "[57] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object tracking with correlation learning. In CVPR, pages 3876\u20133886, 2021.", "[27] Shuai Li, Yu Kong, and Hamid Rezatofighi. Learning of global objective for network flow in multi-object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8855\u20138865, 2022.", "[5] Guillem Braso and Laura Leal-Taixe. Learning a neural solver for multiple object tracking. In CVPR, 2020.", "[18] Andrea Hornakova, Timo Kaiser, Paul Swoboda, Michal Rolinek, Bodo Rosenhahn, and Roberto Henschel. Making higher order mot scalable: An efficient approximate solver for lifted disjoint paths. In ICCV, pages 6330\u20136340, October 2021.", "[59] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In IEEE Int. Conf. Robotics and Autom., pages 13708\u201313715. IEEE, 2021.", "[78] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. IJCV, 129(11):3069\u20133087, 2021.", "[9] Peng Dai, Renliang Weng, Wongun Choi, Changshui Zhang, Zhangping He, and Wei Ding. Learning a proposal classifier for multiple object tracking. In CVPR, pages 2443\u20132452, June 2021.", "[32] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.", "[77] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021.", "[7] Jiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano Soatto. Memot: Multi-object tracking with memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8090\u20138100, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>IDF1 \\uparrow</th><th>HOTA \\uparrow</th><th>MOTA \\uparrow</th><th>AssA \\uparrow</th><th>DetA \\uparrow</th></tr><tr><th colspan=\"6\">DanceTrack</th></tr></thead><tbody><tr><th>CenterTrack [80]</th><td>35.7</td><td>41.8</td><td>86.8</td><td>22.6</td><td>78.1</td></tr><tr><th>FairMOT [78]</th><td>40.8</td><td>39.7</td><td>82.2</td><td>23.8</td><td>66.7</td></tr><tr><th>TraDes [66]</th><td>41.2</td><td>43.3</td><td>86.2</td><td>25.4</td><td>74.5</td></tr><tr><th>GTR [81]</th><td>50.3</td><td>48.0</td><td>84.7</td><td>31.9</td><td>72.5</td></tr><tr><th>QDTrack [35]</th><td>50.4</td><td>54.2</td><td>87.7</td><td>36.8</td><td>80.1</td></tr><tr><th>MOTR [75]</th><td>51.5</td><td>54.2</td><td>79.7</td><td>40.2</td><td>73.5</td></tr><tr><th>ByteTrack [77]</th><td>53.9</td><td>47.7</td><td>89.6</td><td>32.1</td><td>71.0</td></tr><tr><th>SUSHI (Ours)</th><td>60.7</td><td>61.3</td><td>89.9</td><td>46.8</td><td>80.5</td></tr></tbody></table>", "caption": "Table 4: Test set results on DanceTrack benchmark.", "list_citation_info": ["[80] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In ECCV, pages 474\u2013490. Springer, 2020.", "[78] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. IJCV, 129(11):3069\u20133087, 2021.", "[35] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In CVPR, pages 164\u2013173, 2021.", "[75] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages 659\u2013675. Springer, 2022.", "[66] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In CVPR, 2021.", "[81] Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Global tracking transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8771\u20138780, 2022.", "[77] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021."]}, {"table": "<table><tbody><tr><td>Method</td><td>mIDF1 \\uparrow</td><td>mMOTA \\uparrow</td><td>IDF1 \\uparrow</td><td>MOTA \\uparrow</td><td>ID Sw. \\downarrow</td></tr><tr><td colspan=\"6\">BDD - Validation</td></tr><tr><td>MOTR [75]</td><td>43.5</td><td>32.0</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Yu et al. [72]</td><td>44.5</td><td>25.9</td><td>66.8</td><td>56.9</td><td>8315</td></tr><tr><td>QDTrack [35]</td><td>50.8</td><td>36.6</td><td>71.5</td><td>63.5</td><td>6262</td></tr><tr><td>TETer [26]</td><td>53.3</td><td>39.1</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>Unicorn [70]</td><td>54.0</td><td>41.2</td><td>71.3</td><td>66.6</td><td>10876</td></tr><tr><td>ByteTrack [77]</td><td>54.8</td><td>45.5</td><td>70.4</td><td>69.1</td><td>9140</td></tr><tr><td>SUSHI (Ours)</td><td>57.5</td><td>46.0</td><td>75.2</td><td>69.1</td><td>7283</td></tr><tr><td colspan=\"6\">BDD - Test</td></tr><tr><td>Yu et al. [72]</td><td>44.7</td><td>26.3</td><td>68.2</td><td>58.3</td><td>14674</td></tr><tr><td>QDTrack [35]</td><td>52.3</td><td>35.5</td><td>72.3</td><td>64.3</td><td>10790</td></tr><tr><td>TETer [26]</td><td>53.3</td><td>37.4</td><td>\u2013</td><td>\u2013</td><td>\u2013</td></tr><tr><td>ByteTrack [77]</td><td>55.8</td><td>40.1</td><td>71.3</td><td>69.6</td><td>15466</td></tr><tr><td>SUSHI (Ours)</td><td>58.9</td><td>40.8</td><td>75.9</td><td>69.7</td><td>12076</td></tr></tbody></table>", "caption": "Table 5: Validation and test set results on BDD MOT benchmark.", "list_citation_info": ["[72] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2636\u20132645, 2020.", "[70] Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, and Huchuan Lu. Towards grand unification of object tracking. In European Conference on Computer Vision, pages 733\u2013751. Springer, 2022.", "[35] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In CVPR, pages 164\u2013173, 2021.", "[75] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages 659\u2013675. Springer, 2022.", "[26] Siyuan Li, Martin Danelljan, Henghui Ding, Thomas E Huang, and Fisher Yu. Tracking every thing in the wild. In European Conference on Computer Vision, pages 498\u2013515. Springer, 2022.", "[77] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021."]}, {"table": "<table><thead><tr><th>ReID model</th><th>Training data</th><th>IDF1 \\uparrow</th><th>HOTA \\uparrow</th><th>MOTA \\uparrow</th></tr></thead><tbody><tr><th>ResNet50</th><th>[79, 28, 41]</th><td>76.7</td><td>66.1</td><td>68.7</td></tr><tr><th>ResNet50-IBN</th><th>[61]</th><td>76.0</td><td>65.8</td><td>68.3</td></tr></tbody></table>", "caption": "Table 8: Effect of using [5]\u2019s original reID model (ResNet50 trained on 3 datasets) vs ours (ResNet50-IBN trained on a single dataset).", "list_citation_info": ["[79] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1116\u20131124, 2015.", "[61] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain gap for person re-identification. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79\u201388, 2018.", "[5] Guillem Braso and Laura Leal-Taixe. Learning a neural solver for multiple object tracking. In CVPR, 2020."]}, {"table": "<table><tbody><tr><td>Method</td><td>Det Ref.</td><td>IDF1 \\uparrow</td><td>HOTA \\uparrow</td><td>MOTA \\uparrow</td><td>ID Sw. \\downarrow</td></tr><tr><td colspan=\"6\">MOT17 - Public</td></tr><tr><td>Tracktor [3]</td><td>Tracktor</td><td>55.1</td><td>44.8</td><td>56.3</td><td>1987</td></tr><tr><td>LPT [27]</td><td>Tracktor</td><td>57.7</td><td>\u2013</td><td>57.3</td><td>1424</td></tr><tr><td>MPNTrack [5]</td><td>Tracktor</td><td>61.7</td><td>49.0</td><td>58.8</td><td>1185</td></tr><tr><td>Lif_T [17]</td><td>Tracktor</td><td>65.6</td><td>51.3</td><td>60.5</td><td>1189</td></tr><tr><td>ApLift [18]</td><td>Tracktor</td><td>65.6</td><td>51.1</td><td>60.5</td><td>1709</td></tr><tr><td>GMT [15]</td><td>Tracktor</td><td>65.9</td><td>51.2</td><td>60.2</td><td>1675</td></tr><tr><td>LPC_MOT [9]</td><td>Tracktor</td><td>66.8</td><td>51.5</td><td>59.0</td><td>1122</td></tr><tr><td>SUSHI (Ours)</td><td>Tracktor</td><td>68.6</td><td>53.0</td><td>62.2</td><td>1062</td></tr><tr><td>CenterTrack [80]</td><td>CenterTrack</td><td>59.6</td><td>48.2</td><td>61.5</td><td>2583</td></tr><tr><td>TMOH [49]</td><td>TMOH</td><td>62.8</td><td>50.4</td><td>62.1</td><td>1897</td></tr><tr><td>ByteTrack [77]</td><td>ByteTrack</td><td>70.0</td><td>56.1</td><td>67.4</td><td>1331</td></tr><tr><td>SUSHI (Ours)</td><td>ByteTrack</td><td>73.4</td><td>58.5</td><td>68.4</td><td>1033</td></tr><tr><td colspan=\"6\">MOT20 - Public</td></tr><tr><td>Tracktor [3]</td><td>Tracktor</td><td>52.7</td><td>42.1</td><td>52.6</td><td>1648</td></tr><tr><td>LPT [27]</td><td>Tracktor</td><td>53.5</td><td>\u2013</td><td>57.9</td><td>1827</td></tr><tr><td>ApLift [18]</td><td>Tracktor</td><td>56.5</td><td>46.6</td><td>58.9</td><td>2241</td></tr><tr><td>MPNTrack [5]</td><td>Tracktor</td><td>59.1</td><td>46.8</td><td>57.6</td><td>1210</td></tr><tr><td>LPC_MOT [9]</td><td>Tracktor</td><td>62.5</td><td>49.0</td><td>56.3</td><td>1562</td></tr><tr><td>SUSHI (Ours)</td><td>Tracktor</td><td>68.2</td><td>53.4</td><td>61.6</td><td>1078</td></tr><tr><td>TMOH [49]</td><td>TMOH</td><td>61.2</td><td>48.9</td><td>60.1</td><td>2342</td></tr><tr><td>ByteTrack [77]</td><td>ByteTrack</td><td>70.2</td><td>56.4</td><td>67.0</td><td>680</td></tr><tr><td>SUSHI (Ours)</td><td>ByteTrack</td><td>73.3</td><td>58.4</td><td>67.2</td><td>601</td></tr></tbody></table>", "caption": "Table 9: Extended test set results on MOTChallenge under public setting with additional refinement strategies. Det. Ref. denotes the public detection refinement strategy.", "list_citation_info": ["[3] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In ICCV, pages 941\u2013951, 2019.", "[49] Daniel Stadler and Jurgen Beyerer. Improving multiple pedestrian tracking by track management and occlusion handling. In CVPR, pages 10958\u201310967, June 2021.", "[80] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In ECCV, pages 474\u2013490. Springer, 2020.", "[17] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swoboda. Lifted disjoint paths with application in multiple object tracking. In IEEE Int. Conf. Mach. Learn., pages 4364\u20134375. PMLR, 2020.", "[27] Shuai Li, Yu Kong, and Hamid Rezatofighi. Learning of global objective for network flow in multi-object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8855\u20138865, 2022.", "[5] Guillem Braso and Laura Leal-Taixe. Learning a neural solver for multiple object tracking. In CVPR, 2020.", "[18] Andrea Hornakova, Timo Kaiser, Paul Swoboda, Michal Rolinek, Bodo Rosenhahn, and Roberto Henschel. Making higher order mot scalable: An efficient approximate solver for lifted disjoint paths. In ICCV, pages 6330\u20136340, October 2021.", "[15] Jiawei He, Zehao Huang, Naiyan Wang, and Zhaoxiang Zhang. Learnable graph matching: Incorporating graph partitioning with deep feature learning for multiple object tracking. In CVPR, pages 5299\u20135309, 2021.", "[9] Peng Dai, Renliang Weng, Wongun Choi, Changshui Zhang, Zhangping He, and Wei Ding. Learning a proposal classifier for multiple object tracking. In CVPR, pages 2443\u20132452, June 2021.", "[77] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021."]}], "citation_info_to_title": {"[72] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2636\u20132645, 2020.": "Bdd100k: A diverse driving dataset for heterogeneous multitask learning", "[80] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In ECCV, pages 474\u2013490. Springer, 2020.": "Tracking Objects as Points", "[9] Peng Dai, Renliang Weng, Wongun Choi, Changshui Zhang, Zhangping He, and Wei Ding. Learning a proposal classifier for multiple object tracking. In CVPR, pages 2443\u20132452, June 2021.": "Learning a proposal classifier for multiple object tracking", "[79] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification: A benchmark. 2015 IEEE International Conference on Computer Vision (ICCV), pages 1116\u20131124, 2015.": "Scalable person re-identification: A benchmark", "[49] Daniel Stadler and Jurgen Beyerer. Improving multiple pedestrian tracking by track management and occlusion handling. In CVPR, pages 10958\u201310967, June 2021.": "Improving multiple pedestrian tracking by track management and occlusion handling", "[81] Xingyi Zhou, Tianwei Yin, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Global tracking transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8771\u20138780, 2022.": "Global tracking transformers", "[3] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In ICCV, pages 941\u2013951, 2019.": "Tracking without bells and whistles", "[27] Shuai Li, Yu Kong, and Hamid Rezatofighi. Learning of global objective for network flow in multi-object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8855\u20138865, 2022.": "Learning of Global Objective for Network Flow in Multi-Object Tracking", "[15] Jiawei He, Zehao Huang, Naiyan Wang, and Zhaoxiang Zhang. Learnable graph matching: Incorporating graph partitioning with deep feature learning for multiple object tracking. In CVPR, pages 5299\u20135309, 2021.": "Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking", "[5] Guillem Braso and Laura Leal-Taixe. Learning a neural solver for multiple object tracking. In CVPR, 2020.": "Learning a neural solver for multiple object tracking", "[75] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. In European Conference on Computer Vision, pages 659\u2013675. Springer, 2022.": "Motr: End-to-end multiple-object tracking with transformer", "[17] Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, and Paul Swoboda. Lifted disjoint paths with application in multiple object tracking. In IEEE Int. Conf. Mach. Learn., pages 4364\u20134375. PMLR, 2020.": "Lifted disjoint paths with application in multiple object tracking", "[7] Jiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano Soatto. Memot: Multi-object tracking with memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8090\u20138100, 2022.": "Memot: Multi-object tracking with memory", "[36] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In CVPR, pages 164\u2013173, June 2021.": "Quasi-dense similarity learning for multiple object tracking", "[70] Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, and Huchuan Lu. Towards grand unification of object tracking. In European Conference on Computer Vision, pages 733\u2013751. Springer, 2022.": "Towards grand unification of object tracking", "[61] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Person transfer gan to bridge domain gap for person re-identification. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79\u201388, 2018.": "Person Transfer GAN to Bridge Domain Gap for Person Re-Identification", "[18] Andrea Hornakova, Timo Kaiser, Paul Swoboda, Michal Rolinek, Bodo Rosenhahn, and Roberto Henschel. Making higher order mot scalable: An efficient approximate solver for lifted disjoint paths. In ICCV, pages 6330\u20136340, October 2021.": "Making Higher Order MOT Scalable: An Efficient Approximate Solver for Lifted Disjoint Paths", "[55] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In ICCV, pages 10860\u201310869, October 2021.": "Learning to track with object permanence", "[77] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. arXiv preprint arXiv:2110.06864, 2021.": "Bytetrack: Multi-object tracking by associating every detection box", "[57] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object tracking with correlation learning. In CVPR, pages 3876\u20133886, 2021.": "Multiple Object Tracking with Correlation Learning", "[26] Siyuan Li, Martin Danelljan, Henghui Ding, Thomas E Huang, and Fisher Yu. Tracking every thing in the wild. In European Conference on Computer Vision, pages 498\u2013515. Springer, 2022.": "Tracking every thing in the wild", "[32] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. Trackformer: Multi-object tracking with transformers. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.": "Trackformer: Multi-object tracking with transformers", "[59] Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. In IEEE Int. Conf. Robotics and Autom., pages 13708\u201313715. IEEE, 2021.": "Joint Object Detection and Multi-Object Tracking with Graph Neural Networks", "[58] Shuai Wang, Hao Sheng, Yang Zhang, Yubin Wu, and Zhang Xiong. A general recurrent tracking framework without real data. In ICCV, pages 13219\u201313228, October 2021.": "A general recurrent tracking framework without real data", "[78] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. IJCV, 129(11):3069\u20133087, 2021.": "Fairmot: On the fairness of detection and re-identification in multiple object tracking", "[35] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In CVPR, pages 164\u2013173, 2021.": "Quasi-dense similarity learning for multiple object tracking", "[66] Jialian Wu, Jiale Cao, Liangchen Song, Yu Wang, Ming Yang, and Junsong Yuan. Track to detect and segment: An online multi-object tracker. In CVPR, 2021.": "Track to detect and segment: An online multi-object tracker"}, "source_title_to_arxiv_id": {"Learning of Global Objective for Network Flow in Multi-Object Tracking": "2203.16210", "Learnable Graph Matching: Incorporating Graph Partitioning with Deep Feature Learning for Multiple Object Tracking": "2103.16178", "Memot: Multi-object tracking with memory": "2203.16761", "Trackformer: Multi-object tracking with transformers": "2101.02702"}}