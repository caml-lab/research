{"title": "LightViT: Towards Light-Weight Convolution-Free Vision Transformers", "abstract": "Vision transformers (ViTs) are usually considered to be less light-weight\nthan convolutional neural networks (CNNs) due to the lack of inductive bias.\nRecent works thus resort to convolutions as a plug-and-play module and embed\nthem in various ViT counterparts. In this paper, we argue that the\nconvolutional kernels perform information aggregation to connect all tokens;\nhowever, they would be actually unnecessary for light-weight ViTs if this\nexplicit aggregation could function in a more homogeneous way. Inspired by\nthis, we present LightViT as a new family of light-weight ViTs to achieve\nbetter accuracy-efficiency balance upon the pure transformer blocks without\nconvolution. Concretely, we introduce a global yet efficient aggregation scheme\ninto both self-attention and feed-forward network (FFN) of ViTs, where\nadditional learnable tokens are introduced to capture global dependencies; and\nbi-dimensional channel and spatial attentions are imposed over token\nembeddings. Experiments show that our model achieves significant improvements\non image classification, object detection, and semantic segmentation tasks. For\nexample, our LightViT-T achieves 78.7% accuracy on ImageNet with only 0.7G\nFLOPs, outperforming PVTv2-B0 by 8.2% while 11% faster on GPU. Code is\navailable at https://github.com/hunto/LightViT.", "authors": ["Tao Huang", "Lang Huang", "Shan You", "Fei Wang", "Chen Qian", "Chang Xu"], "published_date": "2022_07_12", "pdf_url": "http://arxiv.org/pdf/2207.05557v1", "list_table_and_caption": [{"table": "<table><tr><td>Config</td><td>Value</td></tr><tr><td>Batch size</td><td>1024</td></tr><tr><td>Optimizer</td><td>AdamW</td></tr><tr><td>Weight decay</td><td>0.04</td></tr><tr><td>LR decay</td><td>cosine</td></tr><tr><td>Base LR</td><td>1e-3</td></tr><tr><td>Minimum LR</td><td>1e-6</td></tr><tr><td>Warmup LR</td><td>1e-7</td></tr><tr><td>Warmup epochs</td><td>20</td></tr><tr><td>Training epochs</td><td>300</td></tr><tr><td>Augmentation</td><td>RandAug (2, 9) [4]</td></tr><tr><td>Color jitter</td><td>0.3</td></tr><tr><td>Mixup alpha</td><td>0.2</td></tr><tr><td>Cutmix alpha</td><td>1.0</td></tr><tr><td>Erasing prob.</td><td>0.25</td></tr><tr><td>Drop path rate</td><td>0.1 (T, S), 0.3 (B)</td></tr></table>", "caption": "Table 2: Training settings on ImageNet dataset.", "list_citation_info": ["[4] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702\u2013703, 2020."]}, {"table": "<table><tbody><tr><td>Model</td><th> Blocktype </th><th> Params(M) </th><th> FLOPs(G) </th><th> Throughput(image/s) </th><th> Top-1(%) </th></tr><tr><td>RegNetY-800M [22]</td><td>CNN</td><td>6.3</td><td>0.8</td><td>3321</td><td>76.3</td></tr><tr><td>PVTv2-B0 [31]</td><td>Hybrid</td><td>3.4</td><td>0.6</td><td>2324</td><td>70.5</td></tr><tr><td>SimViT-Micro [15]</td><td>Hybrid</td><td>3.3</td><td>0.7</td><td>1004</td><td>71.1</td></tr><tr><td>MobileViT-XS [21]</td><td>Hybrid</td><td>2.3</td><td>0.7</td><td>1581</td><td>74.8</td></tr><tr><td>LVT [36]</td><td>Hybrid</td><td>5.5</td><td>0.9</td><td>1545</td><td>74.8</td></tr><tr><td>LightViT-T</td><td>Transformer</td><td>9.4</td><td>0.7</td><td>2578</td><td>78.7</td></tr><tr><td>RegNetY-1.6G [22]</td><td>CNN</td><td>11.2</td><td>1.6</td><td>1845</td><td>78.0</td></tr><tr><td>MobileViT-S [21]</td><td>Hybrid</td><td>5.6</td><td>1.1</td><td>1219</td><td>78.4</td></tr><tr><td>PVTv2-B1 [31]</td><td>Hybrid</td><td>13.1</td><td>2.1</td><td>1231</td><td>78.7</td></tr><tr><td>ResT-Small [41]</td><td>Hybrid</td><td>13.7</td><td>1.9</td><td>1298</td><td>79.6</td></tr><tr><td>DeiT-Ti [29]</td><td>Transformer</td><td>5.7</td><td>1.3</td><td>2612</td><td>72.2</td></tr><tr><td>LightViT-S</td><td>Transformer</td><td>19.2</td><td>1.7</td><td>1467</td><td>80.8</td></tr><tr><td>RegNetY-4G{}^{\\dagger} [22]</td><td>CNN</td><td>21.0</td><td>4.0</td><td>1045</td><td>80.0</td></tr><tr><td>Twins-PCPVT-S [3]</td><td>Hybrid</td><td>24.1</td><td>3.8</td><td>807</td><td>81.2</td></tr><tr><td>ResT-Base [41]</td><td>Hybrid</td><td>30.3</td><td>4.3</td><td>735</td><td>81.6</td></tr><tr><td>PVTv2-B2 [31]</td><td>Hybrid</td><td>25.4</td><td>4.0</td><td>695</td><td>82.0</td></tr><tr><td>DeiT-S [29]</td><td>Transformer</td><td>22</td><td>4.6</td><td>961</td><td>79.8</td></tr><tr><td>Swin-T [20]</td><td>Transformer</td><td>29</td><td>4.9</td><td>765</td><td>81.3</td></tr><tr><td>LightViT-B</td><td>Transformer</td><td>35.2</td><td>3.9</td><td>827</td><td>82.1</td></tr></tbody></table>", "caption": "Table 3: Image classification performance on ImageNet validation dataset. Throughput is measured on a single V100 GPU following [29, 20]. Hybrid denotes using both attention and convolution in blocks. All models are trained and evaluated on 224\\times 224 resolution. \\dagger: accuracy reported by DeiT [29].", "list_citation_info": ["[15] G. Li, D. Xu, X. Cheng, L. Si, and C. Zheng. Simvit: Exploring a simple vision transformer with sliding windows, 2021.", "[22] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428\u201310436, 2020.", "[41] Q. Zhang and Y.-B. Yang. Rest: An efficient transformer for visual recognition. Advances in Neural Information Processing Systems, 34, 2021.", "[20] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.", "[21] S. Mehta and M. Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178, 2021.", "[29] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.", "[36] C. Yang, Y. Wang, J. Zhang, H. Zhang, Z. Wei, Z. Lin, and A. Yuille. Lite vision transformer with enhanced self-attention. arXiv preprint arXiv:2112.10809, 2021.", "[3] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34, 2021.", "[31] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, pages 1\u201310, 2022."]}, {"table": "<table><tr><td rowspan=\"2\">Backbone</td><td>Params</td><td>FLOPs</td><td colspan=\"6\">Mask R-CNN 1x schedule</td><td colspan=\"6\">Mask R-CNN 3x + MS schedule</td></tr><tr><td>(M)</td><td>(G)</td><td>AP{}^{b}</td><td>AP{}_{50}^{b}</td><td>AP{}_{75}^{b}</td><td>AP{}^{m}</td><td>AP{}_{50}^{m}</td><td>AP{}_{75}^{m}</td><td>AP{}^{b}</td><td>AP{}_{50}^{b}</td><td>AP{}_{75}^{b}</td><td>AP{}^{m}</td><td>AP{}_{50}^{m}</td><td>AP{}_{75}^{m}</td></tr><tr><td>ResNet-18 [8]</td><td>31</td><td>207</td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td><td>36.9</td><td>57.1</td><td>40.0</td><td>33.6</td><td>53.9</td><td>35.7</td></tr><tr><td>ResNet-50 [8]</td><td>44</td><td>260</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><td>ResNet-101 [8]</td><td>101</td><td>493</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td><td>42.8</td><td>63.2</td><td>47.1</td><td>38.5</td><td>60.1</td><td>41.3</td></tr><tr><td>PVT-T [30]</td><td>33</td><td>208</td><td>36.7</td><td>59.2</td><td>39.3</td><td>35.1</td><td>56.7</td><td>37.3</td><td>39.8</td><td>62.2</td><td>43.0</td><td>37.4</td><td>59.3</td><td>39.9</td></tr><tr><td>PVT-S [30]</td><td>44</td><td>245</td><td>40.4</td><td>62.9</td><td>43.8</td><td>37.8</td><td>60.1</td><td>40.3</td><td>43.0</td><td>65.3</td><td>46.9</td><td>39.9</td><td>62.5</td><td>42.8</td></tr><tr><td>PVT-M [30]</td><td>64</td><td>302</td><td>42.0</td><td>64.4</td><td>45.6</td><td>39.0</td><td>61.6</td><td>42.1</td><td>44.2</td><td>66.0</td><td>48.2</td><td>40.5</td><td>63.1</td><td>43.5</td></tr><tr><td>LightViT-T</td><td>28</td><td>187</td><td>37.8</td><td>60.7</td><td>40.4</td><td>35.9</td><td>57.8</td><td>38.0</td><td>41.5</td><td>64.4</td><td>45.1</td><td>38.4</td><td>61.2</td><td>40.8</td></tr><tr><td>LightViT-S</td><td>38</td><td>204</td><td>40.0</td><td>62.9</td><td>42.6</td><td>37.4</td><td>60.0</td><td>39.3</td><td>43.2</td><td>66.0</td><td>47.4</td><td>39.9</td><td>63.0</td><td>42.7</td></tr><tr><td>LightViT-B</td><td>54</td><td>240</td><td>41.7</td><td>64.5</td><td>45.1</td><td>38.8</td><td>61.4</td><td>41.4</td><td>45.0</td><td>67.9</td><td>48.8</td><td>41.2</td><td>64.8</td><td>44.2</td></tr></table>", "caption": "Table 4: Object detection and instance segmentation performance on COCO val2017. The FLOPs are measured on 800\\times 1280. All the models are pretrained on ImageNet-1K.", "list_citation_info": ["[30] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 568\u2013578, 2021.", "[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016."]}], "citation_info_to_title": {"[31] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, pages 1\u201310, 2022.": "Pvt v2: Improved baselines with pyramid vision transformer", "[30] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 568\u2013578, 2021.": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions", "[36] C. Yang, Y. Wang, J. Zhang, H. Zhang, Z. Wei, Z. Lin, and A. Yuille. Lite vision transformer with enhanced self-attention. arXiv preprint arXiv:2112.10809, 2021.": "Lite vision transformer with enhanced self-attention", "[22] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428\u201310436, 2020.": "Designing network design spaces", "[15] G. Li, D. Xu, X. Cheng, L. Si, and C. Zheng. Simvit: Exploring a simple vision transformer with sliding windows, 2021.": "Simvit: Exploring a simple vision transformer with sliding windows", "[29] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.": "Training data-efficient image transformers & distillation through attention", "[3] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34, 2021.": "Twins: Revisiting the design of spatial attention in vision transformers", "[21] S. Mehta and M. Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178, 2021.": "Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer", "[4] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702\u2013703, 2020.": "Randaugment: Practical automated data augmentation with a reduced search space", "[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.": "Deep residual learning for image recognition", "[20] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[41] Q. Zhang and Y.-B. Yang. Rest: An efficient transformer for visual recognition. Advances in Neural Information Processing Systems, 34, 2021.": "Rest: An efficient transformer for visual recognition"}, "source_title_to_arxiv_id": {"Pvt v2: Improved baselines with pyramid vision transformer": "2106.13797", "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions": "2102.12122", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Rest: An efficient transformer for visual recognition": "2105.13677"}}