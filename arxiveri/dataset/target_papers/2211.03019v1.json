{"title": "Hear The Flow: Optical Flow-Based Self-Supervised Visual Sound Source Localization", "abstract": "Learning to localize the sound source in videos without explicit annotations\nis a novel area of audio-visual research. Existing work in this area focuses on\ncreating attention maps to capture the correlation between the two modalities\nto localize the source of the sound. In a video, oftentimes, the objects\nexhibiting movement are the ones generating the sound. In this work, we capture\nthis characteristic by modeling the optical flow in a video as a prior to\nbetter aid in localizing the sound source. We further demonstrate that the\naddition of flow-based attention substantially improves visual sound source\nlocalization. Finally, we benchmark our method on standard sound source\nlocalization datasets and achieve state-of-the-art performance on the Soundnet\nFlickr and VGG Sound Source datasets. Code:\nhttps://github.com/denfed/heartheflow.", "authors": ["Dennis Fedorishin", "Deen Dayal Mohan", "Bhavin Jawade", "Srirangaraj Setlur", "Venu Govindaraju"], "published_date": "2022_11_06", "pdf_url": "http://arxiv.org/pdf/2211.03019v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>Training Set</td><td>cIoU{}_{0.5}</td><td>AUC{}_{cIoU}</td></tr><tr><th>Attention [28]</th><td rowspan=\"6\">Flickr 10k</td><td>0.436</td><td>0.449</td></tr><tr><th>CoarseToFine [25]</th><td>0.522</td><td>0.496</td></tr><tr><th>AVObject [1]</th><td>0.546</td><td>0.504</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.730</td><td>0.578</td></tr><tr><th>SSPL [30]</th><td>0.743</td><td>0.587</td></tr><tr><th>HTF (Ours)</th><td>0.860</td><td>0.634</td></tr><tr><th>Attention [28]</th><td rowspan=\"7\">Flickr 144k</td><td>0.660</td><td>0.558</td></tr><tr><th>DMC [19]</th><td>0.671</td><td>0.568</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.702</td><td>0.588</td></tr><tr><th>LVS{}^{\\dagger} [6]</th><td>0.697</td><td>0.560</td></tr><tr><th>HardPos [29]</th><td>0.762</td><td>0.597</td></tr><tr><th>SSPL [30]</th><td>0.759</td><td>0.610</td></tr><tr><th>HTF (Ours)</th><td>0.865</td><td>0.639</td></tr><tr><th>LVS{}^{*} [6]</th><td rowspan=\"4\">VGGSound 144k</td><td>0.719</td><td>0.587</td></tr><tr><th>HardPos [29]</th><td>0.768</td><td>0.592</td></tr><tr><th>SSPL [30]</th><td>0.767</td><td>0.605</td></tr><tr><th>HTF (Ours)</th><td>0.848</td><td>0.640</td></tr></tbody></table>", "caption": "Table 1: Quantitative results on the Flickr SoundNet testing dataset where models are trained on the two training subsets of Flickr SoundNet and VGG Sound 144k. \u201c*\u201d Denotes our faithful reproduction of the method, and \u201c\\dagger\u201d denotes our evaluation reproduction using officially provided model weights.", "list_citation_info": ["[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In European Conference on Computer Vision, pages 208\u2013224. Springer, 2020.", "[19] Di Hu, Feiping Nie, and Xuelong Li. Deep multimodal clustering for unsupervised audiovisual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9248\u20139257, 2019.", "[29] Arda Senocak, Hyeonggon Ryu, Junsik Kim, and In So Kweon. Learning sound localization better from semantically similar samples. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4863\u20134867. IEEE, 2022.", "[30] Zengjie Song, Yuxi Wang, Junsong Fan, Tieniu Tan, and Zhaoxiang Zhang. Self-supervised predictive learning: A negative-free method for sound source localization in visual scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3222\u20133231, 2022.", "[28] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4358\u20134366, 2018.", "[25] Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, and Weiyao Lin. Multiple sound sources localization from coarse to fine. In European Conference on Computer Vision, pages 292\u2013308. Springer, 2020.", "[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Training Set</th><th>cIoU{}_{0.5}</th><th>AUC{}_{cIoU}</th></tr></thead><tbody><tr><th>Attention [28]</th><td rowspan=\"4\">VGGSound 10k</td><td>0.160</td><td>0.283</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.297</td><td>0.358</td></tr><tr><th>SSPL [30]</th><td>0.314</td><td>0.369</td></tr><tr><th>HTF (Ours)</th><td>0.393</td><td>0.398</td></tr><tr><th>Attention [28]</th><td rowspan=\"7\">VGGSound 144k</td><td>0.185</td><td>0.302</td></tr><tr><th>AVObject [1]</th><td>0.297</td><td>0.357</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.301</td><td>0.361</td></tr><tr><th>LVS{}^{\\dagger} [6]</th><td>0.288</td><td>0.359</td></tr><tr><th>HardPos [29]</th><td>0.346</td><td>0.380</td></tr><tr><th>SSPL [30]</th><td>0.339</td><td>0.380</td></tr><tr><th>HTF (Ours)</th><td>0.394</td><td>0.400</td></tr></tbody></table>", "caption": "Table 2: Quantitative results on the VGG Sound Source testing dataset where models are trained on the two training subsets of VGG Sound.", "list_citation_info": ["[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In European Conference on Computer Vision, pages 208\u2013224. Springer, 2020.", "[29] Arda Senocak, Hyeonggon Ryu, Junsik Kim, and In So Kweon. Learning sound localization better from semantically similar samples. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4863\u20134867. IEEE, 2022.", "[30] Zengjie Song, Yuxi Wang, Junsong Fan, Tieniu Tan, and Zhaoxiang Zhang. Self-supervised predictive learning: A negative-free method for sound source localization in visual scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3222\u20133231, 2022.", "[28] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4358\u20134366, 2018.", "[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Testing Set</th><th>cIoU{}_{0.5}</th><th>AUC{}_{cIoU}</th></tr></thead><tbody><tr><th>LVS{}^{*} [6]</th><td rowspan=\"2\">VGGSS Heard 110</td><td>0.251</td><td>0.336</td></tr><tr><th>HTF (Ours)</th><td>0.373</td><td>0.386</td></tr><tr><th>LVS{}^{*} [6]</th><td rowspan=\"2\">VGGSS Unheard 110</td><td>0.270</td><td>0.349</td></tr><tr><th>HTF (Ours)</th><td>0.393</td><td>0.400</td></tr></tbody></table>", "caption": "Table 3: Quantitative results on the VGG Sound Source testing dataset on heard and unheard class subsets. Each model is trained on 50k samples belonging to 110 (heard) classes.", "list_citation_info": ["[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Training Set</th><th>cIoU{}_{0.5}</th><th>AUC{}_{cIoU}</th></tr></thead><tbody><tr><th>LVS{}^{*} [6]</th><td rowspan=\"2\">Flickr 10k</td><td>0.659</td><td>0.529</td></tr><tr><th>HTF (Ours)</th><td>0.718</td><td>0.558</td></tr><tr><th>LVS{}^{*} [6]</th><td rowspan=\"2\">Flickr 144k</td><td>0.684</td><td>0.535</td></tr><tr><th>HTF (Ours)</th><td>0.759</td><td>0.575</td></tr><tr><th>LVS{}^{*} [6]</th><td rowspan=\"2\">VGGSound 144k</td><td>0.665</td><td>0.529</td></tr><tr><th>HTF (Ours)</th><td>0.734</td><td>0.564</td></tr></tbody></table>", "caption": "Table 6: Quantitative results on the novel, expanded Flickr SoundNet testing dataset where models are trained on the two training subsets of Flickr SoundNet and VGG Sound 144k. \u201c*\u201d Denotes our faithful reproduction of the method.", "list_citation_info": ["[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><td>Training Set</td><td>cIoU{}_{0.5}</td><td>AUC{}_{cIoU}</td></tr><tr><th>LVS [6]</th><td rowspan=\"2\">Flickr 10k</td><td>0.582</td><td>0.525</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.730</td><td>0.578</td></tr><tr><th>LVS [6]</th><td rowspan=\"3\">Flickr 144k</td><td>0.699</td><td>0.573</td></tr><tr><th>LVS{}^{\\dagger} [6]</th><td>0.697</td><td>0.560</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.702</td><td>0.588</td></tr><tr><th>LVS [6]</th><td rowspan=\"2\">VGGSound 144k</td><td>0.719</td><td>0.582</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.719</td><td>0.587</td></tr></tbody></table>", "caption": "Table 7: Reproduction results of LVS [6] on the Flickr SoundNet testing dataset. \u201c*\u201d Denotes our faithful reproduction of the method, and \u201c\\dagger\u201d denotes our evaluation reproduction using officially provided model weights.", "list_citation_info": ["[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><td>Training Set</td><td>cIoU{}_{0.5}</td><td>AUC{}_{cIoU}</td></tr><tr><th>LVS [6]</th><td rowspan=\"2\">VGGSound 10k</td><td>-</td><td>-</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.297</td><td>0.358</td></tr><tr><th>LVS [6]</th><td rowspan=\"3\">VGGSound 144k</td><td>0.344</td><td>0.382</td></tr><tr><th>LVS{}^{\\dagger} [6]</th><td>0.288</td><td>0.359</td></tr><tr><th>LVS{}^{*} [6]</th><td>0.301</td><td>0.361</td></tr></tbody></table>", "caption": "Table 8: Reproduction results of LVS [6] on the VGG Sound Source testing dataset. \u201c*\u201d Denotes our faithful reproduction of the method, and \u201c\\dagger\u201d denotes our evaluation reproduction using officially provided model weights.", "list_citation_info": ["[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021."]}], "citation_info_to_title": {"[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning of audio-visual objects from video. In European Conference on Computer Vision, pages 208\u2013224. Springer, 2020.": "Self-supervised learning of audio-visual objects from video", "[25] Rui Qian, Di Hu, Heinrich Dinkel, Mengyue Wu, Ning Xu, and Weiyao Lin. Multiple sound sources localization from coarse to fine. In European Conference on Computer Vision, pages 292\u2013308. Springer, 2020.": "Multiple sound sources localization from coarse to fine", "[30] Zengjie Song, Yuxi Wang, Junsong Fan, Tieniu Tan, and Zhaoxiang Zhang. Self-supervised predictive learning: A negative-free method for sound source localization in visual scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3222\u20133231, 2022.": "Self-supervised predictive learning: A negative-free method for sound source localization in visual scenes", "[29] Arda Senocak, Hyeonggon Ryu, Junsik Kim, and In So Kweon. Learning sound localization better from semantically similar samples. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4863\u20134867. IEEE, 2022.": "Learning sound localization better from semantically similar samples", "[19] Di Hu, Feiping Nie, and Xuelong Li. Deep multimodal clustering for unsupervised audiovisual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9248\u20139257, 2019.": "Deep Multimodal Clustering for Unsupervised Audiovisual Learning", "[6] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16867\u201316876, 2021.": "Localizing visual sounds the hard way", "[28] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, and In So Kweon. Learning to localize sound source in visual scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4358\u20134366, 2018.": "Learning to localize sound source in visual scenes"}, "source_title_to_arxiv_id": {"Learning sound localization better from semantically similar samples": "2202.03007", "Localizing visual sounds the hard way": "2104.02691"}}