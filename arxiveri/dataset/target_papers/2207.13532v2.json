{"title": "Contrastive Masked Autoencoders are Stronger Vision Learners", "abstract": "Masked image modeling (MIM) has achieved promising results on various vision\ntasks. However, the limited discriminability of learned representation\nmanifests there is still plenty to go for making a stronger vision learner.\nTowards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new\nself-supervised pre-training method for learning more comprehensive and capable\nvision representations. By elaboratively unifying contrastive learning (CL) and\nmasked image model (MIM) through novel designs, CMAE leverages their respective\nadvantages and learns representations with both strong instance\ndiscriminability and local perceptibility. Specifically, CMAE consists of two\nbranches where the online branch is an asymmetric encoder-decoder and the\ntarget branch is a momentum updated encoder. During training, the online\nencoder reconstructs original images from latent representations of masked\nimages to learn holistic features. The target encoder, fed with the full\nimages, enhances the feature discriminability via contrastive learning with its\nonline counterpart. To make CL compatible with MIM, CMAE introduces two new\ncomponents, i.e. pixel shift for generating plausible positive views and\nfeature decoder for complementing features of contrastive pairs. Thanks to\nthese novel designs, CMAE effectively improves the representation quality and\ntransfer performance over its MIM counterpart. CMAE achieves the\nstate-of-the-art performance on highly competitive benchmarks of image\nclassification, semantic segmentation and object detection. Notably, CMAE-Base\nachieves $85.3\\%$ top-1 accuracy on ImageNet and $52.5\\%$ mIoU on ADE20k,\nsurpassing previous best results by $0.7\\%$ and $1.8\\%$ respectively. Codes\nwill be made publicly available at \\url{https://github.com/ZhichengHuang/CMAE}.", "authors": ["Zhicheng Huang", "Xiaojie Jin", "Chengze Lu", "Qibin Hou", "Ming-Ming Cheng", "Dongmei Fu", "Xiaohui Shen", "Jiashi Feng"], "published_date": "2022_07_27", "pdf_url": "http://arxiv.org/pdf/2207.13532v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td colspan=\"3\">Training objective</td><td>Input</td><td colspan=\"2\">Architecture</td><td rowspan=\"3\">Acc.</td></tr><tr><td></td><td>recons.</td><td>intra-view</td><td>intra-image</td><td>pos. view</td><td>feature</td><td>separate</td></tr><tr><td></td><td>loss</td><td>match</td><td>contrast</td><td>alignment</td><td>complement</td><td>enc./dec.</td></tr><tr><td>MSN [1]</td><td>\\text\u2717</td><td>\u2713</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\\text\u2717</td><td>83.4</td></tr><tr><td>ExtreMA [40]</td><td>\\text\u2717</td><td>\u2713</td><td>\\text\u2717</td><td>\u2713</td><td>\\text\u2717</td><td>\\text\u2717</td><td>83.7</td></tr><tr><td>MAE [24]</td><td>\u2713</td><td>\\text\u2717</td><td>\\text\u2717</td><td>N.A.</td><td>N.A.</td><td>\u2713</td><td>83.6</td></tr><tr><td>CAE [11]</td><td>\u2713</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\u2713</td><td>83.9</td></tr><tr><td>iBot [50]</td><td>\u2713</td><td>\u2713</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\\text\u2717</td><td>84.0</td></tr><tr><td>SIM [37]</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\\text\u2717</td><td>\\text\u2717</td><td>\u2713</td><td>83.8</td></tr><tr><td>CMAE</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>84.7</td></tr></tbody></table>", "caption": "Table 1: Comparison of CMAE with previous methods on training objective, input generation and architecture. The top-1 accuracy on ImageNet is also presented. Please refer to Section 3.4 for more detailed explanations.", "list_citation_info": ["Zhou et al. [2022] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. ibot: Image bert pre-training with online tokenizer. ICLR, 2022.", "Chen et al. [2022] X. Chen, M. Ding, X. Wang, Y. Xin, S. Mo, Y. Wang, S. Han, P. Luo, G. Zeng, and J. Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022.", "Tao et al. [2022b] C. Tao, X. Zhu, G. Huang, Y. Qiao, X. Wang, and J. Dai. Siamese image modeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204, 2022b.", "Assran et al. [2022] M. Assran, M. Caron, I. Misra, P. Bojanowski, F. Bordes, P. Vincent, A. Joulin, M. Rabbat, and N. Ballas. Masked siamese networks for label-efficient learning. arXiv preprint arXiv:2204.07141, 2022.", "Wu et al. [2022] Z. Wu, Z. Lai, X. Sun, and S. Lin. Extreme masking for learning instance and distributed visual representations. arXiv preprint arXiv:2206.04667, 2022.", "He et al. [2022] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000\u201316009. IEEE, 2022."]}, {"table": "<table><tbody><tr><td>Method</td><td>Pre-training epochs</td><td>Params.(M)</td><td>Supervision</td><td>Accuracy</td></tr><tr><td>MoCo-v3 [10]</td><td>300</td><td>88</td><td>RGB</td><td>83.0</td></tr><tr><td>DINO [6]</td><td>1600</td><td>88</td><td>RGB</td><td>82.8</td></tr><tr><td>CIM [17]</td><td>300</td><td>88</td><td>RGB</td><td>83.1</td></tr><tr><td>BEiT [3]</td><td>800</td><td>88</td><td>DALLE</td><td>83.2</td></tr><tr><td>SimMIM [42]</td><td>800</td><td>88</td><td>RGB</td><td>83.8</td></tr><tr><td>PeCo [15]</td><td>800</td><td>88</td><td>Perceptual Codebook</td><td>84.5</td></tr><tr><td>MaskFeat [39]</td><td>1600</td><td>88</td><td>HOG</td><td>84.0</td></tr><tr><td>CAE [11]</td><td>1600</td><td>88</td><td>DALLE+RGB</td><td>83.8</td></tr><tr><td>iBOT [50]</td><td>1600</td><td>88</td><td>RGB</td><td>84.0</td></tr><tr><td>SIM [37]</td><td>1600</td><td>88</td><td>RGB</td><td>83.8</td></tr><tr><td>MAE [24]</td><td>800</td><td>88</td><td>RGB</td><td>83.1</td></tr><tr><td>MAE [24]</td><td>1600</td><td>88</td><td>RGB</td><td>83.6</td></tr><tr><td>CMAE (ours)</td><td>800</td><td>88</td><td>RGB</td><td>84.4</td></tr><tr><td>CMAE (ours)</td><td>1600</td><td>88</td><td>RGB</td><td>84.7</td></tr><tr><td>ConvMAE<sup>*</sup> [18]</td><td>800</td><td>88</td><td>RGB</td><td>84.6</td></tr><tr><td>ConvMAE<sup>*</sup> [18]</td><td>1600</td><td>88</td><td>RGB</td><td>84.6</td></tr><tr><td>CMAE<sup>*</sup> (ours)</td><td>800</td><td>88</td><td>RGB</td><td>85.0</td></tr><tr><td>CMAE<sup>*</sup> (ours)</td><td>1600</td><td>88</td><td>RGB</td><td>85.3</td></tr></tbody></table>", "caption": "Table 2: Comparison of our model with existing methods on ViT-B. We evaluate them with the top-1 accuracy on ImageNet. The symbol of <sup>*</sup> throughout experiments denotes using convolutions instead of linear transformation as the tokenizer for visual patches. ", "list_citation_info": ["Wei et al. [2022] C. Wei, H. Fan, S. Xie, C.-Y. Wu, A. Yuille, and C. Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In CVPR, pages 14668\u201314678. IEEE, 2022.", "Gao et al. [2022] P. Gao, T. Ma, H. Li, J. Dai, and Y. Qiao. Convmae: Masked convolution meets masked autoencoders. arXiv preprint arXiv:2205.03892, 2022.", "Xie et al. [2022a] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu. Simmim: A simple framework for masked image modeling. In CVPR, pages 9653\u20139663. IEEE, 2022a.", "Chen et al. [2021] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In ICCV, pages 9640\u20139649. IEEE, 2021.", "Fang et al. [2022] Y. Fang, L. Dong, H. Bao, X. Wang, and F. Wei. Corrupted image modeling for self-supervised visual pre-training. arXiv preprint arXiv:2202.03382, 2022.", "Zhou et al. [2022] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. ibot: Image bert pre-training with online tokenizer. ICLR, 2022.", "Chen et al. [2022] X. Chen, M. Ding, X. Wang, Y. Xin, S. Mo, Y. Wang, S. Han, P. Luo, G. Zeng, and J. Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022.", "Dong et al. [2021] X. Dong, J. Bao, T. Zhang, D. Chen, W. Zhang, L. Yuan, D. Chen, F. Wen, and N. Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.", "Caron et al. [2021] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 9650\u20139660. IEEE, 2021.", "Tao et al. [2022b] C. Tao, X. Zhu, G. Huang, Y. Qiao, X. Wang, and J. Dai. Siamese image modeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204, 2022b.", "Bao et al. [2021] H. Bao, L. Dong, and F. Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.", "He et al. [2022] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000\u201316009. IEEE, 2022."]}], "citation_info_to_title": {"Chen et al. [2022] X. Chen, M. Ding, X. Wang, Y. Xin, S. Mo, Y. Wang, S. Han, P. Luo, G. Zeng, and J. Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022.": "Context Autoencoder for Self-Supervised Representation Learning", "Zhou et al. [2022] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. ibot: Image bert pre-training with online tokenizer. ICLR, 2022.": "ibot: Image bert pre-training with online tokenizer", "Dong et al. [2021] X. Dong, J. Bao, T. Zhang, D. Chen, W. Zhang, L. Yuan, D. Chen, F. Wen, and N. Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.": "Peco: Perceptual Codebook for BERT Pre-training of Vision Transformers", "Tao et al. [2022b] C. Tao, X. Zhu, G. Huang, Y. Qiao, X. Wang, and J. Dai. Siamese image modeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204, 2022b.": "Siamese Image Modeling for Self-Supervised Vision Representation Learning", "Wei et al. [2022] C. Wei, H. Fan, S. Xie, C.-Y. Wu, A. Yuille, and C. Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In CVPR, pages 14668\u201314678. IEEE, 2022.": "Masked feature prediction for self-supervised visual pre-training", "Gao et al. [2022] P. Gao, T. Ma, H. Li, J. Dai, and Y. Qiao. Convmae: Masked convolution meets masked autoencoders. arXiv preprint arXiv:2205.03892, 2022.": "Convmae: Masked convolution meets masked autoencoders", "Fang et al. [2022] Y. Fang, L. Dong, H. Bao, X. Wang, and F. Wei. Corrupted image modeling for self-supervised visual pre-training. arXiv preprint arXiv:2202.03382, 2022.": "Corrupted Image Modeling for Self-Supervised Visual Pre-Training", "Xie et al. [2022a] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu. Simmim: A simple framework for masked image modeling. In CVPR, pages 9653\u20139663. IEEE, 2022a.": "Simmim: A simple framework for masked image modeling", "Caron et al. [2021] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 9650\u20139660. IEEE, 2021.": "Emerging properties in self-supervised vision transformers", "Wu et al. [2022] Z. Wu, Z. Lai, X. Sun, and S. Lin. Extreme masking for learning instance and distributed visual representations. arXiv preprint arXiv:2206.04667, 2022.": "Extreme Masking for Learning Instance and Distributed Visual Representations", "Assran et al. [2022] M. Assran, M. Caron, I. Misra, P. Bojanowski, F. Bordes, P. Vincent, A. Joulin, M. Rabbat, and N. Ballas. Masked siamese networks for label-efficient learning. arXiv preprint arXiv:2204.07141, 2022.": "Masked Siamese Networks for Label-Efficient Learning", "Bao et al. [2021] H. Bao, L. Dong, and F. Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.": "Beit: Bert pre-training of image transformers", "He et al. [2022] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 16000\u201316009. IEEE, 2022.": "Masked autoencoders are scalable vision learners", "Chen et al. [2021] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers. In ICCV, pages 9640\u20139649. IEEE, 2021.": "An empirical study of training self-supervised vision transformers"}, "source_title_to_arxiv_id": {"Context Autoencoder for Self-Supervised Representation Learning": "2202.03026", "ibot: Image bert pre-training with online tokenizer": "2111.07832", "Simmim: A simple framework for masked image modeling": "2111.09886", "Emerging properties in self-supervised vision transformers": "2104.14294", "Masked Siamese Networks for Label-Efficient Learning": "2204.07141", "Masked autoencoders are scalable vision learners": "2111.06377", "An empirical study of training self-supervised vision transformers": "2104.02057"}}