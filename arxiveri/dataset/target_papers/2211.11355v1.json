{"title": "Blind Knowledge Distillation for Robust Image Classification", "abstract": "Optimizing neural networks with noisy labels is a challenging task,\nespecially if the label set contains real-world noise. Networks tend to\ngeneralize to reasonable patterns in the early training stages and overfit to\nspecific details of noisy samples in the latter ones. We introduce Blind\nKnowledge Distillation - a novel teacher-student approach for learning with\nnoisy labels by masking the ground truth related teacher output to filter out\npotentially corrupted knowledge and to estimate the tipping point from\ngeneralizing to overfitting. Based on this, we enable the estimation of noise\nin the training data with Otsus algorithm. With this estimation, we train the\nnetwork with a modified weighted cross-entropy loss function. We show in our\nexperiments that Blind Knowledge Distillation detects overfitting effectively\nduring training and improves the detection of clean and noisy labels on the\nrecently published CIFAR-N dataset. Code is available at GitHub.", "authors": ["Timo Kaiser", "Lukas Ehmann", "Christoph Reinders", "Bodo Rosenhahn"], "published_date": "2022_11_21", "pdf_url": "http://arxiv.org/pdf/2211.11355v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Acc [%]</th><th>Aggre</th><th>Rand1</th><th>Rand2</th><th>Rand3</th><th>Worst</th></tr></thead><tbody><tr><th>SOP</th><td>95.61</td><td>95.28</td><td>95.31</td><td>95.39</td><td>93.24</td></tr><tr><th>CORES</th><td>95.25</td><td>94.45</td><td>94.88</td><td>94.47</td><td>91.66</td></tr><tr><th>DivideMix</th><td>95.01</td><td>95.16</td><td>95.23</td><td>95.21</td><td>92.56</td></tr><tr><th>ELR+</th><td>94.83</td><td>94.43</td><td>94.20</td><td>94.34</td><td>91.09</td></tr><tr><th>PES</th><td>94.66</td><td>95.06</td><td>95.19</td><td>95.22</td><td>92.68</td></tr><tr><th>ELR</th><td>92.38</td><td>91.46</td><td>91.61</td><td>91.41</td><td>83.58</td></tr><tr><th>CAL</th><td>91.97</td><td>90.93</td><td>90.75</td><td>90.74</td><td>85.36</td></tr><tr><th>CE</th><td>87.77</td><td>85.02</td><td>86.14</td><td>85.16</td><td>77.69</td></tr><tr><th>Ours</th><td>93.68</td><td>92.50</td><td>92.63</td><td>92.54</td><td>86.64</td></tr></tbody></table>", "caption": "Table 1: Classification accuracy of our method compared to standard CE-loss framework and state-of-the-art methodsSOP Liu et al. (2022a),CORES Cheng et al. (2021),DivideMix Li et al. (2020),PES Bai et al. (2021),ELR Liu et al. (2020), andCAL Zhu et al. (2021).", "list_citation_info": ["Cheng et al. [2021] Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependent label noise: A sample sieve approach. In International Conference on Learning Representations, 2021.", "Bai et al. [2021] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021.", "Li et al. [2020] Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In International Conference on Learning Representations, 2020.", "Zhu et al. [2021] Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-dependent label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.", "Liu et al. [2022a] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. arXiv preprint arXiv:2202.14026, 2022.", "Liu et al. [2020] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020."]}], "citation_info_to_title": {"Cheng et al. [2021] Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependent label noise: A sample sieve approach. In International Conference on Learning Representations, 2021.": "Learning with Instance-Dependent Label Noise: A Sample Sieve Approach", "Li et al. [2020] Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In International Conference on Learning Representations, 2020.": "Dividemix: Learning with Noisy Labels as Semi-Supervised Learning", "Liu et al. [2020] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.": "Early-learning regularization prevents memorization of noisy labels", "Liu et al. [2022a] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. arXiv preprint arXiv:2202.14026, 2022.": "Robust training under label noise by over-parameterization", "Zhu et al. [2021] Zhaowei Zhu, Tongliang Liu, and Yang Liu. A second-order approach to learning with instance-dependent label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.": "A second-order approach to learning with instance-dependent label noise", "Bai et al. [2021] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021.": "Understanding and improving early stopping for learning with noisy labels"}, "source_title_to_arxiv_id": {"Robust training under label noise by over-parameterization": "2202.14026"}}