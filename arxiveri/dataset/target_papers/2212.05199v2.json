{"title": "MAGVIT: Masked Generative Video Transformer", "abstract": "We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle\nvarious video synthesis tasks with a single model. We introduce a 3D tokenizer\nto quantize a video into spatial-temporal visual tokens and propose an\nembedding method for masked video token modeling to facilitate multi-task\nlearning. We conduct extensive experiments to demonstrate the quality,\nefficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT\nperforms favorably against state-of-the-art approaches and establishes the\nbest-published FVD on three video generation benchmarks, including the\nchallenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference\ntime by two orders of magnitude against diffusion models and by 60x against\nautoregressive models. (iii) A single MAGVIT model supports ten diverse\ngeneration tasks and generalizes across videos from different visual domains.\nThe source code and trained models will be released to the public at\nhttps://magvit.cs.cmu.edu.", "authors": ["Lijun Yu", "Yong Cheng", "Kihyuk Sohn", "Jos\u00e9 Lezama", "Han Zhang", "Huiwen Chang", "Alexander G. Hauptmann", "Ming-Hsuan Yang", "Yuan Hao", "Irfan Essa", "Lu Jiang"], "published_date": "2022_12_10", "pdf_url": "http://arxiv.org/pdf/2212.05199v2", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>Extra Video</td><td>Class</td><td>FVD\\downarrow</td><td>IS\\uparrow</td></tr><tr><td>RaMViD[35]</td><td></td><td></td><td>-</td><td>21.71\\pm0.21</td></tr><tr><td>StyleGAN-V{}^{*}[51]</td><td></td><td></td><td>-</td><td>23.94\\pm0.73</td></tr><tr><td>DIGAN[73]</td><td></td><td></td><td>577\\pm21</td><td>32.70\\pm0.35</td></tr><tr><td>DVD-GAN[15]</td><td></td><td>\u2713</td><td>-</td><td>32.97\\pm1.70</td></tr><tr><td>Video Diffusion{}^{*}[33]</td><td></td><td></td><td>-</td><td>57.00\\pm0.62</td></tr><tr><td>TATS[21]</td><td></td><td></td><td>420\\pm18</td><td>57.63\\pm0.24</td></tr><tr><td>CCVS+StyleGAN[41]</td><td></td><td></td><td>386\\pm15</td><td>24.47\\pm0.13</td></tr><tr><td>Make-A-Video{}^{*}[50]</td><td></td><td>\u2713</td><td>367</td><td>33.00</td></tr><tr><td>TATS[21]</td><td></td><td>\u2713</td><td>332\\pm18</td><td>79.28\\pm0.38</td></tr><tr><td>CogVideo{}^{*}[34]</td><td>\u2713</td><td>\u2713</td><td>626</td><td>50.46</td></tr><tr><td>Make-A-Video{}^{*}[50]</td><td>\u2713</td><td>\u2713</td><td>81</td><td>82.55</td></tr><tr><td>MAGVIT-B-CG (ours)</td><td></td><td>\u2713</td><td>159\\pm2</td><td>83.55\\pm0.14</td></tr><tr><td>MAGVIT-L-CG (ours)</td><td></td><td>\u2713</td><td>76\\pm2</td><td>89.27\\pm0.15</td></tr></table>", "caption": "Table 1: Generation performance on the UCF-101 dataset.Methods in gray are pretrained on additional large video data.Methods with \\checkmark in the Class column are class-conditional, while the others are unconditional.Methods marked with {}^{*} use custom resolutions, while the others are at 128\\times128.See Appendix C for more comparisons with earlier works.", "list_citation_info": ["[34] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022.", "[21] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In ECCV, 2022.", "[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv:2209.14792, 2022.", "[73] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In ICLR, 2022.", "[35] Tobias H\u00f6ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. arXiv:2206.07696, 2022.", "[33] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In ICLR Workshops, 2022.", "[41] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In NeurIPS, 2021.", "[51] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. StyleGAN-V: A continuous video generator with the price, image quality and perks of StyleGAN2. In CVPR, 2022.", "[15] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv:1907.06571, 2019."]}, {"table": "<table><tr><td>Method</td><td>K600 FVD\\downarrow</td><td>BAIR FVD\\downarrow</td></tr><tr><td>CogVideo[34]</td><td>109.2</td><td>-</td></tr><tr><td>CCVS[41]</td><td>55.0\\pm1.0</td><td>99\\pm2</td></tr><tr><td>Phenaki[63]</td><td>36.4\\pm0.2</td><td>97</td></tr><tr><td>TrIVD-GAN-FP[43]</td><td>25.7\\pm0.7</td><td>103</td></tr><tr><td>Transframer[44]</td><td>25.4</td><td>100</td></tr><tr><td>MaskViT[26]</td><td>-</td><td>94</td></tr><tr><td>FitVid[4]</td><td>-</td><td>94</td></tr><tr><td>MCVD[64]</td><td>-</td><td>90</td></tr><tr><td>N\u00dcWA[69]</td><td>-</td><td>87</td></tr><tr><td>RaMViD[35]</td><td>16.5</td><td>84</td></tr><tr><td>Video Diffusion[33]</td><td>16.2\\pm0.3</td><td>-</td></tr><tr><td>MAGVIT-B-FP (ours)</td><td>24.5\\pm0.9</td><td>76\\pm0.1 (48\\pm0.1)</td></tr><tr><td>MAGVIT-L-FP (ours)</td><td>9.9\\pm0.3</td><td>62\\pm0.1 (31\\pm0.2)</td></tr></table>", "caption": "Table 2: Frame prediction performance on the BAIR and Kinetics-600 datasets.- marks that the value is unavailable in their paper or incomparable to others.The FVD in parentheses uses a debiased evaluation protocol on BAIR detailed in Appendix B.3.See Appendix C for more comparisons with earlier works.", "list_citation_info": ["[26] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u00edn-Mart\u00edn, and Li Fei-Fei. MaskViT: Masked visual pre-training for video prediction. arXiv:2206.11894, 2022.", "[63] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv:2210.02399, 2022.", "[34] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022.", "[69] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00dcWA: Visual synthesis pre-training for neural visual world creation. In ECCV, 2022.", "[44] Charlie Nash, Jo\u00e3o Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, and Peter Battaglia. Transframer: Arbitrary frame prediction with generative models. arXiv:2203.09494, 2022.", "[35] Tobias H\u00f6ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. arXiv:2206.07696, 2022.", "[33] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In ICLR Workshops, 2022.", "[41] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In NeurIPS, 2021.", "[4] Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv:2106.13195, 2021.", "[43] Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data. arXiv:2003.04035, 2020.", "[64] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion for prediction, generation, and interpolation. In NeurIPS, 2022."]}, {"table": "<table><tr><td>Method</td><td>FVD\\downarrow</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td></tr><tr><td>CCVS[41]</td><td>99</td><td>-</td><td>0.729</td><td>-</td></tr><tr><td>MCVD[64]</td><td>90</td><td>16.9</td><td>0.780</td><td>-</td></tr><tr><td>MAGVIT-L-FP (ours)</td><td>62</td><td>19.3</td><td>0.787</td><td>0.123</td></tr></table>", "caption": "Table 3: Image quality metrics on BAIR frame prediction.", "list_citation_info": ["[41] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In NeurIPS, 2021.", "[64] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion for prediction, generation, and interpolation. In NeurIPS, 2022."]}, {"table": "<table><tr><td colspan=\"2\">Method</td><td> Seq. Length </td><td> FP FVD\\downarrow </td><td> MT8 FVD\\downarrow </td></tr><tr><td colspan=\"2\">Latent masking in MaskGIT [12]</td><td>1024</td><td>74</td><td>151</td></tr><tr><td colspan=\"2\">Prefix condition</td><td>1024-1792</td><td>55</td><td>-</td></tr><tr><td rowspan=\"3\"> COMMIT(ours) </td><td>\\mathcal{L}_{\\text{mask}}</td><td rowspan=\"3\">1024</td><td>388</td><td>143</td></tr><tr><td>\\mathcal{L}_{\\text{mask}}+\\mathcal{L}_{\\text{recons}}</td><td>51</td><td>53</td></tr><tr><td>\\mathcal{L}_{\\text{mask}}+\\mathcal{L}_{\\text{recons}}+\\mathcal{L}_{\\text{refine}}</td><td>48</td><td>33</td></tr></table>", "caption": "Table 5: Comparison of conditional masked token modeling on BAIR frame prediction (FP) and eight-task (MT8) benchmarks. - indicates we were not able to train to convergence.", "list_citation_info": ["[12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked generative image transformer. In CVPR, 2022."]}, {"table": "<table><tr><td>Decoding Method</td><td>Tokenizer</td><td>Type</td><td>Param.</td><td>Seq. Len.\\downarrow</td><td># Steps\\downarrow</td><td> FVD\\downarrow </td></tr><tr><td rowspan=\"2\">MaskGIT [12]</td><td>2D-VQ</td><td>NAR</td><td>53M+87M</td><td>4096</td><td>12</td><td>222 (177)</td></tr><tr><td>3D-VQ</td><td>NAR</td><td>41M+87M</td><td>1024</td><td>12</td><td>122 (74)</td></tr><tr><td>MaskViT [26]</td><td>2D-VQ</td><td>NAR</td><td>53M+189M</td><td>4096</td><td>18</td><td>94{}^{*}</td></tr><tr><td>AR</td><td>3D-VQ</td><td>AR</td><td>41M+87M</td><td>1024</td><td>1024</td><td>91 (56)</td></tr><tr><td>MAGVIT (ours)</td><td>3D-VQ</td><td>NAR</td><td>41M+87M</td><td>1024</td><td>12</td><td>76 (48)</td></tr></table>", "caption": "Table 6: Comparison of decoding methods on BAIR frame prediction benchmark.The number of parameters is broken down as VQ + Transformer.NAR is non-autoregressive and AR is autoregressive. FVD and debiased FVD (in parentheses) are reported.{}^{*} marks the quoted number from their paper.", "list_citation_info": ["[26] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u00edn-Mart\u00edn, and Li Fei-Fei. MaskViT: Masked visual pre-training for video prediction. arXiv:2206.11894, 2022.", "[12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked generative image transformer. In CVPR, 2022."]}, {"table": "<table><tr><td rowspan=\"2\">Tokenizer</td><td colspan=\"2\">From Scratch</td><td colspan=\"4\">ImageNet [16] Initialization</td></tr><tr><td>FVD\\downarrow</td><td>IS\\uparrow</td><td>FVD\\downarrow</td><td>IS\\uparrow</td><td>FVD\\downarrow</td><td>IS\\uparrow</td></tr><tr><td>MaskGIT [12] 2D-VQ</td><td>240</td><td>80.9</td><td>216</td><td>82.6</td><td colspan=\"2\">-</td></tr><tr><td>TATS [21] 3D-VQ</td><td>162</td><td>80.6</td><td colspan=\"2\">-</td><td colspan=\"2\">-</td></tr><tr><td></td><td></td><td></td><td colspan=\"2\">Average</td><td colspan=\"2\">Central</td></tr><tr><td>MAGVIT 3D-VQ-B (ours)</td><td>127</td><td>82.1</td><td>103</td><td>84.8</td><td>58</td><td>87.0</td></tr><tr><td>MAGVIT 3D-VQ-L (ours)</td><td>45</td><td>87.1</td><td>35</td><td>88.3</td><td>25</td><td>88.9</td></tr></table>", "caption": "Table 7: Comparison of tokenizer architectures and initialization methods on UCF-101 training set reconstruction results.The 2D-VQ compresses by 8\\times8 spatially and the 3D-VQ compresses by 4\\times8\\times8 spatial-temporally.", "list_citation_info": ["[12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked generative image transformer. In CVPR, 2022.", "[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.", "[21] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In ECCV, 2022."]}], "citation_info_to_title": {"[12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. MaskGIT: Masked generative image transformer. In CVPR, 2022.": "MaskGIT: Masked Generative Image Transformer", "[33] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. In ICLR Workshops, 2022.": "Video Diffusion Models", "[15] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv:1907.06571, 2019.": "Adversarial video generation on complex datasets", "[44] Charlie Nash, Jo\u00e3o Carreira, Jacob Walker, Iain Barr, Andrew Jaegle, Mateusz Malinowski, and Peter Battaglia. Transframer: Arbitrary frame prediction with generative models. arXiv:2203.09494, 2022.": "Transframer: Arbitrary Frame Prediction with Generative Models", "[51] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. StyleGAN-V: A continuous video generator with the price, image quality and perks of StyleGAN2. In CVPR, 2022.": "StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2", "[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.": "ImageNet: A large-scale hierarchical image database", "[43] Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data. arXiv:2003.04035, 2020.": "Transformation-based adversarial video prediction on large-scale data", "[21] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In ECCV, 2022.": "Long video generation with time-agnostic VQGAN and time-sensitive transformer", "[63] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv:2210.02399, 2022.": "Phenaki: Variable length video generation from open domain textual description", "[34] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022.": "CogVideo: Large-scale pretraining for text-to-video generation via transformers", "[35] Tobias H\u00f6ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. arXiv:2206.07696, 2022.": "Diffusion models for video prediction and infilling", "[41] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid. CCVS: Context-aware controllable video synthesis. In NeurIPS, 2021.": "CCVS: Context-aware controllable video synthesis", "[69] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00dcWA: Visual synthesis pre-training for neural visual world creation. In ECCV, 2022.": "N\u00dcWA: Visual synthesis pre-training for neural visual world creation", "[64] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. Masked conditional video diffusion for prediction, generation, and interpolation. In NeurIPS, 2022.": "Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation", "[26] Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u00edn-Mart\u00edn, and Li Fei-Fei. MaskViT: Masked visual pre-training for video prediction. arXiv:2206.11894, 2022.": "MaskViT: Masked visual pre-training for video prediction", "[73] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. In ICLR, 2022.": "Generating videos with dynamics-aware implicit generative adversarial networks", "[4] Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and Dumitru Erhan. Fitvid: Overfitting in pixel-level video prediction. arXiv:2106.13195, 2021.": "Fitvid: Overfitting in Pixel-level Video Prediction", "[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv:2209.14792, 2022.": "Make-a-video: Text-to-video generation without text-video data"}, "source_title_to_arxiv_id": {"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2": "2112.14683"}}