{"title": "Alignment-guided Temporal Attention for Video Action Recognition", "abstract": "Temporal modeling is crucial for various video learning tasks. Most recent\napproaches employ either factorized (2D+1D) or joint (3D) spatial-temporal\noperations to extract temporal contexts from the input frames. While the former\nis more efficient in computation, the latter often obtains better performance.\nIn this paper, we attribute this to a dilemma between the sufficiency and the\nefficiency of interactions among various positions in different frames. These\ninteractions affect the extraction of task-relevant information shared among\nframes. To resolve this issue, we prove that frame-by-frame alignments have the\npotential to increase the mutual information between frame representations,\nthereby including more task-relevant information to boost effectiveness. Then\nwe propose Alignment-guided Temporal Attention (ATA) to extend 1-dimensional\ntemporal attention with parameter-free patch-level alignments between\nneighboring frames. It can act as a general plug-in for image backbones to\nconduct the action recognition task without any model-specific design.\nExtensive experiments on multiple benchmarks demonstrate the superiority and\ngenerality of our module.", "authors": ["Yizhou Zhao", "Zhenyang Li", "Xun Guo", "Yan Lu"], "published_date": "2022_09_30", "pdf_url": "http://arxiv.org/pdf/2210.00132v2", "list_table_and_caption": [{"table": "<p>MethodPretrainFramesTop-1 AccTop-5 AccViewsFLOPs (G)Params (M)ViT-B-VTN Neimark et al. (2021)IN-21K1679.894.21\\times 14218114VidTr-L Zhang et al. (2021)IN-21K3279.193.93\\times 10351-ViViT-L Arnab et al. (2021)IN-21K1681.394.73\\times 43992310.8TimeSformer Bertasius et al. (2021)IN-21K878.093.73\\times 1196121.4TimeSformer-L Bertasius et al. (2021)IN-21K9680.794.73\\times 12380121.4TimeSformer-HR Bertasius et al. (2021)IN-21K1679.794.43\\times 11703121.4MViTv1-B Fan et al. (2021)-1678.493.51\\times 570.536.6MViTv1-B Fan et al. (2021)-3280.294.41\\times 517036.6MViTv1-B Fan et al. (2021)-6481.295.13\\times 345536.6Mformer-B Patrick et al. (2021)IN-21K1679.794.23\\times 10369.5109.1Mformer-L Patrick et al. (2021)IN-21K3280.294.83\\times 101185.1109.1Mformer-HR Patrick et al. (2021)IN-21K1681.195.23\\times 10958.8381.9ATA (Ours)IN-21K3281.495.53\\times 1792.9121.8ATA (Ours)IN-21K3281.995.53\\times 4792.9121.8</p>", "caption": "Table 1: Comparison to the state-of-the-art on Kinetics-400. x\\times y views means x spatial crops with y temporal clips. We report the inference cost with a single view. RED/BLUE indicate SOTA/the second best. IN stands for officially released ImageNet Krizhevsky et al. (2012) pretrained models. The same below.", "list_citation_info": ["Arnab et al. [2021] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid. Vivit: A video vision transformer. In ICCV, 2021.", "Patrick et al. [2021] M. Patrick, D. Campbell, Y. M. Asano, I. M. F. Metze, C. Feichtenhofer, A. Vedaldi, and J. F. Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. In NeurIPS, 2021.", "Bertasius et al. [2021] G. Bertasius, H. Wang, and L. Torresani. Is space-time attention all you need for video understanding? In ICML, 2021.", "Neimark et al. [2021] D. Neimark, O. Bar, M. Zohar, and D. Asselmann. Video transformer network. In ICCV, 2021.", "Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 2012.", "Fan et al. [2021] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision transformers. In ICCV, 2021.", "Zhang et al. [2021] Y. Zhang, X. Li, C. Liu, B. Shuai, Y. Zhu, B. Brattoli, H. Chen, I. Marsic, and J. Tighe. Vidtr: Video transformer without convolutions. In ICCV, 2021."]}, {"table": "<p>MethodPretrainFramesTop-1 AccTop-5 AccViewsFLOPs (G)Params (M)ViViT-L Arnab et al. (2021)IN-21K+K-4001665.489.8-903352.1TimeSformer Bertasius et al. (2021)IN-21K859.574.93\\times 1196.7121.4TimeSformer-L Bertasius et al. (2021)IN-21K9662.481.03\\times 12380121.4TimeSformer-HR Bertasius et al. (2021)IN-21K1662.278.03\\times 11703121.4MViTv1-B Fan et al. (2021)K-4001664.789.23\\times 170.536.6MViTv1-B Fan et al. (2021)K-4003267.190.83\\times 117036.6MViTv1-B Fan et al. (2021)K-4006467.790.93\\times 145536.6Mformer-B Patrick et al. (2021)IN-21K+K-4001666.590.13\\times 1369.5109.1Mformer-L Patrick et al. (2021)IN-21K+K-4003268.191.23\\times 11185.1109.1Mformer-HR Patrick et al. (2021)IN-21K+K-4001667.190.63\\times 1958.8381.9ATA (Ours)IN-21K3267.091.03\\times 1792.9121.8ATA (Ours)IN-21K3267.190.83\\times 4792.9121.8</p>", "caption": "Table 2: Comparison to the state-of-the-art on Something-Something V2. IN-21K+K-400 denotes pretraining the video architecture on K-400 based on ImageNet Krizhevsky et al. (2012) pretrained image models.", "list_citation_info": ["Arnab et al. [2021] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid. Vivit: A video vision transformer. In ICCV, 2021.", "Patrick et al. [2021] M. Patrick, D. Campbell, Y. M. Asano, I. M. F. Metze, C. Feichtenhofer, A. Vedaldi, and J. F. Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. In NeurIPS, 2021.", "Bertasius et al. [2021] G. Bertasius, H. Wang, and L. Torresani. Is space-time attention all you need for video understanding? In ICML, 2021.", "Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 2012.", "Fan et al. [2021] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision transformers. In ICCV, 2021."]}, {"table": "<p>Image BackbonePretrainTemporal ModelingDe-alignmentTop-1 AccTop-5 AccFLOPs (G)Params (M)CycleMLP-B5 Chen et al. (2021)IN-1KAveraging Wang et al. (2016)-74.992.175.382.4Attention Bertasius et al. (2021)-76.893.1122.4102.8ATA (Ours)\u271772.790.9122.4102.8ATA (Ours)\u271377.793.5122.4102.8ConvNeXt-Base Liu et al. (2022a)IN-22KAveraging Wang et al. (2016)-79.094.0123.088.0Attention Bertasius et al. (2021)-80.194.8198.0140.5ATA (Ours)\u271776.092.7198.0140.5ATA (Ours)\u271380.594.8198.0140.5ViT-Base Dosovitskiy et al. (2021)IN-21KAveraging Wang et al. (2016)-76.092.6140.386.2Attention Bertasius et al. (2021)-78.093.7201.8121.8ATA (Ours)\u271779.394.3201.8121.8ATA (Ours)\u271379.694.3201.8121.8</p>", "caption": "Table 3: Ablation study of temporal modeling and de-alignment on Kinetics-400. For temporal modeling, averaging refers to computing the mean of all spatial-temporal features, and attention means factorized 1D temporal attention. The same below.", "list_citation_info": ["Wang et al. [2016] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016.", "Liu et al. [2022a] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In CVPR, 2022a.", "Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.", "Bertasius et al. [2021] G. Bertasius, H. Wang, and L. Torresani. Is space-time attention all you need for video understanding? In ICML, 2021.", "Chen et al. [2021] S. Chen, E. Xie, C. Ge, D. Liang, and P. Luo. Cyclemlp: A mlp-like architecture for dense prediction. ICLR, 2021."]}, {"table": "<p>Image BackbonePretrainTemporal ModelingKinetics-400Something-Something V2ViT-Base Dosovitskiy et al. (2021)IN-21KNone0.2430.295Averaging Wang et al. (2016)1.3550.869Attention Bertasius et al. (2021)1.3710.910ATA (Ours)1.3731.290</p>", "caption": "Table 4: Comparison of Mutual Information on Kinetics-400. None denotes the result calculated right after patch embedding. Mutual information is the average among each pair of adjacent frames.", "list_citation_info": ["Wang et al. [2016] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016.", "Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.", "Bertasius et al. [2021] G. Bertasius, H. Wang, and L. Torresani. Is space-time attention all you need for video understanding? In ICML, 2021."]}, {"table": "<p>Image BackboneTemporal ModelingComplexityTop-1 AccTop-5 AccFLOPs (G)Params (M)ViT-Base Dosovitskiy et al. (2021)Spatial Attention Bertasius et al. (2021)O(TH^{3}W^{3})76.092.6140.386.2Joint Attention Bertasius et al. (2021)O(T^{3}H^{3}W^{3})77.492.8179.786.2Factorized Attention Bertasius et al. (2021)O(TH^{3}W^{3}+T^{3}HW)78.094.3201.8121.8ATA (Ours)O(TH^{3}W^{3}+T^{3}HW)79.694.3201.8121.8</p>", "caption": "Table 6: Ablation study of complexity on Kinetics-400. T, H, and W represent the temporal and spatial sizes of the input feature. Complexities are calculated for each encoder block.", "list_citation_info": ["Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.", "Bertasius et al. [2021] G. Bertasius, H. Wang, and L. Torresani. Is space-time attention all you need for video understanding? In ICML, 2021."]}], "citation_info_to_title": {"Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 2012.": "Imagenet classification with deep convolutional neural networks", "Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale", "Neimark et al. [2021] D. Neimark, O. Bar, M. Zohar, and D. Asselmann. Video transformer network. In ICCV, 2021.": "Video Transformer Network", "Patrick et al. [2021] M. Patrick, D. Campbell, Y. M. Asano, I. M. F. Metze, C. Feichtenhofer, A. Vedaldi, and J. F. Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. In NeurIPS, 2021.": "Keeping your eye on the ball: Trajectory attention in video transformers", "Liu et al. [2022a] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In CVPR, 2022a.": "A convnet for the 2020s", "Arnab et al. [2021] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid. Vivit: A video vision transformer. In ICCV, 2021.": "Vivit: A video vision transformer", "Fan et al. [2021] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer. Multiscale vision transformers. In ICCV, 2021.": "Multiscale Vision Transformers", "Wang et al. [2016] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016.": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition", "Zhang et al. [2021] Y. Zhang, X. Li, C. Liu, B. Shuai, Y. Zhu, B. Brattoli, H. Chen, I. Marsic, and J. Tighe. Vidtr: Video transformer without convolutions. In ICCV, 2021.": "Vidtr: Video transformer without convolutions", "Chen et al. [2021] S. Chen, E. Xie, C. Ge, D. Liang, and P. Luo. Cyclemlp: A mlp-like architecture for dense prediction. ICLR, 2021.": "CycleMLP: A MLP-like Architecture for Dense Prediction", "Bertasius et al. [2021] G. Bertasius, H. Wang, and L. Torresani. Is space-time attention all you need for video understanding? In ICML, 2021.": "Is space-time attention all you need for video understanding?"}, "source_title_to_arxiv_id": {"A convnet for the 2020s": "2201.03545", "Vidtr: Video transformer without convolutions": "2104.11746"}}