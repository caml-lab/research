{"title": "Robust Deepfake On Unrestricted Media: Generation And Detection", "abstract": "Recent advances in deep learning have led to substantial improvements in\ndeepfake generation, resulting in fake media with a more realistic appearance.\nAlthough deepfake media have potential application in a wide range of areas and\nare drawing much attention from both the academic and industrial communities,\nit also leads to serious social and criminal concerns. This chapter explores\nthe evolution of and challenges in deepfake generation and detection. It also\ndiscusses possible ways to improve the robustness of deepfake detection for a\nwide variety of media (e.g., in-the-wild images and videos). Finally, it\nsuggests a focus for future fake media research.", "authors": ["Trung-Nghia Le", "Huy H Nguyen", "Junichi Yamagishi", "Isao Echizen"], "published_date": "2022_02_13", "pdf_url": "http://arxiv.org/pdf/2202.06228v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>2016</th><th>2017</th><th>2018</th><th>2019</th><th>2020</th><th>2021</th></tr></thead><tbody><tr><td>Face2Face [5]</td><td>Deepfakes [2]</td><td>FaceSwapGAN [3]</td><td>ZAO [14]</td><td>Reface [13]</td><td>Zhu et al. [18]</td></tr><tr><td></td><td>FaceApp [12]</td><td>ProGAN [19]</td><td>StyleGAN [20]</td><td>DeepFaceLab [21]</td><td>Le et al. [22]</td></tr><tr><td></td><td>Korshunova et al. [23]</td><td>StarGAN [24]</td><td>NeuralTextures [6]</td><td>StyleGANv2 [25]</td><td></td></tr><tr><td></td><td></td><td>ReenactGAN [26]</td><td>GANimation [27]</td><td>StarGANv2 [28]</td><td></td></tr><tr><td></td><td></td><td>RsGAN [29]</td><td>FSGAN [7]</td><td>InterFaceGAN [30]</td><td></td></tr><tr><td></td><td></td><td>X2Face [31]</td><td>Zhang et al. [32]</td><td>StyleALAE [33]</td><td></td></tr><tr><td></td><td></td><td>FSNet [34]</td><td>Egor et al. [11]</td><td>FaceShifter [4]</td><td></td></tr><tr><td></td><td></td><td>Zhixin et al. [8]</td><td></td><td>ICface [35]</td><td></td></tr><tr><td></td><td></td><td>Kim et al. [36]</td><td></td><td>FaR-GAN [37]</td><td></td></tr></tbody></table>", "caption": "Table 1: Evolution of deepfake generation methods 2016\u20132021.", "list_citation_info": ["[27] A. Pumarola, A. Agudo, A. Martinez, A. Sanfeliu, and F. Moreno-Noguer, \u201cGanimation: One-shot anatomically consistent facial animation,\u201d International Journal of Computer Vision, 2019.", "[6] J. Thies, M. Zollh\u00f6fer, and M. Nie\u00dfner, \u201cDeferred neural rendering: Image synthesis using neural textures,\u201d ACM Transactions on Graphics, vol. 38, no. 4, 2019.", "[23] I. Korshunova, W. Shi, J. Dambre, and L. Theis, \u201cFast face-swap using convolutional neural networks,\u201d in International Conference on Computer Vision, 2017, pp. 3677\u20133685.", "[37] H. Hao, S. Baireddy, A. R. Reibman, and E. J. Delp, \u201cFar-gan for one-shot face reenactment,\u201d in Conference on Computer Vision and Pattern Recognition Workshops, 2020.", "[8] Z. Shu, M. Sahasrabudhe, R. A. Guler, D. Samaras, N. Paragios, and I. Kokkinos, \u201cDeforming autoencoders: Unsupervised disentangling of shape and appearance,\u201d in European Conference on Computer Vision, September 2018.", "[21] I. Perov, D. Gao, N. Chervoniy, K. Liu, S. Marangonda, C. Um\u00e9, M. Dpfks, C. S. Facenheim, L. RP, J. Jiang, S. Zhang, P. Wu, B. Zhou, and W. Zhang, \u201cDeepfacelab: A simple, flexible and extensible face swapping framework,\u201d Arxiv Pre-print Arxiv:2005.05535, 2020.", "[12] \u201cFaceapp - face editor, makeover and beauty app,\u201d https://www.faceapp.com/, 2017, [Online; accessed 18-Feb-2021].", "[22] T.-N. Le, H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cOpenforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild,\u201d in International Conference on Computer Vision, 2021.", "[31] O. Wiles, A. Koepke, and A. Zisserman, \u201cX2face: A network for controlling face generation by using images, audio, and pose codes,\u201d in European Conference on Computer Vision, 2018.", "[26] W. Wu, Y. Zhang, C. Li, C. Qian, and C. C. Loy, \u201cReenactgan: Learning to reenact faces via boundary transfer,\u201d in European Conference on Computer Vision, 2018.", "[18] Y. Zhu, Q. Li, J. Wang, C.-Z. Xu, and Z. Sun, \u201cOne shot face swapping on megapixels,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 4834\u20134844.", "[5] J. Thies, M. Zollh\u00f6fer, M. Stamminger, C. Theobalt, and M. Nie\u00dfner, \u201cFace2face: Real-time face capture and reenactment of rgb videos,\u201d in Computer Vision and Pattern Recognition, 2016.", "[14] \u201cZao app deepfake,\u201d https://zaodownload.com/, 2019, [Online; accessed 20-July-2021].", "[33] S. Pidhorskyi, D. A. Adjeroh, and G. Doretto, \u201cAdversarial latent autoencoders,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[36] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Niessner, P. P\u00e9rez, C. Richardt, M. Zollh\u00f6fer, and C. Theobalt, \u201cDeep video portraits,\u201d ACM Trans. Graph., vol. 37, no. 4, 2018.", "[3] \u201cA denoising autoencoder, adversarial losses and attention mechanisms for face swapping,\u201d https://github.com/shaoanlu/faceswap-GAN, 2018, [Online; accessed 18-Feb-2021].", "[24] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, \u201cStargan: Unified generative adversarial networks for multi-domain image-to-image translation,\u201d in Conference on Computer Vision and Pattern Recognition, 2018.", "[13] \u201cReface,\u201d https://hey.reface.ai/, 2020, [Online; accessed 18-Feb-2021].", "[30] Y. Shen, J. Gu, X. Tang, and B. Zhou, \u201cInterpreting the latent space of gans for semantic face editing,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[25] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, \u201cAnalyzing and improving the image quality of StyleGAN,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[19] T. Karras, T. Aila, S. Laine, and J. Lehtinen, \u201cProgressive growing of gans for improved quality, stability, and variation,\u201d in International Conference on Learning Representations, 2018.", "[11] E. Zakharov, A. Shysheya, E. Burkov, and V. Lempitsky, \u201cFew-shot adversarial learning of realistic neural talking head models,\u201d in International Conference on Computer Vision, 2019.", "[4] L. Li, J. Bao, H. Yang, D. Chen, and F. Wen, \u201cFaceshifter: Towards high fidelity and occlusion aware face swapping,\u201d in Conference on Computer Vision and Pattern Recognition, 2019.", "[2] \u201cDeepfakes software for all,\u201d https://github.com/deepfakes/faceswap, 2017, [Online; accessed 18-Feb-2021].", "[7] Y. Nirkin, Y. Keller, and T. Hassner, \u201cFSGAN: Subject agnostic face swapping and reenactment,\u201d in International Conference on Computer Vision, 2019, pp. 7184\u20137193.", "[28] Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha, \u201cStargan v2: Diverse image synthesis for multiple domains,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[35] S. Tripathy, J. Kannala, and E. Rahtu, \u201cIcface: Interpretable and controllable face reenactment using gans,\u201d in Winter Conference on Applications of Computer Vision, 2020.", "[32] Y. Zhang, S. Zhang, Y. He, C. Li, C. C. Loy, and Z. Liu, \u201cOne-shot face reenactment,\u201d in British Machine Vision Conference, 2019.", "[34] R. Natsume, T. Yatagawa, and S. Morishima, \u201cFsnet: An identity-aware generative model for image-based face swapping,\u201d in Asian Conference on Computer Vision, 2018.", "[29] R. Natsume, T. Yatagawa, and S. Morishima, \u201cRsgan: face swapping and editing using face and hair representation in latent spaces,\u201d in SIGGRAPH, 2018.", "[20] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d in Conference on Computer Vision and Pattern Recognition, 2019, pp. 4401\u20134410."]}, {"table": "<table><thead><tr><th colspan=\"4\">Conventional Approach</th><th>End-to-end Approach</th></tr><tr><th>Data-Driven</th><th>Visual Artifacts</th><th>Frequency Domain</th><th>Attention &amp; Segmentation</th><th></th></tr></thead><tbody><tr><td>Zhou et al. [49] \\bigstar</td><td>Li et al. [50] \\bigstar</td><td>Li et al. [51] \\bigstar</td><td>Nguyen et al. [52] \\bigstar,\\spadesuit</td><td>Zhou et al. [53] \\bigstar,\\clubsuit</td></tr><tr><td>Afchar et al. [54] \\bigstar</td><td>Matern et al. [55] \\bigstar</td><td>Liu et al. [56] \\bigstar</td><td>Dang et al. [57] \\bigstar</td><td>Le et al. [22] \\bigstar,\\clubsuit,\\spadesuit</td></tr><tr><td>Nguyen et al. [58] \\bigstar</td><td>Yuezun et al. [59] \\bigstar</td><td></td><td>Li et al. [48] \\bigstar,\\spadesuit</td><td></td></tr><tr><td>Rossler et al. [60] \\bigstar</td><td>Yang et al. [61] \\bigstar</td><td></td><td>Bojia et al. [62] \\bigstar</td><td></td></tr><tr><td>Wang et al. [63] \\bigstar</td><td>Haliassos et al. [64] \\bigstar</td><td></td><td>Wang et al. [65] \\bigstar</td><td></td></tr><tr><td></td><td>Zhu et al. [66] \\bigstar</td><td></td><td></td><td></td></tr></tbody></table>", "caption": "Table 2: Categorization of deepfake detection methods. \\bigstar,\\clubsuit, and \\spadesuit indicate classification, detection, and segmentation tasks, respectively. Methods that can process multiple faces are shown in blue.", "list_citation_info": ["[58] H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cCapsule-forensics: Using capsule networks to detect forged images and videos,\u201d in International Conference on Acoustics, Speech and Signal Processing, 2019.", "[52] H. H. Nguyen, F. Fang, J. Yamagishi, and I. Echizen, \u201cMulti-task learning for detecting and segmenting manipulated facial images and videos,\u201d in International Conference on Biometrics: Theory, Applications and Systems, 2019.", "[48] L. Li, J. Bao, T. Zhang, H. Yang, D. Chen, F. Wen, and B. Guo, \u201cFace x-ray for more general face forgery detection,\u201d in Conference on Computer Vision and Pattern Recognition, June 2020.", "[65] C. Wang and W. Deng, \u201cRepresentative forgery mining for fake face detection,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 14\u2009923\u201314\u2009932.", "[22] T.-N. Le, H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cOpenforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild,\u201d in International Conference on Computer Vision, 2021.", "[60] A. R\u00f6ssler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Niessner, \u201cFaceforensics++: Learning to detect manipulated facial images,\u201d in International Conference on Computer Vision, Oct 2019, pp. 1\u201311.", "[66] X. Zhu, H. Wang, H. Fei, Z. Lei, and S. Z. Li, \u201cFace forgery detection by 3d decomposition,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 2929\u20132939.", "[50] Y. Li, M. Chang, and S. Lyu, \u201cIn ictu oculi: Exposing ai created fake videos by detecting eye blinking,\u201d in International Workshop on Information Forensics and Security, 2018, pp. 1\u20137.", "[59] Y. Li and S. Lyu, \u201cExposing deepfake videos by detecting face warping artifacts,\u201d in Conference on Computer Vision and Pattern Recognition Workshops, 2019.", "[51] J. Li, H. Xie, J. Li, Z. Wang, and Y. Zhang, \u201cFrequency-aware discriminative feature learning supervised by single-center loss for face forgery detection,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 6458\u20136467.", "[57] H. Dang, F. Liu, J. Stehouwer, X. Liu, and A. K. Jain, \u201cOn the detection of digital face manipulation,\u201d in Conference on Computer Vision and Pattern recognition, 2020, pp. 5781\u20135790.", "[54] D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen, \u201cMesonet: a compact facial video forgery detection network,\u201d in International Workshop on Information Forensics and Security, 2018, pp. 1\u20137.", "[61] X. Yang, Y. Li, and S. Lyu, \u201cExposing deep fakes using inconsistent head poses,\u201d in International Conference on Acoustics, Speech and Signal Processing, 2019, pp. 8261\u20138265.", "[56] H. Liu, X. Li, W. Zhou, Y. Chen, Y. He, H. Xue, W. Zhang, and N. Yu, \u201cSpatial-phase shallow learning: rethinking face forgery detection in frequency domain,\u201d in Conference on Computer Vision and Pattern Recognition, 2021, pp. 772\u2013781.", "[53] T. Zhou, W. Wang, Z. Liang, and J. Shen, \u201cFace forensics in the wild,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 5778\u20135788.", "[55] F. Matern, C. Riess, and M. Stamminger, \u201cExploiting visual artifacts to expose deepfakes and face manipulations,\u201d in Winter Applications of Computer Vision Workshops, 2019, pp. 83\u201392.", "[49] P. Zhou, X. Han, V. I. Morariu, and L. S. Davis, \u201cTwo-stream neural networks for tampered face detection,\u201d in Conference on Computer Vision and Pattern Recognition Workshops, 2017, pp. 1831\u20131839.", "[64] A. Haliassos, K. Vougioukas, S. Petridis, and M. Pantic, \u201cLips don\u2019t lie: A generalisable and robust approach to face forgery detection,\u201d in Conference on Computer Vision and Pattern Recognition, 2021, pp. 5039\u20135049.", "[63] R. Wang, F. Juefei-Xu, L. Ma, X. Xie, Y. Huang, J. Wang, and Y. Liu, \u201cFakespotter: A simple yet robust baseline for spotting ai-synthesized fake faces,\u201d in International Joint Conference on Artificial Intelligence, 2020, pp. 3444\u20133451.", "[62] B. Zi, M. Chang, J. Chen, X. Ma, and Y.-G. Jiang, \u201cWilddeepfake: A challenging real-world dataset for deepfake detection,\u201d in International Conference on Multimedia, 2020, p. 2382\u20132390."]}, {"table": "<table><thead><tr><th>Ranking</th><th>Team</th><th>Overall Log Loss</th></tr></thead><tbody><tr><td>1</td><td>Selim Seferbekov</td><td>0.4279</td></tr><tr><td>2</td><td>WM</td><td>0.4284</td></tr><tr><td>3</td><td>NTechLab</td><td>0.4345</td></tr><tr><td>4</td><td>Eighteen Years Old</td><td>0.4347</td></tr><tr><td>5</td><td>The Medics</td><td>0.4371</td></tr></tbody></table>", "caption": "Table 4: Top-ranked teams in DFDC Challenge 2020 [72].", "list_citation_info": ["[72] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang, and C. C. Ferrer, \u201cThe deepfake detection challenge dataset,\u201d arXiv preprint arXiv:2006.07397, 2020."]}, {"table": "<table><thead><tr><th>Ranking</th><th>Team</th><th>Binary Cross-Entropy Loss</th></tr></thead><tbody><tr><td>1</td><td>Forensics</td><td>0.2674</td></tr><tr><td>2</td><td>RealFace</td><td>0.3699</td></tr><tr><td>3</td><td>VISG</td><td>0.4060</td></tr><tr><td>4</td><td>jiashangplus</td><td>0.4064</td></tr><tr><td>5</td><td>Miao</td><td>0.4132</td></tr></tbody></table>", "caption": "Table 5: Top-ranked teams in DeeperForensics Challenge 2020 [92].", "list_citation_info": ["[92] L. Jiang, Z. Guo, W. Wu, Z. Liu, Z. Liu, C. C. Loy, S. Yang, Y. Xiong, W. Xia, B. Chen, P. Zhuang, S. Li, S. Chen, T. Yao, S. Ding, J. Li, F. Huang, L. Cao, R. Ji, C. Lu, and G. Tan, \u201cDeeperForensics Challenge 2020 on real-world face forgery detection: Methods and results,\u201d arXiv preprint arXiv:2102.09471, 2021."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"2\">Detection</th><th colspan=\"2\">Segmentation</th></tr><tr><th>Method</th><th>Year</th><th>Seen Images</th><th>Unseen Images</th><th>Seen Images</th><th>Unseen Images</th></tr></thead><tbody><tr><th>Mask R-CNN [97]</th><th>2017</th><td>79.2</td><td>42.1</td><td>83.6</td><td>43.7</td></tr><tr><th>MS R-CNN [98]</th><th>2019</th><td>79.0</td><td>42.2</td><td>85.1</td><td>43.3</td></tr><tr><th>RetinaMask [99]</th><th>2019</th><td>80.0</td><td>48.5</td><td>82.8</td><td>48.0</td></tr><tr><th>YOLACT [100]</th><th>2019</th><td>68.1</td><td>49.4</td><td>72.5</td><td>51.8</td></tr><tr><th>YOLACT++ [101]</th><th>2020</th><td>72.9</td><td>53.7</td><td>77.3</td><td>54.7</td></tr><tr><th>CenterMask [102]</th><th>2020</th><td>85.5</td><td>0.03</td><td>87.2</td><td>0.02</td></tr><tr><th>BlendMask [103]</th><th>2020</th><td>87.0</td><td>53.9</td><td>89.2</td><td>54.0</td></tr><tr><th>PolarMask [104]</th><th>2020</th><td>85.0</td><td>51.7</td><td>85.0</td><td>52.7</td></tr><tr><th>MEInst [105]</th><th>2020</th><td>82.8</td><td>46.1</td><td>82.2</td><td>46.0</td></tr><tr><th>CondInst [106]</th><th>2020</th><td>84.0</td><td>52.7</td><td>87.7</td><td>54.1</td></tr></tbody></table>", "caption": "Table 6: OpenForensics benchmark for multi-face forgery detection and segmentation [22]. Seen/Unseen stand for seen/unseen manipulated image generation methods.", "list_citation_info": ["[98] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, \u201cMask Scoring R-CNN,\u201d in Conference on Computer Vision and Pattern Recognition, 2019.", "[97] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \u201cMask r-cnn,\u201d in International Conference on Computer Vision, 2017, pp. 2980\u20132988.", "[106] Z. Tian, C. Shen, and H. Chen, \u201cConditional convolutions for instance segmentation,\u201d in European Conference on Computer Vision, 2020.", "[99] C.-Y. Fu, M. Shvets, and A. C. Berg, \u201cRetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free,\u201d in arXiv preprint arXiv:1901.03353, 2019.", "[100] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYolact: Real-time instance segmentation,\u201d in International Conference on Computer Vision, 2019.", "[102] Y. Lee and J. Park, \u201cCentermask: Real-time anchor-free instance segmentation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[103] H. Chen, K. Sun, Z. Tian, C. Shen, Y. Huang, and Y. Yan, \u201cBlendmask: Top-down meets bottom-up for instance segmentation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[22] T.-N. Le, H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cOpenforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild,\u201d in International Conference on Computer Vision, 2021.", "[104] E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, and P. Luo, \u201cPolarmask: Single shot instance segmentation with polar representation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[105] R. Zhang, Z. Tian, C. Shen, M. You, and Y. Yan, \u201cMask encoding for single shot instance segmentation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.", "[101] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYolact++: Better real-time instance segmentation,\u201d Transactions on Pattern Analysis and Machine Intelligence, 2020."]}, {"table": "<table><thead><tr><th>Adversarial Example</th><th colspan=\"2\">Victim Model</th></tr><tr><th>Attack Method</th><th>XceptionNet [60]</th><th>EfficientNet-B4 [72]</th></tr></thead><tbody><tr><th>None</th><td>99.22</td><td>99.58</td></tr><tr><th>FGSM [108]</th><td>76.98</td><td>19.76</td></tr><tr><th>BIM [109]</th><td>10.92</td><td>8.39</td></tr><tr><th>PSD [110]</th><td>8.19</td><td>12.96</td></tr></tbody></table>", "caption": "Table 7: Performance of deepfake classifiers against adversarial attacks evaluated on OpenForensics dataset [22].", "list_citation_info": ["[108] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d in International Conference on Learning Representations, 2015.", "[110] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d in International Conference on Learning Representations, 2018.", "[109] A. Kurakin, I. Goodfellow, and S. Bengio, \u201cAdversarial examples in the physical world,\u201d in International Conference on Learning Representations Workshop, 2016.", "[72] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang, and C. C. Ferrer, \u201cThe deepfake detection challenge dataset,\u201d arXiv preprint arXiv:2006.07397, 2020.", "[22] T.-N. Le, H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cOpenforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild,\u201d in International Conference on Computer Vision, 2021.", "[60] A. R\u00f6ssler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Niessner, \u201cFaceforensics++: Learning to detect manipulated facial images,\u201d in International Conference on Computer Vision, Oct 2019, pp. 1\u201311."]}, {"table": "<table><thead><tr><th>Victim</th><th colspan=\"3\">Defense Solution</th></tr><tr><th>Model</th><th>None</th><th>For Seen Generators</th><th>For Unseen Generators</th></tr></thead><tbody><tr><th>XceptionNet [60]</th><td>75.60</td><td>98.68</td><td>78.93</td></tr><tr><th>EfficientNet-B4 [72]</th><td>83.89</td><td>93.67</td><td>85.99</td></tr><tr><th>CapsuleNet-R50 [58]</th><td>73.07</td><td>95.99</td><td>76.30</td></tr></tbody></table>", "caption": "Table 8: Performance of defense solutions for deepfake classifiers against seen/unseen scenarios in OpenForensics dataset [22].", "list_citation_info": ["[72] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang, and C. C. Ferrer, \u201cThe deepfake detection challenge dataset,\u201d arXiv preprint arXiv:2006.07397, 2020.", "[58] H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cCapsule-forensics: Using capsule networks to detect forged images and videos,\u201d in International Conference on Acoustics, Speech and Signal Processing, 2019.", "[60] A. R\u00f6ssler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Niessner, \u201cFaceforensics++: Learning to detect manipulated facial images,\u201d in International Conference on Computer Vision, Oct 2019, pp. 1\u201311.", "[22] T.-N. Le, H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cOpenforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild,\u201d in International Conference on Computer Vision, 2021."]}], "citation_info_to_title": {"[34] R. Natsume, T. Yatagawa, and S. Morishima, \u201cFsnet: An identity-aware generative model for image-based face swapping,\u201d in Asian Conference on Computer Vision, 2018.": "Fsnet: An identity-aware generative model for image-based face swapping", "[3] \u201cA denoising autoencoder, adversarial losses and attention mechanisms for face swapping,\u201d https://github.com/shaoanlu/faceswap-GAN, 2018, [Online; accessed 18-Feb-2021].": "A denoising autoencoder, adversarial losses and attention mechanisms for face swapping", "[23] I. Korshunova, W. Shi, J. Dambre, and L. Theis, \u201cFast face-swap using convolutional neural networks,\u201d in International Conference on Computer Vision, 2017, pp. 3677\u20133685.": "Fast face-swap using convolutional neural networks", "[97] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \u201cMask r-cnn,\u201d in International Conference on Computer Vision, 2017, pp. 2980\u20132988.": "Mask R-CNN", "[25] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, \u201cAnalyzing and improving the image quality of StyleGAN,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Analyzing and improving the image quality of StyleGAN", "[58] H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cCapsule-forensics: Using capsule networks to detect forged images and videos,\u201d in International Conference on Acoustics, Speech and Signal Processing, 2019.": "Capsule-forensics: Using capsule networks to detect forged images and videos", "[55] F. Matern, C. Riess, and M. Stamminger, \u201cExploiting visual artifacts to expose deepfakes and face manipulations,\u201d in Winter Applications of Computer Vision Workshops, 2019, pp. 83\u201392.": "Exploiting visual artifacts to expose deepfakes and face manipulations", "[51] J. Li, H. Xie, J. Li, Z. Wang, and Y. Zhang, \u201cFrequency-aware discriminative feature learning supervised by single-center loss for face forgery detection,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 6458\u20136467.": "Frequency-aware discriminative feature learning supervised by single-center loss for face forgery detection", "[100] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYolact: Real-time instance segmentation,\u201d in International Conference on Computer Vision, 2019.": "Yolact: Real-time instance segmentation", "[63] R. Wang, F. Juefei-Xu, L. Ma, X. Xie, Y. Huang, J. Wang, and Y. Liu, \u201cFakespotter: A simple yet robust baseline for spotting ai-synthesized fake faces,\u201d in International Joint Conference on Artificial Intelligence, 2020, pp. 3444\u20133451.": "Fakespotter: A simple yet robust baseline for spotting ai-synthesized fake faces", "[110] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, \u201cTowards deep learning models resistant to adversarial attacks,\u201d in International Conference on Learning Representations, 2018.": "Towards deep learning models resistant to adversarial attacks", "[7] Y. Nirkin, Y. Keller, and T. Hassner, \u201cFSGAN: Subject agnostic face swapping and reenactment,\u201d in International Conference on Computer Vision, 2019, pp. 7184\u20137193.": "FSGAN: Subject agnostic face swapping and reenactment", "[14] \u201cZao app deepfake,\u201d https://zaodownload.com/, 2019, [Online; accessed 20-July-2021].": "Zao app deepfake", "[33] S. Pidhorskyi, D. A. Adjeroh, and G. Doretto, \u201cAdversarial latent autoencoders,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Adversarial Latent Autoencoders", "[65] C. Wang and W. Deng, \u201cRepresentative forgery mining for fake face detection,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 14\u2009923\u201314\u2009932.": "Representative forgery mining for fake face detection", "[48] L. Li, J. Bao, T. Zhang, H. Yang, D. Chen, F. Wen, and B. Guo, \u201cFace x-ray for more general face forgery detection,\u201d in Conference on Computer Vision and Pattern Recognition, June 2020.": "Face x-ray for more general face forgery detection", "[99] C.-Y. Fu, M. Shvets, and A. C. Berg, \u201cRetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free,\u201d in arXiv preprint arXiv:1901.03353, 2019.": "RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free", "[4] L. Li, J. Bao, H. Yang, D. Chen, and F. Wen, \u201cFaceshifter: Towards high fidelity and occlusion aware face swapping,\u201d in Conference on Computer Vision and Pattern Recognition, 2019.": "Faceshifter: Towards high fidelity and occlusion aware face swapping", "[62] B. Zi, M. Chang, J. Chen, X. Ma, and Y.-G. Jiang, \u201cWilddeepfake: A challenging real-world dataset for deepfake detection,\u201d in International Conference on Multimedia, 2020, p. 2382\u20132390.": "Wilddeepfake: A challenging real-world dataset for deepfake detection", "[36] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Niessner, P. P\u00e9rez, C. Richardt, M. Zollh\u00f6fer, and C. Theobalt, \u201cDeep video portraits,\u201d ACM Trans. Graph., vol. 37, no. 4, 2018.": "Deep video portraits", "[52] H. H. Nguyen, F. Fang, J. Yamagishi, and I. Echizen, \u201cMulti-task learning for detecting and segmenting manipulated facial images and videos,\u201d in International Conference on Biometrics: Theory, Applications and Systems, 2019.": "Multi-task learning for detecting and segmenting manipulated facial images and videos", "[103] H. Chen, K. Sun, Z. Tian, C. Shen, Y. Huang, and Y. Yan, \u201cBlendmask: Top-down meets bottom-up for instance segmentation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Blendmask: Top-down meets bottom-up for instance segmentation", "[5] J. Thies, M. Zollh\u00f6fer, M. Stamminger, C. Theobalt, and M. Nie\u00dfner, \u201cFace2face: Real-time face capture and reenactment of rgb videos,\u201d in Computer Vision and Pattern Recognition, 2016.": "Face2face: Real-time face capture and reenactment of RGB videos", "[13] \u201cReface,\u201d https://hey.reface.ai/, 2020, [Online; accessed 18-Feb-2021].": "Reface", "[31] O. Wiles, A. Koepke, and A. Zisserman, \u201cX2face: A network for controlling face generation by using images, audio, and pose codes,\u201d in European Conference on Computer Vision, 2018.": "X2face: A network for controlling face generation by using images, audio, and pose codes", "[11] E. Zakharov, A. Shysheya, E. Burkov, and V. Lempitsky, \u201cFew-shot adversarial learning of realistic neural talking head models,\u201d in International Conference on Computer Vision, 2019.": "Few-shot adversarial learning of realistic neural talking head models", "[64] A. Haliassos, K. Vougioukas, S. Petridis, and M. Pantic, \u201cLips don\u2019t lie: A generalisable and robust approach to face forgery detection,\u201d in Conference on Computer Vision and Pattern Recognition, 2021, pp. 5039\u20135049.": "Lips don\u2019t lie: A generalisable and robust approach to face forgery detection", "[2] \u201cDeepfakes software for all,\u201d https://github.com/deepfakes/faceswap, 2017, [Online; accessed 18-Feb-2021].": "Deepfakes software for all", "[102] Y. Lee and J. Park, \u201cCentermask: Real-time anchor-free instance segmentation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Centermask: Real-time anchor-free instance segmentation", "[28] Y. Choi, Y. Uh, J. Yoo, and J.-W. Ha, \u201cStargan v2: Diverse image synthesis for multiple domains,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Stargan v2: Diverse image synthesis for multiple domains", "[106] Z. Tian, C. Shen, and H. Chen, \u201cConditional convolutions for instance segmentation,\u201d in European Conference on Computer Vision, 2020.": "Conditional convolutions for instance segmentation", "[6] J. Thies, M. Zollh\u00f6fer, and M. Nie\u00dfner, \u201cDeferred neural rendering: Image synthesis using neural textures,\u201d ACM Transactions on Graphics, vol. 38, no. 4, 2019.": "Deferred neural rendering: Image synthesis using neural textures", "[21] I. Perov, D. Gao, N. Chervoniy, K. Liu, S. Marangonda, C. Um\u00e9, M. Dpfks, C. S. Facenheim, L. RP, J. Jiang, S. Zhang, P. Wu, B. Zhou, and W. Zhang, \u201cDeepfacelab: A simple, flexible and extensible face swapping framework,\u201d Arxiv Pre-print Arxiv:2005.05535, 2020.": "Deepfacelab: A simple, flexible and extensible face swapping framework", "[104] E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, and P. Luo, \u201cPolarmask: Single shot instance segmentation with polar representation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Polarmask: Single shot instance segmentation with polar representation", "[109] A. Kurakin, I. Goodfellow, and S. Bengio, \u201cAdversarial examples in the physical world,\u201d in International Conference on Learning Representations Workshop, 2016.": "Adversarial examples in the physical world", "[22] T.-N. Le, H. H. Nguyen, J. Yamagishi, and I. Echizen, \u201cOpenforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild,\u201d in International Conference on Computer Vision, 2021.": "Openforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild", "[57] H. Dang, F. Liu, J. Stehouwer, X. Liu, and A. K. Jain, \u201cOn the detection of digital face manipulation,\u201d in Conference on Computer Vision and Pattern recognition, 2020, pp. 5781\u20135790.": "On the detection of digital face manipulation", "[50] Y. Li, M. Chang, and S. Lyu, \u201cIn ictu oculi: Exposing ai created fake videos by detecting eye blinking,\u201d in International Workshop on Information Forensics and Security, 2018, pp. 1\u20137.": "In ictu oculi: Exposing ai created fake videos by detecting eye blinking", "[49] P. Zhou, X. Han, V. I. Morariu, and L. S. Davis, \u201cTwo-stream neural networks for tampered face detection,\u201d in Conference on Computer Vision and Pattern Recognition Workshops, 2017, pp. 1831\u20131839.": "Two-stream neural networks for tampered face detection", "[108] I. J. Goodfellow, J. Shlens, and C. Szegedy, \u201cExplaining and harnessing adversarial examples,\u201d in International Conference on Learning Representations, 2015.": "Explaining and harnessing adversarial examples", "[27] A. Pumarola, A. Agudo, A. Martinez, A. Sanfeliu, and F. Moreno-Noguer, \u201cGanimation: One-shot anatomically consistent facial animation,\u201d International Journal of Computer Vision, 2019.": "Ganimation: One-shot anatomically consistent facial animation", "[30] Y. Shen, J. Gu, X. Tang, and B. Zhou, \u201cInterpreting the latent space of gans for semantic face editing,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Interpreting the Latent Space of GANs for Semantic Face Editing", "[56] H. Liu, X. Li, W. Zhou, Y. Chen, Y. He, H. Xue, W. Zhang, and N. Yu, \u201cSpatial-phase shallow learning: rethinking face forgery detection in frequency domain,\u201d in Conference on Computer Vision and Pattern Recognition, 2021, pp. 772\u2013781.": "Spatial-phase shallow learning: rethinking face forgery detection in frequency domain", "[18] Y. Zhu, Q. Li, J. Wang, C.-Z. Xu, and Z. Sun, \u201cOne shot face swapping on megapixels,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 4834\u20134844.": "One shot face swapping on megapixels", "[59] Y. Li and S. Lyu, \u201cExposing deepfake videos by detecting face warping artifacts,\u201d in Conference on Computer Vision and Pattern Recognition Workshops, 2019.": "Exposing deepfake videos by detecting face warping artifacts", "[32] Y. Zhang, S. Zhang, Y. He, C. Li, C. C. Loy, and Z. Liu, \u201cOne-shot face reenactment,\u201d in British Machine Vision Conference, 2019.": "One-shot face reenactment", "[37] H. Hao, S. Baireddy, A. R. Reibman, and E. J. Delp, \u201cFar-gan for one-shot face reenactment,\u201d in Conference on Computer Vision and Pattern Recognition Workshops, 2020.": "Far-gan for one-shot face reenactment", "[19] T. Karras, T. Aila, S. Laine, and J. Lehtinen, \u201cProgressive growing of gans for improved quality, stability, and variation,\u201d in International Conference on Learning Representations, 2018.": "Progressive growing of GANs for improved quality, stability, and variation", "[8] Z. Shu, M. Sahasrabudhe, R. A. Guler, D. Samaras, N. Paragios, and I. Kokkinos, \u201cDeforming autoencoders: Unsupervised disentangling of shape and appearance,\u201d in European Conference on Computer Vision, September 2018.": "Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance", "[26] W. Wu, Y. Zhang, C. Li, C. Qian, and C. C. Loy, \u201cReenactgan: Learning to reenact faces via boundary transfer,\u201d in European Conference on Computer Vision, 2018.": "Reenactgan: Learning to reenact faces via boundary transfer", "[29] R. Natsume, T. Yatagawa, and S. Morishima, \u201cRsgan: face swapping and editing using face and hair representation in latent spaces,\u201d in SIGGRAPH, 2018.": "Rsgan: Face Swapping and Editing Using Face and Hair Representation in Latent Spaces", "[24] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, \u201cStargan: Unified generative adversarial networks for multi-domain image-to-image translation,\u201d in Conference on Computer Vision and Pattern Recognition, 2018.": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation", "[53] T. Zhou, W. Wang, Z. Liang, and J. Shen, \u201cFace forensics in the wild,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 5778\u20135788.": "Face forensics in the wild", "[60] A. R\u00f6ssler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Niessner, \u201cFaceforensics++: Learning to detect manipulated facial images,\u201d in International Conference on Computer Vision, Oct 2019, pp. 1\u201311.": "Faceforensics++: Learning to detect manipulated facial images", "[35] S. Tripathy, J. Kannala, and E. Rahtu, \u201cIcface: Interpretable and controllable face reenactment using gans,\u201d in Winter Conference on Applications of Computer Vision, 2020.": "Icface: Interpretable and Controllable Face Reenactment Using GANs", "[98] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, \u201cMask Scoring R-CNN,\u201d in Conference on Computer Vision and Pattern Recognition, 2019.": "Mask Scoring R-CNN", "[72] B. Dolhansky, J. Bitton, B. Pflaum, J. Lu, R. Howes, M. Wang, and C. C. Ferrer, \u201cThe deepfake detection challenge dataset,\u201d arXiv preprint arXiv:2006.07397, 2020.": "The Deepfake Detection Challenge Dataset", "[92] L. Jiang, Z. Guo, W. Wu, Z. Liu, Z. Liu, C. C. Loy, S. Yang, Y. Xiong, W. Xia, B. Chen, P. Zhuang, S. Li, S. Chen, T. Yao, S. Ding, J. Li, F. Huang, L. Cao, R. Ji, C. Lu, and G. Tan, \u201cDeeperForensics Challenge 2020 on real-world face forgery detection: Methods and results,\u201d arXiv preprint arXiv:2102.09471, 2021.": "DeeperForensics Challenge 2020 on real-world face forgery detection: Methods and results", "[54] D. Afchar, V. Nozick, J. Yamagishi, and I. Echizen, \u201cMesonet: a compact facial video forgery detection network,\u201d in International Workshop on Information Forensics and Security, 2018, pp. 1\u20137.": "Mesonet: a compact facial video forgery detection network", "[20] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d in Conference on Computer Vision and Pattern Recognition, 2019, pp. 4401\u20134410.": "A style-based generator architecture for generative adversarial networks", "[12] \u201cFaceapp - face editor, makeover and beauty app,\u201d https://www.faceapp.com/, 2017, [Online; accessed 18-Feb-2021].": "Faceapp - face editor, makeover and beauty app", "[105] R. Zhang, Z. Tian, C. Shen, M. You, and Y. Yan, \u201cMask encoding for single shot instance segmentation,\u201d in Conference on Computer Vision and Pattern Recognition, 2020.": "Mask Encoding for Single Shot Instance Segmentation", "[61] X. Yang, Y. Li, and S. Lyu, \u201cExposing deep fakes using inconsistent head poses,\u201d in International Conference on Acoustics, Speech and Signal Processing, 2019, pp. 8261\u20138265.": "Exposing deep fakes using inconsistent head poses", "[101] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYolact++: Better real-time instance segmentation,\u201d Transactions on Pattern Analysis and Machine Intelligence, 2020.": "Yolact++: Better real-time instance segmentation", "[66] X. Zhu, H. Wang, H. Fei, Z. Lei, and S. Z. Li, \u201cFace forgery detection by 3d decomposition,\u201d in Conference on Computer Vision and Pattern Recognition, June 2021, pp. 2929\u20132939.": "Face Forgery Detection by 3D Decomposition"}, "source_title_to_arxiv_id": {"Frequency-aware discriminative feature learning supervised by single-center loss for face forgery detection": "2103.09096", "Stargan v2: Diverse image synthesis for multiple domains": "1912.01865", "Openforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild": "2107.14480", "One shot face swapping on megapixels": "2105.04932", "Face forensics in the wild": "2103.16076"}}