{"title": "Exploring Discrete Diffusion Models for Image Captioning", "abstract": "The image captioning task is typically realized by an auto-regressive method\nthat decodes the text tokens one by one. We present a diffusion-based\ncaptioning model, dubbed the name DDCap, to allow more decoding flexibility.\nUnlike image generation, where the output is continuous and redundant with a\nfixed length, texts in image captions are categorical and short with varied\nlengths. Therefore, naively applying the discrete diffusion model to text\ndecoding does not work well, as shown in our experiments. To address the\nperformance gap, we propose several key techniques including best-first\ninference, concentrated attention mask, text length prediction, and image-free\ntraining. On COCO without additional caption pre-training, it achieves a CIDEr\nscore of 117.8, which is +5.0 higher than the auto-regressive baseline with the\nsame architecture in the controlled setting. It also performs +26.8 higher\nCIDEr score than the auto-regressive baseline (230.3 v.s.203.5) on a caption\ninfilling task. With 4M vision-language pre-training images and the base-sized\nmodel, we reach a CIDEr score of 125.1 on COCO, which is competitive to the\nbest well-developed auto-regressive frameworks. The code is available at\nhttps://github.com/buxiangzhiren/DDCap.", "authors": ["Zixin Zhu", "Yixuan Wei", "Jianfeng Wang", "Zhe Gan", "Zheng Zhang", "Le Wang", "Gang Hua", "Lijuan Wang", "Zicheng Liu", "Han Hu"], "published_date": "2022_11_21", "pdf_url": "http://arxiv.org/pdf/2211.11694v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>#Param.</td><td>#Images</td><td>C</td><td>B@4</td><td>M</td><td>S</td></tr><tr><th colspan=\"7\">Auto-regressive models</th></tr><tr><th>\\rm{UVLP} [84]</th><td>111.7M</td><td>4M</td><td>116.9</td><td>36.5</td><td>28.4</td><td>21.2</td></tr><tr><th>\\rm{MiniVLM} [68]</th><td>34.5M</td><td>14M</td><td>119.8</td><td>35.6</td><td>28.6</td><td>21.6</td></tr><tr><th>\\rm{DistillVLM} [14]</th><td>34.5M</td><td>7M</td><td>120.8</td><td>35.6</td><td>28.7</td><td>22.1</td></tr><tr><th>\\rm{UFO_{B}} [67]</th><td>0.1B</td><td>4M</td><td>122.8</td><td>36.0</td><td>28.9</td><td>22.2</td></tr><tr><th>\\rm{OSCAR_{B}} [40]</th><td>0.1B+64M{}^{\\dagger}</td><td>7M</td><td>123.7</td><td>36.5</td><td>30.3</td><td>23.1</td></tr><tr><th>\\rm{UNIMO_{B}} [39]</th><td>-</td><td>9M</td><td>124.4</td><td>38.8</td><td>-</td><td>-</td></tr><tr><th>ViTCap [13]</th><td>0.2B</td><td>4M</td><td>125.2</td><td>36.3</td><td>29.3</td><td>22.6</td></tr><tr><th>\\rm{VinVL_{B}} [81]</th><td>0.1B+0.2B{}^{\\dagger}</td><td>6M</td><td>129.3</td><td>38.2</td><td>30.3</td><td>23.6</td></tr><tr><th>\\rm{GIT_{B}} [69]</th><td>129M</td><td>4M</td><td>131.4</td><td>40.4</td><td>30.0</td><td>23.0</td></tr><tr><th>\\rm{LEMON_{B}} [28]</th><td>111.7M</td><td>0.2B</td><td>133.3</td><td>40.3</td><td>30.2</td><td>23.3</td></tr><tr><th>\\rm{SimVLM_{B}} [71]</th><td>-</td><td>1.8B</td><td>134.8</td><td>39.0</td><td>32.9</td><td>24.0</td></tr><tr><th colspan=\"7\">Non-autoregressive models</th></tr><tr><th>\\rm{MNIC} [19]</th><td>-</td><td>-</td><td>108.5</td><td>31.5</td><td>27.5</td><td>21.1</td></tr><tr><th>\\rm{NAIC_{B,KD}} [21]</th><td>-</td><td>-</td><td>115.5</td><td>35.3</td><td>27.3</td><td>20.8</td></tr><tr><th>\\rm{FNIC} [16]</th><td>-</td><td>-</td><td>115.7</td><td>36.2</td><td>27.1</td><td>20.2</td></tr><tr><th>\\rm{DDCap} (Ours)</th><td>280.1M</td><td>4M</td><td>125.1</td><td>37.1</td><td>29.1</td><td>22.7</td></tr></tbody></table>", "caption": "Table 5: Performance comparison on COCO captioning Karpathy [32] split with pretraining, where B@4, M, R, C denote BLEU@4, METEOR, ROUGE-L,CIDEr and SPICE scores. CIDEr optimization is not used for all models. (\\dagger) VinVL/OSCAR: the extra parameters are forobject detector. All of the results do not contain CIDEr optimization.", "list_citation_info": ["[19] Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shanshe Wang, Siwei Ma, and Wen Gao. Masked non-autoregressive image captioning. arXiv preprint arXiv:1906.00717, 2019.", "[39] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409, 2020.", "[68] Jianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946, 2020.", "[81] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. VinVL: Revisiting visual representations in vision-language models. In CVPR, 2021.", "[21] Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang, and Hanqing Lu. Non-autoregressive image captioning with counterfactuals-critical multi-agent learning. arXiv preprint arXiv:2005.04690, 2020.", "[84] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In AAAI, 2020.", "[28] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In CVPR, pages 17980\u201317989, 2022.", "[40] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, 2020.", "[67] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang, Xiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang. Ufo: A unified transformer for vision-language representation learning. arXiv preprint arXiv:2111.10023, 2021.", "[71] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.", "[16] Zheng-cong Fei. Fast image caption generation with position alignment. arXiv preprint arXiv:1912.06365, 2019.", "[69] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.", "[32] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:664\u2013676, 2017.", "[13] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lin Liang, Zhe Gan, Lijuan Wang, Yezhou Yang, and Zicheng Liu. Injecting semantic concepts into end-to-end image captioning. In CVPR, 2022.", "[14] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng Liu. Compressing visual-linguistic model via knowledge distillation. In ICCV, pages 1428\u20131438, 2021."]}], "citation_info_to_title": {"[13] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lin Liang, Zhe Gan, Lijuan Wang, Yezhou Yang, and Zicheng Liu. Injecting semantic concepts into end-to-end image captioning. In CVPR, 2022.": "Injecting Semantic Concepts into End-to-End Image Captioning", "[81] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. VinVL: Revisiting visual representations in vision-language models. In CVPR, 2021.": "VinVL: Revisiting visual representations in vision-language models", "[84] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In AAAI, 2020.": "Unified Vision-Language Pre-Training for Image Captioning and VQA", "[69] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.": "Git: A generative image-to-text transformer for vision and language", "[71] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.": "Simvlm: Simple visual language model pretraining with weak supervision", "[19] Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shanshe Wang, Siwei Ma, and Wen Gao. Masked non-autoregressive image captioning. arXiv preprint arXiv:1906.00717, 2019.": "Masked non-autoregressive image captioning", "[16] Zheng-cong Fei. Fast image caption generation with position alignment. arXiv preprint arXiv:1912.06365, 2019.": "Fast image caption generation with position alignment", "[32] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:664\u2013676, 2017.": "Deep visual-semantic alignments for generating image descriptions", "[21] Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang, and Hanqing Lu. Non-autoregressive image captioning with counterfactuals-critical multi-agent learning. arXiv preprint arXiv:2005.04690, 2020.": "Non-autoregressive image captioning with counterfactuals-critical multi-agent learning", "[28] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In CVPR, pages 17980\u201317989, 2022.": "Scaling up vision-language pre-training for image captioning", "[40] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, 2020.": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "[67] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang, Xiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang. Ufo: A unified transformer for vision-language representation learning. arXiv preprint arXiv:2111.10023, 2021.": "Ufo: A unified transformer for vision-language representation learning", "[14] Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng Liu. Compressing visual-linguistic model via knowledge distillation. In ICCV, pages 1428\u20131438, 2021.": "Compressing visual-linguistic model via knowledge distillation", "[68] Jianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiujun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. Minivlm: A smaller and faster vision-language model. arXiv preprint arXiv:2012.06946, 2020.": "Minivlm: A smaller and faster vision-language model", "[39] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409, 2020.": "Unimo: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"}, "source_title_to_arxiv_id": {"Injecting Semantic Concepts into End-to-End Image Captioning": "2112.05230", "Compressing visual-linguistic model via knowledge distillation": "2104.02096"}}