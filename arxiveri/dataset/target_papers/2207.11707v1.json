{"title": "Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes", "abstract": "This paper proposes a novel test-time adaptation strategy that adjusts the\nmodel pre-trained on the source domain using only unlabeled online data from\nthe target domain to alleviate the performance degradation due to the\ndistribution shift between the source and target domains. Adapting the entire\nmodel parameters using the unlabeled online data may be detrimental due to the\nerroneous signals from an unsupervised objective. To mitigate this problem, we\npropose a shift-agnostic weight regularization that encourages largely updating\nthe model parameters sensitive to distribution shift while slightly updating\nthose insensitive to the shift, during test-time adaptation. This\nregularization enables the model to quickly adapt to the target domain without\nperformance degradation by utilizing the benefit of a high learning rate. In\naddition, we present an auxiliary task based on nearest source prototypes to\nalign the source and target features, which helps reduce the distribution shift\nand leads to further performance improvement. We show that our method exhibits\nstate-of-the-art performance on various standard benchmarks and even\noutperforms its supervised counterpart.", "authors": ["Sungha Choi", "Seunghan Yang", "Seokeon Choi", "Sungrack Yun"], "published_date": "2022_07_24", "pdf_url": "http://arxiv.org/pdf/2207.11707v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Methods</th><th>VLCS</th><th>PACS</th><th>OfficeHome</th><th>Terra</th><th>Average</th></tr><tr><th>ERM{}^{\\dagger}</th><th>76.8\\pm1.0</th><th>83.3\\pm0.6</th><th>67.3\\pm0.3</th><th>46.2\\pm0.2</th><th>68.4</th></tr><tr><th>CORAL{}^{\\dagger}</th><th>77.0\\pm0.5</th><th>83.6\\pm0.6</th><th>68.6\\pm0.2</th><th>48.1\\pm1.3</th><th>69.3</th></tr></thead><tbody><tr><td>ERM</td><td>77.4\\pm0.9</td><td>83.5\\pm0.7</td><td>65.6\\pm0.4</td><td>47.1\\pm1.1</td><td>68.4</td></tr><tr><td>\u2004\u2004+T3A</td><td>79.4\\pm0.4</td><td>86.5\\pm0.3</td><td>67.8\\pm0.5</td><td>45.6\\pm0.7</td><td>69.8</td></tr><tr><td>\u2004\u2004+Ours</td><td>77.0\\pm0.5</td><td>88.9\\pm0.1</td><td>69.2\\pm0.1</td><td>49.5\\pm0.8</td><td>71.2</td></tr><tr><td>CORAL</td><td>77.9\\pm0.9</td><td>85.3\\pm0.1</td><td>67.8\\pm0.3</td><td>44.1\\pm0.4</td><td>68.8</td></tr><tr><td>\u2004\u2004+T3A</td><td>79.3\\pm0.3</td><td>86.3\\pm0.2</td><td>69.5\\pm0.2</td><td>45.4\\pm1.2</td><td>70.1</td></tr><tr><td>\u2004\u2004+Ours</td><td>78.7\\pm0.4</td><td>89.9\\pm0.1</td><td>71.0\\pm0.0</td><td>47.5\\pm0.6</td><td>71.8</td></tr></tbody></table>", "caption": "Table 6: Comparison of accuracy (\\%) on four DG benchmarks. {}^{\\dagger} denotes the reported results from DomainBed [18], and the others are reproduced values.", "list_citation_info": ["[18] Gulrajani, I., Lopez-Paz, D.: In search of lost domain generalization. In: International Conference on Learning Representations (ICLR) (2020)"]}, {"table": "<table><thead><tr><th>Datasets</th><th>Backbone</th><th>\u2006Color</th><th>+Gray.</th><th>+Inve.</th><th>+Blur.</th><th>+H.Fli.</th><th>+Crop.</th></tr></thead><tbody><tr><td rowspan=\"2\">CIFAR-100-C</td><td>WRN-40-2 [68, 23]</td><td>33.23</td><td>33.04</td><td>32.99</td><td>32.71</td><td>32.79</td><td>33.31</td></tr><tr><td>ResNet-50 [21]</td><td>37.61</td><td>36.58</td><td>36.15</td><td>35.65</td><td>35.70</td><td>35.94</td></tr><tr><td rowspan=\"4\">CIFAR-10-C</td><td>WRN-40-2</td><td>10.97</td><td>10.76</td><td>10.63</td><td>10.37</td><td>10.31</td><td>10.68</td></tr><tr><td>WRN-28-10 [68]</td><td>16.73</td><td>16.48</td><td>16.24</td><td>15.70</td><td>15.73</td><td>25.61</td></tr><tr><td>ResNet-50 [21]</td><td>12.47</td><td>12.38</td><td>12.76</td><td>12.52</td><td>12.55</td><td>11.81</td></tr></tbody></table>", "caption": "Table 8: Comparison of error rate (\\%) according to the combination of transform functions. We use the following transformations in Pytorch: ColorJitter (Color), RandomGrayscale (Gray), RandomInvert (Inve.), GaussianBlur (Blur), RandomHorizontalFlip (H.Fli.), and RandomResizedCrop (Crop.).", "list_citation_info": ["[68] Zagoruyko, S., Komodakis, N.: Wide residual networks. British Machine Vision Conference (BMVC) (2016)", "[21] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)"]}, {"table": "<table><thead><tr><th>Models (Cityscapes\\rightarrowCityscapes-C)</th><th>mIoU</th></tr></thead><tbody><tr><th>DeepLabV3+ [6] (ResNet-50)</th><td>27.3%</td></tr><tr><th>\u2004\u2004+TTA (Main+SWR)</th><td>49.8% (\\uparrow 22.5%)</td></tr><tr><th>RobustNet [9] (ResNet-50+DG)</th><td>44.4%</td></tr><tr><th>\u2004\u2004+TTA (Main+SWR)</th><td>55.2% (\\uparrow 10.8%)</td></tr></tbody></table>", "caption": "Table 13: Test-time adaptation (TTA) performance using SWR on Cityscapes-C dataset. DG denotes domain generalization.", "list_citation_info": ["[6] Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 801\u2013818 (2018)", "[9] Choi, S., Jung, S., Yun, H., Kim, J.T., Kim, S., Choo, J.: Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)"]}, {"table": "<table><thead><tr><th>Models (Cityscapes\\rightarrowCityscapes-C)</th><th>LR: 1e-4</th><th>LR: 1e-5</th><th>LR: 1e-6</th></tr></thead><tbody><tr><td>RobustNet [9]+TTA (Main)</td><td>7.3%</td><td>28.5%</td><td>53.1%</td></tr><tr><td>RobustNet [9]+TTA (Main+SWR)</td><td>53.7%</td><td>55.2%</td><td>54.0%</td></tr></tbody></table>", "caption": "Table 14: Comparison of performance with and without SWR according to the learning rate change. Test-time adaptation with the proposed SWR shows superior performance and less sensitivity to changes in the learning rate. LR denotes a learning rate.", "list_citation_info": ["[9] Choi, S., Jung, S., Yun, H., Kim, J.T., Kim, S., Choo, J.: Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)"]}], "citation_info_to_title": {"[9] Choi, S., Jung, S., Yun, H., Kim, J.T., Kim, S., Choo, J.: Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)": "RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening", "[68] Zagoruyko, S., Komodakis, N.: Wide residual networks. British Machine Vision Conference (BMVC) (2016)": "Wide Residual Networks", "[18] Gulrajani, I., Lopez-Paz, D.: In search of lost domain generalization. In: International Conference on Learning Representations (ICLR) (2020)": "In search of lost domain generalization", "[6] Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Proceedings of the European conference on computer vision (ECCV). pp. 801\u2013818 (2018)": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "[21] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)": "Deep Residual Learning for Image Recognition"}, "source_title_to_arxiv_id": {"RobustNet: Improving Domain Generalization in Urban-Scene Segmentation via Instance Selective Whitening": "2103.15597"}}