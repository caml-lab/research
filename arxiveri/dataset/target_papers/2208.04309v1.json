{"title": "3D Vision with Transformers: A Survey", "abstract": "The success of the transformer architecture in natural language processing\nhas recently triggered attention in the computer vision field. The transformer\nhas been used as a replacement for the widely used convolution operators, due\nto its ability to learn long-range dependencies. This replacement was proven to\nbe successful in numerous tasks, in which several state-of-the-art methods rely\non transformers for better learning. In computer vision, the 3D field has also\nwitnessed an increase in employing the transformer for 3D convolution neural\nnetworks and multi-layer perceptron networks. Although a number of surveys have\nfocused on transformers in vision in general, 3D vision requires special\nattention due to the difference in data representation and processing when\ncompared to 2D vision. In this work, we present a systematic and thorough\nreview of more than 100 transformers methods for different 3D vision tasks,\nincluding classification, segmentation, detection, completion, pose estimation,\nand others. We discuss transformer design in 3D vision, which allows it to\nprocess data with various 3D representations. For each application, we\nhighlight key properties and contributions of proposed transformer-based\nmethods. To assess the competitiveness of these methods, we compare their\nperformance to common non-transformer methods on 12 3D benchmarks. We conclude\nthe survey by discussing different open directions and challenges for\ntransformers in 3D vision. In addition to the presented papers, we aim to\nfrequently update the latest relevant papers along with their corresponding\nimplementations at: https://github.com/lahoud/3d-vision-transformers.", "authors": ["Jean Lahoud", "Jiale Cao", "Fahad Shahbaz Khan", "Hisham Cholakkal", "Rao Muhammad Anwer", "Salman Khan", "Ming-Hsuan Yang"], "published_date": "2022_08_08", "pdf_url": "http://arxiv.org/pdf/2208.04309v1", "list_table_and_caption": [{"table": "<table><tr><td colspan=\"6\"> </td></tr><tr><td><p>Method</p></td><td><p>Input</p></td><td><p>Scalability Element</p></td><td><p>Architecture</p></td><td><p>Context</p></td><td><p>Highlight</p></td></tr><tr><td colspan=\"6\"> </td></tr><tr><td><p>Point Transformer [44]</p></td><td><p>points</p></td><td><p>farthest point sampling</p></td><td><p>pure</p></td><td><p>local</p></td><td><p>applies self-attention in a local neighborhood, Transition down and up to increase receptive field</p></td></tr><tr><td><p>Point Transformer [50]</p></td><td><p>points</p></td><td><p>local (ball query)/ global (FPS)</p></td><td><p>with own sortnet</p></td><td><p>local / global</p></td><td><p>extract ordered local feature sets from different subspaces (SortNet), global and local-global attention</p></td></tr><tr><td><p>Attentional S\u0361hapeContextNet [47]</p></td><td><p>points</p></td><td><p>random sampling</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>replace hand-designed bin partitioning and pooling by a weighted sum aggregation function with input learned by self-attention</p></td></tr><tr><td><p>Yang et al.[51]</p></td><td><p>points</p></td><td><p>random sampling</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>absolute and relative position embedding as input to attention module, group attention similar to depthwise separable convolutions [52] and channel shuffle [53].</p></td></tr><tr><td><p>PCT [41]</p></td><td><p>points</p></td><td><p>farthest point sampling</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>offset-attention calculates the element-wise difference between the self-attention and the input features</p></td></tr><tr><td><p>PVT [54]</p></td><td><p>voxels, points</p></td><td><p>local (hash table), global (all)</p></td><td><p>pure</p></td><td><p>local / global</p></td><td><p>combines voxel-based and point-based transformer models to extract feature information</p></td></tr><tr><td><p>TransPCNet* [55]</p></td><td><p>points</p></td><td><p>kNN aggregation</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>feature embedding module and attention module to learn features to detect defects in sewer represented by 3D point clouds</p></td></tr><tr><td><p>Adaptive Wavelet T\u0361ransformer [56]</p></td><td><p>points from graph</p></td><td><p>k nearest neighbor</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>perform multi-scale analysis to generate visual representation decomposition using the lifting scheme approach</p></td></tr><tr><td><p>3DCTN* [57]</p></td><td><p>points</p></td><td><p>query ball</p></td><td><p>hybrid</p></td><td><p>local</p></td><td><p>combines graph convolution layers(local feature aggregation) with transformers (global feature learning )</p></td></tr><tr><td><p>DTNet [58]</p></td><td><p>points</p></td><td><p>farthest point sampling</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>Dual Point Cloud Transformer module to capture long-range position and channel correlations</p></td></tr><tr><td><p>CpT [59]</p></td><td><p>points</p></td><td><p>k nearest neighbor</p></td><td><p>pure</p></td><td><p>local / global</p></td><td><p>uses a dynamic point cloud graph to create a point embedding that is fed into the transformer layer</p></td></tr><tr><td><p>LFT-Net [60]</p></td><td><p>points</p></td><td><p>k nearest neighbor</p></td><td><p>pure</p></td><td><p>local</p></td><td><p>local feature transformer with local position encoding, self-attention pooling function for feature aggregation</p></td></tr><tr><td><p>Point-BERT [61]</p></td><td><p>points</p></td><td><p>FPS and point patches</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>point tokenization which converts a point cloud into discrete point tokens, and masked point modeling for pre-training</p></td></tr><tr><td><p>Liu et al.[62]</p></td><td><p>points</p></td><td><p>FPS, kNN</p></td><td><p>pure</p></td><td><p>local / global</p></td><td><p>radius-based feature abstraction for better feature extraction, group-in-group relation-based transformer architecture</p></td></tr><tr><td><p>Pang et al.[63]</p></td><td><p>points</p></td><td><p>FPS, kNN (or FPS and point patches)</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>Transformer-based autoencoder with asymmetric design and shifting mask tokens operation for pre-training</p></td></tr><tr><td><p>PAT [64]</p></td><td><p>voxels</p></td><td><p>voxelization</p></td><td><p>hybrid</p></td><td><p>local / global</p></td><td><p>patch attention module (PAT) and a multi-scale attention module (MST)</p></td></tr><tr><td><p>3CROSSNet [65]</p></td><td><p>points</p></td><td><p>farthest point sampling</p></td><td><p>hybrid</p></td><td><p>local / global</p></td><td><p>Point-wise Feature Pyramid, Cross-Level Cross-Attention, and CrossScale Cross-Attention</p></td></tr><tr><td><p>3DMedPT [66]</p></td><td><p>points</p></td><td><p>farthest point sampling</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>local context augmentation, relative positional embedding, and local context aggregation at query</p></td></tr><tr><td><p>Wu et al.* [67]</p></td><td><p>points &amp;\u0361others</p></td><td><p>soft k-means clustering</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>centroid attention: summarizing self-attention feature mapping to a smaller number of outputs</p></td></tr><tr><td><p>MLMSPT [68]</p></td><td><p>points</p></td><td><p>farthest point sampling</p></td><td><p>pure</p></td><td><p>local / global</p></td><td><p>point pyramid transformer followed by multi-level and multi-scale transformer</p></td></tr></table>", "caption": "TABLE I: Overview of transformer-based methods for classification. Important attributes for the transformer integration are shown here, which include the input, sampling element that enables transformer processing, architecture (pure or hybrid), and context level on which the transformer operates. A highlight of the main contributions is also included. All the methods also perform object part segmentation except the ones with an asterisk (*)", "list_citation_info": ["[56] H. Huang and Y. Fang, \u201cAdaptive wavelet transformer network for 3d shape representation learning,\u201d in ICLR, 2021.", "[68] X.-F. Han, Y.-J. Kuang, and G.-Q. Xiao, \u201cPoint cloud learning with transformer,\u201d arXiv preprint arXiv:2104.13636, 2021.", "[65] X.-F. Han, Z.-Y. He, J. Chen, and G.-Q. Xiao, \u201c3crossnet: Cross-level cross-scale cross-attention network for point cloud representation,\u201d RA-L, vol. 7, no. 2, pp. 3718\u20133725, 2022.", "[60] Y. Gao, X. Liu, J. Li, Z. Fang, X. Jiang, and K. M. S. Huq, \u201cLft-net: Local feature transformer network for point clouds analysis,\u201d T-ITS, 2022.", "[57] D. Lu, Q. Xie, L. Xu, and J. Li, \u201c3dctn: 3d convolution-transformer network for point cloud classification,\u201d arXiv preprint arXiv:2203.00828, 2022.", "[53] X. Zhang, X. Zhou, M. Lin, and J. Sun, \u201cShufflenet: An extremely efficient convolutional neural network for mobile devices,\u201d in CVPR, 2018, pp. 6848\u20136856.", "[51] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian, \u201cModeling point clouds with self-attention and gumbel subset sampling,\u201d in CVPR, 2019, pp. 3323\u20133332.", "[44] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, \u201cPoint transformer,\u201d in CVPR, 2021, pp. 16\u2009259\u201316\u2009268.", "[54] C. Zhang, H. Wan, S. Liu, X. Shen, and Z. Wu, \u201cPvt: Point-voxel transformer for 3d deep learning,\u201d arXiv preprint arXiv:2108.06076, 2021.", "[61] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, \u201cPoint-bert: Pre-training 3d point cloud transformers with masked point modeling,\u201d arXiv preprint arXiv:2111.14819, 2021.", "[55] Y. Zhou, A. Ji, and L. Zhang, \u201cSewer defect detection from 3d point clouds using a transformer-based deep learning model,\u201d Automation in Construction, vol. 136, p. 104163, 2022.", "[62] S. Liu, K. Fu, M. Wang, and Z. Song, \u201cGroup-in-group relation-based transformer for 3d point cloud learning,\u201d Remote. Sens., vol. 14, no. 7, p. 1563, 2022.", "[50] N. Engel, V. Belagiannis, and K. Dietmayer, \u201cPoint transformer,\u201d IEEE Access, vol. 9, pp. 134\u2009826\u2013134\u2009840, 2021.", "[47] S. Xie, S. Liu, Z. Chen, and Z. Tu, \u201cAttentional shapecontextnet for point cloud recognition,\u201d in CVPR, 2018, pp. 4606\u20134615.", "[64] Z. Cheng, H. Wan, X. Shen, and Z. Wu, \u201cPatchformer: A versatile 3d transformer based on patch attention,\u201d arXiv preprint arXiv:2111.00207, 2021.", "[41] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, \u201cPct: Point cloud transformer,\u201d Computational Visual Media, vol. 7, no. 2, pp. 187\u2013199, 2021.", "[52] F. Chollet, \u201cXception: Deep learning with depthwise separable convolutions,\u201d in CVPR, 2017, pp. 1251\u20131258.", "[58] X.-F. Han, Y.-F. Jin, H.-X. Cheng, and G.-Q. Xiao, \u201cDual transformer for point cloud analysis,\u201d arXiv preprint arXiv:2104.13044, 2021.", "[59] C. Kaul, J. Mitton, H. Dai, and R. Murray-Smith, \u201cCpt: Convolutional point transformer for 3d point cloud processing,\u201d arXiv preprint arXiv:2111.10866, 2021.", "[67] L. Wu, X. Liu, and Q. Liu, \u201cCentroid transformers: Learning to abstract with attention,\u201d arXiv preprint arXiv:2102.08606, 2021.", "[63] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, \u201cMasked autoencoders for point cloud self-supervised learning,\u201d arXiv preprint arXiv:2203.06604, 2022.", "[66] J. Yu, C. Zhang, H. Wang, D. Zhang, Y. Song, T. Xiang, D. Liu, and W. Cai, \u201c3d medical point transformer: Introducing convolution to attention networks for medical point cloud analysis,\u201d arXiv preprint arXiv:2112.04863, 2021."]}, {"table": "<table><tr><td colspan=\"6\"> </td></tr><tr><td><p>Method</p></td><td><p>Input</p></td><td><p>Scalability Element</p></td><td><p>Architecture</p></td><td><p>Context</p></td><td><p>Highlight</p></td></tr><tr><td colspan=\"6\"> </td></tr><tr><td><p>Pointformer [71]</p></td><td><p>points</p></td><td><p>Linformer for scalability</p></td><td><p>pure</p></td><td><p>global/ local</p></td><td><p>a feature learning block with a local, local-global, and global transformer</p></td></tr><tr><td><p>Voxel Transformer [42]</p></td><td><p>voxels</p></td><td><p>voxel discretization</p></td><td><p>pure</p></td><td><p>local/ dilated</p></td><td><p>multi-head self-attention on non-empty voxels through local attention and dilated attention</p></td></tr><tr><td><p>Sheng et al.[72]</p></td><td><p>points</p></td><td><p>proposal to point attention</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>uses raw points and proposals as input into a channel-wise transformer with a proposal-to-point encoding module and a channel-wise decoding module</p></td></tr><tr><td><p>Liu et al.[46]</p></td><td><p>points</p></td><td><p>k-closest points sampling</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>stacked multi-head self-attention and multi-head cross-attention to extract and refine object representations for object candidates</p></td></tr><tr><td><p>DETR3D [73]</p></td><td><p>features</p></td><td><p>object queries</p></td><td><p>hybrid</p></td><td><p>-</p></td><td><p>multi-head attention to refine object queries by incorporating object interactions, similar to DETR [74].</p></td></tr><tr><td><p>3DETR [45]</p></td><td><p>points</p></td><td><p>pointnet++ aggregation</p></td><td><p>almost pure</p></td><td><p>global</p></td><td><p>A transformer encoder is applied directly on the point cloud for extracting feature information, and a transformer decoder to predict 3D bounding boxes</p></td></tr><tr><td><p>SA-Det3D [75]</p></td><td><p>points, voxels, pillars</p></td><td><p>attend to salient regions</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>augment multiple convolution-based methods with full self-attention or deformable self-attention</p></td></tr><tr><td><p>M3DETR [76]</p></td><td><p>points, voxels</p></td><td><p>applied on output</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>combines raw points, voxels, and bird-eye view representations under a unified transformer-based architecture</p></td></tr><tr><td><p>Fan et al.[77]</p></td><td><p>voxels</p></td><td><p>regional grouping</p></td><td><p>pure</p></td><td><p>local</p></td><td><p>transformer operates with sparse regional attention on voxelized input</p></td></tr><tr><td><p>Fast Point Transformer [78]</p></td><td><p>voxels</p></td><td><p>(centroid aware voxelization)</p></td><td><p>pure</p></td><td><p>local</p></td><td><p>speed-up local self-attention networks with voxel hashing architecture and centroid-aware voxelization and devoxelization</p></td></tr><tr><td><p>Voxel Set Transformer [79]</p></td><td><p>voxels</p></td><td><p>voxelization</p></td><td><p>hybrid</p></td><td><p>local</p></td><td><p>A voxel-based set attention module with two cross-attentions. It applies self-attention to token clusters with varying sizes and processes them with a linear complexity.</p></td></tr><tr><td><p>ARM3D [80]</p></td><td><p>points</p></td><td><p>FPS</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>attention module to learn relation features for proposals</p></td></tr><tr><td><p>Yuan et al.[81]</p></td><td><p>points</p></td><td><p>only temporal channel</p></td><td><p>hybrid</p></td><td><p>temporal</p></td><td><p>temporal encoder and spatial decoder with a multi-head attention mechanism to aggregate information from the adjacent video frames</p></td></tr><tr><td><p>Dao et al.[82]</p></td><td><p>voxels</p></td><td><p>voxel discretization</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>vector attention that learns different weights for the point feature channels</p></td></tr><tr><td><p>MLCVNet [83]</p></td><td><p>points</p></td><td><p>pointnet++ aggregation</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>builds on top of [84] and uses self-attention modules to learn contextual information at the patch, object, and global scene levels</p></td></tr><tr><td><p>Yin et al.[85]</p></td><td><p>pillars</p></td><td><p>discretized pillar nodes</p></td><td><p>hybrid</p></td><td><p>local</p></td><td><p>spatial transformer attention and temporal transformer attention on point pillars</p></td></tr><tr><td><p>SCANet [86]</p></td><td><p>BEV</p></td><td><p>after encoder</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>attention after VGG encoder for point cloud BEV and RGB</p></td></tr><tr><td><p>MonoDETR [87]</p></td><td><p>images</p></td><td><p>feature downsampling</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>predicts and encodes depth to use it as input into a depth-aware decoder</p></td></tr><tr><td><p>Transfusion [88]</p></td><td><p>BEV, images</p></td><td><p>feature map from conv backbone</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>convolution backbone to extract feature maps, transformer decoder to fuse LiDAR based queries with image features</p></td></tr><tr><td><p>CAT-Det [89]</p></td><td><p>points,    images</p></td><td><p>ball query</p></td><td><p>hybrid</p></td><td><p>local/ global</p></td><td><p>combines a Pointformer applied on point cloud with Imageformer applied on RGB images</p></td></tr><tr><td><p>PETR [90]</p></td><td><p>images</p></td><td><p>conv-based encoder</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>fuses 3D features from multi-view images with 2D features</p></td></tr><tr><td><p>BoxeR [91]</p></td><td><p>BEV, images</p></td><td><p>conv encoder for 2D, PointPillar for 3D</p></td><td><p>hybrid</p></td><td><p>local</p></td><td><p>applies attention to a sampled grid within a box in 2D and 3D</p></td></tr><tr><td><p>BrT [92]</p></td><td><p>points,    images</p></td><td><p>pointnet++ aggregation</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>bridging point tokens (from point cloud) and patch tokens from images, point-to-patch projection</p></td></tr><tr><td><p>VISTA [93]</p></td><td><p>BEV, RV</p></td><td><p>voxelization, projection to BEV and RV</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>replace MLP in attention with convolutions, apply on BEV and RV</p></td></tr><tr><td><p>PDV [94]</p></td><td><p>voxels</p></td><td><p>voxel discretization</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>voxel centroid localization, density-aware RoI grid pooling, grid point Self-Attention</p></td></tr></table>", "caption": "TABLE II: Overview of 3D object detection methods using the transformer architecture. A highlight of the main design attributes and contributions is shown here. These methods use a variety of input representations, employ multiple sampling strategies for scalability, use a pure or hybrid transformer integration, and apply the transformer locally or globally.", "list_citation_info": ["[45] I. Misra, R. Girdhar, and A. Joulin, \u201cAn end-to-end transformer model for 3d object detection,\u201d in CVPR, 2021, pp. 2906\u20132917.", "[84] C. R. Qi, O. Litany, K. He, and L. J. Guibas, \u201cDeep hough voting for 3d object detection in point clouds,\u201d in CVPR, 2019, pp. 9277\u20139286.", "[75] P. Bhattacharyya, C. Huang, and K. Czarnecki, \u201cSa-det3d: Self-attention based context-aware 3d object detection,\u201d in CVPR, 2021, pp. 3022\u20133031.", "[71] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, \u201c3d object detection with pointformer,\u201d in CVPR, 2021, pp. 7463\u20137472.", "[74] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in ECCV, 2020, pp. 213\u2013229.", "[79] C. He, R. Li, S. Li, and L. Zhang, \u201cVoxel set transformer: A set-to-set approach to 3d object detection from point clouds,\u201d arXiv preprint arXiv:2203.10314, 2022.", "[92] Y. Wang, T. Ye, L. Cao, W. Huang, F. Sun, F. He, and D. Tao, \u201cBridged transformer for vision and point cloud 3d object detection,\u201d in CVPR, 2022, pp. 12\u2009114\u201312\u2009123.", "[82] M.-Q. Dao, E. H\u00e9ry, and V. Fr\u00e9mont, \u201cAttention-based proposals refinement for 3d object detection,\u201d arXiv preprint arXiv:2201.07070, 2022.", "[77] L. Fan, Z. Pang, T. Zhang, Y.-X. Wang, H. Zhao, F. Wang, N. Wang, and Z. Zhang, \u201cEmbracing single stride 3d object detector with sparse transformer,\u201d arXiv preprint arXiv:2112.06375, 2021.", "[80] Y. Lan, Y. Duan, C. Liu, C. Zhu, Y. Xiong, H. Huang, and K. Xu, \u201cArm3d: Attention-based relation module for indoor 3d object detection,\u201d Computational Visual Media, pp. 1\u201320, 2022.", "[72] H. Sheng, S. Cai, Y. Liu, B. Deng, J. Huang, X.-S. Hua, and M.-J. Zhao, \u201cImproving 3d object detection with channel-wise transformer,\u201d in CVPR, 2021, pp. 2743\u20132752.", "[42] J. Mao, Y. Xue, M. Niu, H. Bai, J. Feng, X. Liang, H. Xu, and C. Xu, \u201cVoxel transformer for 3d object detection,\u201d in CVPR, 2021, pp. 3164\u20133173.", "[91] D.-K. Nguyen, J. Ju, O. Booji, M. R. Oswald, and C. G. Snoek, \u201cBoxer: Box-attention for 2d and 3d transformers,\u201d arXiv preprint arXiv:2111.13087, 2021.", "[89] Y. Zhang, J. Chen, and D. Huang, \u201cCat-det: Contrastively augmented transformer for multi-modal 3d object detection,\u201d arXiv preprint arXiv:2204.00325, 2022.", "[73] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon, \u201cDetr3d: 3d object detection from multi-view images via 3d-to-2d queries,\u201d in CoRL, 2022, pp. 180\u2013191.", "[83] Q. Xie, Y.-K. Lai, J. Wu, Z. Wang, Y. Zhang, K. Xu, and J. Wang, \u201cMlcvnet: Multi-level context votenet for 3d object detection,\u201d in CVPR, 2020, pp. 10\u2009447\u201310\u2009456.", "[87] R. Zhang, H. Qiu, T. Wang, X. Xu, Z. Guo, Y. Qiao, P. Gao, and H. Li, \u201cMonodetr: Depth-aware transformer for monocular 3d object detection,\u201d arXiv preprint arXiv:2203.13310, 2022.", "[94] J. S. Hu, T. Kuai, and S. L. Waslander, \u201cPoint density-aware voxels for lidar 3d object detection,\u201d in CVPR, 2022, pp. 8469\u20138478.", "[76] T. Guan, J. Wang, S. Lan, R. Chandra, Z. Wu, L. Davis, and D. Manocha, \u201cM3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers,\u201d in WACV, 2022, pp. 772\u2013782.", "[86] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, \u201cScanet: Spatial-channel attention network for 3d object detection,\u201d in ICASSP, 2019, pp. 1992\u20131996.", "[81] Z. Yuan, X. Song, L. Bai, Z. Wang, and W. Ouyang, \u201cTemporal-channel transformer for 3d lidar-based video object detection for autonomous driving,\u201d TCSVT, vol. 32, no. 4, pp. 2068\u20132078, 2022.", "[88] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, \u201cTransfusion: Robust lidar-camera fusion for 3d object detection with transformers,\u201d arXiv preprint arXiv:2203.11496, 2022.", "[46] Z. Liu, Z. Zhang, Y. Cao, H. Hu, and X. Tong, \u201cGroup-free 3d object detection via transformers,\u201d in CVPR, 2021, pp. 2949\u20132958.", "[85] J. Yin, J. Shen, C. Guan, D. Zhou, and R. Yang, \u201cLidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention,\u201d in CVPR, 2020, pp. 11\u2009495\u201311\u2009504.", "[90] Y. Liu, T. Wang, X. Zhang, and J. Sun, \u201cPetr: Position embedding transformation for multi-view 3d object detection,\u201d arXiv preprint arXiv:2203.05625, 2022.", "[78] C. Park, Y. Jeong, M. Cho, and J. Park, \u201cFast point transformer,\u201d arXiv preprint arXiv:2112.04702, 2021.", "[93] S. Deng, Z. Liang, L. Sun, and K. Jia, \u201cVista: Boosting 3d object detection via dual cross-view spatial attention,\u201d in CVPR, 2022, pp. 8448\u20138457."]}, {"table": "<table><tr><td colspan=\"6\"> </td></tr><tr><td><p>Method</p></td><td><p>Input</p></td><td><p>Scalability Element</p></td><td><p>Architecture</p></td><td><p>Context</p></td><td><p>Highlight</p></td></tr><tr><td colspan=\"6\"> </td></tr><tr><td colspan=\"6\"> </td></tr><tr><td colspan=\"6\">Complete scenes segmentation:</td></tr><tr><td colspan=\"6\"> </td></tr><tr><td><p>Fast Point Transformer [78]</p></td><td><p>voxels</p></td><td><p>centroid aware voxelization</p></td><td><p>pure</p></td><td><p>local</p></td><td><p>speed-up local self-attention networks with voxel hashing architecture and centroid-aware voxelization and devoxelization</p></td></tr><tr><td><p>Stratified Transformer [96]</p></td><td><p>points</p></td><td><p>local aggregation with KPConv</p></td><td><p>hybrid</p></td><td><p>local</p></td><td><p>transformer-based hierarchical structure with a stratified strategy for keys sampling</p></td></tr><tr><td><p>Segment-Fusion [97]</p></td><td><p>points</p></td><td><p>group points to 3D segments</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>graph segmentation to segment features, segment fusion using attention encoder block</p></td></tr><tr><td><p>P4Transformer [98]</p></td><td><p>point cloud video</p></td><td><p>temporal radius &amp; stride, spatial radius &amp; subsampling</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>extracts features of sampled local spatio-temporal using 4D convolution, then use feature vector as input to transformer</p></td></tr><tr><td><p>Wei et al.[99]</p></td><td><p>point cloud video</p></td><td><p>FPS then set abstraction</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>set abstraction &amp; convolution layers to obtain patch features which are input to a transformer</p></td></tr><tr><td colspan=\"6\"> </td></tr><tr><td colspan=\"6\">Panoptic:</td></tr><tr><td colspan=\"6\"> </td></tr><tr><td><p>Xu et al.[100]</p></td><td><p>points, voxels</p></td><td><p>voxelization</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>sparse cross-scale attention network that aggregates sparse features at multiple scales and global voxel-encoded attention</p></td></tr><tr><td colspan=\"6\"> </td></tr><tr><td colspan=\"6\">Medical imaging segmentation</td></tr><tr><td colspan=\"6\"> </td></tr><tr><td><p>UNETR [43]</p></td><td><p>3D MRI images</p></td><td><p>patches</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>patch embedding as input to the transformer, skip connection between encoder and decoder</p></td></tr><tr><td><p>D-Former [101]</p></td><td><p>3D medical images</p></td><td><p>patches</p></td><td><p>hybrid</p></td><td><p>local / global</p></td><td><p>Dilated Transformer that applies self-attention alternately in local and global scopes</p></td></tr><tr><td><p>CoTr [48]</p></td><td><p>3D medical images</p></td><td><p>from CNN encoder</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>CNN encoder, multi-scale deformable self-attention, CNN decoder</p></td></tr><tr><td><p>T-AutoML [102]</p></td><td><p>3D CT scans</p></td><td><p>encodings of architectures, augmentation, hyperparameters</p></td><td><p>hybrid</p></td><td><p>-</p></td><td><p>Transformer to adapt to dynamic length to search for the best network architecture</p></td></tr><tr><td><p>Transfuse [49]</p></td><td><p>3D medical images</p></td><td><p>patches</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>fusion between CNN features and transformer features at multiple scales</p></td></tr><tr><td><p>Karimi et al.[103]</p></td><td><p>3D medical images</p></td><td><p>patches</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>3D patches as input to pure transformer</p></td></tr><tr><td><p>SpecTr [104]</p></td><td><p>3D medical images</p></td><td><p>from CNN encoder</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>depth-wise convolution, spectral normalization, and transformers as encoder</p></td></tr><tr><td><p>TransBTS [105]</p></td><td><p>3D MRI images</p></td><td><p>from CNN encoder</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>transformer at 3D UNet bottleneck</p></td></tr><tr><td><p>Segtran [106]</p></td><td><p>3D medical images</p></td><td><p>FPN, from CNN encoder</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>CNN with FPN as input to Squeeze-and-Expansion Transformer</p></td></tr><tr><td><p>nnFormer [107]</p></td><td><p>3D medical images</p></td><td><p>from CNN embedding</p></td><td><p>hybrid</p></td><td><p>local / global</p></td><td><p>local and global self-attention on CNN embedding with skip self-attention</p></td></tr><tr><td><p>BiTr-UNet [108]</p></td><td><p>3D medical images</p></td><td><p>from CNN encoder</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>Transformer at 3D UNet bottleneck, CBAM for 3D CNN</p></td></tr><tr><td><p>AFTer-UNet [109]</p></td><td><p>3D volume</p></td><td><p>from CNN encoder</p></td><td><p>hybrid</p></td><td><p> global</p></td><td><p>axial fusion transformer to fuse inter-slice and intra-slice information</p></td></tr><tr><td><p>Peiris et al.[110]</p></td><td><p>3D volume</p></td><td><p>patches</p></td><td><p>pure</p></td><td><p>local / global</p></td><td><p>encoder with local/global attention, decoder with parallel window-based self/cross attention</p></td></tr><tr><td><p>Swin UNETR [111]</p></td><td><p>3D medical images</p></td><td><p>patches</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>1D sequence embedding as input to a Swin Transformer [112]</p></td></tr></table>", "caption": "TABLE III: Overview of 3D segmentation methods using the transformer architecture. We divide 3D segmentation methods into three categories: (1) methods that perform 3D semantic segmentation on complete scenes (rather than single object part segmentation), (2) panoptic segmentation, and (3) medical imaging segmentation. 3D medical images are represented by a dense regular grid, in contrast to data collected by LiDARs and RGB-D sensors which is sparse.", "list_citation_info": ["[110] H. Peiris, M. Hayat, Z. Chen, G. Egan, and M. Harandi, \u201cA volumetric transformer for accurate 3d tumor segmentation,\u201d arXiv preprint arXiv:2111.13300, 2021.", "[103] D. Karimi, S. D. Vasylechko, and A. Gholipour, \u201cConvolution-free medical image segmentation using transformers,\u201d in MICCAI, 2021, pp. 78\u201388.", "[111] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. Roth, and D. Xu, \u201cSwin unetr: Swin transformers for semantic segmentation of brain tumors in mri images,\u201d arXiv preprint arXiv:2201.01266, 2022.", "[107] H.-Y. Zhou, J. Guo, Y. Zhang, L. Yu, L. Wang, and Y. Yu, \u201cnnformer: Interleaved transformer for volumetric segmentation,\u201d arXiv preprint arXiv:2109.03201, 2021.", "[49] Y. Zhang, H. Liu, and Q. Hu, \u201cTransfuse: Fusing transformers and cnns for medical image segmentation,\u201d in MICCAI, 2021, pp. 14\u201324.", "[112] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in CVPR, 2021, pp. 10\u2009012\u201310\u2009022.", "[99] Y. Wei, H. Liu, T. Xie, Q. Ke, and Y. Guo, \u201cSpatial-temporal transformer for 3d point cloud sequences,\u201d in WACV, 2022, pp. 1171\u20131180.", "[109] X. Yan, H. Tang, S. Sun, H. Ma, D. Kong, and X. Xie, \u201cAfter-unet: Axial fusion transformer unet for medical image segmentation,\u201d in WACV, 2022, pp. 3971\u20133981.", "[43] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, \u201cUnetr: Transformers for 3d medical image segmentation,\u201d in WACV, 2022, pp. 574\u2013584.", "[105] W. Wang, C. Chen, M. Ding, H. Yu, S. Zha, and J. Li, \u201cTransbts: Multimodal brain tumor segmentation using transformer,\u201d in MICCAI, 2021, pp. 109\u2013119.", "[96] X. Lai, J. Liu, L. Jiang, L. Wang, H. Zhao, S. Liu, X. Qi, and J. Jia, \u201cStratified transformer for 3d point cloud segmentation,\u201d in arXiv preprint arXiv:2203.14508, 2022.", "[100] S. Xu, R. Wan, M. Ye, X. Zou, and T. Cao, \u201cSparse cross-scale attention network for efficient lidar panoptic segmentation,\u201d arXiv preprint arXiv:2201.05972, 2022.", "[106] S. Li, X. Sui, X. Luo, X. Xu, Y. Liu, and R. Goh, \u201cMedical image segmentation using squeeze-and-expansion transformers,\u201d arXiv preprint arXiv:2105.09511, 2021.", "[98] H. Fan, Y. Yang, and M. Kankanhalli, \u201cPoint 4d transformer networks for spatio-temporal modeling in point cloud videos,\u201d in CVPR, 2021, pp. 14\u2009204\u201314\u2009213.", "[101] Y. Wu, K. Liao, J. Chen, D. Z. Chen, J. Wang, H. Gao, and J. Wu, \u201cD-former: A u-shaped dilated transformer for 3d medical image segmentation,\u201d arXiv preprint arXiv:2201.00462, 2022.", "[102] D. Yang, A. Myronenko, X. Wang, Z. Xu, H. R. Roth, and D. Xu, \u201cT-automl: Automated machine learning for lesion segmentation using transformers in 3d medical imaging,\u201d in CVPR, 2021, pp. 3962\u20133974.", "[78] C. Park, Y. Jeong, M. Cho, and J. Park, \u201cFast point transformer,\u201d arXiv preprint arXiv:2112.04702, 2021.", "[108] Q. Jia and H. Shu, \u201cBitr-unet: a cnn-transformer combined network for mri brain tumor segmentation,\u201d arXiv preprint arXiv:2109.12271, 2021.", "[104] B. Yun, Y. Wang, J. Chen, H. Wang, W. Shen, and Q. Li, \u201cSpectr: Spectral transformer for hyperspectral pathology image segmentation,\u201d arXiv preprint arXiv:2103.03604, 2021.", "[48] Y. Xie, J. Zhang, C. Shen, and Y. Xia, \u201cCotr: Efficiently bridging cnn and transformer for 3d medical image segmentation,\u201d in MICCAI, 2021, pp. 171\u2013180.", "[97] A. Thyagharajan, B. Ummenhofer, P. Laddha, O. J. Omer, and S. Subramoney, \u201cSegment-fusion: Hierarchical context fusion for robust 3d semantic segmentation,\u201d in CVPR, 2022, pp. 1236\u20131245."]}, {"table": "<table><tr><td colspan=\"5\"> </td></tr><tr><td>Method</td><td>Input</td><td>Architecture</td><td><p>Context</p></td><td><p>Highlight</p></td></tr><tr><td colspan=\"5\"> </td></tr><tr><td>PoinTr [117]</td><td>points</td><td>hybrid</td><td><p>global/ local</p></td><td><p>a set-to-set translation and geometry-aware transformer for point cloud completion</p></td></tr><tr><td>Wang et al.[118]</td><td>points</td><td>hybrid</td><td><p>global/ local</p></td><td><p>neighboring pooling integrated with transformer encoder-decoder</p></td></tr><tr><td>PointAttN [119]</td><td>points</td><td>hybrid</td><td><p>global/ local</p></td><td><p>a geometric details perception module and a self-feature augment module</p></td></tr><tr><td>SnowflakeNet [120]</td><td>points</td><td>hybrid</td><td><p>local</p></td><td><p>a skip-transformer module to capture shape context</p></td></tr><tr><td>Su et al.[121]</td><td>points</td><td>hybrid</td><td><p>global/ local</p></td><td><p>attention for feature aggregation and upsampling</p></td></tr><tr><td>PCTMA-Net [122]</td><td>points</td><td>hybrid</td><td><p>global</p></td><td><p>a transformer encoder to learn semantic affinity information</p></td></tr><tr><td>AutoSDF [123]</td><td>voxel</td><td>hybrid</td><td><p>global</p></td><td><p>transformer-based autoregressive modeling in low-dimension latent space</p></td></tr><tr><td>ShapeFomer [124]</td><td>voxel</td><td>hybrid</td><td><p>global</p></td><td><p>a transformer to predict a distribution of object completions</p></td></tr><tr><td>PDR [125]</td><td>point</td><td>hybrid</td><td><p>local</p></td><td><p>aggregating local features by the attention operation</p></td></tr><tr><td>MFM-Net [126]</td><td>point</td><td>hybrid</td><td><p>global</p></td><td><p>a self-attention layer for global refinement</p></td></tr></table>", "caption": "TABLE IV: State-of-the-art 3D point cloud completion methods using transformers. These methods use a variety of input representations, employ a pure or hybrid architecture, and apply the transformer locally or globally.", "list_citation_info": ["[123] P. Mittal, Y.-C. Cheng, M. Singh, and S. Tulsiani, \u201cAutosdf: Shape priors for 3d completion, reconstruction and generation,\u201d in CVPR, 2022, pp. 306\u2013315.", "[121] Z. Su, H. Huang, C. Ma, H. Huang, and R. Hu, \u201cPoint cloud completion on structured feature map with feedback network,\u201d Computational Visual Media, 2022.", "[118] Y. Wang, D. J. Tan, N. Navab, and F. Tombari, \u201cLearning local displacements for point cloud completion,\u201d in CVPR, 2022, pp. 1568\u20131577.", "[124] X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, and H. Huang, \u201cShapeformer: Transformer-based shape completion via sparse representation,\u201d in CVPR, 2022, pp. 6239\u20136249.", "[125] Z. Lyu, Z. Kong, X. Xu, L. Pan, and D. Lin, \u201cA conditional point diffusion-refinement paradigm for 3d point cloud completion,\u201d in ICLR, 2022.", "[126] Z. Cao, W. Zhang, X. Wen, Z. Dong, Y. shen Liu, and B. Yang, \u201cMfm-net: Unpaired shape completion network with multi-stage feature matching,\u201d arXiv preprint arXiv:2111.11976, 2021.", "[120] P. Xiang, X. Wen, Y.-S. Liu, Y.-P. Cao, P. Wan, W. Zheng, and Z. Han, \u201cSnowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer,\u201d in ICCV, 2021, pp. 5499\u20135501.", "[119] J. Wang, Y. Cui, D. Guo, J. Li, Q. Liu, and C. Shen, \u201cPointattn: You only need attention for point cloud completion,\u201d arXiv preprint arXiv:2203.08485, 2021.", "[122] J. Lin, M. Rickert, A. Perzylo, and A. Knoll, \u201cPctma-net: Point cloud transformer with mborphing atlas-based point generation network for dense point cloud completion,\u201d in ICRA, 2021, pp. 5657\u20135663.", "[117] X. Yu, Y. Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou, \u201cPointr: Diverse point cloud completion with geometry-aware transformers,\u201d in ICCV, 2021, pp. 12\u2009498\u201312\u2009507."]}, {"table": "<table><tr><td colspan=\"5\"> </td></tr><tr><td><p>Method</p></td><td><p>Input</p></td><td><p>Arch.</p></td><td><p>Context</p></td><td><p>Highlight</p></td></tr><tr><td colspan=\"5\"> </td></tr><tr><td colspan=\"5\">Videos or multi-frames</td></tr><tr><td><p>PoseFormer [127]</p></td><td><p>sequence</p></td><td><p>pure</p></td><td><p>global/ local</p></td><td><p>a spatial-temporal transformer encoder module</p></td></tr><tr><td><p>Crossformer [128]</p></td><td><p>sequence</p></td><td><p>pure</p></td><td><p>global/ local</p></td><td><p>a cross-joint interaction module and a cross-frame interaction module</p></td></tr><tr><td><p>STE [129]</p></td><td><p>sequence</p></td><td><p>pure</p></td><td><p>global/ local</p></td><td><p>vanilla and strided transformers for global and local information aggregation</p></td></tr><tr><td><p>P-STMO [130]</p></td><td><p>sequence</p></td><td><p>pure</p></td><td><p>global/ local</p></td><td><p>self-supervised pre-training technique with transformer</p></td></tr><tr><td><p>MixSTE [131]</p></td><td><p>sequence</p></td><td><p>pure</p></td><td><p>global/ local</p></td><td><p>spatio-temporal attention in an alternating style</p></td></tr><tr><td><p>MHFormer [132]</p></td><td><p>sequence</p></td><td><p>pure</p></td><td><p>global/ local</p></td><td><p>spatio-temporal representations ofmultiple pose hypotheses</p></td></tr><tr><td><p>GraFormer [133]</p></td><td><p>sequence</p></td><td><p>hybrid</p></td><td><p>global/ local</p></td><td><p>a novelmodel by combining graph convolution and transformer</p></td></tr><tr><td colspan=\"5\">Multi-view frames</td></tr><tr><td><p>Epipolar [134]</p></td><td><p>multi-view</p></td><td><p>hybrid</p></td><td><p>local</p></td><td><p>enhance the feature of source view from and source view</p></td></tr><tr><td><p>RayTran [135]</p></td><td><p>multi-view</p></td><td><p>hybrid</p></td><td><p>local</p></td><td><p>a ray-traced transformer to progressively exchange information</p></td></tr><tr><td colspan=\"5\">Singe depth or RGB image</td></tr><tr><td><p>Hand-transformer [136]</p></td><td><p>point</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>a non-autoregressive hand transformer to model the relationships of input points</p></td></tr><tr><td><p>Cheng et al.[137]</p></td><td><p>point</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>an attention based confidence network for multi-view feature fusion</p></td></tr><tr><td><p>METRO [138]</p></td><td><p>image</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>a progressive dimensionality reduction transformer to predict 3D coordinates</p></td></tr><tr><td><p>HOT-Net [139]</p></td><td><p>image</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>a hand-object transformer based on non-autoregressive transformer</p></td></tr><tr><td><p>HandsFormer [140]</p></td><td><p>image</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>self-attention between the keypoints to associate them</p></td></tr><tr><td><p>PI-Net [141]</p></td><td><p>image</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>a self-attention module to improve the embeddings of each person</p></td></tr><tr><td><p>Ugrinovic et al.[142]</p></td><td><p>image</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>encode global information of multiple persons with set transformer</p></td></tr><tr><td colspan=\"5\">6D pose estimation</td></tr><tr><td><p>6D-ViT [143]</p></td><td><p>image</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>a two-branch transformer encoder-decoder structure</p></td></tr><tr><td><p>Dang et al.[144]</p></td><td><p>image</p></td><td><p>hybrid</p></td><td><p>global</p></td><td><p>a DCP-based architecture with transformer for feature extraction</p></td></tr><tr><td><p>Goodwin et al.[145]</p></td><td><p>image</p></td><td><p>pure</p></td><td><p>global</p></td><td><p>zero-shot category-level pose estimation</p></td></tr></table>", "caption": "TABLE V: Summary of 3D pose estimation related approaches using the transformer. Some methods focus on videos or multi-frames, some methods focus on multi-view frames, some methods take single depth or RGB image as input, and some methods are for 6D pose estimation.", "list_citation_info": ["[134] Y. He, R. Yan, K. Fragkiadaki, and S.-I. Yu, \u201cEpipolar transformer for multi-view human pose estimation,\u201d in CVPR Workshops, 2020, pp. 4466\u20134471.", "[141] W. Guo, E. Corona, F. Moreno-Noguer, and X. Alameda-Pineda, \u201cPi-net: Pose interacting network for multi-person monocular 3d pose estimation,\u201d in WACV, 2021, pp. 2796\u20132806.", "[142] N. Ugrinovic, A. Ruiz, A. Agudo, A. Sanfeliu, and F. Moreno-Noguer, \u201cPermutation-invariant relational network for multi-person 3d pose estimation,\u201d arXiv preprint arXiv:2204.04913, 2022.", "[145] W. Goodwin, S. Vaze, I. Havoutis, and I. Posner, \u201cZero-shot category-level object pose estimation,\u201d arXiv preprint arXiv:2204.03635, 2022.", "[131] J. Zhang, Z. Tu, J. Yang, Y. Chen, and J. Yuan, \u201cMixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video,\u201d in CVPR, 2022, pp. 13\u2009232\u201313\u2009242.", "[132] W. Li, H. Liu, H. Tang, P. Wang, and L. V. Gool, \u201cMhformer: Multi-hypothesis transformer for 3d human pose estimation,\u201d in CVPR, 2022, pp. 13\u2009147\u201313\u2009156.", "[136] L. Huang, J. Tan, J. Liu, and J. Yuan, \u201cHand-transformer: Non-autoregressive structured modeling for 3d hand pose estimation,\u201d in ECCV, 2020, pp. 17\u201333.", "[138] K. Lin, L. Wang, and Z. Liu, \u201cEnd-to-end human pose and mesh reconstruction with transformers,\u201d in CVPR, 2021, pp. 1954\u20131963.", "[128] M. Hassanin, A. Khamiss, M. Bennamoun, F. Boussaid, and I. Radwan, \u201cCrossformer: Cross spatio-temporal transformer for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.13387, 2022.", "[130] W. Shan, Z. Liu, X. Zhang, S. Wang, S. Ma, and W. Gao, \u201cP-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.07628, 2022.", "[140] S. Hampali, S. D. Sarkar, M. Rad, and V. Lepetit, \u201cKeypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation,\u201d arXiv preprint arXiv:2104.14639, 2021.", "[144] Z. Dang, L. Wang, Y. Guo, and M. Salzmann, \u201cLearning-based point cloud registration for 6d object pose estimation in the real world,\u201d arXiv preprint arXiv:2203.15309, 2022.", "[139] L. Huang, J. Tan, J. Meng, J. Liu, and J. Yuan, \u201cHot-net: Non-autoregressive transformer for 3d hand-object pose estimation,\u201d in ACM MM, 2020, pp. 3136\u20133145.", "[143] L. Zou, Z. Huang, N. Gu, and G. Wang, \u201c6d-vit: Category-level 6d object pose estimation via transformer-based instance representation learning,\u201d arXiv preprint arXiv:2110.04792, 2021.", "[133] W. Zhao, W. Wang, and Y. Tian, \u201cGraformer: Graph convolution transformer for 3d pose estimation,\u201d in CVPR, 2022, pp. 20\u2009438\u201320\u2009447.", "[129] W. Li, H. Liu, R. Ding, M. Liu, P. Wang, and W. Yang, \u201cExploiting temporal contexts with strided transformer for 3d human pose estimation,\u201d TMM, 2022.", "[135] M. J. Tyszkiewicz, K.-K. Maninis, S. Popov, and V. Ferrari, \u201cRaytran: 3d pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers,\u201d arXiv preprint arXiv:2203.13296, 2022.", "[127] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding, \u201c3d human pose estimation with spatial and temporal transformers,\u201d in ICCV, 2021, pp. 11\u2009656\u201311\u2009665.", "[137] J. Cheng, Y. Wan, D. Zuo, C. Ma, J. Gu, P. Tan, H. Wang, X. Deng, and Y. Zhang, \u201cEfficient virtual view selection for 3d hand pose estimation,\u201d in AAAI, 2022, pp. 419\u2013426."]}, {"table": "<table><tr><td colspan=\"5\"> </td></tr><tr><td>Dataset</td><td>Task</td><td>Size</td><td>Cls</td><td>Note</td></tr><tr><td colspan=\"5\"> </td></tr><tr><td>ModelNet40 [163]</td><td>classification</td><td>12,311 models</td><td>40</td><td>CAD models</td></tr><tr><td>ShapeNet [164]</td><td>segmentation</td><td>16,880 models</td><td>16</td><td>50 different parts</td></tr><tr><td>S3DIS [165]</td><td>segmentation</td><td>271 rooms</td><td>13</td><td>six areas</td></tr><tr><td>ScanObjectNN [166]</td><td>classification</td><td>2902 point clouds</td><td>15</td><td>with background/occlusions</td></tr><tr><td>SUN RGB-D [167]</td><td>detection</td><td>10,335 frames</td><td>37</td><td>Over 64,000 3D BB</td></tr><tr><td>ScanNet [168]</td><td>segm., det.</td><td>1513 scenes</td><td>40</td><td>100 hidden test scenes</td></tr><tr><td>KITTI [169]</td><td>detection</td><td>14,999 frames</td><td>3</td><td>80,256 labeled objects</td></tr><tr><td>NuScenes [170]</td><td>detection</td><td>1k scenes</td><td>23</td><td>40K key frames, 8 attributes</td></tr><tr><td>Completion3D [171]</td><td>completion</td><td>30,958 models</td><td>8</td><td>same size of 2048 \u00d7 3</td></tr><tr><td>PCN [172]</td><td>completion</td><td>30,974 models</td><td>8</td><td>less than 2048 points</td></tr><tr><td>Human3.6M [173]</td><td>pose estimation</td><td>3.6M video frames</td><td>17</td><td>Indoor with 11 actors</td></tr><tr><td>MPI-INF-3DHP [174]</td><td>pose estimation</td><td>14 camera views</td><td>8</td><td>indoor/outdoor with 8 actors</td></tr></table>", "caption": "TABLE VI: Common datasets for 3D vision tasks. These datasets cover a range of 3D applications: classification, segmentation, detection, completion, and pose estimation. They are collected using different sensors for indoor scenes, outdoor scenes, and objects.", "list_citation_info": ["[163] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, \u201c3d shapenets: A deep representation for volumetric shapes,\u201d in CVPR, 2015, pp. 1912\u20131920.", "[173] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, \u201cHuman3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments,\u201d TPAMI, vol. 36, no. 7, pp. 1325\u20131339, 2014.", "[171] L. P. Tchapmi, V. Kosaraju, S. H. Rezatofighi, I. Reid, and S. Savarese, \u201cTopnet: Structural point cloud decoder,\u201d in CVPR, 2019, pp. 383\u2013392.", "[165] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017.", "[164] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su et al., \u201cShapenet: An information-rich 3d model repository,\u201d arXiv preprint arXiv:1512.03012, 2015.", "[168] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d in CVPR, 2017, pp. 5828\u20135839.", "[172] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, \u201cPcn: Point completion network,\u201d in 3DV, 2018, pp. 728\u2013737.", "[170] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in CVPR, 2020, pp. 11\u2009621\u201311\u2009631.", "[174] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt, \u201cMonocular 3d human pose estimation in the wild using improved cnn supervision,\u201d in 3DV, 2017, pp. 506\u2013516.", "[167] S. Song, S. P. Lichtenberg, and J. Xiao, \u201cSun rgb-d: A rgb-d scene understanding benchmark suite,\u201d in CVPR, 2015, pp. 567\u2013576.", "[169] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? the kitti vision benchmark suite,\u201d in CVPR, 2012, pp. 3354\u20133361.", "[166] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, and S.-K. Yeung, \u201cRevisiting point cloud classification: A new benchmark dataset and classification model on real-world data,\u201d in ICCV, 2019, pp. 1588\u20131597."]}, {"table": "<table><tr><td colspan=\"4\"> </td></tr><tr><td><p>Method</p></td><td><p>Repr.</p></td><td><p># points</p></td><td><p>Acc (%)</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Non-Transformer methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>PointNet [23]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>89.2</p></td></tr><tr><td><p>PointNet++ [25]</p></td><td><p>P + N</p></td><td><p>5k</p></td><td><p>91.9</p></td></tr><tr><td><p>PointCNN [175]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>92.5</p></td></tr><tr><td><p>KPConv [28]</p></td><td><p>P</p></td><td><p>6.8k</p></td><td><p>92.9</p></td></tr><tr><td><p>DGCNN [26]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>92.9</p></td></tr><tr><td><p>RS-CNN [176]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.6</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Transformer-based methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>A-ShapeContextNet [47]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>90.0</p></td></tr><tr><td><p>Yang et al.[51]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>91.7</p></td></tr><tr><td><p>3DETR [45]</p></td><td><p>P</p></td><td></td><td><p>91.9</p></td></tr><tr><td><p>Point Transformer [50]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>92.8</p></td></tr><tr><td><p>MLMSPT [68]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>92.9</p></td></tr><tr><td><p>DTNet [58]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>92.9</p></td></tr><tr><td><p>Wu et al.[67]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.2</p></td></tr><tr><td><p>LFT-Net [60]</p></td><td><p>P + N</p></td><td><p>2k</p></td><td><p>93.2</p></td></tr><tr><td><p>PCT [41]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.2</p></td></tr><tr><td><p>3DCTN [57]</p></td><td><p>P + N</p></td><td><p>1k</p></td><td><p>93.3</p></td></tr><tr><td><p>3DMedPT [66]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.4</p></td></tr><tr><td><p>3CROSSNet [65]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.5</p></td></tr><tr><td><p>PAT [64]</p></td><td><p>P + N</p></td><td><p>1k</p></td><td><p>93.6</p></td></tr><tr><td><p>Point Transformer [44]</p></td><td><p>P + N</p></td><td><p>1k</p></td><td><p>93.7</p></td></tr><tr><td><p>Point-BERT [61]</p></td><td><p>P</p></td><td><p>8k</p></td><td><p>93.8*</p></td></tr><tr><td><p>Point-MAE [63]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.8*</p></td></tr><tr><td><p>Adaptive Wavelet [56]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.9</p></td></tr><tr><td><p>CpT [59]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.9</p></td></tr><tr><td><p>Liu et al.[62]</p></td><td><p>P</p></td><td><p>1k</p></td><td><p>93.9</p></td></tr><tr><td><p>PVT [54]</p></td><td><p>P + N</p></td><td><p>1k</p></td><td><p>94.0</p></td></tr></table>", "caption": "TABLE VII: Shape classification results on ModelNet40 benchmark (P: points, N:normals, *: pre-trained).", "list_citation_info": ["[45] I. Misra, R. Girdhar, and A. Joulin, \u201cAn end-to-end transformer model for 3d object detection,\u201d in CVPR, 2021, pp. 2906\u20132917.", "[56] H. Huang and Y. Fang, \u201cAdaptive wavelet transformer network for 3d shape representation learning,\u201d in ICLR, 2021.", "[68] X.-F. Han, Y.-J. Kuang, and G.-Q. Xiao, \u201cPoint cloud learning with transformer,\u201d arXiv preprint arXiv:2104.13636, 2021.", "[65] X.-F. Han, Z.-Y. He, J. Chen, and G.-Q. Xiao, \u201c3crossnet: Cross-level cross-scale cross-attention network for point cloud representation,\u201d RA-L, vol. 7, no. 2, pp. 3718\u20133725, 2022.", "[60] Y. Gao, X. Liu, J. Li, Z. Fang, X. Jiang, and K. M. S. Huq, \u201cLft-net: Local feature transformer network for point clouds analysis,\u201d T-ITS, 2022.", "[57] D. Lu, Q. Xie, L. Xu, and J. Li, \u201c3dctn: 3d convolution-transformer network for point cloud classification,\u201d arXiv preprint arXiv:2203.00828, 2022.", "[51] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian, \u201cModeling point clouds with self-attention and gumbel subset sampling,\u201d in CVPR, 2019, pp. 3323\u20133332.", "[44] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, \u201cPoint transformer,\u201d in CVPR, 2021, pp. 16\u2009259\u201316\u2009268.", "[54] C. Zhang, H. Wan, S. Liu, X. Shen, and Z. Wu, \u201cPvt: Point-voxel transformer for 3d deep learning,\u201d arXiv preprint arXiv:2108.06076, 2021.", "[61] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, \u201cPoint-bert: Pre-training 3d point cloud transformers with masked point modeling,\u201d arXiv preprint arXiv:2111.14819, 2021.", "[62] S. Liu, K. Fu, M. Wang, and Z. Song, \u201cGroup-in-group relation-based transformer for 3d point cloud learning,\u201d Remote. Sens., vol. 14, no. 7, p. 1563, 2022.", "[50] N. Engel, V. Belagiannis, and K. Dietmayer, \u201cPoint transformer,\u201d IEEE Access, vol. 9, pp. 134\u2009826\u2013134\u2009840, 2021.", "[47] S. Xie, S. Liu, Z. Chen, and Z. Tu, \u201cAttentional shapecontextnet for point cloud recognition,\u201d in CVPR, 2018, pp. 4606\u20134615.", "[25] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d in NeurIPS, 2017, pp. 5099\u20135108.", "[26] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \u201cDynamic graph cnn for learning on point clouds,\u201d ACM TOG, vol. 38, no. 5, pp. 1\u201312, 2019.", "[175] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, \u201cPointcnn: Convolution on x-transformed points,\u201d in NeurIPS, 2018, pp. 820\u2013830.", "[28] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas, \u201cKpconv: Flexible and deformable convolution for point clouds,\u201d in CVPR, 2019, pp. 6411\u20136420.", "[64] Z. Cheng, H. Wan, X. Shen, and Z. Wu, \u201cPatchformer: A versatile 3d transformer based on patch attention,\u201d arXiv preprint arXiv:2111.00207, 2021.", "[41] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, \u201cPct: Point cloud transformer,\u201d Computational Visual Media, vol. 7, no. 2, pp. 187\u2013199, 2021.", "[58] X.-F. Han, Y.-F. Jin, H.-X. Cheng, and G.-Q. Xiao, \u201cDual transformer for point cloud analysis,\u201d arXiv preprint arXiv:2104.13044, 2021.", "[59] C. Kaul, J. Mitton, H. Dai, and R. Murray-Smith, \u201cCpt: Convolutional point transformer for 3d point cloud processing,\u201d arXiv preprint arXiv:2111.10866, 2021.", "[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in CVPR, 2017, pp. 652\u2013660.", "[67] L. Wu, X. Liu, and Q. Liu, \u201cCentroid transformers: Learning to abstract with attention,\u201d arXiv preprint arXiv:2102.08606, 2021.", "[63] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, \u201cMasked autoencoders for point cloud self-supervised learning,\u201d arXiv preprint arXiv:2203.06604, 2022.", "[66] J. Yu, C. Zhang, H. Wang, D. Zhang, Y. Song, T. Xiang, D. Liu, and W. Cai, \u201c3d medical point transformer: Introducing convolution to attention networks for medical point cloud analysis,\u201d arXiv preprint arXiv:2112.04863, 2021.", "[176] Y. Liu, B. Fan, S. Xiang, and C. Pan, \u201cRelation-shape convolutional neural network for point cloud analysis,\u201d in CVPR, 2019, pp. 8895\u20138904."]}, {"table": "<table><tr><td colspan=\"4\"> </td></tr><tr><td><p>Method</p></td><td><p>OBJ-BG</p></td><td><p>OBJ-ONLY</p></td><td><p>PB-T50-RS</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Non-Transformer methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>PointNet++ [25]</p></td><td><p>82.3</p></td><td><p>84.3</p></td><td><p>77.9</p></td></tr><tr><td><p>DGCNN [26]</p></td><td><p>82.8</p></td><td><p>86.2</p></td><td><p>78.1</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Transformer-based methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>Point-BERT [61]</p></td><td><p>87.43</p></td><td><p>88.12</p></td><td><p>83.07</p></td></tr><tr><td><p>Pang et al.[63]</p></td><td><p>90.02</p></td><td><p>88.29</p></td><td><p>85.18</p></td></tr></table>", "caption": "TABLE VIII: Object classification results on ScanObjectNN dataset [166]. We report classification accuracy on the three main variants: OBJ-BG, OBJ-ONLY, and PB-T50-RS ", "list_citation_info": ["[61] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, \u201cPoint-bert: Pre-training 3d point cloud transformers with masked point modeling,\u201d arXiv preprint arXiv:2111.14819, 2021.", "[25] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d in NeurIPS, 2017, pp. 5099\u20135108.", "[26] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \u201cDynamic graph cnn for learning on point clouds,\u201d ACM TOG, vol. 38, no. 5, pp. 1\u201312, 2019.", "[63] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, \u201cMasked autoencoders for point cloud self-supervised learning,\u201d arXiv preprint arXiv:2203.06604, 2022.", "[166] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, and S.-K. Yeung, \u201cRevisiting point cloud classification: A new benchmark dataset and classification model on real-world data,\u201d in ICCV, 2019, pp. 1588\u20131597."]}, {"table": "<table><tr><td colspan=\"4\"> </td></tr><tr><td></td><td><p>ShapeNet</p></td><td colspan=\"2\">S3DIS</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>Method</p></td><td><p>ins. mIoU</p></td><td><p>mAcc</p></td><td><p>mIoU</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Non-Transformer methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>ShapeNet [164]</p></td><td><p>81.4</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>PointNet [23]</p></td><td><p>83.7</p></td><td><p>49.0</p></td><td><p>41.1</p></td></tr><tr><td><p>PointNet++ [25]</p></td><td><p>85.1</p></td><td><p>-</p></td><td><p>51.5</p></td></tr><tr><td><p>DGCNN [26]</p></td><td><p>85.2</p></td><td><p>-</p></td><td><p>56.1</p></td></tr><tr><td><p>MinkowskiNet [31]</p></td><td><p>\u2013</p></td><td><p>71.7</p></td><td><p>65.4</p></td></tr><tr><td><p>KPConv [28]</p></td><td><p>86.4</p></td><td><p>72.8</p></td><td><p>67.1</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Transformer-based methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>Point Transformer [44]</p></td><td><p>86.6</p></td><td><p>76.5</p></td><td><p>70.4</p></td></tr><tr><td><p>Point Transformer [50]</p></td><td><p>85.9</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>A-ShapeContext [47]</p></td><td><p>84.6</p></td><td><p>-</p></td><td><p>52.7</p></td></tr><tr><td><p>Yang et al.[51]</p></td><td><p>-</p></td><td><p>70.8</p></td><td><p>60.1</p></td></tr><tr><td><p>PCT [41]</p></td><td><p>86.4</p></td><td><p>67.7</p></td><td><p>61.3</p></td></tr><tr><td><p>PVT [54]</p></td><td><p>86.6</p></td><td><p>-</p></td><td><p>68.2</p></td></tr><tr><td><p>Wavelet Transformer [56]</p></td><td><p>86.6</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>DTNet [58]</p></td><td><p>85.6</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>CpT [59]</p></td><td><p>86.6</p></td><td><p>72.6</p></td><td><p>62.3</p></td></tr><tr><td><p>LFT-Net [60]</p></td><td><p>86.2</p></td><td><p>76.2</p></td><td><p>65.2</p></td></tr><tr><td><p>Point-BERT [61]</p></td><td><p>85.6</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>Liu et al.[62]</p></td><td><p>86.6</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>Point-MAE [63]</p></td><td><p>86.1</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>PAT [64]</p></td><td><p>86.5</p></td><td><p>-</p></td><td><p>68.1</p></td></tr><tr><td><p>3CROSSNet [65]</p></td><td><p>85.3</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>3DMedPT [66]</p></td><td><p>84.3</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>MLMSPT [68]</p></td><td><p>86.4</p></td><td><p>-</p></td><td><p>62.9</p></td></tr><tr><td><p>Segment-Fusion [97]</p></td><td><p>-</p></td><td><p>-</p></td><td><p>65.3</p></td></tr><tr><td><p>Fast Point Transformer [78]</p></td><td><p>-</p></td><td><p>77.9</p></td><td><p>70.3</p></td></tr><tr><td><p>Stratified Transformer [96]</p></td><td><p>86.6</p></td><td><p>78.1</p></td><td><p>72.0</p></td></tr></table>", "caption": "TABLE IX: Object Part Segmentation on ShapeNet [164] and scene semantic segmentation results on S3DIS [165]. We compare transformer-based methods to state-of-the-art non-transformer methods. For part segmentation, we show instance mean IoU, whereas for scene segmentation, we report mean accuracy (mAcc) and mean IoU.", "list_citation_info": ["[63] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, \u201cMasked autoencoders for point cloud self-supervised learning,\u201d arXiv preprint arXiv:2203.06604, 2022.", "[56] H. Huang and Y. Fang, \u201cAdaptive wavelet transformer network for 3d shape representation learning,\u201d in ICLR, 2021.", "[68] X.-F. Han, Y.-J. Kuang, and G.-Q. Xiao, \u201cPoint cloud learning with transformer,\u201d arXiv preprint arXiv:2104.13636, 2021.", "[60] Y. Gao, X. Liu, J. Li, Z. Fang, X. Jiang, and K. M. S. Huq, \u201cLft-net: Local feature transformer network for point clouds analysis,\u201d T-ITS, 2022.", "[65] X.-F. Han, Z.-Y. He, J. Chen, and G.-Q. Xiao, \u201c3crossnet: Cross-level cross-scale cross-attention network for point cloud representation,\u201d RA-L, vol. 7, no. 2, pp. 3718\u20133725, 2022.", "[51] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian, \u201cModeling point clouds with self-attention and gumbel subset sampling,\u201d in CVPR, 2019, pp. 3323\u20133332.", "[44] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, \u201cPoint transformer,\u201d in CVPR, 2021, pp. 16\u2009259\u201316\u2009268.", "[54] C. Zhang, H. Wan, S. Liu, X. Shen, and Z. Wu, \u201cPvt: Point-voxel transformer for 3d deep learning,\u201d arXiv preprint arXiv:2108.06076, 2021.", "[61] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, \u201cPoint-bert: Pre-training 3d point cloud transformers with masked point modeling,\u201d arXiv preprint arXiv:2111.14819, 2021.", "[165] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017.", "[50] N. Engel, V. Belagiannis, and K. Dietmayer, \u201cPoint transformer,\u201d IEEE Access, vol. 9, pp. 134\u2009826\u2013134\u2009840, 2021.", "[47] S. Xie, S. Liu, Z. Chen, and Z. Tu, \u201cAttentional shapecontextnet for point cloud recognition,\u201d in CVPR, 2018, pp. 4606\u20134615.", "[96] X. Lai, J. Liu, L. Jiang, L. Wang, H. Zhao, S. Liu, X. Qi, and J. Jia, \u201cStratified transformer for 3d point cloud segmentation,\u201d in arXiv preprint arXiv:2203.14508, 2022.", "[25] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d in NeurIPS, 2017, pp. 5099\u20135108.", "[31] C. Choy, J. Gwak, and S. Savarese, \u201c4d spatio-temporal convnets: Minkowski convolutional neural networks,\u201d in CVPR, 2019, pp. 3075\u20133084.", "[26] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \u201cDynamic graph cnn for learning on point clouds,\u201d ACM TOG, vol. 38, no. 5, pp. 1\u201312, 2019.", "[164] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su et al., \u201cShapenet: An information-rich 3d model repository,\u201d arXiv preprint arXiv:1512.03012, 2015.", "[28] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas, \u201cKpconv: Flexible and deformable convolution for point clouds,\u201d in CVPR, 2019, pp. 6411\u20136420.", "[64] Z. Cheng, H. Wan, X. Shen, and Z. Wu, \u201cPatchformer: A versatile 3d transformer based on patch attention,\u201d arXiv preprint arXiv:2111.00207, 2021.", "[97] A. Thyagharajan, B. Ummenhofer, P. Laddha, O. J. Omer, and S. Subramoney, \u201cSegment-fusion: Hierarchical context fusion for robust 3d semantic segmentation,\u201d in CVPR, 2022, pp. 1236\u20131245.", "[41] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, \u201cPct: Point cloud transformer,\u201d Computational Visual Media, vol. 7, no. 2, pp. 187\u2013199, 2021.", "[58] X.-F. Han, Y.-F. Jin, H.-X. Cheng, and G.-Q. Xiao, \u201cDual transformer for point cloud analysis,\u201d arXiv preprint arXiv:2104.13044, 2021.", "[59] C. Kaul, J. Mitton, H. Dai, and R. Murray-Smith, \u201cCpt: Convolutional point transformer for 3d point cloud processing,\u201d arXiv preprint arXiv:2111.10866, 2021.", "[78] C. Park, Y. Jeong, M. Cho, and J. Park, \u201cFast point transformer,\u201d arXiv preprint arXiv:2112.04702, 2021.", "[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in CVPR, 2017, pp. 652\u2013660.", "[62] S. Liu, K. Fu, M. Wang, and Z. Song, \u201cGroup-in-group relation-based transformer for 3d point cloud learning,\u201d Remote. Sens., vol. 14, no. 7, p. 1563, 2022.", "[66] J. Yu, C. Zhang, H. Wang, D. Zhang, Y. Song, T. Xiang, D. Liu, and W. Cai, \u201c3d medical point transformer: Introducing convolution to attention networks for medical point cloud analysis,\u201d arXiv preprint arXiv:2112.04863, 2021."]}, {"table": "<table><tr><td colspan=\"3\"> </td></tr><tr><td><p>Method</p></td><td><p>SUN RGB-D mAP25</p></td><td><p>ScanNet mAP25</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Non-Transformer methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>VoteNet [84]</p></td><td><p>59.1</p></td><td><p>62.9</p></td></tr><tr><td><p>H3DNet [177]</p></td><td><p>60.1</p></td><td><p>67.2</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Transformer-based methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>Pointformer [71]</p></td><td><p>61.1</p></td><td><p>64.1</p></td></tr><tr><td><p>Liu et al.[46]</p></td><td><p>63.0</p></td><td><p>69.1</p></td></tr><tr><td><p>3DETR [45]</p></td><td><p>59.1</p></td><td><p>65.0</p></td></tr><tr><td><p>Fast Point Tr.[78]</p></td><td><p>-</p></td><td><p>59.1</p></td></tr><tr><td><p>ARM3D [80]</p></td><td><p>60.1</p></td><td><p>65.9</p></td></tr><tr><td><p>MLCVNet [83]</p></td><td><p>59.8</p></td><td><p>64.5</p></td></tr><tr><td><p>BrT [92]</p></td><td><p>65.4</p></td><td><p>71.3</p></td></tr></table>", "caption": "TABLE X: 3D object detection comparison on SUN RGB-D [167] and ScanNet [168]. We show mean AP (@IoU=0.25) for transformer-based methods and compare them to state-of-the-art non-transformer methods.", "list_citation_info": ["[45] I. Misra, R. Girdhar, and A. Joulin, \u201cAn end-to-end transformer model for 3d object detection,\u201d in CVPR, 2021, pp. 2906\u20132917.", "[83] Q. Xie, Y.-K. Lai, J. Wu, Z. Wang, Y. Zhang, K. Xu, and J. Wang, \u201cMlcvnet: Multi-level context votenet for 3d object detection,\u201d in CVPR, 2020, pp. 10\u2009447\u201310\u2009456.", "[177] Z. Zhang, B. Sun, H. Yang, and Q. Huang, \u201cH3dnet: 3d object detection using hybrid geometric primitives,\u201d in ECCV, 2020, pp. 311\u2013329.", "[168] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d in CVPR, 2017, pp. 5828\u20135839.", "[78] C. Park, Y. Jeong, M. Cho, and J. Park, \u201cFast point transformer,\u201d arXiv preprint arXiv:2112.04702, 2021.", "[84] C. R. Qi, O. Litany, K. He, and L. J. Guibas, \u201cDeep hough voting for 3d object detection in point clouds,\u201d in CVPR, 2019, pp. 9277\u20139286.", "[71] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, \u201c3d object detection with pointformer,\u201d in CVPR, 2021, pp. 7463\u20137472.", "[46] Z. Liu, Z. Zhang, Y. Cao, H. Hu, and X. Tong, \u201cGroup-free 3d object detection via transformers,\u201d in CVPR, 2021, pp. 2949\u20132958.", "[80] Y. Lan, Y. Duan, C. Liu, C. Zhu, Y. Xiong, H. Huang, and K. Xu, \u201cArm3d: Attention-based relation module for indoor 3d object detection,\u201d Computational Visual Media, pp. 1\u201320, 2022.", "[167] S. Song, S. P. Lichtenberg, and J. Xiao, \u201cSun rgb-d: A rgb-d scene understanding benchmark suite,\u201d in CVPR, 2015, pp. 567\u2013576.", "[92] Y. Wang, T. Ye, L. Cao, W. Huang, F. Sun, F. He, and D. Tao, \u201cBridged transformer for vision and point cloud 3d object detection,\u201d in CVPR, 2022, pp. 12\u2009114\u201312\u2009123."]}, {"table": "<table><tr><td colspan=\"5\"> </td></tr><tr><td></td><td></td><td>Car (IoU=0.7)</td><td>Pedestrian (IoU=0.5)</td><td>Cyclist (IOU=0.5)</td></tr><tr><td colspan=\"5\"> </td></tr><tr><td colspan=\"5\"> </td></tr><tr><td><p>Method</p></td><td><p>Input</p></td><td>Easy/Mod./Hard</td><td>Easy/Mod./Hard</td><td>Easy/Mod./Hard</td></tr><tr><td colspan=\"5\"> </td></tr><tr><td colspan=\"5\"> </td></tr><tr><td colspan=\"5\">Non-Transformer methods:</td></tr><tr><td colspan=\"5\"> </td></tr><tr><td><p>PointRCNN [179]</p></td><td><p>L</p></td><td>87.0 / 75.6 / 70.7</td><td>48.0 / 39.4 / 36.0</td><td>75.0 / 58.8 / 52.5</td></tr><tr><td><p>PV-RCNN [180]</p></td><td><p>L</p></td><td>90.3 / 81.4 / 76.8</td><td>52.2 / 43.3 / 40.3</td><td>78.6 / 63.7 / 57.7</td></tr><tr><td><p>Monoflex [181]</p></td><td><p>C</p></td><td>19.9 / 13.9 / 12.1</td><td>-</td><td>-</td></tr><tr><td colspan=\"5\"> </td></tr><tr><td colspan=\"5\">Transformer-based methods:</td></tr><tr><td colspan=\"5\"> </td></tr><tr><td><p>Pointformer [71]</p></td><td><p>L</p></td><td>87.1 / 77.1 / 69.3</td><td>50.7 / 42.4 / 39.6</td><td>75.0 / 59.8 / 54.0</td></tr><tr><td><p>Voxel Trans. [42]</p></td><td><p>L</p></td><td>89.9 / 82.1 / 79.1</td><td>-</td><td>-</td></tr><tr><td><p>Sheng et al.[72]</p></td><td><p>L</p></td><td>87.8 / 81.8 / 77.2</td><td>-</td><td>-</td></tr><tr><td><p>SA-Det3D [75]</p></td><td><p>L</p></td><td>88.3 / 81.5 / 77.0</td><td>-</td><td>82.2 / 68.5 / 61.3</td></tr><tr><td><p>M3DETR [76]</p></td><td><p>L</p></td><td>90.3 / 81.7 / 77.0</td><td>45.7 / 39.9 / 37.7</td><td>83.8 / 66.7 / 59.0</td></tr><tr><td><p>Voxel Set Tr.[79]</p></td><td><p>L</p></td><td>88.5 / 82.1 / 77.5</td><td>-</td><td>-</td></tr><tr><td><p>Dao et al.[82]</p></td><td><p>L</p></td><td>87.1 / 80.3 / 76.1</td><td>-</td><td>78.5 / 64.6 / 57.8</td></tr><tr><td><p>SCANet [86]</p></td><td><p>L+C</p></td><td>76.1 / 66.3 / 58.7</td><td>-</td><td>-</td></tr><tr><td><p>MonoDETR [87]</p></td><td><p>C</p></td><td>25.0 / 16.5 / 13.6</td><td>-</td><td>-</td></tr><tr><td><p>CAT-Det [89]</p></td><td><p>L+C</p></td><td>89.9 / 81.3 / 76.7</td><td>54.3 / 45.4 / 41.9</td><td>83.7 / 68.8 / 61.5</td></tr><tr><td><p>PDV [94]</p></td><td><p>L</p></td><td>90.4 / 81.9 / 77.4</td><td>-</td><td>83.0 / 67.8 / 60.5</td></tr></table>", "caption": "TABLE XI: Performance evaluations with state-of-the-art methods on the KITTI 3D object test set [169]. The results are reported by the mAP with 40 recall points, 0.7 IoU threshold for car, and 0.5 for others. (L: LiDAR, C: Color)", "list_citation_info": ["[79] C. He, R. Li, S. Li, and L. Zhang, \u201cVoxel set transformer: A set-to-set approach to 3d object detection from point clouds,\u201d arXiv preprint arXiv:2203.10314, 2022.", "[181] Y. Zhang, J. Lu, and J. Zhou, \u201cObjects are different: Flexible monocular 3d object detection,\u201d in CVPR, 2021, pp. 3289\u20133298.", "[87] R. Zhang, H. Qiu, T. Wang, X. Xu, Z. Guo, Y. Qiao, P. Gao, and H. Li, \u201cMonodetr: Depth-aware transformer for monocular 3d object detection,\u201d arXiv preprint arXiv:2203.13310, 2022.", "[94] J. S. Hu, T. Kuai, and S. L. Waslander, \u201cPoint density-aware voxels for lidar 3d object detection,\u201d in CVPR, 2022, pp. 8469\u20138478.", "[76] T. Guan, J. Wang, S. Lan, R. Chandra, Z. Wu, L. Davis, and D. Manocha, \u201cM3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers,\u201d in WACV, 2022, pp. 772\u2013782.", "[82] M.-Q. Dao, E. H\u00e9ry, and V. Fr\u00e9mont, \u201cAttention-based proposals refinement for 3d object detection,\u201d arXiv preprint arXiv:2201.07070, 2022.", "[86] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, \u201cScanet: Spatial-channel attention network for 3d object detection,\u201d in ICASSP, 2019, pp. 1992\u20131996.", "[75] P. Bhattacharyya, C. Huang, and K. Czarnecki, \u201cSa-det3d: Self-attention based context-aware 3d object detection,\u201d in CVPR, 2021, pp. 3022\u20133031.", "[71] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, \u201c3d object detection with pointformer,\u201d in CVPR, 2021, pp. 7463\u20137472.", "[169] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? the kitti vision benchmark suite,\u201d in CVPR, 2012, pp. 3354\u20133361.", "[179] S. Shi, X. Wang, and H. Li, \u201cPointrcnn: 3d object proposal generation and detection from point cloud,\u201d in CVPR, 2019, pp. 770\u2013779.", "[72] H. Sheng, S. Cai, Y. Liu, B. Deng, J. Huang, X.-S. Hua, and M.-J. Zhao, \u201cImproving 3d object detection with channel-wise transformer,\u201d in CVPR, 2021, pp. 2743\u20132752.", "[42] J. Mao, Y. Xue, M. Niu, H. Bai, J. Feng, X. Liang, H. Xu, and C. Xu, \u201cVoxel transformer for 3d object detection,\u201d in CVPR, 2021, pp. 3164\u20133173.", "[89] Y. Zhang, J. Chen, and D. Huang, \u201cCat-det: Contrastively augmented transformer for multi-modal 3d object detection,\u201d arXiv preprint arXiv:2204.00325, 2022.", "[180] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cPv-rcnn: Point-voxel feature set abstraction for 3d object detection,\u201d in CVPR, 2020, pp. 10\u2009529\u201310\u2009538."]}, {"table": "<table><tr><td colspan=\"3\"> </td></tr><tr><td><p>Method</p></td><td><p>Modality</p></td><td><p>mAP</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Non-Transformer methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>PointPillars [38]</p></td><td><p>LiDAR</p></td><td><p>30.5</p></td></tr><tr><td><p>3DSSD [182]</p></td><td><p>LiDAR</p></td><td><p>42.7</p></td></tr><tr><td><p>CBGS [183]</p></td><td><p>LiDAR</p></td><td><p>52.8</p></td></tr><tr><td><p>FCOS3D [184]</p></td><td><p>images</p></td><td><p>35.8</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Transformer-based methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>Yin et al.[85]</p></td><td><p>LiDAR</p></td><td><p>45.4</p></td></tr><tr><td><p>SA-Det3D [75]</p></td><td><p>LiDAR</p></td><td><p>47.0</p></td></tr><tr><td><p>Pointformer [71]</p></td><td><p>LiDAR</p></td><td><p>53.6</p></td></tr><tr><td><p>Dao et al.[82]</p></td><td><p>LiDAR</p></td><td><p>47.0</p></td></tr><tr><td><p>VISTA [93]</p></td><td><p>LiDAR</p></td><td><p>63.0</p></td></tr><tr><td><p>Transfusion [88]</p></td><td><p>LiDAR + images</p></td><td><p>68.9</p></td></tr><tr><td><p>PETR [90]</p></td><td><p>images</p></td><td><p>43.4</p></td></tr><tr><td><p>DETR3D [73]</p></td><td><p>images</p></td><td><p>41.2</p></td></tr><tr><td><p>Yuan et al.[81]</p></td><td><p>images</p></td><td><p>50.5</p></td></tr></table>", "caption": "TABLE XII: Performance evaluations of transformer-based methods and state-of-the-art non-transformer methods on nuScenes 3D object detection test set [170]. We report the mAP for detection of 10 classes.", "list_citation_info": ["[73] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon, \u201cDetr3d: 3d object detection from multi-view images via 3d-to-2d queries,\u201d in CoRL, 2022, pp. 180\u2013191.", "[184] T. Wang, X. Zhu, J. Pang, and D. Lin, \u201cFcos3d: Fully convolutional one-stage monocular 3d object detection,\u201d in CVPR, 2021, pp. 913\u2013922.", "[85] J. Yin, J. Shen, C. Guan, D. Zhou, and R. Yang, \u201cLidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention,\u201d in CVPR, 2020, pp. 11\u2009495\u201311\u2009504.", "[90] Y. Liu, T. Wang, X. Zhang, and J. Sun, \u201cPetr: Position embedding transformation for multi-view 3d object detection,\u201d arXiv preprint arXiv:2203.05625, 2022.", "[38] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointpillars: Fast encoders for object detection from point clouds,\u201d in CVPR, 2019, pp. 12\u2009697\u201312\u2009705.", "[82] M.-Q. Dao, E. H\u00e9ry, and V. Fr\u00e9mont, \u201cAttention-based proposals refinement for 3d object detection,\u201d arXiv preprint arXiv:2201.07070, 2022.", "[93] S. Deng, Z. Liang, L. Sun, and K. Jia, \u201cVista: Boosting 3d object detection via dual cross-view spatial attention,\u201d in CVPR, 2022, pp. 8448\u20138457.", "[88] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, \u201cTransfusion: Robust lidar-camera fusion for 3d object detection with transformers,\u201d arXiv preprint arXiv:2203.11496, 2022.", "[75] P. Bhattacharyya, C. Huang, and K. Czarnecki, \u201cSa-det3d: Self-attention based context-aware 3d object detection,\u201d in CVPR, 2021, pp. 3022\u20133031.", "[71] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, \u201c3d object detection with pointformer,\u201d in CVPR, 2021, pp. 7463\u20137472.", "[81] Z. Yuan, X. Song, L. Bai, Z. Wang, and W. Ouyang, \u201cTemporal-channel transformer for 3d lidar-based video object detection for autonomous driving,\u201d TCSVT, vol. 32, no. 4, pp. 2068\u20132078, 2022.", "[170] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in CVPR, 2020, pp. 11\u2009621\u201311\u2009631.", "[182] Z. Yang, Y. Sun, S. Liu, and J. Jia, \u201c3dssd: Point-based 3d single stage object detector,\u201d in CVPR, 2020, pp. 11\u2009040\u201311\u2009048.", "[183] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu, \u201cClass-balanced grouping and sampling for point cloud 3d object detection,\u201d arXiv preprint arXiv:1908.09492, 2019."]}, {"table": "<table><tr><td colspan=\"3\"> </td></tr><tr><td><p>Method</p></td><td><p>Completion3D: Avg. \u2193</p></td><td><p>PCN: Avg. \u2193</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Non-Transformer methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>PCN [172]</p></td><td><p>18.22</p></td><td><p>9.64</p></td></tr><tr><td><p>PMP-Net [185]</p></td><td><p>9.23</p></td><td><p>8.66</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Transformer-based methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>PoinTr [117]</p></td><td><p>-</p></td><td><p>8.38</p></td></tr><tr><td><p>Snowflakenet [120]</p></td><td><p>7.60</p></td><td><p>7.21</p></td></tr><tr><td><p>PointAttN [119]</p></td><td><p>6.63</p></td><td><p>6.86</p></td></tr><tr><td><p>Wang et al.[118]</p></td><td><p>6.64</p></td><td><p>7.96</p></td></tr></table>", "caption": "TABLE XIII: Performance evaluations of point cloud completion on Completion3D [171] and PCN [172]. L2 Chamfer distance is used as metric on Completion3D, while L1 Chamfer distance is used as metric on PCN. ", "list_citation_info": ["[171] L. P. Tchapmi, V. Kosaraju, S. H. Rezatofighi, I. Reid, and S. Savarese, \u201cTopnet: Structural point cloud decoder,\u201d in CVPR, 2019, pp. 383\u2013392.", "[118] Y. Wang, D. J. Tan, N. Navab, and F. Tombari, \u201cLearning local displacements for point cloud completion,\u201d in CVPR, 2022, pp. 1568\u20131577.", "[172] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, \u201cPcn: Point completion network,\u201d in 3DV, 2018, pp. 728\u2013737.", "[120] P. Xiang, X. Wen, Y.-S. Liu, Y.-P. Cao, P. Wan, W. Zheng, and Z. Han, \u201cSnowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer,\u201d in ICCV, 2021, pp. 5499\u20135501.", "[185] X. Wen, P. Xiang, Z. Han, Y.-P. Cao, P. Wan, W. Zheng, and Y.-S. Liu, \u201cPmp-net: Point cloud completion by learning multi-step point moving paths,\u201d in CVPR, 2021, pp. 7443\u20137452.", "[119] J. Wang, Y. Cui, D. Guo, J. Li, Q. Liu, and C. Shen, \u201cPointattn: You only need attention for point cloud completion,\u201d arXiv preprint arXiv:2203.08485, 2021.", "[117] X. Yu, Y. Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou, \u201cPointr: Diverse point cloud completion with geometry-aware transformers,\u201d in ICCV, 2021, pp. 12\u2009498\u201312\u2009507."]}, {"table": "<table><tr><td colspan=\"3\"> </td></tr><tr><td><p>Method</p></td><td><p>Protocol 1: Avg. \u2193</p></td><td><p>Protocol 2: Avg. \u2193</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Non-Transformer methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>UGCN [186]</p></td><td><p>45.6</p></td><td><p>35.5</p></td></tr><tr><td><p>Chen et al. [187]</p></td><td><p>44.6</p></td><td><p>35.6</p></td></tr><tr><td colspan=\"3\"> </td></tr><tr><td colspan=\"3\">Transformer-based methods:</td></tr><tr><td colspan=\"3\"> </td></tr><tr><td><p>METRO [138]</p></td><td><p>54.0</p></td><td><p>36.7</p></td></tr><tr><td><p>PoseFormer [127]</p></td><td><p>44.3</p></td><td><p>34.6</p></td></tr><tr><td><p>CrossFormer [128]</p></td><td><p>43.7</p></td><td><p>34.3</p></td></tr><tr><td><p>STE [129]</p></td><td><p>43.7</p></td><td><p>35.2</p></td></tr><tr><td><p>P-STMO [130]</p></td><td><p>44.1</p></td><td><p>34.4</p></td></tr><tr><td><p>MHFormer [132]</p></td><td><p>43.0</p></td><td><p>-</p></td></tr><tr><td><p>MixSTE [131]</p></td><td><p>42.4</p></td><td><p>33.9</p></td></tr></table>", "caption": "TABLE XIV: 3D pose estimation performance comparison onHuman3.6M [173] under Protocols 1&amp;2 where 2D pose detection is used as input.", "list_citation_info": ["[186] J. Wang, S. Yan, Y. Xiong, and D. Lin, \u201cMotion guided 3d pose estimation from videos,\u201d in ECCV, 2020, pp. 764\u2013780.", "[128] M. Hassanin, A. Khamiss, M. Bennamoun, F. Boussaid, and I. Radwan, \u201cCrossformer: Cross spatio-temporal transformer for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.13387, 2022.", "[173] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, \u201cHuman3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments,\u201d TPAMI, vol. 36, no. 7, pp. 1325\u20131339, 2014.", "[187] T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, and J. Luo, \u201cAnatomy-aware 3d human pose estimation with bone-based pose decomposition,\u201d TCSVT, vol. 32, no. 1, pp. 198\u2013209, 2021.", "[127] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding, \u201c3d human pose estimation with spatial and temporal transformers,\u201d in ICCV, 2021, pp. 11\u2009656\u201311\u2009665.", "[130] W. Shan, Z. Liu, X. Zhang, S. Wang, S. Ma, and W. Gao, \u201cP-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.07628, 2022.", "[131] J. Zhang, Z. Tu, J. Yang, Y. Chen, and J. Yuan, \u201cMixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video,\u201d in CVPR, 2022, pp. 13\u2009232\u201313\u2009242.", "[132] W. Li, H. Liu, H. Tang, P. Wang, and L. V. Gool, \u201cMhformer: Multi-hypothesis transformer for 3d human pose estimation,\u201d in CVPR, 2022, pp. 13\u2009147\u201313\u2009156.", "[129] W. Li, H. Liu, R. Ding, M. Liu, P. Wang, and W. Yang, \u201cExploiting temporal contexts with strided transformer for 3d human pose estimation,\u201d TMM, 2022.", "[138] K. Lin, L. Wang, and Z. Liu, \u201cEnd-to-end human pose and mesh reconstruction with transformers,\u201d in CVPR, 2021, pp. 1954\u20131963."]}, {"table": "<table><tr><td colspan=\"4\"> </td></tr><tr><td><p>Method</p></td><td><p>PCK \u2191</p></td><td><p>AUC \u2191</p></td><td><p>MPJPE \u2193</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Non-Transformer methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>Chen et al. [187]</p></td><td><p>87.9</p></td><td><p>54.0</p></td><td><p>78.8</p></td></tr><tr><td colspan=\"4\"> </td></tr><tr><td colspan=\"4\">Transformer-based methods:</td></tr><tr><td colspan=\"4\"> </td></tr><tr><td><p>PoseFormer [127]</p></td><td><p>88.6</p></td><td><p>56.4</p></td><td><p>77.1</p></td></tr><tr><td><p>CrossFormer [128]</p></td><td><p>89.1</p></td><td><p>57.5</p></td><td><p>76.3</p></td></tr><tr><td><p>P-STMO [130]</p></td><td><p>97.9</p></td><td><p>75.8</p></td><td><p>32.2</p></td></tr><tr><td><p>MHFormer [132]</p></td><td><p>93.8</p></td><td><p>63.3</p></td><td><p>58.0</p></td></tr><tr><td><p>MixSTE [131]</p></td><td><p>94.2</p></td><td><p>63.8</p></td><td><p>57.9</p></td></tr></table>", "caption": "TABLE XV: 3D pose estimation performance comparison on MPI-INF-3DHP [174].", "list_citation_info": ["[128] M. Hassanin, A. Khamiss, M. Bennamoun, F. Boussaid, and I. Radwan, \u201cCrossformer: Cross spatio-temporal transformer for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.13387, 2022.", "[187] T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, and J. Luo, \u201cAnatomy-aware 3d human pose estimation with bone-based pose decomposition,\u201d TCSVT, vol. 32, no. 1, pp. 198\u2013209, 2021.", "[127] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding, \u201c3d human pose estimation with spatial and temporal transformers,\u201d in ICCV, 2021, pp. 11\u2009656\u201311\u2009665.", "[130] W. Shan, Z. Liu, X. Zhang, S. Wang, S. Ma, and W. Gao, \u201cP-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.07628, 2022.", "[131] J. Zhang, Z. Tu, J. Yang, Y. Chen, and J. Yuan, \u201cMixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video,\u201d in CVPR, 2022, pp. 13\u2009232\u201313\u2009242.", "[132] W. Li, H. Liu, H. Tang, P. Wang, and L. V. Gool, \u201cMhformer: Multi-hypothesis transformer for 3d human pose estimation,\u201d in CVPR, 2022, pp. 13\u2009147\u201313\u2009156.", "[174] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt, \u201cMonocular 3d human pose estimation in the wild using improved cnn supervision,\u201d in 3DV, 2017, pp. 506\u2013516."]}], "citation_info_to_title": {"[44] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, \u201cPoint transformer,\u201d in CVPR, 2021, pp. 16\u2009259\u201316\u2009268.": "Point transformer", "[50] N. Engel, V. Belagiannis, and K. Dietmayer, \u201cPoint transformer,\u201d IEEE Access, vol. 9, pp. 134\u2009826\u2013134\u2009840, 2021.": "Point transformer", "[139] L. Huang, J. Tan, J. Meng, J. Liu, and J. Yuan, \u201cHot-net: Non-autoregressive transformer for 3d hand-object pose estimation,\u201d in ACM MM, 2020, pp. 3136\u20133145.": "Hot-net: Non-autoregressive transformer for 3d hand-object pose estimation", "[57] D. Lu, Q. Xie, L. Xu, and J. Li, \u201c3dctn: 3d convolution-transformer network for point cloud classification,\u201d arXiv preprint arXiv:2203.00828, 2022.": "3dctn: 3d Convolution-Transformer Network for Point Cloud Classification", "[169] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? the kitti vision benchmark suite,\u201d in CVPR, 2012, pp. 3354\u20133361.": "Are we ready for autonomous driving? The KITTI Vision Benchmark Suite", "[183] B. Zhu, Z. Jiang, X. Zhou, Z. Li, and G. Yu, \u201cClass-balanced grouping and sampling for point cloud 3d object detection,\u201d arXiv preprint arXiv:1908.09492, 2019.": "Class-balanced grouping and sampling for point cloud 3d object detection", "[175] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen, \u201cPointcnn: Convolution on x-transformed points,\u201d in NeurIPS, 2018, pp. 820\u2013830.": "PointCNN: Convolution on X-Transformed Points", "[60] Y. Gao, X. Liu, J. Li, Z. Fang, X. Jiang, and K. M. S. Huq, \u201cLft-net: Local feature transformer network for point clouds analysis,\u201d T-ITS, 2022.": "Lft-net: Local feature transformer network for point clouds analysis", "[179] S. Shi, X. Wang, and H. Li, \u201cPointrcnn: 3d object proposal generation and detection from point cloud,\u201d in CVPR, 2019, pp. 770\u2013779.": "Pointrcnn: 3d object proposal generation and detection from point cloud", "[164] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su et al., \u201cShapenet: An information-rich 3d model repository,\u201d arXiv preprint arXiv:1512.03012, 2015.": "ShapeNet: An Information-Rich 3D Model Repository", "[31] C. Choy, J. Gwak, and S. Savarese, \u201c4d spatio-temporal convnets: Minkowski convolutional neural networks,\u201d in CVPR, 2019, pp. 3075\u20133084.": "4d spatio-temporal convnets: Minkowski convolutional neural networks", "[187] T. Chen, C. Fang, X. Shen, Y. Zhu, Z. Chen, and J. Luo, \u201cAnatomy-aware 3d human pose estimation with bone-based pose decomposition,\u201d TCSVT, vol. 32, no. 1, pp. 198\u2013209, 2021.": "Anatomy-aware 3d human pose estimation with bone-based pose decomposition", "[122] J. Lin, M. Rickert, A. Perzylo, and A. Knoll, \u201cPctma-net: Point cloud transformer with mborphing atlas-based point generation network for dense point cloud completion,\u201d in ICRA, 2021, pp. 5657\u20135663.": "Pctma-net: Point cloud transformer with mborphing atlas-based point generation network for dense point cloud completion", "[51] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian, \u201cModeling point clouds with self-attention and gumbel subset sampling,\u201d in CVPR, 2019, pp. 3323\u20133332.": "Modeling point clouds with self-attention and gumbel subset sampling", "[59] C. Kaul, J. Mitton, H. Dai, and R. Murray-Smith, \u201cCpt: Convolutional point transformer for 3d point cloud processing,\u201d arXiv preprint arXiv:2111.10866, 2021.": "Cpt: Convolutional point transformer for 3d point cloud processing", "[52] F. Chollet, \u201cXception: Deep learning with depthwise separable convolutions,\u201d in CVPR, 2017, pp. 1251\u20131258.": "Xception: Deep learning with depthwise separable convolutions", "[101] Y. Wu, K. Liao, J. Chen, D. Z. Chen, J. Wang, H. Gao, and J. Wu, \u201cD-former: A u-shaped dilated transformer for 3d medical image segmentation,\u201d arXiv preprint arXiv:2201.00462, 2022.": "D-former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation", "[66] J. Yu, C. Zhang, H. Wang, D. Zhang, Y. Song, T. Xiang, D. Liu, and W. Cai, \u201c3d medical point transformer: Introducing convolution to attention networks for medical point cloud analysis,\u201d arXiv preprint arXiv:2112.04863, 2021.": "3D Medical Point Transformer: Introducing Convolution to Attention Networks for Medical Point Cloud Analysis", "[103] D. Karimi, S. D. Vasylechko, and A. Gholipour, \u201cConvolution-free medical image segmentation using transformers,\u201d in MICCAI, 2021, pp. 78\u201388.": "Convolution-free medical image segmentation using transformers", "[99] Y. Wei, H. Liu, T. Xie, Q. Ke, and Y. Guo, \u201cSpatial-temporal transformer for 3d point cloud sequences,\u201d in WACV, 2022, pp. 1171\u20131180.": "Spatial-temporal transformer for 3d point cloud sequences", "[166] M. A. Uy, Q.-H. Pham, B.-S. Hua, D. T. Nguyen, and S.-K. Yeung, \u201cRevisiting point cloud classification: A new benchmark dataset and classification model on real-world data,\u201d in ICCV, 2019, pp. 1588\u20131597.": "Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data", "[102] D. Yang, A. Myronenko, X. Wang, Z. Xu, H. R. Roth, and D. Xu, \u201cT-automl: Automated machine learning for lesion segmentation using transformers in 3d medical imaging,\u201d in CVPR, 2021, pp. 3962\u20133974.": "T-automl: Automated Machine Learning for Lesion Segmentation Using Transformers in 3D Medical Imaging", "[82] M.-Q. Dao, E. H\u00e9ry, and V. Fr\u00e9mont, \u201cAttention-based proposals refinement for 3d object detection,\u201d arXiv preprint arXiv:2201.07070, 2022.": "Attention-based proposals refinement for 3d object detection", "[136] L. Huang, J. Tan, J. Liu, and J. Yuan, \u201cHand-transformer: Non-autoregressive structured modeling for 3d hand pose estimation,\u201d in ECCV, 2020, pp. 17\u201333.": "Hand-transformer: Non-autoregressive structured modeling for 3d hand pose estimation", "[186] J. Wang, S. Yan, Y. Xiong, and D. Lin, \u201cMotion guided 3d pose estimation from videos,\u201d in ECCV, 2020, pp. 764\u2013780.": "Motion guided 3d pose estimation from videos", "[26] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon, \u201cDynamic graph cnn for learning on point clouds,\u201d ACM TOG, vol. 38, no. 5, pp. 1\u201312, 2019.": "Dynamic Graph CNN for Learning on Point Clouds", "[46] Z. Liu, Z. Zhang, Y. Cao, H. Hu, and X. Tong, \u201cGroup-free 3d object detection via transformers,\u201d in CVPR, 2021, pp. 2949\u20132958.": "Group-free 3d object detection via transformers", "[120] P. Xiang, X. Wen, Y.-S. Liu, Y.-P. Cao, P. Wan, W. Zheng, and Z. Han, \u201cSnowflakenet: Point cloud completion by snowflake point deconvolution with skip-transformer,\u201d in ICCV, 2021, pp. 5499\u20135501.": "Snowflakenet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer", "[71] X. Pan, Z. Xia, S. Song, L. E. Li, and G. Huang, \u201c3d object detection with pointformer,\u201d in CVPR, 2021, pp. 7463\u20137472.": "3D Object Detection with Pointformer", "[72] H. Sheng, S. Cai, Y. Liu, B. Deng, J. Huang, X.-S. Hua, and M.-J. Zhao, \u201cImproving 3d object detection with channel-wise transformer,\u201d in CVPR, 2021, pp. 2743\u20132752.": "Improving 3d object detection with channel-wise transformer", "[172] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, \u201cPcn: Point completion network,\u201d in 3DV, 2018, pp. 728\u2013737.": "PCN: Point Completion Network", "[182] Z. Yang, Y. Sun, S. Liu, and J. Jia, \u201c3dssd: Point-based 3d single stage object detector,\u201d in CVPR, 2020, pp. 11\u2009040\u201311\u2009048.": "3dssd: Point-based 3d single stage object detector", "[77] L. Fan, Z. Pang, T. Zhang, Y.-X. Wang, H. Zhao, F. Wang, N. Wang, and Z. Zhang, \u201cEmbracing single stride 3d object detector with sparse transformer,\u201d arXiv preprint arXiv:2112.06375, 2021.": "Embracing single stride 3d object detector with sparse transformer", "[97] A. Thyagharajan, B. Ummenhofer, P. Laddha, O. J. Omer, and S. Subramoney, \u201cSegment-fusion: Hierarchical context fusion for robust 3d semantic segmentation,\u201d in CVPR, 2022, pp. 1236\u20131245.": "Segment-fusion: Hierarchical context fusion for robust 3d semantic segmentation", "[64] Z. Cheng, H. Wan, X. Shen, and Z. Wu, \u201cPatchformer: A versatile 3d transformer based on patch attention,\u201d arXiv preprint arXiv:2111.00207, 2021.": "Patchformer: A versatile 3d transformer based on patch attention", "[93] S. Deng, Z. Liang, L. Sun, and K. Jia, \u201cVista: Boosting 3d object detection via dual cross-view spatial attention,\u201d in CVPR, 2022, pp. 8448\u20138457.": "Vista: Boosting 3d object detection via dual cross-view spatial attention", "[86] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, \u201cScanet: Spatial-channel attention network for 3d object detection,\u201d in ICASSP, 2019, pp. 1992\u20131996.": "Scanet: Spatial-channel attention network for 3d object detection", "[88] X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, \u201cTransfusion: Robust lidar-camera fusion for 3d object detection with transformers,\u201d arXiv preprint arXiv:2203.11496, 2022.": "Transfusion: Robust lidar-camera fusion for 3d object detection with transformers", "[100] S. Xu, R. Wan, M. Ye, X. Zou, and T. Cao, \u201cSparse cross-scale attention network for efficient lidar panoptic segmentation,\u201d arXiv preprint arXiv:2201.05972, 2022.": "Sparse Cross-Scale Attention Network for Efficient Lidar Panoptic Segmentation", "[145] W. Goodwin, S. Vaze, I. Havoutis, and I. Posner, \u201cZero-shot category-level object pose estimation,\u201d arXiv preprint arXiv:2204.03635, 2022.": "Zero-shot category-level object pose estimation", "[58] X.-F. Han, Y.-F. Jin, H.-X. Cheng, and G.-Q. Xiao, \u201cDual transformer for point cloud analysis,\u201d arXiv preprint arXiv:2104.13044, 2021.": "Dual transformer for point cloud analysis", "[130] W. Shan, Z. Liu, X. Zhang, S. Wang, S. Ma, and W. Gao, \u201cP-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.07628, 2022.": "P-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation", "[107] H.-Y. Zhou, J. Guo, Y. Zhang, L. Yu, L. Wang, and Y. Yu, \u201cnnformer: Interleaved transformer for volumetric segmentation,\u201d arXiv preprint arXiv:2109.03201, 2021.": "nnformer: Interleaved transformer for volumetric segmentation", "[28] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas, \u201cKpconv: Flexible and deformable convolution for point clouds,\u201d in CVPR, 2019, pp. 6411\u20136420.": "Kpconv: Flexible and deformable convolution for point clouds", "[126] Z. Cao, W. Zhang, X. Wen, Z. Dong, Y. shen Liu, and B. Yang, \u201cMfm-net: Unpaired shape completion network with multi-stage feature matching,\u201d arXiv preprint arXiv:2111.11976, 2021.": "Mfm-net: Unpaired shape completion network with multi-stage feature matching", "[91] D.-K. Nguyen, J. Ju, O. Booji, M. R. Oswald, and C. G. Snoek, \u201cBoxer: Box-attention for 2d and 3d transformers,\u201d arXiv preprint arXiv:2111.13087, 2021.": "Boxer: Box-attention for 2d and 3d transformers", "[143] L. Zou, Z. Huang, N. Gu, and G. Wang, \u201c6d-vit: Category-level 6d object pose estimation via transformer-based instance representation learning,\u201d arXiv preprint arXiv:2110.04792, 2021.": "6d-vit: Category-level 6d object pose estimation via transformer-based instance representation learning", "[174] D. Mehta, H. Rhodin, D. Casas, P. Fua, O. Sotnychenko, W. Xu, and C. Theobalt, \u201cMonocular 3d human pose estimation in the wild using improved cnn supervision,\u201d in 3DV, 2017, pp. 506\u2013516.": "Monocular 3D Human Pose Estimation in the Wild using Improved CNN Supervision", "[80] Y. Lan, Y. Duan, C. Liu, C. Zhu, Y. Xiong, H. Huang, and K. Xu, \u201cArm3d: Attention-based relation module for indoor 3d object detection,\u201d Computational Visual Media, pp. 1\u201320, 2022.": "Arm3d: Attention-based Relation Module for Indoor 3D Object Detection", "[48] Y. Xie, J. Zhang, C. Shen, and Y. Xia, \u201cCotr: Efficiently bridging cnn and transformer for 3d medical image segmentation,\u201d in MICCAI, 2021, pp. 171\u2013180.": "Cotr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation", "[23] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in CVPR, 2017, pp. 652\u2013660.": "Pointnet: Deep learning on point sets for 3d classification and segmentation", "[54] C. Zhang, H. Wan, S. Liu, X. Shen, and Z. Wu, \u201cPvt: Point-voxel transformer for 3d deep learning,\u201d arXiv preprint arXiv:2108.06076, 2021.": "Pvt: Point-voxel transformer for 3d deep learning", "[109] X. Yan, H. Tang, S. Sun, H. Ma, D. Kong, and X. Xie, \u201cAfter-unet: Axial fusion transformer unet for medical image segmentation,\u201d in WACV, 2022, pp. 3971\u20133981.": "After-unet: Axial Fusion Transformer Unet for Medical Image Segmentation", "[167] S. Song, S. P. Lichtenberg, and J. Xiao, \u201cSun rgb-d: A rgb-d scene understanding benchmark suite,\u201d in CVPR, 2015, pp. 567\u2013576.": "Sun RGB-D: A RGB-D Scene Understanding Benchmark Suite", "[45] I. Misra, R. Girdhar, and A. Joulin, \u201cAn end-to-end transformer model for 3d object detection,\u201d in CVPR, 2021, pp. 2906\u20132917.": "An end-to-end transformer model for 3d object detection", "[111] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. Roth, and D. Xu, \u201cSwin unetr: Swin transformers for semantic segmentation of brain tumors in mri images,\u201d arXiv preprint arXiv:2201.01266, 2022.": "Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images", "[135] M. J. Tyszkiewicz, K.-K. Maninis, S. Popov, and V. Ferrari, \u201cRaytran: 3d pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers,\u201d arXiv preprint arXiv:2203.13296, 2022.": "Raytran: 3d pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers", "[121] Z. Su, H. Huang, C. Ma, H. Huang, and R. Hu, \u201cPoint cloud completion on structured feature map with feedback network,\u201d Computational Visual Media, 2022.": "Point Cloud Completion on Structured Feature Map with Feedback Network", "[84] C. R. Qi, O. Litany, K. He, and L. J. Guibas, \u201cDeep hough voting for 3d object detection in point clouds,\u201d in CVPR, 2019, pp. 9277\u20139286.": "Deep Hough Voting for 3D Object Detection in Point Clouds", "[96] X. Lai, J. Liu, L. Jiang, L. Wang, H. Zhao, S. Liu, X. Qi, and J. Jia, \u201cStratified transformer for 3d point cloud segmentation,\u201d in arXiv preprint arXiv:2203.14508, 2022.": "Stratified transformer for 3d point cloud segmentation", "[127] C. Zheng, S. Zhu, M. Mendieta, T. Yang, C. Chen, and Z. Ding, \u201c3d human pose estimation with spatial and temporal transformers,\u201d in ICCV, 2021, pp. 11\u2009656\u201311\u2009665.": "3D Human Pose Estimation with Spatial and Temporal Transformers", "[180] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cPv-rcnn: Point-voxel feature set abstraction for 3d object detection,\u201d in CVPR, 2020, pp. 10\u2009529\u201310\u2009538.": "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection", "[63] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, \u201cMasked autoencoders for point cloud self-supervised learning,\u201d arXiv preprint arXiv:2203.06604, 2022.": "Masked Autoencoders for Point Cloud Self-Supervised Learning", "[83] Q. Xie, Y.-K. Lai, J. Wu, Z. Wang, Y. Zhang, K. Xu, and J. Wang, \u201cMlcvnet: Multi-level context votenet for 3d object detection,\u201d in CVPR, 2020, pp. 10\u2009447\u201310\u2009456.": "Mlcvnet: Multi-level context votenet for 3d object detection", "[124] X. Yan, L. Lin, N. J. Mitra, D. Lischinski, D. Cohen-Or, and H. Huang, \u201cShapeformer: Transformer-based shape completion via sparse representation,\u201d in CVPR, 2022, pp. 6239\u20136249.": "Shapeformer: Transformer-based shape completion via sparse representation", "[81] Z. Yuan, X. Song, L. Bai, Z. Wang, and W. Ouyang, \u201cTemporal-channel transformer for 3d lidar-based video object detection for autonomous driving,\u201d TCSVT, vol. 32, no. 4, pp. 2068\u20132078, 2022.": "Temporal-Channel Transformer for 3D LiDAR-Based Video Object Detection for Autonomous Driving", "[92] Y. Wang, T. Ye, L. Cao, W. Huang, F. Sun, F. He, and D. Tao, \u201cBridged transformer for vision and point cloud 3d object detection,\u201d in CVPR, 2022, pp. 12\u2009114\u201312\u2009123.": "Bridged transformer for vision and point cloud 3d object detection", "[108] Q. Jia and H. Shu, \u201cBitr-unet: a cnn-transformer combined network for mri brain tumor segmentation,\u201d arXiv preprint arXiv:2109.12271, 2021.": "Bitr-unet: a cnn-transformer combined network for mri brain tumor segmentation", "[170] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in CVPR, 2020, pp. 11\u2009621\u201311\u2009631.": "nuscenes: A multimodal dataset for autonomous driving", "[61] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, \u201cPoint-bert: Pre-training 3d point cloud transformers with masked point modeling,\u201d arXiv preprint arXiv:2111.14819, 2021.": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling", "[132] W. Li, H. Liu, H. Tang, P. Wang, and L. V. Gool, \u201cMhformer: Multi-hypothesis transformer for 3d human pose estimation,\u201d in CVPR, 2022, pp. 13\u2009147\u201313\u2009156.": "Mhformer: Multi-hypothesis transformer for 3d human pose estimation", "[75] P. Bhattacharyya, C. Huang, and K. Czarnecki, \u201cSa-det3d: Self-attention based context-aware 3d object detection,\u201d in CVPR, 2021, pp. 3022\u20133031.": "Sa-det3d: Self-attention based context-aware 3d object detection", "[94] J. S. Hu, T. Kuai, and S. L. Waslander, \u201cPoint density-aware voxels for lidar 3d object detection,\u201d in CVPR, 2022, pp. 8469\u20138478.": "Point density-aware voxels for lidar 3d object detection", "[55] Y. Zhou, A. Ji, and L. Zhang, \u201cSewer defect detection from 3d point clouds using a transformer-based deep learning model,\u201d Automation in Construction, vol. 136, p. 104163, 2022.": "Sewer Defect Detection from 3D Point Clouds Using a Transformer-Based Deep Learning Model", "[177] Z. Zhang, B. Sun, H. Yang, and Q. Huang, \u201cH3dnet: 3d object detection using hybrid geometric primitives,\u201d in ECCV, 2020, pp. 311\u2013329.": "H3dnet: 3d object detection using hybrid geometric primitives", "[118] Y. Wang, D. J. Tan, N. Navab, and F. Tombari, \u201cLearning local displacements for point cloud completion,\u201d in CVPR, 2022, pp. 1568\u20131577.": "Learning Local Displacements for Point Cloud Completion", "[43] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, \u201cUnetr: Transformers for 3d medical image segmentation,\u201d in WACV, 2022, pp. 574\u2013584.": "Unetr: Transformers for 3d medical image segmentation", "[89] Y. Zhang, J. Chen, and D. Huang, \u201cCat-det: Contrastively augmented transformer for multi-modal 3d object detection,\u201d arXiv preprint arXiv:2204.00325, 2022.": "Cat-det: Contrastively Augmented Transformer for Multi-Modal 3D Object Detection", "[176] Y. Liu, B. Fan, S. Xiang, and C. Pan, \u201cRelation-shape convolutional neural network for point cloud analysis,\u201d in CVPR, 2019, pp. 8895\u20138904.": "Relation-shape convolutional neural network for point cloud analysis", "[110] H. Peiris, M. Hayat, Z. Chen, G. Egan, and M. Harandi, \u201cA volumetric transformer for accurate 3d tumor segmentation,\u201d arXiv preprint arXiv:2111.13300, 2021.": "A volumetric transformer for accurate 3d tumor segmentation", "[184] T. Wang, X. Zhu, J. Pang, and D. Lin, \u201cFcos3d: Fully convolutional one-stage monocular 3d object detection,\u201d in CVPR, 2021, pp. 913\u2013922.": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection", "[65] X.-F. Han, Z.-Y. He, J. Chen, and G.-Q. Xiao, \u201c3crossnet: Cross-level cross-scale cross-attention network for point cloud representation,\u201d RA-L, vol. 7, no. 2, pp. 3718\u20133725, 2022.": "3crossnet: Cross-level cross-scale cross-attention network for point cloud representation", "[165] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017.": "Joint 2d-3d-semantic data for indoor scene understanding", "[62] S. Liu, K. Fu, M. Wang, and Z. Song, \u201cGroup-in-group relation-based transformer for 3d point cloud learning,\u201d Remote. Sens., vol. 14, no. 7, p. 1563, 2022.": "Group-in-group relation-based transformer for 3d point cloud learning", "[163] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao, \u201c3d shapenets: A deep representation for volumetric shapes,\u201d in CVPR, 2015, pp. 1912\u20131920.": "3D ShapeNets: A Deep Representation for Volumetric Shapes", "[105] W. Wang, C. Chen, M. Ding, H. Yu, S. Zha, and J. Li, \u201cTransbts: Multimodal brain tumor segmentation using transformer,\u201d in MICCAI, 2021, pp. 109\u2013119.": "Transbts: Multimodal Brain Tumor Segmentation Using Transformer", "[141] W. Guo, E. Corona, F. Moreno-Noguer, and X. Alameda-Pineda, \u201cPi-net: Pose interacting network for multi-person monocular 3d pose estimation,\u201d in WACV, 2021, pp. 2796\u20132806.": "Pi-net: Pose Interacting Network for Multi-Person Monocular 3D Pose Estimation", "[129] W. Li, H. Liu, R. Ding, M. Liu, P. Wang, and W. Yang, \u201cExploiting temporal contexts with strided transformer for 3d human pose estimation,\u201d TMM, 2022.": "Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation", "[138] K. Lin, L. Wang, and Z. Liu, \u201cEnd-to-end human pose and mesh reconstruction with transformers,\u201d in CVPR, 2021, pp. 1954\u20131963.": "End-to-end human pose and mesh reconstruction with transformers", "[38] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointpillars: Fast encoders for object detection from point clouds,\u201d in CVPR, 2019, pp. 12\u2009697\u201312\u2009705.": "Pointpillars: Fast encoders for object detection from point clouds", "[25] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d in NeurIPS, 2017, pp. 5099\u20135108.": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space", "[98] H. Fan, Y. Yang, and M. Kankanhalli, \u201cPoint 4d transformer networks for spatio-temporal modeling in point cloud videos,\u201d in CVPR, 2021, pp. 14\u2009204\u201314\u2009213.": "Point 4d transformer networks for spatio-temporal modeling in point cloud videos", "[173] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, \u201cHuman3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments,\u201d TPAMI, vol. 36, no. 7, pp. 1325\u20131339, 2014.": "Human36m: Large scale datasets and predictive methods for 3d human sensing in natural environments", "[168] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d in CVPR, 2017, pp. 5828\u20135839.": "Scannet: Richly-annotated 3d reconstructions of indoor scenes", "[112] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in CVPR, 2021, pp. 10\u2009012\u201310\u2009022.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[73] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon, \u201cDetr3d: 3d object detection from multi-view images via 3d-to-2d queries,\u201d in CoRL, 2022, pp. 180\u2013191.": "Detr3d: 3d object detection from multi-view images via 3d-to-2d queries", "[53] X. Zhang, X. Zhou, M. Lin, and J. Sun, \u201cShufflenet: An extremely efficient convolutional neural network for mobile devices,\u201d in CVPR, 2018, pp. 6848\u20136856.": "Shufflenet: An extremely efficient convolutional neural network for mobile devices", "[78] C. Park, Y. Jeong, M. Cho, and J. Park, \u201cFast point transformer,\u201d arXiv preprint arXiv:2112.04702, 2021.": "Fast point transformer", "[79] C. He, R. Li, S. Li, and L. Zhang, \u201cVoxel set transformer: A set-to-set approach to 3d object detection from point clouds,\u201d arXiv preprint arXiv:2203.10314, 2022.": "Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds", "[104] B. Yun, Y. Wang, J. Chen, H. Wang, W. Shen, and Q. Li, \u201cSpectr: Spectral transformer for hyperspectral pathology image segmentation,\u201d arXiv preprint arXiv:2103.03604, 2021.": "Spectr: Spectral transformer for hyperspectral pathology image segmentation", "[68] X.-F. Han, Y.-J. Kuang, and G.-Q. Xiao, \u201cPoint cloud learning with transformer,\u201d arXiv preprint arXiv:2104.13636, 2021.": "Point cloud learning with transformer", "[119] J. Wang, Y. Cui, D. Guo, J. Li, Q. Liu, and C. Shen, \u201cPointattn: You only need attention for point cloud completion,\u201d arXiv preprint arXiv:2203.08485, 2021.": "Pointattn: You only need attention for point cloud completion", "[56] H. Huang and Y. Fang, \u201cAdaptive wavelet transformer network for 3d shape representation learning,\u201d in ICLR, 2021.": "Adaptive Wavelet Transformer Network for 3D Shape Representation Learning", "[49] Y. Zhang, H. Liu, and Q. Hu, \u201cTransfuse: Fusing transformers and cnns for medical image segmentation,\u201d in MICCAI, 2021, pp. 14\u201324.": "Transfuse: Fusing Transformers and CNNs for Medical Image Segmentation", "[87] R. Zhang, H. Qiu, T. Wang, X. Xu, Z. Guo, Y. Qiao, P. Gao, and H. Li, \u201cMonodetr: Depth-aware transformer for monocular 3d object detection,\u201d arXiv preprint arXiv:2203.13310, 2022.": "Monodetr: Depth-aware transformer for monocular 3d object detection", "[140] S. Hampali, S. D. Sarkar, M. Rad, and V. Lepetit, \u201cKeypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation,\u201d arXiv preprint arXiv:2104.14639, 2021.": "Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation", "[133] W. Zhao, W. Wang, and Y. Tian, \u201cGraformer: Graph convolution transformer for 3d pose estimation,\u201d in CVPR, 2022, pp. 20\u2009438\u201320\u2009447.": "Graformer: Graph Convolution Transformer for 3D Pose Estimation", "[67] L. Wu, X. Liu, and Q. Liu, \u201cCentroid transformers: Learning to abstract with attention,\u201d arXiv preprint arXiv:2102.08606, 2021.": "Centroid transformers: Learning to abstract with attention", "[41] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, \u201cPct: Point cloud transformer,\u201d Computational Visual Media, vol. 7, no. 2, pp. 187\u2013199, 2021.": "Pct: Point cloud transformer", "[134] Y. He, R. Yan, K. Fragkiadaki, and S.-I. Yu, \u201cEpipolar transformer for multi-view human pose estimation,\u201d in CVPR Workshops, 2020, pp. 4466\u20134471.": "Epipolar transformer for multi-view human pose estimation", "[125] Z. Lyu, Z. Kong, X. Xu, L. Pan, and D. Lin, \u201cA conditional point diffusion-refinement paradigm for 3d point cloud completion,\u201d in ICLR, 2022.": "A conditional point diffusion-refinement paradigm for 3d point cloud completion", "[131] J. Zhang, Z. Tu, J. Yang, Y. Chen, and J. Yuan, \u201cMixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video,\u201d in CVPR, 2022, pp. 13\u2009232\u201313\u2009242.": "Mixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video", "[42] J. Mao, Y. Xue, M. Niu, H. Bai, J. Feng, X. Liang, H. Xu, and C. Xu, \u201cVoxel transformer for 3d object detection,\u201d in CVPR, 2021, pp. 3164\u20133173.": "Voxel transformer for 3d object detection", "[90] Y. Liu, T. Wang, X. Zhang, and J. Sun, \u201cPetr: Position embedding transformation for multi-view 3d object detection,\u201d arXiv preprint arXiv:2203.05625, 2022.": "Petr: Position Embedding Transformation for Multi-View 3D Object Detection", "[76] T. Guan, J. Wang, S. Lan, R. Chandra, Z. Wu, L. Davis, and D. Manocha, \u201cM3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers,\u201d in WACV, 2022, pp. 772\u2013782.": "M3detr: Multi-representation, multi-scale, mutual-relation 3d object detection with transformers", "[128] M. Hassanin, A. Khamiss, M. Bennamoun, F. Boussaid, and I. Radwan, \u201cCrossformer: Cross spatio-temporal transformer for 3d human pose estimation,\u201d arXiv preprint arXiv:2203.13387, 2022.": "Crossformer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation", "[144] Z. Dang, L. Wang, Y. Guo, and M. Salzmann, \u201cLearning-based point cloud registration for 6d object pose estimation in the real world,\u201d arXiv preprint arXiv:2203.15309, 2022.": "Learning-based point cloud registration for 6d object pose estimation in the real world", "[123] P. Mittal, Y.-C. Cheng, M. Singh, and S. Tulsiani, \u201cAutosdf: Shape priors for 3d completion, reconstruction and generation,\u201d in CVPR, 2022, pp. 306\u2013315.": "Autosdf: Shape priors for 3d completion, reconstruction and generation", "[47] S. Xie, S. Liu, Z. Chen, and Z. Tu, \u201cAttentional shapecontextnet for point cloud recognition,\u201d in CVPR, 2018, pp. 4606\u20134615.": "Attentional ShapeContextNet for Point Cloud Recognition", "[85] J. Yin, J. Shen, C. Guan, D. Zhou, and R. Yang, \u201cLidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention,\u201d in CVPR, 2020, pp. 11\u2009495\u201311\u2009504.": "Lidar-based online 3d video object detection with graph-based message passing and spatiotemporal transformer attention", "[117] X. Yu, Y. Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou, \u201cPointr: Diverse point cloud completion with geometry-aware transformers,\u201d in ICCV, 2021, pp. 12\u2009498\u201312\u2009507.": "Pointr: Diverse point cloud completion with geometry-aware transformers", "[106] S. Li, X. Sui, X. Luo, X. Xu, Y. Liu, and R. Goh, \u201cMedical image segmentation using squeeze-and-expansion transformers,\u201d arXiv preprint arXiv:2105.09511, 2021.": "Medical Image Segmentation Using Squeeze-and-Expansion Transformers", "[171] L. P. Tchapmi, V. Kosaraju, S. H. Rezatofighi, I. Reid, and S. Savarese, \u201cTopnet: Structural point cloud decoder,\u201d in CVPR, 2019, pp. 383\u2013392.": "Topnet: Structural Point Cloud Decoder", "[74] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in ECCV, 2020, pp. 213\u2013229.": "End-to-end object detection with transformers", "[137] J. Cheng, Y. Wan, D. Zuo, C. Ma, J. Gu, P. Tan, H. Wang, X. Deng, and Y. Zhang, \u201cEfficient virtual view selection for 3d hand pose estimation,\u201d in AAAI, 2022, pp. 419\u2013426.": "Efficient virtual view selection for 3D hand pose estimation", "[181] Y. Zhang, J. Lu, and J. Zhou, \u201cObjects are different: Flexible monocular 3d object detection,\u201d in CVPR, 2021, pp. 3289\u20133298.": "Flexible Monocular 3D Object Detection", "[142] N. Ugrinovic, A. Ruiz, A. Agudo, A. Sanfeliu, and F. Moreno-Noguer, \u201cPermutation-invariant relational network for multi-person 3d pose estimation,\u201d arXiv preprint arXiv:2204.04913, 2022.": "Permutation-invariant relational network for multi-person 3d pose estimation", "[185] X. Wen, P. Xiang, Z. Han, Y.-P. Cao, P. Wan, W. Zheng, and Y.-S. Liu, \u201cPmp-net: Point cloud completion by learning multi-step point moving paths,\u201d in CVPR, 2021, pp. 7443\u20137452.": "Pmp-net: Point Cloud Completion by Learning Multi-Step Point Moving Paths"}, "source_title_to_arxiv_id": {"Spatial-temporal transformer for 3d point cloud sequences": "2110.09783", "Group-free 3d object detection via transformers": "2104.00678", "Improving 3d object detection with channel-wise transformer": "2108.10723", "Dual transformer for point cloud analysis": "2104.13044", "nnformer: Interleaved transformer for volumetric segmentation": "2109.03201", "Boxer: Box-attention for 2d and 3d transformers": "2111.13087", "Sa-det3d: Self-attention based context-aware 3d object detection": "2101.02672", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Detr3d: 3d object detection from multi-view images via 3d-to-2d queries": "2110.06922", "Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds": "2203.10314", "A conditional point diffusion-refinement paradigm for 3d point cloud completion": "2112.03530", "Crossformer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation": "2203.13387"}}