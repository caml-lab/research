{"title": "Out-of-Vocabulary Challenge Report", "abstract": "This paper presents final results of the Out-Of-Vocabulary 2022 (OOV)\nchallenge. The OOV contest introduces an important aspect that is not commonly\nstudied by Optical Character Recognition (OCR) models, namely, the recognition\nof unseen scene text instances at training time. The competition compiles a\ncollection of public scene text datasets comprising of 326,385 images with\n4,864,405 scene text instances, thus covering a wide range of data\ndistributions. A new and independent validation and test set is formed with\nscene text instances that are out of vocabulary at training time. The\ncompetition was structured in two tasks, end-to-end and cropped scene text\nrecognition respectively. A thorough analysis of results from baselines and\ndifferent participants is presented. Interestingly, current state-of-the-art\nmodels show a significant performance gap under the newly studied setting. We\nconclude that the OOV dataset proposed in this challenge will be an essential\narea to be explored in order to develop scene text models that achieve more\nrobust and generalized predictions.", "authors": ["Sergi Garcia-Bordils", "Andr\u00e9s Mafla", "Ali Furkan Biten", "Oren Nuriel", "Aviad Aberdam", "Shai Mazor", "Ron Litman", "Dimosthenis Karatzas"], "published_date": "2022_09_14", "pdf_url": "http://arxiv.org/pdf/2209.06717v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td>Average</td><td colspan=\"3\">All</td><td colspan=\"3\">OOV</td><td colspan=\"3\">IV</td></tr><tr><td>Hmean</td><td>P</td><td>R</td><td>Hmean</td><td>P</td><td>R</td><td>Hmean</td><td>P</td><td>R</td><td>Hmean</td></tr><tr><th>TESTR[42]</th><td>15.9</td><td>31.4</td><td>20.1</td><td>25.1</td><td>4.4</td><td>17.8</td><td>7.1</td><td>29.2</td><td>21.4</td><td>24.6</td></tr><tr><th>TextTranSpotter [15]</th><td>18.6</td><td>37.4</td><td>25.0</td><td>29.9</td><td>4.5</td><td>16.4</td><td>7.0</td><td>35.5</td><td>26.2</td><td>30.1</td></tr><tr><th>GLASS[32]</th><td>34.9</td><td>75.8</td><td>30.6</td><td>43.6</td><td>24.9</td><td>27.2</td><td>26.0</td><td>73.7</td><td>31.1</td><td>43.7</td></tr><tr><th>CLOVA OCR DEER</th><td>42.39</td><td>67.17</td><td>52.04</td><td>58.64</td><td>18.58</td><td>48.72</td><td>26.9</td><td>64.51</td><td>52.49</td><td>57.88</td></tr><tr><th>Detector Free E2E</th><td>42.01</td><td>66.15</td><td>52.44</td><td>58.5</td><td>17.97</td><td>49.35</td><td>26.35</td><td>63.44</td><td>52.86</td><td>57.67</td></tr><tr><th>oCLIP_v2</th><td>41.33</td><td>67.37</td><td>46.82</td><td>55.24</td><td>20.28</td><td>48.42</td><td>28.59</td><td>64.41</td><td>46.6</td><td>54.08</td></tr><tr><th>DB threshold2 TRBA</th><td>39.1</td><td>64.08</td><td>49.93</td><td>56.13</td><td>15.26</td><td>42.29</td><td>22.43</td><td>61.6</td><td>50.96</td><td>55.78</td></tr><tr><th>E2E_Mask</th><td>32.13</td><td>47.9</td><td>54.14</td><td>50.83</td><td>8.64</td><td>46.73</td><td>14.58</td><td>45.2</td><td>55.14</td><td>49.68</td></tr><tr><th>YYDS</th><td>28.68</td><td>51.53</td><td>35.54</td><td>42.07</td><td>10.63</td><td>33.36</td><td>16.12</td><td>48.57</td><td>35.83</td><td>41.24</td></tr><tr><th>sudokill-9</th><td>28.34</td><td>51.62</td><td>34.08</td><td>41.06</td><td>11.03</td><td>33.22</td><td>16.56</td><td>48.54</td><td>34.2</td><td>40.12</td></tr><tr><th>PAN</th><td>28.13</td><td>50.5</td><td>34.81</td><td>41.21</td><td>10.5</td><td>33.58</td><td>16.0</td><td>47.45</td><td>34.98</td><td>40.27</td></tr><tr><th>oCLIP</th><td>24.04</td><td>47.72</td><td>7.51</td><td>12.98</td><td>41.21</td><td>48.42</td><td>44.52</td><td>17.46</td><td>1.98</td><td>3.55</td></tr><tr><th>DBNetpp</th><td>20.34</td><td>39.42</td><td>27.0</td><td>32.05</td><td>5.62</td><td>20.75</td><td>8.85</td><td>37.15</td><td>27.84</td><td>31.83</td></tr><tr><th>TH-DL</th><td>9.32</td><td>18.39</td><td>13.23</td><td>15.39</td><td>2.16</td><td>10.87</td><td>3.6</td><td>16.89</td><td>13.55</td><td>15.04</td></tr></tbody></table>", "caption": "Table 2: Harmonic mean, precision and recall for the entire dataset (All), in vocabulary (IV) and out of vocabulary (OOV) across different baseline methods. The average Hmean is the unweighted average of IV and OOV.", "list_citation_info": ["[15] Kittenplon, Y., Lavi, I., Fogel, S., Bar, Y., Manmatha, R., Perona, P.: Towards weakly-supervised text spotting using a multi-task transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4604\u20134613 (2022)", "[42] Zhang, X., Su, Y., Tripathi, S., Tu, Z.: Text spotting transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9519\u20139528 (2022)", "[32] Ronen, R., Tsiper, S., Anschel, O., Lavi, I., Markovitz, A., Manmatha, R.: Glass: Global to local attention for scene-text spotting. arXiv preprint arXiv:2208.03364 (2022)"]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td rowspan=\"2\">Train Set</td><td>Total</td><td colspan=\"2\">IV</td><td colspan=\"2\">OOV</td></tr><tr><td>Word Acc\\uparrow</td><td>Word Acc\\uparrow</td><td>ED\\downarrow</td><td>Word Acc\\uparrow</td><td>ED\\downarrow</td></tr><tr><th>ABINet [8]</th><td>Syn</td><td>38.01</td><td>50.29</td><td>342,552</td><td>25.73</td><td>115536</td></tr><tr><th>Baek et al [3]</th><td>Syn</td><td>44.47</td><td>52.61</td><td>365,566</td><td>36.34</td><td>114,101</td></tr><tr><th>SCATTER [22]</th><td>Syn</td><td>47.79</td><td>56.85</td><td>321,101</td><td>38.74</td><td>103,928</td></tr><tr><th>ABINet [8]</th><td>Real</td><td>59.84</td><td>71.13</td><td>176,126</td><td>48.55</td><td>67478</td></tr><tr><th>Baek et al [3]</th><td>Real</td><td>64.97</td><td>75.98</td><td>138,479</td><td>53.96</td><td>54,346</td></tr><tr><th>SCATTER [22]</th><td>Real</td><td>66.68</td><td>77.98</td><td>128,219</td><td>55.38</td><td>52,535</td></tr><tr><th>OCRFLY_v2</th><td>Syn + Real</td><td>70.31</td><td>81.02</td><td>123,947</td><td>59.61</td><td>46,048</td></tr><tr><th>OOV3decode</th><td>Syn + Real</td><td>70.22</td><td>81.58</td><td>94,259</td><td>58.86</td><td>40,175</td></tr><tr><th>VTBM</th><td>Syn + Real</td><td>70.00</td><td>81.36</td><td>94,701</td><td>58.64</td><td>40,187</td></tr><tr><th>DAT</th><td>-</td><td>69.90</td><td>80.78</td><td>96,513</td><td>59.03</td><td>40,082</td></tr><tr><th>OCRFLY</th><td>Syn + Real</td><td>69.83</td><td>80.63</td><td>131,232</td><td>59.03</td><td>53,243</td></tr><tr><th>GGUI</th><td>-</td><td>69.80</td><td>80.74</td><td>96,597</td><td>58.86</td><td>40,171</td></tr><tr><th>vitE3DCV</th><td>Syn + Real</td><td>69.74</td><td>80.74</td><td>96,477</td><td>58.74</td><td>40,115</td></tr><tr><th>DataMatters</th><td>Syn + Real</td><td>69.68</td><td>80.71</td><td>96,544</td><td>58.65</td><td>40,177</td></tr><tr><th>MaskOCR</th><td>Real</td><td>69.63</td><td>80.60</td><td>108,894</td><td>58.65</td><td>44,971</td></tr><tr><th>SCATTER</th><td>Syn + Real</td><td>69.58</td><td>79.72</td><td>113,482</td><td>59.45</td><td>43,89</td></tr><tr><th>Summer</th><td>Syn + Real</td><td>68.77</td><td>79.48</td><td>103,211</td><td>58.06</td><td>42,118</td></tr><tr><th>LMSS</th><td>Syn + Real</td><td>68.46</td><td>80.81</td><td>116,503</td><td>56.11</td><td>51,165</td></tr><tr><th>UORD</th><td>Real</td><td>68.28</td><td>79.28</td><td>118,185</td><td>57.27</td><td>48,517</td></tr><tr><th>PTVIT</th><td>Syn + Real</td><td>66.29</td><td>77.52</td><td>120,449</td><td>55.06</td><td>49,41</td></tr><tr><th>GORDON</th><td>Syn + Real</td><td>65.86</td><td>77.25</td><td>124,347</td><td>54.47</td><td>48,907</td></tr><tr><th>TRBA_CocoValid</th><td>Syn + Real</td><td>63.98</td><td>77.76</td><td>132,781</td><td>50.20</td><td>60,693</td></tr><tr><th>HuiGuan</th><td>Real</td><td>63.73</td><td>74.77</td><td>162,87</td><td>52.69</td><td>68,926</td></tr><tr><th>EOCR</th><td>-</td><td>46.66</td><td>55.30</td><td>350,166</td><td>38.02</td><td>113,317</td></tr><tr><th>NNRC</th><td>-</td><td>38.54</td><td>45.36</td><td>405,603</td><td>31.73</td><td>136,384</td></tr><tr><th>NN</th><td>-</td><td>37.17</td><td>43.38</td><td>426,074</td><td>30.97</td><td>144,032</td></tr><tr><th>CCL</th><td>Real</td><td>31.06</td><td>47.40</td><td>552,57</td><td>14.73</td><td>202,087</td></tr></tbody></table>", "caption": "Table 3: Word accuracy and Edit Distance for state of the art recognition models trained on different datasets.", "list_citation_info": ["[8] Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: autonomous, bidirectional and iterative language modeling for scene text recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7098\u20137107 (2021)", "[22] Litman, R., Anschel, O., Tsiper, S., Litman, R., Mazor, S., Manmatha, R.: Scatter: selective context attentional scene text recognizer. In: proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11962\u201311972 (2020)", "[3] Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with scene text recognition model comparisons? dataset and model analysis. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 4715\u20134723 (2019)"]}], "citation_info_to_title": {"[42] Zhang, X., Su, Y., Tripathi, S., Tu, Z.: Text spotting transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9519\u20139528 (2022)": "Text Spotting Transformers", "[32] Ronen, R., Tsiper, S., Anschel, O., Lavi, I., Markovitz, A., Manmatha, R.: Glass: Global to local attention for scene-text spotting. arXiv preprint arXiv:2208.03364 (2022)": "Glass: Global to local attention for scene-text spotting", "[3] Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is wrong with scene text recognition model comparisons? dataset and model analysis. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 4715\u20134723 (2019)": "What is wrong with scene text recognition model comparisons? Dataset and model analysis", "[22] Litman, R., Anschel, O., Tsiper, S., Litman, R., Mazor, S., Manmatha, R.: Scatter: selective context attentional scene text recognizer. In: proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11962\u201311972 (2020)": "Scatter: Selective Context Attentional Scene Text Recognizer", "[8] Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: autonomous, bidirectional and iterative language modeling for scene text recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7098\u20137107 (2021)": "Read like humans: autonomous, bidirectional and iterative language modeling for scene text recognition", "[15] Kittenplon, Y., Lavi, I., Fogel, S., Bar, Y., Manmatha, R., Perona, P.: Towards weakly-supervised text spotting using a multi-task transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4604\u20134613 (2022)": "Towards weakly-supervised text spotting using a multi-task transformer"}, "source_title_to_arxiv_id": {"Glass: Global to local attention for scene-text spotting": "2208.03364"}}