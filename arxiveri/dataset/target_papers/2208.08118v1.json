{"title": "Extreme-scale Talking-Face Video Upsampling with Audio-Visual Priors", "abstract": "In this paper, we explore an interesting question of what can be obtained\nfrom an $8\\times8$ pixel video sequence. Surprisingly, it turns out to be quite\na lot. We show that when we process this $8\\times8$ video with the right set of\naudio and image priors, we can obtain a full-length, $256\\times256$ video. We\nachieve this $32\\times$ scaling of an extremely low-resolution input using our\nnovel audio-visual upsampling network. The audio prior helps to recover the\nelemental facial details and precise lip shapes and a single high-resolution\ntarget identity image prior provides us with rich appearance details. Our\napproach is an end-to-end multi-stage framework. The first stage produces a\ncoarse intermediate output video that can be then used to animate single target\nidentity image and generate realistic, accurate and high-quality outputs. Our\napproach is simple and performs exceedingly well (an $8\\times$ improvement in\nFID score) compared to previous super-resolution methods. We also extend our\nmodel to talking-face video compression, and show that we obtain a $3.5\\times$\nimprovement in terms of bits/pixel over the previous state-of-the-art. The\nresults from our network are thoroughly analyzed through extensive ablation\nexperiments (in the paper and supplementary material). We also provide the demo\nvideo along with code and models on our website:\n\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/talking-face-video-upsampling}.", "authors": ["Sindhu B Hegde", "Rudrabha Mukhopadhyay", "Vinay P Namboodiri", "C. V. Jawahar"], "published_date": "2022_08_17", "pdf_url": "http://arxiv.org/pdf/2208.08118v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th colspan=\"5\">AVSpeech (Ephrat et al., 2018)</th><th colspan=\"5\">VoxCeleb2 (Chung et al., 2018)</th></tr><tr><th>Method</th><th>PSNR\\uparrow</th><th>SSIM\\uparrow</th><th>FID\\downarrow</th><th>LMD\\downarrow</th><th>LSE-D\\downarrow</th><th>PSNR\\uparrow</th><th>SSIM\\uparrow</th><th>FID\\downarrow</th><th>LMD\\downarrow</th><th>LSE-D\\downarrow</th></tr></thead><tbody><tr><th>Bicubic</th><td>22.33</td><td>0.60</td><td>102.41</td><td>0.246</td><td>14.18</td><td>22.16</td><td>0.60</td><td>105.14</td><td>0.255</td><td>17.83</td></tr><tr><th>SPARNet (Chen et al., 2020)</th><td>23.17</td><td>0.68</td><td>92.14</td><td>0.201</td><td>12.87</td><td>22.98</td><td>0.67</td><td>83.01</td><td>0.228</td><td>14.07</td></tr><tr><th>TecoGAN (Chu et al., 2020)</th><td>19.26</td><td>0.62</td><td>84.73</td><td>0.213</td><td>13.01</td><td>16.91</td><td>0.54</td><td>82.19</td><td>0.234</td><td>14.12</td></tr><tr><th>Ours</th><td>25.06</td><td>0.73</td><td>11.54</td><td>0.162</td><td>12.43</td><td>24.95</td><td>0.71</td><td>14.10</td><td>0.196</td><td>13.91</td></tr></tbody></table>", "caption": "Table 2. Quantitative comparison for 32\\times SR on AVSpeech (Ephrat et al., 2018) and VoxCeleb2 (Chung et al., 2018) datasets. Our method outperforms the baselines by a significant margin across all metrics. Note that the baselines have also been trained with a single identity image.", "list_citation_info": ["Ephrat et al. (2018) Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, and Michael Rubinstein. 2018. Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation. ACM Trans. Graph. 37, 4, Article 112 (2018), 11 pages. https://doi.org/10.1145/3197517.3201357", "Chung et al. (2018) J. S. Chung, A. Nagrani, and A. Zisserman. 2018. VoxCeleb2: Deep Speaker Recognition. In INTERSPEECH.", "Chu et al. (2020) Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taix\u00e9, and Nils Thuerey. 2020. Learning Temporal Coherence via Self-Supervision for GAN-Based Video Generation. ACM Trans. Graph. 39 (July 2020), 13 pages. https://doi.org/10.1145/3386569.3392457", "Chen et al. (2020) Chaofeng Chen, Dihong Gong, Hao Wang, Zhifeng Li, and Kwan-Yee K. Wong. 2020. Learning Spatial Attention for Face Super-Resolution. IEEE Transactions on Image Processing (TIP)."]}, {"table": "<table><tbody><tr><th>Method</th><td>BPP\\downarrow</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>FID\\downarrow</td></tr><tr><th>H.264 (CRF=23) (min. compression)</th><td>0.109</td><td>32.96</td><td>0.79</td><td>9.75</td></tr><tr><th>H.264 (CRF=36)</th><td>0.027</td><td>19.24</td><td>0.67</td><td>30.12</td></tr><tr><th>H.266</th><td>0.0076</td><td>23.27</td><td>0.70</td><td>58.32</td></tr><tr><th>fs-vid2vid (Wang et al., 2019)</th><td>n/a</td><td>20.36</td><td>0.71</td><td>85.76</td></tr><tr><th>os-synth (Wang et al., 2021)</th><td>0.016</td><td>24.37</td><td>0.80</td><td>69.13</td></tr><tr><th>Ours</th><td>0.023</td><td>24.95</td><td>0.71</td><td>14.10</td></tr><tr><th>Ours (Frame-Interpolation)</th><td>0.0046</td><td>23.72</td><td>0.68</td><td>14.51</td></tr></tbody></table>", "caption": "Table 3. Quantitative comparison for talking-face video compression on VoxCeleb2 dataset (Chung et al., 2018). We achieve the best trade-off in terms of quality versus compression ratio. Our method achieves the lowest FID (indicates very high perceptual quality) and a very low/comparable BPP.", "list_citation_info": ["Wang et al. (2021) Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. 2021. One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "Chung et al. (2018) J. S. Chung, A. Nagrani, and A. Zisserman. 2018. VoxCeleb2: Deep Speaker Recognition. In INTERSPEECH.", "Wang et al. (2019) Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. 2019. Few-shot Video-to-Video Synthesis. In Advances in Neural Information Processing Systems (NeurIPS)."]}, {"table": "<table><tbody><tr><th>Method</th><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>FID\\downarrow</td><td>LMD\\downarrow</td><td>LSE-D\\downarrow</td><td>Yaw\\downarrow</td><td>Pitch\\downarrow</td><td>Roll\\downarrow</td><td>MAE\\downarrow</td></tr><tr><th>Wav2Lip (Prajwal et al., 2020)</th><td>14.18</td><td>0.32</td><td>8.15</td><td>4.320</td><td>9.19</td><td>24.68</td><td>38.31</td><td>28.94</td><td>30.64</td></tr><tr><th>MakeItTalk (Zhou et al., 2020)</th><td>18.88</td><td>0.49</td><td>31.19</td><td>2.012</td><td>11.91</td><td>26.29</td><td>40.13</td><td>31.42</td><td>32.61</td></tr><tr><th>FOMM (Siarohin et al., 2019)</th><td>20.14</td><td>0.56</td><td>21.18</td><td>0.864</td><td>14.03</td><td>19.14</td><td>30.57</td><td>22.76</td><td>24.35</td></tr><tr><th>PC-AVS (Zhou et al., 2021)</th><td>15.68</td><td>0.37</td><td>33.38</td><td>1.063</td><td>8.42</td><td>22.27</td><td>31.89</td><td>25.80</td><td>26.65</td></tr><tr><th>Ours</th><td>24.95</td><td>0.71</td><td>14.10</td><td>0.196</td><td>13.91</td><td>13.55</td><td>21.01</td><td>15.48</td><td>16.68</td></tr></tbody></table>", "caption": "Table 4. Quantitative comparison with A2TF (Prajwal et al., 2020; Zhou et al., 2020) and FR (Siarohin et al., 2019; Zhou et al., 2021) methods on VoxCeleb2 test set.", "list_citation_info": ["Siarohin et al. (2019) Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. First Order Motion Model for Image Animation. In Conference on Neural Information Processing Systems (NeurIPS).", "Zhou et al. (2021) Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. 2021. Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "Zhou et al. (2020) Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. 2020. MakeItTalk: Speaker-Aware Talking-Head Animation. ACM Transactions on Graphics 39, 6 (2020).", "Prajwal et al. (2020) K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C.V. Jawahar. 2020. A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). 484\u2013492. https://doi.org/10.1145/3394171.3413532"]}], "citation_info_to_title": {"Zhou et al. (2021) Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. 2021. Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).": "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation", "Wang et al. (2021) Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. 2021. One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.": "One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing", "Ephrat et al. (2018) Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, and Michael Rubinstein. 2018. Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation. ACM Trans. Graph. 37, 4, Article 112 (2018), 11 pages. https://doi.org/10.1145/3197517.3201357": "Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation", "Chung et al. (2018) J. S. Chung, A. Nagrani, and A. Zisserman. 2018. VoxCeleb2: Deep Speaker Recognition. In INTERSPEECH.": "VoxCeleb2: Deep Speaker Recognition", "Wang et al. (2019) Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. 2019. Few-shot Video-to-Video Synthesis. In Advances in Neural Information Processing Systems (NeurIPS).": "Few-shot Video-to-Video Synthesis", "Prajwal et al. (2020) K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C.V. Jawahar. 2020. A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). 484\u2013492. https://doi.org/10.1145/3394171.3413532": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild", "Chen et al. (2020) Chaofeng Chen, Dihong Gong, Hao Wang, Zhifeng Li, and Kwan-Yee K. Wong. 2020. Learning Spatial Attention for Face Super-Resolution. IEEE Transactions on Image Processing (TIP).": "Learning Spatial Attention for Face Super-Resolution", "Zhou et al. (2020) Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. 2020. MakeItTalk: Speaker-Aware Talking-Head Animation. ACM Transactions on Graphics 39, 6 (2020).": "MakeItTalk: Speaker-Aware Talking-Head Animation", "Chu et al. (2020) Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taix\u00e9, and Nils Thuerey. 2020. Learning Temporal Coherence via Self-Supervision for GAN-Based Video Generation. ACM Trans. Graph. 39 (July 2020), 13 pages. https://doi.org/10.1145/3386569.3392457": "Learning Temporal Coherence via Self-Supervision for GAN-Based Video Generation", "Siarohin et al. (2019) Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. First Order Motion Model for Image Animation. In Conference on Neural Information Processing Systems (NeurIPS).": "First Order Motion Model for Image Animation"}, "source_title_to_arxiv_id": {"First Order Motion Model for Image Animation": "2003.00196"}}