{"title": "CODA: A Real-World Road Corner Case Dataset for Object Detection in Autonomous Driving", "abstract": "Contemporary deep-learning object detection methods for autonomous driving\nusually assume prefixed categories of common traffic participants, such as\npedestrians and cars. Most existing detectors are unable to detect uncommon\nobjects and corner cases (e.g., a dog crossing a street), which may lead to\nsevere accidents in some situations, making the timeline for the real-world\napplication of reliable autonomous driving uncertain. One main reason that\nimpedes the development of truly reliably self-driving systems is the lack of\npublic datasets for evaluating the performance of object detectors on corner\ncases. Hence, we introduce a challenging dataset named CODA that exposes this\ncritical problem of vision-based detectors. The dataset consists of 1500\ncarefully selected real-world driving scenes, each containing four object-level\ncorner cases (on average), spanning more than 30 object categories. On CODA,\nthe performance of standard object detectors trained on large-scale autonomous\ndriving datasets significantly drops to no more than 12.8% in mAR. Moreover, we\nexperiment with the state-of-the-art open-world object detector and find that\nit also fails to reliably identify the novel objects in CODA, suggesting that a\nrobust perception system for autonomous driving is probably still far from\nreach. We expect our CODA dataset to facilitate further research in reliable\ndetection for real-world autonomous driving. Our dataset will be released at\nhttps://coda-dataset.github.io.", "authors": ["Kaican Li", "Kai Chen", "Haoyu Wang", "Lanqing Hong", "Chaoqiang Ye", "Jianhua Han", "Yukuai Chen", "Wei Zhang", "Chunjing Xu", "Dit-Yan Yeung", "Xiaodan Liang", "Zhenguo Li", "Hang Xu"], "published_date": "2022_03_15", "pdf_url": "http://arxiv.org/pdf/2203.07724v3", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Dataset</th><td>#Scenes</td><td>Real</td><td>Weather</td><td>Period</td><td>#Classes</td><td>#Instances</td></tr><tr><th>Lis \\etal [24]</th><td>60</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>2</td><td>{}^{\\dagger}300{}^{\\dagger}</td></tr><tr><th>Fishyscapes L&amp;F [1]</th><td>375</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>3</td><td>{}^{\\dagger}500{}^{\\dagger}</td></tr><tr><th>Fishyscapes Static [1]</th><td>1030</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>3</td><td>{}^{\\dagger}1200{}^{\\dagger}</td></tr><tr><th>StreetHazards [17]</th><td>1500</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>1</td><td>{}^{\\dagger}1500{}^{\\dagger}</td></tr><tr><th>BDD-Anomaly (v1) [17]</th><td>361</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>2</td><td>4476</td></tr><tr><th>CODA-KITTI (Ours)</th><td>309</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>6</td><td>399</td></tr><tr><th>CODA-nuScenes (Ours)</th><td>134</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>17</td><td>1125</td></tr><tr><th>CODA-ONCE (Ours)</th><td>1057</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>32</td><td>4413</td></tr><tr><th>CODA (Ours)</th><td>1500</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>34</td><td>5937</td></tr></tbody></table>", "caption": "Table 1: Comparison with other datasets. CODA is the largest dataset of its kind in multiple aspects.Here we do not compare with the Fishyscapes Web dataset [1], which is neither publicly available nor with detailed statistics. \u201c{}^{\\dagger}\u201d means rough estimates.", "list_citation_info": ["[24] Lis, K., Nakka, K., Fua, P., Salzmann, M.: Detecting the unexpected via image resynthesis. In: ICCV (2019)", "[1] Blum, H., Sarlin, P.E., Nieto, J., Siegwart, R., Cadena, C.: The fishyscapes benchmark: Measuring blind spots in semantic segmentation. arXiv preprint arXiv:1904.03215 (2019)", "[17] Hendrycks, D., Basart, S., Mazeika, M., Mostajabi, M., Steinhardt, J., Song, D.: A benchmark for anomaly segmentation. arXiv preprint arXiv:1911.11132 (2019)"]}, {"table": "<table><tbody><tr><th colspan=\"2\">CODA</th><td colspan=\"2\">ORIGIN</td><td colspan=\"4\">CORNER</td><td colspan=\"4\">COMMON</td><td colspan=\"4\">NOVEL</td></tr><tr><th>Method</th><th>Dataset</th><td>AP</td><td>AR</td><td>AR{}^{*}</td><td>AR{}_{50}</td><td>AR{}_{75}</td><td>AR{}^{10}</td><td>AR{}^{*}</td><td>AR{}_{50}</td><td>AR{}_{75}</td><td>AR{}^{10}</td><td>AR{}^{*}</td><td>AR{}_{50}</td><td>AR{}_{75}</td><td>AR{}^{10}</td></tr><tr><th>RetinaNet{}^{{\\dagger}} [22]</th><th></th><td>34.0</td><td>50.7</td><td>11.9</td><td>25.2</td><td>9.5</td><td>5.4</td><td>28.7</td><td>58.9</td><td>23.5</td><td>23.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Faster R-CNN{}^{{\\dagger}} [34]</th><th></th><td>36.7</td><td>46.9</td><td>6.8</td><td>13.0</td><td>6.4</td><td>4.9</td><td>23.9</td><td>46.8</td><td>20.1</td><td>23.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade R-CNN{}^{{\\dagger}} [5]</th><th></th><td>39.4</td><td>51.6</td><td>8.3</td><td>15.5</td><td>7.6</td><td>5.5</td><td>27.2</td><td>47.0</td><td>29.4</td><td>25.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>D-DETR [49]</th><th></th><td>31.8</td><td>49.4</td><td>7.2</td><td>16.7</td><td>4.9</td><td>3.6</td><td>34.6</td><td>60.2</td><td>36.5</td><td>29.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Sparse R-CNN [39]</th><th>SODA10M</th><td>31.2</td><td>51.0</td><td>6.4</td><td>13.2</td><td>5.4</td><td>3.9</td><td>26.4</td><td>47.1</td><td>25.6</td><td>23.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade Swin [27]</th><th>[13]</th><td>41.1</td><td>52.9</td><td>8.2</td><td>15.5</td><td>7.6</td><td>5.7</td><td>30.4</td><td>51.3</td><td>32.2</td><td>29.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Faster){}^{{\\dagger}} [34]</th><th></th><td>-</td><td>59.7</td><td>8.1</td><td>16.2</td><td>7.4</td><td>3.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade){}^{{\\dagger}} [5]</th><th></th><td>-</td><td>57.1</td><td>7.7</td><td>16.0</td><td>6.8</td><td>2.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ORE [19]</th><th></th><td>49.2</td><td>59.7</td><td>8.3</td><td>16.4</td><td>7.4</td><td>5.6</td><td>18.5</td><td>35.5</td><td>18.2</td><td>18.1</td><td>3.4</td><td>7.6</td><td>2.8</td><td>2.9</td></tr><tr><th>RetinaNet{}^{{\\dagger}} [22]</th><th></th><td>28.6</td><td>40.4</td><td>12.8</td><td>23.2</td><td>11.9</td><td>4.8</td><td>27.5</td><td>58.1</td><td>21.5</td><td>23.6</td><td>9.7</td><td>17.7</td><td>9.1</td><td>5.9</td></tr><tr><th>Faster R-CNN{}^{{\\dagger}} [34]</th><th></th><td>31.0</td><td>40.7</td><td>10.7</td><td>19.2</td><td>10.2</td><td>4.3</td><td>24.4</td><td>48.1</td><td>20.9</td><td>22.0</td><td>7.2</td><td>13.3</td><td>6.8</td><td>5.9</td></tr><tr><th>Cascade R-CNN{}^{{\\dagger}} [5]</th><th></th><td>32.4</td><td>41.4</td><td>10.4</td><td>18.5</td><td>9.7</td><td>4.5</td><td>25.7</td><td>48.4</td><td>23.3</td><td>23.6</td><td>6.9</td><td>12.5</td><td>6.5</td><td>5.7</td></tr><tr><th>D-DETR [49]</th><th>BDD100K</th><td>28.5</td><td>42.3</td><td>9.0</td><td>22.2</td><td>5.6</td><td>2.8</td><td>28.5</td><td>63.0</td><td>22.3</td><td>26.2</td><td>7.0</td><td>17.3</td><td>4.3</td><td>3.9</td></tr><tr><th>Sparse R-CNN{}^{{\\dagger}} [39]</th><th>[45]</th><td>26.7</td><td>40.2</td><td>9.8</td><td>19.0</td><td>8.9</td><td>4.5</td><td>27.4</td><td>51.7</td><td>25.8</td><td>24.3</td><td>8.0</td><td>15.4</td><td>7.4</td><td>5.1</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>34.5</td><td>43.5</td><td>9.9</td><td>17.2</td><td>9.7</td><td>4.9</td><td>31.0</td><td>55.0</td><td>29.9</td><td>29.4</td><td>6.5</td><td>11.4</td><td>6.4</td><td>5.9</td></tr><tr><th>RPN (Faster){}^{{\\dagger}} [34]</th><th></th><td>-</td><td>50.2</td><td>10.6</td><td>20.0</td><td>10.2</td><td>3.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade){}^{{\\dagger}} [5]</th><th></th><td>-</td><td>51.0</td><td>10.6</td><td>20.0</td><td>10.2</td><td>3.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RetinaNet [22]</th><th></th><td>39.7</td><td>47.7</td><td>8.4</td><td>15.6</td><td>7.7</td><td>5.1</td><td>24.5</td><td>43.2</td><td>24.4</td><td>22.2</td><td>6.7</td><td>11.9</td><td>6.4</td><td>4.6</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>40.9</td><td>47.0</td><td>6.8</td><td>12.4</td><td>6.4</td><td>4.8</td><td>20.9</td><td>36.0</td><td>19.6</td><td>19.1</td><td>5.5</td><td>9.6</td><td>5.2</td><td>4.3</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>42.6</td><td>48.1</td><td>6.6</td><td>11.4</td><td>6.6</td><td>5.0</td><td>18.9</td><td>32.6</td><td>20.1</td><td>17.6</td><td>5.3</td><td>8.7</td><td>5.5</td><td>4.4</td></tr><tr><th>D-DETR [49]</th><th>Waymo</th><td>40.4</td><td>49.8</td><td>7.3</td><td>15.8</td><td>5.4</td><td>3.6</td><td>28.5</td><td>49.4</td><td>24.6</td><td>22.5</td><td>5.2</td><td>11.5</td><td>4.0</td><td>3.0</td></tr><tr><th>Sparse R-CNN [39]</th><th>[38]</th><td>38.8</td><td>49.8</td><td>10.1</td><td>19.6</td><td>9.0</td><td>4.7</td><td>29.5</td><td>51.8</td><td>27.0</td><td>22.1</td><td>7.6</td><td>14.3</td><td>7.1</td><td>4.2</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>44.2</td><td>49.0</td><td>5.4</td><td>8.7</td><td>5.5</td><td>4.4</td><td>21.8</td><td>38.1</td><td>18.8</td><td>21.3</td><td>4.3</td><td>6.7</td><td>4.6</td><td>3.7</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>-</td><td>53.9</td><td>7.5</td><td>13.7</td><td>7.5</td><td>3.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>-</td><td>52.8</td><td>7.4</td><td>13.8</td><td>7.3</td><td>3.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Detection results (%) on CODA.The best performance is achieved at 12.8% AR, suggesting that truly reliable object detection is probably still far from reach.Definitions of ORIGIN, CORNER, COMMON, and NOVEL are provided in \u201cclass separation\u201d of Sec. 5.1.\u201cD-DETR\u201d is short for Deformable DETR and \u201cCascade Swin\u201d stands for Swin-Tiny-based Cascade R-CNN.Bold values highlight the best performance among detectors pre-trained on the same dataset,and \u201c{}^{{\\dagger}}\u201d means official checkpoints are adopted.\u201c*\u201d indicates that AR is the primary evaluation metric on CODA, while\u201c-\u201d suggests that the detector cannot report the corresponding values, with reasons explained in \u201cevaluation\u201d of Sec. 5.1.See more results in Appendix D.", "list_citation_info": ["[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: CVPR (2020)", "[19] Joseph, K., Khan, S., Khan, F.S., Balasubramanian, V.N.: Towards open world object detection. In: CVPR (2021)", "[49] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)", "[22] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV (2017)", "[13] Han, J., Liang, X., Xu, H., Chen, K., Hong, L., Mao, J., Ye, C., Zhang, W., Li, Z., Liang, X., Xu, C.: SODA10M: A large-scale 2d self/semi-supervised object detection dataset for autonomous driving. arXiv preprint arXiv:2106.11118 (2021)", "[45] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020)", "[27] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)", "[39] Sun, P., Zhang, R., Jiang, Y., Kong, T., Xu, C., Zhan, W., Tomizuka, M., Li, L., Yuan, Z., Wang, C., et al.: Sparse r-cnn: End-to-end object detection with learnable proposals. In: CVPR (2021)", "[34] Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)", "[5] Cai, Z., Vasconcelos, N.: Cascade R-CNN: delving into high quality object detection. In: CVPR (2018)"]}, {"table": "<table><thead><tr><th>Method</th><th>AR{}^{m}_{50}</th><th>AR{}^{l}_{50}</th><th>AR{}^{m}_{30}</th><th>AR{}^{l}_{30}</th></tr></thead><tbody><tr><th>Faster R-CNN [34]</th><td>6.7</td><td>8.3</td><td>26.4</td><td>28.8</td></tr><tr><th>Memory-based OOD [12]</th><td>2.2</td><td>21.8</td><td>6.6</td><td>39.5</td></tr><tr><th>Synthesize then Compare [43]</th><td>9.0</td><td>17.7</td><td>12.3</td><td>33.3</td></tr><tr><th>COPG (Ours)</th><td>23.8</td><td>44.9</td><td>39.6</td><td>63.9</td></tr></tbody></table>", "caption": "Table 3: Evaluation of COPG and other object/anomaly detectors on detecting corner cases.The experiments are conducted on CODA-KITTI whose construction does not involve COPG (whereas the construction of CODA-ONCE does).Here AR{}^{m}_{50} and AR{}^{l}_{50} represent AR{}_{50} for medium and large objects, since no small corner cases are included in CODA-KITTI, withthe same definition for AR{}^{m}_{30} and AR{}^{l}_{30} under 0.3 IoU threshold.", "list_citation_info": ["[34] Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)", "[43] Xia, Y., Zhang, Y., Liu, F., Shen, W., Yuille, A.L.: Synthesize then compare: Detecting failures and anomalies for semantic segmentation. In: ECCV (2020)", "[12] Gong, D., Liu, L., Le, V., Saha, B., Mansour, M.R., Venkatesh, S., van den Hengel, A.: Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In: ICCV (2019)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">34-way (class-wise)</th><th colspan=\"3\">1-way{}^{{\\dagger}{\\dagger}} (class-agnostic)</th></tr><tr><th>AR</th><th>AR{}_{50}</th><th>AR{}_{75}</th><th>AR</th><th>AR{}_{50}</th><th>AR{}_{75}</th></tr></thead><tbody><tr><th>FsDet [41]</th><td>4.9_{\\pm 0.8}</td><td>9.4_{\\pm 1.9}</td><td>4.4_{\\pm 0.8}</td><td>4.2_{\\pm 0.4}</td><td>7.7_{\\pm 0.7}</td><td>4.0_{\\pm 0.3}</td></tr><tr><th>DeFRCN [31]</th><td>6.7_{\\pm 1.2}</td><td>12.1_{\\pm 1.6}</td><td>6.6_{\\pm 1.6}</td><td>4.5_{\\pm 0.5}</td><td>8.9_{\\pm 0.9}</td><td>4.2_{\\pm 0.5}</td></tr></tbody></table>", "caption": "Table 4: Evaluation of FSOD on CODA.\u201c{}^{{\\dagger}{\\dagger}}\u201d suggests that the reported values are evaluated in a class-agnostic manner, same as the CORNER setting adopted in Tab. 2.", "list_citation_info": ["[41] Wang, X., Huang, T., Gonzalez, J., Darrell, T., Yu, F.: Frustratingly simple few-shot object detection. In: ICML (2020)", "[31] Qiao, L., Zhao, Y., Li, Z., Qiu, X., Wu, J., Zhang, C.: DeFRCN: Decoupled faster R-CNN for few-shot object detection. In: ICCV (2021)"]}, {"table": "<table><tbody><tr><th colspan=\"2\">CODA-ONCE</th><td colspan=\"4\">CORNER</td><td colspan=\"4\">COMMON</td><td colspan=\"4\">NOVEL</td></tr><tr><th>Method</th><th>Dataset</th><td>AR{}^{*}</td><td>AR{}_{50}</td><td>AR{}_{75}</td><td>AR{}^{10}</td><td>AR{}^{*}</td><td>AR{}_{50}</td><td>AR{}_{75}</td><td>AR{}^{10}</td><td>AR{}^{*}</td><td>AR{}_{50}</td><td>AR{}_{75}</td><td>AR{}^{10}</td></tr><tr><th>RetinaNet [22]</th><th></th><td>9.6</td><td>21.8</td><td>7.0</td><td>3.0</td><td>29.8</td><td>59.8</td><td>24.5</td><td>24.8</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>4.0</td><td>8.5</td><td>3.4</td><td>2.3</td><td>23.3</td><td>44.3</td><td>19.6</td><td>22.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>5.6</td><td>11.4</td><td>4.9</td><td>2.9</td><td>27.2</td><td>46.0</td><td>30.7</td><td>25.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Deformable DETR [49]</th><th></th><td>6.1</td><td>13.0</td><td>5.3</td><td>2.7</td><td>36.8</td><td>63.9</td><td>40.0</td><td>31.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Sparse R-CNN [39]</th><th>SODA10M</th><td>4.4</td><td>9.2</td><td>3.7</td><td>2.1</td><td>26.3</td><td>44.5</td><td>25.5</td><td>22.7</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade Swin [27]</th><th>[13]</th><td>5.5</td><td>11.3</td><td>4.8</td><td>2.9</td><td>31.2</td><td>49.1</td><td>35.5</td><td>30.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>5.1</td><td>11.3</td><td>4.0</td><td>1.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>4.8</td><td>11.4</td><td>3.5</td><td>1.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ORE [19]</th><th></th><td>5.3</td><td>11.5</td><td>4.4</td><td>2.8</td><td>19.2</td><td>36.8</td><td>17.1</td><td>18.6</td><td>2.0</td><td>4.8</td><td>1.6</td><td>1.7</td></tr><tr><th>RetinaNet [22]</th><th></th><td>10.2</td><td>19.4</td><td>9.1</td><td>2.6</td><td>30.1</td><td>63.7</td><td>23.3</td><td>25.8</td><td>7.3</td><td>14.1</td><td>6.6</td><td>3.8</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>7.8</td><td>15.1</td><td>7.4</td><td>2.3</td><td>26.8</td><td>51.8</td><td>23.2</td><td>24.3</td><td>5.5</td><td>10.7</td><td>5.2</td><td>4.1</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>7.7</td><td>14.6</td><td>6.9</td><td>2.4</td><td>26.7</td><td>48.8</td><td>24.0</td><td>24.6</td><td>5.7</td><td>10.5</td><td>5.1</td><td>4.3</td></tr><tr><th>Deformable DETR [49]</th><th>BDD100K</th><td>7.8</td><td>18.4</td><td>5.6</td><td>1.9</td><td>30.9</td><td>67.8</td><td>24.4</td><td>28.5</td><td>5.8</td><td>13.4</td><td>4.2</td><td>3.0</td></tr><tr><th>Sparse R-CNN [39]</th><th>[45]</th><td>7.6</td><td>15.4</td><td>6.7</td><td>2.6</td><td>30.3</td><td>56.0</td><td>28.8</td><td>26.9</td><td>5.9</td><td>12.0</td><td>5.3</td><td>3.4</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>7.2</td><td>13.3</td><td>6.8</td><td>2.9</td><td>32.9</td><td>57.0</td><td>33.0</td><td>31.3</td><td>5.3</td><td>9.9</td><td>5.0</td><td>4.5</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>7.9</td><td>16.1</td><td>7.0</td><td>1.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>7.7</td><td>15.8</td><td>6.9</td><td>1.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RetinaNet [22]</th><th></th><td>5.4</td><td>11.1</td><td>4.5</td><td>2.2</td><td>26.1</td><td>45.0</td><td>27.0</td><td>23.9</td><td>3.0</td><td>6.1</td><td>2.7</td><td>1.4</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>4.1</td><td>8.3</td><td>3.5</td><td>2.1</td><td>22.9</td><td>38.9</td><td>21.9</td><td>20.8</td><td>2.3</td><td>4.9</td><td>2.0</td><td>1.4</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>3.8</td><td>7.1</td><td>3.6</td><td>2.2</td><td>20.0</td><td>34.1</td><td>21.8</td><td>18.7</td><td>2.2</td><td>3.8</td><td>2.2</td><td>1.5</td></tr><tr><th>Deformable DETR [49]</th><th>Waymo</th><td>5.5</td><td>11.4</td><td>4.6</td><td>1.9</td><td>30.8</td><td>52.9</td><td>26.8</td><td>24.0</td><td>3.1</td><td>6.5</td><td>2.8</td><td>1.1</td></tr><tr><th>Sparse R-CNN [39]</th><th>[38]</th><td>7.3</td><td>14.6</td><td>6.4</td><td>2.1</td><td>32.4</td><td>55.4</td><td>30.0</td><td>23.9</td><td>4.2</td><td>8.6</td><td>3.8</td><td>1.4</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>2.5</td><td>4.4</td><td>2.4</td><td>1.8</td><td>23.5</td><td>41.1</td><td>20.7</td><td>23.0</td><td>1.2</td><td>2.0</td><td>1.2</td><td>0.8</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>4.6</td><td>9.4</td><td>4.3</td><td>1.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>4.3</td><td>9.2</td><td>3.6</td><td>1.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 8: Detection results (%) on our CODA-ONCE dataset.The dramatic performance decrease still maintains, but compared with Tab. 2, we observe a decrease for the reported AR values of all detectors, suggesting that CODA-ONCE constructed with our automatic corner case proposal generation COPG in Sec. 4.2 is the most challenging subset of CODA.", "list_citation_info": ["[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: CVPR (2020)", "[19] Joseph, K., Khan, S., Khan, F.S., Balasubramanian, V.N.: Towards open world object detection. In: CVPR (2021)", "[49] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)", "[22] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV (2017)", "[13] Han, J., Liang, X., Xu, H., Chen, K., Hong, L., Mao, J., Ye, C., Zhang, W., Li, Z., Liang, X., Xu, C.: SODA10M: A large-scale 2d self/semi-supervised object detection dataset for autonomous driving. arXiv preprint arXiv:2106.11118 (2021)", "[45] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020)", "[27] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)", "[39] Sun, P., Zhang, R., Jiang, Y., Kong, T., Xu, C., Zhan, W., Tomizuka, M., Li, L., Yuan, Z., Wang, C., et al.: Sparse r-cnn: End-to-end object detection with learnable proposals. In: CVPR (2021)", "[34] Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)", "[5] Cai, Z., Vasconcelos, N.: Cascade R-CNN: delving into high quality object detection. In: CVPR (2018)"]}, {"table": "<table><tbody><tr><th colspan=\"2\">CODA-ONCE</th><td colspan=\"4\">CORNER</td><td colspan=\"4\">COMMON</td><td colspan=\"4\">NOVEL</td></tr><tr><th>Method</th><th>Dataset</th><td>AR{}^{s}</td><td>AR{}^{m}</td><td>AR{}^{l}</td><td>AR{}^{1}</td><td>AR{}^{s}</td><td>AR{}^{m}</td><td>AR{}^{l}</td><td>AR{}^{1}</td><td>AR{}^{s}</td><td>AR{}^{m}</td><td>AR{}^{l}</td><td>AR{}^{1}</td></tr><tr><th>RetinaNet [22]</th><th></th><td>3.8</td><td>2.9</td><td>19.4</td><td>0.2</td><td>-</td><td>0.0</td><td>33.4</td><td>4.7</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>0.6</td><td>1.0</td><td>8.5</td><td>0.2</td><td>-</td><td>0.0</td><td>26.0</td><td>3.7</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>1.0</td><td>1.5</td><td>11.9</td><td>0.2</td><td>-</td><td>0.0</td><td>30.8</td><td>4.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Deformable DETR [49]</th><th></th><td>0.7</td><td>1.9</td><td>12.6</td><td>0.2</td><td>-</td><td>1.1</td><td>42.3</td><td>6.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Sparse R-CNN [39]</th><th>SODA10M</th><td>1.3</td><td>1.6</td><td>8.6</td><td>0.3</td><td>-</td><td>3.3</td><td>28.6</td><td>7.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade Swin [27]</th><th>[14]</th><td>1.1</td><td>1.5</td><td>11.5</td><td>0.2</td><td>-</td><td>0.0</td><td>35.8</td><td>3.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>2.2</td><td>1.5</td><td>10.3</td><td>0.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>3.6</td><td>1.5</td><td>9.3</td><td>0.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ORE [19]</th><th></th><td>1.6</td><td>1.9</td><td>10.4</td><td>0.3</td><td>-</td><td>1.7</td><td>21.1</td><td>5.3</td><td>0.5</td><td>0.6</td><td>4.4</td><td>0.4</td></tr><tr><th>RetinaNet [22]</th><th></th><td>2.2</td><td>4.4</td><td>19.2</td><td>0.2</td><td>-</td><td>9.2</td><td>33.1</td><td>5.9</td><td>1.1</td><td>3.4</td><td>13.9</td><td>0.6</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>1.3</td><td>2.6</td><td>15.9</td><td>0.1</td><td>-</td><td>17.2</td><td>29.3</td><td>4.7</td><td>0.4</td><td>2.2</td><td>11.2</td><td>0.5</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>1.7</td><td>2.6</td><td>15.5</td><td>0.1</td><td>-</td><td>12.5</td><td>30.4</td><td>5.4</td><td>0.8</td><td>2.2</td><td>11.5</td><td>0.6</td></tr><tr><th>Deformable DETR [49]</th><th>BDD100K</th><td>1.3</td><td>3.6</td><td>14.5</td><td>0.1</td><td>-</td><td>18.9</td><td>33.1</td><td>7.2</td><td>1.0</td><td>3.0</td><td>10.8</td><td>0.4</td></tr><tr><th>Sparse R-CNN [39]</th><th>[45]</th><td>1.7</td><td>3.3</td><td>14.2</td><td>0.2</td><td>-</td><td>4.4</td><td>34.3</td><td>5.4</td><td>1.3</td><td>2.9</td><td>11.0</td><td>0.5</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>1.3</td><td>2.4</td><td>14.5</td><td>0.3</td><td>-</td><td>27.5</td><td>37.3</td><td>9.6</td><td>0.7</td><td>2.0</td><td>10.8</td><td>0.8</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>2.2</td><td>3.0</td><td>15.3</td><td>0.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>1.9</td><td>2.9</td><td>14.9</td><td>0.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RetinaNet [22]</th><th></th><td>5.4</td><td>11.1</td><td>4.5</td><td>2.2</td><td>26.1</td><td>45.0</td><td>27.0</td><td>23.9</td><td>3.0</td><td>6.1</td><td>2.7</td><td>1.4</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>4.1</td><td>8.3</td><td>3.5</td><td>2.1</td><td>22.9</td><td>38.9</td><td>21.9</td><td>20.8</td><td>2.3</td><td>4.9</td><td>2.0</td><td>1.4</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>3.8</td><td>7.1</td><td>3.6</td><td>2.2</td><td>20.0</td><td>34.1</td><td>21.8</td><td>18.7</td><td>2.2</td><td>3.8</td><td>2.2</td><td>1.5</td></tr><tr><th>Deformable DETR [49]</th><th>Waymo</th><td>5.5</td><td>11.4</td><td>4.6</td><td>1.9</td><td>30.8</td><td>52.9</td><td>26.8</td><td>24.0</td><td>3.1</td><td>6.5</td><td>2.8</td><td>1.1</td></tr><tr><th>Sparse R-CNN [39]</th><th>[38]</th><td>7.3</td><td>14.6</td><td>6.4</td><td>2.1</td><td>32.4</td><td>55.4</td><td>30.0</td><td>23.9</td><td>4.2</td><td>8.6</td><td>3.8</td><td>1.4</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>2.5</td><td>4.4</td><td>2.4</td><td>1.8</td><td>23.5</td><td>41.1</td><td>20.7</td><td>23.0</td><td>1.2</td><td>2.0</td><td>1.2</td><td>0.8</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>4.6</td><td>9.4</td><td>4.3</td><td>1.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>4.3</td><td>9.2</td><td>3.6</td><td>1.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 9: More detection results(%) on our CODA-ONCE dataset.AR{}^{s}, AR{}^{m} and AR{}^{l} are the average recall for small, medium and large objects respectively, following COCO definition [23], while AR{}^{1} represents the average recall when only 1 object prediction is allowed for each image.Here AR^{s} is also marked as \u201c-\u201d, since no small corner cases of common classes are collected in CODA-ONCE subset.", "list_citation_info": ["[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: CVPR (2020)", "[19] Joseph, K., Khan, S., Khan, F.S., Balasubramanian, V.N.: Towards open world object detection. In: CVPR (2021)", "[49] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)", "[14] Han, J., Liang, X., Xu, H., Chen, K., Hong, L., Ye, C., Zhang, W., Li, Z., Liang, X., Xu, C.: Soda10m: Towards large-scale object detection benchmark for autonomous driving. arXiv:2106.11118 (2021)", "[22] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV (2017)", "[45] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020)", "[27] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)", "[39] Sun, P., Zhang, R., Jiang, Y., Kong, T., Xu, C., Zhan, W., Tomizuka, M., Li, L., Yuan, Z., Wang, C., et al.: Sparse r-cnn: End-to-end object detection with learnable proposals. In: CVPR (2021)", "[23] Lin, T., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV (2014)", "[34] Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)", "[5] Cai, Z., Vasconcelos, N.: Cascade R-CNN: delving into high quality object detection. In: CVPR (2018)"]}, {"table": "<table><tbody><tr><th colspan=\"2\">CODA</th><td colspan=\"4\">CORNER</td><td colspan=\"4\">COMMON</td><td colspan=\"4\">NOVEL</td></tr><tr><th>Method</th><th>Dataset</th><td>AR{}^{s}</td><td>AR{}^{m}</td><td>AR{}^{l}</td><td>AR{}^{1}</td><td>AR{}^{s}</td><td>AR{}^{m}</td><td>AR{}^{l}</td><td>AR{}^{1}</td><td>AR{}^{s}</td><td>AR{}^{m}</td><td>AR{}^{l}</td><td>AR{}^{1}</td></tr><tr><th>RetinaNet [22]</th><th></th><td>5.6</td><td>6.5</td><td>20.6</td><td>0.2</td><td>40.0</td><td>1.3</td><td>31.8</td><td>5.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>2.7</td><td>4.6</td><td>10.8</td><td>0.2</td><td>60.0</td><td>4.7</td><td>24.8</td><td>5.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>3.3</td><td>5.1</td><td>13.7</td><td>0.2</td><td>60.0</td><td>4.7</td><td>28.6</td><td>6.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Deformable DETR [49]</th><th></th><td>1.9</td><td>3.5</td><td>13.6</td><td>0.2</td><td>0.0</td><td>6.7</td><td>38.8</td><td>6.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Sparse R-CNN [39]</th><th>SODA10M</th><td>1.7</td><td>4.4</td><td>10.2</td><td>0.5</td><td>70.0</td><td>5.3</td><td>27.2</td><td>8.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Cascade Swin [27]</th><th>[13]</th><td>3.3</td><td>5.3</td><td>13.2</td><td>0.3</td><td>50.0</td><td>4.0</td><td>33.7</td><td>4.7</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>5.1</td><td>5.7</td><td>11.9</td><td>0.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>4.8</td><td>5.3</td><td>11.6</td><td>0.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ORE [19]</th><th></th><td>4.0</td><td>5.5</td><td>13.0</td><td>0.3</td><td>0.0</td><td>1.3</td><td>20.2</td><td>5.5</td><td>1.4</td><td>1.9</td><td>5.9</td><td>2.9</td></tr><tr><th>RetinaNet [22]</th><th></th><td>4.1</td><td>8.3</td><td>21.0</td><td>0.1</td><td>0.0</td><td>7.7</td><td>30.1</td><td>5.4</td><td>3.1</td><td>6.5</td><td>15.9</td><td>1.2</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>3.4</td><td>6.6</td><td>17.8</td><td>0.1</td><td>10.0</td><td>15.7</td><td>26.3</td><td>4.3</td><td>2.2</td><td>4.4</td><td>12.3</td><td>0.9</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>3.3</td><td>6.6</td><td>17.2</td><td>0.1</td><td>60.0</td><td>15.0</td><td>27.5</td><td>5.8</td><td>1.7</td><td>4.1</td><td>12.3</td><td>1.1</td></tr><tr><th>Deformable DETR [49]</th><th>BDD100K</th><td>2.1</td><td>5.4</td><td>15.5</td><td>0.1</td><td>0.0</td><td>17.7</td><td>29.9</td><td>7.1</td><td>1.7</td><td>4.4</td><td>12.0</td><td>0.5</td></tr><tr><th>Sparse R-CNN [39]</th><th>[45]</th><td>3.2</td><td>6.6</td><td>15.6</td><td>0.2</td><td>0.0</td><td>4.0</td><td>30.7</td><td>4.9</td><td>2.8</td><td>5.7</td><td>12.7</td><td>1.1</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>3.4</td><td>6.3</td><td>16.4</td><td>0.2</td><td>0.0</td><td>27.0</td><td>35.5</td><td>9.8</td><td>2.0</td><td>3.9</td><td>11.4</td><td>1.6</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>4.3</td><td>6.9</td><td>17.2</td><td>0.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>3.8</td><td>6.9</td><td>17.1</td><td>0.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RetinaNet [22]</th><th></th><td>1.2</td><td>5.6</td><td>14.0</td><td>0.2</td><td>30.0</td><td>4.0</td><td>27.1</td><td>4.5</td><td>0.6</td><td>4.8</td><td>10.9</td><td>0.0</td></tr><tr><th>Faster R-CNN [34]</th><th></th><td>1.1</td><td>4.5</td><td>11.3</td><td>0.2</td><td>0.0</td><td>50.0</td><td>23.5</td><td>4.2</td><td>0.7</td><td>4.1</td><td>8.6</td><td>0.1</td></tr><tr><th>Cascade R-CNN [5]</th><th></th><td>1.2</td><td>4.5</td><td>10.8</td><td>0.1</td><td>0.0</td><td>10.0</td><td>20.5</td><td>3.7</td><td>0.6</td><td>4.2</td><td>8.2</td><td>0.1</td></tr><tr><th>Deformable DETR [49]</th><th>Waymo</th><td>2.1</td><td>4.9</td><td>11.7</td><td>0.2</td><td>0.0</td><td>8.0</td><td>30.8</td><td>2.6</td><td>1.2</td><td>3.8</td><td>8.3</td><td>0.1</td></tr><tr><th>Sparse R-CNN [39]</th><th>[38]</th><td>3.3</td><td>7.5</td><td>15.2</td><td>0.4</td><td>0.0</td><td>4.0</td><td>33.2</td><td>4.7</td><td>2.4</td><td>5.8</td><td>11.6</td><td>0.3</td></tr><tr><th>Cascade Swin [27]</th><th></th><td>0.3</td><td>3.7</td><td>8.8</td><td>0.3</td><td>0.0</td><td>8.3</td><td>24.5</td><td>2.8</td><td>0.2</td><td>3.5</td><td>6.4</td><td>0.2</td></tr><tr><th>RPN (Faster) [34]</th><th></th><td>1.3</td><td>5.1</td><td>12.3</td><td>0.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>RPN (Cascade) [5]</th><th></th><td>1.0</td><td>5.0</td><td>12.3</td><td>0.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 10: More detection results(%) on our CODA dataset.AR{}^{s}, AR{}^{m} and AR{}^{l} are the average recall for small, medium and large objects respectively, following COCO definition [23], while AR{}^{1} represents the average recall when only 1 object prediction is allowed for each image.", "list_citation_info": ["[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: CVPR (2020)", "[19] Joseph, K., Khan, S., Khan, F.S., Balasubramanian, V.N.: Towards open world object detection. In: CVPR (2021)", "[49] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)", "[22] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV (2017)", "[13] Han, J., Liang, X., Xu, H., Chen, K., Hong, L., Mao, J., Ye, C., Zhang, W., Li, Z., Liang, X., Xu, C.: SODA10M: A large-scale 2d self/semi-supervised object detection dataset for autonomous driving. arXiv preprint arXiv:2106.11118 (2021)", "[45] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020)", "[27] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)", "[39] Sun, P., Zhang, R., Jiang, Y., Kong, T., Xu, C., Zhan, W., Tomizuka, M., Li, L., Yuan, Z., Wang, C., et al.: Sparse r-cnn: End-to-end object detection with learnable proposals. In: CVPR (2021)", "[23] Lin, T., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV (2014)", "[34] Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)", "[5] Cai, Z., Vasconcelos, N.: Cascade R-CNN: delving into high quality object detection. In: CVPR (2018)"]}], "citation_info_to_title": {"[1] Blum, H., Sarlin, P.E., Nieto, J., Siegwart, R., Cadena, C.: The fishyscapes benchmark: Measuring blind spots in semantic segmentation. arXiv preprint arXiv:1904.03215 (2019)": "The fishyscapes benchmark: Measuring blind spots in semantic segmentation", "[39] Sun, P., Zhang, R., Jiang, Y., Kong, T., Xu, C., Zhan, W., Tomizuka, M., Li, L., Yuan, Z., Wang, C., et al.: Sparse r-cnn: End-to-end object detection with learnable proposals. In: CVPR (2021)": "Sparse R-CNN: End-to-End Object Detection with Learnable Proposals", "[22] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV (2017)": "Focal loss for dense object detection", "[13] Han, J., Liang, X., Xu, H., Chen, K., Hong, L., Mao, J., Ye, C., Zhang, W., Li, Z., Liang, X., Xu, C.: SODA10M: A large-scale 2d self/semi-supervised object detection dataset for autonomous driving. arXiv preprint arXiv:2106.11118 (2021)": "SODA10M: A large-scale 2d self/semi-supervised object detection dataset for autonomous driving", "[24] Lis, K., Nakka, K., Fua, P., Salzmann, M.: Detecting the unexpected via image resynthesis. In: ICCV (2019)": "Detecting the unexpected via image resynthesis", "[12] Gong, D., Liu, L., Le, V., Saha, B., Mansour, M.R., Venkatesh, S., van den Hengel, A.: Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In: ICCV (2019)": "Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection", "[43] Xia, Y., Zhang, Y., Liu, F., Shen, W., Yuille, A.L.: Synthesize then compare: Detecting failures and anomalies for semantic segmentation. In: ECCV (2020)": "Synthesize then compare: Detecting failures and anomalies for semantic segmentation", "[49] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159 (2020)": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "[41] Wang, X., Huang, T., Gonzalez, J., Darrell, T., Yu, F.: Frustratingly simple few-shot object detection. In: ICML (2020)": "Frustratingly simple few-shot object detection", "[45] Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell, T.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In: CVPR (2020)": "Bdd100k: A diverse driving dataset for heterogeneous multitask learning", "[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: CVPR (2020)": "Scalability in perception for autonomous driving: Waymo open dataset", "[14] Han, J., Liang, X., Xu, H., Chen, K., Hong, L., Ye, C., Zhang, W., Li, Z., Liang, X., Xu, C.: Soda10m: Towards large-scale object detection benchmark for autonomous driving. arXiv:2106.11118 (2021)": "Soda10m: Towards large-scale object detection benchmark for autonomous driving", "[17] Hendrycks, D., Basart, S., Mazeika, M., Mostajabi, M., Steinhardt, J., Song, D.: A benchmark for anomaly segmentation. arXiv preprint arXiv:1911.11132 (2019)": "A benchmark for anomaly segmentation", "[31] Qiao, L., Zhao, Y., Li, Z., Qiu, X., Wu, J., Zhang, C.: DeFRCN: Decoupled faster R-CNN for few-shot object detection. In: ICCV (2021)": "DeFRCN: Decoupled faster R-CNN for few-shot object detection", "[34] Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object detection with region proposal networks. In: NeurIPS (2015)": "Faster R-CNN: Towards real-time object detection with region proposal networks", "[5] Cai, Z., Vasconcelos, N.: Cascade R-CNN: delving into high quality object detection. In: CVPR (2018)": "Cascade R-CNN: Delving into High Quality Object Detection", "[23] Lin, T., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV (2014)": "Microsoft COCO: Common Objects in Context", "[19] Joseph, K., Khan, S., Khan, F.S., Balasubramanian, V.N.: Towards open world object detection. In: CVPR (2021)": "Towards open world object detection", "[27] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021)": "Swin transformer: Hierarchical vision transformer using shifted windows"}, "source_title_to_arxiv_id": {"Towards open world object detection": "2103.02603", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}