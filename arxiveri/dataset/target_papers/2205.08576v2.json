{"title": "Label-Efficient Self-Supervised Federated Learning for Tackling Data Heterogeneity in Medical Imaging", "abstract": "The collection and curation of large-scale medical datasets from multiple\ninstitutions is essential for training accurate deep learning models, but\nprivacy concerns often hinder data sharing. Federated learning (FL) is a\npromising solution that enables privacy-preserving collaborative learning among\ndifferent institutions, but it generally suffers from performance deterioration\ndue to heterogeneous data distributions and a lack of quality labeled data. In\nthis paper, we present a robust and label-efficient self-supervised FL\nframework for medical image analysis. Our method introduces a novel\nTransformer-based self-supervised pre-training paradigm that pre-trains models\ndirectly on decentralized target task datasets using masked image modeling, to\nfacilitate more robust representation learning on heterogeneous data and\neffective knowledge transfer to downstream models. Extensive empirical results\non simulated and real-world medical imaging non-IID federated datasets show\nthat masked image modeling with Transformers significantly improves the\nrobustness of models against various degrees of data heterogeneity. Notably,\nunder severe data heterogeneity, our method, without relying on any additional\npre-training data, achieves an improvement of 5.06%, 1.53% and 4.58% in test\naccuracy on retinal, dermatology and chest X-ray classification compared to the\nsupervised baseline with ImageNet pre-training. In addition, we show that our\nfederated self-supervised pre-training methods yield models that generalize\nbetter to out-of-distribution data and perform more effectively when\nfine-tuning with limited labeled data, compared to existing FL algorithms. The\ncode is available at https://github.com/rui-yan/SSL-FL.", "authors": ["Rui Yan", "Liangqiong Qu", "Qingyue Wei", "Shih-Cheng Huang", "Liyue Shen", "Daniel Rubin", "Lei Xing", "Yuyin Zhou"], "published_date": "2022_05_17", "pdf_url": "http://arxiv.org/pdf/2205.08576v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Models</th><td><p>Central</p></td><td><p>Split-1</p></td><td><p>Split-2</p></td><td><p>Split-3</p></td></tr><tr><th colspan=\"5\">Training from scratch (i.e. random initialization) </th></tr><tr><th>ViT-B [17]</th><td><p>73.70</p></td><td><p>74.33</p></td><td><p>69.50</p></td><td><p>64.13</p></td></tr><tr><th colspan=\"5\">Supervised Pre-Training on ImageNet-22K </th></tr><tr><th>ViT-B [17]</th><td><p>77.23</p></td><td><p>76.97</p></td><td><p>74.10</p></td><td><p>72.37</p></td></tr><tr><th colspan=\"5\">Self-Supervised Pre-Training on ImageNet-22K </th></tr><tr><th>BEiT [25]</th><td><p>78.90</p></td><td><p>78.03</p></td><td><p>74.90</p></td><td><p>72.50</p></td></tr><tr><th>MAE [26]</th><td>80.87</td><td>80.00</td><td><p>73.70</p></td><td><p>71.50</p></td></tr><tr><th colspan=\"5\">Self-Supervised Federated Pre-Training on Retina </th></tr><tr><th>Fed-BEiT</th><td><p>79.47</p></td><td><p>78.40</p></td><td>76.80</td><td>76.77</td></tr><tr><th>Fed-MAE</th><td>81.93</td><td>81.67</td><td>79.40</td><td>77.43</td></tr></tbody></table>", "caption": "TABLE II: Test Accuracy (%) on Retina dataset. The best result is in red, and the second-best result is in blue.", "list_citation_info": ["[25] H. Bao, L. Dong, and F. Wei, \u201cBeit: Bert pre-training of image transformers,\u201d in ICLR, 2021.", "[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.", "[26] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d arXiv preprint arXiv:2111.06377, 2021."]}, {"table": "<table><tbody><tr><th>Models</th><td><p>Central</p></td><td><p>Split-1</p></td><td><p>Split-2</p></td><td><p>Split-3</p></td></tr><tr><th colspan=\"5\">Training from scratch (i.e. random initialization) </th></tr><tr><th>ViT-B [17]</th><td><p>87.26</p></td><td><p>87.42</p></td><td><p>86.41</p></td><td><p>83.75</p></td></tr><tr><th colspan=\"5\">Supervised Pre-Training on ImageNet-22K </th></tr><tr><th>ViT-B [17]</th><td>95.52</td><td>94.76</td><td>92.26</td><td><p>88.43</p></td></tr><tr><th colspan=\"5\">Self-Supervised Pre-Training on ImageNet-22K </th></tr><tr><th>BEiT [25]</th><td><p>94.76</p></td><td><p>94.23</p></td><td><p>87.22</p></td><td><p>84.44</p></td></tr><tr><th>MAE [26]</th><td>97.06</td><td>96.09</td><td>90.16</td><td><p>89.27</p></td></tr><tr><th colspan=\"5\">Self-Supervised Federated Pre-Training on Derm </th></tr><tr><th>Fed-BEiT</th><td><p>92.74</p></td><td><p>92.30</p></td><td><p>89.96</p></td><td>89.96</td></tr><tr><th>Fed-MAE</th><td><p>93.55</p></td><td><p>94.31</p></td><td><p>89.96</p></td><td>89.57</td></tr></tbody></table>", "caption": "TABLE III: Test Accuracy (%) on Derm dataset.", "list_citation_info": ["[25] H. Bao, L. Dong, and F. Wei, \u201cBeit: Bert pre-training of image transformers,\u201d in ICLR, 2021.", "[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.", "[26] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d arXiv preprint arXiv:2111.06377, 2021."]}, {"table": "<table><tbody><tr><td>Models</td><td colspan=\"2\">Central</td><td colspan=\"2\">Real-world Split</td></tr><tr><td></td><td><p>Accuracy</p></td><td><p>AUC</p></td><td><p>Accuracy</p></td><td><p>AUC</p></td></tr><tr><td colspan=\"5\">Training from scratch (i.e. random initialization) </td></tr><tr><td><p>ViT-B [17]</p></td><td><p>91.42</p></td><td><p>95.30</p></td><td><p>77.08</p></td><td><p>88.69</p></td></tr><tr><td colspan=\"5\">Supervised Pre-Training on ImageNet-22K </td></tr><tr><td><p>ViT-B [17]</p></td><td><p>95.35</p></td><td><p>98.68</p></td><td><p>86.89</p></td><td>95.05</td></tr><tr><td colspan=\"5\">Supervised Pre-Training on ChestX-ray14 [61] </td></tr><tr><td><p>ViT-B [17]</p></td><td><p>92.78</p></td><td><p>97.61</p></td><td><p>86.26</p></td><td><p>93.59</p></td></tr><tr><td colspan=\"5\">Self-Supervised Pre-Training on ImageNet-22K </td></tr><tr><td><p>BEiT [25]</p></td><td><p>95.62</p></td><td>98.98</td><td><p>84.45</p></td><td><p>90.56</p></td></tr><tr><td><p>MAE [26]</p></td><td>96.35</td><td>98.87</td><td><p>84.32</p></td><td><p>90.76</p></td></tr><tr><td colspan=\"5\">Self-Supervised Federated Pre-Training on COVID-FL </td></tr><tr><td><p>Fed-BEiT</p></td><td><p>95.75</p></td><td><p>97.49</p></td><td>87.09</td><td><p>92.79</p></td></tr><tr><td><p>Fed-MAE</p></td><td>95.77</td><td><p>98.83</p></td><td>91.47</td><td>97.22</td></tr><tr><td colspan=\"5\">Self-Supervised Pre-Training on external ChestX-ray14 [61] </td></tr><tr><td><p>BEiT[25]</p></td><td><p>96.07</p></td><td><p>97.70</p></td><td><p>90.28</p></td><td><p>95.11</p></td></tr><tr><td><p>MAE[26]</p></td><td><p>96.35</p></td><td><p>98.70</p></td><td><p>91.04</p></td><td><p>95.89</p></td></tr></tbody></table>", "caption": "TABLE IV: Test Accuracy and AUC (%) on COVID-FL dataset.", "list_citation_info": ["[25] H. Bao, L. Dong, and F. Wei, \u201cBeit: Bert pre-training of image transformers,\u201d in ICLR, 2021.", "[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.", "[26] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d arXiv preprint arXiv:2111.06377, 2021.", "[61] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, \u201cChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases,\u201d in CVPR, 2017, pp. 2097\u20132106."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">FL methods</td><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">Pre-training</td><td colspan=\"2\">Real-world Split</td></tr><tr><td>Malignant</td><td>Benign</td></tr><tr><td>FedAvg [22]</td><td>ViT-B [17]</td><td>None</td><td>15.4</td><td>97.7</td></tr><tr><td>FedAvg</td><td>EfficientNet</td><td>ImageNet</td><td>16.1</td><td>97.4</td></tr><tr><td>FedMatch [62]</td><td>EfficientNet</td><td>ImageNet</td><td>16.0</td><td>97.3</td></tr><tr><td>FedPerl [63]</td><td>EfficientNet</td><td>ImageNet</td><td>17.8</td><td>97.4</td></tr><tr><td>FedAvg</td><td>ViT-B</td><td>ImageNet</td><td>23.5</td><td>98.1</td></tr><tr><td>Fed-BEiT</td><td>ViT-B</td><td>Skin-FL</td><td>24.2</td><td>98.4</td></tr><tr><td>Fed-MAE</td><td>ViT-B</td><td>Skin-FL</td><td>23.6</td><td>98.5</td></tr></tbody></table>", "caption": "TABLE V: F1-score (%) on Skin-FL dataset.", "list_citation_info": ["[22] D. Yang, Z. Xu, W. Li, A. Myronenko, H. R. Roth, S. Harmon, S. Xu, B. Turkbey, E. Turkbey, X. Wang et al., \u201cFederated semi-supervised learning for covid region segmentation in chest ct using multi-national data from china, italy, japan,\u201d Medical image analysis, vol. 70, p. 101992, 2021.", "[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.", "[62] W. Jeong, J. Yoon, E. Yang, and S. J. Hwang, \u201cFederated semi-supervised learning with inter-client consistency & disjoint learning,\u201d arXiv, 2020.", "[63] T. Bdair, N. Navab, S. Albarqouni et al., \u201cSemi-supervised federated peer learning for skin lesion classification,\u201d Machine Learning for Biomedical Imaging, vol. 1, no. April 2022 issue, pp. 1\u201310, 2022."]}], "citation_info_to_title": {"[22] D. Yang, Z. Xu, W. Li, A. Myronenko, H. R. Roth, S. Harmon, S. Xu, B. Turkbey, E. Turkbey, X. Wang et al., \u201cFederated semi-supervised learning for covid region segmentation in chest ct using multi-national data from china, italy, japan,\u201d Medical image analysis, vol. 70, p. 101992, 2021.": "Federated Semi-Supervised Learning for COVID Region Segmentation in Chest CT Using Multi-National Data from China, Italy, Japan", "[26] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d arXiv preprint arXiv:2111.06377, 2021.": "Masked Autoencoders are Scalable Vision Learners", "[17] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in ICLR, 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[25] H. Bao, L. Dong, and F. Wei, \u201cBeit: Bert pre-training of image transformers,\u201d in ICLR, 2021.": "Beit: Bert pre-training of image transformers", "[63] T. Bdair, N. Navab, S. Albarqouni et al., \u201cSemi-supervised federated peer learning for skin lesion classification,\u201d Machine Learning for Biomedical Imaging, vol. 1, no. April 2022 issue, pp. 1\u201310, 2022.": "Semi-supervised federated peer learning for skin lesion classification", "[62] W. Jeong, J. Yoon, E. Yang, and S. J. Hwang, \u201cFederated semi-supervised learning with inter-client consistency & disjoint learning,\u201d arXiv, 2020.": "Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning", "[61] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, \u201cChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases,\u201d in CVPR, 2017, pp. 2097\u20132106.": "Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases"}, "source_title_to_arxiv_id": {"Masked Autoencoders are Scalable Vision Learners": "2111.06377"}}