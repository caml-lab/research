{"title": "RTMDet: An Empirical Study of Designing Real-Time Object Detectors", "abstract": "In this paper, we aim to design an efficient real-time object detector that\nexceeds the YOLO series and is easily extensible for many object recognition\ntasks such as instance segmentation and rotated object detection. To obtain a\nmore efficient model architecture, we explore an architecture that has\ncompatible capacities in the backbone and neck, constructed by a basic building\nblock that consists of large-kernel depth-wise convolutions. We further\nintroduce soft labels when calculating matching costs in the dynamic label\nassignment to improve accuracy. Together with better training techniques, the\nresulting object detector, named RTMDet, achieves 52.8% AP on COCO with 300+\nFPS on an NVIDIA 3090 GPU, outperforming the current mainstream industrial\ndetectors. RTMDet achieves the best parameter-accuracy trade-off with\ntiny/small/medium/large/extra-large model sizes for various application\nscenarios, and obtains new state-of-the-art performance on real-time instance\nsegmentation and rotated object detection. We hope the experimental results can\nprovide new insights into designing versatile real-time object detectors for\nmany object recognition tasks. Code and models are released at\nhttps://github.com/open-mmlab/mmdetection/tree/3.x/configs/rtmdet.", "authors": ["Chengqi Lyu", "Wenwei Zhang", "Haian Huang", "Yue Zhou", "Yudong Wang", "Yanyi Liu", "Shilong Zhang", "Kai Chen"], "published_date": "2022_12_14", "pdf_url": "http://arxiv.org/pdf/2212.07784v2", "list_table_and_caption": [{"table": "<table><thead><tr><th><p>config</p></th><th><p>Object detection and instance segmentation</p></th><th><p>Rotate object detection</p></th></tr></thead><tbody><tr><td><p>optimizer</p></td><td><p>AdamW [38]</p></td><td><p>AdamW</p></td></tr><tr><td><p>base learning rate</p></td><td><p>0.004</p></td><td><p>0.00025</p></td></tr><tr><td><p>weight decay</p></td><td><p>0.05 (0 for bias and norm [33])</p></td><td><p>0.05 (0 for bias and norm)</p></td></tr><tr><td><p>optimizer momentum</p></td><td><p>0.9</p></td><td><p>0.9</p></td></tr><tr><td><p>batch size</p></td><td><p>256</p></td><td><p>8</p></td></tr><tr><td><p>learning rate schedule</p></td><td><p>Flat-Cosine</p></td><td><p>Flat-Cosine</p></td></tr><tr><td><p>training epochs</p></td><td><p>300</p></td><td><p>36</p></td></tr><tr><td><p>warmup iterations</p></td><td><p>1000</p></td><td><p>1000</p></td></tr><tr><td><p>input size</p></td><td><p>640 \\times640</p></td><td><p>1024 \\times1024</p></td></tr><tr><td><p>augmentation</p></td><td><p>cached Mosaic and MixUp (first 280 epochs); LSJ [22, 80] (last 20 epochs)</p></td><td><p>random Flip and Rotate</p></td></tr><tr><td><p>EMA decay</p></td><td><p>0.9998</p></td><td><p>0.9998</p></td></tr></tbody></table>", "caption": "Table 1: Training settings for object detection, instance segmentation and rotate object detection.", "list_citation_info": ["[38] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.", "[33] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 558\u2013567, 2019.", "[22] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin Cubuk, Quoc Le, Barret Zoph, Google Research, Brain Team, and U Berkeley. Simple copy-paste is a strong data augmentation method for instance segmentation. 2022."]}, {"table": "<table><tbody><tr><td>Model</td><td><p>Input shape</p></td><td><p>Params(M) \\downarrow</p></td><td><p>FLOPs(G) \\downarrow</p></td><td><p>Latency(ms) \\downarrow</p></td><td><p>AP(%)  \\uparrow</p></td><td><p>AP50(%)  \\uparrow</p></td></tr><tr><td>YOLOv5-n [25]</td><td><p>640(LB)</p></td><td><p>1.9</p></td><td><p>2.3</p></td><td><p>1.51</p></td><td><p>28.0</p></td><td><p>45.7</p></td></tr><tr><td>YOLOX-tiny [21]</td><td><p>416\\times416</p></td><td><p>5.1</p></td><td><p>3.3</p></td><td><p>0.82</p></td><td><p>32.8</p></td><td><p>50.3</p></td></tr><tr><td>YOLOv6-n [42]</td><td><p>640(LB)</p></td><td><p>4.3</p></td><td><p>5.6</p></td><td><p>0.79</p></td><td><p>35.9</p></td><td><p>51.2</p></td></tr><tr><td>YOLOv6-tiny</td><td><p>640(LB)</p></td><td><p>9.7</p></td><td><p>12.5</p></td><td><p>0.86</p></td><td><p>40.3</p></td><td><p>56.6</p></td></tr><tr><td>RTMDet-tiny</td><td>640\\times640</td><td>4.8</td><td>8.1</td><td>0.98</td><td>41.1</td><td>57.9</td></tr><tr><td>YOLOv5-s</td><td><p>640(LB)</p></td><td><p>7.2</p></td><td><p>8.3</p></td><td><p>1.63</p></td><td><p>37.4</p></td><td><p>56.8</p></td></tr><tr><td>YOLOX-s</td><td><p>640\\times640</p></td><td><p>9.0</p></td><td><p>13.4</p></td><td><p>1.20</p></td><td><p>40.5</p></td><td><p>59.3</p></td></tr><tr><td>YOLOv6-s</td><td><p>640(LB)</p></td><td><p>17.2</p></td><td><p>22.1</p></td><td><p>0.92</p></td><td><p>43.5</p></td><td><p>60.4</p></td></tr><tr><td>PPYOLOE-s [84]</td><td><p>640\\times640</p></td><td><p>7.9</p></td><td><p>8.7</p></td><td><p>1.34</p></td><td><p>43.0</p></td><td><p>59.6</p></td></tr><tr><td>RTMDet-s</td><td>640\\times640</td><td>8.99</td><td>14.8</td><td>1.22</td><td>44.6</td><td>61.9</td></tr><tr><td>YOLOv5-m</td><td><p>640(LB)</p></td><td><p>21.2</p></td><td><p>24.5</p></td><td><p>1.89</p></td><td><p>45.4</p></td><td><p>64.1</p></td></tr><tr><td>YOLOX-m</td><td><p>640\\times640</p></td><td><p>25.3</p></td><td><p>36.9</p></td><td><p>1.68</p></td><td><p>46.9</p></td><td><p>65.6</p></td></tr><tr><td>YOLOv6-m</td><td><p>640(LB)</p></td><td><p>34.3</p></td><td><p>41.1</p></td><td><p>1.21</p></td><td><p>48.5</p></td><td><p>-</p></td></tr><tr><td>PPYOLOE-m</td><td><p>640\\times640</p></td><td><p>23.4</p></td><td><p>25.0</p></td><td><p>1.75</p></td><td><p>49.0</p></td><td><p>65.9</p></td></tr><tr><td>RTMDet-m</td><td>640\\times640</td><td>24.7</td><td>39.3</td><td>1.62</td><td>49.4</td><td>66.8</td></tr><tr><td>YOLOv5-l</td><td><p>640(LB)</p></td><td><p>46.5</p></td><td><p>54.6</p></td><td><p>2.46</p></td><td><p>49.0</p></td><td><p>67.3</p></td></tr><tr><td>YOLOX-l</td><td><p>640\\times640</p></td><td><p>54.2</p></td><td><p>77.8</p></td><td><p>2.19</p></td><td><p>49.7</p></td><td><p>68.0</p></td></tr><tr><td>YOLOv6-l</td><td><p>640(LB)</p></td><td><p>58.5</p></td><td><p>72.0</p></td><td><p>1.91</p></td><td><p>51.0</p></td><td><p>-</p></td></tr><tr><td>YOLOv7 [71]</td><td><p>640(LB)</p></td><td><p>36.9</p></td><td><p>52.4</p></td><td><p>2.63</p></td><td><p>51.2</p></td><td><p>-</p></td></tr><tr><td>PPYOLOE-l</td><td><p>640\\times640</p></td><td><p>52.2</p></td><td><p>55.0</p></td><td><p>2.57</p></td><td><p>51.4</p></td><td><p>68.6</p></td></tr><tr><td>RTMDet-l</td><td>640\\times640</td><td>52.3</td><td>80.2</td><td>2.40</td><td>51.5</td><td>68.8</td></tr><tr><td>YOLOv5-x</td><td><p>640(LB)</p></td><td><p>86.7</p></td><td><p>102.9</p></td><td><p>2.92</p></td><td><p>50.7</p></td><td><p>68.9</p></td></tr><tr><td>YOLOX-x</td><td><p>640\\times640</p></td><td><p>99.1</p></td><td><p>141.0</p></td><td><p>2.98</p></td><td><p>51.1</p></td><td><p>69.4</p></td></tr><tr><td>PPYOLOE-x</td><td><p>640\\times640</p></td><td><p>98.4</p></td><td><p>103.3</p></td><td><p>3.07</p></td><td><p>52.3</p></td><td><p>69.5</p></td></tr><tr><td>RTMDet-x</td><td>640\\times640</td><td>94.9</td><td>141.7</td><td>3.10</td><td>52.8</td><td>70.4</td></tr></tbody></table>", "caption": "Table 2: Comparison of RTMDet with previous practices on the number of parameters, FLOPS, latency, and accuracy on COCO val2017 set. For a fair comparison, all models are trained for 300 epochs without using extra detection data or knowledge distillation. The inference speeds of all models are measured in the same environment. (LB) means LetterBox resize proposed in [25]. The results of the proposed RTMDet are marked in gray. The best results are in bold", "list_citation_info": ["[21] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. YOLOX: Exceeding YOLO series in 2021. CoRR, abs/2107.08430, 2021.", "[71] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. CoRR, abs/2207.02696, 2022.", "[84] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, and Baohua Lai. PP-YOLOE: An evolved version of YOLO. CoRR, abs/2203.16250, 2022.", "[25] glenn jocher et al. yolov5. https://github.com/ultralytics/yolov5, 2021.", "[42] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan Zhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and Xiaolin Wei. Yolov6: A single-stage object detection framework for industrial applications. CoRR, abs/2209.02976, 2022."]}, {"table": "<table><thead><tr><th>Model</th><th><p>Input shape</p></th><th><p>Epochs</p></th><th><p>Params(M) \\downarrow</p></th><th><p>FLOPs(G) \\downarrow</p></th><th><p>Latency(ms) \\downarrow</p></th><th><p>Box AP(%) \\uparrow</p></th><th><p>Mask AP(%) \\uparrow</p></th></tr></thead><tbody><tr><td>SparseInst-R50 [11]</td><td><p>640-853</p></td><td><p>147</p></td><td><p>31.6</p></td><td><p>99.1</p></td><td><p>-</p></td><td><p>-</p></td><td><p>34.2</p></td></tr><tr><td>SOLOv2-R50-FPN [77]</td><td><p>800-1333</p></td><td><p>36</p></td><td><p>46.4</p></td><td><p>253.5</p></td><td><p>-</p></td><td><p>-</p></td><td><p>37.5</p></td></tr><tr><td>CondInst-R50-FPN [69]</td><td><p>800-1333</p></td><td><p>36</p></td><td><p>33.9</p></td><td><p>240.8</p></td><td><p>-</p></td><td><p>42.6</p></td><td><p>38.2</p></td></tr><tr><td>Cascade-R50-FPN [5]</td><td><p>800-1333</p></td><td><p>36</p></td><td><p>77.1</p></td><td><p>403.6</p></td><td><p>-</p></td><td><p>44.3</p></td><td><p>38.5</p></td></tr><tr><td>RTMDet-Ins-R50-FPN</td><td>800-1333</td><td>36</td><td>35.9</td><td>295.2</td><td>-</td><td>45.3</td><td>39.7</td></tr><tr><td>YOLOv5n-seg [25]</td><td><p>640(LB)</p></td><td><p>300</p></td><td><p>2.0</p></td><td><p>3.6</p></td><td><p>1.65</p></td><td><p>27.6</p></td><td><p>23.4</p></td></tr><tr><td>YOLOv5s-seg</td><td><p>640(LB)</p></td><td><p>300</p></td><td><p>7.6</p></td><td><p>13.2</p></td><td><p>1.90</p></td><td><p>37.6</p></td><td><p>31.7</p></td></tr><tr><td>YOLOv5m-seg</td><td><p>640(LB)</p></td><td><p>300</p></td><td><p>22</p></td><td><p>35.4</p></td><td><p>2.71</p></td><td><p>45.0</p></td><td><p>37.1</p></td></tr><tr><td>YOLOv5l-seg</td><td><p>640(LB)</p></td><td><p>300</p></td><td><p>47.9</p></td><td><p>73.9</p></td><td><p>3.44</p></td><td><p>49.0</p></td><td><p>39.9</p></td></tr><tr><td>YOLOv5x-seg</td><td><p>640(LB)</p></td><td><p>300</p></td><td><p>88.8</p></td><td><p>132.9</p></td><td><p>5.10</p></td><td><p>50.7</p></td><td><p>41.4</p></td></tr><tr><td>RTMDet-Ins-tiny</td><td>640\\times640</td><td>300</td><td>5.6</td><td>11.8</td><td>1.70</td><td>40.5</td><td>35.4</td></tr><tr><td>RTMDet-Ins-s</td><td>640\\times640</td><td>300</td><td>10.2</td><td>21.5</td><td>1.93</td><td>44.0</td><td>38.7</td></tr><tr><td>RTMDet-Ins-m</td><td>640\\times640</td><td>300</td><td>27.6</td><td>54.1</td><td>2.69</td><td>48.8</td><td>42.1</td></tr><tr><td>RTMDet-Ins-l</td><td>640\\times640</td><td>300</td><td>57.4</td><td>106.6</td><td>3.68</td><td>51.2</td><td>43.7</td></tr><tr><td>RTMDet-Ins-x</td><td>640\\times640</td><td>300</td><td>102.7</td><td>182.7</td><td>5.31</td><td>52.4</td><td>44.6</td></tr></tbody></table>", "caption": "Table 3: Comparison of RTMDet-Ins with previous instance segmentation methods on the number of parameters, FLOPS, latency, and accuracy on COCO val2017 set. (LB) means LetterBox resize proposed in [25]. The results of the proposed RTMDet-Ins are marked in gray. The best results are in bold. Different from the object detection model, box NMS and post-processing of top-100 masks are included in the speed measurement", "list_citation_info": ["[77] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. SOLOv2: Dynamic, faster and stronger. In NeurIPS, 2020.", "[5] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object detection. In CVPR, 2018.", "[25] glenn jocher et al. yolov5. https://github.com/ultralytics/yolov5, 2021.", "[69] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European conference on computer vision, pages 282\u2013298. Springer, 2020.", "[11] Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Wenqiang Zhang, Qian Zhang, Chang Huang, Zhaoxiang Zhang, and Wenyu Liu. Sparse instance activation for real-time instance segmentation. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2022."]}, {"table": "<table><tbody><tr><td>Method</td><td>Pretrain</td><td>Backbone</td><td>MS</td><td>mAP(%)</td><td>PL</td><td>BD</td><td>BR</td><td>GTF</td><td>SV</td><td>LV</td><td>SH</td><td>TC</td><td>BC</td><td>ST</td><td>SBF</td><td>RA</td><td>HA</td><td>SP</td><td>HC</td></tr><tr><td>Anchor-based Methods</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>RoI Trans. [12]</td><td>IN</td><td>R101 [32]</td><td>\\checkmark</td><td>69.56</td><td>88.64</td><td>78.52</td><td>43.44</td><td>75.92</td><td>68.81</td><td>73.68</td><td>83.59</td><td>90.74</td><td>77.27</td><td>81.46</td><td>58.39</td><td>53.54</td><td>62.83</td><td>58.93</td><td>47.67</td></tr><tr><td>Gliding Vertex [85]</td><td>IN</td><td>R101</td><td>\\checkmark</td><td>75.02</td><td>89.64</td><td>85.00</td><td>52.26</td><td>77.34</td><td>73.01</td><td>73.14</td><td>86.82</td><td>90.74</td><td>79.02</td><td>86.81</td><td>59.55</td><td>70.91</td><td>72.94</td><td>70.86</td><td>57.32</td></tr><tr><td>CSL [87]</td><td>IN</td><td>R152</td><td>\\checkmark</td><td>76.17</td><td>90.25</td><td>85.53</td><td>54.64</td><td>75.31</td><td>70.44</td><td>73.51</td><td>77.62</td><td>90.84</td><td>86.15</td><td>86.69</td><td>69.60</td><td>68.04</td><td>73.83</td><td>71.10</td><td>68.93</td></tr><tr><td>R{}^{3}Det [88]</td><td>IN</td><td>R152</td><td>\\checkmark</td><td>76.47</td><td>89.80</td><td>83.77</td><td>48.11</td><td>66.77</td><td>78.76</td><td>83.27</td><td>87.84</td><td>90.82</td><td>85.38</td><td>85.51</td><td>65.57</td><td>62.68</td><td>67.53</td><td>78.56</td><td>72.62</td></tr><tr><td>DCL [86]</td><td>IN</td><td>R152</td><td>\\checkmark</td><td>77.37</td><td>89.26</td><td>83.60</td><td>53.54</td><td>72.76</td><td>79.04</td><td>82.56</td><td>87.31</td><td>90.67</td><td>86.59</td><td>86.98</td><td>67.49</td><td>66.88</td><td>73.29</td><td>70.56</td><td>69.99</td></tr><tr><td>S{}^{2}ANet [28]</td><td>IN</td><td>R50</td><td>\\checkmark</td><td>79.42</td><td>88.89</td><td>83.60</td><td>57.74</td><td>81.95</td><td>79.94</td><td>83.19</td><td>89.11</td><td>90.78</td><td>84.87</td><td>87.81</td><td>70.30</td><td>68.25</td><td>78.30</td><td>77.01</td><td>69.58</td></tr><tr><td>ReDet [29]</td><td>IN</td><td>Re50 [29]</td><td>\\checkmark</td><td>80.10</td><td>88.81</td><td>82.48</td><td>60.83</td><td>80.82</td><td>78.34</td><td>86.06</td><td>88.31</td><td>90.87</td><td>88.77</td><td>87.03</td><td>68.65</td><td>66.90</td><td>79.26</td><td>79.71</td><td>74.67</td></tr><tr><td>GWD [89]</td><td>IN</td><td>R152</td><td>\\checkmark</td><td>80.23</td><td>89.66</td><td>84.99</td><td>59.26</td><td>82.19</td><td>78.97</td><td>84.83</td><td>87.70</td><td>90.21</td><td>86.54</td><td>86.85</td><td>73.47</td><td>67.77</td><td>76.92</td><td>79.22</td><td>74.92</td></tr><tr><td>KLD [90]</td><td>IN</td><td>R152</td><td>\\checkmark</td><td>80.63</td><td>89.92</td><td>85.13</td><td>59.19</td><td>81.33</td><td>78.82</td><td>84.38</td><td>87.50</td><td>89.80</td><td>87.33</td><td>87.00</td><td>72.57</td><td>71.35</td><td>77.12</td><td>79.34</td><td>78.68</td></tr><tr><td>Oriented RCNN [83]</td><td>IN</td><td>R50</td><td>\\checkmark</td><td>80.87</td><td>89.84</td><td>85.43</td><td>61.09</td><td>79.82</td><td>79.71</td><td>85.35</td><td>88.82</td><td>90.88</td><td>86.68</td><td>87.73</td><td>72.21</td><td>70.80</td><td>82.42</td><td>78.18</td><td>74.11</td></tr><tr><td>RoI Trans. + KFIoU [91]</td><td>IN</td><td>Swin-tiny[51]</td><td>\\checkmark</td><td>80.93</td><td>89.44</td><td>84.41</td><td>62.22</td><td>82.51</td><td>80.10</td><td>86.07</td><td>88.68</td><td>90.90</td><td>87.32</td><td>88.38</td><td>72.80</td><td>71.95</td><td>78.96</td><td>74.95</td><td>75.27</td></tr><tr><td>Oriented RCNN</td><td>MAE</td><td>RVSA [73]</td><td>\\checkmark</td><td>81.18</td><td>89.40</td><td>83.94</td><td>59.76</td><td>82.10</td><td>81.73</td><td>85.32</td><td>88.88</td><td>90.86</td><td>85.69</td><td>87.65</td><td>63.70</td><td>69.94</td><td>84.72</td><td>84.16</td><td>79.90</td></tr><tr><td>Anchor-free Methods</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CFA [27]</td><td>IN</td><td>R152</td><td>\\checkmark</td><td>76.67</td><td>89.08</td><td>83.20</td><td>54.37</td><td>66.87</td><td>81.23</td><td>80.96</td><td>87.17</td><td>90.21</td><td>84.32</td><td>86.09</td><td>52.34</td><td>69.94</td><td>75.52</td><td>80.76</td><td>67.96</td></tr><tr><td>DAFNe [40]</td><td>IN</td><td>R101</td><td>\\checkmark</td><td>76.95</td><td>89.40</td><td>86.27</td><td>53.70</td><td>60.51</td><td>82.04</td><td>81.17</td><td>88.66</td><td>90.37</td><td>83.81</td><td>87.27</td><td>53.93</td><td>69.38</td><td>75.61</td><td>81.26</td><td>70.86</td></tr><tr><td>SASM [34]</td><td>IN</td><td>RX101 [82]</td><td>\\checkmark</td><td>77.19</td><td>88.41</td><td>83.32</td><td>54.00</td><td>74.34</td><td>80.87</td><td>84.10</td><td>88.04</td><td>90.74</td><td>82.85</td><td>86.26</td><td>63.96</td><td>66.78</td><td>78.40</td><td>73.84</td><td>61.97</td></tr><tr><td>Oriented RepPoints [44]</td><td>IN</td><td>Swin-tiny</td><td></td><td>77.63</td><td>89.11</td><td>82.32</td><td>56.71</td><td>74.95</td><td>80.70</td><td>83.73</td><td>87.67</td><td>90.81</td><td>87.11</td><td>85.85</td><td>63.60</td><td>68.60</td><td>75.95</td><td>73.54</td><td>63.76</td></tr><tr><td>PPYOLOE-R-s</td><td>IN</td><td>CRN-s [84]</td><td></td><td>73.82</td><td>88.80</td><td>79.24</td><td>45.92</td><td>66.88</td><td>80.41</td><td>82.95</td><td>88.20</td><td>90.61</td><td>82.91</td><td>86.37</td><td>55.80</td><td>64.11</td><td>65.09</td><td>79.50</td><td>50.43</td></tr><tr><td>PPYOLOE-R-s</td><td>IN</td><td>CRN-s</td><td>\\checkmark</td><td>79.42</td><td>88.93</td><td>83.95</td><td>56.60</td><td>79.40</td><td>82.57</td><td>85.89</td><td>88.64</td><td>90.87</td><td>87.82</td><td>87.54</td><td>68.94</td><td>63.46</td><td>76.66</td><td>79.19</td><td>70.87</td></tr><tr><td>PPYOLOE-R-m</td><td>IN</td><td>CRN-m</td><td></td><td>77.64</td><td>89.23</td><td>79.92</td><td>51.14</td><td>72.94</td><td>81.86</td><td>84.56</td><td>88.68</td><td>90.85</td><td>86.85</td><td>87.48</td><td>59.16</td><td>68.34</td><td>73.78</td><td>81.72</td><td>68.10</td></tr><tr><td>PPYOLOE-R-m</td><td>IN</td><td>CRN-m</td><td>\\checkmark</td><td>79.71</td><td>88.63</td><td>84.45</td><td>56.27</td><td>79.12</td><td>83.52</td><td>86.16</td><td>88.77</td><td>90.81</td><td>88.01</td><td>88.39</td><td>70.41</td><td>61.44</td><td>77.65</td><td>77.70</td><td>74.30</td></tr><tr><td>PPYOLOE-R-l</td><td>IN</td><td>CRN-l</td><td></td><td>78.14</td><td>89.18</td><td>81.00</td><td>54.01</td><td>70.22</td><td>81.85</td><td>85.16</td><td>88.81</td><td>90.81</td><td>86.99</td><td>88.01</td><td>62.87</td><td>67.87</td><td>76.56</td><td>79.13</td><td>69.65</td></tr><tr><td>PPYOLOE-R-l</td><td>IN</td><td>CRN-l</td><td>\\checkmark</td><td>80.02</td><td>88.40</td><td>84.75</td><td>58.91</td><td>76.35</td><td>83.13</td><td>86.10</td><td>88.79</td><td>90.87</td><td>88.74</td><td>87.71</td><td>67.71</td><td>68.44</td><td>77.92</td><td>76.17</td><td>76.35</td></tr><tr><td>PPYOLOE-R-x</td><td>IN</td><td>CRN-x</td><td></td><td>78.28</td><td>89.49</td><td>79.70</td><td>55.04</td><td>75.59</td><td>82.40</td><td>85.20</td><td>88.35</td><td>90.76</td><td>85.69</td><td>87.70</td><td>63.17</td><td>69.52</td><td>77.09</td><td>75.08</td><td>69.38</td></tr><tr><td>PPYOLOE-R-x</td><td>IN</td><td>CRN-x</td><td>\\checkmark</td><td>80.73</td><td>88.45</td><td>84.46</td><td>60.57</td><td>77.70</td><td>83.34</td><td>85.36</td><td>88.97</td><td>90.78</td><td>88.53</td><td>87.47</td><td>69.26</td><td>65.96</td><td>77.86</td><td>81.36</td><td>80.93</td></tr><tr><td>RTMDet-R-tiny</td><td>IN</td><td>Ours-tiny</td><td></td><td>75.36</td><td>89.21</td><td>80.03</td><td>47.88</td><td>69.73</td><td>82.05</td><td>83.33</td><td>88.63</td><td>90.91</td><td>86.31</td><td>86.85</td><td>59.94</td><td>62.30</td><td>74.23</td><td>71.97</td><td>57.03</td></tr><tr><td>RTMDet-R-tiny</td><td>IN</td><td>Ours-tiny</td><td>\\checkmark</td><td>79.82</td><td>87.89</td><td>85.70</td><td>55.83</td><td>81.28</td><td>81.47</td><td>85.12</td><td>88.91</td><td>90.88</td><td>88.15</td><td>87.96</td><td>67.29</td><td>68.59</td><td>77.71</td><td>80.50</td><td>69.96</td></tr><tr><td>RTMDet-R-s</td><td>IN</td><td>Ours-s</td><td></td><td>76.93</td><td>89.18</td><td>80.45</td><td>52.09</td><td>71.35</td><td>81.55</td><td>84.05</td><td>88.79</td><td>90.89</td><td>87.83</td><td>86.98</td><td>59.58</td><td>62.28</td><td>75.90</td><td>81.96</td><td>61.04</td></tr><tr><td>RTMDet-R-s</td><td>IN</td><td>Ours-s</td><td>\\checkmark</td><td>79.98</td><td>88.16</td><td>86.09</td><td>56.80</td><td>78.79</td><td>80.62</td><td>85.06</td><td>88.64</td><td>90.82</td><td>86.90</td><td>86.70</td><td>66.23</td><td>70.22</td><td>78.17</td><td>81.71</td><td>74.58</td></tr><tr><td>RTMDet-R-m</td><td>IN</td><td>Ours-m</td><td></td><td>78.24</td><td>89.17</td><td>84.65</td><td>53.92</td><td>74.67</td><td>81.48</td><td>83.99</td><td>88.71</td><td>90.85</td><td>87.43</td><td>87.20</td><td>59.39</td><td>66.68</td><td>77.71</td><td>82.40</td><td>65.28</td></tr><tr><td>RTMDet-R-m</td><td>IN</td><td>Ours-m</td><td>\\checkmark</td><td>80.26</td><td>87.10</td><td>85.83</td><td>56.30</td><td>80.28</td><td>80.04</td><td>84.67</td><td>88.22</td><td>90.88</td><td>88.49</td><td>87.57</td><td>70.74</td><td>69.99</td><td>78.35</td><td>80.88</td><td>74.53</td></tr><tr><td>RTMDet-R-l</td><td>IN</td><td>Ours-l</td><td></td><td>78.85</td><td>89.43</td><td>84.21</td><td>55.20</td><td>75.06</td><td>80.81</td><td>84.53</td><td>88.97</td><td>90.90</td><td>87.38</td><td>87.25</td><td>63.09</td><td>67.87</td><td>78.09</td><td>80.78</td><td>69.13</td></tr><tr><td>RTMDet-R-l</td><td>IN</td><td>Ours-l</td><td>\\checkmark</td><td>80.54</td><td>88.36</td><td>84.96</td><td>57.33</td><td>80.46</td><td>80.58</td><td>84.88</td><td>88.08</td><td>90.90</td><td>86.32</td><td>87.57</td><td>69.29</td><td>70.61</td><td>78.63</td><td>80.97</td><td>79.24</td></tr><tr><td>RTMDet-R-l</td><td>COCO</td><td>Ours-l</td><td>\\checkmark</td><td>81.33</td><td>88.01</td><td>86.17</td><td>58.54</td><td>82.44</td><td>81.30</td><td>84.82</td><td>88.71</td><td>90.89</td><td>88.77</td><td>87.37</td><td>71.96</td><td>71.18</td><td>81.23</td><td>81.40</td><td>77.13</td></tr></tbody></table>", "caption": "Table 4: Comparison of RTMDet-R with previous rotated object detection methods on the number of parameters, FLOPs, latency, and accuracy on DOTA-v1.0 test set. IN and COCO denote ImageNet pretraining and COCO pretraining. MAE means MAE unsupervised pretraining [30] on the MillionAID [54]. R50 and X50 denote ResNet-50 and ResNeXt-50 (likewise for R101, R152 and X101). Re50 denotes ReResNet-50, RVSA denotes RVSA-ViTAE-B and CRN denotes CSPRepResNet. MS means multi-scale training and testing. DOTA-v1.0 has 15 different object categories: plane (PL), baseball diamond (BD), bridge (BR), ground track field (GTF), small vehicle (SV), large vehicle (LV), ship (SH), tennis court (TC), basketball court (BC), storage tank (ST), soccer ball field (SBF), roundabout (RA), harbor (HA), swimming pool (SP), and helicopter (HC). The AP of each category is listed. The bold fonts indicate the best performance. The results of the proposed RTMDet-R are marked in gray", "list_citation_info": ["[82] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, 2017.", "[29] Jiaming Han, Jian Ding, Nan Xue, and Gui-Song Xia. Redet: A rotation-equivariant detector for aerial object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2786\u20132795, 2021.", "[87] Xue Yang and Junchi Yan. Arbitrary-oriented object detection with circular smooth label. In European Conference on Computer Vision, pages 677\u2013694. Springer, 2020.", "[54] Yang Long, Gui-Song Xia, Shengyang Li, Wen Yang, Michael Ying Yang, Xiao Xiang Zhu, Liangpei Zhang, and Deren Li. On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid. IEEE Journal of selected topics in applied earth observations and remote sensing, 14:4205\u20134230, 2021.", "[91] Xue Yang, Yue Zhou, Gefan Zhang, Jitui Yang, Wentao Wang, Junchi Yan, Xiaopeng Zhang, and Qi Tian. The kfiou loss for rotated object detection. arXiv preprint arXiv:2201.12558, 2022.", "[83] Xingxing Xie, Gong Cheng, Jiabao Wang, Xiwen Yao, and Junwei Han. Oriented r-cnn for object detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 3520\u20133529, 2021.", "[85] Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Gui-Song Xia, and Xiang Bai. Gliding vertex on the horizontal bounding box for multi-oriented object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(4):1452\u20131459, 2020.", "[86] Xue Yang, Liping Hou, Yue Zhou, Wentao Wang, and Junchi Yan. Dense label encoding for boundary discontinuity free rotation detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15819\u201315829, 2021.", "[44] Wentong Li, Yijie Chen, Kaixuan Hu, and Jianke Zhu. Oriented reppoints for aerial object detection. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1819\u20131828, 2022.", "[12] Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu. Learning roi transformer for oriented object detection in aerial images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2849\u20132858, 2019.", "[28] Jiaming Han, Jian Ding, Jie Li, and Gui-Song Xia. Align deep features for oriented object detection. IEEE Transactions on Geoscience and Remote Sensing, 2021.", "[73] Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Advancing plain vision transformer towards remote sensing foundation model. IEEE Transactions on Geoscience and Remote Sensing, pages 1\u20131, 2022.", "[89] Xue Yang, Junchi Yan, Qi Ming, Wentao Wang, Xiaopeng Zhang, and Qi Tian. Rethinking rotated object detection with gaussian wasserstein distance loss. In International Conference on Machine Learning, pages 11830\u201311841. PMLR, 2021.", "[40] Steven Lang, Fabrizio Ventola, and Kristian Kersting. Dafne: A one-stage anchor-free deep model for oriented object detection. arXiv preprint arXiv:2109.06148, 2021.", "[27] Zonghao Guo, Chang Liu, Xiaosong Zhang, Jianbin Jiao, Xiangyang Ji, and Qixiang Ye. Beyond bounding-box: Convex-hull feature adaptation for oriented and densely packed object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8792\u20138801, 2021.", "[51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 9992\u201310002. IEEE, 2021.", "[34] Liping Hou, Ke Lu, Jian Xue, and Yuqiu Li. Shape-adaptive selection and measurement for oriented object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.", "[90] Xue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian, and Junchi Yan. Learning high-precision bounding box for rotated object detection via kullback-leibler divergence. Advances in Neural Information Processing Systems, 34, 2021.", "[30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.", "[84] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, and Baohua Lai. PP-YOLOE: An evolved version of YOLO. CoRR, abs/2203.16250, 2022.", "[88] Xue Yang, Junchi Yan, Ziming Feng, and Tao He. R3det: Refined single-stage detector with feature refinement for rotating object. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 3163\u20133171, 2021.", "[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016."]}, {"table": "<table><thead><tr><th>Method</th><th>MS</th><th>PL</th><th>BD</th><th>BR</th><th>GTF</th><th>SV</th><th>LV</th><th>SH</th><th>TC</th><th>BC</th><th>ST</th><th>SBF</th><th>RA</th><th>HA</th><th>SP</th><th>HC</th><th>CC</th><th>mAP</th></tr></thead><tbody><tr><th>RetinaNet-OBB [47]</th><th></th><td>71.43</td><td>77.64</td><td>42.12</td><td>64.65</td><td>44.53</td><td>56.79</td><td>73.31</td><td>90.84</td><td>76.02</td><td>59.96</td><td>46.95</td><td>69.24</td><td>59.65</td><td>64.52</td><td>48.06</td><td>0.83</td><td>59.16</td></tr><tr><th>FR-OBB [66]</th><th></th><td>71.89</td><td>74.47</td><td>44.45</td><td>59.87</td><td>51.28</td><td>69.98</td><td>79.37</td><td>90.78</td><td>77.38</td><td>67.50</td><td>47.75</td><td>69.72</td><td>61.22</td><td>65.28</td><td>60.47</td><td>1.54</td><td>62.00</td></tr><tr><th>MASK RCNN [31]</th><th></th><td>76.84</td><td>73.51</td><td>49.90</td><td>57.80</td><td>51.31</td><td>71.34</td><td>79.75</td><td>90.46</td><td>74.21</td><td>66.07</td><td>46.21</td><td>70.61</td><td>63.07</td><td>64.46</td><td>57.81</td><td>9.42</td><td>62.67</td></tr><tr><th>HTC [8]</th><th></th><td>77.80</td><td>73.67</td><td>51.40</td><td>63.99</td><td>51.54</td><td>73.31</td><td>80.31</td><td>90.48</td><td>75.12</td><td>67.34</td><td>48.51</td><td>70.63</td><td>64.84</td><td>64.48</td><td>55.87</td><td>5.15</td><td>63.40</td></tr><tr><th>DAFNe</th><th>\\checkmark</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>71.99</td></tr><tr><th>OWSR [43]</th><th>\\checkmark</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>74.90</td></tr><tr><th>ReDet</th><th></th><td>79.20</td><td>82.81</td><td>51.92</td><td>71.41</td><td>52.38</td><td>75.73</td><td>80.92</td><td>90.83</td><td>75.81</td><td>68.64</td><td>49.29</td><td>72.03</td><td>73.36</td><td>70.55</td><td>63.33</td><td>11.53</td><td>66.86</td></tr><tr><th>ReDet</th><th>\\checkmark</th><td>88.51</td><td>86.45</td><td>61.23</td><td>81.20</td><td>67.60</td><td>83.65</td><td>90.00</td><td>90.86</td><td>84.30</td><td>75.33</td><td>71.49</td><td>72.06</td><td>78.32</td><td>74.73</td><td>76.10</td><td>46.98</td><td>76.80</td></tr><tr><th>RTMDet-R-tiny</th><th></th><td>77.79</td><td>83.03</td><td>48.45</td><td>73.37</td><td>59.33</td><td>81.30</td><td>88.89</td><td>90.88</td><td>80.73</td><td>76.26</td><td>51.81</td><td>71.59</td><td>75.81</td><td>75.19</td><td>54.36</td><td>20.01</td><td>69.30</td></tr><tr><th>RTMDet-R-tiny</th><th>\\checkmark</th><td>88.14</td><td>83.09</td><td>51.80</td><td>77.54</td><td>65.99</td><td>82.22</td><td>89.81</td><td>90.88</td><td>80.54</td><td>81.34</td><td>64.64</td><td>71.51</td><td>77.13</td><td>76.32</td><td>72.11</td><td>46.67</td><td>74.98</td></tr><tr><th>RTMDet-R-s</th><th></th><td>80.05</td><td>84.36</td><td>50.65</td><td>72.04</td><td>59.54</td><td>81.79</td><td>89.22</td><td>90.90</td><td>83.07</td><td>76.27</td><td>56.82</td><td>72.13</td><td>76.25</td><td>77.04</td><td>65.66</td><td>32.84</td><td>71.79</td></tr><tr><th>RTMDet-R-s</th><th>\\checkmark</th><td>88.14</td><td>85.82</td><td>52.90</td><td>82.09</td><td>65.58</td><td>81.83</td><td>89.78</td><td>90.82</td><td>83.31</td><td>82.47</td><td>68.51</td><td>70.93</td><td>78.00</td><td>75.77</td><td>73.09</td><td>47.32</td><td>76.02</td></tr><tr><th>RTMDet-R-m</th><th></th><td>80.34</td><td>86.00</td><td>54.02</td><td>72.98</td><td>63.21</td><td>82.09</td><td>89.46</td><td>90.87</td><td>85.12</td><td>76.69</td><td>63.12</td><td>72.14</td><td>77.91</td><td>76.04</td><td>71.57</td><td>32.24</td><td>73.36</td></tr><tr><th>RTMDet-R-m</th><th>\\checkmark</th><td>89.07</td><td>86.71</td><td>52.57</td><td>82.47</td><td>66.13</td><td>82.55</td><td>89.77</td><td>90.88</td><td>84.39</td><td>83.34</td><td>69.51</td><td>73.03</td><td>77.82</td><td>75.98</td><td>80.21</td><td>42.00</td><td>76.65</td></tr><tr><th>RTMDet-R-l</th><th></th><td>80.73</td><td>84.79</td><td>54.09</td><td>76.30</td><td>63.56</td><td>83.06</td><td>89.77</td><td>90.89</td><td>86.65</td><td>76.98</td><td>63.68</td><td>70.31</td><td>78.11</td><td>75.91</td><td>75.09</td><td>31.20</td><td>73.82</td></tr><tr><th>RTMDet-R-l</th><th>\\checkmark</th><td>89.31</td><td>86.38</td><td>55.09</td><td>83.17</td><td>66.11</td><td>82.44</td><td>89.85</td><td>90.84</td><td>86.95</td><td>83.76</td><td>68.35</td><td>74.36</td><td>77.60</td><td>77.39</td><td>77.87</td><td>60.37</td><td>78.12</td></tr></tbody></table>", "caption": "Table A2: Comparison with state-of-the-art methods on DOTA v1.5 dataset. MS means multi-scale training and testing. For the DOTA-v1.5 dataset, we use 4 NVIDIA A100 GPUs for training. Since we find that COCO pretraining significantly improves the results on DOTA-v1.5, we use COCO pretraining by default. DOTA-v1.5 has 16 different object categories: plane (PL), baseball diamond (BD), bridge (BR), ground track field (GTF), small vehicle (SV), large vehicle (LV), ship (SH), tennis court (TC), basketball court (BC), storage tank (ST), soccer ball field (SBF), roundabout (RA), harbor (HA), swimming pool (SP), helicopter (HC) and container crane (CC). The AP of each category is listed. The bold fonts indicate the best performance. The results of the proposed RTMDet-R are marked in gray", "list_citation_info": ["[47] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.", "[31] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask R-CNN. In ICCV, 2017.", "[43] Chengzheng Li, Chunyan Xu, Zhen Cui, Dan Wang, Zequn Jie, Tong Zhang, and Jian Yang. Learning object-wise semantic representation for detection in remote sensing imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.", "[66] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.", "[8] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019."]}], "citation_info_to_title": {"[77] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. SOLOv2: Dynamic, faster and stronger. In NeurIPS, 2020.": "SOLOv2: Dynamic, faster and stronger", "[28] Jiaming Han, Jian Ding, Jie Li, and Gui-Song Xia. Align deep features for oriented object detection. IEEE Transactions on Geoscience and Remote Sensing, 2021.": "Align deep features for oriented object detection", "[88] Xue Yang, Junchi Yan, Ziming Feng, and Tao He. R3det: Refined single-stage detector with feature refinement for rotating object. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 3163\u20133171, 2021.": "R3det: Refined single-stage detector with feature refinement for rotating object", "[84] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang, Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing Dang, Shengyu Wei, Yuning Du, and Baohua Lai. PP-YOLOE: An evolved version of YOLO. CoRR, abs/2203.16250, 2022.": "PP-YOLOE: An evolved version of YOLO", "[47] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.": "Focal loss for dense object detection", "[87] Xue Yang and Junchi Yan. Arbitrary-oriented object detection with circular smooth label. In European Conference on Computer Vision, pages 677\u2013694. Springer, 2020.": "Arbitrary-oriented object detection with circular smooth label", "[29] Jiaming Han, Jian Ding, Nan Xue, and Gui-Song Xia. Redet: A rotation-equivariant detector for aerial object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2786\u20132795, 2021.": "Redet: A rotation-equivariant detector for aerial object detection", "[34] Liping Hou, Ke Lu, Jian Xue, and Yuqiu Li. Shape-adaptive selection and measurement for oriented object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022.": "Shape-Adaptive Selection and Measurement for Oriented Object Detection", "[82] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, 2017.": "Aggregated residual transformations for deep neural networks", "[12] Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu. Learning roi transformer for oriented object detection in aerial images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2849\u20132858, 2019.": "Learning ROI Transformer for Oriented Object Detection in Aerial Images", "[30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.": "Masked Autoencoders are Scalable Vision Learners", "[22] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin Cubuk, Quoc Le, Barret Zoph, Google Research, Brain Team, and U Berkeley. Simple copy-paste is a strong data augmentation method for instance segmentation. 2022.": "Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation", "[42] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, Weiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan Zhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and Xiaolin Wei. Yolov6: A single-stage object detection framework for industrial applications. CoRR, abs/2209.02976, 2022.": "Yolov6: A single-stage object detection framework for industrial applications", "[91] Xue Yang, Yue Zhou, Gefan Zhang, Jitui Yang, Wentao Wang, Junchi Yan, Xiaopeng Zhang, and Qi Tian. The kfiou loss for rotated object detection. arXiv preprint arXiv:2201.12558, 2022.": "The kfiou loss for rotated object detection", "[5] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into high quality object detection. In CVPR, 2018.": "Cascade R-CNN: Delving into high quality object detection", "[85] Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Gui-Song Xia, and Xiang Bai. Gliding vertex on the horizontal bounding box for multi-oriented object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(4):1452\u20131459, 2020.": "Gliding vertex on the horizontal bounding box for multi-oriented object detection", "[25] glenn jocher et al. yolov5. https://github.com/ultralytics/yolov5, 2021.": "Yolov5", "[90] Xue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian, and Junchi Yan. Learning high-precision bounding box for rotated object detection via kullback-leibler divergence. Advances in Neural Information Processing Systems, 34, 2021.": "Learning high-precision bounding box for rotated object detection via kullback-leibler divergence", "[40] Steven Lang, Fabrizio Ventola, and Kristian Kersting. Dafne: A one-stage anchor-free deep model for oriented object detection. arXiv preprint arXiv:2109.06148, 2021.": "Dafne: A one-stage anchor-free deep model for oriented object detection", "[11] Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Wenqiang Zhang, Qian Zhang, Chang Huang, Zhaoxiang Zhang, and Wenyu Liu. Sparse instance activation for real-time instance segmentation. In Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2022.": "Sparse Instance Activation for Real-Time Instance Segmentation", "[51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 9992\u201310002. IEEE, 2021.": "Swin Transformer: Hierarchical vision transformer using shifted windows", "[31] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross B. Girshick. Mask R-CNN. In ICCV, 2017.": "Mask R-CNN", "[69] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In European conference on computer vision, pages 282\u2013298. Springer, 2020.": "Conditional Convolutions for Instance Segmentation", "[73] Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Advancing plain vision transformer towards remote sensing foundation model. IEEE Transactions on Geoscience and Remote Sensing, pages 1\u20131, 2022.": "Advancing plain vision transformer towards remote sensing foundation model", "[27] Zonghao Guo, Chang Liu, Xiaosong Zhang, Jianbin Jiao, Xiangyang Ji, and Qixiang Ye. Beyond bounding-box: Convex-hull feature adaptation for oriented and densely packed object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8792\u20138801, 2021.": "Beyond bounding-box: Convex-hull feature adaptation for oriented and densely packed object detection", "[8] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "Hybrid Task Cascade for Instance Segmentation", "[83] Xingxing Xie, Gong Cheng, Jiabao Wang, Xiwen Yao, and Junwei Han. Oriented r-cnn for object detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 3520\u20133529, 2021.": "Oriented R-CNN for Object Detection", "[54] Yang Long, Gui-Song Xia, Shengyang Li, Wen Yang, Michael Ying Yang, Xiao Xiang Zhu, Liangpei Zhang, and Deren Li. On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid. IEEE Journal of selected topics in applied earth observations and remote sensing, 14:4205\u20134230, 2021.": "On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid", "[43] Chengzheng Li, Chunyan Xu, Zhen Cui, Dan Wang, Zequn Jie, Tong Zhang, and Jian Yang. Learning object-wise semantic representation for detection in remote sensing imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.": "Learning Object-wise Semantic Representation for Detection in Remote Sensing Imagery", "[33] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 558\u2013567, 2019.": "Bag of tricks for image classification with convolutional neural networks", "[38] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.": "Adam: A method for stochastic optimization", "[44] Wentong Li, Yijie Chen, Kaixuan Hu, and Jianke Zhu. Oriented reppoints for aerial object detection. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1819\u20131828, 2022.": "Oriented reppoints for aerial object detection", "[89] Xue Yang, Junchi Yan, Qi Ming, Wentao Wang, Xiaopeng Zhang, and Qi Tian. Rethinking rotated object detection with gaussian wasserstein distance loss. In International Conference on Machine Learning, pages 11830\u201311841. PMLR, 2021.": "Rethinking rotated object detection with Gaussian Wasserstein distance loss", "[21] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. YOLOX: Exceeding YOLO series in 2021. CoRR, abs/2107.08430, 2021.": "YOLOX: Exceeding YOLO series in 2021", "[66] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.": "Faster R-CNN: Towards real-time object detection with region proposal networks", "[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.": "Deep Residual Learning for Image Recognition", "[86] Xue Yang, Liping Hou, Yue Zhou, Wentao Wang, and Junchi Yan. Dense label encoding for boundary discontinuity free rotation detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 15819\u201315829, 2021.": "Dense label encoding for boundary discontinuity free rotation detection", "[71] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. CoRR, abs/2207.02696, 2022.": "Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"}, "source_title_to_arxiv_id": {"Redet: A rotation-equivariant detector for aerial object detection": "2103.07733", "Masked Autoencoders are Scalable Vision Learners": "2111.06377", "Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation": "2012.07177", "Swin Transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "On creating benchmark dataset for aerial image interpretation: Reviews, guidances, and million-aid": "2006.12485"}}