{"title": "Real-time Online Video Detection with Temporal Smoothing Transformers", "abstract": "Streaming video recognition reasons about objects and their actions in every\nframe of a video. A good streaming recognition model captures both long-term\ndynamics and short-term changes of video. Unfortunately, in most existing\nmethods, the computational complexity grows linearly or quadratically with the\nlength of the considered dynamics. This issue is particularly pronounced in\ntransformer-based architectures. To address this issue, we reformulate the\ncross-attention in a video transformer through the lens of kernel and apply two\nkinds of temporal smoothing kernel: A box kernel or a Laplace kernel. The\nresulting streaming attention reuses much of the computation from frame to\nframe, and only requires a constant time update each frame. Based on this idea,\nwe build TeSTra, a Temporal Smoothing Transformer, that takes in arbitrarily\nlong inputs with constant caching and computing overhead. Specifically, it runs\n$6\\times$ faster than equivalent sliding-window based transformers with 2,048\nframes in a streaming setting. Furthermore, thanks to the increased temporal\nspan, TeSTra achieves state-of-the-art results on THUMOS'14 and\nEPIC-Kitchen-100, two standard online action detection and action anticipation\ndatasets. A real-time version of TeSTra outperforms all but one prior\napproaches on the THUMOS'14 dataset.", "authors": ["Yue Zhao", "Philipp Kr\u00e4henb\u00fchl"], "published_date": "2022_09_19", "pdf_url": "http://arxiv.org/pdf/2209.09236v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">method</th><td rowspan=\"2\">Pre-train</td><td colspan=\"8\">mAP@\\tau_{o}</td><td rowspan=\"2\">average</td></tr><tr><td>0.25</td><td>0.50</td><td>0.75</td><td>1.0</td><td>1.25</td><td>1.50</td><td>1.75</td><td>2.0</td></tr><tr><th>RED [17]</th><td rowspan=\"4\">ANet1.3</td><td>45.3</td><td>42.1</td><td>39.6</td><td>37.5</td><td>35.8</td><td>34.4</td><td>33.2</td><td>32.1</td><td>37.5</td></tr><tr><th>TRN [53]</th><td>45.1</td><td>42.4</td><td>40.7</td><td>39.1</td><td>37.7</td><td>36.4</td><td>35.3</td><td>34.3</td><td>38.9</td></tr><tr><th>TTM [49]</th><td>45.9</td><td>43.7</td><td>42.4</td><td>41.0</td><td>39.9</td><td>39.4</td><td>37.9</td><td>37.3</td><td>40.9</td></tr><tr><th>LSTR [54]</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>50.1</td></tr><tr><th>Ours</th><td></td><td>64.7</td><td>61.8</td><td>58.7</td><td>55.7</td><td>53.2</td><td>51.1</td><td>49.2</td><td>47.8</td><td>55.3</td></tr><tr><th>TTM [49]</th><td rowspan=\"3\">K400</td><td>46.8</td><td>45.5</td><td>44.6</td><td>43.6</td><td>41.9</td><td>41.1</td><td>40.4</td><td>38.7</td><td>42.8</td></tr><tr><th>LSTR{}^{\\dagger} [54]</th><td>60.4</td><td>58.6</td><td>56.0</td><td>53.3</td><td>50.9</td><td>48.9</td><td>47.1</td><td>45.7</td><td>52.6</td></tr><tr><th>Ours</th><td>66.2</td><td>63.5</td><td>60.5</td><td>57.4</td><td>54.8</td><td>52.6</td><td>50.5</td><td>48.9</td><td>56.8</td></tr></tbody></table>", "caption": "Table 2: Result of online action anticipation on THUMOS\u201914.{}^{\\dagger} was reproduced by us because LSTR [54] only reported ActivityNet-pretrained results", "list_citation_info": ["[54] Xu, M., Xiong, Y., Chen, H., Li, X., Xia, W., Tu, Z., Soatto, S.: Long short-term transformer for online action detection. In: NeurIPS (2021)", "[17] Gao, J., Yang, Z., Nevatia, R.: Red: Reinforced encoder-decoder networks for action anticipation. In: BMVC (2017)", "[53] Xu, M., Gao, M., Chen, Y.T., Davis, L.S., Crandall, D.J.: Temporal recurrent networks for online action detection. In: ICCV (2019)", "[49] Wang, X., Zhang, S., Qing, Z., Shao, Y., Zuo, Z., Gao, C., Sang, N.: OadTR: Online action detection with transformers. In: ICCV (2021)"]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td rowspan=\"2\">Input</td><td rowspan=\"2\">Pre-train</td><td colspan=\"3\">overall</td><td colspan=\"3\">unseen</td><td colspan=\"3\">tail</td></tr><tr><td>verb</td><td>noun</td><td>action</td><td>verb</td><td>noun</td><td>action</td><td>verb</td><td>noun</td><td>action</td></tr><tr><th>RULSTM [16]</th><td rowspan=\"4\">RGB</td><td>IN-1k</td><td>27.5</td><td>29.0</td><td>13.3</td><td>29.8</td><td>23.8</td><td>13.1</td><td>19.9</td><td>21.4</td><td>10.6</td></tr><tr><th>AVT [19]</th><td>IN-1k</td><td>27.2</td><td>30.7</td><td>13.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AVT [19]</th><td>IN-21k</td><td>30.2</td><td>31.7</td><td>14.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>IN-1k</td><td>26.8</td><td>36.2</td><td>17.0</td><td>27.1</td><td>30.1</td><td>13.3</td><td>19.3</td><td>28.6</td><td>13.7</td></tr><tr><th>RULSTM [16]</th><td rowspan=\"4\">RGB+OF+Obj</td><td>IN-1k</td><td>27.8</td><td>30.8</td><td>14.0</td><td>28.8</td><td>27.2</td><td>14.2</td><td>19.8</td><td>22.0</td><td>11.1</td></tr><tr><th>TempAgg [39]</th><td>IN-1k</td><td>23.2</td><td>31.4</td><td>14.7</td><td>28.0</td><td>26.2</td><td>14.5</td><td>14.5</td><td>22.5</td><td>11.8</td></tr><tr><th>AVT+ [19]</th><td>IN-1k</td><td>25.5</td><td>31.8</td><td>14.8</td><td>25.5</td><td>23.6</td><td>11.5</td><td>18.5</td><td>25.8</td><td>12.6</td></tr><tr><th>AVT+ [19]</th><td>IN-21k</td><td>28.2</td><td>32.0</td><td>15.9</td><td>29.5</td><td>23.9</td><td>11.9</td><td>21.1</td><td>25.8</td><td>14.1</td></tr><tr><th>Ours</th><td>RGB+OF</td><td>IN-1k</td><td>30.8</td><td>35.8</td><td>17.6</td><td>29.6</td><td>26.0</td><td>12.8</td><td>23.2</td><td>29.2</td><td>14.2</td></tr></tbody></table>", "caption": "Table 3: Result of action anticipation on EK100.The upper half lists RGB-only methods; in lower half all types of inputs are allowed", "list_citation_info": ["[19] Girdhar, R., Grauman, K.: Anticipative video transformer. In: ICCV (2021)", "[39] Sener, F., Singhania, D., Yao, A.: Temporal aggregate representations for long-range video understanding. In: ECCV (2020)", "[16] Furnari, A., Farinella, G.M.: Rolling-unrolling lstms for action anticipation from first-person video. TPAMI (2020)"]}], "citation_info_to_title": {"[19] Girdhar, R., Grauman, K.: Anticipative video transformer. In: ICCV (2021)": "Anticipative video transformer", "[53] Xu, M., Gao, M., Chen, Y.T., Davis, L.S., Crandall, D.J.: Temporal recurrent networks for online action detection. In: ICCV (2019)": "Temporal Recurrent Networks for Online Action Detection", "[16] Furnari, A., Farinella, G.M.: Rolling-unrolling lstms for action anticipation from first-person video. TPAMI (2020)": "Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video", "[54] Xu, M., Xiong, Y., Chen, H., Li, X., Xia, W., Tu, Z., Soatto, S.: Long short-term transformer for online action detection. In: NeurIPS (2021)": "Long short-term transformer for online action detection", "[17] Gao, J., Yang, Z., Nevatia, R.: Red: Reinforced encoder-decoder networks for action anticipation. In: BMVC (2017)": "Red: Reinforced Encoder-Decoder Networks for Action Anticipation", "[39] Sener, F., Singhania, D., Yao, A.: Temporal aggregate representations for long-range video understanding. In: ECCV (2020)": "Temporal Aggregate Representations for Long-Range Video Understanding", "[49] Wang, X., Zhang, S., Qing, Z., Shao, Y., Zuo, Z., Gao, C., Sang, N.: OadTR: Online action detection with transformers. In: ICCV (2021)": "OadTR: Online action detection with transformers"}, "source_title_to_arxiv_id": {"Long short-term transformer for online action detection": "2107.03377"}}