{"title": "OPAL: Occlusion Pattern Aware Loss for Unsupervised Light Field Disparity Estimation", "abstract": "Light field disparity estimation is an essential task in computer vision with\nvarious applications. Although supervised learning-based methods have achieved\nboth higher accuracy and efficiency than traditional optimization-based\nmethods, the dependency on ground-truth disparity for training limits the\noverall generalization performance not to say for real-world scenarios where\nthe ground-truth disparity is hard to capture. In this paper, we argue that\nunsupervised methods can achieve comparable accuracy, but, more importantly,\nmuch higher generalization capacity and efficiency than supervised methods.\nSpecifically, we present the Occlusion Pattern Aware Loss, named OPAL, which\nsuccessfully extracts and encodes the general occlusion patterns inherent in\nthe light field for loss calculation. OPAL enables: i) accurate and robust\nestimation by effectively handling occlusions without using any ground-truth\ninformation for training and ii) much efficient performance by significantly\nreducing the network parameters required for accurate inference. Besides, a\ntransformer-based network and a refinement module are proposed for achieving\neven more accurate results. Extensive experiments demonstrate our method not\nonly significantly improves the accuracy compared with the SOTA unsupervised\nmethods, but also possesses strong generalization capacity, even for real-world\ndata, compared with supervised methods. Our code will be made publicly\navailable.", "authors": ["Peng Li", "Jiayin Zhao", "Jingyao Wu", "Chao Deng", "Haoqian Wang", "Tao Yu"], "published_date": "2022_03_04", "pdf_url": "http://arxiv.org/pdf/2203.02231v3", "list_table_and_caption": [{"table": "<p>.Methods4D LF Benchmark [ MSE\\times100 / BadPix(0.07)]HCI Blender [ MSE\\times100 / BadPix(0.07)]Cross-DatasetAverage Boxes  Cotton  Dino  Sideboard  Buddha  Buddha2  MonasRoom  Papillon Optimization-basedACC [12] 14.151 / 27.294  10.083 / 9.182  1.311 / 22.358  5.588 / 24.660  1.419 / 10.113  0.523 / 10.467  0.657 / 10.562  5.691 / 22.514  4.928 / 17.144 SPO [33] 9.113 / 15.060  1.376 / 2.858  0.410 / 2.767  1.143 / 9.547  0.554 / 2.336  1.046 / 14.191  0.563 / 6.690  0.772 / 25.525  1.872 / 9.872 CAE [31] 8.327 / 21.623  1.804 / 5.281  0.382 / 14.175  0.960 / 17.627  0.643 / 3.240  0.342 / 6.051  0.493 / 7.314  0.642 / 7.861  1.699 / 10.397 OAVC [6] 6.99 / 16.1  0.60 / 2.55  0.27 / 3.94  1.05 / 12.4  0.36 / 1.78  1.29 / 11.7  0.44 / 6.01  0.84 / 14.4  1.46 / 8.61 SupervisedEPINet-5\\times5 [24] 6.140 / 12.462  0.200 / 0.549  0.163 / 1.242  0.818 / 4.763  0.775 / 4.516  4.884 / 46.022  2.170 / 19.670  16.121 / 48.952  3.884 / 17.272 EPINet-9\\times9 [24] 6.054 / 11.891  0.230 / 0.492  0.181 / 1.222  0.794 / 4.660  0.392 / 1.543  0.634 / 34.772  1.338 / 10.770  6.126 / 35.564  1.969 / 12.614 LFattNet [27] 4.090 / 9.841  0.210 / 0.252  0.080 / 0.752  0.502 / 2.591  0.324 / 2.028  6.060 / 34.233  0.782 / 10.757  4.900 / 34.812  2.164 / 11.909 UnsupervisedUnsup [22] 11.356 / 45.126  6.464 / 30.179  1.893 / 29.664  4.550 / 26.893  1.269 / 12.372  1.362 / 30.894  1.683 / 19.294  3.724 / 27.933  4.038 / 15.455 Mono [34] 9.749 / 22.884  1.081 / 3.808  0.657 / 5.402  2.795 / 10.947  - / -  - / -  - / -  - / -  - / - OccUnNet [13] 7.45 / 26.24  0.80 / 8.46  0.63 / 8.25  1.79 / 14.20  0.34 / 4.11  - / -  0.57 / 10.57  1.11 / 36.36  - / - Ours fast 5.520 / 17.173  0.546 / 2.331  0.428 / 3.952  1.141 / 9.691  0.312 / 2.819  1.523 / 20.628  0.428 / 7.709  0.626 / 18.94  1.315 / 10.405 Ours 4.928 / 14.622  0.431 / 1.728  0.320 / 3.634  1.010 / 8.426  0.323 / 2.322  1.058 / 12.44  0.367 / 6.732  0.581 / 12.145  1.127 / 7.756 </p>", "caption": "Table 1: Quantitative comparisons with other state-of-the-art methods on the 4D Light Field Benchmark [9] and HCI Blender [30]", "list_citation_info": ["[24] Shin, C., Jeon, H.G., Yoon, Y., Kweon, I.S., Kim, S.J.: Epinet: A fully-convolutional neural network using epipolar geometry for depth from light field images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4748\u20134757 (2018)", "[34] Zhou, W., Zhou, E., Liu, G., Lin, L., Lumsdaine, A.: Unsupervised monocular depth estimation from light field image. IEEE Transactions on Image Processing 29, 1606\u20131617 (2020)", "[30] Wanner, S., Meister, S., Goldl\u00fccke, B.: Datasets and Benchmarks for Densely Sampled 4D Light Fields. Datasets and Benchmarks for Densely Sampled 4D Light Fields (2013)", "[6] Han, K., Xiang, W., Wang, E., Huang, T.: A novel occlusion-aware vote cost for light field depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)", "[22] Peng, J., Xiong, Z., Liu, D., Chen, X.: Unsupervised depth estimation from light field using a convolutional neural network. In: 2018 International Conference on 3D Vision (3DV). pp. 295\u2013303. IEEE (2018)", "[9] Honauer, K., Johannsen, O., Kondermann, D., Goldluecke, B.: A dataset and evaluation methodology for depth estimation on 4d light fields. In: Asian Conference on Computer Vision (2016)", "[13] Jin, J., Hou, J.: Occlusion-aware unsupervised learning of depth from 4-d light fields (2021)", "[31] Williem, W., Park, I.K.: Robust light field depth estimation for noisy scene with occlusion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016)", "[12] Jeon, H.G., Park, J., Choe, G., Park, J., Bok, Y., Tai, Y.W., So Kweon, I.: Accurate depth map estimation from a lenslet light field camera. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1547\u20131555 (2015)", "[27] Tsai, Y.J., Liu, Y.L., Ouhyoung, M., Chuang, Y.Y.: Attention-based view selection networks for light-field disparity estimation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 12095\u201312103 (2020)", "[33] Zhang, S., Sheng, H., Li, C., Zhang, J., Xiong, Z.: Robust depth estimation for light field via spinning parallelogram operator. Computer Vision and Image Understanding 145, 148\u2013159 (2016)"]}, {"table": "<table><tr><td>Methods</td><td> Runtime/s </td><td> Parameters/M </td><td> MSE\\times100 </td><td> BadPix(0.07) </td></tr><tr><td> ACC [12] </td><td> 628.36 </td><td> - </td><td> 4.928 </td><td> 17.144 </td></tr><tr><td> SPO [33] </td><td> 380.64 </td><td> - </td><td> 1.872 </td><td> 9.872 </td></tr><tr><td> CAE [31] </td><td> 529.13 </td><td> - </td><td> 1.699 </td><td> 10.397 </td></tr><tr><td> OAVC [6] </td><td> 0.19 </td><td> - </td><td> 1.46 </td><td> 8.61 </td></tr><tr><td> EPINet-5\\times5 [24] </td><td> 2.14 </td><td> 5.113 </td><td> 3.884 </td><td> 17.272 </td></tr><tr><td> EPINet-9\\times9 [24] </td><td> 2.24 </td><td> 5.118 </td><td> 1.969 </td><td> 12.614 </td></tr><tr><td> LFattNet [27] </td><td> 6.28 </td><td> 5.058 </td><td> 2.164 </td><td> 11.909 </td></tr><tr><td> Unsup [22] </td><td> 5.73 </td><td> - </td><td> 4.038 </td><td> 15.455 </td></tr><tr><td> Ours fast </td><td> 0.098 </td><td> 0.774 </td><td> 1.315 </td><td> 10.405 </td></tr><tr><td> Ours </td><td> 0.46 </td><td> 1.047 </td><td> 1.127 </td><td> 7.756 </td></tr></table>", "caption": "Table 2: Comprehensive comparisons of efficiency via running time, parameters, MSE{\\times 100} and BadPix(0.07) on synthetic datasets", "list_citation_info": ["[24] Shin, C., Jeon, H.G., Yoon, Y., Kweon, I.S., Kim, S.J.: Epinet: A fully-convolutional neural network using epipolar geometry for depth from light field images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4748\u20134757 (2018)", "[6] Han, K., Xiang, W., Wang, E., Huang, T.: A novel occlusion-aware vote cost for light field depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)", "[22] Peng, J., Xiong, Z., Liu, D., Chen, X.: Unsupervised depth estimation from light field using a convolutional neural network. In: 2018 International Conference on 3D Vision (3DV). pp. 295\u2013303. IEEE (2018)", "[31] Williem, W., Park, I.K.: Robust light field depth estimation for noisy scene with occlusion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016)", "[12] Jeon, H.G., Park, J., Choe, G., Park, J., Bok, Y., Tai, Y.W., So Kweon, I.: Accurate depth map estimation from a lenslet light field camera. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1547\u20131555 (2015)", "[27] Tsai, Y.J., Liu, Y.L., Ouhyoung, M., Chuang, Y.Y.: Attention-based view selection networks for light-field disparity estimation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 12095\u201312103 (2020)", "[33] Zhang, S., Sheng, H., Li, C., Zhang, J., Xiong, Z.: Robust depth estimation for light field via spinning parallelogram operator. Computer Vision and Image Understanding 145, 148\u2013159 (2016)"]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td colspan=\"5\">MSE\\times100</td></tr><tr><td> Boxes </td><td> Cotton </td><td> Dino </td><td>Sideboard</td><td> Average </td></tr><tr><td>Ours fast w/o OPAL</td><td> 8.368 </td><td> 2.834 </td><td> 1.027 </td><td> 3.084 </td><td> 3.828 </td></tr><tr><td>Ours fast (\\beta=1)</td><td> 5.562 </td><td> 0.841 </td><td> 0.509 </td><td> 1.000 </td><td> 1.978 </td></tr><tr><td>Ours fast (\\beta=2)</td><td> 5.467 </td><td> 1.109 </td><td> 0.509 </td><td> 1.225 </td><td> 2.007 </td></tr><tr><td>Ours fast (\\beta=4)</td><td> 5.520 </td><td> 0.546 </td><td> 0.428 </td><td> 1.141 </td><td> 1.908 </td></tr><tr><td>Ours w/o refinement</td><td> 5.297 </td><td> 0.510 </td><td> 0.394 </td><td> 1.080 </td><td> 1.820 </td></tr><tr><td>Ours w/o EPI-Trans</td><td> 5.371 </td><td> 0.462 </td><td> 0.341 </td><td> 1.06 </td><td> 1.808 </td></tr><tr><td>Ours w/o \\ell_{D_{raw}}</td><td> 5.133 </td><td> 0.431 </td><td> 0.335 </td><td> 1.12 </td><td> 1.755 </td></tr><tr><td>Ours w/o \\ell_{smooth}</td><td> 6.775 </td><td> 0.990 </td><td> 0.612 </td><td> 1.553 </td><td> 2.482 </td></tr><tr><td>Ours</td><td> 4.928 </td><td> 0.431 </td><td> 0.320 </td><td> 1.010 </td><td> 1.672 </td></tr></table>", "caption": "Table 3: Ablation study on OPAL, down-sampling factor \\beta, loss items and our network\u2019s componments on 4D Light Field Benchmark [9]", "list_citation_info": ["[9] Honauer, K., Johannsen, O., Kondermann, D., Goldluecke, B.: A dataset and evaluation methodology for depth estimation on 4d light fields. In: Asian Conference on Computer Vision (2016)"]}], "citation_info_to_title": {"[22] Peng, J., Xiong, Z., Liu, D., Chen, X.: Unsupervised depth estimation from light field using a convolutional neural network. In: 2018 International Conference on 3D Vision (3DV). pp. 295\u2013303. IEEE (2018)": "Unsupervised depth estimation from light field using a convolutional neural network", "[9] Honauer, K., Johannsen, O., Kondermann, D., Goldluecke, B.: A dataset and evaluation methodology for depth estimation on 4d light fields. In: Asian Conference on Computer Vision (2016)": "A dataset and evaluation methodology for depth estimation on 4d light fields", "[30] Wanner, S., Meister, S., Goldl\u00fccke, B.: Datasets and Benchmarks for Densely Sampled 4D Light Fields. Datasets and Benchmarks for Densely Sampled 4D Light Fields (2013)": "Datasets and Benchmarks for Densely Sampled 4D Light Fields", "[13] Jin, J., Hou, J.: Occlusion-aware unsupervised learning of depth from 4-d light fields (2021)": "Occlusion-aware unsupervised learning of depth from 4-d light fields", "[31] Williem, W., Park, I.K.: Robust light field depth estimation for noisy scene with occlusion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016)": "Robust light field depth estimation for noisy scene with occlusion", "[33] Zhang, S., Sheng, H., Li, C., Zhang, J., Xiong, Z.: Robust depth estimation for light field via spinning parallelogram operator. Computer Vision and Image Understanding 145, 148\u2013159 (2016)": "Robust depth estimation for light field via spinning parallelogram operator", "[12] Jeon, H.G., Park, J., Choe, G., Park, J., Bok, Y., Tai, Y.W., So Kweon, I.: Accurate depth map estimation from a lenslet light field camera. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1547\u20131555 (2015)": "Accurate depth map estimation from a lenslet light field camera", "[27] Tsai, Y.J., Liu, Y.L., Ouhyoung, M., Chuang, Y.Y.: Attention-based view selection networks for light-field disparity estimation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 12095\u201312103 (2020)": "Attention-based view selection networks for light-field disparity estimation", "[6] Han, K., Xiang, W., Wang, E., Huang, T.: A novel occlusion-aware vote cost for light field depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)": "A novel occlusion-aware vote cost for light field depth estimation", "[24] Shin, C., Jeon, H.G., Yoon, Y., Kweon, I.S., Kim, S.J.: Epinet: A fully-convolutional neural network using epipolar geometry for depth from light field images. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4748\u20134757 (2018)": "Epinet: A fully-convolutional neural network using epipolar geometry for depth from light field images", "[34] Zhou, W., Zhou, E., Liu, G., Lin, L., Lumsdaine, A.: Unsupervised monocular depth estimation from light field image. IEEE Transactions on Image Processing 29, 1606\u20131617 (2020)": "Unsupervised Monocular Depth Estimation from Light Field Image"}, "source_title_to_arxiv_id": {"Occlusion-aware unsupervised learning of depth from 4-d light fields": "2106.03043"}}