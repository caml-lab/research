{"title": "Video Question Answering: Datasets, Algorithms and Challenges", "abstract": "Video Question Answering (VideoQA) aims to answer natural language questions\naccording to the given videos. It has earned increasing attention with recent\nresearch trends in joint vision and language understanding. Yet, compared with\nImageQA, VideoQA is largely underexplored and progresses slowly. Although\ndifferent algorithms have continually been proposed and shown success on\ndifferent VideoQA datasets, we find that there lacks a meaningful survey to\ncategorize them, which seriously impedes its advancements. This paper thus\nprovides a clear taxonomy and comprehensive analyses to VideoQA, focusing on\nthe datasets, algorithms, and unique challenges. We then point out the research\ntrend of studying beyond factoid QA to inference QA towards the cognition of\nvideo contents, Finally, we conclude some promising directions for future\nexploration.", "authors": ["Yaoyao Zhong", "Junbin Xiao", "Wei Ji", "Yicong Li", "Weihong Deng", "Tat-Seng Chua"], "published_date": "2022_03_02", "pdf_url": "http://arxiv.org/pdf/2203.01225v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><th rowspan=\"2\">Techniques &amp; Insights</th><th colspan=\"2\">Encoder</th><td rowspan=\"2\">Pre-trainingDataset</td><td rowspan=\"2\">TGIF-QA(Frame-QA)</td><td rowspan=\"2\">MSVD-QA</td><td rowspan=\"2\">MSRVTT-QA</td></tr><tr><th>Video</th><th>Text</th></tr><tr><th>STVQA(Jang et al., 2019)</th><th>Att</th><th>RN, Flow</th><th>GV</th><td>/</td><td>52.0</td><td>/</td><td>/</td></tr><tr><th>PSAC(Li et al., 2019)</th><th>Att</th><th>RN</th><th>GV</th><td>/</td><td>55.7</td><td>/</td><td>/</td></tr><tr><th>QueST(Jiang et al., 2020)</th><th>Att</th><th>RN, C3D</th><th>GV</th><td>/</td><td>59.7</td><td>36.1</td><td>34.6</td></tr><tr><th>CoMem(Gao et al., 2018)</th><th>Mem</th><th>RN, Flow</th><th>GV</th><td>/</td><td>51.5</td><td>/</td><td>/</td></tr><tr><th>HME(Fan et al., 2019)</th><th>Mem</th><th>RN, VGG, C3D</th><th>GV</th><td>/</td><td>53.8</td><td>33.7</td><td>33.0</td></tr><tr><th>LGCN(Huang et al., 2020)</th><th>GNN</th><th>RN, RoI</th><th>GV</th><td>/</td><td>56.3</td><td>34.3</td><td>/</td></tr><tr><th>HGA(Jiang and Han, 2020)</th><th>GNN</th><th>RN, VGG, C3D</th><th>GV</th><td>/</td><td>55.1</td><td>34.7</td><td>35.5</td></tr><tr><th>B2A(Park et al., 2021)</th><th>GNN, MG</th><th>RN, RX(3D)</th><th>GV</th><td>/</td><td>57.5</td><td>37.2</td><td>36.9</td></tr><tr><th>HAIR(Liu et al., 2021a)</th><th>GNN, Mem, HL</th><th>RoI</th><th>GV</th><td>/</td><td>60.2</td><td>37.5</td><td>36.9</td></tr><tr><th>MASN(Seo et al., 2021a)</th><th>GNN</th><th>RN, I3D, RoI</th><th>GV</th><td></td><td>59.5</td><td>38.0</td><td>35.2</td></tr><tr><th>DualVGR(Wang et al., 2021)</th><th>GNN</th><th>RN, RX(3D)</th><th>GV</th><td>/</td><td>/</td><td>39.0</td><td>35.5</td></tr><tr><th>PGAT(Peng et al., 2021)</th><th>GNN, MG, HL</th><th>RN, RX(3D), RoI</th><th>GV</th><td>/</td><td>61.1</td><td>39.0</td><td>38.1</td></tr><tr><th>HCRN(Le et al., 2020)</th><th>MN, HL</th><th>RN, RX(3D)</th><th>GV</th><td>/</td><td>55.9</td><td>36.1</td><td>35.6</td></tr><tr><th>HOSTR(Dang et al., 2021)</th><th>MN, GNN, HL</th><th>RN, RX(3D), RoI</th><th>GV</th><td>/</td><td>58.2</td><td>39.4</td><td>35.9</td></tr><tr><th>HQGA(Xiao et al., 2022a)</th><th>MN, GNN, HL, MG</th><th>RN, RX(3D), RoI</th><th>BT</th><td>/</td><td>61.3</td><td>41.2</td><td>38.6</td></tr><tr><th>MHN(Peng et al., 2022)</th><th>TF, HL, MG</th><th>RN, RX(3D)</th><th>GV</th><td>/</td><td>58.1</td><td>40.4</td><td>38.6</td></tr><tr><th>VGT(Xiao et al., 2022b)</th><th>TF, GNN</th><th>RN, RoI</th><th>BT</th><td>/</td><td>61.6</td><td>/</td><td>39.7</td></tr><tr><th>ClipBERT(Lei et al., 2021)</th><th>TF, CM-PF</th><th>RN (E2E)</th><th>BT</th><td>VG&amp;COCO</td><td>60.3</td><td>/</td><td>37.4</td></tr><tr><th>CoMVT(Seo et al., 2021b)</th><th>TF, CM-PF</th><th>S3D</th><th>BT</th><td>HowTo100M</td><td>/</td><td>42.6</td><td>39.5</td></tr><tr><th>VQA-T(Yang et al., 2021a)</th><th>TF, CM-PF</th><th>S3D</th><th>BT</th><td>H2VQA69M</td><td>/</td><td>46.3</td><td>41.5</td></tr><tr><th>SiaSRea(Yu et al., 2021)</th><th>TF, GNN, CM-PF</th><th>RN (E2E)</th><th>BT</th><td>VG&amp;COCO</td><td>60.2</td><td>45.5</td><td>41.6</td></tr><tr><th>MERLOT(Zellers et al., 2021)</th><th>TF, CM-PF</th><th>ViT(E2E)</th><th>BT</th><td>YT-T &amp; CC</td><td>69.5</td><td>/</td><td>43.1</td></tr><tr><th>VIOLET(Fu et al., 2021)</th><th>TF, CM-PF</th><th>VSwin (E2E)</th><th>BT</th><td>Web&amp;YT-T&amp;CC</td><td>68.9</td><td>47.9</td><td>43.9</td></tr></tbody></table>", "caption": "Table 2: Performance on Factoid VideoQA tasks. (Att: Attention, MG: Multi-Granularity, HL: Hierarchical Learning, CM-PF: Cross-modal Pre-training and Fine-tuning, Mem: Memory, GNN: Graph Neural Networks, MN: Modular Networks, TF: Transformer. RN: ResNet at frame-level, RX(3D): 3D ResNeXt at clip-level, RoI: Region-of-interest features from Faster R-CNN, GV: GloVe, BT: BERT, VG: Visual Genome (Krishna et al., 2017), YT-T: Youtube-Temporal-180M (Zellers et al., 2021), Web: WebVid2M (Bain et al., 2021), CC: Conceptual Captions-3M (Sharma et al., 2018). ViT (Dosovitskiy et al., 2020)and VSwin (Liu et al., 2021b)are Transformer-style visual encoders. Attention is found in all methods, but we omit it for those methods that do not emphasize attention.) ", "list_citation_info": ["Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations.", "Peng et al. (2022) Min Peng, Chongyang Wang, Yuan Gao, Yu Shi, and Xiang-Dong Zhou. 2022. Multilevel hierarchical network with multiscale sampling for video question answering. IJCAI.", "Jiang et al. (2020) Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and Yue Gao. 2020. Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11101\u201311108.", "Seo et al. (2021a) Ahjeong Seo, Gi-Cheon Kang, Joonhan Park, and Byoung-Tak Zhang. 2021a. Attend what you need: Motion-appearance synergistic networks for video question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6167\u20136177.", "Wang et al. (2021) Jianyu Wang, Bingkun Bao, and Changsheng Xu. 2021. Dualvgr: A dual-visual graph reasoning unit for video question answering. IEEE Transactions on Multimedia.", "Fan et al. (2019) Chenyou Fan, Xiaofan Zhang, Shu Zhang, et al. 2019. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1999\u20132007.", "Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373.", "Gao et al. (2018) Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. 2018. Motion-appearance co-memory networks for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6576\u20136585.", "Jang et al. (2019) Yunseok Jang, Yale Song, Chris Dongjoo Kim, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2019. Video question answering with spatio-temporal reasoning. International Journal of Computer Vision, 127(10):1385\u20131412.", "Xiao et al. (2022a) Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. 2022a. Video as conditional graph hierarchy for multi-granular question answering. In Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI), pages 2804\u20132812.", "Park et al. (2021) Jungin Park, Jiyoung Lee, and Kwanghoon Sohn. 2021. Bridge to answer: Structure-aware graph interaction network for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15526\u201315535.", "Le et al. (2020) Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9972\u20139981.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738.", "Yang et al. (2021a) Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2021a. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697.", "Huang et al. (2020) Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. 2020. Location-aware graph convolutional networks for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11021\u201311028.", "Liu et al. (2021a) Fei Liu, Jing Liu, Weining Wang, and Hanqing Lu. 2021a. Hair: Hierarchical visual-semantic relational reasoning for video question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1698\u20131707.", "Dang et al. (2021) Long Hoang Dang, Thao Minh Le, Vuong Le, and Truyen Tran. 2021. Hierarchical object-oriented spatio-temporal reasoning for video question answering. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pages 636\u2013642.", "Liu et al. (2021b) Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. 2021b. Video swin transformer. arXiv preprint arXiv:2106.13230.", "Li et al. (2019) Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, et al. 2019. Beyond rnns: Positional self-attention with co-attention for video question answering. In AAAI, pages 8658\u20138665.", "Seo et al. (2021b) Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid. 2021b. Look before you speak: Visually contextualized utterances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16877\u201316887.", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341.", "Fu et al. (2021) Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, et al. 2021. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv:2111.12681.", "Zellers et al. (2021) Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34.", "Yu et al. (2021) Weijiang Yu, Haoteng Zheng, Mengfei Li, Lei Ji, Lijun Wu, Nong Xiao, and Nan Duan. 2021. Learning from inside: Self-driven siamese sampling and reasoning for video question answering. Advances in Neural Information Processing Systems, 34.", "Peng et al. (2021) Liang Peng, Shuangji Yang, Yi Bin, and Guoqing Wang. 2021. Progressive graph attention network for video question answering. In Proceedings of the 29th ACM International Conference on Multimedia, pages 2871\u20132879.", "Jiang and Han (2020) Pin Jiang and Yahong Han. 2020. Reasoning with heterogeneous graph alignment for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11109\u201311116.", "Xiao et al. (2022b) Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. 2022b. Video graph transformer for video question answering. In European Conference on Computer Vision, pages 39\u201358. Springer.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Techniques &amp; Insights</td><td colspan=\"2\">NExT-QA</td><td colspan=\"3\">TGIF-QA</td></tr><tr><td>Val.</td><td>Test</td><td>Act</td><td>Tran.</td><td>Cnt</td></tr><tr><td>STVQA(Jang et al., 2017)</td><td>Att</td><td>47.9</td><td>47.6</td><td>62.9</td><td>69.4</td><td>4.22</td></tr><tr><td>CoMem(Gao et al., 2018)</td><td>Mem</td><td>48.0</td><td>48.5</td><td>68.2</td><td>74.3</td><td>4.10</td></tr><tr><td>HME(Fan et al., 2019)</td><td>Mem</td><td>48.7</td><td>49.2</td><td>73.9</td><td>77.8</td><td>4.02</td></tr><tr><td>HCRN(Le et al., 2020)</td><td>MN, HL</td><td>48.2</td><td>48.9</td><td>75.0</td><td>81.4</td><td>3.82</td></tr><tr><td>HGA(Jiang and Han, 2020)</td><td>GNN, HL</td><td>49.7</td><td>50.0</td><td>75.4</td><td>81.0</td><td>4.09</td></tr><tr><td>MASN(Seo et al., 2021a)</td><td>GNN</td><td>/</td><td>/</td><td>84.4</td><td>87.4</td><td>3.75</td></tr><tr><td>MHN(Peng et al., 2022)</td><td>TF, HL, MG</td><td>/</td><td>/</td><td>83.5</td><td>90.2</td><td>3.57</td></tr><tr><td>IGV(Li et al., 2022d)</td><td>GNN, Causal</td><td>51.0</td><td>51.3</td><td>/</td><td>/</td><td>/</td></tr><tr><td>HQGA(Xiao et al., 2022a)</td><td>MN, GNN, HL, MG</td><td>51.4</td><td>51.8</td><td>76.9</td><td>85.6</td><td>/</td></tr><tr><td>P3D-G(Cherian et al., 2022)</td><td>GNN, TF, HL</td><td>53.4</td><td>/</td><td>/</td><td>/</td><td>/</td></tr><tr><td>ATP(Buch et al., 2022)</td><td>TF</td><td>54.3</td><td>/</td><td>/</td><td>/</td><td>/</td></tr><tr><td>VGT(Xiao et al., 2022b)</td><td>TF,GNN</td><td>55.0</td><td>53.7</td><td>95.0</td><td>97.6</td><td>/</td></tr><tr><td>ClipBERT(Lei et al., 2021)</td><td>TF, CM-PF</td><td>/</td><td>/</td><td>82.8</td><td>87.8</td><td>/</td></tr><tr><td>SiasRea(Yu et al., 2021)</td><td>TF, CM-PF, GNN</td><td>/</td><td>/</td><td>79.7</td><td>85.3</td><td>/</td></tr><tr><td>MERLOT(Zellers et al., 2021)</td><td>TF, CM-PF</td><td>/</td><td>/</td><td>94.0</td><td>96.2</td><td>/</td></tr><tr><td>VIOLET(Fu et al., 2021)</td><td>TF, CM-PF</td><td>/</td><td>/</td><td>92.5</td><td>95.7</td><td>/</td></tr><tr><td>Human</td><td>/</td><td>88.4</td><td>/</td><td>/</td><td>/</td><td>/</td></tr></tbody></table>", "caption": "Table 3: Performance on Inference VideoQA tasks. For the counting (Cnt) task in TGIF-QA, value of mean square error (MSE) is reported for evaluation.", "list_citation_info": ["Peng et al. (2022) Min Peng, Chongyang Wang, Yuan Gao, Yu Shi, and Xiang-Dong Zhou. 2022. Multilevel hierarchical network with multiscale sampling for video question answering. IJCAI.", "Jang et al. (2017) Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766.", "Buch et al. (2022) Shyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. 2022. Revisiting the\" video\" in video-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2917\u20132927.", "Le et al. (2020) Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9972\u20139981.", "Seo et al. (2021a) Ahjeong Seo, Gi-Cheon Kang, Joonhan Park, and Byoung-Tak Zhang. 2021a. Attend what you need: Motion-appearance synergistic networks for video question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6167\u20136177.", "Jiang and Han (2020) Pin Jiang and Yahong Han. 2020. Reasoning with heterogeneous graph alignment for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11109\u201311116.", "Xiao et al. (2022b) Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. 2022b. Video graph transformer for video question answering. In European Conference on Computer Vision, pages 39\u201358. Springer.", "Fan et al. (2019) Chenyou Fan, Xiaofan Zhang, Shu Zhang, et al. 2019. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1999\u20132007.", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341.", "Cherian et al. (2022) Anoop Cherian, Chiori Hori, Tim K Marks, and Jonathan Le Roux. 2022. (2.5+ 1) d spatio-temporal scene graphs for video question answering. In AAAI.", "Li et al. (2022d) Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. 2022d. Invariant grounding for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2928\u20132937.", "Fu et al. (2021) Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, et al. 2021. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv:2111.12681.", "Zellers et al. (2021) Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34.", "Gao et al. (2018) Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. 2018. Motion-appearance co-memory networks for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6576\u20136585.", "Yu et al. (2021) Weijiang Yu, Haoteng Zheng, Mengfei Li, Lei Ji, Lijun Wu, Nong Xiao, and Nan Duan. 2021. Learning from inside: Self-driven siamese sampling and reasoning for video question answering. Advances in Neural Information Processing Systems, 34.", "Xiao et al. (2022a) Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. 2022a. Video as conditional graph hierarchy for multi-granular question answering. In Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI), pages 2804\u20132812."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><th rowspan=\"2\">Techniques &amp;Insights</th><td colspan=\"2\">TVQA</td><td rowspan=\"2\">TVQA+</td><td rowspan=\"2\">KnowITVQA</td></tr><tr><td>w/o ts</td><td>w/ ts</td></tr><tr><th>PAMN(Kim et al., 2019)</th><th>Mem</th><td>66.8</td><td>/</td><td>/</td><td>/</td></tr><tr><th>STAGE(Lei et al., 2020)</th><th>Att</th><td>70.2</td><td>/</td><td>74.8</td><td>/</td></tr><tr><th>HCRN(Le et al., 2021)</th><th>MN, HL</th><td>66.1</td><td>71.3</td><td>/</td><td>/</td></tr><tr><th>MSAN(Kim et al., 2020)</th><th>Att</th><td>/</td><td>71.1</td><td>/</td><td>/</td></tr><tr><th>BERT-VQA(Kenton and Toutanova, 2019)</th><th>TF</th><td>/</td><td>73.6</td><td>/</td><td>/</td></tr><tr><th>MMFT-BERT(Urooj et al., 2020)</th><th>TF</th><td>/</td><td>72.9</td><td>/</td><td>/</td></tr><tr><th>ROLL(Garcia and Nakashima, 2020)</th><th>TF</th><td>/</td><td>/</td><td>69.6</td><td>71.5</td></tr><tr><th>RHA(Li et al., 2021a)</th><th>GNN, HL</th><td>/</td><td>/</td><td>73.4</td><td>/</td></tr><tr><th>SPCR(Kim et al., 2021b)</th><th>Att</th><td>/</td><td>76.2</td><td>76.2</td><td>/</td></tr><tr><th>V2T(Engin et al., 2021)</th><th>TF</th><td>/</td><td>/</td><td>/</td><td>78.1</td></tr><tr><th>MERLOT(Zellers et al., 2021)</th><th>TF, CM-PF</th><td>/</td><td>78.7</td><td>80.9</td><td>/</td></tr><tr><th>Human</th><th>/</th><td>89.4</td><td>91.5</td><td>90.5</td><td>/</td></tr></tbody></table>", "caption": "Table 4: Performance on MM VideoQA and KB VideoQA tasks. For TVQA, we report results on test-public data split. (ts: Timestamp Annotation.) ", "list_citation_info": ["Lei et al. (2020) Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. 2020. Tvqa+: Spatio-temporal grounding for video question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8211\u20138225.", "Urooj et al. (2020) Aisha Urooj, Amir Mazaheri, Mubarak Shah, et al. 2020. Mmft-bert: Multimodal fusion transformer with bert encodings for visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4648\u20134660.", "Engin et al. (2021) Deniz Engin, Fran\u00e7ois Schnitzler, Ngoc QK Duong, and Yannis Avrithis. 2021. On the hidden treasure of dialog in video question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2064\u20132073.", "Kim et al. (2020) Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, and Chang D Yoo. 2020. Modality shifting attention network for multi-modal video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106\u201310115.", "Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186.", "Le et al. (2021) Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2021. Hierarchical conditional relation networks for multimodal video question answering. International Journal of Computer Vision, 129(11):3027\u20133050.", "Kim et al. (2021b) Seonhoon Kim, Seohyeong Jeong, Eunbyul Kim, Inho Kang, and Nojun Kwak. 2021b. Self-supervised pre-training and contrastive representation learning for multiple-choice video qa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13171\u201313179.", "Zellers et al. (2021) Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34.", "Li et al. (2021a) Fangtao Li, Ting Bai, Chenyu Cao, Zihe Liu, Chenghao Yan, and Bin Wu. 2021a. Relation-aware hierarchical attention framework for video question answering. In Proceedings of the 2021 International Conference on Multimedia Retrieval, pages 164\u2013172.", "Garcia and Nakashima (2020) Noa Garcia and Yuta Nakashima. 2020. Knowledge-based video question answering with unsupervised scene descriptions. In European Conference on Computer Vision, pages 581\u2013598. Springer.", "Kim et al. (2019) Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, et al. 2019. Progressive attention memory network for movie story question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8337\u20138346."]}, {"table": "<table><tbody><tr><td>Dataset</td><td>MTax</td><td>QTax</td><td>Data Source</td><td>Goal</td><td>#Video/#QA</td><td>Annotation</td><td>Task</td></tr><tr><td>VideoQA(FiB) (Zhu et al., 2017)</td><td>Vid</td><td>F</td><td>Multiple source</td><td>Temporal reasoning</td><td>109K/390K</td><td>Auto</td><td>MC</td></tr><tr><td>VideoQA (Zeng et al., 2017)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Description</td><td>18K/174K</td><td>Auto, Man</td><td>OE</td></tr><tr><td>MSVD-QA (Xu et al., 2017)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Description</td><td>1.9K/50K</td><td>Auto</td><td>OE</td></tr><tr><td>MSRVTT-QA (Xu et al., 2017)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Description</td><td>10K/243K</td><td>Auto</td><td>OE</td></tr><tr><td>YouTube2Text-QA (Zhao et al., 2017a)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Description</td><td>1.9K/48K</td><td>Auto</td><td>MC, OE</td></tr><tr><td>MarioQA (Mun et al., 2017)</td><td>Vid</td><td>F</td><td>Game</td><td>Temporal reasoning</td><td>92K/92K</td><td>Auto</td><td>OE</td></tr><tr><td>ActivityNet-QA (Yu et al., 2019)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Description</td><td>5.8K/58K</td><td>Man</td><td>OE</td></tr><tr><td>EgoVQA (Fan, 2019)</td><td>Vid</td><td>F</td><td>Egocentric videos</td><td>First-person VideoQA</td><td>520/580</td><td>Man</td><td>MC</td></tr><tr><td>HowToVQA69M (Yang et al., 2021a)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Pre-training for downstream tasks</td><td>69M/69M</td><td>Auto</td><td>OE</td></tr><tr><td>iVQA (Yang et al., 2021a)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Removing language bias</td><td>10K/10K</td><td>Man</td><td>OE</td></tr><tr><td>ASRL-QA (Sadhu et al., 2021)</td><td>Vid</td><td>F</td><td>Internet videos</td><td>VideoQA with phrases</td><td>35K/162K</td><td>Auto</td><td>OE</td></tr><tr><td>Charades-SRL-QA (Sadhu et al., 2021)</td><td>Vid</td><td>F</td><td>Crowd-Sourced</td><td>VideoQA with phrases</td><td>9.5K/71K</td><td>Auto</td><td>OE</td></tr><tr><td>WebVidVQA3M (Yang et al., 2022a)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Pre-training for downstream tasks</td><td>2M/3M</td><td>Auto</td><td>OE</td></tr><tr><td>FIBER (Castro et al., 2022b)</td><td>Vid</td><td>F</td><td>Web videos</td><td>Fill-in-the-blanks task with diverse answers</td><td>28K/28K</td><td>Man</td><td>OE</td></tr><tr><td>WildQA (Castro et al., 2022a)</td><td>Vid</td><td>F</td><td>In-the-wild videos</td><td>In-the-wild videos with evidence selection</td><td>369/916</td><td>Man</td><td>OE</td></tr><tr><td>MovieQA (Tapaswi et al., 2016)</td><td>MM</td><td>F</td><td>Movies</td><td>Text &amp; Visual story comprehension</td><td>6.7K/6.4K</td><td>Man</td><td>MC</td></tr><tr><td>MovieFIB (Maharaj et al., 2017)</td><td>MM</td><td>F</td><td>Movies</td><td>Description</td><td>118K/348K</td><td>Auto</td><td>OE</td></tr><tr><td>PororoQA (Kim et al., 2017)</td><td>MM</td><td>F</td><td>Cartoon</td><td>Story comprehension</td><td>171/8.9K</td><td>Man</td><td>MC</td></tr><tr><td>TVQA (Lei et al., 2018)</td><td>MM</td><td>F</td><td>TV shows</td><td>Subtitle &amp; Concept comprehension</td><td>21K/152K</td><td>Man</td><td>MC</td></tr><tr><td>TVQA+ (Lei et al., 2020)</td><td>MM</td><td>F</td><td>TV shows</td><td>Spatio-temporal VideoQA</td><td>4.1K/29K</td><td>Man</td><td>MC</td></tr><tr><td>LifeQA (Castro et al., 2020)</td><td>MM</td><td>F</td><td>Web videos</td><td>Real-life understanding</td><td>275/2.3K</td><td>Man</td><td>MC</td></tr><tr><td>How2QA (Li et al., 2020)</td><td>MM</td><td>F</td><td>Web videos</td><td>Multimodal challenges</td><td>22K/44K</td><td>Man</td><td>MC</td></tr><tr><td>Env-QA (Gao et al., 2021)</td><td>MM</td><td>F</td><td>Egocentric videos</td><td>Exploring &amp; interacting with environments</td><td>23K/85K</td><td>Auto, Man</td><td>OE</td></tr><tr><td>Pano-AVQA (Yun et al., 2021)</td><td>MM</td><td>F</td><td>360{}^{\\circ} videos</td><td>Spherical spatial &amp; audio-visual relation</td><td>5.4K/51.7K</td><td>Man</td><td>OE</td></tr><tr><td>DramaQA (Choi et al., 2021)</td><td>MM</td><td>F</td><td>TV shows</td><td>Story comprehension</td><td>23K/17K</td><td>Man</td><td>MC</td></tr><tr><td>MUSIC-AVQA (Li et al., 2022a)</td><td>MM</td><td>F</td><td>Musical videos</td><td>Audio-Visual VideoQA</td><td>9.3K/45K</td><td>Man</td><td>OE</td></tr><tr><td>TGIF-QA (Jang et al., 2017)</td><td>Vid</td><td>I</td><td>Animated GIF</td><td>Spatio-temporal reasoning</td><td>71K/165K</td><td>Auto, Man</td><td>MC, OE</td></tr><tr><td>SVQA (Song et al., 2018)</td><td>Vid</td><td>I</td><td>Synthetic videos</td><td>Logical compositional questions</td><td>12K/118K</td><td>Auto</td><td>OE</td></tr><tr><td>Social-IQ (Zadeh et al., 2019)</td><td>MM</td><td>I</td><td>Web videos</td><td>Measuring social intelligence</td><td>1.2K/7.5K</td><td>Man</td><td>MC</td></tr><tr><td>PsTuts-VQA (Zhao et al., 2020)</td><td>KB</td><td>I</td><td>Tutorial videos</td><td>Narrated instructional videos</td><td>76/17K</td><td>Man</td><td>MC</td></tr><tr><td>KnowIT VQA (Garcia et al., 2020)</td><td>KB</td><td>I</td><td>TV shows</td><td>Knowledge in VideoQA</td><td>12K/24K</td><td>Man</td><td>MC</td></tr><tr><td>KnowIT-X VQA (Wu et al., 2021b)</td><td>KB</td><td>I</td><td>TV shows</td><td>Transfer learning</td><td>12K/21K</td><td>Man</td><td>MC</td></tr><tr><td>NEWSKVQA (Gupta and Gupta, 2022)</td><td>KB</td><td>I</td><td>News videos</td><td>Knowledge-based QA of news videos</td><td>12K/1M</td><td>Auto</td><td>MC</td></tr><tr><td>V2C-QA (Fang et al., 2020)</td><td>Vid</td><td>I</td><td>Web videos</td><td>Commonsense reasoning</td><td>1.5K/37K</td><td>Auto</td><td>OE</td></tr><tr><td>TutorialVQA (Colas et al., 2020)</td><td>Vid</td><td>I</td><td>Tutorial videos</td><td>Multi-step &amp; non-factoid VideoQA</td><td>408/6.1K</td><td>Man</td><td>OE</td></tr><tr><td>CLEVRER (Yi et al., 2020)</td><td>Vid</td><td>I</td><td>Synthetic videos</td><td>Temporal &amp; causal structures</td><td>10K/305K</td><td>Auto</td><td>MC, OE</td></tr><tr><td>TGIF-QA-R (Peng et al., 2021)</td><td>Vid</td><td>I</td><td>Animated GIF</td><td>Overcoming answer biases</td><td>71K/165K</td><td>Auto</td><td>MC</td></tr><tr><td>SUTD-TrafficQA (Xu et al., 2021)</td><td>Vid</td><td>I</td><td>Traffic scenes</td><td>Understanding &amp; inference in traffic</td><td>10K/62K</td><td>Man</td><td>MC</td></tr><tr><td>AGQA(Grunde-McLaughlin et al., 2021)</td><td>Vid</td><td>I</td><td>Homemade videos</td><td>Compositional reasoning</td><td>9.6K/192M</td><td>Auto</td><td>OE</td></tr><tr><td>AGQA 2.0(Gandhi et al., 2022)</td><td>Vid</td><td>I</td><td>Homemade videos</td><td>Compositional consistency</td><td>9.6K/4.55M</td><td>Auto</td><td>OE</td></tr><tr><td>NExT-QA (Xiao et al., 2021)</td><td>Vid</td><td>I</td><td>Web videos</td><td>Causal &amp; temporal action interactions</td><td>5.4K/52K</td><td>Man</td><td>MC, OE</td></tr><tr><td>STAR (Wu et al., 2021a)</td><td>Vid</td><td>I</td><td>Homemade videos</td><td>Situated reasoning in real-world videos</td><td>22K/60K</td><td>Auto</td><td>MC</td></tr><tr><td>Causal-VidQA (Li et al., 2022b)</td><td>Vid</td><td>I</td><td>Web videos</td><td>Evidence &amp; commonsense reasoning</td><td>26K/107K</td><td>Man</td><td>MC</td></tr><tr><td>VQuAD (Gupta et al., 2022)</td><td>Vid</td><td>I</td><td>Synthetic videos</td><td>Spatio &amp; temporal reasoning</td><td>7K/1.3M</td><td>Auto</td><td>OE</td></tr></tbody></table>", "caption": "Table A1: VideoQA datasets in the literature. (MTax: Modality-based Taxonomy, QTax: Question-based Taxonomy, Vid: VideoQA, MM: Multi-modal VideoQA, KB: Knowledge-based VideoQA, F: Factoid VideoQA, I: Inference VideoQA, Auto: automatic generation, Man: manual annotation, MC: multi-choice QA, OE: open-ended QA.)", "list_citation_info": ["Grunde-McLaughlin et al. (2021) Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. 2021. Agqa: A benchmark for compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11287\u201311297.", "Maharaj et al. (2017) Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. 2017. A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6884\u20136893.", "Colas et al. (2020) Anthony Colas, Seokhwan Kim, Franck Dernoncourt, Siddhesh Gupte, Daisy Zhe Wang, and Doo Soon Kim. 2020. Tutorialvqa: Question answering dataset for tutorial videos. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5450\u20135455.", "Zeng et al. (2017) Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun. 2017. Leveraging video descriptions to learn video question answering. In Thirty-First AAAI Conference on Artificial Intelligence.", "Gao et al. (2021) Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. 2021. Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1675\u20131685.", "Kim et al. (2017) Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. 2017. Deepstory: video story qa by deep embedded memory networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 2016\u20132022.", "Lei et al. (2020) Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. 2020. Tvqa+: Spatio-temporal grounding for video question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8211\u20138225.", "Jang et al. (2017) Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766.", "Yang et al. (2021a) Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2021a. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697.", "Yang et al. (2022a) Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2022a. Learning to answer visual questions from web videos. IEEE Transactions on Pattern Analysis and Machine Intelligence.", "Xu et al. (2021) Li Xu, He Huang, and Jun Liu. 2021. Sutd-trafficqa: A question answering benchmark and an efficient network for video reasoning over traffic events. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9878\u20139888.", "Gupta et al. (2022) Vivek Gupta, Badri N Patro, Hemant Parihar, and Vinay P Namboodiri. 2022. Vquad: Video question answering diagnostic dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 282\u2013291.", "Sadhu et al. (2021) Arka Sadhu, Kan Chen, and Ram Nevatia. 2021. Video question answering with phrases via semantic roles. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2460\u20132478.", "Yu et al. (2019) Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9127\u20139134.", "Xu et al. (2017) Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653.", "Zhao et al. (2020) Wentian Zhao, Seokhwan Kim, Ning Xu, and Hailin Jin. 2020. Video question answering on screencast tutorials. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, pages 1061\u20131068.", "Fan (2019) Chenyou Fan. 2019. Egovqa-an egocentric video question answering benchmark dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0\u20130.", "Wu et al. (2021a) Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. 2021a. Star: A benchmark for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).", "Tapaswi et al. (2016) Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4631\u20134640.", "Zadeh et al. (2019) Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. 2019. Social-iq: A question answering benchmark for artificial social intelligence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8807\u20138817.", "Xiao et al. (2021) Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. 2021. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9777\u20139786.", "Li et al. (2020) Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020. Hero: Hierarchical encoder for video+ language omni-representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2046\u20132065.", "Zhao et al. (2017a) Zhou Zhao, Jinghao Lin, Xinghua Jiang, Deng Cai, Xiaofei He, and Yueting Zhuang. 2017a. Video question answering via hierarchical dual-level attention network learning. In Proceedings of the 25th ACM international conference on Multimedia, pages 1050\u20131058.", "Castro et al. (2020) Santiago Castro, Mahmoud Azab, Jonathan Stroud, Cristina Noujaim, Ruoyao Wang, Jia Deng, and Rada Mihalcea. 2020. Lifeqa: A real-life dataset for video question answering. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4352\u20134358.", "Choi et al. (2021) Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang, Minsu Lee, and Byoung-Tak Zhang. 2021. Dramaqa: Character-centered video story understanding with hierarchical qa. In AAAI, volume 35.", "Yun et al. (2021) Heeseung Yun, Youngjae Yu, Wonsuk Yang, Kangil Lee, and Gunhee Kim. 2021. Pano-avqa: Grounded audio-visual question answering on 360deg videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2031\u20132041.", "Song et al. (2018) Xiaomeng Song, Yucheng Shi, Xin Chen, and Yahong Han. 2018. Explore multi-step reasoning in video question answering. In Proceedings of the 26th ACM international conference on Multimedia, pages 239\u2013247.", "Peng et al. (2021) Liang Peng, Shuangji Yang, Yi Bin, and Guoqing Wang. 2021. Progressive graph attention network for video question answering. In Proceedings of the 29th ACM International Conference on Multimedia, pages 2871\u20132879.", "Castro et al. (2022b) Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Oana Ignat, Nan Liu, Jonathan Stroud, and Rada Mihalcea. 2022b. Fiber: Fill-in-the-blanks as a challenging video understanding evaluation framework. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2925\u20132940.", "Gandhi et al. (2022) Mona Gandhi, Mustafa Omer Gul, Eva Prakash, Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. 2022. Measuring compositional consistency for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.", "Li et al. (2022b) Jiangtong Li, Li Niu, and Liqing Zhang. 2022b. From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21273\u201321282.", "Gupta and Gupta (2022) Pranay Gupta and Manish Gupta. 2022. Newskvqa: Knowledge-aware news video question answering. arXiv preprint arXiv:2202.04015.", "Fang et al. (2020) Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, et al. 2020. Video2commonsense: Generating commonsense descriptions to enrich video captioning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 840\u2013860.", "Li et al. (2022a) Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. 2022a. Learning to answer questions in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19108\u201319118.", "Yi et al. (2020) Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, et al. 2020. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations.", "Zhu et al. (2017) Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexander G Hauptmann. 2017. Uncovering the temporal context for video question answering. International Journal of Computer Vision, 124(3):409\u2013421.", "Garcia et al. (2020) Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2020. Knowit vqa: Answering knowledge-based questions about videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10826\u201310834.", "Castro et al. (2022a) Santiago Castro, Naihao Deng, Pingxuan Huang, Mihai Burzo, and Rada Mihalcea. 2022a. In-the-wild video question answering. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5613\u20135635.", "Wu et al. (2021b) Tianran Wu, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima, and Haruo Takemura. 2021b. Transferring domain-agnostic knowledge in video question answering. In 32nd British Machine Vision Conference 2021, BMVC 2021, Online, November 22-25, 2021, page 301.", "Mun et al. (2017) Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and Bohyung Han. 2017. Marioqa: Answering questions by watching gameplay videos. In Proceedings of the IEEE International Conference on Computer Vision, pages 2867\u20132875.", "Lei et al. (2018) Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. 2018. Tvqa: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1369\u20131379."]}], "citation_info_to_title": {"Liu et al. (2021a) Fei Liu, Jing Liu, Weining Wang, and Hanqing Lu. 2021a. Hair: Hierarchical visual-semantic relational reasoning for video question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1698\u20131707.": "Hair: Hierarchical visual-semantic relational reasoning for video question answering", "Li et al. (2022a) Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen, and Di Hu. 2022a. Learning to answer questions in dynamic audio-visual scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19108\u201319118.": "Learning to answer questions in dynamic audio-visual scenarios", "Lei et al. (2020) Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. 2020. Tvqa+: Spatio-temporal grounding for video question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8211\u20138225.": "Tvqa+: Spatio-temporal grounding for video question answering", "Colas et al. (2020) Anthony Colas, Seokhwan Kim, Franck Dernoncourt, Siddhesh Gupte, Daisy Zhe Wang, and Doo Soon Kim. 2020. Tutorialvqa: Question answering dataset for tutorial videos. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5450\u20135455.": "Tutorialvqa: Question answering dataset for tutorial videos", "Mun et al. (2017) Jonghwan Mun, Paul Hongsuck Seo, Ilchae Jung, and Bohyung Han. 2017. Marioqa: Answering questions by watching gameplay videos. In Proceedings of the IEEE International Conference on Computer Vision, pages 2867\u20132875.": "Marioqa: Answering questions by watching gameplay videos", "Song et al. (2018) Xiaomeng Song, Yucheng Shi, Xin Chen, and Yahong Han. 2018. Explore multi-step reasoning in video question answering. In Proceedings of the 26th ACM international conference on Multimedia, pages 239\u2013247.": "Explore multi-step reasoning in video question answering", "Gandhi et al. (2022) Mona Gandhi, Mustafa Omer Gul, Eva Prakash, Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. 2022. Measuring compositional consistency for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.": "Measuring Compositional Consistency for Video Question Answering", "Tapaswi et al. (2016) Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4631\u20134640.": "Movieqa: Understanding stories in movies through question-answering", "Jang et al. (2017) Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766.": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738.": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "Kenton and Toutanova (2019) Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186.": "Bert: Pre-training of deep bidirectional transformers for language understanding", "Maharaj et al. (2017) Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. 2017. A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6884\u20136893.": "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering", "Yang et al. (2022a) Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2022a. Learning to answer visual questions from web videos. IEEE Transactions on Pattern Analysis and Machine Intelligence.": "Learning to answer visual questions from web videos", "Xiao et al. (2022b) Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. 2022b. Video graph transformer for video question answering. In European Conference on Computer Vision, pages 39\u201358. Springer.": "Video Graph Transformer for Video Question Answering", "Zhao et al. (2020) Wentian Zhao, Seokhwan Kim, Ning Xu, and Hailin Jin. 2020. Video question answering on screencast tutorials. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, pages 1061\u20131068.": "Video Question Answering on Screencast Tutorials", "Li et al. (2020) Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020. Hero: Hierarchical encoder for video+ language omni-representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2046\u20132065.": "Hero: Hierarchical Encoder for Video+ Language Omni-Representation Pre-Training", "Xiao et al. (2021) Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. 2021. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9777\u20139786.": "Next-qa: Next phase of question-answering to explaining temporal actions", "Garcia et al. (2020) Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2020. Knowit vqa: Answering knowledge-based questions about videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10826\u201310834.": "Knowit VQA: Answering Knowledge-Based Questions about Videos", "Park et al. (2021) Jungin Park, Jiyoung Lee, and Kwanghoon Sohn. 2021. Bridge to answer: Structure-aware graph interaction network for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15526\u201315535.": "Bridge to answer: Structure-aware graph interaction network for video question answering", "Castro et al. (2022a) Santiago Castro, Naihao Deng, Pingxuan Huang, Mihai Burzo, and Rada Mihalcea. 2022a. In-the-wild video question answering. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5613\u20135635.": "In-the-wild video question answering", "Urooj et al. (2020) Aisha Urooj, Amir Mazaheri, Mubarak Shah, et al. 2020. Mmft-bert: Multimodal fusion transformer with bert encodings for visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4648\u20134660.": "Mmft-bert: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering", "Zhao et al. (2017a) Zhou Zhao, Jinghao Lin, Xinghua Jiang, Deng Cai, Xiaofei He, and Yueting Zhuang. 2017a. Video question answering via hierarchical dual-level attention network learning. In Proceedings of the 25th ACM international conference on Multimedia, pages 1050\u20131058.": "Video question answering via hierarchical dual-level attention network learning", "Castro et al. (2020) Santiago Castro, Mahmoud Azab, Jonathan Stroud, Cristina Noujaim, Ruoyao Wang, Jia Deng, and Rada Mihalcea. 2020. Lifeqa: A real-life dataset for video question answering. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4352\u20134358.": "Lifeqa: A Real-Life Dataset for Video Question Answering", "Jang et al. (2019) Yunseok Jang, Yale Song, Chris Dongjoo Kim, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2019. Video question answering with spatio-temporal reasoning. International Journal of Computer Vision, 127(10):1385\u20131412.": "Video Question Answering with Spatio-Temporal Reasoning", "Grunde-McLaughlin et al. (2021) Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala. 2021. Agqa: A benchmark for compositional spatio-temporal reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11287\u201311297.": "Agqa: A Benchmark for Compositional Spatio-Temporal Reasoning", "Dang et al. (2021) Long Hoang Dang, Thao Minh Le, Vuong Le, and Truyen Tran. 2021. Hierarchical object-oriented spatio-temporal reasoning for video question answering. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pages 636\u2013642.": "Hierarchical Object-Oriented Spatio-Temporal Reasoning for Video Question Answering", "Fan (2019) Chenyou Fan. 2019. Egovqa-an egocentric video question answering benchmark dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0\u20130.": "Egovqa-an egocentric video question answering benchmark dataset", "Seo et al. (2021b) Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid. 2021b. Look before you speak: Visually contextualized utterances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16877\u201316887.": "Look before you speak: Visually contextualized utterances", "Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373.": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations", "Kim et al. (2019) Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, et al. 2019. Progressive attention memory network for movie story question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8337\u20138346.": "Progressive attention memory network for movie story question answering", "Fang et al. (2020) Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, et al. 2020. Video2commonsense: Generating commonsense descriptions to enrich video captioning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 840\u2013860.": "Video2commonsense: Generating commonsense descriptions to enrich video captioning", "Yun et al. (2021) Heeseung Yun, Youngjae Yu, Wonsuk Yang, Kangil Lee, and Gunhee Kim. 2021. Pano-avqa: Grounded audio-visual question answering on 360deg videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2031\u20132041.": "Pano-avqa: Grounded audio-visual question answering on 360deg videos", "Kim et al. (2020) Junyeong Kim, Minuk Ma, Trung Pham, Kyungsu Kim, and Chang D Yoo. 2020. Modality shifting attention network for multi-modal video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10106\u201310115.": "Modality Shifting Attention Network for Multi-Modal Video Question Answering", "Liu et al. (2021b) Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. 2021b. Video swin transformer. arXiv preprint arXiv:2106.13230.": "Video Swin Transformer", "Yu et al. (2021) Weijiang Yu, Haoteng Zheng, Mengfei Li, Lei Ji, Lijun Wu, Nong Xiao, and Nan Duan. 2021. Learning from inside: Self-driven siamese sampling and reasoning for video question answering. Advances in Neural Information Processing Systems, 34.": "Learning from inside: Self-driven siamese sampling and reasoning for video question answering", "Cherian et al. (2022) Anoop Cherian, Chiori Hori, Tim K Marks, and Jonathan Le Roux. 2022. (2.5+ 1) d spatio-temporal scene graphs for video question answering. In AAAI.": "Anoop Cherian et al (2022) - (25+ 1) d spatio-temporal scene graphs for video question answering", "Gao et al. (2021) Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. 2021. Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1675\u20131685.": "Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments", "Zellers et al. (2021) Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34.": "Merlot: Multimodal Neural Script Knowledge Models", "Garcia and Nakashima (2020) Noa Garcia and Yuta Nakashima. 2020. Knowledge-based video question answering with unsupervised scene descriptions. In European Conference on Computer Vision, pages 581\u2013598. Springer.": "Knowledge-based video question answering with unsupervised scene descriptions", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341.": "Less is more: Clipbert for video-and-language learning via sparse sampling", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565.": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "Yu et al. (2019) Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9127\u20139134.": "Activitynet-qa: A dataset for understanding complex web videos via question answering", "Xu et al. (2021) Li Xu, He Huang, and Jun Liu. 2021. Sutd-trafficqa: A question answering benchmark and an efficient network for video reasoning over traffic events. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9878\u20139888.": "Sutd-trafficqa: A question answering benchmark and an efficient network for video reasoning over traffic events", "Kim et al. (2017) Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. 2017. Deepstory: video story qa by deep embedded memory networks. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 2016\u20132022.": "Deepstory: Video Story QA by Deep Embedded Memory Networks", "Wu et al. (2021a) Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum, and Chuang Gan. 2021a. Star: A benchmark for situated reasoning in real-world videos. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).": "Star: A benchmark for situated reasoning in real-world videos", "Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations.": "An image is worth 16x16 words: Transformers for image recognition at scale", "Lei et al. (2018) Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. 2018. Tvqa: Localized, compositional video question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1369\u20131379.": "Tvqa: Localized, compositional video question answering", "Zhu et al. (2017) Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexander G Hauptmann. 2017. Uncovering the temporal context for video question answering. International Journal of Computer Vision, 124(3):409\u2013421.": "Uncovering the temporal context for video question answering", "Peng et al. (2021) Liang Peng, Shuangji Yang, Yi Bin, and Guoqing Wang. 2021. Progressive graph attention network for video question answering. In Proceedings of the 29th ACM International Conference on Multimedia, pages 2871\u20132879.": "Progressive Graph Attention Network for Video Question Answering", "Seo et al. (2021a) Ahjeong Seo, Gi-Cheon Kang, Joonhan Park, and Byoung-Tak Zhang. 2021a. Attend what you need: Motion-appearance synergistic networks for video question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6167\u20136177.": "Attend what you need: Motion-appearance synergistic networks for video question answering", "Huang et al. (2020) Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. 2020. Location-aware graph convolutional networks for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11021\u201311028.": "Location-aware graph convolutional networks for video question answering", "Zadeh et al. (2019) Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, and Louis-Philippe Morency. 2019. Social-iq: A question answering benchmark for artificial social intelligence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8807\u20138817.": "Social-iq: A question answering benchmark for artificial social intelligence", "Kim et al. (2021b) Seonhoon Kim, Seohyeong Jeong, Eunbyul Kim, Inho Kang, and Nojun Kwak. 2021b. Self-supervised pre-training and contrastive representation learning for multiple-choice video qa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13171\u201313179.": "Self-supervised pre-training and contrastive representation learning for multiple-choice video qa", "Wang et al. (2021) Jianyu Wang, Bingkun Bao, and Changsheng Xu. 2021. Dualvgr: A dual-visual graph reasoning unit for video question answering. IEEE Transactions on Multimedia.": "Dualvgr: A Dual-Visual Graph Reasoning Unit for Video Question Answering", "Xiao et al. (2022a) Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. 2022a. Video as conditional graph hierarchy for multi-granular question answering. In Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI), pages 2804\u20132812.": "Video as conditional graph hierarchy for multi-granular question answering", "Engin et al. (2021) Deniz Engin, Fran\u00e7ois Schnitzler, Ngoc QK Duong, and Yannis Avrithis. 2021. On the hidden treasure of dialog in video question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2064\u20132073.": "On the hidden treasure of dialog in video question answering", "Li et al. (2019) Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, et al. 2019. Beyond rnns: Positional self-attention with co-attention for video question answering. In AAAI, pages 8658\u20138665.": "Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering", "Li et al. (2022d) Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. 2022d. Invariant grounding for video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2928\u20132937.": "Invariant Grounding for Video Question Answering", "Yi et al. (2020) Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, et al. 2020. Clevrer: Collision events for video representation and reasoning. In International Conference on Learning Representations.": "Clevrer: Collision events for video representation and reasoning", "Buch et al. (2022) Shyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. 2022. Revisiting the\" video\" in video-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2917\u20132927.": "Revisiting the video in video-language understanding", "Li et al. (2021a) Fangtao Li, Ting Bai, Chenyu Cao, Zihe Liu, Chenghao Yan, and Bin Wu. 2021a. Relation-aware hierarchical attention framework for video question answering. In Proceedings of the 2021 International Conference on Multimedia Retrieval, pages 164\u2013172.": "Relation-aware hierarchical attention framework for video question answering", "Yang et al. (2021a) Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. 2021a. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697.": "Just ask: Learning to answer questions from millions of narrated videos", "Gupta et al. (2022) Vivek Gupta, Badri N Patro, Hemant Parihar, and Vinay P Namboodiri. 2022. Vquad: Video question answering diagnostic dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 282\u2013291.": "Vquad: Video question answering diagnostic dataset", "Zeng et al. (2017) Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun. 2017. Leveraging video descriptions to learn video question answering. In Thirty-First AAAI Conference on Artificial Intelligence.": "Leveraging video descriptions to learn video question answering", "Gao et al. (2018) Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. 2018. Motion-appearance co-memory networks for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6576\u20136585.": "Motion-Appearance Co-Memory Networks for Video Question Answering", "Castro et al. (2022b) Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Oana Ignat, Nan Liu, Jonathan Stroud, and Rada Mihalcea. 2022b. Fiber: Fill-in-the-blanks as a challenging video understanding evaluation framework. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2925\u20132940.": "Fiber: Fill-in-the-blanks as a challenging video understanding evaluation framework", "Fan et al. (2019) Chenyou Fan, Xiaofan Zhang, Shu Zhang, et al. 2019. Heterogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1999\u20132007.": "Heterogeneous memory enhanced multimodal attention model for video question answering", "Sadhu et al. (2021) Arka Sadhu, Kan Chen, and Ram Nevatia. 2021. Video question answering with phrases via semantic roles. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2460\u20132478.": "Video question answering with phrases via semantic roles", "Jiang and Han (2020) Pin Jiang and Yahong Han. 2020. Reasoning with heterogeneous graph alignment for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11109\u201311116.": "Reasoning with heterogeneous graph alignment for video question answering", "Le et al. (2020) Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9972\u20139981.": "Hierarchical conditional relation networks for video question answering", "Peng et al. (2022) Min Peng, Chongyang Wang, Yuan Gao, Yu Shi, and Xiang-Dong Zhou. 2022. Multilevel hierarchical network with multiscale sampling for video question answering. IJCAI.": "Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering", "Choi et al. (2021) Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang, Minsu Lee, and Byoung-Tak Zhang. 2021. Dramaqa: Character-centered video story understanding with hierarchical qa. In AAAI, volume 35.": "Dramaqa: Character-centered video story understanding with hierarchical qa", "Gupta and Gupta (2022) Pranay Gupta and Manish Gupta. 2022. Newskvqa: Knowledge-aware news video question answering. arXiv preprint arXiv:2202.04015.": "Newskvqa: Knowledge-aware news video question answering", "Fu et al. (2021) Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, et al. 2021. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv:2111.12681.": "Violet: End-to-end video-language transformers with masked visual-token modeling", "Xu et al. (2017) Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653.": "Video question answering via gradually refined attention over appearance and motion", "Li et al. (2022b) Jiangtong Li, Li Niu, and Liqing Zhang. 2022b. From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21273\u201321282.": "From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering", "Le et al. (2021) Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2021. Hierarchical conditional relation networks for multimodal video question answering. International Journal of Computer Vision, 129(11):3027\u20133050.": "Hierarchical conditional relation networks for multimodal video question answering", "Wu et al. (2021b) Tianran Wu, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima, and Haruo Takemura. 2021b. Transferring domain-agnostic knowledge in video question answering. In 32nd British Machine Vision Conference 2021, BMVC 2021, Online, November 22-25, 2021, page 301.": "Transferring domain-agnostic knowledge in video question answering", "Jiang et al. (2020) Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and Yue Gao. 2020. Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11101\u201311108.": "Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering"}, "source_title_to_arxiv_id": {"Frozen in time: A joint video and image encoder for end-to-end retrieval": "2104.00650", "Bridge to answer: Structure-aware graph interaction network for video question answering": "2104.14085", "Video Swin Transformer": "2106.13230", "Violet: End-to-end video-language transformers with masked visual-token modeling": "2111.12681"}}