{"title": "Shape Preserving Facial Landmarks with Graph Attention Networks", "abstract": "Top-performing landmark estimation algorithms are based on exploiting the\nexcellent ability of large convolutional neural networks (CNNs) to represent\nlocal appearance. However, it is well known that they can only learn weak\nspatial relationships. To address this problem, we propose a model based on the\ncombination of a CNN with a cascade of Graph Attention Network regressors. To\nthis end, we introduce an encoding that jointly represents the appearance and\nlocation of facial landmarks and an attention mechanism to weigh the\ninformation according to its reliability. This is combined with a multi-task\napproach to initialize the location of graph nodes and a coarse-to-fine\nlandmark description scheme. Our experiments confirm that the proposed model\nlearns a global representation of the structure of the face, achieving top\nperformance in popular benchmarks on head pose and landmark estimation. The\nimprovement provided by our model is most significant in situations involving\nlarge changes in the local appearance of landmarks.", "authors": ["Andr\u00e9s Prados-Torreblanca", "Jos\u00e9 M. Buenaposada", "Luis Baumela"], "published_date": "2022_10_13", "pdf_url": "http://arxiv.org/pdf/2210.07233v1", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"4\">300W</th><th colspan=\"4\">WFLW</th><th colspan=\"4\">MERL-RAV</th></tr><tr><th></th><th colspan=\"4\">Angular error (^{\\circ})(\\downarrow)</th><th colspan=\"4\">Angular error (^{\\circ})(\\downarrow)</th><th colspan=\"4\">Angular error (^{\\circ})(\\downarrow)</th></tr><tr><th>Method</th><th>yaw</th><th>pitch</th><th>roll</th><th>mean</th><th>yaw</th><th>pitch</th><th>roll</th><th>mean</th><th>yaw</th><th>pitch</th><th>roll</th><th>mean</th></tr></thead><tbody><tr><th>Yang [Yang et al.(2015)Yang, Mou, Zhang, Patras, Gunes, andRobinson]</th><td>4.2</td><td>5.1</td><td>2.4</td><td>3.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>JFA [Xu and Kakadiaris(2017)]</th><td>2.5</td><td>3.0</td><td>2.6</td><td>2.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ASMNet [Fard et al.(2021)Fard, Abdollahi, and Mahoor]</th><td>1.62</td><td>1.80</td><td>1.24</td><td>1.55</td><td>2.97</td><td>2.93</td><td>2.21</td><td>2.70</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>MNN [Valle et al.(2021)Valle, Buenaposada, and Baumela]</th><td>-</td><td>-</td><td>-</td><td>1.56</td><td>-</td><td>-</td><td>-</td><td>2.08</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SPIGA (Ours)</th><td>1.41</td><td>1.70</td><td>0.77</td><td>1.29</td><td>1.78</td><td>1.86</td><td>0.93</td><td>1.52</td><td>3.23</td><td>2.24</td><td>1.71</td><td>2.39</td></tr></tbody></table>", "caption": "Table 1: Head pose MAE, in degrees, for 300W public, WFLW and MERL-RAV datasets.", "list_citation_info": ["[Xu and Kakadiaris(2017)] Xiang Xu and Ioannis A. Kakadiaris. Joint head pose estimation and face alignment framework using global and local CNN features. In IEEE Int. Conf. on Automatic Face and Gesture Recognition, pages 642\u2013649. IEEE Computer Society, 2017.", "[Fard et al.(2021)Fard, Abdollahi, and Mahoor] Ali Pourramezan Fard, Hojjat Abdollahi, and Mohammad H. Mahoor. Asmnet: A lightweight deep neural network for face alignment and pose estimation. In CVPRW, pages 1521\u20131530. CVF/IEEE, 2021.", "[Yang et al.(2015)Yang, Mou, Zhang, Patras, Gunes, and Robinson] Heng Yang, Wenxuan Mou, Yichi Zhang, Ioannis Patras, Hatice Gunes, and Peter Robinson. Face alignment assisted by head pose estimation. In BMVC, pages 130.1\u2013130.13, 2015.", "[Valle et al.(2021)Valle, Buenaposada, and Baumela] Roberto Valle, Jos\u00e9 M. Buenaposada, and Luis Baumela. Multi-task head pose estimation in-the-wild. IEEE TPAMI, 43(8):2874\u20132881, 2021."]}, {"table": "<table><tbody><tr><th>Metric</th><th>Method</th><td>Testset</td><td>Pose</td><td>Expression</td><td>Illumination</td><td>Make-up</td><td>Occlusion</td><td>Blur</td></tr><tr><th></th><th colspan=\"8\">Bounding boxes from WFLW benchmark</th></tr><tr><th rowspan=\"15\">NME_{int-ocul} (%)(\\downarrow)</th><th>3DDE [Valle et al.(2019)Valle, Buenaposada, Vald\u00e9s, andBaumela]</th><td>4.68</td><td>8.62</td><td>5.21</td><td>4.65</td><td>4.60</td><td>5.77</td><td>5.41</td></tr><tr><th>DeCaFA [Dapogny et al.(2019)Dapogny, Cord, and Bailly]</th><td>4.62</td><td>8.11</td><td>4.65</td><td>4.41</td><td>4.63</td><td>5.74</td><td>5.38</td></tr><tr><th>AVS+SAN [Qian et al.(2019)Qian, Sun, Wu, Qian, and Jia]</th><td>4.39</td><td>8.42</td><td>4.68</td><td>4.24</td><td>4.37</td><td>5.60</td><td>4.86</td></tr><tr><th>AWing [Wang et al.(2019)Wang, Bo, and Fuxin]</th><td>4.36</td><td>7.38</td><td>4.58</td><td>4.32</td><td>4.27</td><td>5.19</td><td>4.96</td></tr><tr><th colspan=\"8\">Bounding boxes from GT landmarks (HRnet [Wang et al.(2021)Wang, Sun, Cheng, Jiang, Deng, Zhao, Liu, Mu, Tan,Wang, Liu, and Xiao] annotations)</th></tr><tr><th>GlomFace [Zhu et al.(2022)Zhu, Wan, Xie, Li, and Gu]</th><td>4.81</td><td>8.17</td><td>-</td><td>-</td><td>-</td><td>5.14</td><td>-</td></tr><tr><th>LUVLI [Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian,Koike-Akino, Liu, and Feng]</th><td>4.37</td><td>7.56</td><td>4.77</td><td>4.30</td><td>4.33</td><td>5.29</td><td>4.94</td></tr><tr><th>SDFL [Lin et al.(2021)Lin, Zhu, Wang, Liao, Qian, Lu, and Zhou]</th><td>4.35</td><td>7.42</td><td>4.63</td><td>4.29</td><td>4.22</td><td>5.19</td><td>5.08</td></tr><tr><th>AWing [Wang et al.(2019)Wang, Bo, and Fuxin]</th><td>4.21</td><td>7.21</td><td>4.46</td><td>4.23</td><td>4.02</td><td>4.99</td><td>4.82</td></tr><tr><th>SLD [Li et al.(2020)Li, Lu, Zheng, Liao, Lin, Luo, Cheng, Xiao, Lu, Kuo,and Miao]</th><td>4.21</td><td>7.36</td><td>4.49</td><td>4.12</td><td>4.05</td><td>4.98</td><td>4.82</td></tr><tr><th>HIHc<sup>1</sup><sup>1</sup>1Use RetinaFace detections. [Lan et al.(2021)Lan, Hu, and Cheng]</th><td>4.18</td><td>7.20</td><td>4.19</td><td>4.45</td><td>3.97</td><td>5.00</td><td>4.81</td></tr><tr><th>ADNet [Huang et al.(2021)Huang, Yang, Li, Kim, and Wei]</th><td>4.14</td><td>6.96</td><td>4.38</td><td>4.09</td><td>4.05</td><td>5.06</td><td>4.79</td></tr><tr><th>DTLD-s [Li et al.(2022)Li, Guo, Rhee, Han, and Han]</th><td>4.14</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SPLT [Xia et al.(2022)Xia, Qu, Huang, Zhang, Wang, and Xu]</th><td>4.14</td><td>6.96</td><td>4.45</td><td>4.05</td><td>4.00</td><td>5.06</td><td>4.79</td></tr><tr><th>SPIGA (Ours)</th><td>4.06</td><td>7.14</td><td>4.46</td><td>4.00</td><td>3.81</td><td>4.95</td><td>4.65</td></tr><tr><th rowspan=\"7\">FR_{10} (%)(\\downarrow)</th><th>GlomFace [Zhu et al.(2022)Zhu, Wan, Xie, Li, and Gu]</th><td>3.77</td><td>17.48</td><td>-</td><td>-</td><td>-</td><td>6.73</td><td>-</td></tr><tr><th>DTLD-s [Li et al.(2022)Li, Guo, Rhee, Han, and Han]</th><td>3.44</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>LUVLI [Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian,Koike-Akino, Liu, and Feng]</th><td>3.12</td><td>15.95</td><td>3.18</td><td>2.15</td><td>3.40</td><td>6.39</td><td>3.23</td></tr><tr><th>SDFL [Lin et al.(2021)Lin, Zhu, Wang, Liao, Qian, Lu, and Zhou]</th><td>2.72</td><td>12.88</td><td>1.59</td><td>2.58</td><td>2.43</td><td>5.71</td><td>3.62</td></tr><tr><th>AWing [Wang et al.(2019)Wang, Bo, and Fuxin]</th><td>2.04</td><td>9.20</td><td>1.27</td><td>2.01</td><td>0.97</td><td>4.21</td><td>2.72</td></tr><tr><th>SLD [Li et al.(2020)Li, Lu, Zheng, Liao, Lin, Luo, Cheng, Xiao, Lu, Kuo,and Miao]</th><td>3.04</td><td>15.95</td><td>2.86</td><td>2.72</td><td>1.46</td><td>5.29</td><td>4.01</td></tr><tr><th>HIHc{}^{1} [Lan et al.(2021)Lan, Hu, and Cheng]</th><td>2.96</td><td>15.03</td><td>1.59</td><td>2.58</td><td>1.46</td><td>6.11</td><td>3.49</td></tr><tr><th></th><th>ADNet [Huang et al.(2021)Huang, Yang, Li, Kim, and Wei]</th><td>2.72</td><td>12.72</td><td>2.15</td><td>2.44</td><td>1.94</td><td>5.79</td><td>3.54</td></tr><tr><th></th><th>SPLT [Xia et al.(2022)Xia, Qu, Huang, Zhang, Wang, and Xu]</th><td>2.76</td><td>12.27</td><td>2.23</td><td>1.86</td><td>3.40</td><td>5.98</td><td>3.88</td></tr><tr><th></th><th>SPIGA (ours)</th><td>2.08</td><td>11.66</td><td>2.23</td><td>1.58</td><td>1.46</td><td>4.48</td><td>2.20</td></tr><tr><th rowspan=\"4\">AUC_{10} (%)(\\uparrow)</th><th>AWing [Wang et al.(2019)Wang, Bo, and Fuxin]</th><td>58.95</td><td>33.37</td><td>57.18</td><td>59.58</td><td>60.17</td><td>52.75</td><td>53.93</td></tr><tr><th>SLD [Li et al.(2020)Li, Lu, Zheng, Liao, Lin, Luo, Cheng, Xiao, Lu, Kuo,and Miao]</th><td>58.93</td><td>31.50</td><td>56.63</td><td>59.53</td><td>60.38</td><td>52.35</td><td>53.29</td></tr><tr><th>HIHc{}^{1} [Lan et al.(2021)Lan, Hu, and Cheng]</th><td>59.70</td><td>34.20</td><td>59.00</td><td>60.60</td><td>60.40</td><td>52.70</td><td>54.90</td></tr><tr><th>ADNet [Huang et al.(2021)Huang, Yang, Li, Kim, and Wei]</th><td>60.22</td><td>34.41</td><td>52.34</td><td>58.05</td><td>60.07</td><td>52.95</td><td>54.80</td></tr><tr><th></th><th>SPLT [Xia et al.(2022)Xia, Qu, Huang, Zhang, Wang, and Xu]</th><td>59.50</td><td>34.80</td><td>57.40</td><td>60.10</td><td>60.50</td><td>51.50</td><td>53.50</td></tr><tr><th></th><th>SPIGA (Ours)</th><td>60.56</td><td>35.31</td><td>57.97</td><td>61.31</td><td>62.24</td><td>53.31</td><td>55.31</td></tr></tbody></table>", "caption": "Table 2: Evaluation of landmark detection on WFLW.", "list_citation_info": ["[Valle et al.(2019)Valle, Buenaposada, Vald\u00e9s, and Baumela] Roberto Valle, Jos\u00e9 M. Buenaposada, Antonio Vald\u00e9s, and Luis Baumela. Face alignment using a 3D deeply-initialized ensemble of regression trees. CVIU, 189:102846, 2019.", "[Wang et al.(2021)Wang, Sun, Cheng, Jiang, Deng, Zhao, Liu, Mu, Tan, Wang, Liu, and Xiao] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation learning for visual recognition. IEEE TPAMI, 43(10):3349\u20133364, 2021.", "[Li et al.(2022)Li, Guo, Rhee, Han, and Han] Hui Li, Zidong Guo, Seon-Min Rhee, Seungju Han, and Jae-Joon Han. Towards accurate facial landmark detection via cascaded transformers. In Proceedings of the IEEE/CVF CVPR, pages 4176\u20134185, June 2022.", "[Zhu et al.(2022)Zhu, Wan, Xie, Li, and Gu] Congcong Zhu, Xintong Wan, Shaorong Xie, Xiaoqiang Li, and Yinzheng Gu. Occlusion-robust face alignment using a viewpoint-invariant hierarchical network architecture. In Proceedings of the IEEE/CVF CVPR, pages 11112\u201311121, June 2022.", "[Xia et al.(2022)Xia, Qu, Huang, Zhang, Wang, and Xu] Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang, Xi Wang, and Min Xu. Sparse local patch transformer for robust face alignment and landmarks inherent relation learning. In Proceedings of the IEEE/CVF CVPR, pages 4052\u20134061, June 2022.", "[Lin et al.(2021)Lin, Zhu, Wang, Liao, Qian, Lu, and Zhou] Chunze Lin, Beier Zhu, Quan Wang, Renjie Liao, Chen Qian, Jiwen Lu, and Jie Zhou. Structure-coherent deep feature learning for robust face alignment. IEEE TIP, 30:5313\u20135326, 2021.", "[Wang et al.(2019)Wang, Bo, and Fuxin] Xinyao Wang, Liefeng Bo, and Li Fuxin. Adaptive wing loss for robust face alignment via heatmap regression. In ICCV, October 2019.", "[Lan et al.(2021)Lan, Hu, and Cheng] Xing Lan, Qinghao Hu, and Jian Cheng. Revisting quantization error in face alignment. In ICCVW, pages 1521\u20131530, October 2021.", "[Dapogny et al.(2019)Dapogny, Cord, and Bailly] Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Decafa: Deep convolutional cascade for face alignment in the wild. In ICCV, pages 6892\u20136900. IEEE, 2019.", "[Qian et al.(2019)Qian, Sun, Wu, Qian, and Jia] Shengju Qian, Keqiang Sun, Wayne Wu, Chen Qian, and Jiaya Jia. Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation. In ICCV, October 2019.", "[Huang et al.(2021)Huang, Yang, Li, Kim, and Wei] Yangyu Huang, Hao Yang, Chong Li, Jongyoo Kim, and Fangyun Wei. Adnet: Leveraging error-bias towards normal direction in face alignment. In ICCV, pages 3080\u20133090, October 2021.", "[Li et al.(2020)Li, Lu, Zheng, Liao, Lin, Luo, Cheng, Xiao, Lu, Kuo, and Miao] Weijian Li, Yuhang Lu, Kang Zheng, Haofu Liao, Chihung Lin, Jiebo Luo, Chi-Tung Cheng, Jing Xiao, Le Lu, Chang-Fu Kuo, and Shun Miao. Structured landmark detection via topology-adapting deep graph learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, ECCV, pages 266\u2013283. Springer International Publishing, 2020.", "[Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian, Koike-Akino, Liu, and Feng] Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang, Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, and Chen Feng. Luvli face alignment: Estimating landmarks\u2019 location, uncertainty, and visibility likelihood. In CVPR, pages 8233\u20138243, 2020."]}, {"table": "<table><thead><tr><th></th><th colspan=\"4\">NME{}_{box}(%)(\\downarrow)</th><th colspan=\"4\">AUC{}^{7}_{box}(%)(\\uparrow)</th></tr><tr><th>Method</th><th>All</th><th>Frontal</th><th>Half-Prof.</th><th>Profile</th><th>All</th><th>Frontal</th><th>Half-Prof.</th><th>Profile</th></tr></thead><tbody><tr><th>DU-Net</th><td>1.99</td><td>1.89</td><td>2.50</td><td>1.92</td><td>71.80</td><td>73.25</td><td>64.78</td><td>72.79</td></tr><tr><th>LUVLI [Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian,Koike-Akino, Liu, and Feng]</th><td>1.61</td><td>1.74</td><td>1.79</td><td>1.25</td><td>77.08</td><td>75.33</td><td>74.69</td><td>82.10</td></tr><tr><th>SPIGA (Ours)</th><td>1.51</td><td>1.62</td><td>1.68</td><td>1.19</td><td>78.47</td><td>76.96</td><td>75.64</td><td>83.00</td></tr></tbody></table>", "caption": "Table 3: Evaluation of landmark detection on MERL-RAV.", "list_citation_info": ["[Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian, Koike-Akino, Liu, and Feng] Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang, Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, and Chen Feng. Luvli face alignment: Estimating landmarks\u2019 location, uncertainty, and visibility likelihood. In CVPR, pages 8233\u20138243, 2020."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">NME_{box} (%)(\\downarrow)</th><th colspan=\"2\">AUC_{box}^{7} (%)(\\uparrow)</th><th>NME_{int-ocul} (%)(\\downarrow)</th></tr><tr><th></th><th>300W priv.</th><th>COFW-68</th><th>300W priv.</th><th>COFW-68</th><th>COFW-68</th></tr></thead><tbody><tr><td>HRNetV2-W18 [Wang et al.(2021)Wang, Sun, Cheng, Jiang, Deng, Zhao, Liu, Mu, Tan,Wang, Liu, and Xiao]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>5.06</td></tr><tr><td>HG\\times1+SAAT [Zhu et al.(2021)Zhu, Li, Li, and Dai]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.61</td></tr><tr><td>LUVLI(8) [Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian,Koike-Akino, Liu, and Feng]</td><td>2.24</td><td>2.75</td><td>68.3</td><td>60.8</td><td>-</td></tr><tr><td>GlomFace [Zhu et al.(2022)Zhu, Wan, Xie, Li, and Gu]</td><td>-</td><td>2.69 <sup>2</sup><sup>2</sup>2Result comes from a personal communication with authors of [Zhu et al.(2022)Zhu, Wan, Xie, Li, and Gu], 2.09 mistakenly in the paper.</td><td>-</td><td>-</td><td>4.21</td></tr><tr><td>SLD [Li et al.(2020)Li, Lu, Zheng, Liao, Lin, Luo, Cheng, Xiao, Lu, Kuo,and Miao]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.22</td></tr><tr><td>SDFL [Lin et al.(2021)Lin, Zhu, Wang, Liao, Qian, Lu, and Zhou]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.18</td></tr><tr><td>SPLT [Xia et al.(2022)Xia, Qu, Huang, Zhang, Wang, and Xu]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.10</td></tr><tr><td>DTLD-s [Li et al.(2022)Li, Guo, Rhee, Han, and Han]</td><td>2.05</td><td>2.47</td><td>70.9</td><td>65.0</td><td>-</td></tr><tr><td>SPIGA(4) (ours)</td><td>2.03</td><td>2.52</td><td>71.0</td><td>64.1</td><td>3.93</td></tr></tbody></table>", "caption": "Table 4: Landmark detection results on 300W private and COFW-68. In (\u00b7) we show the number of HG modules.", "list_citation_info": ["[Wang et al.(2021)Wang, Sun, Cheng, Jiang, Deng, Zhao, Liu, Mu, Tan, Wang, Liu, and Xiao] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation learning for visual recognition. IEEE TPAMI, 43(10):3349\u20133364, 2021.", "[Li et al.(2022)Li, Guo, Rhee, Han, and Han] Hui Li, Zidong Guo, Seon-Min Rhee, Seungju Han, and Jae-Joon Han. Towards accurate facial landmark detection via cascaded transformers. In Proceedings of the IEEE/CVF CVPR, pages 4176\u20134185, June 2022.", "[Zhu et al.(2022)Zhu, Wan, Xie, Li, and Gu] Congcong Zhu, Xintong Wan, Shaorong Xie, Xiaoqiang Li, and Yinzheng Gu. Occlusion-robust face alignment using a viewpoint-invariant hierarchical network architecture. In Proceedings of the IEEE/CVF CVPR, pages 11112\u201311121, June 2022.", "[Xia et al.(2022)Xia, Qu, Huang, Zhang, Wang, and Xu] Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang, Xi Wang, and Min Xu. Sparse local patch transformer for robust face alignment and landmarks inherent relation learning. In Proceedings of the IEEE/CVF CVPR, pages 4052\u20134061, June 2022.", "[Lin et al.(2021)Lin, Zhu, Wang, Liao, Qian, Lu, and Zhou] Chunze Lin, Beier Zhu, Quan Wang, Renjie Liao, Chen Qian, Jiwen Lu, and Jie Zhou. Structure-coherent deep feature learning for robust face alignment. IEEE TIP, 30:5313\u20135326, 2021.", "[Zhu et al.(2021)Zhu, Li, Li, and Dai] Congcong Zhu, Xiaoqiang Li, Jide Li, and Songmin Dai. Improving robustness of facial landmark detection by defending against adversarial attacks. In ICCV, pages 11751\u201311760, October 2021.", "[Li et al.(2020)Li, Lu, Zheng, Liao, Lin, Luo, Cheng, Xiao, Lu, Kuo, and Miao] Weijian Li, Yuhang Lu, Kang Zheng, Haofu Liao, Chihung Lin, Jiebo Luo, Chi-Tung Cheng, Jing Xiao, Le Lu, Chang-Fu Kuo, and Shun Miao. Structured landmark detection via topology-adapting deep graph learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, ECCV, pages 266\u2013283. Springer International Publishing, 2020.", "[Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian, Koike-Akino, Liu, and Feng] Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang, Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, and Chen Feng. Luvli face alignment: Estimating landmarks\u2019 location, uncertainty, and visibility likelihood. In CVPR, pages 8233\u20138243, 2020."]}], "citation_info_to_title": {"[Lin et al.(2021)Lin, Zhu, Wang, Liao, Qian, Lu, and Zhou] Chunze Lin, Beier Zhu, Quan Wang, Renjie Liao, Chen Qian, Jiwen Lu, and Jie Zhou. Structure-coherent deep feature learning for robust face alignment. IEEE TIP, 30:5313\u20135326, 2021.": "Structure-coherent deep feature learning for robust face alignment", "[Li et al.(2020)Li, Lu, Zheng, Liao, Lin, Luo, Cheng, Xiao, Lu, Kuo, and Miao] Weijian Li, Yuhang Lu, Kang Zheng, Haofu Liao, Chihung Lin, Jiebo Luo, Chi-Tung Cheng, Jing Xiao, Le Lu, Chang-Fu Kuo, and Shun Miao. Structured landmark detection via topology-adapting deep graph learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, ECCV, pages 266\u2013283. Springer International Publishing, 2020.": "Structured landmark detection via topology-adapting deep graph learning", "[Wang et al.(2019)Wang, Bo, and Fuxin] Xinyao Wang, Liefeng Bo, and Li Fuxin. Adaptive wing loss for robust face alignment via heatmap regression. In ICCV, October 2019.": "Adaptive wing loss for robust face alignment via heatmap regression", "[Wang et al.(2021)Wang, Sun, Cheng, Jiang, Deng, Zhao, Liu, Mu, Tan, Wang, Liu, and Xiao] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation learning for visual recognition. IEEE TPAMI, 43(10):3349\u20133364, 2021.": "Deep high-resolution representation learning for visual recognition", "[Kumar et al.(2020)Kumar, Marks, Mou, Wang, Jones, Cherian, Koike-Akino, Liu, and Feng] Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang, Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, and Chen Feng. Luvli face alignment: Estimating landmarks\u2019 location, uncertainty, and visibility likelihood. In CVPR, pages 8233\u20138243, 2020.": "Luvli face alignment: Estimating landmarks\u2019 location, uncertainty, and visibility likelihood", "[Zhu et al.(2021)Zhu, Li, Li, and Dai] Congcong Zhu, Xiaoqiang Li, Jide Li, and Songmin Dai. Improving robustness of facial landmark detection by defending against adversarial attacks. In ICCV, pages 11751\u201311760, October 2021.": "Improving robustness of facial landmark detection by defending against adversarial attacks", "[Dapogny et al.(2019)Dapogny, Cord, and Bailly] Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Decafa: Deep convolutional cascade for face alignment in the wild. In ICCV, pages 6892\u20136900. IEEE, 2019.": "Decafa: Deep Convolutional Cascade for Face Alignment in the Wild", "[Xia et al.(2022)Xia, Qu, Huang, Zhang, Wang, and Xu] Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang, Xi Wang, and Min Xu. Sparse local patch transformer for robust face alignment and landmarks inherent relation learning. In Proceedings of the IEEE/CVF CVPR, pages 4052\u20134061, June 2022.": "Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning", "[Yang et al.(2015)Yang, Mou, Zhang, Patras, Gunes, and Robinson] Heng Yang, Wenxuan Mou, Yichi Zhang, Ioannis Patras, Hatice Gunes, and Peter Robinson. Face alignment assisted by head pose estimation. In BMVC, pages 130.1\u2013130.13, 2015.": "Face alignment assisted by head pose estimation", "[Li et al.(2022)Li, Guo, Rhee, Han, and Han] Hui Li, Zidong Guo, Seon-Min Rhee, Seungju Han, and Jae-Joon Han. Towards accurate facial landmark detection via cascaded transformers. In Proceedings of the IEEE/CVF CVPR, pages 4176\u20134185, June 2022.": "Towards accurate facial landmark detection via cascaded transformers", "[Xu and Kakadiaris(2017)] Xiang Xu and Ioannis A. Kakadiaris. Joint head pose estimation and face alignment framework using global and local CNN features. In IEEE Int. Conf. on Automatic Face and Gesture Recognition, pages 642\u2013649. IEEE Computer Society, 2017.": "Joint head pose estimation and face alignment framework using global and local CNN features", "[Qian et al.(2019)Qian, Sun, Wu, Qian, and Jia] Shengju Qian, Keqiang Sun, Wayne Wu, Chen Qian, and Jiaya Jia. Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation. In ICCV, October 2019.": "Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation", "[Fard et al.(2021)Fard, Abdollahi, and Mahoor] Ali Pourramezan Fard, Hojjat Abdollahi, and Mohammad H. Mahoor. Asmnet: A lightweight deep neural network for face alignment and pose estimation. In CVPRW, pages 1521\u20131530. CVF/IEEE, 2021.": "Asmnet: A lightweight deep neural network for face alignment and pose estimation", "[Valle et al.(2021)Valle, Buenaposada, and Baumela] Roberto Valle, Jos\u00e9 M. Buenaposada, and Luis Baumela. Multi-task head pose estimation in-the-wild. IEEE TPAMI, 43(8):2874\u20132881, 2021.": "Multi-task head pose estimation in-the-wild", "[Lan et al.(2021)Lan, Hu, and Cheng] Xing Lan, Qinghao Hu, and Jian Cheng. Revisting quantization error in face alignment. In ICCVW, pages 1521\u20131530, October 2021.": "Revisting quantization error in face alignment", "[Zhu et al.(2022)Zhu, Wan, Xie, Li, and Gu] Congcong Zhu, Xintong Wan, Shaorong Xie, Xiaoqiang Li, and Yinzheng Gu. Occlusion-robust face alignment using a viewpoint-invariant hierarchical network architecture. In Proceedings of the IEEE/CVF CVPR, pages 11112\u201311121, June 2022.": "Occlusion-robust face alignment using a viewpoint-invariant hierarchical network architecture", "[Huang et al.(2021)Huang, Yang, Li, Kim, and Wei] Yangyu Huang, Hao Yang, Chong Li, Jongyoo Kim, and Fangyun Wei. Adnet: Leveraging error-bias towards normal direction in face alignment. In ICCV, pages 3080\u20133090, October 2021.": "Adnet: Leveraging error-bias towards normal direction in face alignment", "[Valle et al.(2019)Valle, Buenaposada, Vald\u00e9s, and Baumela] Roberto Valle, Jos\u00e9 M. Buenaposada, Antonio Vald\u00e9s, and Luis Baumela. Face alignment using a 3D deeply-initialized ensemble of regression trees. CVIU, 189:102846, 2019.": "Face alignment using a 3D deeply-initialized ensemble of regression trees"}, "source_title_to_arxiv_id": {"Asmnet: A lightweight deep neural network for face alignment and pose estimation": "2103.00119", "Multi-task head pose estimation in-the-wild": "2202.02299"}}