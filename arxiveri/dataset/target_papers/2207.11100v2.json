{"title": "Zero-Shot Video Captioning with Evolving Pseudo-Tokens", "abstract": "We introduce a zero-shot video captioning method that employs two frozen\nnetworks: the GPT-2 language model and the CLIP image-text matching model. The\nmatching score is used to steer the language model toward generating a sentence\nthat has a high average matching score to a subset of the video frames. Unlike\nzero-shot image captioning methods, our work considers the entire sentence at\nonce. This is achieved by optimizing, during the generation process, part of\nthe prompt from scratch, by modifying the representation of all other tokens in\nthe prompt, and by repeating the process iteratively, gradually improving the\nspecificity and comprehensiveness of the generated sentence. Our experiments\nshow that the generated captions are coherent and display a broad range of\nreal-world knowledge. Our code is available at:\nhttps://github.com/YoadTew/zero-shot-video-to-text", "authors": ["Yoad Tewel", "Yoav Shalev", "Roy Nadler", "Idan Schwartz", "Lior Wolf"], "published_date": "2022_07_22", "pdf_url": "http://arxiv.org/pdf/2207.11100v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th></th><td colspan=\"5\">Supervised Metrics</td><td colspan=\"4\">Unsupervised Metrics</td></tr><tr><th>Dataset</th><th>Method</th><td>B@4</td><td>M</td><td>C</td><td>R</td><td>\\text{CLIP-S}^{\\text{Ref}}</td><td>CLIP-S</td><td>BLIP-S</td><td>Retrieval</td><td>PP</td></tr><tr><th rowspan=\"5\">MSR-VTT</th><th>VNS-GRU [6]</th><td>0.453</td><td>0.299</td><td>0.530</td><td>0.634</td><td>0.739</td><td>0.626</td><td>0.623</td><td>0.446</td><td>118.81</td></tr><tr><th>SemSynAN [34]</th><td>0.464</td><td>0.304</td><td>0.519</td><td>0.647</td><td>0.733</td><td>0.619</td><td>0.608</td><td>0.437</td><td>155.01</td></tr><tr><th colspan=\"9\">Zero-Shot Methods</th><td></td></tr><tr><th>ZeroCap* [47]</th><td>0.023</td><td>0.129</td><td>0.058</td><td>0.304</td><td>0.739</td><td>0.710</td><td>0.575</td><td>0.442</td><td>54.71</td></tr><tr><th>MAGIC* [44]</th><td>0.055</td><td>0.133</td><td>0.074</td><td>0.354</td><td>0.628</td><td>0.566</td><td>0.434</td><td>0.392</td><td>30.48</td></tr><tr><th></th><th>Ours</th><td>0.030</td><td>0.146</td><td>0.113</td><td>0.277</td><td>0.785</td><td>0.775</td><td>0.675</td><td>0.504</td><td>18.35</td></tr><tr><th rowspan=\"4\">MSVD</th><th>VNS-GRU [6]</th><td>0.665</td><td>0.421</td><td>1.215</td><td>0.797</td><td>0.780</td><td>0.673</td><td>0.646</td><td>0.557</td><td>418.72</td></tr><tr><th>SemSynAN [34]</th><td>0.644</td><td>0.419</td><td>1.115</td><td>0.795</td><td>0.767</td><td>0.660</td><td>0.633</td><td>0.546</td><td>242.46</td></tr><tr><th colspan=\"9\">Zero-Shot Methods</th><td></td></tr><tr><th>ZeroCap* [47]</th><td>0.029</td><td>0.163</td><td>0.096</td><td>0.354</td><td>0.762</td><td>0.765</td><td>0.642</td><td>0.500</td><td>28.44</td></tr><tr><th></th><th>MAGIC* [44]</th><td>0.066</td><td>0.161</td><td>0.140</td><td>0.401</td><td>0.670</td><td>0.623</td><td>0.497</td><td>0.469</td><td>29.84</td></tr><tr><th></th><th>Ours</th><td>0.030</td><td>0.178</td><td>0.174</td><td>0.314</td><td>0.805</td><td>0.822</td><td>0.743</td><td>0.569</td><td>18.94</td></tr></tbody></table>", "caption": "Table 1: Quantitative results for video captioning. We separate the results into two categories: (i) supervised metrics that require human references, B@4 = BLEU-4, M = METEOR, C = CIDEr, S = SPICE, and \\text{CLIP-S}^{\\text{Ref}}. (ii) Unsupervised metrics that use a pre-trained model, CLIP-S = CLIP-based image-text similarity, BLIP-S = BLIP-based image-text similarity [20], Retrieval = VideoCLIP-based video-text similarity [53], and PP = caption perplexity computed with BERT [8]. (*) denotes that the model is adapted from image captioning to video captioning.", "list_citation_info": ["[47] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf. Zero-shot image-to-text generation for visual-semantic arithmetic. CVPR, 2022.", "[6] H. Chen, J. Li, and X. Hu. Delving deeper into the decoder for video captioning. ECAI, 2021.", "[44] Y. Su, T. Lan, Y. Liu, F. Liu, D. Yogatama, Y. Wang, L. Kong, and N. Collier. Language Models Can See: Plugging Visual Controls in Text Generation. arXiv e-prints, page arXiv:2205.02655, May 2022.", "[34] J. Perez-Martin, B. Bustos, and J. Perez. Improving video captioning with temporal composition of a visual-syntactic embedding. In WACV, 2021.", "[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019.", "[20] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022.", "[53] H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze, L. Zettlemoyer, and C. Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. 2021."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"5\">Supervised Metrics</td><td colspan=\"2\">Unsupervised Metrics</td></tr><tr><th>Method</th><td>B@4</td><td>M</td><td>C</td><td>S</td><td>\\text{CLIP-S}^{\\text{Ref}}</td><td>CLIP-S</td><td>PP</td></tr><tr><th>VinVL [60]</th><td>0.41</td><td>0.311</td><td>1.409</td><td>0.252</td><td>0.83</td><td>0.780</td><td>24.16</td></tr><tr><th></th><td colspan=\"6\">Zero-Shot Methods</td><td></td></tr><tr><th>ZeroCap [47]</th><td>0.029</td><td>0.12</td><td>0.131</td><td>0.055</td><td>0.778</td><td>0.870</td><td>25.737</td></tr><tr><th>MAGIC [44]</th><td>0.129</td><td>0.174</td><td>0.493</td><td>0.113</td><td>0.763</td><td>0.737</td><td>37.126</td></tr><tr><th>Ours</th><td>0.022</td><td>0.127</td><td>0.172</td><td>0.073</td><td>0.798</td><td>0.885</td><td>19.049</td></tr></tbody></table>", "caption": "Table 2: Quantitative results for image captioning methods. We evaluate supervised metrics that measure text correspondence to human references and unsupervised metrics that are computed without referring to the human annotation.", "list_citation_info": ["[44] Y. Su, T. Lan, Y. Liu, F. Liu, D. Yogatama, Y. Wang, L. Kong, and N. Collier. Language Models Can See: Plugging Visual Controls in Text Generation. arXiv e-prints, page arXiv:2205.02655, May 2022.", "[47] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf. Zero-shot image-to-text generation for visual-semantic arithmetic. CVPR, 2022.", "[60] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Image</th><th>Video</th></tr></thead><tbody><tr><th>MAGIC [44]</th><td>1.65</td><td>1.77</td></tr><tr><th>ZeroCap [47]</th><td>2.52</td><td>2.30</td></tr><tr><th>Ours</th><td>4.01</td><td>4.14</td></tr></tbody></table>", "caption": "Table 3: Mean Opinion Score (MOS, scale of 1\u20135) for caption quality using real-world images and videos.", "list_citation_info": ["[44] Y. Su, T. Lan, Y. Liu, F. Liu, D. Yogatama, Y. Wang, L. Kong, and N. Collier. Language Models Can See: Plugging Visual Controls in Text Generation. arXiv e-prints, page arXiv:2205.02655, May 2022.", "[47] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf. Zero-shot image-to-text generation for visual-semantic arithmetic. CVPR, 2022."]}, {"table": "<table><tbody><tr><th></th><td></td><td colspan=\"6\">Supervised Metrics</td><td colspan=\"3\">Unsupervised Metrics</td></tr><tr><th>Method</th><td>Prefix</td><td>B@4</td><td>M</td><td>C</td><td>R</td><td>S</td><td>\\text{CLIP-S}^{\\text{Ref}}</td><td>CLIP-S</td><td>BLIP-S</td><td>PP</td></tr><tr><th>ZeroCap [47]</th><td>None</td><td>0.021</td><td>0.1</td><td>0.139</td><td>0.207</td><td>0.051</td><td>0.760</td><td>0.821</td><td>0.604</td><td>109.959</td></tr><tr><th>ZeroCap [47]</th><td>\u2018A\u2019</td><td>0.026</td><td>0.116</td><td>0.145</td><td>0.276</td><td>0.054</td><td>0.771</td><td>0.845</td><td>0.611</td><td>33.661</td></tr><tr><th>ZeroCap [47]</th><td>\u2018Image of a\u2019</td><td>0.029</td><td>0.12</td><td>0.131</td><td>0.268</td><td>0.055</td><td>0.778</td><td>0.870</td><td>0.605</td><td>25.737</td></tr><tr><th>Ours</th><td>None</td><td>0.024</td><td>0.127</td><td>0.200</td><td>0.239</td><td>0.071</td><td>0.791</td><td>0.852</td><td>0.652</td><td>20.412</td></tr><tr><th>Ours</th><td>Random</td><td>0.022</td><td>0.127</td><td>0.172</td><td>0.228</td><td>0.073</td><td>0.798</td><td>0.885</td><td>0.651</td><td>19.049</td></tr></tbody></table>", "caption": "Table 4: Quantitative results for image captioning on the MS-COCO test set using different prefixes.", "list_citation_info": ["[47] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf. Zero-shot image-to-text generation for visual-semantic arithmetic. CVPR, 2022."]}], "citation_info_to_title": {"[20] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022.": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "[47] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf. Zero-shot image-to-text generation for visual-semantic arithmetic. CVPR, 2022.": "Zero-shot image-to-text generation for visual-semantic arithmetic", "[60] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.": "Vinvl: Revisiting visual representations in vision-language models", "[44] Y. Su, T. Lan, Y. Liu, F. Liu, D. Yogatama, Y. Wang, L. Kong, and N. Collier. Language Models Can See: Plugging Visual Controls in Text Generation. arXiv e-prints, page arXiv:2205.02655, May 2022.": "Language Models Can See: Plugging Visual Controls in Text Generation", "[6] H. Chen, J. Li, and X. Hu. Delving deeper into the decoder for video captioning. ECAI, 2021.": "Delving Deeper into the Decoder for Video Captioning", "[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019.": "Bert: Pre-training of deep bidirectional transformers for language understanding", "[53] H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze, L. Zettlemoyer, and C. Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. 2021.": "Videoclip: Contrastive pre-training for zero-shot video-text understanding", "[34] J. Perez-Martin, B. Bustos, and J. Perez. Improving video captioning with temporal composition of a visual-syntactic embedding. In WACV, 2021.": "Improving video captioning with temporal composition of a visual-syntactic embedding"}, "source_title_to_arxiv_id": {"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation": "2201.12086", "Zero-shot image-to-text generation for visual-semantic arithmetic": "2111.14447"}}