{"title": "Meta Balanced Network for Fair Face Recognition", "abstract": "Although deep face recognition has achieved impressive progress in recent\nyears, controversy has arisen regarding discrimination based on skin tone,\nquestioning their deployment into real-world scenarios. In this paper, we aim\nto systematically and scientifically study this bias from both data and\nalgorithm aspects. First, using the dermatologist approved Fitzpatrick Skin\nType classification system and Individual Typology Angle, we contribute a\nbenchmark called Identity Shades (IDS) database, which effectively quantifies\nthe degree of the bias with respect to skin tone in existing face recognition\nalgorithms and commercial APIs. Further, we provide two skin-tone aware\ntraining datasets, called BUPT-Globalface dataset and BUPT-Balancedface\ndataset, to remove bias in training data. Finally, to mitigate the algorithmic\nbias, we propose a novel meta-learning algorithm, called Meta Balanced Network\n(MBN), which learns adaptive margins in large margin loss such that the model\noptimized by this loss can perform fairly across people with different skin\ntones. To determine the margins, our method optimizes a meta skewness loss on a\nclean and unbiased meta set and utilizes backward-on-backward automatic\ndifferentiation to perform a second order gradient descent step on the current\nmargins. Extensive experiments show that MBN successfully mitigates bias and\nlearns more balanced performance for people with different skin tones in face\nrecognition. The proposed datasets are available at\nhttp://www.whdeng.cn/RFW/index.html.", "authors": ["Mei Wang", "Yaobin Zhang", "Weihong Deng"], "published_date": "2022_05_13", "pdf_url": "http://arxiv.org/pdf/2205.06548v1", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th colspan=\"8\">The skin tone of IDS-8</th></tr><tr><th>I</th><th>II</th><th>III</th><th>IV</th><th>V</th><th>VI</th><th>VII</th><th>VIII</th></tr></thead><tbody><tr><th>Microsoft [12]</th><td>88.67</td><td>87.15</td><td>80.20</td><td>78.45</td><td>83.13</td><td>82.09</td><td>76.10</td><td>74.90</td></tr><tr><th>Face++ [13]</th><td>93.68</td><td>93.89</td><td>92.45</td><td>92.49</td><td>88.58</td><td>89.36</td><td>88.23</td><td>87.40</td></tr><tr><th>Baidu [14]</th><td>90.52</td><td>88.04</td><td>90.52</td><td>89.68</td><td>86.72</td><td>87.19</td><td>78.18</td><td>78.62</td></tr><tr><th>Amazon [15]</th><td>91.22</td><td>90.12</td><td>84.96</td><td>85.13</td><td>86.85</td><td>88.40</td><td>85.79</td><td>86.36</td></tr><tr><th>mean</th><td>91.02</td><td>89.80</td><td>87.03</td><td>86.44</td><td>86.32</td><td>86.76</td><td>82.07</td><td>81.82</td></tr><tr><th>Center-loss [16]</th><td>87.59</td><td>87.14</td><td>79.21</td><td>77.87</td><td>81.90</td><td>83.68</td><td>78.55</td><td>77.51</td></tr><tr><th>Sphereface [17]</th><td>90.59</td><td>90.71</td><td>82.75</td><td>82.42</td><td>85.50</td><td>88.06</td><td>81.95</td><td>81.82</td></tr><tr><th>Arcface<sup>1</sup> [18]</th><td>92.33</td><td>91.70</td><td>83.97</td><td>83.05</td><td>87.46</td><td>88.40</td><td>84.07</td><td>83.23</td></tr><tr><th>VGGface2<sup>1</sup> [19]</th><td>90.22</td><td>89.66</td><td>84.93</td><td>83.82</td><td>86.03</td><td>87.29</td><td>83.37</td><td>82.72</td></tr><tr><th>mean</th><td>90.18</td><td>89.80</td><td>82.71</td><td>81.79</td><td>85.22</td><td>86.86</td><td>81.98</td><td>81.32</td></tr></tbody></table><ul><li>1<p>Arcface is a ResNet-34 model trained with CASIA-Webface. VGGFace2 is a SeNet model trained with VGGFace2.</p></li></ul>", "caption": "TABLE I: Bias with respect to skin tone in commercial APIs and SOTA FR algorithms. Verification accuracies (%) on our IDS-8 are given. Skin gradually darkens with the increase of tone value (from I to VIII). We make the best results bold, and make the worst in red.", "list_citation_info": ["[12] \u201cMicrosoft azure,\u201d https://www.azure.cn.", "[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.", "[19] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVggface2: A dataset for recognising faces across pose and age,\u201d in 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018). IEEE, 2018, pp. 67\u201374.", "[14] \u201cBaidu cloud vision api,\u201d http://ai.baidu.com.", "[16] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, \u201cA discriminative feature learning approach for deep face recognition,\u201d in European conference on computer vision. Springer, 2016, pp. 499\u2013515.", "[13] \u201cFace++ research toolkit,\u201d www.faceplusplus.com.", "[15] \u201cAmazon\u2019s rekognition tool,\u201d https://aws.amazon.com/rekognition/.", "[17] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u201cSphereface: Deep hypersphere embedding for face recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, vol. 1, 2017."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th rowspan=\"2\">LFW [9]</th><th colspan=\"4\">IDS-4</th></tr><tr><th>I</th><th>II</th><th>III</th><th>IV</th></tr></thead><tbody><tr><th>Microsoft [12]</th><th>98.22</th><td>87.60</td><td>79.67</td><td>82.83</td><td>75.83</td></tr><tr><th>Face++ [13]</th><th>97.03</th><td>93.90</td><td>92.47</td><td>88.55</td><td>87.50</td></tr><tr><th>Baidu [14]</th><th>98.67</th><td>89.13</td><td>90.27</td><td>86.53</td><td>77.97</td></tr><tr><th>Amazon [15]</th><th>98.50</th><td>90.45</td><td>84.87</td><td>87.20</td><td>86.27</td></tr><tr><th>mean</th><th>98.11</th><td>90.27</td><td>86.82</td><td>86.28</td><td>81.89</td></tr><tr><th>Center-loss [16]</th><th>98.75</th><td>87.18</td><td>79.32</td><td>81.92</td><td>78.00</td></tr><tr><th>Sphereface [17]</th><th>99.27</th><td>90.80</td><td>82.95</td><td>87.02</td><td>82.28</td></tr><tr><th>Arcface<sup>1</sup> [18]</th><th>99.40</th><td>92.15</td><td>83.98</td><td>88.00</td><td>84.93</td></tr><tr><th>VGGface2<sup>2</sup> [19]</th><th>99.30</th><td>89.90</td><td>84.93</td><td>86.13</td><td>83.38</td></tr><tr><th>mean</th><th>99.18</th><td>90.01</td><td>82.80</td><td>85.77</td><td>82.15</td></tr></tbody></table><ul><li>1<p>Arcface is a ResNet-34 model trained with CASIA-Webface.</p></li><li>2<p>VGGFace2 here is a SeNet model trained with VGGFace2.</p></li></ul>", "caption": "TABLE II: Bias with respect to skin tone in commercial APIs and SOTA FR algorithms. Accuracies (%) on our IDS-4 are given. ", "list_citation_info": ["[12] \u201cMicrosoft azure,\u201d https://www.azure.cn.", "[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.", "[19] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVggface2: A dataset for recognising faces across pose and age,\u201d in 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018). IEEE, 2018, pp. 67\u201374.", "[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, \u201cLabeled faces in the wild: A database for studying face recognition in unconstrained environments,\u201d Technical Report 07-49, University of Massachusetts, Amherst, Tech. Rep., 2007.", "[14] \u201cBaidu cloud vision api,\u201d http://ai.baidu.com.", "[16] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, \u201cA discriminative feature learning approach for deep face recognition,\u201d in European conference on computer vision. Springer, 2016, pp. 499\u2013515.", "[13] \u201cFace++ research toolkit,\u201d www.faceplusplus.com.", "[15] \u201cAmazon\u2019s rekognition tool,\u201d https://aws.amazon.com/rekognition/.", "[17] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u201cSphereface: Deep hypersphere embedding for face recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, vol. 1, 2017."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Training Databases</th><th rowspan=\"2\">LFW [9]</th><th rowspan=\"2\">CFP-FP [68]</th><th rowspan=\"2\">AgeDB-30 [69]</th><th colspan=\"4\">IDS-4</th><th colspan=\"8\">IDS-8</th></tr><tr><td>I</td><td>II</td><td>III</td><td>IV</td><td>I</td><td>II</td><td>III</td><td>IV</td><td>V</td><td>VI</td><td>VII</td><td>VIII</td></tr><tr><th>CASIA-WebFace [25]</th><th>99.40</th><th>93.91</th><th>93.35</th><th>92.15</th><th>83.98</th><th>88.00</th><th>84.93</th><th>92.33</th><th>91.70</th><th>83.97</th><th>83.05</th><th>87.46</th><th>88.40</th><th>84.07</th><th>83.23</th></tr><tr><td>Balancedface{}^{*} (ours)</td><td>99.55</td><td>92.74</td><td>95.15</td><td>93.92</td><td>90.60</td><td>92.98</td><td>90.98</td><td>93.95</td><td>93.85</td><td>90.14</td><td>91.14</td><td>92.97</td><td>93.55</td><td>91.70</td><td>90.32</td></tr></tbody></table>", "caption": "TABLE III: Verification accuracy (%) of ResNet-34 models trained with CASIA-Webface [25] and our Balancedface{}^{*}.", "list_citation_info": ["[25] D. Yi, Z. Lei, S. Liao, and S. Z. Li, \u201cLearning face representation from scratch,\u201d arXiv preprint arXiv:1411.7923, 2014.", "[68] S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, and D. W. Jacobs, \u201cFrontal to profile face verification in the wild,\u201d in 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2016, pp. 1\u20139.", "[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, \u201cLabeled faces in the wild: A database for studying face recognition in unconstrained environments,\u201d Technical Report 07-49, University of Massachusetts, Amherst, Tech. Rep., 2007.", "[69] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou, \u201cAgedb: the first manually collected, in-the-wild age database,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017, pp. 51\u201359."]}, {"table": "<table><tbody><tr><th colspan=\"2\">Test\\rightarrow</th><th colspan=\"4\">IDS-4</th><th rowspan=\"2\">Avg</th><th colspan=\"2\">Fairness</th></tr><tr><td>Train ratio \\downarrow</td><td>Method\\downarrow</td><td>I</td><td>II</td><td>III</td><td>IV</td><td>STD</td><td>SER</td></tr><tr><th rowspan=\"2\">4:2:2:2</th><th>N-Softmax [71]</th><th>89.67</th><th>84.68</th><th>87.97</th><th>84.17</th><th>86.62</th><th>2.64</th><th>1.53</th></tr><tr><td>MBN(soft)</td><td>91.05</td><td>88.10</td><td>90.17</td><td>88.80</td><td>89.53</td><td>1.33</td><td>1.32</td></tr><tr><th rowspan=\"2\">5:\\frac{5}{3}:\\frac{5}{3}:\\frac{5}{3}</th><th>N-Softmax [71]</th><th>89.88</th><th>85.13</th><th>88.52</th><th>83.42</th><th>86.74</th><th>2.98</th><th>1.64</th></tr><tr><td>MBN(soft)</td><td>91.28</td><td>87.73</td><td>90.68</td><td>88.15</td><td>89.46</td><td>1.78</td><td>1.41</td></tr><tr><th rowspan=\"2\">6:\\frac{4}{3}:\\frac{4}{3}:\\frac{4}{3}</th><th>N-Softmax [71]</th><th>90.43</th><th>84.75</th><th>88.32</th><th>83.32</th><th>86.70</th><th>3.26</th><th>1.74</th></tr><tr><td>MBN(soft)</td><td>91.28</td><td>87.57</td><td>90.35</td><td>87.22</td><td>89.10</td><td>2.02</td><td>1.47</td></tr><tr><th rowspan=\"2\">7:1:1:1</th><th>N-Softmax [71]</th><th>90.67</th><th>84.37</th><th>87.77</th><th>82.97</th><th>86.44</th><th>3.46</th><th>1.83</th></tr><tr><td>MBN(soft)</td><td>90.85</td><td>86.82</td><td>89.20</td><td>86.08</td><td>88.24</td><td>2.19</td><td>1.52</td></tr></tbody></table>", "caption": "TABLE IV: Results on the verification experiments by varying distribution in the training set. Fairness is measured by the standard deviation (STD) (lower is better) and the skewed error ratio (SER) (1 is the best).", "list_citation_info": ["[71] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille, \u201cNormface: l 2 hypersphere embedding for face verification,\u201d in Proceedings of the 25th ACM international conference on Multimedia. ACM, 2017, pp. 1041\u20131049."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><th colspan=\"4\">IDS-4</th><th rowspan=\"2\">Avg</th><th colspan=\"2\">Fairness</th></tr><tr><td>I</td><td>II</td><td>III</td><td>IV</td><td>STD</td><td>SER</td></tr><tr><th>Fisher vector [72]</th><th>70.23</th><th>66.88</th><th>69.55</th><th>63.22</th><th>67.47</th><th>3.18</th><th>1.24</th></tr><tr><td>Triplet [8]</td><td>95.80</td><td>91.03</td><td>92.77</td><td>90.47</td><td>92.52</td><td>2.40</td><td>2.27</td></tr><tr><th>Softmax</th><th>95.62</th><th>90.85</th><th>91.97</th><th>89.98</th><th>92.10</th><th>2.48</th><th>2.29</th></tr><tr><td>M-RBN(soft) [28]</td><td>93.50</td><td>90.06</td><td>94.50</td><td>93.43</td><td>92.87</td><td>1.94</td><td>1.81</td></tr><tr><th>RL-RBN(soft) [28]</th><th>94.53</th><th>94.20</th><th>95.03</th><th>94.05</th><th>94.45</th><th>0.44</th><th>1.20</th></tr><tr><td>MBN(soft)</td><td>94.62</td><td>94.18</td><td>94.72</td><td>93.87</td><td>94.35</td><td>0.39</td><td>1.16</td></tr><tr><th>Cosface [27]</th><th>96.63</th><th>93.50</th><th>94.68</th><th>92.17</th><th>94.25</th><th>1.90</th><th>2.33</th></tr><tr><td>M-RBN(cos) [28]</td><td>96.15</td><td>93.43</td><td>95.73</td><td>94.76</td><td>95.02</td><td>1.21</td><td>1.70</td></tr><tr><th>RL-RBN(cos) [28]</th><th>96.03</th><th>94.58</th><th>95.15</th><th>94.27</th><th>95.01</th><th>0.77</th><th>1.45</th></tr><tr><td>MBN(cos)</td><td>96.07</td><td>94.87</td><td>95.52</td><td>94.43</td><td>95.22</td><td>0.72</td><td>1.42</td></tr><tr><th>Arcface [18]</th><th>97.37</th><th>94.55</th><th>95.68</th><th>93.87</th><th>95.37</th><th>1.53</th><th>2.33</th></tr><tr><td>M-RBN(arc) [28]</td><td>97.03</td><td>94.40</td><td>95.58</td><td>95.18</td><td>95.55</td><td>1.10</td><td>1.89</td></tr><tr><th>RL-RBN(arc) [28]</th><th>97.08</th><th>95.57</th><th>95.63</th><th>94.87</th><th>95.79</th><th>0.93</th><th>1.76</th></tr><tr><td>MBN(arc)</td><td>96.87</td><td>95.63</td><td>96.20</td><td>95.00</td><td>95.93</td><td>0.80</td><td>1.59</td></tr></tbody></table>", "caption": "TABLE V: Verification accuracy (%) of methods trained with different loss function ([BUPT-Globalface-4, ResNet34, loss*]).", "list_citation_info": ["[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.", "[8] F. Schroff, D. Kalenichenko, and J. Philbin, \u201cFacenet: A unified embedding for face recognition and clustering,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 815\u2013823.", "[28] M. Wang and W. Deng, \u201cMitigating bias in face recognition using skewness-aware reinforcement learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9322\u20139331.", "[27] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5265\u20135274.", "[72] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman, \u201cFisher vector faces in the wild.\u201d in BMVC, vol. 2, no. 3, 2013, p. 4."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><th colspan=\"8\">The skin tone of IDS-8</th><th rowspan=\"2\">Avg</th><th colspan=\"2\">Fairness</th></tr><tr><td>I</td><td>II</td><td>III</td><td>IV</td><td>V</td><td>VI</td><td>VII</td><td>VIII</td><td>STD</td><td>SER</td></tr><tr><th>Softmax</th><th>95.06</th><th>94.98</th><th>90.29</th><th>90.02</th><th>90.64</th><th>93.02</th><th>89.88</th><th>87.97</th><th>91.48</th><th>2.58</th><th>2.43</th></tr><tr><td>MBN(soft)</td><td>94.05</td><td>94.18</td><td>93.47</td><td>93.75</td><td>93.37</td><td>94.94</td><td>92.96</td><td>92.34</td><td>93.63</td><td>0.80</td><td>1.51</td></tr><tr><th>Cosface [27]</th><th>96.30</th><th>95.97</th><th>93.19</th><th>93.08</th><th>94.16</th><th>94.56</th><th>92.86</th><th>91.57</th><th>93.96</th><th>1.61</th><th>2.28</th></tr><tr><td>MBN(cos)</td><td>95.87</td><td>95.84</td><td>94.64</td><td>94.82</td><td>94.70</td><td>96.20</td><td>94.58</td><td>94.12</td><td>95.09</td><td>0.76</td><td>1.55</td></tr><tr><th>Arcface [18]</th><th>97.34</th><th>97.09</th><th>94.43</th><th>94.72</th><th>95.36</th><th>95.86</th><th>94.25</th><th>92.78</th><th>95.23</th><th>1.52</th><th>2.72</th></tr><tr><td>MBN(arc)</td><td>97.11</td><td>96.83</td><td>94.94</td><td>95.45</td><td>95.54</td><td>96.73</td><td>94.81</td><td>94.83</td><td>95.78</td><td>0.96</td><td>1.79</td></tr></tbody></table>", "caption": "TABLE VI: Verification accuracy (%) of methods trained with different loss function ([BUPT-Globalface-8, ResNet34, loss*]).", "list_citation_info": ["[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.", "[27] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5265\u20135274."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Methods</td><td colspan=\"4\">IDS-4</td><td rowspan=\"2\">Avg</td><td colspan=\"2\">Fairness</td></tr><tr><td>I</td><td>II</td><td>III</td><td>IV</td><td>STD</td><td>SER</td></tr><tr><td>Fisher vector [72]</td><td>70.23</td><td>66.88</td><td>69.55</td><td>63.22</td><td>67.47</td><td>3.18</td><td>1.24</td></tr><tr><td>Triplet [8]</td><td>94.58</td><td>91.48</td><td>93.17</td><td>91.60</td><td>92.71</td><td>1.47</td><td>1.57</td></tr><tr><td>Softmax</td><td>94.18</td><td>91.23</td><td>92.82</td><td>91.42</td><td>92.37</td><td>1.42</td><td>1.51</td></tr><tr><td>RL-RBN(soft) [28]</td><td>94.30</td><td>93.87</td><td>94.13</td><td>94.45</td><td>94.19</td><td>0.25</td><td>1.10</td></tr><tr><td>MBN(soft)</td><td>93.45</td><td>93.82</td><td>93.90</td><td>93.83</td><td>93.75</td><td>0.20</td><td>1.07</td></tr><tr><td>Cosface [27]</td><td>95.12</td><td>92.98</td><td>93.93</td><td>92.93</td><td>93.74</td><td>1.03</td><td>1.45</td></tr><tr><td>RL-RBN(cos) [28]</td><td>95.47</td><td>94.52</td><td>95.15</td><td>95.27</td><td>95.10</td><td>0.41</td><td>1.21</td></tr><tr><td>MBN(cos)</td><td>95.37</td><td>94.43</td><td>95.17</td><td>95.05</td><td>94.99</td><td>0.39</td><td>1.19</td></tr><tr><td>Arcface [18]</td><td>96.18</td><td>93.72</td><td>94.67</td><td>93.98</td><td>94.64</td><td>1.11</td><td>1.65</td></tr><tr><td>RL-RBN(arc) [28]</td><td>96.27</td><td>94.82</td><td>94.68</td><td>95.00</td><td>95.19</td><td>0.73</td><td>1.43</td></tr><tr><td>MBN(arc)</td><td>96.25</td><td>94.85</td><td>95.32</td><td>95.38</td><td>95.45</td><td>0.58</td><td>1.37</td></tr></tbody></table>", "caption": "TABLE VII: Verification accuracy (%) of methods trained with different loss function ([BUPT-Balancedface-4, ResNet34, loss*]).", "list_citation_info": ["[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.", "[8] F. Schroff, D. Kalenichenko, and J. Philbin, \u201cFacenet: A unified embedding for face recognition and clustering,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 815\u2013823.", "[28] M. Wang and W. Deng, \u201cMitigating bias in face recognition using skewness-aware reinforcement learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9322\u20139331.", "[27] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5265\u20135274.", "[72] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman, \u201cFisher vector faces in the wild.\u201d in BMVC, vol. 2, no. 3, 2013, p. 4."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><th colspan=\"8\">The skin tone of IDS-8</th><th rowspan=\"2\">Avg</th><th colspan=\"2\">Fairness</th></tr><tr><td>I</td><td>II</td><td>III</td><td>IV</td><td>V</td><td>VI</td><td>VII</td><td>VIII</td><td>STD</td><td>SER</td></tr><tr><th>Softmax</th><th>94.28</th><th>93.09</th><th>90.70</th><th>90.75</th><th>91.49</th><th>93.31</th><th>91.27</th><th>91.57</th><th>92.06</th><th>1.33</th><th>1.63</th></tr><tr><td>MBN(soft)</td><td>93.61</td><td>93.23</td><td>93.21</td><td>92.78</td><td>92.89</td><td>94.55</td><td>93.82</td><td>93.11</td><td>93.40</td><td>0.58</td><td>1.33</td></tr><tr><th>Cosface [27]</th><th>94.38</th><th>94.58</th><th>92.07</th><th>92.30</th><th>93.68</th><th>94.70</th><th>92.99</th><th>91.77</th><th>93.31</th><th>1.18</th><th>1.55</th></tr><tr><td>MBN(cos)</td><td>94.92</td><td>95.37</td><td>94.28</td><td>94.53</td><td>94.06</td><td>95.09</td><td>94.84</td><td>93.58</td><td>94.58</td><td>0.59</td><td>1.39</td></tr><tr><th>Arcface [18]</th><th>96.03</th><th>95.90</th><th>93.57</th><th>93.36</th><th>93.48</th><th>95.23</th><th>94.28</th><th>93.28</th><th>94.39</th><th>1.16</th><th>1.69</th></tr><tr><td>MBN(arc)</td><td>95.93</td><td>95.57</td><td>94.28</td><td>94.77</td><td>94.57</td><td>95.57</td><td>94.21</td><td>94.46</td><td>94.92</td><td>0.67</td><td>1.42</td></tr></tbody></table>", "caption": "TABLE VIII: Verification accuracy (%) of methods trained with different loss function ([BUPT-Balancedface-8, ResNet34, loss*]).", "list_citation_info": ["[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.", "[27] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5265\u20135274."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Methods</th><th colspan=\"4\">IDS-4</th><th rowspan=\"2\">Avg</th><th colspan=\"2\">Fairness</th></tr><tr><td>I</td><td>II</td><td>III</td><td>IV</td><td>STD</td><td>SER</td></tr><tr><th>Arcface [18]</th><th>97.37</th><th>94.55</th><th>95.68</th><th>93.87</th><th>95.37</th><th>1.53</th><th>2.33</th></tr><tr><td>Re-weight [59]</td><td>96.35</td><td>94.25</td><td>95.32</td><td>93.48</td><td>94.85</td><td>1.25</td><td>1.79</td></tr><tr><th>Adversarial [44]</th><th>96.63</th><th>94.17</th><th>95.27</th><th>93.70</th><th>94.94</th><th>1.30</th><th>1.87</th></tr><tr><td>MBN(arc)</td><td>96.87</td><td>95.63</td><td>96.20</td><td>95.00</td><td>95.93</td><td>0.80</td><td>1.59</td></tr></tbody></table>", "caption": "TABLE IX: Verification accuracy (%) of other debiasing methods trained with Arcface loss ([BUPT-Globalface-4, ResNet34, Arcface]).", "list_citation_info": ["[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.", "[44] M. Alvi, A. Zisserman, and C. Nell\u00e5ker, \u201cTurning a blind eye: Explicit removal of biases and variation from deep neural network embeddings,\u201d in Proceedings of the European Conference on Computer Vision (ECCV) Workshops, 2018, pp. 0\u20130.", "[59] M. Ren, W. Zeng, B. Yang, and R. Urtasun, \u201cLearning to reweight examples for robust deep learning,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 4334\u20134343."]}], "citation_info_to_title": {"[59] M. Ren, W. Zeng, B. Yang, and R. Urtasun, \u201cLearning to reweight examples for robust deep learning,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 4334\u20134343.": "Learning to reweight examples for robust deep learning", "[71] F. Wang, X. Xiang, J. Cheng, and A. L. Yuille, \u201cNormface: l 2 hypersphere embedding for face verification,\u201d in Proceedings of the 25th ACM international conference on Multimedia. ACM, 2017, pp. 1041\u20131049.": "Normface: l 2 hypersphere embedding for face verification", "[17] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u201cSphereface: Deep hypersphere embedding for face recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, vol. 1, 2017.": "Sphereface: Deep Hypersphere Embedding for Face Recognition", "[15] \u201cAmazon\u2019s rekognition tool,\u201d https://aws.amazon.com/rekognition/.": "Amazons Rekognition Tool", "[8] F. Schroff, D. Kalenichenko, and J. Philbin, \u201cFacenet: A unified embedding for face recognition and clustering,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 815\u2013823.": "Facenet: A unified embedding for face recognition and clustering", "[25] D. Yi, Z. Lei, S. Liao, and S. Z. Li, \u201cLearning face representation from scratch,\u201d arXiv preprint arXiv:1411.7923, 2014.": "Learning face representation from scratch", "[13] \u201cFace++ research toolkit,\u201d www.faceplusplus.com.": "Face++ Research Toolkit", "[9] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, \u201cLabeled faces in the wild: A database for studying face recognition in unconstrained environments,\u201d Technical Report 07-49, University of Massachusetts, Amherst, Tech. Rep., 2007.": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "[68] S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel, R. Chellappa, and D. W. Jacobs, \u201cFrontal to profile face verification in the wild,\u201d in 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2016, pp. 1\u20139.": "Frontal to profile face verification in the wild", "[28] M. Wang and W. Deng, \u201cMitigating bias in face recognition using skewness-aware reinforcement learning,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 9322\u20139331.": "Mitigating bias in face recognition using skewness-aware reinforcement learning", "[69] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou, \u201cAgedb: the first manually collected, in-the-wild age database,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017, pp. 51\u201359.": "Agedb: The First Manually Collected, In-the-Wild Age Database", "[12] \u201cMicrosoft azure,\u201d https://www.azure.cn.": "Microsoft Azure", "[19] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVggface2: A dataset for recognising faces across pose and age,\u201d in 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018). IEEE, 2018, pp. 67\u201374.": "Vggface2: A dataset for recognising faces across pose and age", "[72] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman, \u201cFisher vector faces in the wild.\u201d in BMVC, vol. 2, no. 3, 2013, p. 4.": "Fisher vector faces in the wild", "[16] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, \u201cA discriminative feature learning approach for deep face recognition,\u201d in European conference on computer vision. Springer, 2016, pp. 499\u2013515.": "A discriminative feature learning approach for deep face recognition", "[27] H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu, \u201cCosface: Large margin cosine loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 5265\u20135274.": "Cosface: Large margin cosine loss for deep face recognition", "[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.": "Arcface: Additive Angular Margin Loss for Deep Face Recognition", "[44] M. Alvi, A. Zisserman, and C. Nell\u00e5ker, \u201cTurning a blind eye: Explicit removal of biases and variation from deep neural network embeddings,\u201d in Proceedings of the European Conference on Computer Vision (ECCV) Workshops, 2018, pp. 0\u20130.": "Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings", "[14] \u201cBaidu cloud vision api,\u201d http://ai.baidu.com.": "Baidu Cloud Vision API"}, "source_title_to_arxiv_id": {"Vggface2: A dataset for recognising faces across pose and age": "1710.08092", "Arcface: Additive Angular Margin Loss for Deep Face Recognition": "1801.07698"}}