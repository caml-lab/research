{"title": "An Efficient Multi-Scale Fusion Network for 3D Organ at Risk (OAR) Segmentation", "abstract": "Accurate segmentation of organs-at-risks (OARs) is a precursor for optimizing\nradiation therapy planning. Existing deep learning-based multi-scale fusion\narchitectures have demonstrated a tremendous capacity for 2D medical image\nsegmentation. The key to their success is aggregating global context and\nmaintaining high resolution representations. However, when translated into 3D\nsegmentation problems, existing multi-scale fusion architectures might\nunderperform due to their heavy computation overhead and substantial data diet.\nTo address this issue, we propose a new OAR segmentation framework, called\nOARFocalFuseNet, which fuses multi-scale features and employs focal modulation\nfor capturing global-local context across multiple scales. Each resolution\nstream is enriched with features from different resolution scales, and\nmulti-scale information is aggregated to model diverse contextual ranges. As a\nresult, feature representations are further boosted. The comprehensive\ncomparisons in our experimental setup with OAR segmentation as well as\nmulti-organ segmentation show that our proposed OARFocalFuseNet outperforms the\nrecent state-of-the-art methods on publicly available OpenKBP datasets and\nSynapse multi-organ segmentation. Both of the proposed methods (3D-MSF and\nOARFocalFuseNet) showed promising performance in terms of standard evaluation\nmetrics. Our best performing method (OARFocalFuseNet) obtained a dice\ncoefficient of 0.7995 and hausdorff distance of 5.1435 on OpenKBP datasets and\ndice coefficient of 0.8137 on Synapse multi-organ segmentation dataset.", "authors": ["Abhishek Srivastava", "Debesh Jha", "Elif Keles", "Bulent Aydogan", "Mohamed Abazeed", "Ulas Bagci"], "published_date": "2022_08_15", "pdf_url": "http://arxiv.org/pdf/2208.07417v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Mean DSC</td><td>Mean <abbr>HD</abbr></td><td>Mean <abbr>ASD</abbr></td><td>Brainstem</td><td>Spinal cord</td><td>Right parotid</td><td>Left parotid</td><td>Mandible</td></tr><tr><td>UNet-3D [3]</td><td>0.7781</td><td>5.0662</td><td>0.6839</td><td>0.7941</td><td>0.7444</td><td>0.7416</td><td>0.7601</td><td>0.8503</td></tr><tr><td>AttUNet [12]</td><td>0.7811</td><td>4.9981</td><td>0.6024</td><td>0.7919</td><td>0.7439</td><td>0.7394</td><td>0.7691</td><td>0.8611</td></tr><tr><td>DynUNet [13]</td><td>0.7931</td><td>6.3316</td><td>0.6460</td><td>0.7958</td><td>0.7521</td><td>0.7696</td><td>0.7731</td><td>0.8747</td></tr><tr><td>UNETR [6]</td><td>0.7810</td><td>9.5582</td><td>0.8527</td><td>0.7791</td><td>0.7339</td><td>0.7610</td><td>0.7692</td><td>0.8616</td></tr><tr><td>SwinUNETR [5]</td><td>0.7986</td><td>6.7520</td><td>0.6409</td><td>0.8085</td><td>0.7604</td><td>0.7706</td><td>0.7723</td><td>0.8813</td></tr><tr><td>3D-MSF(Ours)</td><td>0.7870</td><td>5.5505</td><td>0.6152</td><td>0.7903</td><td>0.7478</td><td>0.7498</td><td>0.7816</td><td>0.8655</td></tr><tr><td>OARFocalFuseNet(Ours)</td><td>0.7995</td><td>5.1435</td><td>0.5743</td><td>0.8031</td><td>0.7402</td><td>0.7725</td><td>0.7987</td><td>0.8832</td></tr></tbody></table>", "caption": "TABLE I: Comparisons of the results on OpenKBP dataset. We report the mean <abbr>DSC</abbr>, mean <abbr>HD</abbr>, mean <abbr>ASD</abbr>, and <abbr>DSC</abbr> for each organ.", "list_citation_info": ["[6] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, \u201cUnetr: Transformers for 3d medical image segmentation,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 574\u2013584.", "[5] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. Roth, and D. Xu, \u201cSwin unetr: Swin transformers for semantic segmentation of brain tumors in mri images,\u201d arXiv preprint arXiv:2201.01266, 2022.", "[12] O. Oktay et al., \u201cAttention u-net: Learning where to look for the pancreas,\u201d arXiv preprint arXiv:1804.03999, 2018.", "[13] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, \u201cnnu-net: a self-configuring method for deep learning-based biomedical image segmentation,\u201d Nature methods, vol. 18, no. 2, pp. 203\u2013211, 2021.", "[3] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in Proceedings of the MICCAI, 2015, pp. 234\u2013241."]}, {"table": "<table><tbody><tr><td colspan=\"2\">Framework</td><td rowspan=\"2\">Mean DSC</td><td rowspan=\"2\">Aorta</td><td rowspan=\"2\">Gallbladder</td><td rowspan=\"2\">Kidney (L)</td><td rowspan=\"2\">Kidney (R)</td><td rowspan=\"2\">Liver</td><td rowspan=\"2\">Pancreas</td><td rowspan=\"2\">Spleen</td><td rowspan=\"2\">Stomach</td></tr><tr><td>Encoder</td><td>Decoder</td></tr><tr><td colspan=\"2\">V-Net [14]</td><td>0.6881</td><td>0.7534</td><td>0.5187</td><td>0.7710</td><td>0.8075</td><td>0.8784</td><td>0.4005</td><td>0.8056</td><td>0.5698</td></tr><tr><td colspan=\"2\">DARR [15]</td><td>0.6977</td><td>0.7474</td><td>0.5377</td><td>0.7231</td><td>0.7324</td><td>0.9408</td><td>0.5418</td><td>0.8990</td><td>0.4596</td></tr><tr><td>R50</td><td>U-Net [3]</td><td>0.7468</td><td>0.8418</td><td>0.6284</td><td>0.7919</td><td>0.7129</td><td>0.9335</td><td>0.4823</td><td>0.8441</td><td>0.7392</td></tr><tr><td>R50</td><td>AttnUNet [12]</td><td>0.7557</td><td>0.5592</td><td>0.6391</td><td>0.7920</td><td>0.7271</td><td>0.9356</td><td>0.4937</td><td>0.8719</td><td>0.7495</td></tr><tr><td>ViT [4]</td><td>None</td><td>0.6150</td><td>0.4438</td><td>0.3959</td><td>0.6746</td><td>0.6294</td><td>0.8921</td><td>0.4314</td><td>0.7545</td><td>0.6978</td></tr><tr><td>ViT [4]</td><td>CUP</td><td>0.6786</td><td>0.7019</td><td>0.4510</td><td>0.7470</td><td>0.6740</td><td>0.9132</td><td>0.4200</td><td>0.8175</td><td>0.7044</td></tr><tr><td>R50-ViT [4]</td><td>CUP</td><td>0.7129</td><td>0.7373</td><td>0.5513</td><td>0.7580</td><td>0.7220</td><td>0.9151</td><td>0.4599</td><td>0.8199</td><td>0.7395</td></tr><tr><td colspan=\"2\">TransUNet [7]</td><td>0.7748</td><td>0.8723</td><td>0.6313</td><td>0.8187</td><td>0.7702</td><td>0.9408</td><td>0.5586</td><td>0.8508</td><td>0.7562</td></tr><tr><td colspan=\"2\">3D-MSF(Ours)</td><td>0.8084</td><td>0.8883</td><td>0.6968</td><td>0.8382</td><td>0.8204</td><td>0.9343</td><td>0.6460</td><td>0.8705</td><td>0.7723</td></tr><tr><td colspan=\"2\">OARFocalFuseNet(Ours)</td><td>0.8137</td><td>0.9085</td><td>0.6752</td><td>0.8424</td><td>0.8237</td><td>0.9496</td><td>0.6808</td><td>0.8698</td><td>0.7595</td></tr></tbody></table>", "caption": "TABLE II: Comparisons of results on Synapse multi-organ CT dataset (average dice score and dice score for each organ).", "list_citation_info": ["[15] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, \u201cDomain adaptive relational reasoning for 3d multi-organ segmentation,\u201d in Proceedings of the MICCAI, 2020, pp. 656\u2013666.", "[7] J. Chen et al., \u201cTransunet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.", "[12] O. Oktay et al., \u201cAttention u-net: Learning where to look for the pancreas,\u201d arXiv preprint arXiv:1804.03999, 2018.", "[14] F. Milletari, N. Navab, and S.-A. Ahmadi, \u201cV-net: Fully convolutional neural networks for volumetric medical image segmentation,\u201d in Proceedings of the fourth international conference on 3D vision (3DV), 2016, pp. 565\u2013571.", "[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.", "[3] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in Proceedings of the MICCAI, 2015, pp. 234\u2013241."]}], "citation_info_to_title": {"[5] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. Roth, and D. Xu, \u201cSwin unetr: Swin transformers for semantic segmentation of brain tumors in mri images,\u201d arXiv preprint arXiv:2201.01266, 2022.": "Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images", "[14] F. Milletari, N. Navab, and S.-A. Ahmadi, \u201cV-net: Fully convolutional neural networks for volumetric medical image segmentation,\u201d in Proceedings of the fourth international conference on 3D vision (3DV), 2016, pp. 565\u2013571.": "V-net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "[12] O. Oktay et al., \u201cAttention u-net: Learning where to look for the pancreas,\u201d arXiv preprint arXiv:1804.03999, 2018.": "Attention U-Net: Learning Where to Look for the Pancreas", "[6] A. Hatamizadeh, Y. Tang, V. Nath, D. Yang, A. Myronenko, B. Landman, H. R. Roth, and D. Xu, \u201cUnetr: Transformers for 3d medical image segmentation,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022, pp. 574\u2013584.": "Unetr: Transformers for 3d medical image segmentation", "[7] J. Chen et al., \u201cTransunet: Transformers make strong encoders for medical image segmentation,\u201d arXiv preprint arXiv:2102.04306, 2021.": "Transunet: Transformers make strong encoders for medical image segmentation", "[13] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, \u201cnnu-net: a self-configuring method for deep learning-based biomedical image segmentation,\u201d Nature methods, vol. 18, no. 2, pp. 203\u2013211, 2021.": "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation", "[15] S. Fu, Y. Lu, Y. Wang, Y. Zhou, W. Shen, E. Fishman, and A. Yuille, \u201cDomain adaptive relational reasoning for 3d multi-organ segmentation,\u201d in Proceedings of the MICCAI, 2020, pp. 656\u2013666.": "Domain Adaptive Relational Reasoning for 3D Multi-Organ Segmentation", "[3] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in Proceedings of the MICCAI, 2015, pp. 234\u2013241.": "U-net: Convolutional networks for biomedical image segmentation", "[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.": "An image is worth 16x16 words: Transformers for image recognition at scale"}, "source_title_to_arxiv_id": {"Attention U-Net: Learning Where to Look for the Pancreas": "1804.03999", "Transunet: Transformers make strong encoders for medical image segmentation": "2102.04306"}}