{"title": "OneRing: A Simple Method for Source-free Open-partial Domain Adaptation", "abstract": "In this paper, we investigate Source-free Open-partial Domain Adaptation\n(SF-OPDA), which addresses the situation where there exist both domain and\ncategory shifts between source and target domains. Under the SF-OPDA setting,\nwhich aims to address data privacy concerns, the model cannot access source\ndata anymore during target adaptation. We propose a novel training scheme to\nlearn a (n+1)-way classifier to predict the n source classes and the unknown\nclass, where samples of only known source categories are available for\ntraining. Furthermore, for target adaptation, we simply adopt a weighted\nentropy minimization to adapt the source pretrained model to the unlabeled\ntarget domain without source data. In experiments, we show our simple method\nsurpasses current OPDA approaches which demand source data during adaptation.\nWhen augmented with a closed-set domain adaptation approach during target\nadaptation, our source-free method further outperforms the current\nstate-of-the-art OPDA method by 2.5%, 7.2% and 13% on Office-31, Office-Home\nand VisDA respectively.", "authors": ["Shiqi Yang", "Yaxing Wang", "Kai Wang", "Shangling Jui", "Joost van de Weijer"], "published_date": "2022_06_07", "pdf_url": "http://arxiv.org/pdf/2206.03600v2", "list_table_and_caption": [{"table": "<table><thead><tr><th>Metric</th><th>ERM koltchinskii2011oracle </th><th> +CM zhu2022crossmatch </th><th>ADA volpi2018generalizing </th><th> +CM zhu2022crossmatch </th><th>MEADA zhao2020maximum </th><th> +CM zhu2022crossmatch </th><th>One Ring-S</th></tr></thead><tbody><tr><td>Acc</td><td>79.8</td><td>78.3</td><td>80.1</td><td>78.6</td><td>80.3</td><td>79.0</td><td>67.3</td></tr><tr><td>UNK</td><td>27.0</td><td>37.6</td><td>25.2</td><td>34.5</td><td>25.1</td><td>41.1</td><td>77.0</td></tr><tr><td>OS*</td><td>85.1</td><td>82.4</td><td>85.6</td><td>83.0</td><td>85.8</td><td>82.8</td><td>66.3</td></tr><tr><td>H</td><td>40.7</td><td>51.1</td><td>38.7</td><td>48.5</td><td>38.6</td><td>54.7</td><td>71.3</td></tr></tbody></table>", "caption": "Table 2: Accuracy (%) on Office-31 dataset using ResNet-18 as backbone. Open-set Single Domain Generalization where |\\mathcal{C}_{s}|=10, |\\mathcal{C}_{t}|=21, |\\mathcal{C}_{s}\\cap\\mathcal{C}_{t}|=10. The second highest H score is underlined.", "list_citation_info": ["(82) Ronghang Zhu and Sheng Li. Crossmatch: Cross-classifier consistency regularization for open-set single domain generalization. In ICLR, 2022.", "(79) Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas. Maximum-entropy adversarial data augmentation for improved generalization and robustness. NeurIPS, 33:14435\u201314447, 2020.", "(20) Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems: \u00c9cole D\u2019\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVIII-2008, volume 2033. Springer Science & Business Media, 2011.", "(68) Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. NeurIPS, 31, 2018."]}, {"table": "<table><tbody><tr><th>Method</th><th>SF</th><td>P2R</td><td>R2P</td><td>P2S</td><td>S2P</td><td>R2S</td><td>S2R</td><td>Avg</td></tr><tr><th>OSBP saito2018open </th><th>\u2717</th><td>33.6</td><td>33.0</td><td>30.6</td><td>30.5</td><td>30.6</td><td>33.7</td><td>32.0</td></tr><tr><th>DANCE saito2020universal </th><th>\u2717</th><td>21.0</td><td>47.3</td><td>37.0</td><td>27.7</td><td>46.7</td><td>21.0</td><td>33.5</td></tr><tr><th>UAN you2019universal </th><th>\u2717</th><td>41.9</td><td>43.6</td><td>39.1</td><td>38.9</td><td>38.7</td><td>43.7</td><td>41.0</td></tr><tr><th>CMU fu2020learning </th><th>\u2717</th><td>50.8</td><td>52.2</td><td>45.1</td><td>44.8</td><td>45.6</td><td>51.0</td><td>48.3</td></tr><tr><th>DCC li2021domain </th><th>\u2717</th><td>56.9</td><td>50.3</td><td>43.7</td><td>44.9</td><td>43.3</td><td>56.2</td><td>49.2</td></tr><tr><th>OVANet saito2021ovanet </th><th>\u2717</th><td>56.0</td><td>51.7</td><td>47.1</td><td>47.4</td><td>44.9</td><td>57.2</td><td>50.7</td></tr><tr><th>One Ring-S</th><th></th><td>59.1</td><td>42.9</td><td>43.8</td><td>35.5</td><td>39.5</td><td>52.9</td><td>45.6</td></tr><tr><th>One Ring</th><th>\u2713</th><td>57.9</td><td>52.0</td><td>46.5</td><td>49.6</td><td>44.1</td><td>57.8</td><td>51.3</td></tr></tbody></table>", "caption": "Table 7: H-score (%) on DomainNet using ResNet-50 as backbone. Universal Domain adaptation where |\\mathcal{C}_{s}|=200, |\\mathcal{C}_{t}|=295, |\\mathcal{C}_{s}\\cap\\mathcal{C}_{t}|=150. The second highest H score is underlined. SF indicates whether source-free.", "list_citation_info": ["(53) Kuniaki Saito and Kate Saenko. Ovanet: One-vs-all network for universal domain adaptation. In ICCV, pages 9000\u20139009, 2021.", "(76) Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain adaptation. In CVPR, pages 2720\u20132729, 2019.", "(13) Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Learning to detect open classes for universal domain adaptation. In ECCV, pages 567\u2013583. Springer, 2020.", "(52) Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through self supervision. NeurIPS, 33, 2020.", "(55) Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by backpropagation. In ECCV, pages 153\u2013168, 2018.", "(29) Guangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for universal domain adaptation. In CVPR, pages 9757\u20139766, 2021."]}, {"table": "<table><tbody><tr><td>Method</td><td>SVHN</td><td>CIFAR10</td><td>CIFAR + 10</td><td>CIFAR + 50</td></tr><tr><td>OSRCI neal2018open </td><td>89.9</td><td>87.2</td><td>91.1</td><td>90.3</td></tr><tr><td>(ARPL + CS) chen2021adversarial </td><td>96.8</td><td>93.9</td><td>98.1</td><td>96.7</td></tr><tr><td>MSP vaze2022openset </td><td>96.0</td><td>90.1</td><td>95.6</td><td>94.0</td></tr><tr><td>MLS vaze2022openset </td><td>97.1</td><td>93.6</td><td>97.9</td><td>96.5</td></tr><tr><td>One Ring-S</td><td>97.3</td><td>93.7</td><td>97.8</td><td>96.2</td></tr></tbody></table>", "caption": "Table 8: Results of Open-set Recognition task. All results indicate the area under the Receiver-Operator curve (AUROC) averaged over five \u2018known/unknown\u2019 class splits. All methods are augmented with improved optimization strategies from vaze2022openset . Results are taken from vaze2022openset .", "list_citation_info": ["(43) Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with counterfactual images. In ECCV, pages 613\u2013628, 2018.", "(65) Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need. In ICLR, 2022.", "(5) Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. IEEE TPAMI, 2021."]}], "citation_info_to_title": {"(43) Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, and Fuxin Li. Open set learning with counterfactual images. In ECCV, pages 613\u2013628, 2018.": "Open set learning with counterfactual images", "(82) Ronghang Zhu and Sheng Li. Crossmatch: Cross-classifier consistency regularization for open-set single domain generalization. In ICLR, 2022.": "Crossmatch: Cross-classifier consistency regularization for open-set single domain generalization", "(68) Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. NeurIPS, 31, 2018.": "Generalizing to Unseen Domains via Adversarial Data Augmentation", "(52) Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through self supervision. NeurIPS, 33, 2020.": "Universal Domain Adaptation through Self Supervision", "(20) Vladimir Koltchinskii. Oracle inequalities in empirical risk minimization and sparse recovery problems: \u00c9cole D\u2019\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVIII-2008, volume 2033. Springer Science & Business Media, 2011.": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: \u00c9cole D\u2019\u00c9t\u00e9 de Probabilit\u00e9s de Saint-Flour XXXVIII-2008, Volume 2033", "(53) Kuniaki Saito and Kate Saenko. Ovanet: One-vs-all network for universal domain adaptation. In ICCV, pages 9000\u20139009, 2021.": "Ovanet: One-vs-all network for universal domain adaptation", "(79) Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas. Maximum-entropy adversarial data augmentation for improved generalization and robustness. NeurIPS, 33:14435\u201314447, 2020.": "Maximum-entropy adversarial data augmentation for improved generalization and robustness", "(13) Bo Fu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Learning to detect open classes for universal domain adaptation. In ECCV, pages 567\u2013583. Springer, 2020.": "Learning to detect open classes for universal domain adaptation", "(29) Guangrui Li, Guoliang Kang, Yi Zhu, Yunchao Wei, and Yi Yang. Domain consensus clustering for universal domain adaptation. In CVPR, pages 9757\u20139766, 2021.": "Domain Consensus Clustering for Universal Domain Adaptation", "(65) Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need. In ICLR, 2022.": "Open-set recognition: A good closed-set classifier is all you need", "(5) Guangyao Chen, Peixi Peng, Xiangqian Wang, and Yonghong Tian. Adversarial reciprocal points learning for open set recognition. IEEE TPAMI, 2021.": "Adversarial Reciprocal Points Learning for Open Set Recognition", "(76) Kaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain adaptation. In CVPR, pages 2720\u20132729, 2019.": "Universal Domain Adaptation", "(55) Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by backpropagation. In ECCV, pages 153\u2013168, 2018.": "Open set domain adaptation by backpropagation"}, "source_title_to_arxiv_id": {"Open-set recognition: A good closed-set classifier is all you need": "2110.06207"}}