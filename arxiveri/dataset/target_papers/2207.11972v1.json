{"title": "TransCL: Transformer Makes Strong and Flexible Compressive Learning", "abstract": "Compressive learning (CL) is an emerging framework that integrates signal\nacquisition via compressed sensing (CS) and machine learning for inference\ntasks directly on a small number of measurements. It can be a promising\nalternative to classical image-domain methods and enjoys great advantages in\nmemory saving and computational efficiency. However, previous attempts on CL\nare not only limited to a fixed CS ratio, which lacks flexibility, but also\nlimited to MNIST/CIFAR-like datasets and do not scale to complex real-world\nhigh-resolution (HR) data or vision tasks. In this paper, a novel\ntransformer-based compressive learning framework on large-scale images with\narbitrary CS ratios, dubbed TransCL, is proposed. Specifically, TransCL first\nutilizes the strategy of learnable block-based compressed sensing and proposes\na flexible linear projection strategy to enable CL to be performed on\nlarge-scale images in an efficient block-by-block manner with arbitrary CS\nratios. Then, regarding CS measurements from all blocks as a sequence, a pure\ntransformer-based backbone is deployed to perform vision tasks with various\ntask-oriented heads. Our sufficient analysis presents that TransCL exhibits\nstrong resistance to interference and robust adaptability to arbitrary CS\nratios. Extensive experiments for complex HR data demonstrate that the proposed\nTransCL can achieve state-of-the-art performance in image classification and\nsemantic segmentation tasks. In particular, TransCL with a CS ratio of $10\\%$\ncan obtain almost the same performance as when operating directly on the\noriginal data and can still obtain satisfying performance even with an\nextremely low CS ratio of $1\\%$. The source codes of our proposed TransCL is\navailable at \\url{https://github.com/MC-E/TransCL/}.", "authors": ["Chong Mou", "Jian Zhang"], "published_date": "2022_07_25", "pdf_url": "http://arxiv.org/pdf/2207.11972v1", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td> Meas.</td><td>Param./Flops</td><td>Top-1 ACC</td></tr><tr><td>ResNet-18[72]</td><td>3\\times50176</td><td>12M/1.9G</td><td>69.83</td></tr><tr><td>ResNet-50[72]</td><td>3\\times50176</td><td>25M/4.2G</td><td>76.20</td></tr><tr><td>ResNet-101[72]</td><td>3\\times50176</td><td>45M/7.8G</td><td>77.41</td></tr><tr><td>ResNet-152[72]</td><td>3\\times50176</td><td>60M/11.58G</td><td>78.33</td></tr><tr><td>ViT-B-16[49]</td><td>3\\times147456</td><td>86M/49.3G</td><td>83.97</td></tr><tr><td>ViT-B-32[49]</td><td>3\\times147456</td><td>88M/12.3G</td><td>81.28</td></tr><tr><td>PVT-Large[53]</td><td>3\\times50176</td><td>62M/9.9G</td><td>81.71</td></tr><tr><td>DeiT-B \\uparrow 384[50]</td><td>3\\times147456</td><td>86M/49.4G</td><td>83.12</td></tr><tr><td>Twins[73]</td><td>3\\times50176</td><td>99M/14.8G</td><td>83.70</td></tr><tr><td>VCL-T-10</td><td>3\\times14745</td><td>589M/50.8</td><td>67.59</td></tr><tr><td>VCL-T-1</td><td>3\\times1474</td><td>136M/49.6</td><td>59.89</td></tr><tr><td>MCL-T-10</td><td>3\\times14770</td><td>87M/49.4</td><td>74.07</td></tr><tr><td>MCL-T-1</td><td>3\\times1477</td><td>86M/49.3</td><td>68.92</td></tr><tr><td>TransCL-16,32-10</td><td>3\\times14745</td><td>86,88M/49.3,12.3G</td><td>83.86,81.82</td></tr><tr><td>TransCL-16,32-5</td><td>3\\times7372</td><td>86,88M/49.3,12.3G</td><td>83.05,81.53</td></tr><tr><td>TransCL-16,32-2.5</td><td>3\\times3686</td><td>86,88M/49.3,12.3G</td><td>81.34,80.50</td></tr><tr><td>TransCL-16,32-1</td><td>3\\times1474</td><td>86,88M/49.3,12.3G</td><td>78.86,78.00</td></tr><tr><td>TransCL-16-10<img/></td><td>3\\times14745</td><td>86M/49.3G</td><td>83.29</td></tr><tr><td>TransCL-16-5<img/></td><td>3\\times7372</td><td>86M/49.3G</td><td>82.46</td></tr><tr><td>TransCL-16-2.5<img/></td><td>3\\times3686</td><td>86M/49.3G</td><td>81.24</td></tr><tr><td>TransCL-16-1<img/></td><td>3\\times1474</td><td>86M/49.3G</td><td>78.58</td></tr></table>", "caption": "TABLE I: Image classification performance on validation dataset of ImageNet. \u201cTop-1 ACC\u201d denotes the top-1 (\\%) accuracy. Performances of different methods, model parameters, and the number of measurements for each image are reported.", "list_citation_info": ["[50] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou, \u201cTraining data-efficient image transformers & distillation through attention,\u201d in Proceedings of the International Conference on Machine Learning (ICML), 2021, pp. 10\u2009347\u201310\u2009357.", "[73] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen, \u201cTwins: Revisiting the design of spatial attention in vision transformers,\u201d in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2021, pp. 9355\u20139366.", "[72] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.", "[53] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, \u201cPyramid vision transformer: A versatile backbone for dense prediction without convolutions,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021, pp. 568\u2013578.", "[49] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in Proceedings of the International Conference on Learning Representations (ICLR), 2020."]}, {"table": "<table><tr><td> CSRatio </td><td>Method</td><td>Backbone</td><td>Meas.</td><td> CIFAR-10 </td><td> CIFAR-100 </td></tr><tr><td rowspan=\"6\">25\\%</td><td>VCL [66]</td><td>ResNet110</td><td>768</td><td>78.56</td><td>53.03</td></tr><tr><td>MCL [30]</td><td>ResNet110</td><td>760</td><td>89.22</td><td>67.21</td></tr><tr><td>VCL-T</td><td>ViT-B-32</td><td>768</td><td>93.80</td><td>78.65</td></tr><tr><td>MCL-T</td><td>ViT-B-32</td><td>760</td><td>94.51</td><td>78.60</td></tr><tr><td>TransCL-16</td><td>ViT-B-16</td><td>760</td><td>94.35</td><td>81.45</td></tr><tr><td>TransCL-32</td><td>ViT-B-32</td><td>760</td><td>95.18</td><td>82.33</td></tr><tr><td rowspan=\"6\">10\\%</td><td>VCL [66]</td><td>ResNet110</td><td>306</td><td>67.65</td><td>47.90</td></tr><tr><td>MCL [30]</td><td>ResNet110</td><td>308</td><td>84.74</td><td>60.30</td></tr><tr><td>MCLwP [31]</td><td>AllCNN [74]</td><td>308</td><td>85.84</td><td>59.83</td></tr><tr><td>VCL-T</td><td>ViT-B-32</td><td>306</td><td>89.22</td><td>70.22</td></tr><tr><td>MCL-T</td><td>ViT-B-32</td><td>308</td><td>90.16</td><td>67.53</td></tr><tr><td>TransCL-16</td><td>ViT-B-16</td><td>306</td><td>90.66</td><td>72.80</td></tr><tr><td></td><td>TransCL-32</td><td>ViT-B-32</td><td>306</td><td>91.57</td><td>72.11</td></tr><tr><td rowspan=\"6\">1.8\\%</td><td>VCL [66]</td><td>ResNet110</td><td>54</td><td>61.96</td><td>41.03</td></tr><tr><td>MCL [30]</td><td>ResNet110</td><td>54</td><td>64.14</td><td>33.67</td></tr><tr><td>VCL-T</td><td>ViT-B-32</td><td>54</td><td>61.96</td><td>41.86</td></tr><tr><td>MCL-T</td><td>ViT-B-32</td><td>54</td><td>62.28</td><td>36.43</td></tr><tr><td>TransCL-16</td><td>ViT-B-16</td><td>54</td><td>68.44</td><td>42.66</td></tr><tr><td>TransCL-32</td><td>ViT-B-32</td><td>54</td><td>69.60</td><td>42.75</td></tr></table>", "caption": "TABLE II: Image classification performance on validation datasets of CIFAR-10 and CIFAR-100 [33]. Top-1 (\\%) accuracy is presented below.", "list_citation_info": ["[66] E. Zisselman, A. Adler, and M. Elad, \u201cCompressed learning for image classification: A deep neural network approach,\u201d in Handbook of Numerical Analysis, 2018, pp. 3\u201317.", "[31] D. T. Tran, M. Gabbouj, and A. Iosifidis, \u201cMultilinear compressive learning with prior knowledge,\u201d arXiv preprint arXiv:2002.07203, 2020.", "[33] A. Krizhevsky, G. Hinton et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009. [Online]. Available: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf", "[30] D. T. Tran, M. Yama\u00e7, A. Degerli, M. Gabbouj, and A. Iosifidis, \u201cMultilinear compressive learning,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 32, pp. 1512\u20131524, 2021.", "[74] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, \u201cStriving for simplicity: The all convolutional net,\u201d arXiv preprint arXiv:1412.6806, 2014."]}, {"table": "<table><tr><td>Methods (Backbone)</td><td>Meas.</td><td>Param./Flops</td><td>mIoU/Acc</td></tr><tr><td>PSP[75] (ResNet-269)</td><td>3\\times223729</td><td>73M/246G</td><td>44.94/81.69</td></tr><tr><td>GFF[76] (ResNet-101)</td><td>3\\times262144</td><td>141M/811G</td><td>45.33/-</td></tr><tr><td>APC[77] (ResNet-101)</td><td>3\\times331776</td><td>76M/357G</td><td>45.38/-</td></tr><tr><td>Twins[73] (SVT-L)</td><td>3\\times262144</td><td>133M/297G</td><td>48.80/-</td></tr><tr><td>SETR[54] (ViT-L-16)</td><td>3\\times262144</td><td>309M/316G</td><td>50.28/83.46</td></tr><tr><td>Ours (TransCL-16-10)</td><td>3\\times26214</td><td>309M/316G</td><td>50.00/83.51</td></tr><tr><td>Ours (TransCL-16-5)</td><td>3\\times13107</td><td>309M/316G</td><td>49.96/83.44</td></tr><tr><td>Ours (TransCL-16-2.5)</td><td>3\\times6553</td><td>309M/316G</td><td>48.60/83.13</td></tr><tr><td>Ours (TransCL-16-1)</td><td>\\mathbf{3}\\mathbf{\\times}\\mathbf{2621}</td><td>309M/316G</td><td>46.57/82.14</td></tr></table>", "caption": "TABLE III: Quantitative comparison of semantic segmentation task on ADE20K [68] dataset. Performances of different methods and the number of measurements for each image are reported.", "list_citation_info": ["[54] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr, and L. Zhang, \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 6881\u20136890.", "[76] X. Li, H. Zhao, L. Han, Y. Tong, S. Tan, and K. Yang, \u201cGated fully fusion for semantic segmentation,\u201d in Proceedings of the AAAI conference on artificial intelligence (AAAI), 2020, pp. 11\u2009418\u201311\u2009425.", "[73] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen, \u201cTwins: Revisiting the design of spatial attention in vision transformers,\u201d in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2021, pp. 9355\u20139366.", "[75] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2881\u20132890.", "[77] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, \u201cAdaptive pyramid context network for semantic segmentation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7519\u20137528.", "[68] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba, \u201cSemantic understanding of scenes through the ade20k dataset,\u201d International Journal of Computer Vision (IJCV), vol. 127, pp. 302\u2013321, 2019."]}, {"table": "<table><tr><td>Methods (Backbone)</td><td>Meas.</td><td>Param./Flops</td><td>mIoU</td></tr><tr><td>PSP [75] (ResNet-101)</td><td>3\\times508369</td><td>73M/554G</td><td>78.50</td></tr><tr><td>CCNet [78] (ResNet-101)</td><td>3\\times591361</td><td>71M/698G</td><td>80.20</td></tr><tr><td>GFF [76] (ResNet-101)</td><td>3\\times746496</td><td>141M/2305G</td><td>80.40</td></tr><tr><td>SETR [54] (ViT-L-16)</td><td>3\\times589825</td><td>309M/818G</td><td>82.15</td></tr><tr><td>Ours (TransCL-16-10)</td><td>3\\times58982</td><td>309M/818G</td><td>82.02</td></tr><tr><td>Ours (TransCL-16-5)</td><td>3\\times29491</td><td>309M/818G</td><td>81.73</td></tr><tr><td>Ours (TransCL-16-2.5)</td><td>3\\times14745</td><td>309M/818G</td><td>81.53</td></tr><tr><td>Ours (TransCL-16-1)</td><td>3\\times5898</td><td>309M/818G</td><td>79.29</td></tr></table>", "caption": "TABLE IV: Quantitative comparison of semantic segmentation task on Cityscapes [70] dataset. Performances of different methods and the number of measurements for each image are reported.", "list_citation_info": ["[54] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr, and L. Zhang, \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 6881\u20136890.", "[76] X. Li, H. Zhao, L. Han, Y. Tong, S. Tan, and K. Yang, \u201cGated fully fusion for semantic segmentation,\u201d in Proceedings of the AAAI conference on artificial intelligence (AAAI), 2020, pp. 11\u2009418\u201311\u2009425.", "[75] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2881\u20132890.", "[78] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, \u201cCcnet: Criss-cross attention for semantic segmentation,\u201d in Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2019, pp. 603\u2013612.", "[70] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset for semantic urban scene understanding,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 3213\u20133223."]}, {"table": "<table><tr><td>Methods (Backbone)</td><td>Meas.</td><td>Param./Flops</td><td>mIoU</td></tr><tr><td>PSP [75] (ResNet-101)</td><td>3\\times223729</td><td>73M/246G</td><td>47.80</td></tr><tr><td>GFF [76] (ResNet-101)</td><td>3\\times262144</td><td>141M/811G</td><td>54.20</td></tr><tr><td>APC [77] (ResNet-101)</td><td>3\\times262144</td><td>76M/282G</td><td>54.70</td></tr><tr><td>SETR [54] (ViT-L-16)</td><td>3\\times230400</td><td>309M/281G</td><td>55.83</td></tr><tr><td>Ours (TransCL-16-10)</td><td>3\\times23040</td><td>309M/281G</td><td>55.52</td></tr><tr><td>Ours (TransCL-16-5)</td><td>3\\times11520</td><td>309M/281G</td><td>55.29</td></tr><tr><td>Ours (TransCL-16-2.5)</td><td>3\\times5760</td><td>309M/281G</td><td>54.03</td></tr><tr><td>Ours (TransCL-16-1)</td><td>3\\times2304</td><td>309M/281G</td><td>51.83</td></tr></table>", "caption": "TABLE V: Quantitative comparison of semantic segmentation task on Pascal Context [69] dataset. Performances of different methods and the number of measurements for each image are reported.", "list_citation_info": ["[54] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr, and L. Zhang, \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 6881\u20136890.", "[76] X. Li, H. Zhao, L. Han, Y. Tong, S. Tan, and K. Yang, \u201cGated fully fusion for semantic segmentation,\u201d in Proceedings of the AAAI conference on artificial intelligence (AAAI), 2020, pp. 11\u2009418\u201311\u2009425.", "[75] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2881\u20132890.", "[77] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, \u201cAdaptive pyramid context network for semantic segmentation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7519\u20137528.", "[69] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille, \u201cThe role of context for object detection and semantic segmentation in the wild,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 891\u2013898."]}, {"table": "<table><tr><td>Methods</td><td>ResCL-152</td><td>ResCL-101</td><td>ResCL-50</td><td>DensCL-201</td><td>DensCL-121</td><td>Two Stages</td><td>TransCL-16</td><td>TransCL-32</td></tr><tr><td> Speed(ms/image) </td><td>16.9</td><td>11.9</td><td>7.5</td><td>11.9</td><td>8.4</td><td>7.4+25.2</td><td>30.15</td><td>\\mathbf{7.4}</td></tr><tr><td> PeakMemory (GB) </td><td>2.2</td><td>1.9</td><td>1.5</td><td>2.3</td><td>1.7</td><td>\\max(1.5,{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\pgfsys@color@rgb@stroke{1}{0}{0}\\pgfsys@color@rgb@fill{1}{0}{0}6.8})</td><td>3.2</td><td>1.5</td></tr><tr><td>Param. (M)</td><td>60</td><td>45</td><td>25</td><td>20</td><td>8</td><td>88+0.6</td><td>86</td><td>88</td></tr><tr><td>Flops (G)</td><td>30.1</td><td>23.0</td><td>12.1</td><td>12.8</td><td>8.4</td><td>12.3+112</td><td>49.3</td><td>12.3</td></tr><tr><td>Accuracy (\\%)</td><td>75.22</td><td>71.77</td><td>70.19</td><td>72.05</td><td>69.23</td><td>80.65</td><td>83.36</td><td>81.82</td></tr></table>", "caption": "TABLE VII: Comparison of complexity of various strategies to infer a 384\\times 384 image and the top-1 (\\%) accuracy on ImageNet [67] validation set. The additional complexity produced in measurement reconstruction stage is highlighted in red.", "list_citation_info": ["[67] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \u201cImagenet large scale visual recognition challenge,\u201d International Journal of Computer Vision, vol. 115, pp. 211\u2013252, 2015."]}, {"table": "<table><tr><td></td><td>VCL [66]</td><td>MCL [30]</td><td>TransCL (Ours)</td></tr><tr><td>Parameters</td><td>2.0\\times 10^{9}</td><td>1.1\\times 10^{5}</td><td>\\mathbf{2.7\\times 10^{3}}</td></tr><tr><td>Flops</td><td>1.3\\times 10^{9}</td><td>4.6\\times 10^{7}</td><td>\\mathbf{2.6\\times 10^{6}}</td></tr></table>", "caption": "TABLE IX: Complexity comparison of various CL methods to sample a 384\\times 384 color image, with the CS ratio being 1\\%.", "list_citation_info": ["[66] E. Zisselman, A. Adler, and M. Elad, \u201cCompressed learning for image classification: A deep neural network approach,\u201d in Handbook of Numerical Analysis, 2018, pp. 3\u201317.", "[30] D. T. Tran, M. Yama\u00e7, A. Degerli, M. Gabbouj, and A. Iosifidis, \u201cMultilinear compressive learning,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 32, pp. 1512\u20131524, 2021."]}], "citation_info_to_title": {"[54] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr, and L. Zhang, \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 6881\u20136890.": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers", "[33] A. Krizhevsky, G. Hinton et al., \u201cLearning multiple layers of features from tiny images,\u201d 2009. [Online]. Available: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf": "Learning multiple layers of features from tiny images", "[72] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778.": "Deep residual learning for image recognition", "[69] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille, \u201cThe role of context for object detection and semantic segmentation in the wild,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014, pp. 891\u2013898.": "The role of context for object detection and semantic segmentation in the wild", "[30] D. T. Tran, M. Yama\u00e7, A. Degerli, M. Gabbouj, and A. Iosifidis, \u201cMultilinear compressive learning,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 32, pp. 1512\u20131524, 2021.": "Multilinear Compressive Learning", "[73] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen, \u201cTwins: Revisiting the design of spatial attention in vision transformers,\u201d in Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2021, pp. 9355\u20139366.": "Twins: Revisiting the design of spatial attention in vision transformers", "[70] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset for semantic urban scene understanding,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 3213\u20133223.": "The Cityscapes Dataset for Semantic Urban Scene Understanding", "[75] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2881\u20132890.": "Pyramid scene parsing network", "[78] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, \u201cCcnet: Criss-cross attention for semantic segmentation,\u201d in Proceedings of the IEEE International Conference on Computer Vision (CVPR), 2019, pp. 603\u2013612.": "Ccnet: Criss-cross attention for semantic segmentation", "[50] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou, \u201cTraining data-efficient image transformers & distillation through attention,\u201d in Proceedings of the International Conference on Machine Learning (ICML), 2021, pp. 10\u2009347\u201310\u2009357.": "Training data-efficient image transformers & distillation through attention", "[68] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba, \u201cSemantic understanding of scenes through the ade20k dataset,\u201d International Journal of Computer Vision (IJCV), vol. 127, pp. 302\u2013321, 2019.": "Semantic understanding of scenes through the ade20k dataset", "[67] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., \u201cImagenet large scale visual recognition challenge,\u201d International Journal of Computer Vision, vol. 115, pp. 211\u2013252, 2015.": "Imagenet large scale visual recognition challenge", "[31] D. T. Tran, M. Gabbouj, and A. Iosifidis, \u201cMultilinear compressive learning with prior knowledge,\u201d arXiv preprint arXiv:2002.07203, 2020.": "Multilinear compressive learning with prior knowledge", "[49] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in Proceedings of the International Conference on Learning Representations (ICLR), 2020.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[66] E. Zisselman, A. Adler, and M. Elad, \u201cCompressed learning for image classification: A deep neural network approach,\u201d in Handbook of Numerical Analysis, 2018, pp. 3\u201317.": "Compressed learning for image classification: A deep neural network approach", "[53] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao, \u201cPyramid vision transformer: A versatile backbone for dense prediction without convolutions,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021, pp. 568\u2013578.": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions", "[76] X. Li, H. Zhao, L. Han, Y. Tong, S. Tan, and K. Yang, \u201cGated fully fusion for semantic segmentation,\u201d in Proceedings of the AAAI conference on artificial intelligence (AAAI), 2020, pp. 11\u2009418\u201311\u2009425.": "Gated Fully Fusion for Semantic Segmentation", "[74] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller, \u201cStriving for simplicity: The all convolutional net,\u201d arXiv preprint arXiv:1412.6806, 2014.": "Striving for simplicity: The all convolutional net", "[77] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, \u201cAdaptive pyramid context network for semantic segmentation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 7519\u20137528.": "Adaptive Pyramid Context Network for Semantic Segmentation"}, "source_title_to_arxiv_id": {"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers": "2012.15840", "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions": "2102.12122"}}