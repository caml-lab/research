{"title": "Mix-up Self-Supervised Learning for Contrast-agnostic Applications", "abstract": "Contrastive self-supervised learning has attracted significant research\nattention recently. It learns effective visual representations from unlabeled\ndata by embedding augmented views of the same image close to each other while\npushing away embeddings of different images. Despite its great success on\nImageNet classification, COCO object detection, etc., its performance degrades\non contrast-agnostic applications, e.g., medical image classification, where\nall images are visually similar to each other. This creates difficulties in\noptimizing the embedding space as the distance between images is rather small.\nTo solve this issue, we present the first mix-up self-supervised learning\nframework for contrast-agnostic applications. We address the low variance\nacross images based on cross-domain mix-up and build the pretext task based on\ntwo synergistic objectives: image reconstruction and transparency prediction.\nExperimental results on two benchmark datasets validate the effectiveness of\nour method, where an improvement of 2.5% ~ 7.4% in top-1 accuracy was obtained\ncompared to existing self-supervised learning methods.", "authors": ["Yichen Zhang", "Yifang Yin", "Ying Zhang", "Roger Zimmermann"], "published_date": "2022_04_02", "pdf_url": "http://arxiv.org/pdf/2204.00901v1", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\"><p>Pre-trained model</p></td><td rowspan=\"2\">Mix</td><td colspan=\"5\">Top-1 Accuracy (%)</td></tr><tr><td><p>Eosinophil</p></td><td><p>Lymphocyte</p></td><td><p>Monocyte</p></td><td><p>Neutrophil</p></td><td><p>Average</p></td></tr><tr><td><p>Rand</p></td><td>\\times</td><td><p>76.40</p></td><td><p>99.19</p></td><td><p>75.02</p></td><td><p>94.55</p></td><td><p>86.29 (-)</p></td></tr><tr><td><p>SimCLR [2]</p></td><td>\\times</td><td><p>80.26</p></td><td><p>99.84</p></td><td><p>83.39</p></td><td><p>91.99</p></td><td><p>88.86 (+1.94%)</p></td></tr><tr><td><p>Dino [6]</p></td><td>\\times</td><td><p>87.60</p></td><td><p>100.00</p></td><td><p>74.68</p></td><td><p>94.39</p></td><td><p>89.22 (+2.93%)</p></td></tr><tr><td><p>MoCo [3]</p></td><td>\\times</td><td><p>83.79</p></td><td><p>100.00</p></td><td><p>77.10</p></td><td>96.15</td><td><p>89.26 (+2.97%)</p></td></tr><tr><td><p>Rotation [13]</p></td><td>\\times</td><td><p>87.48</p></td><td><p>100.00</p></td><td><p>75.48</p></td><td><p>94.07</p></td><td><p>89.26 (+2.97%)</p></td></tr><tr><td><p>ImageNet-self</p></td><td>\\times</td><td><p>85.07</p></td><td><p>100.00</p></td><td><p>78.71</p></td><td><p>93.91</p></td><td><p>89.43 (+3.14%)</p></td></tr><tr><td><p>Mixup+SimCLR</p></td><td>\\surd</td><td>88.28</td><td><p>100.00</p></td><td><p>75.02</p></td><td><p>93.59</p></td><td><p>89.22 (+2.93%)</p></td></tr><tr><td><p>MixCo [5]</p></td><td>\\surd</td><td><p>86.19</p></td><td><p>100.00</p></td><td><p>77.90</p></td><td><p>94.71</p></td><td><p>89.71 (+3.42%)</p></td></tr><tr><td>MixSSL (Ours)</td><td>\\surd</td><td><p>87.69</p></td><td>100.00</td><td>85.81</td><td><p>95.35</p></td><td><p>92.19 (+5.90%)</p></td></tr></table>", "caption": "Table 1: Performance comparison between our framework and state-of-the-art methods on the blood cell dataset.", "list_citation_info": ["[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in ICML, 2020, pp. 1597\u20131607.", "[3] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in CVPR, 2020, pp. 9729\u20139738.", "[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin, \u201cEmerging properties in self-supervised vision transformers,\u201d arXiv preprint arXiv:2104.14294, 2021.", "[13] Spyros Gidaris, Praveer Singh, and Nikos Komodakis, \u201cUnsupervised representation learning by predicting image rotations,\u201d arXiv preprint arXiv:1803.07728, 2018.", "[5] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun, \u201cMixco: Mix-up contrastive learning for visual representation,\u201d arXiv preprint arXiv:2010.06300, 2020."]}, {"table": "<table><tr><td rowspan=\"2\"><p>Pre-trained model</p></td><td rowspan=\"2\">Mix</td><td colspan=\"5\">Metrics (%)</td></tr><tr><td><p>Precision</p></td><td><p>Recall</p></td><td><p>F1 score</p></td><td><p>ROC-AUC</p></td><td><p>Accuracy</p></td></tr><tr><td><p>Rand</p></td><td>\\times</td><td><p>63.32</p></td><td><p>68.51</p></td><td><p>65.88</p></td><td><p>75.70</p></td><td><p>70.45 (-)</p></td></tr><tr><td><p>SimCLR [2]</p></td><td>\\times</td><td><p>71.82</p></td><td><p>83.69</p></td><td><p>77.21</p></td><td><p>82.26</p></td><td><p>79.55 (+9.10)</p></td></tr><tr><td><p>Dino [6]</p></td><td>\\times</td><td><p>75.31</p></td><td><p>87.72</p></td><td><p>81.06</p></td><td><p>84.47</p></td><td><p>82.95 (+12.50)</p></td></tr><tr><td><p>MoCo [3]</p></td><td>\\times</td><td><p>73.12</p></td><td>93.26</td><td><p>81.91</p></td><td><p>85.90</p></td><td><p>82.95 (+12.50)</p></td></tr><tr><td><p>Rotation [13]</p></td><td>\\times</td><td><p>81.46</p></td><td><p>78.17</p></td><td><p>79.71</p></td><td><p>86.12</p></td><td><p>83.52 (+13.07)</p></td></tr><tr><td><p>ImageNet-self</p></td><td>\\times</td><td><p>79.72</p></td><td><p>80.86</p></td><td><p>80.35</p></td><td><p>86.21</p></td><td><p>83.52 (+13.07)</p></td></tr><tr><td><p>Mixup+SimCLR</p></td><td>\u2713</td><td><p>75.31</p></td><td><p>79.55</p></td><td><p>77.36</p></td><td><p>83.91</p></td><td><p>80.68 (+10.23)</p></td></tr><tr><td><p>MixCo [5]</p></td><td>\u2713</td><td>81.72</td><td><p>79.53</p></td><td><p>80.61</p></td><td><p>86.92</p></td><td><p>84.09 (+13.64)</p></td></tr><tr><td>MixSSL (Ours)</td><td>\u2713</td><td><p>81.27</p></td><td><p>89.02</p></td><td>85.01</td><td>89.06</td><td><p>86.93 (+16.48)</p></td></tr></table>", "caption": "Table 2: Performance comparison between our framework and state-of-the-art methods on the cervix dataset.", "list_citation_info": ["[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in ICML, 2020, pp. 1597\u20131607.", "[3] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in CVPR, 2020, pp. 9729\u20139738.", "[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin, \u201cEmerging properties in self-supervised vision transformers,\u201d arXiv preprint arXiv:2104.14294, 2021.", "[13] Spyros Gidaris, Praveer Singh, and Nikos Komodakis, \u201cUnsupervised representation learning by predicting image rotations,\u201d arXiv preprint arXiv:1803.07728, 2018.", "[5] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun, \u201cMixco: Mix-up contrastive learning for visual representation,\u201d arXiv preprint arXiv:2010.06300, 2020."]}], "citation_info_to_title": {"[3] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in CVPR, 2020, pp. 9729\u20139738.": "Momentum contrast for unsupervised visual representation learning", "[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton, \u201cA simple framework for contrastive learning of visual representations,\u201d in ICML, 2020, pp. 1597\u20131607.": "A simple framework for contrastive learning of visual representations", "[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin, \u201cEmerging properties in self-supervised vision transformers,\u201d arXiv preprint arXiv:2104.14294, 2021.": "Emerging properties in self-supervised vision transformers", "[5] Sungnyun Kim, Gihun Lee, Sangmin Bae, and Se-Young Yun, \u201cMixco: Mix-up contrastive learning for visual representation,\u201d arXiv preprint arXiv:2010.06300, 2020.": "Mixco: Mix-up contrastive learning for visual representation", "[13] Spyros Gidaris, Praveer Singh, and Nikos Komodakis, \u201cUnsupervised representation learning by predicting image rotations,\u201d arXiv preprint arXiv:1803.07728, 2018.": "Unsupervised representation learning by predicting image rotations"}, "source_title_to_arxiv_id": {"Emerging properties in self-supervised vision transformers": "2104.14294"}}