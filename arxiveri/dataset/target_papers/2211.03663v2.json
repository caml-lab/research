{"title": "Generalizable Re-Identification from Videos with Cycle Association", "abstract": "In this paper, we are interested in learning a generalizable person\nre-identification (re-ID) representation from unlabeled videos. Compared with\n1) the popular unsupervised re-ID setting where the training and test sets are\ntypically under the same domain, and 2) the popular domain generalization (DG)\nre-ID setting where the training samples are labeled, our novel scenario\ncombines their key challenges: the training samples are unlabeled, and\ncollected form various domains which do no align with the test domain. In other\nwords, we aim to learn a representation in an unsupervised manner and directly\nuse the learned representation for re-ID in novel domains. To fulfill this\ngoal, we make two main contributions: First, we propose Cycle Association\n(CycAs), a scalable self-supervised learning method for re-ID with low training\ncomplexity; and second, we construct a large-scale unlabeled re-ID dataset\nnamed LMP-video, tailored for the proposed method. Specifically, CycAs learns\nre-ID features by enforcing cycle consistency of instance association between\ntemporally successive video frame pairs, and the training cost is merely linear\nto the data size, making large-scale training possible. On the other hand, the\nLMP-video dataset is extremely large, containing 50 million unlabeled person\nimages cropped from over 10K Youtube videos, therefore is sufficient to serve\nas fertile soil for self-supervised learning. Trained on LMP-video, we show\nthat CycAs learns good generalization towards novel domains. The achieved\nresults sometimes even outperform supervised domain generalizable models.\nRemarkably, CycAs achieves 82.2% Rank-1 on Market-1501 and 49.0% Rank-1 on\nMSMT17 with zero human annotation, surpassing state-of-the-art supervised DG\nre-ID methods. Moreover, we also demonstrate the superiority of CycAs under the\ncanonical unsupervised re-ID and the pretrain-and-finetune scenarios.", "authors": ["Zhongdao Wang", "Zhaopeng Dou", "Jingwei Zhang", "Liang Zheng", "Yifan Sun", "Yali Li", "Shengjin Wang"], "published_date": "2022_11_07", "pdf_url": "http://arxiv.org/pdf/2211.03663v2", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Setting</th><th>Train w/</th><th>Adapt. w/</th><th>Test</th><th>Representative</th></tr><tr><th>ID label</th><th>ID label</th><th>domain</th><th>existing methods</th></tr></thead><tbody><tr><td>Supervised re-ID</td><td>\u2713</td><td>No Adapt.</td><td>seen</td><td>IDE [1], PCB [10], ArcFace [2], CircleLoss [4], MGN [11]</td></tr><tr><td>Unsupervised re-ID</td><td>\u2717</td><td>No Adapt.</td><td>seen</td><td>PUL [12], BUC [13], UGA [14], SpCL [15],</td></tr><tr><td>UDA re-ID</td><td>\u2713</td><td>\u2717</td><td>seen</td><td>SPGAN [16], PTGAN [17], MMT [18], SpCL [15]</td></tr><tr><td>Unsup. Pretrain\\rightarrow Sup.</td><td>\u2717</td><td>\u2713</td><td>seen</td><td>LUP [9], ViT-pretrain [19]</td></tr><tr><td>Unsup. Pretrain\\rightarrow Unsup.</td><td>\u2717</td><td>\u2717</td><td>seen</td><td>LUP [9], ViT-pretrain [19]</td></tr><tr><td>DG re-ID</td><td>\u2713</td><td>No Adapt.</td><td>unseen</td><td>DIMN [20], SNR [21]. EasyReID [22], RaMoE [23]</td></tr><tr><td>Unsupervised DG re-ID (this work)</td><td>\u2717</td><td>No Adapt.</td><td>unseen</td><td>CycAs (conference version of this work), TrackContrast [24]</td></tr></tbody></table>", "caption": "TABLE I: Comparison of existing re-ID experimental setups. ", "list_citation_info": ["[12] H. Fan, L. Zheng, C. Yan, and Y. Yang, \u201cUnsupervised person re-identification: Clustering and fine-tuning,\u201d ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 14, no. 4, p. 83, 2018.", "[2] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in CVPR, 2019.", "[18] Y. Ge, D. Chen, and H. Li, \u201cMutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification,\u201d ICLR, 2020.", "[16] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, \u201cImage-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification,\u201d in CVPR, 2018.", "[22] J. Jia, Q. Ruan, and T. M. Hospedales, \u201cFrustratingly easy person re-identification: Generalizing person re-id in practice,\u201d BMVC, 2019.", "[11] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, \u201cLearning discriminative features with multiple granularities for person re-identification,\u201d in ACM MM, 2018.", "[4] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y. Wei, \u201cCircle loss: A unified perspective of pair similarity optimization,\u201d in CVPR, 2020.", "[20] J. Song, Y. Yang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, \u201cGeneralizable person re-identification by domain-invariant mapping network,\u201d in CVPR, 2019.", "[14] J. Wu, Y. Yang, H. Liu, S. Liao, Z. Lei, and S. Z. Li, \u201cUnsupervised graph association for person re-identification,\u201d in ICCV, 2019.", "[23] Y. Dai, X. Li, J. Liu, Z. Tong, and L.-Y. Duan, \u201cGeneralizable person re-identification with relevance-aware mixture of experts,\u201d in CVPR, 2021.", "[1] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, \u201cScalable person re-identification: A benchmark,\u201d in CVPR, 2015.", "[19] H. Luo, P. Wang, Y. Xu, F. Ding, Y. Zhou, F. Wang, H. Li, and R. Jin, \u201cSelf-supervised pre-training for transformer-based person re-identification,\u201d arXiv preprint arXiv:2111.12084, 2021.", "[13] Y. Lin, X. Dong, L. Zheng, Y. Yan, and Y. Yang, \u201cA bottom-up clustering approach to unsupervised person re-identification,\u201d in AAAI, 2019.", "[9] D. Fu, D. Chen, J. Bao, H. Yang, L. Yuan, L. Zhang, H. Li, and D. Chen, \u201cUnsupervised pre-training for person re-identification,\u201d in CVPR, 2021.", "[15] Y. Ge, F. Zhu, D. Chen, R. Zhao et al., \u201cSelf-paced contrastive learning with hybrid memory for domain adaptive object re-id,\u201d NeurIPS, 2020.", "[17] L. Wei, S. Zhang, W. Gao, and Q. Tian, \u201cPerson transfer gan to bridge domain gap for person re-identification,\u201d in CVPR, 2018.", "[21] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang, \u201cStyle normalization and restitution for generalizable person re-identification,\u201d in CVPR, 2020.", "[10] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, \u201cBeyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline),\u201d in ECCV, 2018.", "[24] W. Huang, Y. Bai, Q. Ren, X. Zhao, M. Feng, and Y. Wang, \u201cLarge-scale unsupervised person re-identification with contrastive learning,\u201d arXiv preprint arXiv:2105.07914, 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><td rowspan=\"3\">Sup.</td><td colspan=\"8\">Protocol-1</td><td colspan=\"8\">Protocol-2</td></tr><tr><td colspan=\"2\">Market [1]</td><td colspan=\"2\">Duke [26]</td><td colspan=\"2\">CUHK03 [27]</td><td colspan=\"2\">MSMT17 [17]</td><td colspan=\"2\">PRID [44]</td><td colspan=\"2\">GRID [45]</td><td colspan=\"2\">VIPeR [46]</td><td colspan=\"2\">iLIDS [47]</td></tr><tr><td>mAP</td><td>R-1</td><td>mAP</td><td>R-1</td><td>mAP</td><td>R-1</td><td>mAP</td><td>R-1</td><td>mAP</td><td>R-1</td><td>mAP</td><td>R-1</td><td>mAP</td><td>R-1</td><td>mAP</td><td>R-1</td></tr><tr><th>Agg_Align [48]</th><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>25.5</td><td>17.2</td><td>24.7</td><td>15.9</td><td>52.9</td><td>42.8</td><td>74.7</td><td>63.8</td></tr><tr><th>CrossGrad [49]</th><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>28.2</td><td>18.8</td><td>16.0</td><td>9.0</td><td>30.4</td><td>20.9</td><td>61.3</td><td>49.7</td></tr><tr><th>PPA [50]</th><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>45.3</td><td>31.9</td><td>38.0</td><td>26.9</td><td>54.5</td><td>45.1</td><td>72.7</td><td>64.5</td></tr><tr><th>DIMN [20]</th><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>52.0</td><td>39.2</td><td>41.1</td><td>29.3</td><td>60.1</td><td>51.2</td><td>78.4</td><td>70.2</td></tr><tr><th>SNR [21]</th><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>66.5</td><td>52.1</td><td>47.7</td><td>40.2</td><td>61.3</td><td>52.9</td><td>89.9</td><td>84.1</td></tr><tr><th>M{}^{3}L[51]</th><td>\u2713</td><td>50.2</td><td>75.9</td><td>51.1</td><td>69.2</td><td>32.1</td><td>33.1</td><td>12.9</td><td>33.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DML [23]</th><td>\u2713</td><td>49.9</td><td>75.4</td><td>49.4</td><td>65.8</td><td>32.6</td><td>32.9</td><td>9.9</td><td>24.5</td><td>60.4</td><td>47.3</td><td>49.0</td><td>39.4</td><td>58.0</td><td>49.2</td><td>84.0</td><td>77.3</td></tr><tr><th>RaMoE [23]</th><td>\u2713</td><td>56.5</td><td>82.0</td><td>56.9</td><td>73.6</td><td>35.5</td><td>36.6</td><td>13.5</td><td>34.1</td><td>67.3</td><td>57.7</td><td>54.2</td><td>46.8</td><td>64.6</td><td>56.6</td><td>90.2</td><td>85.0</td></tr><tr><th>TrackContrast{}^{\\dagger} [24]</th><td>\u2717</td><td>36.2</td><td>72.7</td><td>31.2</td><td>51.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CycAs_R50{}^{\\dagger}</th><td>\u2717</td><td>57.5</td><td>80.3</td><td>51.8</td><td>65.8</td><td>26.5</td><td>25.8</td><td>20.2</td><td>43.9</td><td>67.7</td><td>58.8</td><td>62.3</td><td>52.5</td><td>66.0</td><td>57.3</td><td>90.4</td><td>85.2</td></tr><tr><th>CycAs_Swin{}^{\\dagger}</th><td>\u2717</td><td>60.4</td><td>82.2</td><td>56.2</td><td>69.1</td><td>37.1</td><td>36.3</td><td>24.1</td><td>49.0</td><td>79.2</td><td>71.5</td><td>66.4</td><td>55.8</td><td>68.8</td><td>60.2</td><td>91.3</td><td>87.4</td></tr></tbody></table>", "caption": "TABLE II: Comparison to existing domain generalizable re-ID methods. The superscript {}^{\\dagger} indicates unsupervised methods, otherwise supervised. For supervised methods, Protocol-1 requires leave-one-out evaluation, i.e. train on three of the four datasets (Market [1], Duke [26], CUHK03 [27] and MSMT17 [17]) and test on the ramaining one. In Protocol-2, the training set is the joint set of Market+Duke+CUHK02+CUHK03+CUHK-SYSU [43], and the test sets are four small re-ID datasets.Sup. indicates supervised or unsupervised method.Unsupervised methods employ self-collected training data and are evaluated on test sets in each protocol, respectively.", "list_citation_info": ["[48] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang, and J. Sun, \u201cAlignedreid: Surpassing human-level performance in person re-identification,\u201d arXiv preprint arXiv:1711.08184, 2017.", "[44] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, \u201cPerson re-identification by descriptive and discriminative classification,\u201d in Scandinavian conference on Image analysis. Springer, 2011, pp. 91\u2013102.", "[17] L. Wei, S. Zhang, W. Gao, and Q. Tian, \u201cPerson transfer gan to bridge domain gap for person re-identification,\u201d in CVPR, 2018.", "[50] S. Qiao, C. Liu, W. Shen, and A. L. Yuille, \u201cFew-shot image recognition by predicting parameters from activations,\u201d in CVPR, 2018.", "[20] J. Song, Y. Yang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, \u201cGeneralizable person re-identification by domain-invariant mapping network,\u201d in CVPR, 2019.", "[27] W. Li, R. Zhao, T. Xiao, and X. Wang, \u201cDeepreid: Deep filter pairing neural network for person re-identification,\u201d in CVPR, 2014.", "[21] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang, \u201cStyle normalization and restitution for generalizable person re-identification,\u201d in CVPR, 2020.", "[45] C. C. Loy, T. Xiang, and S. Gong, \u201cTime-delayed correlation analysis for multi-camera activity understanding,\u201d 2010.", "[46] D. Gray and H. Tao, \u201cViewpoint invariant pedestrian recognition with an ensemble of localized features,\u201d in ECCV, 2008.", "[49] S. Shankar, V. Piratla, S. Chakrabarti, S. Chaudhuri, P. Jyothi, and S. Sarawagi, \u201cGeneralizing across domains via cross-gradient training,\u201d 2018.", "[47] T. Wang, S. Gong, X. Zhu, and S. Wang, \u201cPerson re-identification by video ranking,\u201d in ECCV, 2014.", "[23] Y. Dai, X. Li, J. Liu, Z. Tong, and L.-Y. Duan, \u201cGeneralizable person re-identification with relevance-aware mixture of experts,\u201d in CVPR, 2021.", "[51] Y. Zhao, Z. Zhong, F. Yang, Z. Luo, Y. Lin, S. Li, and N. Sebe, \u201cLearning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification,\u201d in CVPR, 2021.", "[26] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, \u201cPerformance measures and a data set for multi-target, multi-camera tracking,\u201d in ECCV, 2016.", "[43] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, \u201cEnd-to-end deep learning for person search,\u201d arXiv preprint arXiv:1604.01850, 2016.", "[1] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, \u201cScalable person re-identification: A benchmark,\u201d in CVPR, 2015.", "[24] W. Huang, Y. Bai, Q. Ren, X. Zhao, M. Feng, and Y. Wang, \u201cLarge-scale unsupervised person re-identification with contrastive learning,\u201d arXiv preprint arXiv:2105.07914, 2021."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Category</td><td rowspan=\"2\">Require</td><td>Training</td><td colspan=\"2\">Market [1]</td><td colspan=\"2\">Duke [26]</td><td colspan=\"2\">CUHK03 [27]</td><td colspan=\"2\">MSMT17 [17]</td></tr><tr><td>Complexity</td><td>R1</td><td>mAP</td><td>R1</td><td>mAP</td><td>R1</td><td>mAP</td><td>R1</td><td>mAP</td></tr><tr><td>SPGAN [53]</td><td>UDA</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N)</td><td>51.5</td><td>22.8</td><td>41.1</td><td>22.3</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SPGAN+LMP [53]</td><td>UDA</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N)</td><td>57.7</td><td>26.7</td><td>46.4</td><td>26.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TJ-AIDL [54]</td><td>UDA</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N)</td><td>58.2</td><td>26.5</td><td>44.3</td><td>23.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>HHL [55]</td><td>UDA</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N)</td><td>62.2</td><td>31.4</td><td>46.9</td><td>27.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ECN [56]</td><td>UDA</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N)</td><td>75.1</td><td>43.0</td><td>63.3</td><td>40.4</td><td>-</td><td>-</td><td>30.2</td><td>10.2</td></tr><tr><td>TAUDL [57]</td><td>Tracklet</td><td>MOT</td><td>\\mathcal{O}(N)</td><td>63.7</td><td>41.2</td><td>61.7</td><td>43.5</td><td>44.7</td><td>31.2</td><td>28.4</td><td>12.5</td></tr><tr><td>UTAL [58]</td><td>Tracklet</td><td>MOT</td><td>\\mathcal{O}(N)</td><td>69.2</td><td>46.2</td><td>62.3</td><td>44.6</td><td>56.3</td><td>42.3</td><td>31.4</td><td>13.1</td></tr><tr><td>UGA [14]</td><td>Tracklet</td><td>MOT</td><td>\\mathcal{O}(N)</td><td>87.2</td><td>70.3</td><td>75.0</td><td>53.3</td><td>-</td><td>-</td><td>49.5</td><td>21.7</td></tr><tr><td>PUL [12]</td><td>Clustering</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N\\log N)\\sim\\mathcal{O}(N^{2})</td><td>44.7</td><td>20.1</td><td>30.4</td><td>16.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CAMEL [59]</td><td>Clustering</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N\\log N)\\sim\\mathcal{O}(N^{2})</td><td>54.5</td><td>26.3</td><td>-</td><td>-</td><td>39.4</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BUC [13]</td><td>Clustering</td><td>None</td><td>\\mathcal{O}(N\\log N)\\sim\\mathcal{O}(N^{2})</td><td>66.2</td><td>38.3</td><td>47.4</td><td>27.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CDS [60]</td><td>Clustering</td><td>Sup. Pretrain.</td><td>\\mathcal{O}(N\\log N)\\sim\\mathcal{O}(N^{2})</td><td>71.6</td><td>39.9</td><td>67.2</td><td>42.7</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SpCL [15]</td><td>Clustering</td><td>None</td><td>\\mathcal{O}(N\\log N)\\sim\\mathcal{O}(N^{2})</td><td>88.1</td><td>73.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>42.3</td><td>19.1</td></tr><tr><td>CycAs{}^{\\texttt{asy}}</td><td>Self-Sup</td><td>None</td><td>\\mathcal{O}(N)</td><td>84.8</td><td>64.8</td><td>77.9</td><td>60.1</td><td>47.4</td><td>41.0</td><td>50.1</td><td>26.7</td></tr><tr><td>CycAs{}^{\\texttt{sym}}</td><td>-</td><td>None</td><td>\\mathcal{O}(N)</td><td>88.1</td><td>71.8</td><td>79.7</td><td>62.7</td><td>56.4</td><td>49.6</td><td>61.8</td><td>36.2</td></tr><tr><td>IDE</td><td>Supervised</td><td>Label</td><td>\\mathcal{O}(N)</td><td>89.2</td><td>73.9</td><td>80.0</td><td>63.1</td><td>54.2</td><td>47.2</td><td>60.2</td><td>33.4</td></tr></tbody></table>", "caption": "TABLE III: Comparison with state-of-the-art methods on four standard datasets. Note all the methods starts from a ImageNet pretrained model. The requirement Pretain and None refers to whether pretraining on labeled re-ID datasets is needed. Training complexity refers to space and time complexity of the training process w.r.t. the dataset size N.CycAs{}^{\\texttt{asy}} is our method, and CycAs{}^{\\texttt{sym}} refers to an upper bound of our method.", "list_citation_info": ["[12] H. Fan, L. Zheng, C. Yan, and Y. Yang, \u201cUnsupervised person re-identification: Clustering and fine-tuning,\u201d ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 14, no. 4, p. 83, 2018.", "[15] Y. Ge, F. Zhu, D. Chen, R. Zhao et al., \u201cSelf-paced contrastive learning with hybrid memory for domain adaptive object re-id,\u201d NeurIPS, 2020.", "[17] L. Wei, S. Zhang, W. Gao, and Q. Tian, \u201cPerson transfer gan to bridge domain gap for person re-identification,\u201d in CVPR, 2018.", "[55] Z. Zhong, L. Zheng, S. Li, and Y. Yang, \u201cGeneralizing a person retrieval model hetero-and homogeneously,\u201d in ECCV, 2018.", "[59] H.-X. Yu, A. Wu, and W.-S. Zheng, \u201cCross-view asymmetric metric learning for unsupervised person re-identification,\u201d in ICCV, 2017, pp. 994\u20131002.", "[53] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, \u201cImage-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification,\u201d in CVPR, 2018.", "[27] W. Li, R. Zhao, T. Xiao, and X. Wang, \u201cDeepreid: Deep filter pairing neural network for person re-identification,\u201d in CVPR, 2014.", "[57] M. Li, X. Zhu, and S. Gong, \u201cUnsupervised person re-identification by deep learning tracklet association,\u201d in ECCV, 2018.", "[56] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, \u201cInvariance matters: Exemplar memory for domain adaptive person re-identification,\u201d in CVPR, 2019.", "[14] J. Wu, Y. Yang, H. Liu, S. Liao, Z. Lei, and S. Z. Li, \u201cUnsupervised graph association for person re-identification,\u201d in ICCV, 2019.", "[54] J. Wang, X. Zhu, S. Gong, and W. Li, \u201cTransferable joint attribute-identity deep learning for unsupervised person re-identification,\u201d in CVPR, 2018.", "[13] Y. Lin, X. Dong, L. Zheng, Y. Yan, and Y. Yang, \u201cA bottom-up clustering approach to unsupervised person re-identification,\u201d in AAAI, 2019.", "[60] J. Wu, S. Liao, X. Wang, Y. Yang, S. Z. Li et al., \u201cClustering and dynamic sampling based unsupervised domain adaptation for person re-identification,\u201d in ICME, 2019.", "[26] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, \u201cPerformance measures and a data set for multi-target, multi-camera tracking,\u201d in ECCV, 2016.", "[1] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, \u201cScalable person re-identification: A benchmark,\u201d in CVPR, 2015.", "[58] \u2014\u2014, \u201cUnsupervised tracklet person re-identification,\u201d IEEE transactions on pattern analysis and machine intelligence, 2019."]}, {"table": "<table><tbody><tr><th>Method</th><td>Market</td><td>Duke</td><td>CUHK03</td><td>MSMT17</td></tr><tr><th>PCB [10]</th><td>81.6/93.8</td><td>69.2/83.3</td><td>57.5/63.7</td><td>-</td></tr><tr><th>BOT [30]</th><td>85.9/94.5</td><td>76.4/86.4</td><td>-</td><td>-</td></tr><tr><th>DSA [64]</th><td>87.6/95.7</td><td>74.3/86.2</td><td>75.2/78.9</td><td>-</td></tr><tr><th>ABDNet [65]</th><td>88.3/95.6</td><td>78.6/89.0</td><td>-</td><td>60.8/82.3</td></tr><tr><th>OSNet [66]</th><td>84.9/94.8</td><td>73.5/88.6</td><td>67.8/72.3</td><td>52.9/78.7</td></tr><tr><th>MHN [67]</th><td>85.0/95.1</td><td>77.2/89.1</td><td>72.4/77.2</td><td>-</td></tr><tr><th>BDB [68]</th><td>86.7/95.3</td><td>76.0/89.0</td><td>76.7/79.4</td><td>-</td></tr><tr><th>SONA [sona]</th><td>88.8/95.6</td><td>78.3/89.4</td><td>79.2/81.8</td><td>-</td></tr><tr><th>GCP [69]</th><td>88.9/95.2</td><td>78.6/87.9</td><td>75.6/77.9</td><td>-</td></tr><tr><th>ISP [70]</th><td>88.6/95.3</td><td>80.0/89.6</td><td>74.1/76.5</td><td>-</td></tr><tr><th>GASM [71]</th><td>84.7/95.3</td><td>74.4/88.3</td><td>-</td><td>52.5/79.5</td></tr><tr><th>IN+MGN</th><td>87.5/95.1</td><td>79.4/89.0</td><td>70.5/71.2</td><td>63.7/85.1</td></tr><tr><th>MoCo+MGN</th><td>88.2/95.3</td><td>79.5/89.1</td><td>67.1/67.0</td><td>62.7/84.3</td></tr><tr><th>LUP+MGN</th><td>91.0/96.4</td><td>82.1/91.0</td><td>74.7/75.4</td><td>65.7/85.5</td></tr><tr><th>CycAs+MGN</th><td>91.2/96.5</td><td>82.7/91.1</td><td>76.3/76.9</td><td>65.8/86.1</td></tr></tbody></table>", "caption": "TABLE IV: Results of fine-tuning a state-of-the-art re-ID model, MGN, with different pre-train models. IN refers to ImageNet supervised pre-trained model. MoCo refers to self-supervised pre-trained model trained on ImageNet with MoCo [31]. LUP refers to self-supervised pre-trained model trained on the person-centric LUP [9] dataset with MoCo. ", "list_citation_info": ["[71] L. He and W. Liu, \u201cGuided saliency feature learning for person re-identification in crowded scenes,\u201d in ECCV, 2020.", "[64] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, \u201cDensely semantically aligned person re-identification,\u201d in CVPR, 2019.", "[70] K. Zhu, H. Guo, Z. Liu, M. Tang, and J. Wang, \u201cIdentity-guided human semantic parsing for person re-identification,\u201d in ECCV, 2020.", "[30] H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang, \u201cBag of tricks and a strong baseline for deep person re-identification,\u201d in Workshops in CVPR, 2019.", "[68] Z. Dai, M. Chen, X. Gu, S. Zhu, and P. Tan, \u201cBatch dropblock network for person re-identification and beyond,\u201d in ICCV, 2019.", "[69] H. Park and B. Ham, \u201cRelation network for person re-identification,\u201d in AAAI, 2020.", "[66] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, \u201cOmni-scale feature learning for person re-identification,\u201d in ICCV, 2019.", "[65] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang, \u201cAbd-net: Attentive but diverse person re-identification,\u201d in ICCV, 2019.", "[31] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in CVPR, 2020.", "[67] B. Chen, W. Deng, and J. Hu, \u201cMixed high-order attention network for person re-identification,\u201d in ICCV, 2019.", "[9] D. Fu, D. Chen, J. Bao, H. Yang, L. Yuan, L. Zhang, H. Li, and D. Chen, \u201cUnsupervised pre-training for person re-identification,\u201d in CVPR, 2021.", "[10] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, \u201cBeyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline),\u201d in ECCV, 2018."]}], "citation_info_to_title": {"[10] Y. Sun, L. Zheng, Y. Yang, Q. Tian, and S. Wang, \u201cBeyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline),\u201d in ECCV, 2018.": "Beyond Part Models: Person Retrieval with Refined Part Pooling (and a Strong Convolutional Baseline)", "[57] M. Li, X. Zhu, and S. Gong, \u201cUnsupervised person re-identification by deep learning tracklet association,\u201d in ECCV, 2018.": "Unsupervised person re-identification by deep learning tracklet association", "[44] M. Hirzer, C. Beleznai, P. M. Roth, and H. Bischof, \u201cPerson re-identification by descriptive and discriminative classification,\u201d in Scandinavian conference on Image analysis. Springer, 2011, pp. 91\u2013102.": "Person re-identification by descriptive and discriminative classification", "[49] S. Shankar, V. Piratla, S. Chakrabarti, S. Chaudhuri, P. Jyothi, and S. Sarawagi, \u201cGeneralizing across domains via cross-gradient training,\u201d 2018.": "Generalizing across domains via cross-gradient training", "[4] Y. Sun, C. Cheng, Y. Zhang, C. Zhang, L. Zheng, Z. Wang, and Y. Wei, \u201cCircle loss: A unified perspective of pair similarity optimization,\u201d in CVPR, 2020.": "Circle Loss: A Unified Perspective of Pair Similarity Optimization", "[67] B. Chen, W. Deng, and J. Hu, \u201cMixed high-order attention network for person re-identification,\u201d in ICCV, 2019.": "Mixed High-Order Attention Network for Person Re-Identification", "[69] H. Park and B. Ham, \u201cRelation network for person re-identification,\u201d in AAAI, 2020.": "Relation Network for Person Re-identification", "[64] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, \u201cDensely semantically aligned person re-identification,\u201d in CVPR, 2019.": "Densely semantically aligned person re-identification", "[9] D. Fu, D. Chen, J. Bao, H. Yang, L. Yuan, L. Zhang, H. Li, and D. Chen, \u201cUnsupervised pre-training for person re-identification,\u201d in CVPR, 2021.": "Unsupervised pre-training for person re-identification", "[18] Y. Ge, D. Chen, and H. Li, \u201cMutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification,\u201d ICLR, 2020.": "Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification", "[23] Y. Dai, X. Li, J. Liu, Z. Tong, and L.-Y. Duan, \u201cGeneralizable person re-identification with relevance-aware mixture of experts,\u201d in CVPR, 2021.": "Generalizable person re-identification with relevance-aware mixture of experts", "[48] X. Zhang, H. Luo, X. Fan, W. Xiang, Y. Sun, Q. Xiao, W. Jiang, C. Zhang, and J. Sun, \u201cAlignedreid: Surpassing human-level performance in person re-identification,\u201d arXiv preprint arXiv:1711.08184, 2017.": "Alignedreid: Surpassing human-level performance in person re-identification", "[59] H.-X. Yu, A. Wu, and W.-S. Zheng, \u201cCross-view asymmetric metric learning for unsupervised person re-identification,\u201d in ICCV, 2017, pp. 994\u20131002.": "Cross-view asymmetric metric learning for unsupervised person re-identification", "[56] Z. Zhong, L. Zheng, Z. Luo, S. Li, and Y. Yang, \u201cInvariance matters: Exemplar memory for domain adaptive person re-identification,\u201d in CVPR, 2019.": "Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-Identification", "[66] K. Zhou, Y. Yang, A. Cavallaro, and T. Xiang, \u201cOmni-scale feature learning for person re-identification,\u201d in ICCV, 2019.": "Omni-scale feature learning for person re-identification", "[14] J. Wu, Y. Yang, H. Liu, S. Liao, Z. Lei, and S. Z. Li, \u201cUnsupervised graph association for person re-identification,\u201d in ICCV, 2019.": "Unsupervised graph association for person re-identification", "[30] H. Luo, Y. Gu, X. Liao, S. Lai, and W. Jiang, \u201cBag of tricks and a strong baseline for deep person re-identification,\u201d in Workshops in CVPR, 2019.": "Bag of tricks and a strong baseline for deep person re-identification", "[24] W. Huang, Y. Bai, Q. Ren, X. Zhao, M. Feng, and Y. Wang, \u201cLarge-scale unsupervised person re-identification with contrastive learning,\u201d arXiv preprint arXiv:2105.07914, 2021.": "Large-scale unsupervised person re-identification with contrastive learning", "[20] J. Song, Y. Yang, Y.-Z. Song, T. Xiang, and T. M. Hospedales, \u201cGeneralizable person re-identification by domain-invariant mapping network,\u201d in CVPR, 2019.": "Generalizable person re-identification by domain-invariant mapping network", "[17] L. Wei, S. Zhang, W. Gao, and Q. Tian, \u201cPerson transfer gan to bridge domain gap for person re-identification,\u201d in CVPR, 2018.": "Person Transfer GAN to Bridge Domain Gap for Person Re-identification", "[65] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and Z. Wang, \u201cAbd-net: Attentive but diverse person re-identification,\u201d in ICCV, 2019.": "Abd-net: Attentive but diverse person re-identification", "[53] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, \u201cImage-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification,\u201d in CVPR, 2018.": "Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-Identification", "[16] W. Deng, L. Zheng, Q. Ye, G. Kang, Y. Yang, and J. Jiao, \u201cImage-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification,\u201d in CVPR, 2018.": "Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-Identification", "[12] H. Fan, L. Zheng, C. Yan, and Y. Yang, \u201cUnsupervised person re-identification: Clustering and fine-tuning,\u201d ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 14, no. 4, p. 83, 2018.": "Unsupervised person re-identification: Clustering and fine-tuning", "[46] D. Gray and H. Tao, \u201cViewpoint invariant pedestrian recognition with an ensemble of localized features,\u201d in ECCV, 2008.": "Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features", "[31] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, \u201cMomentum contrast for unsupervised visual representation learning,\u201d in CVPR, 2020.": "Momentum contrast for unsupervised visual representation learning", "[45] C. C. Loy, T. Xiang, and S. Gong, \u201cTime-delayed correlation analysis for multi-camera activity understanding,\u201d 2010.": "Time-delayed correlation analysis for multi-camera activity understanding", "[22] J. Jia, Q. Ruan, and T. M. Hospedales, \u201cFrustratingly easy person re-identification: Generalizing person re-id in practice,\u201d BMVC, 2019.": "Frustratingly easy person re-identification: Generalizing person re-id in practice", "[13] Y. Lin, X. Dong, L. Zheng, Y. Yan, and Y. Yang, \u201cA bottom-up clustering approach to unsupervised person re-identification,\u201d in AAAI, 2019.": "A bottom-up clustering approach to unsupervised person re-identification", "[1] L. Zheng, L. Shen, L. Tian, S. Wang, J. Wang, and Q. Tian, \u201cScalable person re-identification: A benchmark,\u201d in CVPR, 2015.": "Scalable person re-identification: A benchmark", "[71] L. He and W. Liu, \u201cGuided saliency feature learning for person re-identification in crowded scenes,\u201d in ECCV, 2020.": "Guided saliency feature learning for person re-identification in crowded scenes", "[27] W. Li, R. Zhao, T. Xiao, and X. Wang, \u201cDeepreid: Deep filter pairing neural network for person re-identification,\u201d in CVPR, 2014.": "Deepreid: Deep Filter Pairing Neural Network for Person Re-identification", "[21] X. Jin, C. Lan, W. Zeng, Z. Chen, and L. Zhang, \u201cStyle normalization and restitution for generalizable person re-identification,\u201d in CVPR, 2020.": "Style normalization and restitution for generalizable person re-identification", "[43] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang, \u201cEnd-to-end deep learning for person search,\u201d arXiv preprint arXiv:1604.01850, 2016.": "End-to-end deep learning for person search", "[50] S. Qiao, C. Liu, W. Shen, and A. L. Yuille, \u201cFew-shot image recognition by predicting parameters from activations,\u201d in CVPR, 2018.": "Few-shot image recognition by predicting parameters from activations", "[54] J. Wang, X. Zhu, S. Gong, and W. Li, \u201cTransferable joint attribute-identity deep learning for unsupervised person re-identification,\u201d in CVPR, 2018.": "Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification", "[47] T. Wang, S. Gong, X. Zhu, and S. Wang, \u201cPerson re-identification by video ranking,\u201d in ECCV, 2014.": "Person re-identification by video ranking", "[26] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, \u201cPerformance measures and a data set for multi-target, multi-camera tracking,\u201d in ECCV, 2016.": "Performance measures and a data set for multi-target, multi-camera tracking", "[2] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in CVPR, 2019.": "Arcface: Additive Angular Margin Loss for Deep Face Recognition", "[19] H. Luo, P. Wang, Y. Xu, F. Ding, Y. Zhou, F. Wang, H. Li, and R. Jin, \u201cSelf-supervised pre-training for transformer-based person re-identification,\u201d arXiv preprint arXiv:2111.12084, 2021.": "Self-supervised pre-training for transformer-based person re-identification", "[55] Z. Zhong, L. Zheng, S. Li, and Y. Yang, \u201cGeneralizing a person retrieval model hetero-and homogeneously,\u201d in ECCV, 2018.": "Generalizing a person retrieval model hetero-and homogeneously", "[58] \u2014\u2014, \u201cUnsupervised tracklet person re-identification,\u201d IEEE transactions on pattern analysis and machine intelligence, 2019.": "Unsupervised tracklet person re-identification", "[11] G. Wang, Y. Yuan, X. Chen, J. Li, and X. Zhou, \u201cLearning discriminative features with multiple granularities for person re-identification,\u201d in ACM MM, 2018.": "Learning discriminative features with multiple granularities for person re-identification", "[68] Z. Dai, M. Chen, X. Gu, S. Zhu, and P. Tan, \u201cBatch dropblock network for person re-identification and beyond,\u201d in ICCV, 2019.": "Batch DropBlock Network for Person Re-Identification and Beyond", "[60] J. Wu, S. Liao, X. Wang, Y. Yang, S. Z. Li et al., \u201cClustering and dynamic sampling based unsupervised domain adaptation for person re-identification,\u201d in ICME, 2019.": "Clustering and dynamic sampling based unsupervised domain adaptation for person re-identification", "[51] Y. Zhao, Z. Zhong, F. Yang, Z. Luo, Y. Lin, S. Li, and N. Sebe, \u201cLearning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification,\u201d in CVPR, 2021.": "Learning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification", "[70] K. Zhu, H. Guo, Z. Liu, M. Tang, and J. Wang, \u201cIdentity-guided human semantic parsing for person re-identification,\u201d in ECCV, 2020.": "Identity-guided human semantic parsing for person re-identification", "[15] Y. Ge, F. Zhu, D. Chen, R. Zhao et al., \u201cSelf-paced contrastive learning with hybrid memory for domain adaptive object re-id,\u201d NeurIPS, 2020.": "Self-paced contrastive learning with hybrid memory for domain adaptive object re-id"}, "source_title_to_arxiv_id": {"Large-scale unsupervised person re-identification with contrastive learning": "2105.07914", "Arcface: Additive Angular Margin Loss for Deep Face Recognition": "1801.07698", "Self-supervised pre-training for transformer-based person re-identification": "2111.12084"}}