{"title": "REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory", "abstract": "In this paper, we propose an end-to-end Retrieval-Augmented Visual Language\nModel (REVEAL) that learns to encode world knowledge into a large-scale memory,\nand to retrieve from it to answer knowledge-intensive queries. REVEAL consists\nof four key components: the memory, the encoder, the retriever and the\ngenerator. The large-scale memory encodes various sources of multimodal world\nknowledge (e.g. image-text pairs, question answering pairs, knowledge graph\ntriplets, etc) via a unified encoder. The retriever finds the most relevant\nknowledge entries in the memory, and the generator fuses the retrieved\nknowledge with the input query to produce the output. A key novelty in our\napproach is that the memory, encoder, retriever and generator are all\npre-trained end-to-end on a massive amount of data. Furthermore, our approach\ncan use a diverse set of multimodal knowledge sources, which is shown to result\nin significant gains. We show that REVEAL achieves state-of-the-art results on\nvisual question answering and image captioning.", "authors": ["Ziniu Hu", "Ahmet Iscen", "Chen Sun", "Zirui Wang", "Kai-Wei Chang", "Yizhou Sun", "Cordelia Schmid", "David A. Ross", "Alireza Fathi"], "published_date": "2022_12_10", "pdf_url": "http://arxiv.org/pdf/2212.05221v2", "list_table_and_caption": [{"table": "<table><thead><tr><th>Knowledge Source</th><th>Corpus Size</th><th>Type of Text</th><th>Avg. Text Length</th></tr></thead><tbody><tr><td>WIT [36]</td><td>5,233,186</td><td>Wikipedia Passage</td><td>258</td></tr><tr><td>CC12M [5]</td><td>10,009,901</td><td>Alt-Text Caption</td><td>37</td></tr><tr><td>VQA-V2 [12]</td><td>123,287</td><td>Question Answer</td><td>111</td></tr><tr><td>WikiData [39]</td><td>4,947,397</td><td>Linearlized Triplets</td><td>326</td></tr></tbody></table>", "caption": "Table 2: Statistics of the knowledge sources used.", "list_citation_info": ["[36] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT: wikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR \u201921: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, pages 2443\u20132449. ACM, 2021.", "[39] Denny Vrandecic and Markus Kr\u00f6tzsch. Wikidata: a free collaborative knowledgebase. Commun. ACM, 57(10):78\u201385, 2014.", "[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6325\u20136334. IEEE Computer Society, 2017.", "[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021."]}, {"table": "<table><tbody><tr><th>VQA Model Name</th><td>Knowledge Sources</td><td>Accuracy (\\%)</td><td># params.</td></tr><tr><th>MUTAN+AN [28]</th><td>Wikipedia + ConceptNet</td><td>27.8</td><td>-</td></tr><tr><th>ConceptBERT [11]</th><td>Wikipedia</td><td>33.7</td><td>-</td></tr><tr><th>KRISP [27]</th><td>Wikipedia + ConceptNet</td><td>38.4</td><td>-</td></tr><tr><th>Visual Retriever-Reader [26]</th><td>Google Search</td><td>39.2</td><td>-</td></tr><tr><th>MAVEx [45]</th><td>Wikipedia+ConceptNet+Google Images</td><td>39.4</td><td>-</td></tr><tr><th>KAT-Explicit [13]</th><td>Wikidata</td><td>44.3</td><td>0.77B</td></tr><tr><th>PICa-Base [47]</th><td>Frozen GPT-3</td><td>43.3</td><td>(175B frozen)</td></tr><tr><th>PICa-Full [47]</th><td>Frozen GPT-3</td><td>48.0</td><td>(175B frozen)</td></tr><tr><th>KAT [13] (Single)</th><td>Wikidata + Frozen GPT-3</td><td>53.1</td><td>0.77B + (176B frozen)</td></tr><tr><th>KAT [13] (Ensemble)</th><td>Wikidata + Frozen GPT-3</td><td>54.4</td><td>2.31B + (176B frozen)</td></tr><tr><th>ReVIVE [23] (Single)</th><td>Wikidata + Frozen GPT-3</td><td>56.6</td><td>0.77B + (176.9B frozen)</td></tr><tr><th>ReVIVE [23] (Ensemble)</th><td>Wikidata+Frozen GPT-3</td><td>58.0</td><td>2.31B + (176.9B frozen)</td></tr><tr><th>ReVeaL-Base</th><td>WIT + CC12M + Wikidata + VQA-2</td><td>55.2</td><td>0.4B</td></tr><tr><th>ReVeaL-Large</th><td>WIT + CC12M + Wikidata + VQA-2</td><td>58.0</td><td>1.4B</td></tr><tr><th>ReVeaL</th><td>WIT + CC12M + Wikidata + VQA-2</td><td>59.1</td><td>2.1B</td></tr></tbody></table>", "caption": "Table 3: Visual Question Answering results on OK-VQA, compared with existing methods that use different knowledge sources.", "list_citation_info": ["[23] Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, and Lu Yuan. REVIVE: regional visual representation matters in knowledge-based visual question answering. CoRR, abs/2206.01201, 2022.", "[28] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3195\u20133204. Computer Vision Foundation / IEEE, 2019.", "[27] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based VQA. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 14111\u201314121. Computer Vision Foundation / IEEE, 2021.", "[11] Fran\u00e7ois Gard\u00e8res, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. ConceptBert: Concept-aware representation for visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 489\u2013498, Online, 2020. Association for Computational Linguistics.", "[13] Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and Jianfeng Gao. KAT: A knowledge augmented transformer for vision-and-language. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 956\u2013968, Seattle, United States, 2022. Association for Computational Linguistics.", "[47] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of GPT-3 for few-shot knowledge-based VQA. ArXiv preprint, abs/2109.05014, 2021.", "[26] Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. Weakly-supervised visual-retriever-reader for knowledge-based question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6417\u20136431, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.", "[45] Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. Multi-modal answer validation for knowledge-based VQA. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pages 2712\u20132721. AAAI Press, 2022."]}, {"table": "<table><tbody><tr><th>VQA Model Name</th><td>Accuracy (\\%)</td></tr><tr><th>ViLBERT [25]</th><td>30.6</td></tr><tr><th>LXMERT [37]</th><td>30.7</td></tr><tr><th>ClipCap [29]</th><td>30.9</td></tr><tr><th>KRISP [27]</th><td>33.7</td></tr><tr><th>GPV-2 [20]</th><td>48.6</td></tr><tr><th>ReVeaL-Base</th><td>50.4</td></tr><tr><th>ReVeaL-Large</th><td>51.5</td></tr><tr><th>ReVeaL</th><td>52.2</td></tr></tbody></table>", "caption": "Table 4: Visual Question Answering results on A-OKVQA.", "list_citation_info": ["[25] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13\u201323, 2019.", "[37] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, Hong Kong, China, 2019. Association for Computational Linguistics.", "[29] Ron Mokady, Amir Hertz, and Amit H. Bermano. Clipcap: CLIP prefix for image captioning. ArXiv preprint, abs/2111.09734, 2021.", "[27] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based VQA. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 14111\u201314121. Computer Vision Foundation / IEEE, 2021.", "[20] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, and Aniruddha Kembhavi. Webly supervised concept expansion for general purpose vision models. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI, volume 13696 of Lecture Notes in Computer Science, pages 662\u2013681. Springer, 2022."]}, {"table": "<table><tbody><tr><th>Model Name</th><td>MSCOCO</td><td>NoCaps</td><td># params.</td></tr><tr><th>Flamingo [2]</th><td>138.1</td><td>-</td><td>80B</td></tr><tr><th>VinVL [51]</th><td>140.9</td><td>105.1</td><td>0.4B</td></tr><tr><th>SimVLM [44]</th><td>143.3</td><td>112.2</td><td>1.5B</td></tr><tr><th>CoCa [48]</th><td>143.6</td><td>122.4</td><td>2.1B</td></tr><tr><th>ReVeaL-Base</th><td>141.1</td><td>115.8</td><td>0.4B</td></tr><tr><th>ReVeaL-Large</th><td>144.5</td><td>121.3</td><td>1.4B</td></tr><tr><th>ReVeaL</th><td>145.4</td><td>123.0</td><td>2.1B</td></tr></tbody></table>", "caption": "Table 5: Image Captioning results on MSCOCO (Karpathy-test split) and NoCaps (val set). Evaluated using the CIDEr metric.", "list_citation_info": ["[51] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 5579\u20135588. Computer Vision Foundation / IEEE, 2021.", "[48] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. CoRR, abs/2205.01917, 2022.", "[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. CoRR, abs/2204.14198, 2022.", "[44] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022."]}, {"table": "<table><thead><tr><th>Retrieval Method</th><th>Acc@10</th><th>Acc@100</th><th>OKVQA Acc.</th><th>GFLOPs</th></tr><tr><th>ALIGN [19] (fixed)</th><th>0.638</th><th>0.793</th><th>44.7</th><th>-</th></tr></thead><tbody><tr><td>Attention Distill [16]</td><td>0.674</td><td>0.835</td><td>45.9</td><td>119</td></tr><tr><td>EMDR{}^{2} [46]</td><td>0.691</td><td>0.869</td><td>46.5</td><td>561</td></tr><tr><td>Perplexity Distill [17]</td><td>0.704</td><td>0.886</td><td>46.7</td><td>561</td></tr><tr><td>Ours (Attentive Fusion)</td><td>0.726</td><td>0.894</td><td>47.3</td><td>120</td></tr></tbody></table>", "caption": "Table 6: Analysis of Retrieval Training Method: We train ReVeaL-Base (frozen generator, only train randomly initialized retriever) to retrieve from the WIT dataset (only text passage without image), and show the retrieval accuracy at the first 10 or 100 results, as well as fine-tuned OKVQA accuracy.", "list_citation_info": ["[46] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 72\u201377, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.", "[16] Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.", "[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4904\u20134916. PMLR, 2021.", "[17] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. CoRR, abs/2208.03299, 2022."]}], "citation_info_to_title": {"[11] Fran\u00e7ois Gard\u00e8res, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. ConceptBert: Concept-aware representation for visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 489\u2013498, Online, 2020. Association for Computational Linguistics.": "ConceptBert: Concept-aware representation for visual question answering", "[29] Ron Mokady, Amir Hertz, and Amit H. Bermano. Clipcap: CLIP prefix for image captioning. ArXiv preprint, abs/2111.09734, 2021.": "Clipcap: CLIP prefix for image captioning", "[51] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 5579\u20135588. Computer Vision Foundation / IEEE, 2021.": "Vinvl: Revisiting visual representations in vision-language models", "[28] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3195\u20133204. Computer Vision Foundation / IEEE, 2019.": "OK-VQA: A visual question answering benchmark requiring external knowledge", "[47] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of GPT-3 for few-shot knowledge-based VQA. ArXiv preprint, abs/2109.05014, 2021.": "An empirical study of GPT-3 for few-shot knowledge-based VQA", "[27] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based VQA. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 14111\u201314121. Computer Vision Foundation / IEEE, 2021.": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA", "[26] Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. Weakly-supervised visual-retriever-reader for knowledge-based question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6417\u20136431, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.": "Weakly-supervised visual-retriever-reader for knowledge-based question answering", "[37] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, Hong Kong, China, 2019. Association for Computational Linguistics.": "LXMERT: Learning cross-modality encoder representations from transformers", "[46] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 72\u201377, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.": "End-to-end open-domain question answering with BERTserini", "[25] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13\u201323, 2019.": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks", "[36] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT: wikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR \u201921: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, pages 2443\u20132449. ACM, 2021.": "WIT: Wikipedia-Based Image Text Dataset for Multimodal Multilingual Machine Learning", "[48] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. CoRR, abs/2205.01917, 2022.": "Coca: Contrastive captioners are image-text foundation models", "[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. CoRR, abs/2204.14198, 2022.": "Flamingo: a visual language model for few-shot learning", "[17] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. CoRR, abs/2208.03299, 2022.": "Few-shot learning with retrieval augmented language models", "[23] Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, and Lu Yuan. REVIVE: regional visual representation matters in knowledge-based visual question answering. CoRR, abs/2206.01201, 2022.": "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering", "[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4904\u20134916. PMLR, 2021.": "Scaling up visual and vision-language representation learning with noisy text supervision", "[45] Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. Multi-modal answer validation for knowledge-based VQA. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pages 2712\u20132721. AAAI Press, 2022.": "Multi-modal answer validation for knowledge-based VQA", "[13] Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and Jianfeng Gao. KAT: A knowledge augmented transformer for vision-and-language. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 956\u2013968, Seattle, United States, 2022. Association for Computational Linguistics.": "KAT: A knowledge augmented transformer for vision-and-language", "[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6325\u20136334. IEEE Computer Society, 2017.": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "[39] Denny Vrandecic and Markus Kr\u00f6tzsch. Wikidata: a free collaborative knowledgebase. Commun. ACM, 57(10):78\u201385, 2014.": "Wikidata: a free collaborative knowledgebase", "[44] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.": "Simvlm: Simple visual language model pretraining with weak supervision", "[20] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, and Aniruddha Kembhavi. Webly supervised concept expansion for general purpose vision models. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI, volume 13696 of Lecture Notes in Computer Science, pages 662\u2013681. Springer, 2022.": "Webly Supervised Concept Expansion for General Purpose Vision Models", "[16] Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.": "Distilling knowledge from reader to retriever for question answering"}, "source_title_to_arxiv_id": {"Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts": "2102.08981", "Distilling knowledge from reader to retriever for question answering": "2012.04584"}}