{"title": "Revisiting the Importance of Amplifying Bias for Debiasing", "abstract": "In image classification, \"debiasing\" aims to train a classifier to be less\nsusceptible to dataset bias, the strong correlation between peripheral\nattributes of data samples and a target class. For example, even if the frog\nclass in the dataset mainly consists of frog images with a swamp background\n(i.e., bias-aligned samples), a debiased classifier should be able to correctly\nclassify a frog at a beach (i.e., bias-conflicting samples). Recent debiasing\napproaches commonly use two components for debiasing, a biased model $f_B$ and\na debiased model $f_D$. $f_B$ is trained to focus on bias-aligned samples\n(i.e., overfitted to the bias) while $f_D$ is mainly trained with\nbias-conflicting samples by concentrating on samples which $f_B$ fails to\nlearn, leading $f_D$ to be less susceptible to the dataset bias. While the\nstate-of-the-art debiasing techniques have aimed to better train $f_D$, we\nfocus on training $f_B$, an overlooked component until now. Our empirical\nanalysis reveals that removing the bias-conflicting samples from the training\nset for $f_B$ is important for improving the debiasing performance of $f_D$.\nThis is due to the fact that the bias-conflicting samples work as noisy samples\nfor amplifying the bias for $f_B$ since those samples do not include the bias\nattribute. To this end, we propose a simple yet effective data sample selection\nmethod which removes the bias-conflicting samples to construct a bias-amplified\ndataset for training $f_B$. Our data sample selection method can be directly\napplied to existing reweighting-based debiasing approaches, obtaining\nconsistent performance boost and achieving the state-of-the-art performance on\nboth synthetic and real-world datasets.", "authors": ["Jungsoo Lee", "Jeonghoon Park", "Daeyoung Kim", "Juyoung Lee", "Edward Choi", "Jaegul Choo"], "published_date": "2022_05_29", "pdf_url": "http://arxiv.org/pdf/2205.14594v4", "list_table_and_caption": [{"table": "<table><tbody><tr><td colspan=\"2\" rowspan=\"2\">Method</td><td colspan=\"4\">Colored MNIST</td><td colspan=\"4\">BFFHQ</td><td colspan=\"2\">Dogs &amp; Cats</td><td colspan=\"2\">BAR</td></tr><tr><td>0.5%</td><td>1.0%</td><td>2.0%</td><td>5.0%</td><td>0.5%</td><td>1.0%</td><td>2.0%</td><td>5.0%</td><td>1.0%</td><td>5.0%</td><td>1.0%</td><td>5.0%</td></tr><tr><td>Vanilla (He et al. 2015)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>34.75</td><td>51.14</td><td>65.72</td><td>82.82</td><td>55.64</td><td>60.96</td><td>69.00</td><td>82.88</td><td>48.06</td><td>69.88</td><td>70.55</td><td>82.53</td></tr><tr><td>HEX (Wang et al. 2019a)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>42.25</td><td>47.02</td><td>72.82</td><td>85.50</td><td>56.96</td><td>62.32</td><td>70.72</td><td>83.40</td><td>46.76</td><td>72.60</td><td>70.48</td><td>81.20</td></tr><tr><td>LNL (Kim et al. 2019)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>36.29</td><td>49.48</td><td>63.30</td><td>81.30</td><td>56.88</td><td>62.64</td><td>69.80</td><td>83.08</td><td>50.90</td><td>73.96</td><td>-</td><td>-</td></tr><tr><td>EnD (Tartaglione, Barbano, and Grangetto 2021)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>35.33</td><td>48.97</td><td>67.01</td><td>82.09</td><td>55.96</td><td>60.88</td><td>69.72</td><td>82.88</td><td>48.56</td><td>68.24</td><td>-</td><td>-</td></tr><tr><td>ReBias (Bahng et al. 2020)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>60.86</td><td>82.78</td><td>92.00</td><td>96.45</td><td>55.76</td><td>60.68</td><td>69.60</td><td>82.64</td><td>48.70</td><td>65.74</td><td>73.04</td><td>83.90</td></tr><tr><td>LfF (Nam et al. 2020)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>63.55</td><td>76.81</td><td>84.18</td><td>89.65</td><td>65.19</td><td>69.24</td><td>73.08</td><td>79.80</td><td>71.72</td><td>84.32</td><td>70.16</td><td>82.95</td></tr><tr><td>DisEnt (Lee et al. 2021)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>68.49</td><td>79.99</td><td>84.09</td><td>89.91</td><td>62.08</td><td>66.00</td><td>69.92</td><td>80.68</td><td>65.74</td><td>81.58</td><td>70.33</td><td>83.13</td></tr><tr><td rowspan=\"2\">LfF + Ours</td><td rowspan=\"2\">\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>69.70</td><td>81.17</td><td>85.20</td><td>90.04</td><td>67.36</td><td>75.08</td><td>80.32</td><td>85.48</td><td>81.52</td><td>88.60</td><td>73.36</td><td>83.87</td></tr><tr><td>(+ 6.15)</td><td>(+ 4.36)</td><td>(+ 1.02)</td><td>(+ 0.39)</td><td>(+ 2.17)</td><td>(+ 5.84)</td><td>(+ 7.24)</td><td>(+ 5.68)</td><td>(+ 9.80)</td><td>(+ 4.28)</td><td>(+ 3.20)</td><td>(+ 0.92)</td></tr><tr><td rowspan=\"2\">DisEnt + Ours</td><td rowspan=\"2\">\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>71.34</td><td>82.11</td><td>84.66</td><td>90.15</td><td>67.56</td><td>73.48</td><td>79.48</td><td>84.84</td><td>80.74</td><td>86.84</td><td>73.29</td><td>84.96</td></tr><tr><td>(+ 2.85)</td><td>(+ 2.12)</td><td>(+ 0.57)</td><td>(+ 0.24)</td><td>(+ 5.48)</td><td>(+ 7.48)</td><td>(+ 9.56)</td><td>(+ 4.16)</td><td>(+ 15.00)</td><td>(+ 5.26)</td><td>(+ 2.96)</td><td>(+ 1.83)</td></tr></tbody></table>", "caption": "Table 1: Image classification accuracy on unbiased test sets with varying ratios of bias-conflicting samples.The cross and check represent whether each model 1) uses bias labels during training and 2) requires predefined bias type.For LfF and DisEnt, the performance gains are shaded in grey.Best performing results are marked in bold.", "list_citation_info": ["Wang et al. (2019a) Wang, H.; He, Z.; Lipton, Z. L.; and Xing, E. P. 2019a. Learning Robust Representations by Projecting Superficial Statistics Out. In International Conference on Learning Representations.", "Tartaglione, Barbano, and Grangetto (2021) Tartaglione, E.; Barbano, C. A.; and Grangetto, M. 2021. EnD: Entangling and Disentangling Deep Representations for Bias Correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 13508\u201313517.", "Bahng et al. (2020) Bahng, H.; Chun, S.; Yun, S.; Choo, J.; and Oh, S. J. 2020. Learning De-biased Representations with Biased Representations. In International Conference on Machine Learning (ICML).", "Kim et al. (2019) Kim, B.; Kim, H.; Kim, K.; Kim, S.; and Kim, J. 2019. Learning Not to Learn: Training Deep Neural Networks With Biased Data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "Lee et al. (2021) Lee, J.; Kim, E.; Lee, J.; Lee, J.; and Choo, J. 2021. Learning Debiased Representation via Disentangled Feature Augmentation. In Advances in Neural Information Processing Systems.", "Nam et al. (2020) Nam, J.; Cha, H.; Ahn, S.; Lee, J.; and Shin, J. 2020. Learning from Failure: Training Debiased Classifier from Biased Classifier. In Advances in Neural Information Processing Systems.", "He et al. (2015) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">Colored MNIST</th><th colspan=\"3\">BFFHQ</th><th colspan=\"3\">Dogs &amp; Cats</th><th colspan=\"3\">BAR</th></tr><tr><th>unbiased\\downarrow</th><th>biased\\uparrow</th><th>\\text{acc}_{\\text{diff}}\\uparrow</th><th>unbiased\\downarrow</th><th>biased\\uparrow</th><th>\\text{acc}_{\\text{diff}}\\uparrow</th><th>unbiased\\downarrow</th><th>biased\\uparrow</th><th>\\text{acc}_{\\text{diff}}\\uparrow</th><th>unbiased\\downarrow</th><th>biased\\uparrow</th><th>\\text{acc}_{\\text{diff}}\\uparrow</th></tr></thead><tbody><tr><th>f_{B} of LfF (Nam et al. 2020)</th><td>15.02 \\pm1.22</td><td>88.38 \\pm14.61</td><td>73.36</td><td>55.84 \\pm1.80</td><td>99.50 \\pm0.21</td><td>43.66</td><td>36.3 \\pm3.49</td><td>96.18 \\pm0.62</td><td>59.88</td><td>69.07 \\pm0.87</td><td>98.33 \\pm0.78</td><td>29.26</td></tr><tr><th>f_{B} of LfF + Ours</th><td>13.29 \\pm1.61</td><td>99.10 \\pm0.05</td><td>85.81</td><td>37.96 \\pm1.77</td><td>98.9 \\pm0.16</td><td>60.94</td><td>13.42 \\pm1.42</td><td>94.35 \\pm0.19</td><td>80.93</td><td>53.53 \\pm2.90</td><td>96.00 \\pm0.73</td><td>42.47</td></tr></tbody></table>", "caption": "Table 8: Comparisons of f_{B} on the unbiased test set and the biased test set. Utilizing our method encourages f_{B} to be further biased in LfF. We use 1% ratio of bias-conflicting samples for each dataset.Applying our method on LfF is shaded in grey.For \\text{acc}_{\\text{diff}}, we compute the difference between the average of biased test set accuracy and that of unbiased test set accuracy.", "list_citation_info": ["Nam et al. (2020) Nam, J.; Cha, H.; Ahn, S.; Lee, J.; and Shin, J. 2020. Learning from Failure: Training Debiased Classifier from Biased Classifier. In Advances in Neural Information Processing Systems."]}, {"table": "<table><tbody><tr><th>Method</th><td>Colored MNIST</td><td>BFFHQ</td><td>Dogs &amp; Cats</td><td>BAR</td></tr><tr><th>JTT (Liu et al. 2021)</th><td>74.36\\pm5.06</td><td>60.78\\pm2.62</td><td>66.38\\pm8.22</td><td>71.70\\pm0.53</td></tr><tr><th rowspan=\"2\">JTT + Ours</th><td>78.88\\pm2.64</td><td>64.96\\pm1.50</td><td>82.91\\pm1.97</td><td>73.32\\pm0.53</td></tr><tr><td>(+4.52)</td><td>(+4.18)</td><td>(+16.53)</td><td>(+1.62)</td></tr></tbody></table>", "caption": "Table 11:  Comparisons of unbiased test set accuracies of JTT trained with and without our approach. We use 1% ratio of bias-conflicting samples for each dataset. The performance improvements are shaded in grey.", "list_citation_info": ["Liu et al. (2021) Liu, E. Z.; Haghgoo, B.; Chen, A. S.; Raghunathan, A.; Koh, P. W.; Sagawa, S.; Liang, P.; and Finn, C. 2021. Just Train Twice: Improving Group Robustness without Training Group Information. In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research."]}, {"table": "<table><tbody><tr><td colspan=\"2\" rowspan=\"2\">Method</td><td colspan=\"4\">Colored MNIST</td><td colspan=\"2\">Dogs &amp; Cats</td></tr><tr><td>0.5%</td><td>1.0%</td><td>2.0%</td><td>5.0%</td><td>1.0%</td><td>5.0%</td></tr><tr><td>Vanilla (He et al. 2015)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>34.75\\pm1.68</td><td>51.14\\pm3.12</td><td>65.72\\pm3.74</td><td>82.82\\pm0.63</td><td>48.06\\pm7.09</td><td>69.88\\pm1.73</td></tr><tr><td>HEX (Wang et al. 2019a)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>42.25\\pm1.83</td><td>47.02\\pm15.08</td><td>72.82\\pm1.03</td><td>85.50\\pm1.94</td><td>46.76\\pm5.3</td><td>72.60\\pm3.01</td></tr><tr><td>LNL (Kim et al. 2019)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>36.29\\pm0.77</td><td>49.48\\pm5.29</td><td>63.30\\pm2.73</td><td>81.30\\pm0.80</td><td>50.90\\pm4.62</td><td>73.96\\pm1.86</td></tr><tr><td>EnD (Tartaglione, Barbano, and Grangetto 2021)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>35.33\\pm1.23</td><td>48.97\\pm11.54</td><td>67.01\\pm2.81</td><td>82.09\\pm2.20</td><td>48.56\\pm5.69</td><td>68.24\\pm2.24</td></tr><tr><td>ReBias (Bahng et al. 2020)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>60.86\\pm1.96</td><td>82.78\\pm1.36</td><td>92.00\\pm0.47</td><td>96.45\\pm0.17</td><td>48.70\\pm6.05</td><td>65.74\\pm0.51</td></tr><tr><td>LfF (Nam et al. 2020)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>63.55\\pm6.97</td><td>76.81\\pm4.56</td><td>84.18\\pm1.15</td><td>89.65\\pm0.49</td><td>71.72\\pm4.56</td><td>84.32\\pm1.87</td></tr><tr><td>DisEnt (Lee et al. 2021)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>68.49\\pm3.36</td><td>79.99\\pm2.15</td><td>84.09\\pm1.46</td><td>89.91\\pm0.55</td><td>65.74\\pm3.31</td><td>81.58\\pm2.44</td></tr><tr><td rowspan=\"2\">LfF + Ours</td><td rowspan=\"2\">\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>69.70\\pm4.10</td><td>81.17\\pm0.68</td><td>85.20\\pm0.85</td><td>90.04\\pm0.18</td><td>81.52\\pm1.13</td><td>88.60\\pm1.79</td></tr><tr><td>(+ 6.15)</td><td>(+ 4.36)</td><td>(+ 1.02)</td><td>(+ 0.39)</td><td>(+ 9.80)</td><td>(+ 4.28)</td></tr><tr><td rowspan=\"2\">DisEnt + Ours</td><td rowspan=\"2\">\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>71.34\\pm1.30</td><td>82.11\\pm0.54</td><td>84.66\\pm1.72</td><td>90.15\\pm0.48</td><td>80.74\\pm2.80</td><td>86.84\\pm0.77</td></tr><tr><td>(+ 2.85)</td><td>(+ 2.12)</td><td>(+ 0.57)</td><td>(+ 0.24)</td><td>(+ 15.00)</td><td>(+ 5.26)</td></tr></tbody></table>", "caption": "Table 15: Image classification accuracies on unbiased test sets of Colored MNIST and Dogs &amp; Cats with varying ratios of bias-conflicting samples.The cross and check represent whether each model 1) uses bias labels during training and 2) requires predefined bias type.For LfF and DisEnt, the performance gains are shaded in grey.Best performing results are marked in bold.", "list_citation_info": ["Wang et al. (2019a) Wang, H.; He, Z.; Lipton, Z. L.; and Xing, E. P. 2019a. Learning Robust Representations by Projecting Superficial Statistics Out. In International Conference on Learning Representations.", "Tartaglione, Barbano, and Grangetto (2021) Tartaglione, E.; Barbano, C. A.; and Grangetto, M. 2021. EnD: Entangling and Disentangling Deep Representations for Bias Correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 13508\u201313517.", "Bahng et al. (2020) Bahng, H.; Chun, S.; Yun, S.; Choo, J.; and Oh, S. J. 2020. Learning De-biased Representations with Biased Representations. In International Conference on Machine Learning (ICML).", "Kim et al. (2019) Kim, B.; Kim, H.; Kim, K.; Kim, S.; and Kim, J. 2019. Learning Not to Learn: Training Deep Neural Networks With Biased Data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "Lee et al. (2021) Lee, J.; Kim, E.; Lee, J.; Lee, J.; and Choo, J. 2021. Learning Debiased Representation via Disentangled Feature Augmentation. In Advances in Neural Information Processing Systems.", "Nam et al. (2020) Nam, J.; Cha, H.; Ahn, S.; Lee, J.; and Shin, J. 2020. Learning from Failure: Training Debiased Classifier from Biased Classifier. In Advances in Neural Information Processing Systems.", "He et al. (2015) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385."]}, {"table": "<table><tbody><tr><td colspan=\"2\" rowspan=\"2\">Method</td><td colspan=\"4\">BFFHQ</td><td colspan=\"2\">BAR</td></tr><tr><td>0.5%</td><td>1.0%</td><td>2.0%</td><td>5.0%</td><td>1.0%</td><td>5.0%</td></tr><tr><td>Vanilla (He et al. 2015)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>55.64\\pm0.44</td><td>60.96\\pm1.00</td><td>69.00\\pm0.50</td><td>82.88\\pm0.49</td><td>70.55\\pm0.87</td><td>82.53\\pm1.08</td></tr><tr><td>HEX (Wang et al. 2019a)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>56.96\\pm0.62</td><td>62.32\\pm1.21</td><td>70.72\\pm0.89</td><td>83.40\\pm0.34</td><td>70.48\\pm1.74</td><td>81.20\\pm0.68</td></tr><tr><td>LNL (Kim et al. 2019)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>56.88\\pm1.13</td><td>62.64\\pm0.99</td><td>69.80\\pm1.03</td><td>83.08\\pm0.93</td><td>70.65\\pm1.28</td><td>82.43\\pm1.25</td></tr><tr><td>EnD (Tartaglione, Barbano, and Grangetto 2021)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>55.96\\pm0.91</td><td>60.88\\pm1.17</td><td>69.72\\pm1.14</td><td>82.88\\pm0.74</td><td>71.07\\pm2.03</td><td>82.78\\pm0.30</td></tr><tr><td>ReBias (Bahng et al. 2020)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2713</td><td>55.76\\pm1.50</td><td>60.68\\pm1.24</td><td>69.60\\pm1.33</td><td>82.64\\pm0.64</td><td>73.04\\pm1.04</td><td>83.90\\pm0.82</td></tr><tr><td>LfF (Nam et al. 2020)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>65.19\\pm3.23</td><td>69.24\\pm2.07</td><td>73.08\\pm2.70</td><td>79.80\\pm1.09</td><td>70.16\\pm0.77</td><td>82.95\\pm0.27</td></tr><tr><td>DisEnt (Lee et al. 2021)</td><td>\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>62.08\\pm3.89</td><td>66.00\\pm1.33</td><td>69.92\\pm2.72</td><td>80.68\\pm0.25</td><td>70.33\\pm0.19</td><td>83.13\\pm0.46</td></tr><tr><td rowspan=\"2\">LfF + Ours</td><td rowspan=\"2\">\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>67.36\\pm3.10</td><td>75.08\\pm2.29</td><td>80.32\\pm2.07</td><td>85.48\\pm2.88</td><td>73.36\\pm0.97</td><td>83.87\\pm0.82</td></tr><tr><td>(+ 2.17)</td><td>(+ 5.84)</td><td>(+ 7.24)</td><td>(+ 5.68)</td><td>(+ 3.20)</td><td>(+ 0.92)</td></tr><tr><td rowspan=\"2\">DisEnt + Ours</td><td rowspan=\"2\">\\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717 \\textpdfrenderTextRenderingMode=FillStroke,LineWidth=.5pt, \u2717</td><td>67.56\\pm2.11</td><td>73.48\\pm2.12</td><td>79.48\\pm1.80</td><td>84.84\\pm2.11</td><td>73.29\\pm0.41</td><td>84.96\\pm0.69</td></tr><tr><td>(+ 5.48)</td><td>(+ 7.48)</td><td>(+ 9.56)</td><td>(+ 4.16)</td><td>(+ 2.96)</td><td>(+ 1.83)</td></tr></tbody></table>", "caption": "Table 16: Image classification accuracies on unbiased test sets of BFFHQ and BAR with varying ratios of bias-conflicting samples.The cross and check represent whether each model 1) uses bias labels during training and 2) requires predefined bias type.For LfF and DisEnt, the performance gains are shaded in grey.Best performing results are marked in bold.", "list_citation_info": ["Wang et al. (2019a) Wang, H.; He, Z.; Lipton, Z. L.; and Xing, E. P. 2019a. Learning Robust Representations by Projecting Superficial Statistics Out. In International Conference on Learning Representations.", "Tartaglione, Barbano, and Grangetto (2021) Tartaglione, E.; Barbano, C. A.; and Grangetto, M. 2021. EnD: Entangling and Disentangling Deep Representations for Bias Correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 13508\u201313517.", "Bahng et al. (2020) Bahng, H.; Chun, S.; Yun, S.; Choo, J.; and Oh, S. J. 2020. Learning De-biased Representations with Biased Representations. In International Conference on Machine Learning (ICML).", "Kim et al. (2019) Kim, B.; Kim, H.; Kim, K.; Kim, S.; and Kim, J. 2019. Learning Not to Learn: Training Deep Neural Networks With Biased Data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "Lee et al. (2021) Lee, J.; Kim, E.; Lee, J.; Lee, J.; and Choo, J. 2021. Learning Debiased Representation via Disentangled Feature Augmentation. In Advances in Neural Information Processing Systems.", "Nam et al. (2020) Nam, J.; Cha, H.; Ahn, S.; Lee, J.; and Shin, J. 2020. Learning from Failure: Training Debiased Classifier from Biased Classifier. In Advances in Neural Information Processing Systems.", "He et al. (2015) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385."]}], "citation_info_to_title": {"Bahng et al. (2020) Bahng, H.; Chun, S.; Yun, S.; Choo, J.; and Oh, S. J. 2020. Learning De-biased Representations with Biased Representations. In International Conference on Machine Learning (ICML).": "Learning De-biased Representations with Biased Representations", "Nam et al. (2020) Nam, J.; Cha, H.; Ahn, S.; Lee, J.; and Shin, J. 2020. Learning from Failure: Training Debiased Classifier from Biased Classifier. In Advances in Neural Information Processing Systems.": "Learning from Failure: Training Debiased Classifier from Biased Classifier", "Kim et al. (2019) Kim, B.; Kim, H.; Kim, K.; Kim, S.; and Kim, J. 2019. Learning Not to Learn: Training Deep Neural Networks With Biased Data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).": "Learning Not to Learn: Training Deep Neural Networks With Biased Data", "Liu et al. (2021) Liu, E. Z.; Haghgoo, B.; Chen, A. S.; Raghunathan, A.; Koh, P. W.; Sagawa, S.; Liang, P.; and Finn, C. 2021. Just Train Twice: Improving Group Robustness without Training Group Information. In Proceedings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research.": "Just Train Twice: Improving Group Robustness without Training Group Information", "Wang et al. (2019a) Wang, H.; He, Z.; Lipton, Z. L.; and Xing, E. P. 2019a. Learning Robust Representations by Projecting Superficial Statistics Out. In International Conference on Learning Representations.": "Learning Robust Representations by Projecting Superficial Statistics Out", "Tartaglione, Barbano, and Grangetto (2021) Tartaglione, E.; Barbano, C. A.; and Grangetto, M. 2021. EnD: Entangling and Disentangling Deep Representations for Bias Correction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 13508\u201313517.": "EnD: Entangling and Disentangling Deep Representations for Bias Correction", "He et al. (2015) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385.": "Deep Residual Learning for Image Recognition", "Lee et al. (2021) Lee, J.; Kim, E.; Lee, J.; Lee, J.; and Choo, J. 2021. Learning Debiased Representation via Disentangled Feature Augmentation. In Advances in Neural Information Processing Systems.": "Learning Debiased Representation via Disentangled Feature Augmentation"}, "source_title_to_arxiv_id": {"EnD: Entangling and Disentangling Deep Representations for Bias Correction": "2103.02023", "Learning Debiased Representation via Disentangled Feature Augmentation": "2107.01372"}}