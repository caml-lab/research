{"title": "End-to-End Semantic Video Transformer for Zero-Shot Action Recognition", "abstract": "While video action recognition has been an active area of research for\nseveral years, zero-shot action recognition has only recently started gaining\ntraction. In this work, we propose a novel end-to-end trained transformer model\nwhich is capable of capturing long range spatiotemporal dependencies\nefficiently, contrary to existing approaches which use 3D-CNNs. Moreover, to\naddress a common ambiguity in the existing works about classes that can be\nconsidered as previously unseen, we propose a new experimentation setup that\nsatisfies the zero-shot learning premise for action recognition by avoiding\noverlap between the training and testing classes. The proposed approach\nsignificantly outperforms the state of the arts in zero-shot action recognition\nin terms of the the top-1 accuracy on UCF-101, HMDB-51 and ActivityNet\ndatasets. The code and proposed experimentation setup are available in GitHub:\nhttps://github.com/Secure-and-Intelligent-Systems-Lab/SemanticVideoTransformer", "authors": ["Keval Doshi", "Yasin Yilmaz"], "published_date": "2022_03_10", "pdf_url": "http://arxiv.org/pdf/2203.05156v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>Protocol</td><td>UCF</td><td>HMDB</td><td>ActivityNet</td></tr><tr><th>DataAug [47]</th><td>OE</td><td>18.3</td><td>19.7</td><td>-</td></tr><tr><th>InfDem [32]</th><td>OE</td><td>17.8</td><td>21.3</td><td>-</td></tr><tr><th>Bidirectional [42]</th><td>OE</td><td>21.4</td><td>18.9</td><td>-</td></tr><tr><th>TARN [4]</th><td>OE</td><td>19</td><td>19.5</td><td>-</td></tr><tr><th>Action2Vec [21]</th><td>OE</td><td>22.1</td><td>23.5</td><td>-</td></tr><tr><th>OD [27]</th><td>OE</td><td>26.9</td><td>30.2</td><td>-</td></tr><tr><th>CLASTER [19]</th><td>OE</td><td>46.4</td><td>36.8</td><td>-</td></tr><tr><th>GGM [27]</th><td>OE</td><td>20.3</td><td>20.7</td><td>-</td></tr><tr><th>ViSET-96(600) + CD (Ours)</th><td>OE</td><td>68.3</td><td>40.2</td><td>44.8</td></tr><tr><th>E2E (605classes)</th><td>R</td><td>44.1</td><td>29.8</td><td>26.6</td></tr><tr><th>E2E (664classes)</th><td>R</td><td>48</td><td>32.7</td><td>-</td></tr><tr><th>ViSET-96(505) + CD (Ours)</th><td>R</td><td>45.6</td><td>31.3</td><td>35.8</td></tr><tr><th>ViSET-96(564) + CD (Ours)</th><td>R</td><td>53.2</td><td>34.5</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Comparison with the state-of-the-art methods on standard benchmark datasets using the open-ended (OE) and Restrictive (R) protocols. All the methods are evaluated by randomly splitting the dataset in half and averaging the results over 10 trials.", "list_citation_info": ["[32] Alina Roitberg, Ziad Al-Halah, and Rainer Stiefelhagen. Informed democracy: voting-based novelty detection for action recognition. arXiv preprint arXiv:1810.12819, 2018.", "[4] Mina Bishay, Georgios Zoumpourlis, and Ioannis Patras. Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition. arXiv preprint arXiv:1907.09021, 2019.", "[42] Qian Wang and Ke Chen. Zero-shot visual recognition via bidirectional latent embedding. International Journal of Computer Vision, 124(3):356\u2013383, 2017.", "[19] Shreyank N Gowda, Laura Sevilla-Lara, Frank Keller, and Marcus Rohrbach. Claster: Clustering with reinforcement learning for zero-shot action recognition. arXiv preprint arXiv:2101.07042, 2021.", "[27] Devraj Mandal, Sanath Narayan, Sai Kumar Dwivedi, Vikram Gupta, Shuaib Ahmed, Fahad Shahbaz Khan, and Ling Shao. Out-of-distribution detection for generalized zero-shot action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9985\u20139993, 2019.", "[21] Meera Hahn, Andrew Silva, and James M Rehg. Action2vec: A crossmodal embedding approach to action learning. arXiv preprint arXiv:1901.00484, 2019.", "[47] Xun Xu, Timothy M Hospedales, and Shaogang Gong. Multi-task zero-shot action recognition with prioritised data augmentation. In European Conference on Computer Vision, pages 343\u2013359. Springer, 2016."]}, {"table": "<table><thead><tr><th>Method</th><th>Protocol</th><th>UCF</th><th>HMDB</th><th>ActivityNet</th></tr></thead><tbody><tr><th>E2E (605classes)</th><td>R</td><td>35.3</td><td>24.8</td><td>20.0</td></tr><tr><th>E2E (664classes)</th><td>R</td><td>37.6</td><td>26.9</td><td>-</td></tr><tr><th>ViSET-96(505) + CD</th><td>R</td><td>36.1</td><td>25.7</td><td>26.3</td></tr><tr><th>ViSET-96(564) + CD</th><td>R</td><td>40.2</td><td>30.5</td><td>-</td></tr><tr><th>ViSET-96(505) + CL</th><td>R</td><td>33.9</td><td>25.2</td><td>24.7</td></tr><tr><th>ViSET-96(564) + CL</th><td>R</td><td>38.3</td><td>27.6</td><td>-</td></tr></tbody></table>", "caption": "Table 3: Comparison with the E2E [5] approach under the Restrictive (R) protocol using the entire datasets for testing without any random split. E2E and ViSET are trained on Kinetics-700 and Kinetics-600, respectively.", "list_citation_info": ["[5] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4613\u20134623, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Protocol</th><th>FZSL Split</th></tr></thead><tbody><tr><th>CLASTER</th><td>FZSL</td><td>24.3</td></tr><tr><th>E2E (664 classes)</th><td>FZSL</td><td>30.8</td></tr><tr><th>ViSET-96(600) + CD</th><td>FZSL</td><td>36.7</td></tr></tbody></table>", "caption": "Table 4: Comparison with E2E [5] and our implementation of CLASTER [19] using the proposed Fair ZSL protocol. The models are tested on the test set proposed in Section 3.3.", "list_citation_info": ["[5] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4613\u20134623, 2020.", "[19] Shreyank N Gowda, Laura Sevilla-Lara, Frank Keller, and Marcus Rohrbach. Claster: Clustering with reinforcement learning for zero-shot action recognition. arXiv preprint arXiv:2101.07042, 2021."]}], "citation_info_to_title": {"[42] Qian Wang and Ke Chen. Zero-shot visual recognition via bidirectional latent embedding. International Journal of Computer Vision, 124(3):356\u2013383, 2017.": "Zero-shot visual recognition via bidirectional latent embedding", "[19] Shreyank N Gowda, Laura Sevilla-Lara, Frank Keller, and Marcus Rohrbach. Claster: Clustering with reinforcement learning for zero-shot action recognition. arXiv preprint arXiv:2101.07042, 2021.": "Claster: Clustering with reinforcement learning for zero-shot action recognition", "[27] Devraj Mandal, Sanath Narayan, Sai Kumar Dwivedi, Vikram Gupta, Shuaib Ahmed, Fahad Shahbaz Khan, and Ling Shao. Out-of-distribution detection for generalized zero-shot action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9985\u20139993, 2019.": "Out-of-distribution detection for generalized zero-shot action recognition", "[4] Mina Bishay, Georgios Zoumpourlis, and Ioannis Patras. Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition. arXiv preprint arXiv:1907.09021, 2019.": "Tarn: Temporal attentive relation network for few-shot and zero-shot action recognition", "[21] Meera Hahn, Andrew Silva, and James M Rehg. Action2vec: A crossmodal embedding approach to action learning. arXiv preprint arXiv:1901.00484, 2019.": "Action2vec: A crossmodal embedding approach to action learning", "[5] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4613\u20134623, 2020.": "Rethinking zero-shot video classification: End-to-end training for realistic applications", "[32] Alina Roitberg, Ziad Al-Halah, and Rainer Stiefelhagen. Informed democracy: voting-based novelty detection for action recognition. arXiv preprint arXiv:1810.12819, 2018.": "Informed democracy: voting-based novelty detection for action recognition", "[47] Xun Xu, Timothy M Hospedales, and Shaogang Gong. Multi-task zero-shot action recognition with prioritised data augmentation. In European Conference on Computer Vision, pages 343\u2013359. Springer, 2016.": "Multi-task zero-shot action recognition with prioritised data augmentation"}, "source_title_to_arxiv_id": {"Claster: Clustering with reinforcement learning for zero-shot action recognition": "2101.07042"}}