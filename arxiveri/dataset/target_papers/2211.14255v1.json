{"title": "Degenerate Swin to Win: Plain Window-based Transformer without Sophisticated Operations", "abstract": "The formidable accomplishment of Transformers in natural language processing\nhas motivated the researchers in the computer vision community to build Vision\nTransformers. Compared with the Convolution Neural Networks (CNN), a Vision\nTransformer has a larger receptive field which is capable of characterizing the\nlong-range dependencies. Nevertheless, the large receptive field of Vision\nTransformer is accompanied by the huge computational cost. To boost efficiency,\nthe window-based Vision Transformers emerge. They crop an image into several\nlocal windows, and the self-attention is conducted within each window. To bring\nback the global receptive field, window-based Vision Transformers have devoted\na lot of efforts to achieving cross-window communications by developing several\nsophisticated operations. In this work, we check the necessity of the key\ndesign element of Swin Transformer, the shifted window partitioning. We\ndiscover that a simple depthwise convolution is sufficient for achieving\neffective cross-window communications. Specifically, with the existence of the\ndepthwise convolution, the shifted window configuration in Swin Transformer\ncannot lead to an additional performance improvement. Thus, we degenerate the\nSwin Transformer to a plain Window-based (Win) Transformer by discarding\nsophisticated shifted window partitioning. The proposed Win Transformer is\nconceptually simpler and easier for implementation than Swin Transformer.\nMeanwhile, our Win Transformer achieves consistently superior performance than\nSwin Transformer on multiple computer vision tasks, including image\nrecognition, semantic segmentation, and object detection.", "authors": ["Tan Yu", "Ping Li"], "published_date": "2022_11_25", "pdf_url": "http://arxiv.org/pdf/2211.14255v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>Input Size</td><td>Params. (M)</td><td>FLOPs (G)</td><td>Top-1 Acc. (\\%)</td></tr><tr><td>ReGNetY-4G (Radosavovic et al., 2020)</td><td>224</td><td>21</td><td>4.0</td><td>80.0</td></tr><tr><td>ConvNeXt-T (Liu et al., 2022)</td><td>224</td><td>29</td><td>4.5</td><td>82.1</td></tr><tr><td>DeiT-S (Touvron et al., 2021a)</td><td>224</td><td>22</td><td>4.6</td><td>79.8</td></tr><tr><td>PVT-S (Wang et al., 2021)</td><td>224</td><td>25</td><td>3.8</td><td>79.8</td></tr><tr><td>PiT-S (Heo et al., 2021)</td><td>224</td><td>24</td><td>2.9</td><td>80.9</td></tr><tr><td>T2T-14 (Yuan et al., 2021)</td><td>224</td><td>22</td><td>5.2</td><td>81.5</td></tr><tr><td>TNT-S (Han et al., 2021a)</td><td>224</td><td>24</td><td>5.2</td><td>81.3</td></tr><tr><td>Swin-T (Liu et al., 2021)</td><td>224</td><td>29</td><td>4.5</td><td>81.3</td></tr><tr><td>Shuffle-T (Huang et al., 2021)</td><td>224</td><td>29</td><td>4.6</td><td>82.5</td></tr><tr><td>NesT-T (Zhang et al., 2022)</td><td>224</td><td>17</td><td>5.8</td><td>81.5</td></tr><tr><td>Focal-T (Yang et al., 2021)</td><td>224</td><td>29</td><td>4.9</td><td>82.2</td></tr><tr><td>CrossFormer-S (Wang et al., 2022)</td><td>224</td><td>31</td><td>4.9</td><td>82.5</td></tr><tr><td>CSWin-T (Dong et al., 2021)</td><td>224</td><td>23</td><td>4.3</td><td>82.7</td></tr><tr><td>Win-T (ours)</td><td>224</td><td>29</td><td>4.6</td><td>82.3</td></tr><tr><td>ReGNetY-8G (Radosavovic et al., 2020)</td><td>224</td><td>39</td><td>8.0</td><td>81.7</td></tr><tr><td>ConvNeXt-T (Liu et al., 2022)</td><td>224</td><td>50</td><td>8.7</td><td>83.1</td></tr><tr><td>PVT-L (Wang et al., 2021)</td><td>224</td><td>61</td><td>9.8</td><td>81.7</td></tr><tr><td>T2T-19 (Yuan et al., 2021)</td><td>224</td><td>39</td><td>8.9</td><td>81.9</td></tr><tr><td>MViT-B (Fan et al., 2021)</td><td>224</td><td>37</td><td>7.8</td><td>83.0</td></tr><tr><td>Swin-S (Liu et al., 2021)</td><td>224</td><td>50</td><td>8.7</td><td>83.0</td></tr><tr><td>Twins-B (Chu et al., 2021)</td><td>224</td><td>56</td><td>8.3</td><td>83.2</td></tr><tr><td>Shuffle-S (Huang et al., 2021)</td><td>224</td><td>50</td><td>8.9</td><td>83.5</td></tr><tr><td>NesT-S (Zhang et al., 2022)</td><td>224</td><td>38</td><td>10.4</td><td>83.3</td></tr><tr><td>Focal-S (Yang et al., 2021)</td><td>224</td><td>51</td><td>9.1</td><td>83.5</td></tr><tr><td>CrossFormer-B (Wang et al., 2022)</td><td>224</td><td>52</td><td>9.2</td><td>83.4</td></tr><tr><td>CSWin-S (Dong et al., 2021)</td><td>224</td><td>35</td><td>6.9</td><td>83.6</td></tr><tr><td>Win-S (ours)</td><td>224</td><td>50</td><td>8.9</td><td>83.7</td></tr><tr><td>ConvNeXt-B (Liu et al., 2022)</td><td>224</td><td>89</td><td>15.4</td><td>83.8</td></tr><tr><td>DeiT-B (Touvron et al., 2021a)</td><td>384</td><td>86</td><td>55.4</td><td>83.1</td></tr><tr><td>DeiT-B (Touvron et al., 2021a)</td><td>224</td><td>86</td><td>17.5</td><td>81.8</td></tr><tr><td>PiT-B (Heo et al., 2021)</td><td>224</td><td>74</td><td>12.5</td><td>82.0</td></tr><tr><td>Swin-B (Liu et al., 2021)</td><td>224</td><td>88</td><td>15.4</td><td>83.5</td></tr><tr><td>Twins-L (Chu et al., 2021)</td><td>224</td><td>99</td><td>14.8</td><td>83.7</td></tr><tr><td>Shuffle-B (Huang et al., 2021)</td><td>224</td><td>88</td><td>15.6</td><td>84.0</td></tr><tr><td>NesT-B (Zhang et al., 2022)</td><td>224</td><td>68</td><td>17.9</td><td>83.8</td></tr><tr><td>Focal-B (Yang et al., 2021)</td><td>224</td><td>90</td><td>16.0</td><td>83.8</td></tr><tr><td>CrossFormer-L (Wang et al., 2022)</td><td>224</td><td>92</td><td>16.1</td><td>84.0</td></tr><tr><td>CSWin-B (Dong et al., 2021)</td><td>224</td><td>78</td><td>15.0</td><td>84.2</td></tr><tr><td>Win-B (ours)</td><td>224</td><td>88</td><td>15.6</td><td>83.5</td></tr></tbody></table>", "caption": "Table 2: Comparisons with convolutional neural networks and other Vision Transformers on ImageNet-1K image classification. ", "list_citation_info": ["Chu et al. [2021] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 9355\u20139366, virtual, 2021.", "Wang et al. [2021] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 548\u2013558, Montreal, Canada, 2021.", "Dong et al. [2021] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows, 2021.", "Zhang et al. [2022] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan \u00d6. Arik, and Tomas Pfister. Nested hierarchical transformer: Towards accurate, data-efficient and interpretable visual understanding. In Proceedings of the thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI), pages 3417\u20133425, Virtual Event, 2022.", "Heo et al. [2021] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 11916\u201311925, Montreal, Canada, 2021.", "Yuan et al. [2021] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis E. H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-Token ViT: Training vision transformers from scratch on ImageNet. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 538\u2013547, Montreal, Canada, 2021.", "Radosavovic et al. [2020] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10425\u201310433, Seattle, WA, 2020.", "Fan et al. [2021] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6804\u20136815, Montreal, Canada, 2021.", "Touvron et al. [2021a] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proceedings of the 38th International Conference on Machine Learning (ICML), pages 10347\u201310357, Virtual Event, 2021a.", "Liu et al. [2022] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, 2022.", "Yang et al. [2021] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 30008\u201330022, virtual, 2021.", "Han et al. [2021a] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. In Advances in Neural Information Processing Systems (NeurIPS), pages 15908\u201315919, virtual, 2021a.", "Liu et al. [2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9992\u201310002, Montreal, Canada, 2021.", "Wang et al. [2022] Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei Liu. CrossFormer: A versatile vision transformer hinging on cross-scale attention. In Proceedings of the Tenth International Conference on Learning Representations (ICLR), Virtual Event, 2022.", "Huang et al. [2021] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Params. (M)</th><th>FLOPs (G)</th><th>SS mIoU (\\%)</th><th>MS mIoU (\\%)</th></tr></thead><tbody><tr><td>ConvNeXt-T (Liu et al., 2022)</td><td>60</td><td>939</td><td>-</td><td>46.7</td></tr><tr><td>TwinsP-S (Chu et al., 2021)</td><td>55</td><td>919</td><td>46.2</td><td>47.5</td></tr><tr><td>Twins-S (Chu et al., 2021)</td><td>54</td><td>901</td><td>46.2</td><td>47.1</td></tr><tr><td>CSWin-T (Dong et al., 2021)</td><td>60</td><td>959</td><td>49.3</td><td>50.4</td></tr><tr><td>Shuffle-T (Huang et al., 2021)</td><td>60</td><td>949</td><td>46.6</td><td>47.6</td></tr><tr><td>Focal-T (Yang et al., 2021)</td><td>62</td><td>998</td><td>45.8</td><td>47.0</td></tr><tr><td>Swin-T (Dong et al., 2021)</td><td>60</td><td>945</td><td>44.5</td><td>45.8</td></tr><tr><td>Win-T (ours)</td><td>60</td><td>949</td><td>45.7</td><td>47.2</td></tr><tr><td>ConvNeXt-S (Liu et al., 2022)</td><td>82</td><td>1027</td><td>-</td><td>49.6</td></tr><tr><td>TwinsP-B (Chu et al., 2021)</td><td>74</td><td>977</td><td>47.1</td><td>48.4</td></tr><tr><td>Twins-B (Chu et al., 2021)</td><td>89</td><td>1020</td><td>47.7</td><td>48.9</td></tr><tr><td>CSWin-S (Dong et al., 2021)</td><td>65</td><td>1027</td><td>50.4</td><td>50.8</td></tr><tr><td>Shuffle-S (Huang et al., 2021)</td><td>81</td><td>1044</td><td>48.4</td><td>49.6</td></tr><tr><td>Focal-S (Yang et al., 2021)</td><td>85</td><td>1130</td><td>48.0</td><td>50.0</td></tr><tr><td>Swin-S (Dong et al., 2021)</td><td>81</td><td>1038</td><td>47.6</td><td>49.4</td></tr><tr><td>Win-S (ours)</td><td>81</td><td>1044</td><td>48.4</td><td>49.8</td></tr><tr><td>ConvNeXt-B (Liu et al., 2022)</td><td>122</td><td>1170</td><td>-</td><td>49.9</td></tr><tr><td>TwinsP-L (Chu et al., 2021)</td><td>92</td><td>1041</td><td>48.6</td><td>49.8</td></tr><tr><td>Twins-L (Chu et al., 2021)</td><td>133</td><td>1164</td><td>48.8</td><td>50.2</td></tr><tr><td>CSWin-B (Dong et al., 2021)</td><td>109</td><td>1222</td><td>51.1</td><td>51.7</td></tr><tr><td>Shuffle-B (Huang et al., 2021)</td><td>121</td><td>1196</td><td>49.0</td><td>50.5</td></tr><tr><td>Focal-B (Yang et al., 2021)</td><td>126</td><td>1354</td><td>49.0</td><td>50.5</td></tr><tr><td>Swin-B (Dong et al., 2021)</td><td>121</td><td>1188</td><td>48.1</td><td>49.7</td></tr><tr><td>Win-B (ours)</td><td>121</td><td>1196</td><td>49.2</td><td>50.7</td></tr></tbody></table>", "caption": "Table 7: Comparisons with the state-of-the-art convolutional neural networks and Vision Transformers on ADE20K semantic segmentation. FLOPs is measured on the input image of 1024\\times 1024 resolution. SS denotes the single-scale setting and MS denotes the multi-scale setting. ", "list_citation_info": ["Chu et al. [2021] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 9355\u20139366, virtual, 2021.", "Dong et al. [2021] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows, 2021.", "Liu et al. [2022] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, 2022.", "Yang et al. [2021] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 30008\u201330022, virtual, 2021.", "Huang et al. [2021] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><th>Params</th><th>FLOPs</th><td>AP{}^{b}</td><td>AP{}^{b}_{50}</td><td>AP{}^{b}_{75}</td><td>AP{}^{m}</td><td>AP{}^{m}_{50}</td><td>AP{}^{m}_{75}</td></tr><tr><th>ResNet-50 (He et al., 2016)</th><th>44 M</th><th>260 G</th><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><th>ConvNeXt-T (Liu et al., 2022)</th><th>48 M</th><th>262 G</th><td>46.2</td><td>67.9</td><td>50.8</td><td>41.7</td><td>65.0</td><td>44.9</td></tr><tr><th>PVT-S (Wang et al., 2021)</th><th>44 M</th><th>256 G</th><td>43.0</td><td>65.3</td><td>46.9</td><td>39.9</td><td>62.5</td><td>42.8</td></tr><tr><th>ViL-S (Zhang et al., 2021)</th><th>45 M</th><th>218 G</th><td>47.1</td><td>68.7</td><td>51.5</td><td>42.7</td><td>65.9</td><td>46.2</td></tr><tr><th>TwinsP-S (Chu et al., 2021)</th><th>44 M</th><th>245 G</th><td>46.8</td><td>69.3</td><td>51.8</td><td>42.6</td><td>66.3</td><td>46.0</td></tr><tr><th>Twins-S (Chu et al., 2021)</th><th>44 M</th><th>228 G</th><td>46.8</td><td>69.2</td><td>51.2</td><td>42.6</td><td>66.3</td><td>45.8</td></tr><tr><th>CSWin-T (Dong et al., 2021)</th><th>42 M</th><th>279 G</th><td>49.0</td><td>70.7</td><td>53.7</td><td>43.6</td><td>67.9</td><td>46.6</td></tr><tr><th>Shuffle-T (Huang et al., 2021)</th><th>48 M</th><th>268 G</th><td>46.8</td><td>68.9</td><td>51.5</td><td>42.3</td><td>66.0</td><td>45.6</td></tr><tr><th>Focal-T (Yang et al., 2021)</th><th>49 M</th><th>291 G</th><td>47.2</td><td>69.4</td><td>51.9</td><td>42.7</td><td>66.5</td><td>45.9</td></tr><tr><th>Swin-T (Dong et al., 2021)</th><th>48 M</th><th>264 G</th><td>46.0</td><td>68.2</td><td>50.2</td><td>41.6</td><td>65.1</td><td>44.8</td></tr><tr><th>Win-T (ours)</th><th>48 M</th><th>268 G</th><td>46.6</td><td>68.4</td><td>51.1</td><td>42.1</td><td>65.7</td><td>45.3</td></tr><tr><th>ResNet-101 (He et al., 2016)</th><th>63 M</th><th>336 G</th><td>42.8</td><td>63.2</td><td>47.1</td><td>38.5</td><td>60.1</td><td>41.3</td></tr><tr><th>PVT-M (Wang et al., 2021)</th><th>64 M</th><th>302 G</th><td>44.2</td><td>66.0</td><td>48.2</td><td>40.5</td><td>63.1</td><td>43.5</td></tr><tr><th>ViL-M (Zhang et al., 2021)</th><th>60 M</th><th>261 G</th><td>44.6</td><td>66.3</td><td>48.5</td><td>40.7</td><td>63.8</td><td>43.7</td></tr><tr><th>TwinsP-B (Chu et al., 2021)</th><th>64 M</th><th>302 G</th><td>47.9</td><td>70.1</td><td>52.5</td><td>43.2</td><td>67.2</td><td>46.3</td></tr><tr><th>Twins-B (Chu et al., 2021)</th><th>76 M</th><th>340 G</th><td>48.0</td><td>69.5</td><td>52.7</td><td>43.0</td><td>66.8</td><td>46.6</td></tr><tr><th>CSWin-S (Dong et al., 2021)</th><th>54 M</th><th>342 G</th><td>50.0</td><td>71.3</td><td>54.7</td><td>44.5</td><td>68.4</td><td>47.7</td></tr><tr><th>Shuffle-S (Huang et al., 2021)</th><th>69 M</th><th>359 G</th><td>48.4</td><td>70.1</td><td>53.5</td><td>43.3</td><td>67.3</td><td>46.7</td></tr><tr><th>Focal-S (Yang et al., 2021)</th><th>71 M</th><th>401 G</th><td>48.8</td><td>70.5</td><td>53.6</td><td>43.8</td><td>67.7</td><td>47.2</td></tr><tr><th>Swin-S (Dong et al., 2021)</th><th>69 M</th><th>354 G</th><td>48.5</td><td>70.2</td><td>53.5</td><td>43.3</td><td>67.3</td><td>46.6</td></tr><tr><th>Win-S (ours)</th><th>69 M</th><th>359 G</th><td>48.6</td><td>69.8</td><td>53.3</td><td>43.4</td><td>67.3</td><td>46.7</td></tr><tr><th>ResNeXt101-64 (Xie et al., 2017)</th><th>101 M</th><th>493 G</th><td>44.4</td><td>64.9</td><td>48.8</td><td>39.7</td><td>61.9</td><td>42.6</td></tr><tr><th>PVT-L (Wang et al., 2021)</th><th>81 M</th><th>364 G</th><td>44.5</td><td>66.0</td><td>48.3</td><td>40.7</td><td>63.4</td><td>43.7</td></tr><tr><th>ViL-B (Zhang et al., 2021)</th><th>76 M</th><th>365 G</th><td>45.7</td><td>67.2</td><td>49.9</td><td>41.3</td><td>64.4</td><td>44.5</td></tr><tr><th>CSWin-B (Dong et al., 2021)</th><th>97 M</th><th>526 G</th><td>50.8</td><td>72.1</td><td>55.8</td><td>44.9</td><td>69.1</td><td>48.3</td></tr><tr><th>Focal-B (Yang et al., 2021)</th><th>110 M</th><th>533 G</th><td>49.0</td><td>70.1</td><td>53.6</td><td>43.7</td><td>67.6</td><td>47.0</td></tr><tr><th>Swin-B (Dong et al., 2021)</th><th>107 M</th><th>496 G</th><td>48.5</td><td>69.8</td><td>53.2</td><td>43.4</td><td>66.8</td><td>46.9</td></tr><tr><th>Win-B (ours)</th><th>108 M</th><th>503 G</th><td>49.1</td><td>70.2</td><td>53.8</td><td>43.8</td><td>67.8</td><td>47.1</td></tr></tbody></table>", "caption": "Table 9: Comparisons with convolutional neural networks and Vision Transformers based on Mask-RCNN framework on MSCOCO-2017 object detection and instance segmentation. FLOPs is evaluated on 1280\\times 800resolution. AP{}^{b} denotes the box-level average precision for evaluating the performance of object detection and AP{}^{m} denotes the mask-level average precision for instance segmentation.  ", "list_citation_info": ["Chu et al. [2021] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 9355\u20139366, virtual, 2021.", "Wang et al. [2021] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 548\u2013558, Montreal, Canada, 2021.", "Dong et al. [2021] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows, 2021.", "Xie et al. [2017] Saining Xie, Ross B. Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5987\u20135995, Honolulu, HI, 2017.", "Zhang et al. [2021] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2978\u20132988, Montreal, Canada, 2021.", "Liu et al. [2022] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, 2022.", "Yang et al. [2021] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 30008\u201330022, virtual, 2021.", "He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, Las Vegas, NV, 2016.", "Huang et al. [2021] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021."]}], "citation_info_to_title": {"Chu et al. [2021] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 9355\u20139366, virtual, 2021.": "Twins: Revisiting the design of spatial attention in vision transformers", "Dong et al. [2021] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows, 2021.": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows", "Zhang et al. [2022] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan \u00d6. Arik, and Tomas Pfister. Nested hierarchical transformer: Towards accurate, data-efficient and interpretable visual understanding. In Proceedings of the thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI), pages 3417\u20133425, Virtual Event, 2022.": "Nested hierarchical transformer: Towards accurate, data-efficient and interpretable visual understanding", "Wang et al. [2022] Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei Liu. CrossFormer: A versatile vision transformer hinging on cross-scale attention. In Proceedings of the Tenth International Conference on Learning Representations (ICLR), Virtual Event, 2022.": "CrossFormer: A versatile vision transformer hinging on cross-scale attention", "Yuan et al. [2021] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis E. H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-Token ViT: Training vision transformers from scratch on ImageNet. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 538\u2013547, Montreal, Canada, 2021.": "Tokens-to-Token ViT: Training vision transformers from scratch on ImageNet", "Zhang et al. [2021] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2978\u20132988, Montreal, Canada, 2021.": "Multi-scale vision longformer: A new vision transformer for high-resolution image encoding", "He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, Las Vegas, NV, 2016.": "Deep residual learning for image recognition", "Yang et al. [2021] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), pages 30008\u201330022, virtual, 2021.": "Focal attention for long-range interactions in vision transformers", "Han et al. [2021a] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. In Advances in Neural Information Processing Systems (NeurIPS), pages 15908\u201315919, virtual, 2021a.": "Transformer in transformer", "Radosavovic et al. [2020] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10425\u201310433, Seattle, WA, 2020.": "Designing network design spaces", "Liu et al. [2022] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, 2022.": "A convnet for the 2020s", "Fan et al. [2021] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6804\u20136815, Montreal, Canada, 2021.": "Multiscale Vision Transformers", "Liu et al. [2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9992\u201310002, Montreal, Canada, 2021.": "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows", "Touvron et al. [2021a] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Proceedings of the 38th International Conference on Machine Learning (ICML), pages 10347\u201310357, Virtual Event, 2021a.": "Training data-efficient image transformers & distillation through attention", "Xie et al. [2017] Saining Xie, Ross B. Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5987\u20135995, Honolulu, HI, 2017.": "Aggregated Residual Transformations for Deep Neural Networks", "Wang et al. [2021] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 548\u2013558, Montreal, Canada, 2021.": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "Huang et al. [2021] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, and Bin Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021.": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer", "Heo et al. [2021] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 11916\u201311925, Montreal, Canada, 2021.": "Rethinking spatial dimensions of vision transformers"}, "source_title_to_arxiv_id": {"A convnet for the 2020s": "2201.03545", "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows": "2103.14030", "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions": "2102.12122"}}