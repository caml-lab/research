{"title": "Deep Learning Models for Automated Classification of Dog Emotional States from Facial Expressions", "abstract": "Similarly to humans, facial expressions in animals are closely linked with\nemotional states. However, in contrast to the human domain, automated\nrecognition of emotional states from facial expressions in animals is\nunderexplored, mainly due to difficulties in data collection and establishment\nof ground truth concerning emotional states of non-verbal users. We apply\nrecent deep learning techniques to classify (positive) anticipation and\n(negative) frustration of dogs on a dataset collected in a controlled\nexperimental setting. We explore the suitability of different backbones (e.g.\nResNet, ViT) under different supervisions to this task, and find that features\nof a self-supervised pretrained ViT (DINO-ViT) are superior to the other\nalternatives. To the best of our knowledge, this work is the first to address\nthe task of automatic classification of canine emotions on data acquired in a\ncontrolled experiment.", "authors": ["Tali Boneh-Shitrit", "Shir Amir", "Annika Bremhorst", "Daniel S. Mills", "Stefanie Riemer", "Dror Fried", "Anna Zamansky"], "published_date": "2022_06_11", "pdf_url": "http://arxiv.org/pdf/2206.05619v1", "list_table_and_caption": [{"table": "<table><thead><tr><th colspan=\"2\">Backbone</th><th>Train Accuracy</th><th>Val. Accuracy</th></tr></thead><tbody><tr><th rowspan=\"2\">Sup.</th><th>ResNet50 [28]</th><td>0.800</td><td>0.809</td></tr><tr><th>ViT [21]</th><td>0.869</td><td>0.780</td></tr><tr><th rowspan=\"2\">DINO</th><th>ResNet50 [13]</th><td>0.870</td><td>0.813</td></tr><tr><th>ViT [13]</th><td>0.878</td><td>0.853</td></tr></tbody></table>", "caption": "Table 1: Classification Results. The best results are achieved using a pre-trained DINO-ViT as a backbone.", "list_citation_info": ["[13] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. ICCV, 2021.", "[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.", "[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016."]}], "citation_info_to_title": {"[13] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. ICCV, 2021.": "Emerging properties in self-supervised vision transformers", "[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016.": "Deep Residual Learning for Image Recognition", "[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale"}, "source_title_to_arxiv_id": {"Emerging properties in self-supervised vision transformers": "2104.14294"}}