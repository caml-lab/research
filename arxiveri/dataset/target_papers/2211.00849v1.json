{"title": "P$^3$OVD: Fine-grained Visual-Text Prompt-Driven Self-Training for Open-Vocabulary Object Detection", "abstract": "Inspired by the success of visual-language methods (VLMs) in zero-shot\nclassification, recent works attempt to extend this line of work into object\ndetection by leveraging the localization ability of pre-trained VLMs and\ngenerating pseudo labels for unseen classes in a self-training manner. However,\nsince the current VLMs are usually pre-trained with aligning sentence embedding\nwith global image embedding, the direct use of them lacks fine-grained\nalignment for object instances, which is the core of detection. In this paper,\nwe propose a simple but effective Pretrain-adaPt-Pseudo labeling paradigm for\nOpen-Vocabulary Detection (P$^3$OVD) that introduces a fine-grained visual-text\nprompt adapting stage to enhance the current self-training paradigm with a more\npowerful fine-grained alignment. During the adapting stage, we enable VLM to\nobtain fine-grained alignment by using learnable text prompts to resolve an\nauxiliary dense pixel-wise prediction task. Furthermore, we propose a visual\nprompt module to provide the prior task information (i.e., the categories need\nto be predicted) for the vision branch to better adapt the pretrained VLM to\nthe downstream tasks. Experiments show that our method achieves the\nstate-of-the-art performance for open-vocabulary object detection, e.g., 31.5%\nmAP on unseen classes of COCO.", "authors": ["Yanxin Long", "Jianhua Han", "Runhui Huang", "Xu Hang", "Yi Zhu", "Chunjing Xu", "Xiaodan Liang"], "published_date": "2022_11_02", "pdf_url": "http://arxiv.org/pdf/2211.00849v1", "list_table_and_caption": [{"table": "<br/><table><tbody><tr><td>Method</td><td>VL Model</td><td>Using COCO caption</td><td>Novel</td><td>Base</td><td>Overall</td></tr><tr><td>SB[18]</td><td>\u2717</td><td>\u2717</td><td>0.31</td><td>29.2</td><td>24.9</td></tr><tr><td>LAB[18]</td><td>\u2717</td><td>\u2717</td><td>0.22</td><td>20.8</td><td>18.0</td></tr><tr><td>DSES[18]</td><td>\u2717</td><td>\u2717</td><td>0.27</td><td>26.7</td><td>22.1</td></tr><tr><td>DELO[66]</td><td>\u2717</td><td>\u2717</td><td>3.41</td><td>13.8</td><td>13.0</td></tr><tr><td>PL[67]</td><td>\u2717</td><td>\u2717</td><td>4.12</td><td>35.9</td><td>27.9</td></tr><tr><td>OVR-CNN [16]</td><td>-</td><td>\u2713</td><td>22.8</td><td>46.0</td><td>39.9</td></tr><tr><td>ViLD* [19]</td><td>CLIP</td><td>\u2717</td><td>27.6</td><td>59.5</td><td>51.3</td></tr><tr><td colspan=\"6\">Self-training based open-vocabulary detection methods</td></tr><tr><td>OVD-ALBEF[20]</td><td>ALBEF</td><td>\u2713</td><td>30.8</td><td>46.1</td><td>42.1</td></tr><tr><td>Detic[7]</td><td>-</td><td>\u2717</td><td>27.8</td><td>47.1</td><td>45.0</td></tr><tr><td>P{}^{3}OVD (w/o FT)</td><td>CLIP</td><td>\u2717</td><td>29.8</td><td>51.8</td><td>46.1</td></tr><tr><td>P{}^{3}OVD (Ours)</td><td>CLIP</td><td>\u2717</td><td>31.5</td><td>51.9</td><td>46.6</td></tr></tbody></table>", "caption": "TABLE I: Open-vocabulary detection performance comparison on COCO datasets. P{}^{3}OVD (w/o FT) denotes the P{}^{3}OVD without fine-grained adapting stage. We can observe that P{}^{3}OVD achieves the state-of-the-art detection performance on the novel classes. Without the fine-tuning stage, the performance on the novel classes suffers a large drop. Note that OVR-CNN pre-trains a self-designed vision-language model with a vision-to-language (V2L) module by itself. ViLD* trains the detector with data augmentations of large-scale jittering (LSJ) [6] and longer training schedule (16x). mAP (%) is reported. ", "list_citation_info": ["[16] A. Zareian, K. D. Rosa, D. H. Hu, and S.-F. Chang, \u201cOpen-vocabulary object detection using captions,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14\u2009393\u201314\u2009402.", "[19] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, \u201cOpen-vocabulary object detection via vision and language knowledge distillation,\u201d arXiv preprint arXiv:2104.13921, vol. 2, 2021.", "[66] P. Zhu, H. Wang, and V. Saligrama, \u201cDon\u2019t even look once: Synthesizing features for zero-shot detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009693\u201311\u2009702.", "[67] S. Rahman, S. Khan, and N. Barnes, \u201cImproved visual-semantic alignment for zero-shot object detection,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 11\u2009932\u201311\u2009939.", "[7] X. Zhou, R. Girdhar, A. Joulin, P. Kr\u00e4henb\u00fchl, and I. Misra, \u201cDetecting twenty-thousand classes using image-level supervision,\u201d arXiv preprint arXiv:2201.02605, 2022.", "[18] A. Bansal, K. Sikka, G. Sharma, R. Chellappa, and A. Divakaran, \u201cZero-shot object detection,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 384\u2013400.", "[20] M. Gao, C. Xing, J. C. Niebles, J. Li, R. Xu, W. Liu, and C. Xiong, \u201cTowards open vocabulary object detection without human-provided bounding boxes,\u201d arXiv preprint arXiv:2111.09452, 2021.", "[6] G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V. Le, and B. Zoph, \u201cSimple copy-paste is a strong data augmentation method for instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 2918\u20132928."]}], "citation_info_to_title": {"[18] A. Bansal, K. Sikka, G. Sharma, R. Chellappa, and A. Divakaran, \u201cZero-shot object detection,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 384\u2013400.": "Zero-shot object detection", "[20] M. Gao, C. Xing, J. C. Niebles, J. Li, R. Xu, W. Liu, and C. Xiong, \u201cTowards open vocabulary object detection without human-provided bounding boxes,\u201d arXiv preprint arXiv:2111.09452, 2021.": "Towards open vocabulary object detection without human-provided bounding boxes", "[16] A. Zareian, K. D. Rosa, D. H. Hu, and S.-F. Chang, \u201cOpen-vocabulary object detection using captions,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14\u2009393\u201314\u2009402.": "Open-vocabulary object detection using captions", "[7] X. Zhou, R. Girdhar, A. Joulin, P. Kr\u00e4henb\u00fchl, and I. Misra, \u201cDetecting twenty-thousand classes using image-level supervision,\u201d arXiv preprint arXiv:2201.02605, 2022.": "Detecting twenty-thousand classes using image-level supervision", "[6] G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V. Le, and B. Zoph, \u201cSimple copy-paste is a strong data augmentation method for instance segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 2918\u20132928.": "Simple copy-paste is a strong data augmentation method for instance segmentation", "[66] P. Zhu, H. Wang, and V. Saligrama, \u201cDon\u2019t even look once: Synthesizing features for zero-shot detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009693\u201311\u2009702.": "Don\u2019t even look once: Synthesizing features for zero-shot detection", "[19] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui, \u201cOpen-vocabulary object detection via vision and language knowledge distillation,\u201d arXiv preprint arXiv:2104.13921, vol. 2, 2021.": "Open-vocabulary object detection via vision and language knowledge distillation", "[67] S. Rahman, S. Khan, and N. Barnes, \u201cImproved visual-semantic alignment for zero-shot object detection,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, 2020, pp. 11\u2009932\u201311\u2009939.": "Improved visual-semantic alignment for zero-shot object detection"}, "source_title_to_arxiv_id": {"Zero-shot object detection": "1803.06049", "Simple copy-paste is a strong data augmentation method for instance segmentation": "2012.07177"}}