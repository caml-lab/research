{"title": "MFFN: Multi-view Feature Fusion Network for Camouflaged Object Detection", "abstract": "Recent research about camouflaged object detection (COD) aims to segment\nhighly concealed objects hidden in complex surroundings. The tiny, fuzzy\ncamouflaged objects result in visually indistinguishable properties. However,\ncurrent single-view COD detectors are sensitive to background distractors.\nTherefore, blurred boundaries and variable shapes of the camouflaged objects\nare challenging to be fully captured with a single-view detector. To overcome\nthese obstacles, we propose a behavior-inspired framework, called Multi-view\nFeature Fusion Network (MFFN), which mimics the human behaviors of finding\nindistinct objects in images, i.e., observing from multiple angles, distances,\nperspectives. Specifically, the key idea behind it is to generate multiple ways\nof observation (multi-view) by data augmentation and apply them as inputs. MFFN\ncaptures critical boundary and semantic information by comparing and fusing\nextracted multi-view features. In addition, our MFFN exploits the dependence\nand interaction between views and channels. Specifically, our methods leverage\nthe complementary information between different views through a two-stage\nattention module called Co-attention of Multi-view (CAMV). And we design a\nlocal-overall module called Channel Fusion Unit (CFU) to explore the\nchannel-wise contextual clues of diverse feature maps in an iterative manner.\nThe experiment results show that our method performs favorably against existing\nstate-of-the-art methods via training with the same data. The code will be\navailable at https://github.com/dwardzheng/MFFN_COD.", "authors": ["Dehua Zheng", "Xiaochen Zheng", "Laurence T. Yang", "Yuan Gao", "Chenlu Zhu", "Yiheng Ruan"], "published_date": "2022_10_12", "pdf_url": "http://arxiv.org/pdf/2210.06361v3", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Accepted by</th><th rowspan=\"2\">Model</th><td colspan=\"5\">CHAMELEON</td><td colspan=\"4\">COD10K</td><td colspan=\"6\">NC4K</td></tr><tr><td>S_{m}\\uparrow</td><td>F_{\\beta}^{\\omega}\\uparrow</td><td>MAE\\downarrow</td><td>F_{\\beta}\\uparrow</td><td>E_{m}\\uparrow</td><td>S_{m}\\uparrow</td><td>F_{\\beta}^{\\omega}\\uparrow</td><td>MAE\\downarrow</td><td>F_{\\beta}\\uparrow</td><td>E_{m}\\uparrow</td><td>S_{m}\\uparrow</td><td>F_{\\beta}^{\\omega}\\uparrow</td><td>MAE\\downarrow</td><td>F_{\\beta}\\uparrow</td><td>E_{m}\\uparrow</td></tr><tr><th colspan=\"17\">Salient Object Detection / Medical Image Segmentation</th></tr><tr><th>CVPR2018</th><th>PiCANet [28]</th><td>0.765</td><td>0.552</td><td>0.085</td><td>0.618</td><td>0.846</td><td>0.696</td><td>0.415</td><td>0.081</td><td>0.489</td><td>0.788</td><td>0.758</td><td>0.57</td><td>0.088</td><td>0.64</td><td>0.835</td></tr><tr><th>CVPR2019</th><th>BASNet [40]</th><td>0.847</td><td>0.771</td><td>0.044</td><td>0.795</td><td>0.894</td><td>0.661</td><td>0.432</td><td>0.071</td><td>0.486</td><td>0.749</td><td>0.695</td><td>0.546</td><td>0.095</td><td>0.61</td><td>0.785</td></tr><tr><th>CVPR2019</th><th>CPD [51]</th><td>0.857</td><td>0.731</td><td>0.048</td><td>0.771</td><td>0.923</td><td>0.75</td><td>0.531</td><td>0.053</td><td>0.595</td><td>0.853</td><td>0.787</td><td>0.645</td><td>0.072</td><td>0.705</td><td>0.866</td></tr><tr><th>CVPR2019</th><th>PoolNet [27]</th><td>0.845</td><td>0.69</td><td>0.054</td><td>0.749</td><td>0.933</td><td>0.74</td><td>0.506</td><td>0.056</td><td>0.575</td><td>0.844</td><td>0.785</td><td>0.635</td><td>0.073</td><td>0.699</td><td>0.865</td></tr><tr><th>ICCV2019</th><th>EGNet [63]</th><td>0.797</td><td>0.649</td><td>0.065</td><td>0.702</td><td>0.884</td><td>0.736</td><td>0.517</td><td>0.061</td><td>0.582</td><td>0.854</td><td>0.777</td><td>0.639</td><td>0.075</td><td>0.696</td><td>0.864</td></tr><tr><th>AAAI2020</th><th>F3Net [50]</th><td>0.848</td><td>0.744</td><td>0.047</td><td>0.77</td><td>0.917</td><td>0.739</td><td>0.544</td><td>0.051</td><td>0.593</td><td>0.819</td><td>0.78</td><td>0.656</td><td>0.07</td><td>0.705</td><td>0.848</td></tr><tr><th>ICCV2019</th><th>SCRN [52]</th><td>0.876</td><td>0.741</td><td>0.042</td><td>0.787</td><td>0.939</td><td>0.789</td><td>0.575</td><td>0.047</td><td>0.651</td><td>0.88</td><td>0.83</td><td>0.698</td><td>0.059</td><td>0.757</td><td>0.897</td></tr><tr><th>CVPR2020</th><th>CSNet [11]</th><td>0.856</td><td>0.718</td><td>0.047</td><td>0.766</td><td>0.928</td><td>0.778</td><td>0.569</td><td>0.047</td><td>0.634</td><td>0.871</td><td>0.75</td><td>0.603</td><td>0.088</td><td>0.655</td><td>0.793</td></tr><tr><th>CVPR2020</th><th>SSAL [60]</th><td>0.757</td><td>0.639</td><td>0.071</td><td>0.702</td><td>0.856</td><td>0.668</td><td>0.454</td><td>0.066</td><td>0.527</td><td>0.7789</td><td>0.699</td><td>0.561</td><td>0.093</td><td>0.644</td><td>0.812</td></tr><tr><th>CVPR2020</th><th>UCNet [59]</th><td>0.88</td><td>0.817</td><td>0.036</td><td>0.836</td><td>0.941</td><td>0.776</td><td>0.633</td><td>0.042</td><td>0.681</td><td>0.867</td><td>0.811</td><td>0.729</td><td>0.055</td><td>0.775</td><td>0.886</td></tr><tr><th>CVPR2020</th><th>MINet [36]</th><td>0.855</td><td>0.771</td><td>0.036</td><td>0.802</td><td>0.937</td><td>0.77</td><td>0.608</td><td>0.042</td><td>0.657</td><td>0.859</td><td>0.812</td><td>0.72</td><td>0.056</td><td>0.764</td><td>0.887</td></tr><tr><th>CVPR2020</th><th>ITSD [65]</th><td>0.814</td><td>0.662</td><td>0.057</td><td>0.705</td><td>0.901</td><td>0.767</td><td>0.557</td><td>0.051</td><td>0.615</td><td>0.861</td><td>0.811</td><td>0.679</td><td>0.064</td><td>0.729</td><td>0.883</td></tr><tr><th>MICCAI2020</th><th>PraNet [5]</th><td>0.86</td><td>0.763</td><td>0.044</td><td>0.789</td><td>0.935</td><td>0.789</td><td>0.629</td><td>0.045</td><td>0.671</td><td>0.879</td><td>0.822</td><td>0.724</td><td>0.059</td><td>0.763</td><td>0.888</td></tr><tr><th colspan=\"17\">Camouflaged Object Detection</th></tr><tr><th>CVPR2020</th><th>SINet [6]</th><td>0.872</td><td>0.806</td><td>0.034</td><td>0.827</td><td>0.946</td><td>0.776</td><td>0.631</td><td>0.043</td><td>0.679</td><td>0.874</td><td>0.808</td><td>0.723</td><td>0.058</td><td>0.769</td><td>0.883</td></tr><tr><th>CVPR2021</th><th>SLSR [32]</th><td>0.89</td><td>0.822</td><td>0.03</td><td>0.841</td><td>0.948</td><td>0.804</td><td>0.673</td><td>0.037</td><td>0.715</td><td>0.892</td><td>0.84</td><td>0.766</td><td>0.048</td><td>0.804</td><td>0.907</td></tr><tr><th>CVPR2021</th><th>MGL-R [58]</th><td>0.893</td><td>0.812</td><td>0.031</td><td>0.833</td><td>0.941</td><td>0.814</td><td>0.666</td><td>0.035</td><td>0.71</td><td>0.89</td><td>0.833</td><td>0.739</td><td>0.053</td><td>0.782</td><td>0.893</td></tr><tr><th>CVPR2021</th><th>PFNet [34]</th><td>0.882</td><td>0.81</td><td>0.033</td><td>0.828</td><td>0.945</td><td>0.8</td><td>0.66</td><td>0.04</td><td>0.701</td><td>0.89</td><td>0.829</td><td>0.745</td><td>0.053</td><td>0.784</td><td>0.898</td></tr><tr><th>CVPR2021</th><th>UJSC* [20]</th><td>0.891</td><td>0.833</td><td>.0.030</td><td>0.847</td><td>0.955</td><td>0.809</td><td>0.684</td><td>0.035</td><td>0.721</td><td>0.891</td><td>0.842</td><td>0.771</td><td>0.047</td><td>0.806</td><td>0.907</td></tr><tr><th>IJCAI2021</th><th>C2FNet [45]</th><td>0.888</td><td>0.828</td><td>0.032</td><td>0.844</td><td>0.946</td><td>0.813</td><td>0.686</td><td>0.036</td><td>0.723</td><td>0.9</td><td>0.838</td><td>0.762</td><td>0.049</td><td>0.795</td><td>0.904</td></tr><tr><th>ICCV2021</th><th>UGTR [56]</th><td>0.888</td><td>0.794</td><td>0.031</td><td>0.819</td><td>0.94</td><td>0.817</td><td>0.666</td><td>0.036</td><td>0.711</td><td>0.89</td><td>0.839</td><td>0.746</td><td>0.052</td><td>0.787</td><td>0.899</td></tr><tr><th>CVPR2022</th><th>ZoomNet [35]</th><td>0.902</td><td>0.845</td><td>0.023</td><td>0.864</td><td>0.958</td><td>0.838</td><td>0.729</td><td>0.029</td><td>0.766</td><td>0.911</td><td>0.853</td><td>0.784</td><td>0.043</td><td>0.818</td><td>0.912</td></tr><tr><th>OURS</th><th>MFFN</th><td>0.905</td><td>0.852</td><td>0.021</td><td>0.871</td><td>0.963</td><td>0.846</td><td>0.745</td><td>0.028</td><td>0.782</td><td>0.917</td><td>0.856</td><td>0.791</td><td>0.042</td><td>0.827</td><td>0.915</td></tr></tbody></table>", "caption": "Table 1: Comparison of evaluation results of different models on CHAMELEON,COD10K and NC4K. The best model results will be highlighted in green.", "list_citation_info": ["[5] Dengping Fan, Gepeng Ji, Tao Zhou, Geng Chen, Huazhong Fu, Shen Jianbing, and Ling Shao. Pranet: Parallel reverse attention network for polyp segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), October 2020.", "[51] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[34] Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, and Deng-Ping Fan. Camouflaged object segmentation with distraction mining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8772\u20138781, June 2021.", "[63] Jiaxing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[50] Jun Wei, Shuhui Wang, and Qingming Huang. F3net: Fusion, feedback and focus for salient object detection. AAAI Conference on Artificial Intelligence (AAAI), February 2020.", "[11] Shanghua Gao, Yongqiang Tan, Mingming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Proceedings of the European Conference on Computer Vision (ECCV), August 2020.", "[27] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[36] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[58] Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, and Deng-Ping Fan. Mutual graph learning for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.", "[35] Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, and Huchuan Lu. Zoom in and out: A mixed-scale triplet network for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.", "[59] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[60] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-supervised salient object detection via scribble annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[6] Dengping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[28] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "[52] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[32] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, and Deng-Ping Fan. Simultaneously localize, segment and rank the camouflaged objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.", "[65] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for accurate and fast saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[45] Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, and Nian Liu. Context-aware cross-level fusion network for camouflaged object detection. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, August 2021.", "[56] Fan Yang, Qiang Zhai, Xin Li, Rui Huang, Ao Luo, Hong Cheng, and Deng-Ping Fan. Uncertainty-guided transformer reasoning for camouflaged object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021.", "[20] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang, and Yuchao Dai. Uncertainty-aware joint salient object and camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.", "[40] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019."]}, {"table": "<table><tbody><tr><th>Method</th><td>MFFN(Ours)</td><td>UGTR [56]</td><td>UJSC [20]</td><td>ZoomNet [35]</td><td>PfNet  [34]</td><td>MGL-R [58]</td><td>SLSR [32]</td></tr><tr><th>Parameters</th><td>36.554M</td><td>48.868M</td><td>217.982M</td><td>32.382M</td><td>46.498M</td><td>63.595M</td><td>50.935M</td></tr></tbody></table>", "caption": "Table 2: Comparison of the number of parameters of our proposed MFFN and other SOTA models.", "list_citation_info": ["[35] Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, and Huchuan Lu. Zoom in and out: A mixed-scale triplet network for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.", "[58] Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, and Deng-Ping Fan. Mutual graph learning for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.", "[56] Fan Yang, Qiang Zhai, Xin Li, Rui Huang, Ao Luo, Hong Cheng, and Deng-Ping Fan. Uncertainty-guided transformer reasoning for camouflaged object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021.", "[34] Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, and Deng-Ping Fan. Camouflaged object segmentation with distraction mining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8772\u20138781, June 2021.", "[32] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, and Deng-Ping Fan. Simultaneously localize, segment and rank the camouflaged objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.", "[20] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang, and Yuchao Dai. Uncertainty-aware joint salient object and camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th rowspan=\"2\">Backbone</th><th colspan=\"5\">ECSSD</th><th colspan=\"4\">HKU-IS</th><th colspan=\"6\">DUTS-TE</th></tr><tr><th>S_{m}\\uparrow</th><th>F_{\\beta}^{\\omega}\\uparrow</th><th>MAE\\downarrow</th><th>F_{\\beta}\\uparrow</th><th>E_{m}\\uparrow</th><th>S_{m}\\uparrow</th><th>F_{\\beta}^{\\omega}\\uparrow</th><th>MAE\\downarrow</th><th>F_{\\beta}\\uparrow</th><th>E_{m}\\uparrow</th><th>S_{m}\\uparrow</th><th>F_{\\beta}^{\\omega}\\uparrow</th><th>MAE\\downarrow</th><th>F_{\\beta}\\uparrow</th><th>E_{m}\\uparrow</th></tr></thead><tbody><tr><th>PAGEnet [49]</th><th>Vgg16</th><td>0.912</td><td>0.886</td><td>0.042</td><td>0.904</td><td>0.947</td><td>0.903</td><td>0.865</td><td>0.037</td><td>0.884</td><td>0.948</td><td>0.854</td><td>0.769</td><td>0.052</td><td>0.793</td><td>0.896</td></tr><tr><th>PiCANet [29]</th><th>ResNet50</th><td>0.917</td><td>0.867</td><td>0.046</td><td>0.890</td><td>0.952</td><td>0.904</td><td>0.840</td><td>0.043</td><td>0.866</td><td>0.950</td><td>0.869</td><td>0.755</td><td>0.051</td><td>0.791</td><td>0.920</td></tr><tr><th>PoolNet [27]</th><th>ResNet50</th><td>0.926</td><td>0.904</td><td>0.035</td><td>0.918</td><td>0.956</td><td>0.919</td><td>0.888</td><td>0.030</td><td>0.903</td><td>0.958</td><td>0.887</td><td>0.817</td><td>0.037</td><td>0.840</td><td>0.926</td></tr><tr><th>HRS [57]</th><th>ResNet50</th><td>0.883</td><td>0.859</td><td>0.054</td><td>0.894</td><td>0.934</td><td>0.882</td><td>0.851</td><td>0.042</td><td>0.883</td><td>0.941</td><td>0.829</td><td>0.746</td><td>0.051</td><td>0.791</td><td>0.899</td></tr><tr><th>GCPANet [3]</th><th>ResNet50</th><td>0.927</td><td>0.903</td><td>0.035</td><td>0.916</td><td>0.955</td><td>0.920</td><td>0.889</td><td>0.031</td><td>0.901</td><td>0.958</td><td>0.891</td><td>0.821</td><td>0.038</td><td>0.841</td><td>0.929</td></tr><tr><th>SAMNet [31]</th><th>Handcraft</th><td>0.907</td><td>0.858</td><td>0.050</td><td>0.883</td><td>0.945</td><td>0.898</td><td>0.837</td><td>0.045</td><td>0.864</td><td>0.946</td><td>0.849</td><td>0.729</td><td>0.058</td><td>0.768</td><td>0.901</td></tr><tr><th>VST [30]</th><th>T2T-ViTt-14</th><td>0.932</td><td>0.910</td><td>0.033</td><td>0.920</td><td>0.964</td><td>0.928</td><td>0.897</td><td>0.029</td><td>0.907</td><td>0.968</td><td>0.896</td><td>0.828</td><td>0.037</td><td>0.845</td><td>0.939</td></tr><tr><th>Auto-MSFNet [61]</th><th>ResNet50</th><td>0.914</td><td>0.916</td><td>0.033</td><td>0.927</td><td>0.954</td><td>0.908</td><td>0.903</td><td>0.027</td><td>0.912</td><td>0.959</td><td>0.877</td><td>0.841</td><td>0.034</td><td>0.855</td><td>0.931</td></tr><tr><th>SGL-KRN [53]</th><th>ResNet50</th><td>0.923</td><td>0.910</td><td>0.036</td><td>0.924</td><td>0.954</td><td>0.921</td><td>0.904</td><td>0.028</td><td>0.915</td><td>0.961</td><td>0.893</td><td>0.847</td><td>0.034</td><td>0.865</td><td>0.939</td></tr><tr><th>CTDNet [64]</th><th>ResNet50</th><td>0.925</td><td>0.915</td><td>0.032</td><td>0.927</td><td>0.956</td><td>0.921</td><td>0.909</td><td>0.027</td><td>0.918</td><td>0.961</td><td>0.893</td><td>0.847</td><td>0.034</td><td>0.862</td><td>0.935</td></tr><tr><th>MINet [36]</th><th>ResNet50</th><td>0.925</td><td>0.911</td><td>0.033</td><td>0.923</td><td>0.957</td><td>0.919</td><td>0.897</td><td>0.029</td><td>0.909</td><td>0.960</td><td>0.884</td><td>0.825</td><td>0.037</td><td>0.844</td><td>0.927</td></tr><tr><th>MFFN(ours)</th><th>ResNet50</th><td>0.929</td><td>0.917</td><td>0.032</td><td>0.927</td><td>0.959</td><td>0.921</td><td>0.903</td><td>0.028</td><td>0.913</td><td>0.959</td><td>0.888</td><td>0.833</td><td>0.038</td><td>0.850</td><td>0.924</td></tr></tbody></table>", "caption": "Table 6: Comparison of evaluation results of different Salient object detection(SOD) models on ECSSD [55], HKU-IS [21] and DUTS-TE [47]. The best results are highlighted in red, green and blue", "list_citation_info": ["[31] Yun Liu, Xin-Yu Zhang, Jia-Wang Bian, Le Zhang, and Ming-Ming Cheng. Samnet: Stereoscopically attentive multi-scale network for lightweight salient object detection. IEEE Transactions on Image Processing, 30:3804\u20133814, 2021.", "[30] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei Han. Visual saliency transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4722\u20134732, October 2021.", "[57] Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, and Huchuan Lu. Towards high-resolution salient object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.", "[55] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2013.", "[64] Zhirui Zhao, Changqun Xia, Chenxi Xie, and Jia Li. Complementary trilateral decoder for fast and accurate salient object detection. In Proceedings of the 29th acm international conference on multimedia, pages 4967\u20134975, 2021.", "[27] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[53] Binwei Xu, Haoran Liang, Ronghua Liang, and Peng Chen. Locate globally, segment locally: A progressive architecture with knowledge review network for salient object detection. Proceedings of the AAAI Conference on Artificial Intelligence, 35(4):3004\u20133012, May 2021.", "[47] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.", "[61] Miao Zhang, Tingwei Liu, Yongri Piao, Shunyu Yao, and Huchuan Lu. Auto-msfnet: Search multi-scale fusion network for salient object detection. In Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page 667\u2013676. Association for Computing Machinery, 2021.", "[36] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[29] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "[21] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "[49] Wenguan Wang, Shuyang Zhao, Jianbing Shen, Steven C. H. Hoi, and Ali Borji. Salient object detection with pyramid attention and salient edges. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[3] Zuyao Chen, Qianqian Xu, Runmin Cong, and Qingming Huang. Global context-aware progressive aggregation network for salient object detection. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):10599\u201310606, Apr. 2020."]}], "citation_info_to_title": {"[49] Wenguan Wang, Shuyang Zhao, Jianbing Shen, Steven C. H. Hoi, and Ali Borji. Salient object detection with pyramid attention and salient edges. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "Salient Object Detection with Pyramid Attention and Salient Edges", "[56] Fan Yang, Qiang Zhai, Xin Li, Rui Huang, Ao Luo, Hong Cheng, and Deng-Ping Fan. Uncertainty-guided transformer reasoning for camouflaged object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021.": "Uncertainty-guided transformer reasoning for camouflaged object detection", "[47] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.": "Learning to detect salient objects with image-level supervision", "[29] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.": "Picanet: Learning pixel-wise contextual attention for saliency detection", "[40] Xuebin Qin, Zichen Zhang, Chenyang Huang, Chao Gao, Masood Dehghan, and Martin Jagersand. Basnet: Boundary-aware salient object detection. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "Basnet: Boundary-aware salient object detection", "[34] Haiyang Mei, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, and Deng-Ping Fan. Camouflaged object segmentation with distraction mining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8772\u20138781, June 2021.": "Camouflaged object segmentation with distraction mining", "[57] Yi Zeng, Pingping Zhang, Jianming Zhang, Zhe Lin, and Huchuan Lu. Towards high-resolution salient object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.": "Towards high-resolution salient object detection", "[35] Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, and Huchuan Lu. Zoom in and out: A mixed-scale triplet network for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.": "Zoom in and out: A mixed-scale triplet network for camouflaged object detection", "[28] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.": "Picanet: Learning pixel-wise contextual attention for saliency detection", "[58] Qiang Zhai, Xin Li, Fan Yang, Chenglizhao Chen, Hong Cheng, and Deng-Ping Fan. Mutual graph learning for camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.": "Mutual Graph Learning for Camouflaged Object Detection", "[55] Qiong Yan, Li Xu, Jianping Shi, and Jiaya Jia. Hierarchical saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2013.": "Hierarchical saliency detection", "[65] Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for accurate and fast saliency detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.": "Interactive Two-Stream Decoder for Accurate and Fast Saliency Detection", "[27] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "A simple pooling-based design for real-time salient object detection", "[64] Zhirui Zhao, Changqun Xia, Chenxi Xie, and Jia Li. Complementary trilateral decoder for fast and accurate salient object detection. In Proceedings of the 29th acm international conference on multimedia, pages 4967\u20134975, 2021.": "Complementary Trilateral Decoder for Fast and Accurate Salient Object Detection", "[63] Jiaxing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "Egnet: Edge guidance network for salient object detection", "[60] Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu, and Yuchao Dai. Weakly-supervised salient object detection via scribble annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.": "Weakly-supervised salient object detection via scribble annotations", "[3] Zuyao Chen, Qianqian Xu, Runmin Cong, and Qingming Huang. Global context-aware progressive aggregation network for salient object detection. Proceedings of the AAAI Conference on Artificial Intelligence, 34(07):10599\u201310606, Apr. 2020.": "Global context-aware progressive aggregation network for salient object detection", "[59] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Saeed Anwar, Fatemeh Sadat Saleh, Tong Zhang, and Nick Barnes. Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "Uc-net: Uncertainty inspired rgb-d saliency detection via conditional variational autoencoders", "[31] Yun Liu, Xin-Yu Zhang, Jia-Wang Bian, Le Zhang, and Ming-Ming Cheng. Samnet: Stereoscopically attentive multi-scale network for lightweight salient object detection. IEEE Transactions on Image Processing, 30:3804\u20133814, 2021.": "Samnet: Stereoscopically attentive multi-scale network for lightweight salient object detection", "[51] Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "Cascaded partial decoder for fast and accurate salient object detection", "[53] Binwei Xu, Haoran Liang, Ronghua Liang, and Peng Chen. Locate globally, segment locally: A progressive architecture with knowledge review network for salient object detection. Proceedings of the AAAI Conference on Artificial Intelligence, 35(4):3004\u20133012, May 2021.": "Locate globally, segment locally: A progressive architecture with knowledge review network for salient object detection", "[5] Dengping Fan, Gepeng Ji, Tao Zhou, Geng Chen, Huazhong Fu, Shen Jianbing, and Ling Shao. Pranet: Parallel reverse attention network for polyp segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), October 2020.": "Pranet: Parallel reverse attention network for polyp segmentation", "[36] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.": "Multi-scale interactive network for salient object detection", "[32] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, and Deng-Ping Fan. Simultaneously localize, segment and rank the camouflaged objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.": "Simultaneously localize, segment and rank the camouflaged objects", "[45] Yujia Sun, Geng Chen, Tao Zhou, Yi Zhang, and Nian Liu. Context-aware cross-level fusion network for camouflaged object detection. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, August 2021.": "Context-aware cross-level fusion network for camouflaged object detection", "[20] Aixuan Li, Jing Zhang, Yunqiu Lv, Bowen Liu, Tong Zhang, and Yuchao Dai. Uncertainty-aware joint salient object and camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.": "Uncertainty-aware joint salient object and camouflaged object detection", "[6] Dengping Fan, Ge-Peng Ji, Guolei Sun, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Camouflaged object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.": "Camouflaged object detection", "[50] Jun Wei, Shuhui Wang, and Qingming Huang. F3net: Fusion, feedback and focus for salient object detection. AAAI Conference on Artificial Intelligence (AAAI), February 2020.": "F3net: Fusion, feedback and focus for salient object detection", "[61] Miao Zhang, Tingwei Liu, Yongri Piao, Shunyu Yao, and Huchuan Lu. Auto-msfnet: Search multi-scale fusion network for salient object detection. In Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page 667\u2013676. Association for Computing Machinery, 2021.": "Auto-msfnet: Search multi-scale fusion network for salient object detection", "[11] Shanghua Gao, Yongqiang Tan, Mingming Cheng, Chengze Lu, Yunpeng Chen, and Shuicheng Yan. Highly efficient salient object detection with 100k parameters. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Proceedings of the European Conference on Computer Vision (ECCV), August 2020.": "Highly efficient salient object detection with 100k parameters", "[30] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei Han. Visual saliency transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4722\u20134732, October 2021.": "Visual Saliency Transformer", "[52] Zhe Wu, Li Su, and Qingming Huang. Stacked cross refinement network for edge-aware salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "Stacked cross refinement network for edge-aware salient object detection", "[21] Guanbin Li and Yizhou Yu. Visual saliency based on multiscale deep features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.": "Visual Saliency Based on Multiscale Deep Features"}, "source_title_to_arxiv_id": {"Hierarchical saliency detection": "1408.5418", "Simultaneously localize, segment and rank the camouflaged objects": "2103.04011", "Uncertainty-aware joint salient object and camouflaged object detection": "2104.02628"}}