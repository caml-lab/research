{"title": "Class-Aware Universum Inspired Re-Balance Learning for Long-Tailed Recognition", "abstract": "Data augmentation for minority classes is an effective strategy for\nlong-tailed recognition, thus developing a large number of methods. Although\nthese methods all ensure the balance in sample quantity, the quality of the\naugmented samples is not always satisfactory for recognition, being prone to\nsuch problems as over-fitting, lack of diversity, semantic drift, etc. For\nthese issues, we propose the Class-aware Universum Inspired Re-balance\nLearning(CaUIRL) for long-tailed recognition, which endows the Universum with\nclass-aware ability to re-balance individual minority classes from both sample\nquantity and quality. In particular, we theoretically prove that the\nclassifiers learned by CaUIRL are consistent with those learned under the\nbalanced condition from a Bayesian perspective. In addition, we further develop\na higher-order mixup approach, which can automatically generate class-aware\nUniversum(CaU) data without resorting to any external data. Unlike the\ntraditional Universum, such generated Universum additionally takes the domain\nsimilarity, class separability, and sample diversity into account. Extensive\nexperiments on benchmark datasets demonstrate the surprising advantages of our\nmethod, especially the top1 accuracy in minority classes is improved by 1.9% 6%\ncompared to the state-of-the-art method.", "authors": ["Enhao Zhang", "Chuanxing Geng", "Songcan Chen"], "published_date": "2022_07_26", "pdf_url": "http://arxiv.org/pdf/2207.12808v3", "list_table_and_caption": [{"table": "<table><thead><tr><th colspan=\"2\">Methods</th><th>ERM</th><th>Over-sampling</th><th>LDAM-DRW</th><th>M2m</th><th>MiSLAS*</th><th>OPeN</th><th>CaUIRL</th></tr></thead><tbody><tr><th rowspan=\"2\">Cifar-10-LT</th><th><p>v=100</p></th><td>79.6(0.2)</td><td>75.1(0.4)</td><td>80.5(0.6)</td><td>81.3(0.4)</td><td>82.1</td><td>84.6(0.2)</td><td>86.46(0.2)</td></tr><tr><th><p>v=50</p></th><td>84.9(0.4)</td><td>82.2(0.4)</td><td>85.3(0.2)</td><td>85.5(0.3)</td><td>85.7</td><td>87.9(0.2)</td><td>89.04(0.1)</td></tr><tr><th rowspan=\"2\">Cifar-100-LT</th><th><p>v=100</p></th><td>47.0(0.5)</td><td>42.5(0.3)</td><td>46.8(0.2)</td><td>46.5(0.5)</td><td>47</td><td>51.5(0.4)</td><td>52.64(0.2)</td></tr><tr><th><p>v=50</p></th><td>52.4(0.4)</td><td>48.0(0.2)</td><td>52.6(0.2)</td><td>52.9(0.2)</td><td>52.3</td><td>56.3(0.4)</td><td>57.35(0.1)</td></tr><tr><th>CelebA-5</th><th><p>v=10.7</p></th><td>78.6(0.1)</td><td>76.4(0.2)</td><td>78.5(0.5)</td><td>76.9(0.4)</td><td>-</td><td>79.7(0.2)</td><td>81.48(0.3)</td></tr></tbody></table><ul><li>\u2022<p>The experimental results in the table are consistent with those in [10]. * indicates the results in the original paper, the others are results obtained with the same settings as our model. Missing results indicate datasets not evaluated in the cited papers.</p></li></ul>", "caption": "TABLE III: Results on Cifar-10-LT, Cifar-100-LT, CelebA-5.", "list_citation_info": ["[10] S. Zada, I. Benou, M. Irani, \u201dPure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images,\u201d arXiv preprint arXiv:2112.08810, 2021, (ICML2022 accepted)."]}], "citation_info_to_title": {"[10] S. Zada, I. Benou, M. Irani, \u201dPure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images,\u201d arXiv preprint arXiv:2112.08810, 2021, (ICML2022 accepted).": "Pure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images"}, "source_title_to_arxiv_id": {"Pure Noise to the Rescue of Insufficient Data: Improving Imbalanced Classification by Training on Random Noise Images": "2112.08810"}}