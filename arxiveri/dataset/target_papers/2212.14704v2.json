{"title": "Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models", "abstract": "Recent CLIP-guided 3D optimization methods, such as DreamFields and\nPureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D\nsynthesis. However, due to scratch training and random initialization without\nprior knowledge, these methods often fail to generate accurate and faithful 3D\nstructures that conform to the input text. In this paper, we make the first\nattempt to introduce explicit 3D shape priors into the CLIP-guided 3D\noptimization process. Specifically, we first generate a high-quality 3D shape\nfrom the input text in the text-to-shape stage as a 3D shape prior. We then use\nit as the initialization of a neural radiance field and optimize it with the\nfull prompt. To address the challenging text-to-shape generation task, we\npresent a simple yet effective approach that directly bridges the text and\nimage modalities with a powerful text-to-image diffusion model. To narrow the\nstyle domain gap between the images synthesized by the text-to-image diffusion\nmodel and shape renderings used to train the image-to-shape generator, we\nfurther propose to jointly optimize a learnable text prompt and fine-tune the\ntext-to-image diffusion model for rendering-style image generation. Our method,\nDream3D, is capable of generating imaginative 3D content with superior visual\nquality and shape accuracy compared to state-of-the-art methods.", "authors": ["Jiale Xu", "Xintao Wang", "Weihao Cheng", "Yan-Pei Cao", "Ying Shan", "Xiaohu Qie", "Shenghua Gao"], "published_date": "2022_12_28", "pdf_url": "http://arxiv.org/pdf/2212.14704v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"2\">CLIP R-Precision \\uparrow</td></tr><tr><td>ViT-B/32</td><td>ViT-B/16</td></tr><tr><th>DreamFields [25]</th><td>63.24</td><td>92.65</td></tr><tr><th>CLIP-Mesh [28]</th><td>75.00</td><td>91.18</td></tr><tr><th>PureCLIPNeRF [31]</th><td>73.53</td><td>88.24</td></tr><tr><th>Ours w/o 3D prior</th><td>75.47</td><td>94.34</td></tr><tr><th>Ours</th><td>85.29</td><td>98.53</td></tr></tbody></table>", "caption": "Table 1: Quantitative comparison on Text-guided 3D content synthesis. All the methods utilize CLIP ViT/16 as the guidance model during optimization, while we use two different CLIP models to calculate the CLIP retrieval precision.", "list_citation_info": ["[25] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 867\u2013876, June 2022.", "[28] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Popa Tiberiu. Clip-mesh: Generating textured meshes from text using pretrained image-text models. December 2022.", "[31] Han-Hung Lee and Angel X Chang. Understanding pure clip guidance for voxel grid nerf models. arXiv preprint arXiv:2209.15172, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>FID \\downarrow</th></tr></thead><tbody><tr><th>CLIP-Forge [59]</th><td>112.38</td></tr><tr><th>Ours w/o text-to-image</th><td>58.36</td></tr><tr><th>Ours w/o fine-tuning SD</th><td>61.88</td></tr><tr><th>Ours</th><td>40.83</td></tr></tbody></table>", "caption": "Table 2: Quantitative comparison on text-to-shape generation.", "list_citation_info": ["[59] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18603\u201318613, June 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>FID \\downarrow</th><th>FPD \\downarrow</th><th>MMD \\uparrow</th></tr></thead><tbody><tr><th>CLIP-Forge [59]</th><td>112.38</td><td>6.896</td><td>0.670</td></tr><tr><th>Ours</th><td>40.83</td><td>1.301</td><td>0.725</td></tr></tbody></table>", "caption": "Table 5: Additional quantitative results compared with CLIP-Forge on text-guided shape generation.", "list_citation_info": ["[59] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18603\u201318613, June 2022."]}], "citation_info_to_title": {"[59] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18603\u201318613, June 2022.": "Clip-forge: Towards zero-shot text-to-shape generation", "[25] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 867\u2013876, June 2022.": "Zero-shot text-guided object generation with dream fields", "[28] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Popa Tiberiu. Clip-mesh: Generating textured meshes from text using pretrained image-text models. December 2022.": "Clip-mesh: Generating Textured Meshes from Text Using Pretrained Image-Text Models", "[31] Han-Hung Lee and Angel X Chang. Understanding pure clip guidance for voxel grid nerf models. arXiv preprint arXiv:2209.15172, 2022.": "Understanding Pure Clip Guidance for Voxel Grid NeRF Models"}, "source_title_to_arxiv_id": {"Clip-mesh: Generating Textured Meshes from Text Using Pretrained Image-Text Models": "2203.13333"}}