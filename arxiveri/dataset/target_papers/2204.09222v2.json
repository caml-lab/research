{"title": "K-LITE: Learning Transferable Visual Models with External Knowledge", "abstract": "The new generation of state-of-the-art computer vision systems are trained\nfrom natural language supervision, ranging from simple object category names to\ndescriptive captions. This form of supervision ensures high generality and\nusability of the learned visual models, due to the broad concept coverage\nachieved via large-scale data collection process. Alternatively, we argue that\nlearning with external knowledge is a promising way which leverages a much more\nstructured source of supervision and offers sample efficiency. We propose\nK-LITE, a simple strategy to leverage external knowledge for building\ntransferable visual systems: In training, it enriches entities in text with\nWordNet and Wiktionary knowledge, leading to an efficient and scalable approach\nto learning image representations that uses knowledge about the visual\nconcepts. In evaluation, the text is also augmented with external knowledge and\nthen used to reference learned visual concepts (or describe new ones) to enable\nzero-shot and few-shot transfer of the pre-trained models. We study the\nperformance of K-LITE on two important computer vision problems, image\nclassification and object detection, benchmarking on 20 and 13 different\nexisting datasets, respectively. The proposed knowledge-augmented models show\nsignificant improvement in transfer learning performance over existing methods.\nOur code is available at https://github.com/microsoft/klite.", "authors": ["Sheng Shen", "Chunyuan Li", "Xiaowei Hu", "Jianwei Yang", "Yujia Xie", "Pengchuan Zhang", "Zhe Gan", "Lijuan Wang", "Lu Yuan", "Ce Liu", "Kurt Keutzer", "Trevor Darrell", "Anna Rohrbach", "Jianfeng Gao"], "published_date": "2022_04_20", "pdf_url": "http://arxiv.org/pdf/2204.09222v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th colspan=\"5\">Pre-training</th><td colspan=\"2\">Downstream</td></tr><tr><th>Task</th><th></th><td>#Instances</td><td>#Concepts</td><td>Vocab. Size</td><td>#Ins/#C.</td><td colspan=\"2\">Concept Overlap (%)</td></tr><tr><th rowspan=\"5\">IC</th><th>Dataset</th><td></td><td></td><td></td><td></td><td>ImageNet-1K</td><td>20-datasets</td></tr><tr><th>ImageNet-21K deng2009imagenet </th><td>13M</td><td>19.2K / 18.4K</td><td>13.5K / 12.9K</td><td>591 \\pm 537</td><td>11.82</td><td>13.26</td></tr><tr><th>GCC-3M sharma2018conceptual </th><td>3.3M</td><td>681K / 64.5K</td><td>29.6K / 13.0K</td><td>9.5 \\pm303</td><td>35.97</td><td>19.73</td></tr><tr><th>GCC-12M changpinyo2021conceptual </th><td>12M</td><td>10.2M / 728K</td><td>1.24M / 264K</td><td>5.6 \\pm 353</td><td>61.02</td><td>31.34</td></tr><tr><th>YFCC-14M thomee2016yfcc100m </th><td>14M</td><td>14.2M / 1.25M</td><td>2.41M / 473K</td><td>8.3 \\pm 1354</td><td>65.23</td><td>34.65</td></tr><tr><th rowspan=\"2\">OD</th><th>Dataset</th><td></td><td></td><td></td><td></td><td>LVIS</td><td>13-datasets</td></tr><tr><th>Object-365 shao2019objects365 </th><td>9.6M</td><td>365 / 365</td><td>452 / 452</td><td>26.3K \\pm 12.4K</td><td>13.46</td><td>21.26</td></tr></tbody></table>", "caption": "Table 1: Statistics of training and test datasets used in our experiments. #Instances indicates #Image for IC and #Regions for OD, respectively. For #Concept and Vocabulary size, we report numbers for the full set and for items with frequency larger than 5. #Ins/C. reports the mean and standard derivation for the numbers of instances per concept.", "list_citation_info": ["(76) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.", "(15) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.", "(75) Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In ICCV, 2019.", "(11) Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.", "(84) Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 2016."]}, {"table": "<table><tbody><tr><td colspan=\"2\">Training Data</td><td rowspan=\"2\">Method</td><td>ImageNet-1K</td><td colspan=\"3\">ICinW (20 datasets)</td></tr><tr><td><p>Dataset</p></td><td># Samples</td><td>Zero-shot</td><td>Zero-shot</td><td>Linear Probing</td><td>Fine-tuning</td></tr><tr><td rowspan=\"2\">ImageNet-21K</td><td>13M (full)</td><td>UniCL</td><td>28.16</td><td>27.15</td><td>53.07 \\pm 4.15</td><td>55.96 \\pm 2.50</td></tr><tr><td>13M (full)</td><td>K-Lite</td><td>30.23</td><td>33.44</td><td>53.92 \\pm 1.05</td><td>57.81 \\pm 1.48</td></tr><tr><td rowspan=\"5\">YFCC-14M + ImageNet-21K</td><td>14M (half)</td><td>UniCL</td><td>34.43</td><td>34.30</td><td>53.50 \\pm 2.22</td><td>56.45 \\pm 2.48</td></tr><tr><td>14M (half)</td><td>K-Lite</td><td>36.67</td><td>36.50</td><td>49.48 \\pm 2.23</td><td>55.88 \\pm 1.64</td></tr><tr><td>14M (half)</td><td>K-Lite<sup>\\diamondsuit</sup></td><td>42.36</td><td>36.50</td><td>54.28 \\pm 3.66</td><td>52.11 \\pm 4.90</td></tr><tr><td>27M (full)</td><td>UniCL</td><td>43.06</td><td>35.99</td><td>55.96 \\pm 3.38</td><td>58.25 \\pm 2.98</td></tr><tr><td>27M (full)</td><td>K-Lite</td><td>45.67</td><td>38.89</td><td>57.06 \\pm 1.48</td><td>58.24 \\pm 2.36</td></tr><tr><td rowspan=\"5\">GCC-15M + ImageNet-21K</td><td>15M (half)</td><td>UniCL</td><td>41.64</td><td>36.31</td><td>53.86 \\pm 2.73</td><td>59.04 \\pm 3.13</td></tr><tr><td>15M (half)</td><td>K-Lite</td><td>44.26</td><td>39.53</td><td>55.91 \\pm 2.53</td><td>58.20 \\pm 3.39</td></tr><tr><td>15M (half)</td><td>K-Lite<sup>\\diamondsuit</sup></td><td>47.30</td><td>40.32</td><td>57.38 \\pm 2.70</td><td>60.72 \\pm 2.29</td></tr><tr><td>28M (full)</td><td>UniCL</td><td>46.83</td><td>38.90</td><td>57.92 \\pm 3.31</td><td>60.99 \\pm 2.74</td></tr><tr><td>28M (full)</td><td>K-Lite</td><td>48.76</td><td>41.34</td><td>58.56 \\pm 3.12</td><td>63.39 \\pm 1.74</td></tr></tbody></table>", "caption": "Table 3: Overall comparisons of our knowledge-augmented models. Each model is pre-trained with 32 epochs following CLIP radford2021learning /UniCL yang2022unicl . <sup>\\diamondsuit</sup> It indicates that the Combine scheme is used for the image-caption data, otherwise the default is the Concat scheme decribed in Section 3.2. The linear probing and fine-tuning results are reported for 5-shot settings over 3 random seeds.", "list_citation_info": ["(71) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.", "(95) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Lu Yuan, Ce Liu, and Jianfeng Gao. Unified contrastive learning in image-text-label space. CVPR, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th colspan=\"8\">LVIS</th><th colspan=\"4\">ODinW (13 datasets)</th></tr><tr><th></th><th>APr</th><th>APc</th><th>APf</th><th>-</th><th>\\mathcal{S}_{\\text{LVIS}}</th><th>\\mathcal{S}_{\\text{wn\\_path}}</th><th>\\mathcal{S}_{\\text{wn\\_def}}</th><th>\\mathcal{S}_{\\text{wiki\\_def}}</th><th>-</th><th>\\mathcal{S}_{\\text{wn\\_path}}</th><th>\\mathcal{S}_{\\text{wn\\_def}}</th><th>\\mathcal{S}_{\\text{wiki\\_def}}</th></tr></thead><tbody><tr><th>GLIP-A (li2021grounded, )</th><td>14.2</td><td>13.9</td><td>23.4</td><td>18.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>28.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Baseline GLIP<sup>\\heartsuit</sup></th><td>8.6</td><td>14.0</td><td>23.1</td><td>17.9</td><td>17.6</td><td>17.1</td><td>17.2</td><td>15.0</td><td>27.5</td><td>26.8</td><td>21.0</td><td>18.5</td></tr><tr><th>K-Lite</th><td>14.8</td><td>18.6</td><td>24.8</td><td>16.9</td><td>21.3</td><td>18.7</td><td>21.4</td><td>20.5</td><td>25.0</td><td>30.3</td><td>28.4</td><td>31.7</td></tr></tbody></table>", "caption": "Table 4: Zero-shot task transfer performance on OD. APr/APc/APf indicates the AP values for rare, common, frequent groups of categories on LVIS. Cell coloring follows the same protocol as in Table 2. <sup>\\heartsuit</sup>GLIP is implemented with parallel text encoding in Section 3.3 without external knowledge.", "list_citation_info": ["(50) Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. CVPR, 2022."]}, {"table": "<table><thead><tr><th>Dataset</th><th>#Concepts</th><th>Train size</th><th>Test size</th><th>Evaluation metric</th><th>Source link</th></tr></thead><tbody><tr><td>Hateful Memes kiela2020hateful </td><td>2</td><td>8,500</td><td>500</td><td>ROC AUC</td><td>Facebook</td></tr><tr><td>PatchCamelyon veeling2018rotation </td><td>2</td><td>262,144</td><td>32,768</td><td>Accuracy</td><td>Tensorflow</td></tr><tr><td>Rendered-SST2 radford2021learning </td><td>2</td><td>6,920</td><td>1,821</td><td>Accuracy</td><td>OpenAI</td></tr><tr><td>KITTI Distance fritsch2013new </td><td>4</td><td>6,347</td><td>711</td><td>Accuracy</td><td>KITTI website</td></tr><tr><td>FER 2013 fer2013 </td><td>7</td><td>28,709</td><td>3,589</td><td>Accuracy</td><td>Kaggle fer2013</td></tr><tr><td>CIFAR-10 krizhevsky2009learning </td><td>10</td><td>50,000</td><td>10,000</td><td>Accuracy</td><td>Tensorflow</td></tr><tr><td>EuroSAT helber2019eurosat </td><td>10</td><td>5,000</td><td>5,000</td><td>Accuracy</td><td>Tensorflow</td></tr><tr><td>MNIST deng2012mnist </td><td>10</td><td>60,000</td><td>10,000</td><td>Accuracy</td><td>Tensorflow</td></tr><tr><td>VOC 2007 Classification everingham2010pascal </td><td>20</td><td>2,501</td><td>4,952</td><td>11-point mAP</td><td>VOC 2007</td></tr><tr><td>Oxford-IIIT Pets parkhi2012cats </td><td>37</td><td>3,680</td><td>3,669</td><td>Mean-per-class</td><td>Oxford-IIIT Pets</td></tr><tr><td>GTSRB stallkamp2011german </td><td>43</td><td>26,640</td><td>12,630</td><td>Accuracy</td><td>GTSRB website</td></tr><tr><td>Resisc-45 cheng2017remote </td><td>45</td><td>3,150</td><td>25,200</td><td>Accuracy</td><td>Tensorflow</td></tr><tr><td>Describable Textures cimpoi2014describing </td><td>47</td><td>1,880</td><td>1,880</td><td>Accuracy</td><td>DTD website</td></tr><tr><td>CIFAR-100 krizhevsky2009learning </td><td>100</td><td>50,000</td><td>10,000</td><td>Accuracy</td><td>Tensorflow</td></tr><tr><td>FGVC Aircraft (variants) maji2013fine </td><td>100</td><td>3,334</td><td>3,333</td><td>Mean-per-class</td><td>FGVC website</td></tr><tr><td>Food-101 bossard2014food </td><td>101</td><td>75,750</td><td>25,250</td><td>Accuracy</td><td>Tensorflow</td></tr><tr><td>Caltech-101 fei2004learning </td><td>102</td><td>3,060</td><td>6,084</td><td>Mean-per-class</td><td>Tensorflow</td></tr><tr><td>Oxford Flowers 102 nilsback2008automated </td><td>102</td><td>1,020</td><td>6,149</td><td>Mean-per-class</td><td>Tensorflow</td></tr><tr><td>Stanford Cars krause20133d </td><td>196</td><td>8,144</td><td>8,041</td><td>Accuracy</td><td>Stanford Cars</td></tr><tr><td>Country-211 radford2021learning </td><td>211</td><td>31,650</td><td>21,100</td><td>Accuracy</td><td>OpenAI</td></tr></tbody></table>", "caption": "Table 5: Statistics of 20 datasets used in image classification.", "list_citation_info": ["(16) Li Deng. The MNIST database of handwritten digit images for machine learning research. IEEE signal processing magazine, 2012.", "(71) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.", "(13) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017.", "(22) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. In CVPR workshop, 2004.", "(8) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In ECCV, 2014.", "(24) Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger. A new performance measure and evaluation benchmark for road detection algorithms. In ITSC. IEEE, 2013.", "(80) Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In IJCNN, 2011.", "(37) Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. NeurIPS, 2020.", "(41) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.", "(31) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.", "(14) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.", "(66) Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012.", "(20) Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 2010.", "(59) Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.", "(40) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV workshops, 2013.", "(87) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In MICCAI, 2018.", "(65) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing. IEEE, 2008.", "(1) FER 2013: Kaggle challenges in representation learning facial expression recognition. https://www.kaggle.com/."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Dataset</th><th rowspan=\"2\">#Concepts</th><th colspan=\"2\">#Image</th><th colspan=\"2\">#Annotated Regions</th><th rowspan=\"2\">Source link</th></tr><tr><th>Train</th><th>Test</th><th>Train</th><th>Test</th></tr></thead><tbody><tr><td>CottontailRabbits</td><td>1</td><td>1980</td><td>10</td><td>2070</td><td>11</td><td>Roboflow</td></tr><tr><td>EgoHands(generic) egohands2015iccv </td><td>1</td><td>3840</td><td>480</td><td>12015</td><td>1514</td><td>Roboflow</td></tr><tr><td>Packages</td><td>1</td><td>19</td><td>3</td><td>31</td><td>5</td><td>Roboflow</td></tr><tr><td>Raccoon</td><td>1</td><td>150</td><td>17</td><td>164</td><td>20</td><td>Roboflow</td></tr><tr><td>Pistols</td><td>1</td><td>2377</td><td>297</td><td>2728</td><td>358</td><td>Roboflow</td></tr><tr><td>Pothole</td><td>1</td><td>465</td><td>67</td><td>1256</td><td>154</td><td>Roboflow</td></tr><tr><td>NorthAmericaMushrooms</td><td>2</td><td>41</td><td>5</td><td>67</td><td>9</td><td>Roboflow</td></tr><tr><td>ThermalDogsAndPeople</td><td>2</td><td>142</td><td>20</td><td>181</td><td>27</td><td>Roboflow</td></tr><tr><td>ShellfishOpenImages</td><td>3</td><td>407</td><td>58</td><td>859</td><td>116</td><td>Roboflow</td></tr><tr><td>AerialMaritimeDrone(large)</td><td>5</td><td>52</td><td>7</td><td>873</td><td>78</td><td>Roboflow</td></tr><tr><td>VehiclesOpenImages</td><td>5</td><td>878</td><td>126</td><td>1676</td><td>258</td><td>Roboflow</td></tr><tr><td>Aquarium</td><td>7</td><td>448</td><td>63</td><td>3324</td><td>584</td><td>Roboflow</td></tr><tr><td>PascalVOC everingham2010pascal </td><td>20</td><td>13690</td><td>3422</td><td>31356</td><td>7835</td><td>Roboflow</td></tr></tbody></table>", "caption": "Table 6: Statistics of 13 datasets used in object detection. Box mAP is used as the evaluation metric. Datasets are downloaded from Roboflow. For the datasets without a citation, we refer to Roboflow links for the original sources.", "list_citation_info": ["(5) Sven Bambach, Stefan Lee, David Crandall, and Chen Yu. Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions. In ICCV, 2015.", "(20) Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 2010."]}, {"table": "<table><tbody><tr><td colspan=\"2\">Training Data</td><td rowspan=\"2\">Method</td><td colspan=\"4\">COCO Retrieval</td></tr><tr><td><p>Dataset</p></td><td># Samples</td><td>I2T R@1</td><td>I2T R@5</td><td>T2I R@1</td><td>T2I R@5</td></tr><tr><td rowspan=\"2\">ImageNet-21K</td><td>13M (full)</td><td>UniCL</td><td>2.66</td><td>7.46</td><td>0.98</td><td>3.54</td></tr><tr><td>13M (full)</td><td>K-Lite</td><td>4.04</td><td>12.20</td><td>1.91</td><td>6.48</td></tr><tr><td rowspan=\"2\">YFCC-14M + ImageNet-21K</td><td>14M (half)</td><td>UniCL</td><td>21.80</td><td>45.38</td><td>13.33</td><td>32.14</td></tr><tr><td>14M (half)</td><td>K-Lite<sup></sup></td><td>22.44</td><td>47.28</td><td>14.38</td><td>33.77</td></tr><tr><td>GCC-15M + ImageNet-21K</td><td>15M (half)</td><td>UniCL</td><td>31.88</td><td>57.76</td><td>21.41</td><td>44.69</td></tr><tr><td></td><td>15M (half)</td><td>K-Lite<sup></sup></td><td>32.68</td><td>58.88</td><td>22.08</td><td>45.41</td></tr></tbody></table>", "caption": "Table 7:  Overall comparisons of our knowledge-augmented models on zero-shot COCO Retrieval Evaluation. Each model is pre-trained with 32 epochs following UniCL yang2022unicl .", "list_citation_info": ["(95) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Lu Yuan, Ce Liu, and Jianfeng Gao. Unified contrastive learning in image-text-label space. CVPR, 2022."]}, {"table": "<table><thead><tr><th>Dataset</th><th>#Concepts</th><th>Improvement</th><th>Rareness</th><th>Overlap-Difference</th><th>Coverage</th></tr></thead><tbody><tr><th>Hateful Memes kiela2020hateful </th><th>2</th><td>6.64</td><td>0.33</td><td>0.00</td><td>1.00</td></tr><tr><th>PatchCamelyon veeling2018rotation </th><th>2</th><td>14.00</td><td>0.33</td><td>0.00</td><td>1.00</td></tr><tr><th>Rendered-SST2 radford2021learning </th><th>2</th><td>0.16</td><td>0.00</td><td>0.00</td><td>1.00</td></tr><tr><th>KITTI Distance fritsch2013new </th><th>4</th><td>13.30</td><td>0.01</td><td>0.01</td><td>1.00</td></tr><tr><th>FER 2013 fer2013 </th><th>7</th><td>0.19</td><td>0.00</td><td>0.00</td><td>1.00</td></tr><tr><th>CIFAR-10 krizhevsky2009learning </th><th>10</th><td>0.15</td><td>0.00</td><td>0.00</td><td>1.00</td></tr><tr><th>EuroSAT helber2019eurosat </th><th>10</th><td>-2.50</td><td>0.00</td><td>0.03</td><td>1.00</td></tr><tr><th>MNIST deng2012mnist </th><th>10</th><td>0.09</td><td>0.00</td><td>0.01</td><td>0.60</td></tr><tr><th>VOC 2007 Classification everingham2010pascal </th><th>20</th><td>-1.60</td><td>0.02</td><td>0.00</td><td>0.85</td></tr><tr><th>Oxford-IIIT Pets parkhi2012cats </th><th>37</th><td>17.80</td><td>0.06</td><td>0.09</td><td>0.97</td></tr><tr><th>GTSRB stallkamp2011german </th><th>43</th><td>3.95</td><td>0.04</td><td>0.02</td><td>1.00</td></tr><tr><th>Resisc-45 cheng2017remote </th><th>45</th><td>3.29</td><td>0.03</td><td>0.06</td><td>0.98</td></tr><tr><th>Describable Textures cimpoi2014describing </th><th>47</th><td>4.57</td><td>0.02</td><td>0.11</td><td>1.00</td></tr><tr><th>CIFAR-100 krizhevsky2009learning </th><th>100</th><td>5.50</td><td>0.01</td><td>0.03</td><td>1.00</td></tr><tr><th>FGVC Aircraft (variants) maji2013fine </th><th>100</th><td>-0.10</td><td>0.00</td><td>0.01</td><td>0.10</td></tr><tr><th>Food-101 bossard2014food </th><th>101</th><td>26.40</td><td>0.01</td><td>0.21</td><td>1.00</td></tr><tr><th>Caltech-101 fei2004learning </th><th>102</th><td>1.81</td><td>0.01</td><td>0.11</td><td>1.00</td></tr><tr><th>Oxford Flowers 102 nilsback2008automated </th><th>102</th><td>30.20</td><td>0.02</td><td>0.26</td><td>0.97</td></tr><tr><th>Stanford Cars krause20133d </th><th>196</th><td>0.43</td><td>0.34</td><td>0.00</td><td>0.00</td></tr><tr><th>Country-211 radford2021learning </th><th>211</th><td>1.09</td><td>0.02</td><td>0.06</td><td>1.00</td></tr></tbody></table>", "caption": "Table 8:  Correlation of the improvement of K-Lite over UniCL on 20 datasets with normalized rareness, overlap difference and coverage.", "list_citation_info": ["(16) Li Deng. The MNIST database of handwritten digit images for machine learning research. IEEE signal processing magazine, 2012.", "(71) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.", "(13) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017.", "(22) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. In CVPR workshop, 2004.", "(8) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In ECCV, 2014.", "(24) Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger. A new performance measure and evaluation benchmark for road detection algorithms. In ITSC. IEEE, 2013.", "(80) Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In IJCNN, 2011.", "(37) Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. NeurIPS, 2020.", "(41) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.", "(31) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.", "(14) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.", "(66) Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012.", "(20) Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 2010.", "(59) Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.", "(40) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV workshops, 2013.", "(87) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In MICCAI, 2018.", "(65) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing. IEEE, 2008.", "(1) FER 2013: Kaggle challenges in representation learning facial expression recognition. https://www.kaggle.com/."]}, {"table": "<table><thead><tr><th>Method</th><th># Parameters</th><th># Training Data</th><th colspan=\"4\">LVIS</th></tr></thead><tbody><tr><td></td><td></td><td></td><td>APr</td><td>APc</td><td>APf</td><td>AP</td></tr><tr><th>GLIP-A (li2021grounded, )</th><th>151M</th><th rowspan=\"2\">Object-365</th><th>14.2</th><th>13.9</th><th>23.4</th><th>18.5</th></tr><tr><td>K-Lite</td><td>151M</td><td>14.8</td><td>18.6</td><td>24.8</td><td>21.3</td></tr><tr><th>GLIP-C (li2021grounded, )</th><th>231M</th><th rowspan=\"2\">Object-365 + Gold Grounding Data</th><th>17.7</th><th>19.5</th><th>31.0</th><th>24.9</th></tr><tr><td>K-Lite</td><td>151M</td><td>17.2</td><td>24.6</td><td>29.0</td><td>26.1</td></tr></tbody></table>", "caption": "Table 9: Zero-shot task transfer performance on LVIS dataset. In li2021grounded , GLIP-A is a two-encoder model without fusion module trained on Object365, and GLIP-C is a two-encoder model with fusion module trained on Object365 + Gold Grounding Data.", "list_citation_info": ["(50) Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. CVPR, 2022."]}], "citation_info_to_title": {"(24) Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger. A new performance measure and evaluation benchmark for road detection algorithms. In ITSC. IEEE, 2013.": "A new performance measure and evaluation benchmark for road detection algorithms", "(22) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. In CVPR workshop, 2004.": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories", "(50) Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. CVPR, 2022.": "Grounded language-image pre-training", "(15) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.": "Imagenet: A large-scale hierarchical image database", "(40) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV workshops, 2013.": "3D Object Representations for Fine-Grained Categorization", "(84) Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 2016.": "Yfcc100m: The new data in multimedia research", "(65) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics & Image Processing. IEEE, 2008.": "Automated flower classification over a large number of classes", "(20) Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 2010.": "The Pascal Visual Object Classes (VOC) Challenge", "(87) Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In MICCAI, 2018.": "Rotation Equivariant CNNs for Digital Pathology", "(11) Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "(76) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "(80) Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In IJCNN, 2011.": "The German Traffic Sign Recognition Benchmark: A Multi-Class Classification Competition", "(8) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components with random forests. In ECCV, 2014.": "Food-101\u2013mining discriminative components with random forests", "(59) Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.": "Fine-grained visual classification of aircraft", "(13) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017.": "Remote Sensing Image Scene Classification: Benchmark and State of the Art", "(1) FER 2013: Kaggle challenges in representation learning facial expression recognition. https://www.kaggle.com/.": "FER 2013: Kaggle Challenges in Representation Learning Facial Expression Recognition", "(66) Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, 2012.": "Cats and dogs", "(75) Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In ICCV, 2019.": "Objects365: A large-scale, high-quality dataset for object detection", "(5) Sven Bambach, Stefan Lee, David Crandall, and Chen Yu. Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions. In ICCV, 2015.": "Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions", "(95) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Lu Yuan, Ce Liu, and Jianfeng Gao. Unified contrastive learning in image-text-label space. CVPR, 2022.": "Unified Contrastive Learning in Image-Text-Label Space", "(37) Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. NeurIPS, 2020.": "The title is The hateful memes challenge: Detecting hate speech in multimodal memes", "(41) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.": "Learning Multiple Layers of Features from Tiny Images", "(31) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.": "EuroSat: A novel dataset and deep learning benchmark for land use and land cover classification", "(14) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014.": "Describing textures in the wild", "(16) Li Deng. The MNIST database of handwritten digit images for machine learning research. IEEE signal processing magazine, 2012.": "The MNIST database of handwritten digit images for machine learning research", "(71) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.": "Learning transferable visual models from natural language supervision"}, "source_title_to_arxiv_id": {"Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts": "2102.08981"}}