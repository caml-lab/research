{"title": "Uncertainty-based Network for Few-shot Image Classification", "abstract": "The transductive inference is an effective technique in the few-shot learning\ntask, where query sets update prototypes to improve themselves. However, these\nmethods optimize the model by considering only the classification scores of the\nquery instances as confidence while ignoring the uncertainty of these\nclassification scores. In this paper, we propose a novel method called\nUncertainty-Based Network, which models the uncertainty of classification\nresults with the help of mutual information. Specifically, we first data\naugment and classify the query instance and calculate the mutual information of\nthese classification scores. Then, mutual information is used as uncertainty to\nassign weights to classification scores, and the iterative update strategy\nbased on classification scores and uncertainties assigns the optimal weights to\nquery instances in prototype optimization. Extensive results on four benchmarks\nshow that Uncertainty-Based Network achieves comparable performance in\nclassification accuracy compared to state-of-the-art method.", "authors": ["Minglei Yuan", "Qian Xu", "Chunhao Cai", "Yin-Dong Zheng", "Tao Wang", "Tong Lu"], "published_date": "2022_05_17", "pdf_url": "http://arxiv.org/pdf/2205.08157v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FE</td><td rowspan=\"2\">Tr</td><td colspan=\"2\">miniImageNet(%)</td><td colspan=\"2\">tieredImageNet(%)</td></tr><tr><td><p>1-shot</p></td><td><p>5-shot</p></td><td><p>1-shot</p></td><td><p>5-shot</p></td></tr><tr><td><p>ProtoNet [1]</p></td><td><p>C64</p></td><td><p>No</p></td><td><p>49.42\\pm0.78</p></td><td><p>68.20\\pm0.66</p></td><td><p>53.31\\pm0.89</p></td><td><p>72.69\\pm0.74</p></td></tr><tr><td><p>MatchNet [2]</p></td><td><p>C64</p></td><td><p>No</p></td><td><p>43.56\\pm0.84</p></td><td><p>55.31\\pm0.73</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>MAML [21]</p></td><td><p>C64</p></td><td><p>BN</p></td><td><p>48.70\\pm1.84</p></td><td><p>63.15\\pm0.91</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>RN [3]</p></td><td><p>C64</p></td><td><p>BN</p></td><td><p>50.44\\pm0.82</p></td><td><p>65.32\\pm0.70</p></td><td><p>54.48\\pm0.93</p></td><td><p>71.32\\pm0.78</p></td></tr><tr><td><p>TPN [4]</p></td><td><p>C64</p></td><td><p>Yes</p></td><td><p>53.75</p></td><td><p>69.43</p></td><td><p>57.53</p></td><td><p>72.85</p></td></tr><tr><td><p>Ent [5]</p></td><td><p>C64</p></td><td><p>Yes</p></td><td><p>50.46\\pm0.62</p></td><td><p>66.68\\pm0.52</p></td><td><p>58.05\\pm0.68</p></td><td><p>74.24\\pm0.56</p></td></tr><tr><td><p>UCN</p></td><td><p>C64</p></td><td><p>Yes</p></td><td>57.97\\pm0.66</td><td>73.01\\pm0.75</td><td>58.95\\pm1.13</td><td>75.82\\pm0.79</td></tr><tr><td><p>EGNN [6]</p></td><td><p>C256</p></td><td><p>Yes</p></td><td><p>-</p></td><td><p>76.37</p></td><td><p>-</p></td><td><p>70.15</p></td></tr><tr><td><p>UCN</p></td><td><p>C256</p></td><td><p>Yes</p></td><td>59.19\\pm1.14</td><td>77.70\\pm0.64</td><td>68.90\\pm1.17</td><td>82.78\\pm0.67</td></tr><tr><td><p>TADAM [15]</p></td><td><p>R12</p></td><td><p>No</p></td><td><p>58.5  \\pm 0.3</p></td><td><p>76.7  \\pm 0.3</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>CAN [8]</p></td><td><p>R12</p></td><td><p>No</p></td><td><p>63.85\\pm0.48</p></td><td><p>79.44\\pm0.34</p></td><td><p>69.89\\pm0.51</p></td><td><p>84.23\\pm0.37</p></td></tr><tr><td><p>DeepEMD [22]</p></td><td><p>R12</p></td><td><p>No</p></td><td><p>65.91\\pm0.82</p></td><td><p>82.41\\pm0.56</p></td><td><p>71.16\\pm0.87</p></td><td><p>86.03\\pm0.58</p></td></tr><tr><td><p>FEAT [23]</p></td><td><p>R12</p></td><td><p>No</p></td><td><p>66.78\\pm0.20</p></td><td><p>82.05\\pm0.14</p></td><td><p>70.80\\pm0.23</p></td><td><p>84.79\\pm0.16</p></td></tr><tr><td><p>CAN [8]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>67.19\\pm0.55</p></td><td><p>80.64\\pm0.35</p></td><td><p>73.21\\pm0.58</p></td><td><p>84.93\\pm0.38</p></td></tr><tr><td><p>DPGN [7]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>67.77\\pm0.32</p></td><td><p>84.60\\pm0.43</p></td><td><p>72.45\\pm0.51</p></td><td><p>87.24\\pm0.39</p></td></tr><tr><td><p>EPNet [24]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>66.50\\pm0.89</p></td><td><p>81.06\\pm0.60</p></td><td><p>76.53\\pm0.87</p></td><td><p>87.32\\pm0.64</p></td></tr><tr><td><p>Ent [5]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>62.35\\pm0.66</p></td><td><p>74.53\\pm0.54</p></td><td><p>68.41\\pm0.73</p></td><td><p>83.41\\pm0.52</p></td></tr><tr><td><p>IEPT[25]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>67.05\\pm0.44</p></td><td><p>82.90\\pm0.30</p></td><td><p>72.24\\pm0.50</p></td><td><p>86.73\\pm0.34</p></td></tr><tr><td><p>UAFS[12]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>64.22\\pm0.67</p></td><td><p>79.99\\pm0.49</p></td><td><p>69.13\\pm0.84</p></td><td><p>84.33\\pm0.59</p></td></tr><tr><td><p>UCN</p></td><td><p>R12</p></td><td><p>Yes</p></td><td>76.78\\pm1.01</td><td>85.35\\pm0.61</td><td>80.00\\pm1.12</td><td>87.50\\pm0.64</td></tr><tr><td><p>MCT{}^{\\dagger} [9]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>78.55\\pm0.86</p></td><td><p>86.03\\pm0.42</p></td><td>82.32\\pm0.81</td><td><p>87.36\\pm0.50</p></td></tr><tr><td><p>UCN{}^{\\dagger}</p></td><td><p>R12</p></td><td><p>Yes</p></td><td>86.40\\pm0.46</td><td>87.22\\pm0.43</td><td><p>82.02\\pm1.01</p></td><td>90.83\\pm0.39</td></tr><tr><td><p>CTM [26]</p></td><td><p>R18</p></td><td><p>No</p></td><td><p>64.12\\pm0.82</p></td><td><p>80.51\\pm0.13</p></td><td><p>68.41\\pm0.39</p></td><td><p>84.24\\pm1.73</p></td></tr><tr><td><p>UCN</p></td><td><p>R18</p></td><td><p>Yes</p></td><td>73.98\\pm0.94</td><td>83.94\\pm0.59</td><td>75.79\\pm1.03</td><td>87.02\\pm0.43</td></tr><tr><td><p>CC+rot [27]</p></td><td><p>W28</p></td><td><p>Yes</p></td><td><p>62.93\\pm0.45</p></td><td><p>79.87\\pm0.33</p></td><td><p>70.53\\pm0.51</p></td><td><p>84.98\\pm0.36</p></td></tr><tr><td><p>EPNet [24]</p></td><td><p>W28</p></td><td><p>Yes</p></td><td><p>70.74\\pm0.85</p></td><td><p>84.34\\pm0.53</p></td><td><p>78.50\\pm0.91</p></td><td><p>88.36\\pm0.57</p></td></tr><tr><td><p>SIB [28]</p></td><td><p>W28</p></td><td><p>Yes</p></td><td><p>70.0  \\pm0.6</p></td><td><p>79.2  \\pm0.4</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>E{}^{3}BM [29]</p></td><td><p>W28</p></td><td><p>Yes</p></td><td><p>71.4  \\pm0.5</p></td><td><p>81.2  \\pm0.4</p></td><td><p>75.6  \\pm0.6</p></td><td><p>84.3  \\pm0.4</p></td></tr><tr><td><p>UCN</p></td><td><p>W28</p></td><td><p>Yes</p></td><td>78.55\\pm0.97</td><td>85.99\\pm0.57</td><td>79.66\\pm1.02</td><td>88.87\\pm0.63</td></tr></tbody></table>", "caption": "Table 1: Classification accuracy on miniImageNet and tieredImageNet reported with 95% confidence intervals.\u201dFE\u201d column describes the backbone, \u201dTr\u201d indicates whether methods are transductive, and the partially transductive methods are marked as \u201dBN\u201d because of using the mean and variance of query instance batches in BatchNorm layers [6, 7].The superscript \\dagger indicates dense classification [20] is adopted over feature maps.C64/256 is for Conv-64/256, R12/18 for ResNet-12/18 and W28 for WRN-28-10.", "list_citation_info": ["[5] Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto, \u201cA baseline for few-shot image classification,\u201d in ICLR, 2020.", "[3] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales, \u201cLearning to compare: Relation network for few-shot learning,\u201d in CVPR, 2018, pp. 1199\u20131208.", "[20] Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, and Andrei Bursuc, \u201cDense classification and implanting for few-shot learning,\u201d in CVPR, 2019, pp. 9258\u20139267.", "[28] Shell Xu Hu, Pablo Garcia Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D. Lawrence, and Andreas C. Damianou, \u201cEmpirical bayes transductive meta-learning with synthetic gradients,\u201d in ICLR, 2020.", "[8] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen, \u201cCross attention network for few-shot classification,\u201d in NeurIPS, 2019, pp. 4005\u20134016.", "[25] Manli Zhang, Jianhong Zhang, Zhiwu Lu, Tao Xiang, Mingyu Ding, and Songfang Huang, \u201cIEPT: instance-level and episode-level pretext tasks for few-shot learning,\u201d in ICLR, 2021.", "[15] Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste, \u201cTADAM: task dependent adaptive metric for improved few-shot learning,\u201d in NeurIPS, 2018, pp. 719\u2013729.", "[7] Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu Liu, \u201cDPGN: distribution propagation graph network for few-shot learning,\u201d in CVPR, 2020, pp. 13387\u201313396.", "[4] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang, \u201cLearning to propagate labels: Transductive propagation network for few-shot learning,\u201d in ICLR, 2019.", "[24] Pau Rodr\u00edguez, Issam H. Laradji, Alexandre Drouin, and Alexandre Lacoste, \u201cEmbedding propagation: Smoother manifold for few-shot classification,\u201d in ECCV, 2020, pp. 121\u2013138.", "[12] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Shih-Fu Chang, \u201cUncertainty-aware few-shot image classification,\u201d in IJCAI, 2021, pp. 3420\u20133426.", "[2] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra, \u201cMatching networks for one shot learning,\u201d in NIPS, 2016, pp. 3630\u20133638.", "[21] Chelsea Finn, Pieter Abbeel, and Sergey Levine, \u201cModel-agnostic meta-learning for fast adaptation of deep networks,\u201d in ICML, 2017, vol. 70, pp. 1126\u20131135.", "[27] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, and Matthieu Cord, \u201cBoosting few-shot visual learning with self-supervision,\u201d in ICCV, 2019, pp. 8058\u20138067.", "[22] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen, \u201cDeepemd: Few-shot image classification with differentiable earth mover\u2019s distance and structured classifiers,\u201d in CVPR, 2020, pp. 12200\u201312210.", "[23] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha, \u201cFew-shot learning via embedding adaptation with set-to-set functions,\u201d in CVPR, 2020, pp. 8805\u20138814.", "[1] Jake Snell, Kevin Swersky, and Richard S. Zemel, \u201cPrototypical networks for few-shot learning,\u201d in NIPS, 2017, pp. 4077\u20134087.", "[26] Hongyang Li, David Eigen, Samuel Dodge, Matthew Zeiler, and Xiaogang Wang, \u201cFinding task-relevant features for few-shot learning by category traversal,\u201d in CVPR, 2019, pp. 1\u201310.", "[6] Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo, \u201cEdge-labeling graph neural network for few-shot learning,\u201d in CVPR, 2019, pp. 11\u201320.", "[9] Seong Min Kye, Haebeom Lee, Hoirin Kim, and Sung Ju Hwang, \u201cTransductive few-shot learning with meta-learned confidence,\u201d CoRR, vol. abs/2002.12017, 2020.", "[29] Yaoyao Liu, Bernt Schiele, and Qianru Sun, \u201cAn ensemble of epoch-wise empirical bayes for few-shot learning,\u201d in ECCV, 2020, vol. 12361, pp. 404\u2013421."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">FE</td><td rowspan=\"2\">Tr</td><td colspan=\"2\">FC100(%)</td><td colspan=\"2\">CIFAR-FS(%)</td></tr><tr><td><p>1-shot</p></td><td><p>5-shot</p></td><td><p>1-shot</p></td><td><p>5-shot</p></td></tr><tr><td><p>TADAM [15]</p></td><td><p>R12</p></td><td><p>No</p></td><td><p>40.1   \\pm0.4</p></td><td><p>56.1   \\pm0.4</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>DeepEMD [22]</p></td><td><p>R12</p></td><td><p>No</p></td><td><p>46.47\\pm0.48</p></td><td>63.22\\pm0.71</td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>DPGN [7]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>-</p></td><td><p>-</p></td><td><p>77.9  \\pm0.5</p></td><td><p>90.2   \\pm0.4</p></td></tr><tr><td><p>UAFS[12]</p></td><td><p>R12</p></td><td><p>Yes</p></td><td><p>41.99\\pm0.58</p></td><td><p>57.43\\pm0.38</p></td><td><p>74.08\\pm0.72</p></td><td><p>85.92\\pm0.42</p></td></tr><tr><td><p>UCN</p></td><td><p>R12</p></td><td><p>Yes</p></td><td>48.00\\pm1.04</td><td><p>60.39\\pm0.82</p></td><td>88.90\\pm0.95</td><td>92.18\\pm0.63</td></tr><tr><td><p>SIB [28]</p></td><td><p>W28</p></td><td><p>Yes</p></td><td><p>-</p></td><td><p>-</p></td><td><p>80.0   \\pm0.6</p></td><td><p>85.3   \\pm0.4</p></td></tr><tr><td><p>UCN</p></td><td><p>W28</p></td><td><p>Yes</p></td><td>47.61\\pm0.99</td><td><p>59.74\\pm0.78</p></td><td>85.64\\pm0.93</td><td>88.90\\pm0.58</td></tr></tbody></table>", "caption": "Table 2: Classification accuracy on FC100 and CIFAR-FS are reported with 95% confidence intervals. ", "list_citation_info": ["[22] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen, \u201cDeepemd: Few-shot image classification with differentiable earth mover\u2019s distance and structured classifiers,\u201d in CVPR, 2020, pp. 12200\u201312210.", "[15] Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste, \u201cTADAM: task dependent adaptive metric for improved few-shot learning,\u201d in NeurIPS, 2018, pp. 719\u2013729.", "[12] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Shih-Fu Chang, \u201cUncertainty-aware few-shot image classification,\u201d in IJCAI, 2021, pp. 3420\u20133426.", "[28] Shell Xu Hu, Pablo Garcia Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D. Lawrence, and Andreas C. Damianou, \u201cEmpirical bayes transductive meta-learning with synthetic gradients,\u201d in ICLR, 2020.", "[7] Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu Liu, \u201cDPGN: distribution propagation graph network for few-shot learning,\u201d in CVPR, 2020, pp. 13387\u201313396."]}], "citation_info_to_title": {"[5] Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto, \u201cA baseline for few-shot image classification,\u201d in ICLR, 2020.": "A baseline for few-shot image classification", "[22] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen, \u201cDeepemd: Few-shot image classification with differentiable earth mover\u2019s distance and structured classifiers,\u201d in CVPR, 2020, pp. 12200\u201312210.": "Deepemd: Few-shot image classification with differentiable earth mover\u2019s distance and structured classifiers", "[12] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Shih-Fu Chang, \u201cUncertainty-aware few-shot image classification,\u201d in IJCAI, 2021, pp. 3420\u20133426.": "Uncertainty-aware few-shot image classification", "[27] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, and Matthieu Cord, \u201cBoosting few-shot visual learning with self-supervision,\u201d in ICCV, 2019, pp. 8058\u20138067.": "Boosting few-shot visual learning with self-supervision", "[2] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra, \u201cMatching networks for one shot learning,\u201d in NIPS, 2016, pp. 3630\u20133638.": "Matching networks for one shot learning", "[8] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen, \u201cCross attention network for few-shot classification,\u201d in NeurIPS, 2019, pp. 4005\u20134016.": "Cross attention network for few-shot classification", "[20] Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, and Andrei Bursuc, \u201cDense classification and implanting for few-shot learning,\u201d in CVPR, 2019, pp. 9258\u20139267.": "Dense classification and implanting for few-shot learning", "[6] Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo, \u201cEdge-labeling graph neural network for few-shot learning,\u201d in CVPR, 2019, pp. 11\u201320.": "Edge-labeling graph neural network for few-shot learning", "[9] Seong Min Kye, Haebeom Lee, Hoirin Kim, and Sung Ju Hwang, \u201cTransductive few-shot learning with meta-learned confidence,\u201d CoRR, vol. abs/2002.12017, 2020.": "Transductive Few-Shot Learning with Meta-Learned Confidence", "[26] Hongyang Li, David Eigen, Samuel Dodge, Matthew Zeiler, and Xiaogang Wang, \u201cFinding task-relevant features for few-shot learning by category traversal,\u201d in CVPR, 2019, pp. 1\u201310.": "Finding task-relevant features for few-shot learning by category traversal", "[15] Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, and Alexandre Lacoste, \u201cTADAM: task dependent adaptive metric for improved few-shot learning,\u201d in NeurIPS, 2018, pp. 719\u2013729.": "TADAM: task dependent adaptive metric for improved few-shot learning", "[29] Yaoyao Liu, Bernt Schiele, and Qianru Sun, \u201cAn ensemble of epoch-wise empirical bayes for few-shot learning,\u201d in ECCV, 2020, vol. 12361, pp. 404\u2013421.": "An ensemble of epoch-wise empirical bayes for few-shot learning", "[3] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales, \u201cLearning to compare: Relation network for few-shot learning,\u201d in CVPR, 2018, pp. 1199\u20131208.": "Learning to compare: Relation network for few-shot learning", "[21] Chelsea Finn, Pieter Abbeel, and Sergey Levine, \u201cModel-agnostic meta-learning for fast adaptation of deep networks,\u201d in ICML, 2017, vol. 70, pp. 1126\u20131135.": "Model-agnostic meta-learning for fast adaptation of deep networks", "[1] Jake Snell, Kevin Swersky, and Richard S. Zemel, \u201cPrototypical networks for few-shot learning,\u201d in NIPS, 2017, pp. 4077\u20134087.": "Prototypical networks for few-shot learning", "[24] Pau Rodr\u00edguez, Issam H. Laradji, Alexandre Drouin, and Alexandre Lacoste, \u201cEmbedding propagation: Smoother manifold for few-shot classification,\u201d in ECCV, 2020, pp. 121\u2013138.": "Embedding propagation: Smoother manifold for few-shot classification", "[28] Shell Xu Hu, Pablo Garcia Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D. Lawrence, and Andreas C. Damianou, \u201cEmpirical bayes transductive meta-learning with synthetic gradients,\u201d in ICLR, 2020.": "Empirical bayes transductive meta-learning with synthetic gradients", "[25] Manli Zhang, Jianhong Zhang, Zhiwu Lu, Tao Xiang, Mingyu Ding, and Songfang Huang, \u201cIEPT: instance-level and episode-level pretext tasks for few-shot learning,\u201d in ICLR, 2021.": "IEPT: Instance-Level and Episode-Level Pretext Tasks for Few-Shot Learning", "[4] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang, \u201cLearning to propagate labels: Transductive propagation network for few-shot learning,\u201d in ICLR, 2019.": "Learning to propagate labels: Transductive propagation network for few-shot learning", "[23] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha, \u201cFew-shot learning via embedding adaptation with set-to-set functions,\u201d in CVPR, 2020, pp. 8805\u20138814.": "Few-shot learning via embedding adaptation with set-to-set functions", "[7] Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu Liu, \u201cDPGN: distribution propagation graph network for few-shot learning,\u201d in CVPR, 2020, pp. 13387\u201313396.": "DPGN: Distribution Propagation Graph Network for Few-Shot Learning"}, "source_title_to_arxiv_id": {"Uncertainty-aware few-shot image classification": "2010.04525"}}