{"title": "Interaction Visual Transformer for Egocentric Action Anticipation", "abstract": "Human-object interaction is one of the most important visual cues and we\npropose a novel way to represent human-object interactions for egocentric\naction anticipation. We propose a novel transformer variant to model\ninteractions by computing the change in the appearance of objects and human\nhands due to the execution of the actions and use those changes to refine the\nvideo representation. Specifically, we model interactions between hands and\nobjects using Spatial Cross-Attention (SCA) and further infuse contextual\ninformation using Trajectory Cross-Attention to obtain environment-refined\ninteraction tokens. Using these tokens, we construct an interaction-centric\nvideo representation for action anticipation. We term our model InAViT which\nachieves state-of-the-art action anticipation performance on large-scale\negocentric datasets EPICKTICHENS100 (EK100) and EGTEA Gaze+. InAViT outperforms\nother visual transformer-based methods including object-centric video\nrepresentation. On the EK100 evaluation server, InAViT is the top-performing\nmethod on the public leaderboard (at the time of submission) where it\noutperforms the second-best model by 3.3% on mean-top5 recall.", "authors": ["Debaditya Roy", "Ramanathan Rajendiran", "Basura Fernando"], "published_date": "2022_11_25", "pdf_url": "http://arxiv.org/pdf/2211.14154v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">Overall (%)</td><td colspan=\"3\">Unseen (%)</td><td colspan=\"3\">Tail (%)</td></tr><tr><td>Verb</td><td>Noun</td><td>Action</td><td>Verb</td><td>Noun</td><td>Action</td><td>Verb</td><td>Noun</td><td>Action</td></tr><tr><th>RU-LSTM [10]</th><td>25.25</td><td>26.69</td><td>11.19</td><td>19.36</td><td>26.87</td><td>09.65</td><td>17.56</td><td>15.97</td><td>07.92</td></tr><tr><th>Temp. Agg. [53]</th><td>21.76</td><td>30.59</td><td>12.55</td><td>17.86</td><td>27.04</td><td>10.46</td><td>13.59</td><td>20.62</td><td>08.85</td></tr><tr><th>AVT [19]</th><td>26.69</td><td>32.33</td><td>16.74</td><td>21.03</td><td>27.64</td><td>12.89</td><td>19.28</td><td>24.03</td><td>13.81</td></tr><tr><th>Abs. goal [50]</th><td>31.40</td><td>30.10</td><td>14.29</td><td>31.36</td><td>35.56</td><td>17.34</td><td>22.90</td><td>16.42</td><td>07.70</td></tr><tr><th>TransAction [22]</th><td>36.15</td><td>32.20</td><td>13.39</td><td>27.60</td><td>24.24</td><td>10.05</td><td>32.06</td><td>29.87</td><td>11.88</td></tr><tr><th>MeMViT [61]</th><td>32.20</td><td>37.00</td><td>17.70</td><td>28.60</td><td>27.40</td><td>15.20</td><td>25.30</td><td>31.00</td><td>15.50</td></tr><tr><th>MF{}^{*} [43]</th><td>45.14</td><td>45.97</td><td>19.75</td><td>40.36</td><td>45.28</td><td>19.49</td><td>39.17</td><td>35.91</td><td>14.11</td></tr><tr><th>ORViT-MF{}^{*} [24]</th><td>43.74</td><td>46.61</td><td>21.53</td><td>38.99</td><td>45.32</td><td>21.47</td><td>37.28</td><td>35.78</td><td>15.96</td></tr><tr><th>InAViT (Ours)</th><td>49.14</td><td>49.97</td><td>23.75</td><td>44.36</td><td>49.28</td><td>23.49</td><td>43.17</td><td>39.91</td><td>18.11</td></tr></tbody></table>", "caption": "Table 4: Comparison with state-of-the-art on EK100 evaluation server.InAViT significantly outperforms other Transformer-based approaches such as AVT and MeMViT. Interaction-centric InAViT also performs better than MotionFormer and object-centric Motionformer (ORViT-MF). {}^{*}We trained MF and ORViT-MF for action anticipation using their official repositories.", "list_citation_info": ["[19] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13505\u201313515, 2021.", "[61] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13587\u201313597, 2022.", "[43] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo\u00e3o F Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. Advances in neural information processing systems, 34:12493\u201312506, 2021.", "[24] Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson. Object-region video transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3148\u20133159, 2022.", "[22] Xiao Gu, Jianing Qiu, Yao Guo, Benny Lo, and Guang-Zhong Yang. Transaction: ICL-SJTU submission to epic-kitchens action anticipation challenge 2021. CoRR, abs/2107.13259, 2021.", "[50] Debaditya Roy and Basura Fernando. Predicting the next action by modeling the abstract goal. arXiv preprint arXiv:2209.05044, 2022.", "[53] Fadime Sener, Dipika Singhania, and Angela Yao. Temporal aggregate representations for long-range video understanding. In European Conference on Computer Vision, pages 154\u2013171. Springer, 2020.", "[10] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, 130(1):33\u201355, 2022."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">Top-1 Accuracy (%)</td><td colspan=\"3\">Mean Class Accuracy (%)</td></tr><tr><td>VERB</td><td>NOUN</td><td>ACT.</td><td>VERB</td><td>NOUN</td><td>ACT.</td></tr><tr><th>I3D-Res50 [5]</th><td>48.0</td><td>42.1</td><td>34.8</td><td>31.3</td><td>30.0</td><td>23.2</td></tr><tr><th>FHOI [33]</th><td>49.0</td><td>45.5</td><td>36.6</td><td>32.5</td><td>32.7</td><td>25.3</td></tr><tr><th>RU-LSTM [14]</th><td>50.3</td><td>48.1</td><td>38.6</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AVT [19]</th><td>54.9</td><td>52.2</td><td>43.0</td><td>49.9</td><td>48.3</td><td>35.2</td></tr><tr><th>Abs. Goal [50]</th><td>64.8</td><td>65.3</td><td>49.8</td><td>63.4</td><td>55.6</td><td>37.4</td></tr><tr><th>MF{}^{*}</th><td>77.8</td><td>75.6</td><td>66.6</td><td>77.5</td><td>72.1</td><td>56.9</td></tr><tr><th>ORVIT-MF{}^{*}</th><td>78.8</td><td>76.3</td><td>67.3</td><td>78.8</td><td>75.8</td><td>57.2</td></tr><tr><th>InAViT (Ours)</th><td>79.3</td><td>77.6</td><td>67.8</td><td>79.2</td><td>76.9</td><td>58.2</td></tr></tbody></table>", "caption": "Table 5: Comparison of anticipation performance on EGTEA Gaze+. {}^{*}We trained MF and ORViT-MF for action anticipation using their official repositories.", "list_citation_info": ["[19] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13505\u201313515, 2021.", "[33] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Forecasting human-object interaction: joint prediction of motor attention and actions in first person video. In European Conference on Computer Vision, pages 704\u2013721. Springer, 2020.", "[50] Debaditya Roy and Basura Fernando. Predicting the next action by modeling the abstract goal. arXiv preprint arXiv:2209.05044, 2022.", "[14] Antonino Furnari and Giovanni Maria Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE transactions on pattern analysis and machine intelligence, 43(11):4021\u20134036, 2020.", "[5] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017."]}], "citation_info_to_title": {"[24] Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson. Object-region video transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3148\u20133159, 2022.": "Object-region video transformers", "[50] Debaditya Roy and Basura Fernando. Predicting the next action by modeling the abstract goal. arXiv preprint arXiv:2209.05044, 2022.": "Predicting the next action by modeling the abstract goal", "[5] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.": "Quo vadis, action recognition? a new model and the kinetics dataset", "[14] Antonino Furnari and Giovanni Maria Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE transactions on pattern analysis and machine intelligence, 43(11):4021\u20134036, 2020.": "Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video", "[43] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo\u00e3o F Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. Advances in neural information processing systems, 34:12493\u201312506, 2021.": "Keeping your eye on the ball: Trajectory attention in video transformers", "[10] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision, 130(1):33\u201355, 2022.": "Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-Kitchens-100", "[19] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13505\u201313515, 2021.": "Anticipative Video Transformer", "[61] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13587\u201313597, 2022.": "Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition", "[22] Xiao Gu, Jianing Qiu, Yao Guo, Benny Lo, and Guang-Zhong Yang. Transaction: ICL-SJTU submission to epic-kitchens action anticipation challenge 2021. CoRR, abs/2107.13259, 2021.": "Transaction: ICL-SJTU submission to epic-kitchens action anticipation challenge 2021", "[33] Miao Liu, Siyu Tang, Yin Li, and James M Rehg. Forecasting human-object interaction: joint prediction of motor attention and actions in first person video. In European Conference on Computer Vision, pages 704\u2013721. Springer, 2020.": "Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video", "[53] Fadime Sener, Dipika Singhania, and Angela Yao. Temporal aggregate representations for long-range video understanding. In European Conference on Computer Vision, pages 154\u2013171. Springer, 2020.": "Temporal Aggregate Representations for Long-Range Video Understanding"}, "source_title_to_arxiv_id": {"Predicting the next action by modeling the abstract goal": "2209.05044"}}