{"title": "A Survey of Robust 3D Object Detection Methods in Point Clouds", "abstract": "The purpose of this work is to review the state-of-the-art LiDAR-based 3D\nobject detection methods, datasets, and challenges. We describe novel data\naugmentation methods, sampling strategies, activation functions, attention\nmechanisms, and regularization methods. Furthermore, we list recently\nintroduced normalization methods, learning rate schedules and loss functions.\nMoreover, we also cover advantages and limitations of 10 novel autonomous\ndriving datasets. We evaluate novel 3D object detectors on the KITTI, nuScenes,\nand Waymo dataset and show their accuracy, speed, and robustness. Finally, we\nmention the current challenges in 3D object detection in LiDAR point clouds and\nlist some open issues.", "authors": ["Walter Zimmer", "Emec Ercelik", "Xingcheng Zhou", "Xavier Jair Diaz Ortiz", "Alois Knoll"], "published_date": "2022_03_31", "pdf_url": "http://arxiv.org/pdf/2204.00106v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Name</td><td>Year</td><td>PC frames</td><td>RGB images</td><td>Classes</td><td>Diff. weather/time</td><td>Obj./frame</td><td>Location</td></tr><tr><td>KITTI [2]</td><td>2012</td><td>15.4k</td><td>15k</td><td>8</td><td>No/No</td><td>13</td><td>Karlsruhe (Germany)</td></tr><tr><td>nuScenes[3]</td><td>2019</td><td>400k</td><td>1.4M</td><td>23</td><td>Yes/Yes</td><td>35</td><td>Boston (USA), Singapore (SG)</td></tr><tr><td>Waymo [4]</td><td>2019</td><td>200k</td><td>1M</td><td>4</td><td>Yes/Yes</td><td>60</td><td>SF , Phoenix, Mt. View (USA)</td></tr><tr><td>ArgoVerse [38]</td><td>2019</td><td>44k</td><td>490k</td><td>15</td><td>Yes/Yes</td><td>45</td><td>Pittsburgh, Miami (USA)</td></tr><tr><td>Lyft Lvl. 5[39]</td><td>2020</td><td>323k</td><td>46k</td><td>9</td><td>No/No</td><td>28</td><td>Palo Alto (USA)</td></tr><tr><td>A2D2 [40]</td><td>2020</td><td>41k</td><td>41k</td><td>38</td><td>Yes/No</td><td>-</td><td>Ingolstadt (Germany)</td></tr><tr><td>LIBRE [41]</td><td>2020</td><td>-</td><td>-</td><td>-</td><td>Yes/Yes</td><td>-</td><td>Nagoya (Japan)</td></tr><tr><td>ONCE [42]</td><td>2021</td><td>16k</td><td>16k</td><td>5</td><td>Yes/Yes</td><td>26</td><td>China</td></tr><tr><td>IPS300+ [43]</td><td>2021</td><td>1.25k</td><td>2.5k</td><td>8</td><td>No</td><td>320</td><td>China</td></tr><tr><td>A9-Dataset [44]</td><td>2022</td><td>17.4k</td><td>68.4k</td><td>10</td><td>Yes/Yes</td><td>34</td><td>Munich (Germany)</td></tr></tbody></table>", "caption": "TABLE I: Autonomous Driving datasets that are used for 3D object detection. ", "list_citation_info": ["[44] C. Cress, W. Zimmer, L. Strand, M. Fortkord, S. Dai, V. Lakshminarasimhan, and A. Knoll, \u201cA9-dataset: Multi-sensor infrastructure-based dataset for mobility research,\u201d arXiv preprint arXiv, 2022.", "[3] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuScenes: A Multimodal Dataset for Autonomous Driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009621\u201311\u2009631.", "[40] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung, L. Hauswald, V. H. Pham, M. M\u00fchlegg, S. Dorn et al., \u201cA2d2: Audi autonomous driving dataset,\u201d arXiv preprint arXiv:2004.06320, 2020.", "[38] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett, D. Wang, P. Carr, S. Lucey, D. Ramanan et al., \u201cArgoverse: 3d tracking and forecasting with rich maps,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8748\u20138757.", "[4] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine et al., \u201cScalability in perception for autonomous driving: Waymo open dataset,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 2446\u20132454.", "[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets Robotics: The KITTI Dataset,\u201d p. 6, 2013.", "[39] J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, L. Chen, A. Jain, S. Omari, V. Iglovikov, and P. Ondruska, \u201cOne thousand and one hours: Self-driving motion prediction dataset,\u201d arXiv preprint arXiv:2006.14480, 2020.", "[43] H. Wang, X. Zhang, J. Li, Z. Li, L. Yang, S. Pan, and Y. Deng, \u201cIps300+: a challenging multimodal dataset for intersection perception system,\u201d arXiv preprint arXiv:2106.02781, 2021.", "[42] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W. Zhang, Z. Li et al., \u201cOne million scenes for autonomous driving: Once dataset,\u201d arXiv preprint arXiv:2106.11037, 2021.", "[41] A. Carballo, J. Lambert, A. Monrroy, D. Wong, P. Narksri, Y. Kitsukawa, E. Takeuchi, S. Kato, and K. Takeda, \u201cLibre: The multiple 3d lidar dataset,\u201d in 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2020, pp. 1094\u20131101."]}, {"table": "<table><tbody><tr><th>Method</th><th></th><td colspan=\"2\">KITTI</td><td colspan=\"2\">nuScenes</td><td colspan=\"2\">Waymo</td></tr><tr><th></th><th>Year</th><td>3D mAP</td><td>FPS (ms)</td><td>NDS</td><td>FPS (ms)</td><td>3D mAP</td><td>FPS (ms)</td></tr><tr><th>PointPillars [11]</th><th>2019</th><td>74.31%</td><td>62 (16 ms)</td><td>0.44</td><td>62 (16 ms)</td><td>45.52%</td><td>62 (16 ms)</td></tr><tr><th>3DSSD [12]</th><th>2020</th><td>79.57%</td><td>26 (38ms)</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SA-SSD [13]</th><th>2020</th><td>79.79%</td><td>25 (40ms)</td><td>-</td><td>-</td><td>61.48%</td><td>25 (40ms)</td></tr><tr><th>CIA-SSD [45]</th><th>2021</th><td>80.28%</td><td>33 (30 ms)</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SE-SSD [18]</th><th>2021</th><td>83.73%</td><td>33 (30 ms)</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PV-RCNN [15]</th><th>2020</th><td>81.43%</td><td>13 (80 ms)</td><td>-</td><td>-</td><td>70.30%</td><td>4 (300 ms)</td></tr><tr><th>PV-RCNN++ [16]</th><th>2021</th><td>81.88%</td><td>16 (60 ms)</td><td>-</td><td>-</td><td>74.81%</td><td>10 (100 ms)</td></tr><tr><th>CenterPoint [17]</th><th>2021</th><td>74.87%</td><td>20 (51 ms)</td><td>0.71</td><td>16 (60 ms)</td><td>79.25%</td><td>11 (89 ms)</td></tr></tbody></table>", "caption": "TABLE II: Comparison of the state-of-the-art methods on the KITTI (3D car, moderate), nuScenes and Waymo Level 1 test set.", "list_citation_info": ["[12] Z. Yang, Y. Sun, S. Liu, and J. Jia, \u201c3dssd: Point-based 3d single stage object detector,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11\u2009040\u201311\u2009048.", "[13] C. He, H. Zeng, J. Huang, X.-S. Hua, and L. Zhang, \u201cStructure aware single-stage 3d object detection from point cloud,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009873\u201311\u2009882.", "[45] W. Zheng, W. Tang, S. Chen, L. Jiang, and C.-W. Fu, \u201cCia-ssd: Confident iou-aware single-stage object detector from point cloud,\u201d arXiv preprint arXiv:2012.03015, 2020.", "[18] W. Zheng, W. Tang, L. Jiang, and C.-W. Fu, \u201cSe-ssd: Self-ensembling single-stage object detector from point cloud,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14\u2009494\u201314\u2009503.", "[15] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cPV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection,\u201d 2020, pp. 10\u2009529\u201310\u2009538.", "[17] T. Yin, X. Zhou, and P. Krahenbuhl, \u201cCenter-based 3d object detection and tracking,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 11\u2009784\u201311\u2009793.", "[11] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointPillars: Fast Encoders for Object Detection From Point Clouds,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Long Beach, CA, USA: IEEE, Jun. 2019, pp. 12\u2009689\u201312\u2009697.", "[16] S. Shi, L. Jiang, J. Deng, Z. Wang, C. Guo, J. Shi, X. Wang, and H. Li, \u201cPv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection,\u201d arXiv preprint arXiv:2102.00463, 2021."]}], "citation_info_to_title": {"[17] T. Yin, X. Zhou, and P. Krahenbuhl, \u201cCenter-based 3d object detection and tracking,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 11\u2009784\u201311\u2009793.": "Center-based 3d object detection and tracking", "[11] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointPillars: Fast Encoders for Object Detection From Point Clouds,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Long Beach, CA, USA: IEEE, Jun. 2019, pp. 12\u2009689\u201312\u2009697.": "PointPillars: Fast Encoders for Object Detection From Point Clouds", "[38] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett, D. Wang, P. Carr, S. Lucey, D. Ramanan et al., \u201cArgoverse: 3d tracking and forecasting with rich maps,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8748\u20138757.": "Argoverse: 3d tracking and forecasting with rich maps", "[18] W. Zheng, W. Tang, L. Jiang, and C.-W. Fu, \u201cSe-ssd: Self-ensembling single-stage object detector from point cloud,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14\u2009494\u201314\u2009503.": "Se-ssd: Self-ensembling single-stage object detector from point cloud", "[4] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine et al., \u201cScalability in perception for autonomous driving: Waymo open dataset,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 2446\u20132454.": "Scalability in perception for autonomous driving: Waymo open dataset", "[2] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets Robotics: The KITTI Dataset,\u201d p. 6, 2013.": "Vision meets Robotics: The KITTI Dataset", "[13] C. He, H. Zeng, J. Huang, X.-S. Hua, and L. Zhang, \u201cStructure aware single-stage 3d object detection from point cloud,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009873\u201311\u2009882.": "Structure aware single-stage 3d object detection from point cloud", "[16] S. Shi, L. Jiang, J. Deng, Z. Wang, C. Guo, J. Shi, X. Wang, and H. Li, \u201cPv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection,\u201d arXiv preprint arXiv:2102.00463, 2021.": "Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection", "[39] J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, L. Chen, A. Jain, S. Omari, V. Iglovikov, and P. Ondruska, \u201cOne thousand and one hours: Self-driving motion prediction dataset,\u201d arXiv preprint arXiv:2006.14480, 2020.": "One thousand and one hours: Self-driving motion prediction dataset", "[43] H. Wang, X. Zhang, J. Li, Z. Li, L. Yang, S. Pan, and Y. Deng, \u201cIps300+: a challenging multimodal dataset for intersection perception system,\u201d arXiv preprint arXiv:2106.02781, 2021.": "Ips300+: A Challenging Multimodal Dataset for Intersection Perception System", "[41] A. Carballo, J. Lambert, A. Monrroy, D. Wong, P. Narksri, Y. Kitsukawa, E. Takeuchi, S. Kato, and K. Takeda, \u201cLibre: The multiple 3d lidar dataset,\u201d in 2020 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2020, pp. 1094\u20131101.": "Libre: The multiple 3d lidar dataset", "[12] Z. Yang, Y. Sun, S. Liu, and J. Jia, \u201c3dssd: Point-based 3d single stage object detector,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11\u2009040\u201311\u2009048.": "3dssd: Point-based 3d single stage object detector", "[15] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cPV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection,\u201d 2020, pp. 10\u2009529\u201310\u2009538.": "PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection", "[44] C. Cress, W. Zimmer, L. Strand, M. Fortkord, S. Dai, V. Lakshminarasimhan, and A. Knoll, \u201cA9-dataset: Multi-sensor infrastructure-based dataset for mobility research,\u201d arXiv preprint arXiv, 2022.": "A9-dataset: Multi-sensor infrastructure-based dataset for mobility research", "[40] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung, L. Hauswald, V. H. Pham, M. M\u00fchlegg, S. Dorn et al., \u201cA2d2: Audi autonomous driving dataset,\u201d arXiv preprint arXiv:2004.06320, 2020.": "A2d2: Audi autonomous driving dataset", "[3] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuScenes: A Multimodal Dataset for Autonomous Driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009621\u201311\u2009631.": "nuScenes: A Multimodal Dataset for Autonomous Driving", "[42] J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W. Zhang, Z. Li et al., \u201cOne million scenes for autonomous driving: Once dataset,\u201d arXiv preprint arXiv:2106.11037, 2021.": "One million scenes for autonomous driving: Once dataset", "[45] W. Zheng, W. Tang, S. Chen, L. Jiang, and C.-W. Fu, \u201cCia-ssd: Confident iou-aware single-stage object detector from point cloud,\u201d arXiv preprint arXiv:2012.03015, 2020.": "Cia-ssd: Confident iou-aware single-stage object detector from point cloud"}, "source_title_to_arxiv_id": {"One thousand and one hours: Self-driving motion prediction dataset": "2006.14480"}}