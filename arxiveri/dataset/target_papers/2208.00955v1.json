{"title": "Large-Scale Product Retrieval with Weakly Supervised Representation Learning", "abstract": "Large-scale weakly supervised product retrieval is a practically useful yet\ncomputationally challenging problem. This paper introduces a novel solution for\nthe eBay Visual Search Challenge (eProduct) held at the Ninth Workshop on\nFine-Grained Visual Categorisation workshop (FGVC9) of CVPR 2022. This\ncompetition presents two challenges: (a) E-commerce is a drastically\nfine-grained domain including many products with subtle visual differences; (b)\nA lacking of target instance-level labels for model training, with only coarse\ncategory labels and product titles available. To overcome these obstacles, we\nformulate a strong solution by a set of dedicated designs: (a) Instead of using\ntext training data directly, we mine thousands of pseudo-attributes from\nproduct titles and use them as the ground truths for multi-label\nclassification. (b) We incorporate several strong backbones with advanced\ntraining recipes for more discriminative representation learning. (c) We\nfurther introduce a number of post-processing techniques including whitening,\nre-ranking and model ensemble for retrieval enhancement. By achieving 71.53%\nMAR, our solution \"Involution King\" achieves the second position on the\nleaderboard.", "authors": ["Xiao Han", "Kam Woh Ng", "Sauradip Nag", "Zhiyu Qu"], "published_date": "2022_08_01", "pdf_url": "http://arxiv.org/pdf/2208.00955v1", "list_table_and_caption": [{"table": "<table><tr><td>Training config</td><td>ConvNeXt-XL[18] / Swin-L[16]</td></tr><tr><td>training resolution</td><td>224 / 384</td></tr><tr><td>inference resoultion</td><td>316 &amp; 384 / 384</td></tr><tr><td>optimizer</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>1e-4</td></tr><tr><td>poly loss \\epsilon [14]</td><td>0.5</td></tr><tr><td>optimizer momentum</td><td>\\beta_{1},\\beta_{2}=0.9,0.999</td></tr><tr><td>batch size</td><td>28 \\times 8 / 14 \\times 8</td></tr><tr><td>training epochs</td><td>20</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td></tr><tr><td>warmup epochs</td><td>5</td></tr><tr><td>warmup schedule</td><td>linear</td></tr><tr><td>TrivialAugment [20]</td><td>num_magnitude_bins = 31</td></tr><tr><td>random erasing [35]</td><td>0.25</td></tr><tr><td>stochastic depth [10]</td><td>0.4</td></tr><tr><td>head init scale [26]</td><td>0.01</td></tr><tr><td>exp. mov. avg. (EMA) [21]</td><td>0.9999</td></tr><tr><td>reranking [34]</td><td>K_{1},K_{2},\\alpha=8,5,0.5</td></tr></table>", "caption": "Table 1: All hyper-parameters used in our solution. Multiple hyper-parameters (e.g., 224 / 384 training resolution) are for each model (e.g., ConvNeXt-XL / Swin-L) respectively.", "list_citation_info": ["[18] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[35] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, 2020.", "[20] Samuel G M\u00fcller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmentation. In ICCV, 2021.", "[21] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 1992.", "[14] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classification loss functions. In ICLR, 2022.", "[34] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-ranking person re-identification with k-reciprocal encoding. In CVPR, 2017.", "[16] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.", "[26] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In ICCV, 2021.", "[10] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016."]}, {"table": "<table><tr><td>Method</td><td>Used Info.</td><td>Objective</td><td>MAR@10</td></tr><tr><td>SLC</td><td>coarse labels</td><td>SupCon [13]</td><td>16.47</td></tr><tr><td>SLC</td><td>coarse labels</td><td>X-Entropy [32, 1]</td><td>21.61</td></tr><tr><td>CMC</td><td>product titles</td><td>InfoNCE [25, 22]</td><td>32.59</td></tr><tr><td>SLC</td><td>pseudo labels</td><td>SupCon [2]</td><td>37.35</td></tr><tr><td>MLC</td><td>pseudo attributes</td><td>X-Entropy</td><td>45.69</td></tr></table>", "caption": "Table 2: The comparisons of different learning objectives.SLC: Single-Label Classification / Contrastive;CMC: Contratsive Multi-view Coding;MLC: Multi-Label Classification.", "list_citation_info": ["[13] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 2020.", "[2] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018.", "[32] Andrew Zhai and Hao-Yu Wu. Classification is a strong baseline for deep metric learning. In BMVC, 2018.", "[25] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, 2020."]}, {"table": "<table><tr><td>Component</td><td>MAR@10</td></tr><tr><td>Baseline (ConvNeXt-XL [18])</td><td>\\sim 59.00</td></tr><tr><td>+ Higher Inference Resolution</td><td>\\sim 61.00</td></tr><tr><td>+ Drop Path</td><td>\\sim 64.00</td></tr><tr><td>+ PolyLoss (\\epsilon=0.5)</td><td>\\sim 64.50</td></tr><tr><td>+ EMA</td><td>\\sim 65.00</td></tr><tr><td>+ Feature Whitening</td><td>\\sim 66.00</td></tr><tr><td>+ Re-ranking</td><td>\\sim 69.00</td></tr><tr><td>+ Ensemble (4 models)</td><td>71.53</td></tr></table>", "caption": "Table 3: The ablation study of different components we used. The symbol \\sim represents approximate numbers.", "list_citation_info": ["[18] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022."]}], "citation_info_to_title": {"[14] Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, and Dragomir Anguelov. Polyloss: A polynomial expansion perspective of classification loss functions. In ICLR, 2022.": "Polyloss: A Polynomial Expansion Perspective of Classification Loss Functions", "[18] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.": "A convnet for the 2020s", "[35] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, 2020.": "Random Erasing Data Augmentation", "[2] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, 2018.": "Deep clustering for unsupervised learning of visual features", "[32] Andrew Zhai and Hao-Yu Wu. Classification is a strong baseline for deep metric learning. In BMVC, 2018.": "Classification is a strong baseline for deep metric learning", "[10] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In ECCV, 2016.": "Deep Networks with Stochastic Depth", "[34] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. Re-ranking person re-identification with k-reciprocal encoding. In CVPR, 2017.": "Re-ranking person re-identification with k-reciprocal encoding", "[26] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In ICCV, 2021.": "Going deeper with image transformers", "[20] Samuel G M\u00fcller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmentation. In ICCV, 2021.": "Trivialaugment: Tuning-free yet state-of-the-art data augmentation", "[25] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, 2020.": "Contrastive multiview coding", "[21] Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 1992.": "Acceleration of Stochastic Approximation by Averaging", "[16] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[13] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. NeurIPS, 2020.": "Supervised contrastive learning"}, "source_title_to_arxiv_id": {"A convnet for the 2020s": "2201.03545", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}