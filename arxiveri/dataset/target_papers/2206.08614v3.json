{"title": "Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment", "abstract": "Computational inference of aesthetics is an ill-defined task due to its\nsubjective nature. Many datasets have been proposed to tackle the problem by\nproviding pairs of images and aesthetic scores based on human ratings. However,\nhumans are better at expressing their opinion, taste, and emotions by means of\nlanguage rather than summarizing them in a single number. In fact, photo\ncritiques provide much richer information as they reveal how and why users rate\nthe aesthetics of visual stimuli. In this regard, we propose the Reddit Photo\nCritique Dataset (RPCD), which contains tuples of image and photo critiques.\nRPCD consists of 74K images and 220K comments and is collected from a Reddit\ncommunity used by hobbyists and professional photographers to improve their\nphotography skills by leveraging constructive community feedback. The proposed\ndataset differs from previous aesthetics datasets mainly in three aspects,\nnamely (i) the large scale of the dataset and the extension of the comments\ncriticizing different aspects of the image, (ii) it contains mostly UltraHD\nimages, and (iii) it can easily be extended to new data as it is collected\nthrough an automatic pipeline. To the best of our knowledge, in this work, we\npropose the first attempt to estimate the aesthetic quality of visual stimuli\nfrom the critiques. To this end, we exploit the polarity of the sentiment of\ncriticism as an indicator of aesthetic judgment. We demonstrate how sentiment\npolarity correlates positively with the aesthetic judgment available for two\naesthetic assessment benchmarks. Finally, we experiment with several models by\nusing the sentiment scores as a target for ranking images. Dataset and\nbaselines are available (https://github.com/mediatechnologycenter/aestheval).", "authors": ["Daniel Vera Nieto", "Luigi Celona", "Clara Fernandez-Labrador"], "published_date": "2022_06_17", "pdf_url": "http://arxiv.org/pdf/2206.08614v3", "list_table_and_caption": [{"table": "<table><tr><td>Dataset</td><td> AVA-CommentsZhou et al. (2016) </td><td> DPC-Captions{}^{**}Jin et al. (2019) </td><td> PCCDChang et al. (2017) </td><td> RPCD(Our) </td></tr><tr><td>Images</td><td>253,961</td><td>117,132</td><td>4,235</td><td>73,965</td></tr><tr><td>Avg image resolution</td><td>607\\times537</td><td>606\\times534</td><td>1414\\times1202</td><td>2993\\times2716</td></tr><tr><td>Attributes</td><td>\u2013</td><td>5</td><td>7</td><td>7{}^{*}</td></tr><tr><td>Comments</td><td>3,601,761</td><td>208,926</td><td>29,645</td><td>219,790</td></tr><tr><td>Comments per image</td><td>14.1</td><td>1.8</td><td>6.6</td><td>2.9</td></tr><tr><td>Avg words per comment</td><td>14.6</td><td>24.5</td><td>41.1</td><td>49.1</td></tr><tr><td>Max words per comment</td><td>2146</td><td>549</td><td>780</td><td>1286</td></tr><tr><td>Content category</td><td>66</td><td>66</td><td>27</td><td>6{}^{*}</td></tr><tr><td>Rating scale</td><td>1-10</td><td>1-10</td><td>1-10</td><td>0-1{}^{*}</td></tr><tr><td>Avg raters per image</td><td>6</td><td>15</td><td>7</td><td>\u2013</td></tr></table><p>{}^{*}The aspect is obtained through machined-based annotation. See Appendix A.4.</p><p>{}^{**}The figures reported on this table are produced using the code made available by the authors of the dataset and differ from those stated in the original paper.</p>", "caption": "Table 1: Comparison of the properties in different benchmark datasets on image aesthetic captioning.", "list_citation_info": ["Zhou et al. [2016] Ye Zhou, Xin Lu, Junping Zhang, and James Z Wang. Joint image and text representation for aesthetics analysis. In International Conference on Multimedia, pages 262\u2013266. ACM, 2016.", "Chang et al. [2017] Kuang-Yu Chang, Kung-Hung Lu, and Chu-Song Chen. Aesthetic critiques generation for photos. In International Conference on Computer Vision, pages 3514\u20133523. IEEE, 2017.", "Jin et al. [2019] Xin Jin, Le Wu, Geng Zhao, Xiaodong Li, Xiaokun Zhang, Shiming Ge, Dongqing Zou, Bin Zhou, and Xinghui Zhou. Aesthetic attributes assessment of images. In International Conference on Multimedia, pages 311\u2013319. ACM, 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">AVA</td><td colspan=\"3\">PCCD</td><td colspan=\"3\">RPCD</td></tr><tr><td>SRCC</td><td>LCC</td><td>Acc. (%)</td><td>SRCC</td><td>LCC</td><td>Acc. (%)</td><td>SRCC</td><td>LCC</td><td>Acc. (%)</td></tr><tr><td>NIMA Talebi and Milanfar (2018)</td><td>0.253</td><td>0.259</td><td>90.20</td><td>0.066</td><td>0.070</td><td>93.87</td><td>0.120</td><td>0.116</td><td>63.25</td></tr><tr><td>ViT + Linear probe{}^{*}</td><td>0.570</td><td>0.570</td><td>76.43</td><td>0.156</td><td>0.165</td><td>93.04</td><td>0.172</td><td>0.173</td><td>64.58</td></tr><tr><td>AestheticViT{}^{*}</td><td>0.544</td><td>0.550</td><td>90.54</td><td>0.228</td><td>0.262</td><td>93.86</td><td>0.250</td><td>0.253</td><td>65.27</td></tr></table><p>{}^{*}Best performing models. See Appendix B</p>", "caption": "Table 2: Sentiment score baseline on the three considered datasets.", "list_citation_info": ["Talebi and Milanfar [2018] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE Transactions on Image Processing, 27(8):3998\u20134011, 2018."]}, {"table": "<table><tr><td></td><td>Bleu1</td><td>Bleu2</td><td>Bleu3</td><td>Bleu4</td><td>METEOR</td><td>ROUGE</td><td>CIDEr</td><td>SPICE</td></tr><tr><td>PCCD</td><td>0.165</td><td>0.065</td><td>0.028</td><td>0.011</td><td>0.063</td><td>0.137</td><td>0.049</td><td>0.048</td></tr><tr><td>RPCD</td><td>0.211</td><td>0.088</td><td>0.038</td><td>0.017</td><td>0.077</td><td>0.157</td><td>0.048</td><td>0.040</td></tr></table>", "caption": "Table 3: Aesthetic image captioning using BLIP Li et al. (2022) on PCCD and our RPCD.", "list_citation_info": ["Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, 2022."]}, {"table": "<table><tr><td>Model</td><td>SRCC</td><td>LCC</td><td>Accuracy (%)</td></tr><tr><td>Murray et al. Murray et al. (2012)</td><td>\u2013</td><td>\u2013</td><td>66.70</td></tr><tr><td>Lu et al. Lu et al. (2014)</td><td>\u2013</td><td>\u2013</td><td>74.46</td></tr><tr><td>Ma et al. Ma et al. (2017)</td><td>\u2013</td><td>\u2013</td><td>81.70</td></tr><tr><td>Kong et al. Kong et al. (2016)</td><td>0.558</td><td>\u2013</td><td>77.33</td></tr><tr><td>Talebi et al. Talebi and Milanfar (2018)</td><td>0.612</td><td>0.636</td><td>81.51</td></tr><tr><td>Chen et al. Chen et al. (2020)</td><td>0.649</td><td>0.671</td><td>83.20</td></tr><tr><td>Xu et al. Xu et al. (2020)</td><td>0.724</td><td>0.725</td><td>80.90</td></tr><tr><td>Ke et al. Ke et al. (2021)</td><td>0.726</td><td>0.738</td><td>81.15</td></tr><tr><td>Celona et al. Celona et al. (2022)</td><td>0.731</td><td>0.732</td><td>80.75</td></tr><tr><td>Hosu et al. Hosu et al. (2019)</td><td>0.756</td><td>0.757</td><td>81.72</td></tr><tr><td>ViT-L/16 - 21k</td><td>0.793</td><td>0.793</td><td>82.85</td></tr></table>", "caption": "Table 7: Comparison of our baseline with state-of-the-art methods on the AVA dataset for image aesthetic assessment. In each column, the best and second-best results are marked in boldface and underlined, respectively. The \u201c\u2013\u201d means that the result is not available.", "list_citation_info": ["Celona et al. [2022] Luigi Celona, Marco Leonardi, Paolo Napoletano, and Alessandro Rozza. Composition and style attributes guided image aesthetic assessment. IEEE Transactions on Image Processing, 31:5009\u20135024, 2022.", "Talebi and Milanfar [2018] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE Transactions on Image Processing, 27(8):3998\u20134011, 2018.", "Ke et al. [2021] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In International Conference on Computer Vision, pages 5148\u20135157. IEEE/CVF, 2021.", "Hosu et al. [2019] Vlad Hosu, Bastian Goldlucke, and Dietmar Saupe. Effective aesthetics prediction with multi-level spatially pooled features. In Conference on Computer Vision and Pattern Recognition, pages 9375\u20139383. IEEE/CVF, 2019.", "Kong et al. [2016] Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. Photo aesthetics ranking network with attributes and content adaptation. In European Conference on Computer Vision, pages 662\u2013679. Springer, 2016.", "Chen et al. [2020] Qiuyu Chen, Wei Zhang, Ning Zhou, Peng Lei, Yi Xu, Yu Zheng, and Jianping Fan. Adaptive fractional dilated convolution network for image aesthetics assessment. In Computer Vision and Pattern Recognition, pages 14114\u201314123. IEEE/CVF, 2020.", "Lu et al. [2014] Xin Lu, Zhe Lin, Hailin Jin, Jianchao Yang, and James Z Wang. Rapid: Rating pictorial aesthetics using deep learning. In International Conference on Multimedia, pages 457\u2013466. ACM, 2014.", "Murray et al. [2012] Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: A large-scale database for aesthetic visual analysis. In Conference on Computer Vision and Pattern Recognition, pages 2408\u20132415. IEEE, 2012.", "Ma et al. [2017] Shuang Ma, Jing Liu, and Chang Wen Chen. A-lamp: Adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. In Conference on Computer Vision and Pattern Recognition, pages 4535\u20134544. IEEE, 2017.", "Xu et al. [2020] Yifei Xu, Nuo Zhang, Pingping Wei, Genan Sang, Li Li, and Feng Yuan. Deep neural framework with visual attention and global context for predicting image aesthetics. IEEE Access, 2020."]}], "citation_info_to_title": {"Murray et al. [2012] Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: A large-scale database for aesthetic visual analysis. In Conference on Computer Vision and Pattern Recognition, pages 2408\u20132415. IEEE, 2012.": "Ava: A large-scale database for aesthetic visual analysis", "Hosu et al. [2019] Vlad Hosu, Bastian Goldlucke, and Dietmar Saupe. Effective aesthetics prediction with multi-level spatially pooled features. In Conference on Computer Vision and Pattern Recognition, pages 9375\u20139383. IEEE/CVF, 2019.": "Effective aesthetics prediction with multi-level spatially pooled features", "Ke et al. [2021] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In International Conference on Computer Vision, pages 5148\u20135157. IEEE/CVF, 2021.": "Musiq: Multi-scale image quality transformer", "Xu et al. [2020] Yifei Xu, Nuo Zhang, Pingping Wei, Genan Sang, Li Li, and Feng Yuan. Deep neural framework with visual attention and global context for predicting image aesthetics. IEEE Access, 2020.": "Deep Neural Framework with Visual Attention and Global Context for Predicting Image Aesthetics", "Celona et al. [2022] Luigi Celona, Marco Leonardi, Paolo Napoletano, and Alessandro Rozza. Composition and style attributes guided image aesthetic assessment. IEEE Transactions on Image Processing, 31:5009\u20135024, 2022.": "Composition and style attributes guided image aesthetic assessment", "Kong et al. [2016] Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, and Charless Fowlkes. Photo aesthetics ranking network with attributes and content adaptation. In European Conference on Computer Vision, pages 662\u2013679. Springer, 2016.": "Photo aesthetics ranking network with attributes and content adaptation", "Zhou et al. [2016] Ye Zhou, Xin Lu, Junping Zhang, and James Z Wang. Joint image and text representation for aesthetics analysis. In International Conference on Multimedia, pages 262\u2013266. ACM, 2016.": "Joint image and text representation for aesthetics analysis", "Ma et al. [2017] Shuang Ma, Jing Liu, and Chang Wen Chen. A-lamp: Adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. In Conference on Computer Vision and Pattern Recognition, pages 4535\u20134544. IEEE, 2017.": "A-lamp: Adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment", "Jin et al. [2019] Xin Jin, Le Wu, Geng Zhao, Xiaodong Li, Xiaokun Zhang, Shiming Ge, Dongqing Zou, Bin Zhou, and Xinghui Zhou. Aesthetic attributes assessment of images. In International Conference on Multimedia, pages 311\u2013319. ACM, 2019.": "Aesthetic Attributes Assessment of Images", "Chang et al. [2017] Kuang-Yu Chang, Kung-Hung Lu, and Chu-Song Chen. Aesthetic critiques generation for photos. In International Conference on Computer Vision, pages 3514\u20133523. IEEE, 2017.": "Aesthetic critiques generation for photos", "Talebi and Milanfar [2018] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE Transactions on Image Processing, 27(8):3998\u20134011, 2018.": "Nima: Neural image assessment", "Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, 2022.": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "Chen et al. [2020] Qiuyu Chen, Wei Zhang, Ning Zhou, Peng Lei, Yi Xu, Yu Zheng, and Jianping Fan. Adaptive fractional dilated convolution network for image aesthetics assessment. In Computer Vision and Pattern Recognition, pages 14114\u201314123. IEEE/CVF, 2020.": "Adaptive fractional dilated convolution network for image aesthetics assessment", "Lu et al. [2014] Xin Lu, Zhe Lin, Hailin Jin, Jianchao Yang, and James Z Wang. Rapid: Rating pictorial aesthetics using deep learning. In International Conference on Multimedia, pages 457\u2013466. ACM, 2014.": "Rapid: Rating pictorial aesthetics using deep learning"}, "source_title_to_arxiv_id": {"Composition and style attributes guided image aesthetic assessment": "2111.04647", "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation": "2201.12086"}}