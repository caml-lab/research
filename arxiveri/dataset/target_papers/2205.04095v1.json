{"title": "SmoothNets: Optimizing CNN architecture design for differentially private deep learning", "abstract": "The arguably most widely employed algorithm to train deep neural networks\nwith Differential Privacy is DPSGD, which requires clipping and noising of\nper-sample gradients. This introduces a reduction in model utility compared to\nnon-private training. Empirically, it can be observed that this accuracy\ndegradation is strongly dependent on the model architecture. We investigated\nthis phenomenon and, by combining components which exhibit good individual\nperformance, distilled a new model architecture termed SmoothNet, which is\ncharacterised by increased robustness to the challenges of DP-SGD training.\nExperimentally, we benchmark SmoothNet against standard architectures on two\nbenchmark datasets and observe that our architecture outperforms others,\nreaching an accuracy of 73.5\\% on CIFAR-10 at $\\varepsilon=7.0$ and 69.2\\% at\n$\\varepsilon=7.0$ on ImageNette, a state-of-the-art result compared to prior\narchitectural modifications for DP.", "authors": ["Nicolas W. Remerscheid", "Alexander Ziller", "Daniel Rueckert", "Georgios Kaissis"], "published_date": "2022_05_09", "pdf_url": "http://arxiv.org/pdf/2205.04095v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Reference</th><th>Dataset</th><th>\\varepsilon</th><th>Accuracy</th><th>Type of work</th></tr></thead><tbody><tr><td>Papernot et al. 2020</td><td rowspan=\"4\">CIFAR-10</td><td>7.53</td><td>66.2\\%</td><td>Architecture</td></tr><tr><td>Klause et al. 2022</td><td>7.5</td><td>71.7\\%</td><td>Architecture</td></tr><tr><td>D\u00f6rmann et al. 2021</td><td>7.42</td><td>70.1\\%</td><td>Training method</td></tr><tr><td>De et al. 2022</td><td>8.0</td><td>81.4\\%</td><td>Training method</td></tr><tr><td>Klause et al. 2022</td><td>ImageNette</td><td>7.5</td><td>64.8\\%</td><td>Architecture</td></tr></tbody></table>", "caption": "Table 1: Results of other works on the same datasets utilised in our study. Type of work refers to the categories specified in Section 1.1.", "list_citation_info": ["De et al. (2022) De, S., Berrada, L., Hayes, J., Smith, S. L., and Balle, B. Unlocking high-accuracy differentially private image classification through scale, 2022. URL https://arxiv.org/abs/2204.13650.", "Papernot et al. (2020) Papernot, N., Thakurta, A., Song, S., Chien, S., and Erlingsson, U. Tempered sigmoid activations for deep learning with differential privacy. arXiv preprint arXiv:2007.14191, pp. 10, 2020.", "Klause et al. (2022) Klause, H., Ziller, A., Rueckert, D., Hammernik, K., and Kaissis, G. Differentially private training of residual networks with scale normalisation. arXiv preprint arXiv:2203.00324, 2022.", "D\u00f6rmann et al. (2021) D\u00f6rmann, F., Frisk, O., Andersen, L. N., and Pedersen, C. F. Not all noise is accounted equally: How differentially private learning benefits from large sampling rates. In 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP), pp. 1\u20136. IEEE, 2021."]}], "citation_info_to_title": {"Papernot et al. (2020) Papernot, N., Thakurta, A., Song, S., Chien, S., and Erlingsson, U. Tempered sigmoid activations for deep learning with differential privacy. arXiv preprint arXiv:2007.14191, pp. 10, 2020.": "Tempered sigmoid activations for deep learning with differential privacy", "D\u00f6rmann et al. (2021) D\u00f6rmann, F., Frisk, O., Andersen, L. N., and Pedersen, C. F. Not all noise is accounted equally: How differentially private learning benefits from large sampling rates. In 2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP), pp. 1\u20136. IEEE, 2021.": "Not all noise is accounted equally: How differentially private learning benefits from large sampling rates", "De et al. (2022) De, S., Berrada, L., Hayes, J., Smith, S. L., and Balle, B. Unlocking high-accuracy differentially private image classification through scale, 2022. URL https://arxiv.org/abs/2204.13650.": "Unlocking high-accuracy differentially private image classification through scale", "Klause et al. (2022) Klause, H., Ziller, A., Rueckert, D., Hammernik, K., and Kaissis, G. Differentially private training of residual networks with scale normalisation. arXiv preprint arXiv:2203.00324, 2022.": "Differentially private training of residual networks with scale normalisation"}, "source_title_to_arxiv_id": {"Unlocking high-accuracy differentially private image classification through scale": "2204.13650"}}