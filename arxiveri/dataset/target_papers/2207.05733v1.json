{"title": "A Skeleton-aware Graph Convolutional Network for Human-Object Interaction Detection", "abstract": "Detecting human-object interactions is essential for comprehensive\nunderstanding of visual scenes. In particular, spatial connections between\nhumans and objects are important cues for reasoning interactions. To this end,\nwe propose a skeleton-aware graph convolutional network for human-object\ninteraction detection, named SGCN4HOI. Our network exploits the spatial\nconnections between human keypoints and object keypoints to capture their\nfine-grained structural interactions via graph convolutions. It fuses such\ngeometric features with visual features and spatial configuration features\nobtained from human-object pairs. Furthermore, to better preserve the object\nstructural information and facilitate human-object interaction detection, we\npropose a novel skeleton-based object keypoints representation. The performance\nof SGCN4HOI is evaluated in the public benchmark V-COCO dataset. Experimental\nresults show that the proposed approach outperforms the state-of-the-art\npose-based models and achieves competitive performance against other models.", "authors": ["Manli Zhu", "Edmond S. L. Ho", "Hubert P. H. Shum"], "published_date": "2022_07_11", "pdf_url": "http://arxiv.org/pdf/2207.05733v1", "list_table_and_caption": [{"table": "<table><tr><td>Pose-based Method</td><td>Backbone</td><td>AP_{role}^{\\#1}</td><td>AP_{role}^{\\#2}</td></tr><tr><td>RPNN [31]</td><td>ResNet-50</td><td>-</td><td>47.5</td></tr><tr><td>TIN{}^{*} [14]</td><td>ResNet-50</td><td>48.7</td><td>-</td></tr><tr><td>PastaNet [32]</td><td>ResNet-50</td><td>51.0</td><td>57.5</td></tr><tr><td>PMFNet [15]</td><td>ResNet-50</td><td>52.0</td><td>-</td></tr><tr><td>PD-Net [23]</td><td>ResNet-152</td><td>52.0</td><td>-</td></tr><tr><td>ACP{}^{*} [33]</td><td>ResNet-152</td><td>53.0</td><td>-</td></tr><tr><td>FCMNet [34]</td><td>ResNet-50</td><td>53.1</td><td>-</td></tr><tr><td>SIGN [17]</td><td>ResNet50-FPN</td><td>53.1</td><td>-</td></tr><tr><td>SGCN4HOI</td><td>ResNet-152</td><td>53.1</td><td>57.9</td></tr></table>", "caption": "TABLE I: Comparison of performance with pose-based methods. AP_{role}^{\\#1} and AP_{role}^{\\#2} represent the performance under Scenario1 and Scenario2 in V-COCO respectively.", "list_citation_info": ["[32] Y.-L. Li, L. Xu, X. Liu, X. Huang, Y. Xu, S. Wang, H.-S. Fang, Z. Ma, M. Chen, and C. Lu, \u201cPastanet: Toward human activity knowledge engine,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 379\u2013388.", "[14] Y.-L. Li, S. Zhou, X. Huang, L. Xu, Z. Ma, H.-S. Fang, Y. Wang, and C. Lu, \u201cTransferable interactiveness knowledge for human-object interaction detection,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 3580\u20133589.", "[17] S. Zheng, S. Chen, and Q. Jin, \u201cSkeleton-based interactive graph network for human object interaction detection,\u201d in 2020 IEEE International Conference on Multimedia and Expo (ICME), 2020, pp. 1\u20136.", "[15] B. Wan, D. Zhou, Y. Liu, R. Li, and X. He, \u201cPose-aware multi-level feature network for human object interaction detection,\u201d in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 9468\u20139477.", "[23] X. Zhong, C. Ding, X. Qu, and D. Tao, \u201cPolysemy deciphering network for human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 69\u201385.", "[31] P. Zhou and M. Chi, \u201cRelation parsing neural network for human-object interaction detection,\u201d in 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019. IEEE, 2019, pp. 843\u2013851.", "[34] Y. Liu, Q. Chen, and A. Zisserman, \u201cAmplifying key cues for human-object-interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 248\u2013265.", "[33] D.-J. Kim, X. Sun, J. Choi, S. Lin, and I. S. Kweon, \u201cDetecting human-object interactions with action co-occurrence priors,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 718\u2013736."]}, {"table": "<table><tr><td>Other Method</td><td>Backbone</td><td>AP_{role}^{\\#1}</td><td>AP_{role}^{\\#2}</td></tr><tr><td>VSRL [6]</td><td>ResNet-50</td><td>31.8</td><td>-</td></tr><tr><td>InteractNet [28]</td><td>ResNet-50</td><td>40.0</td><td>48.0</td></tr><tr><td>iCAN [12]</td><td>ResNet-50</td><td>45.3</td><td>52.4</td></tr><tr><td>VCL [35]</td><td>ResNet-101</td><td>48.3</td><td>-</td></tr><tr><td>UnionDet [36]</td><td>ResNet-50</td><td>47.5</td><td>56.2</td></tr><tr><td>IPNet [7]</td><td>Hourglass-104</td><td>51.0</td><td>-</td></tr><tr><td>DRG [37]</td><td>ResNet-50</td><td>51.0</td><td>-</td></tr><tr><td>VSGNet [11]</td><td>ResNet-152</td><td>51.8</td><td>57.0</td></tr><tr><td>HOI Transformer [3]</td><td>ResNet-101</td><td>52.9</td><td>-</td></tr><tr><td>HOTR [4] (transformer method)</td><td>ResNet-50</td><td>55.2</td><td>64.4</td></tr><tr><td>SGCN4HOI</td><td>ResNet-152</td><td>53.1</td><td>57.9</td></tr></table>", "caption": "TABLE II: Comparison of performance with other methods. Our model achieves a good trade-off between performance and computational complexity comparing with transformer-based models.", "list_citation_info": ["[35] Z. Hou, X. Peng, Y. Qiao, and D. Tao, \u201cVisual compositional learning for human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 584\u2013600.", "[12] C. Gao, Y. Zou, and J.-B. Huang, \u201cican: Instance-centric attention network for human-object interaction detection,\u201d arXiv preprint arXiv:1808.10437, 2018.", "[4] B. Kim, J. Lee, J. Kang, E.-S. Kim, and H. J. Kim, \u201cHotr: End-to-end human-object interaction detection with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 74\u201383.", "[7] T. Wang, T. Yang, M. Danelljan, F. S. Khan, X. Zhang, and J. Sun, \u201cLearning human-object interaction detection using interaction points,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 4115\u20134124.", "[11] O. Ulutan, A. S. M. Iftekhar, and B. S. Manjunath, \u201cVsgnet: Spatial attention network for detecting human object interactions using graph convolutions,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 13\u2009614\u201313\u2009623.", "[28] G. Gkioxari, R. Girshick, P. Doll\u00e1r, and K. He, \u201cDetecting and recognizing human-object interactions,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 8359\u20138367.", "[37] C. Gao, J. Xu, Y. Zou, and J.-B. Huang, \u201cDrg: Dual relation graph for human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 696\u2013712.", "[3] C. Zou, B. Wang, Y. Hu, J. Liu, Q. Wu, Y. Zhao, B. Li, C. Zhang, C. Zhang, Y. Wei, and J. Sun, \u201cEnd-to-end human object interaction detection with hoi transformer,\u201d in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 11\u2009820\u201311\u2009829.", "[6] S. Gupta and J. Malik, \u201cVisual semantic role labeling,\u201d arXiv preprint arXiv:1505.04474, 2015.", "[36] B. Kim, T. Choi, J. Kang, and H. J. Kim, \u201cUniondet: Union-level detector towards real-time human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 498\u2013514."]}, {"table": "<table><tr><td>HOI Class</td><td>InteractNet [28]</td><td>iCAN [12]</td><td>VSGNet [11]</td><td>SGCN4HOI</td></tr><tr><td>hold-obj</td><td>26.38</td><td>29.06</td><td>48.27</td><td>51.55</td></tr><tr><td>sit-instr</td><td>19.88</td><td>26.04</td><td>29.9</td><td>29.55</td></tr><tr><td>ride-instr</td><td>55.23</td><td>61.9</td><td>70.84</td><td>68.9</td></tr><tr><td>look-obj</td><td>20.2</td><td>26.49</td><td>42.78</td><td>47.56</td></tr><tr><td>hit-instr</td><td>62.32</td><td>74.11</td><td>76.08</td><td>78.63</td></tr><tr><td>hit-obj</td><td>43.32</td><td>46.13</td><td>48.6</td><td>50.62</td></tr><tr><td>eat-obj</td><td>32.37</td><td>37.73</td><td>38.3</td><td>43.63</td></tr><tr><td>eat-instr</td><td>1.97</td><td>8.26</td><td>6.3</td><td>3.18</td></tr><tr><td>jump-instr</td><td>45.14</td><td>51.45</td><td>52.66</td><td>55.14</td></tr><tr><td>lay-instr</td><td>20.99</td><td>22.4</td><td>21.66</td><td>20.86</td></tr><tr><td>talk_on_phone</td><td>31.77</td><td>52.82</td><td>62.23</td><td>63.55</td></tr><tr><td>carry-obj</td><td>33.11</td><td>32.02</td><td>39.09</td><td>38.09</td></tr><tr><td>throw-obj</td><td>40.44</td><td>40.62</td><td>45.12</td><td>53.09</td></tr><tr><td>catch-obj</td><td>42.52</td><td>47.61</td><td>44.84</td><td>46.87</td></tr><tr><td>cut-instr</td><td>22.97</td><td>37.18</td><td>46.78</td><td>47.81</td></tr><tr><td>cut-obj</td><td>36.4</td><td>34.76</td><td>36.58</td><td>37.64</td></tr><tr><td>work_on_comp</td><td>57.26</td><td>56.29</td><td>64.6</td><td>69.11</td></tr><tr><td>ski-instr</td><td>36.47</td><td>41.46</td><td>50.59</td><td>47.74</td></tr><tr><td>surf-instr</td><td>65.59</td><td>77.15</td><td>82.22</td><td>82.81</td></tr><tr><td>skateboard-instr</td><td>75.51</td><td>79.35</td><td>87.8</td><td>89.32</td></tr><tr><td>drink-instr</td><td>33.81</td><td>32.19</td><td>54.41</td><td>47.99</td></tr><tr><td>kick-obj</td><td>69.44</td><td>66.89</td><td>69.85</td><td>77.37</td></tr><tr><td>read-obj</td><td>23.85</td><td>30.74</td><td>42.83</td><td>41.57</td></tr><tr><td>snowboard-instr</td><td>63.85</td><td>74.35</td><td>79.9</td><td>81.35</td></tr><tr><td>Average</td><td>40.0</td><td>45.3</td><td>51.8</td><td>53.1</td></tr></table>", "caption": "TABLE III: Per class mAP comparisons with the existing methods in Scenario 1.", "list_citation_info": ["[12] C. Gao, Y. Zou, and J.-B. Huang, \u201cican: Instance-centric attention network for human-object interaction detection,\u201d arXiv preprint arXiv:1808.10437, 2018.", "[28] G. Gkioxari, R. Girshick, P. Doll\u00e1r, and K. He, \u201cDetecting and recognizing human-object interactions,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 8359\u20138367.", "[11] O. Ulutan, A. S. M. Iftekhar, and B. S. Manjunath, \u201cVsgnet: Spatial attention network for detecting human object interactions using graph convolutions,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 13\u2009614\u201313\u2009623."]}], "citation_info_to_title": {"[33] D.-J. Kim, X. Sun, J. Choi, S. Lin, and I. S. Kweon, \u201cDetecting human-object interactions with action co-occurrence priors,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 718\u2013736.": "Detecting human-object interactions with action co-occurrence priors", "[7] T. Wang, T. Yang, M. Danelljan, F. S. Khan, X. Zhang, and J. Sun, \u201cLearning human-object interaction detection using interaction points,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 4115\u20134124.": "Learning human-object interaction detection using interaction points", "[37] C. Gao, J. Xu, Y. Zou, and J.-B. Huang, \u201cDrg: Dual relation graph for human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 696\u2013712.": "Drg: Dual relation graph for human-object interaction detection", "[23] X. Zhong, C. Ding, X. Qu, and D. Tao, \u201cPolysemy deciphering network for human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 69\u201385.": "Polysemy deciphering network for human-object interaction detection", "[35] Z. Hou, X. Peng, Y. Qiao, and D. Tao, \u201cVisual compositional learning for human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 584\u2013600.": "Visual Compositional Learning for Human-Object Interaction Detection", "[28] G. Gkioxari, R. Girshick, P. Doll\u00e1r, and K. He, \u201cDetecting and recognizing human-object interactions,\u201d in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 8359\u20138367.": "Detecting and recognizing human-object interactions", "[6] S. Gupta and J. Malik, \u201cVisual semantic role labeling,\u201d arXiv preprint arXiv:1505.04474, 2015.": "Visual semantic role labeling", "[32] Y.-L. Li, L. Xu, X. Liu, X. Huang, Y. Xu, S. Wang, H.-S. Fang, Z. Ma, M. Chen, and C. Lu, \u201cPastanet: Toward human activity knowledge engine,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 379\u2013388.": "Pastanet: Toward human activity knowledge engine", "[36] B. Kim, T. Choi, J. Kang, and H. J. Kim, \u201cUniondet: Union-level detector towards real-time human-object interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 498\u2013514.": "Uniondet: Union-level detector towards real-time human-object interaction detection", "[12] C. Gao, Y. Zou, and J.-B. Huang, \u201cican: Instance-centric attention network for human-object interaction detection,\u201d arXiv preprint arXiv:1808.10437, 2018.": "ICAN: Instance-centric Attention Network for Human-Object Interaction Detection", "[17] S. Zheng, S. Chen, and Q. Jin, \u201cSkeleton-based interactive graph network for human object interaction detection,\u201d in 2020 IEEE International Conference on Multimedia and Expo (ICME), 2020, pp. 1\u20136.": "Skeleton-based interactive graph network for human object interaction detection", "[31] P. Zhou and M. Chi, \u201cRelation parsing neural network for human-object interaction detection,\u201d in 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019. IEEE, 2019, pp. 843\u2013851.": "Relation parsing neural network for human-object interaction detection", "[4] B. Kim, J. Lee, J. Kang, E.-S. Kim, and H. J. Kim, \u201cHotr: End-to-end human-object interaction detection with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 74\u201383.": "Hotr: End-to-end human-object interaction detection with transformers", "[11] O. Ulutan, A. S. M. Iftekhar, and B. S. Manjunath, \u201cVsgnet: Spatial attention network for detecting human object interactions using graph convolutions,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 13\u2009614\u201313\u2009623.": "Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions", "[15] B. Wan, D. Zhou, Y. Liu, R. Li, and X. He, \u201cPose-aware multi-level feature network for human object interaction detection,\u201d in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 9468\u20139477.": "Pose-aware multi-level feature network for human object interaction detection", "[14] Y.-L. Li, S. Zhou, X. Huang, L. Xu, Z. Ma, H.-S. Fang, Y. Wang, and C. Lu, \u201cTransferable interactiveness knowledge for human-object interaction detection,\u201d in 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 3580\u20133589.": "Transferable interactiveness knowledge for human-object interaction detection", "[3] C. Zou, B. Wang, Y. Hu, J. Liu, Q. Wu, Y. Zhao, B. Li, C. Zhang, C. Zhang, Y. Wei, and J. Sun, \u201cEnd-to-end human object interaction detection with hoi transformer,\u201d in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 11\u2009820\u201311\u2009829.": "End-to-end human object interaction detection with hoi transformer", "[34] Y. Liu, Q. Chen, and A. Zisserman, \u201cAmplifying key cues for human-object-interaction detection,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 248\u2013265.": "Amplifying key cues for human-object-interaction detection"}, "source_title_to_arxiv_id": {"Hotr: End-to-end human-object interaction detection with transformers": "2104.13682"}}