{"title": "SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer", "abstract": "Clinically, the accurate annotation of lesions/tissues can significantly\nfacilitate the disease diagnosis. For example, the segmentation of optic\ndisc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the\nsegmentation of skin lesions on dermoscopic images is helpful to the melanoma\ndiagnosis, etc. With the advancement of deep learning techniques, a wide range\nof methods proved the lesions/tissues segmentation can also facilitate the\nautomated disease diagnosis models. However, existing methods are limited in\nthe sense that they can only capture static regional correlations in the\nimages. Inspired by the global and dynamic nature of Vision Transformer, in\nthis paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans)\nto transfer the segmentation knowledge to the disease diagnosis network.\nSpecifically, we first propose an asymmetric multi-scale interaction strategy\nto correlate each single low-level diagnosis feature with multi-scale\nsegmentation features. Then, an effective strategy called SeA-block is adopted\nto vitalize diagnosis feature via correlated segmentation features. To model\nthe segmentation-diagnosis interaction, SeA-block first embeds the diagnosis\nfeature based on the segmentation information via the encoder, and then\ntransfers the embedding back to the diagnosis feature space by a decoder.\nExperimental results demonstrate that SeATrans surpasses a wide range of\nstate-of-the-art (SOTA) segmentation-assisted diagnosis methods on several\ndisease diagnosis tasks.", "authors": ["Junde Wu", "Huihui Fang", "Fangxin Shang", "Dalu Yang", "Zhaowei Wang", "Jing Gao", "Yehui Yang", "Yanwu Xu"], "published_date": "2022_06_12", "pdf_url": "http://arxiv.org/pdf/2206.05763v2", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th></th><th colspan=\"4\">Glaucoma</th><th colspan=\"4\">Thyroid Cancer</th><th colspan=\"4\">Melanoma</th></tr><tr><th></th><th></th><th>ACC</th><th>SPE</th><th>SEN</th><th>AUC</th><th>ACC</th><th>SPE</th><th>SEN</th><th>AUC</th><th>ACC</th><th>SPE</th><th>SEN</th><th>AUC</th></tr></thead><tbody><tr><th rowspan=\"10\">-homo</th><th>ConViT [6]</th><td>80.45</td><td>86.56</td><td>55.69</td><td>82.87</td><td>80.85</td><td>90.31</td><td>64.67</td><td>81.02</td><td>79.89</td><td>90.87</td><td>34.72</td><td>77.46</td></tr><tr><th>Swin [20]</th><td>81.95</td><td>91.56</td><td>43.03</td><td>82.32</td><td>82.76</td><td>85.70</td><td>73.82</td><td>80.34</td><td>80.76</td><td>89.11</td><td>47.29</td><td>76.75</td></tr><tr><th>DualStage [4]</th><td>80.20</td><td>90.31</td><td>39.24</td><td>80.37</td><td>79.56</td><td>85.64</td><td>70.93</td><td>77.15</td><td>78.26</td><td>87.84</td><td>38.89</td><td>72.34</td></tr><tr><th>DENet [10]</th><td>80.04</td><td>85.00</td><td>59.49</td><td>84.70</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AGCNN [19]</th><td>81.20</td><td>89.68</td><td>41.77</td><td>82.16</td><td>84.78</td><td>88.69</td><td>71.05</td><td>82.85</td><td>82.60</td><td>92.20</td><td>43.83</td><td>80.17</td></tr><tr><th>ColNet [36]</th><td>79.69</td><td>79.69</td><td>79.74</td><td>85.36</td><td>87.60</td><td>94.08</td><td>72.47</td><td>84.43</td><td>85.21</td><td>98.31</td><td>30.98</td><td>80.72</td></tr><tr><th>MagNet [14]</th><td>83.20</td><td>94.06</td><td>39.24</td><td>77.52</td><td>78.91</td><td>86.71</td><td>69.25</td><td>75.68</td><td>75.54</td><td>81.08</td><td>52.77</td><td>71.77</td></tr><tr><th>CMSNET [37]</th><td>64.16</td><td>73.41</td><td>61.85</td><td>80.86</td><td>74.52</td><td>87.03</td><td>67.32</td><td>76.71</td><td>82.88</td><td>98.32</td><td>17.14</td><td>78.55</td></tr><tr><th>L2T-KT [32]</th><td>80.20</td><td>80.62</td><td>78.48</td><td>86.24</td><td>81.49</td><td>90.31</td><td>75.59</td><td>84.29</td><td>79.34</td><td>84.69</td><td>58.10</td><td>81.90</td></tr><tr><th>SeATrans</th><td>86.96</td><td>90.93</td><td>70.88</td><td>88.47</td><td>85.54</td><td>91.75</td><td>78.84</td><td>86.84</td><td>85.59</td><td>93.77</td><td>62.74</td><td>84.56</td></tr><tr><th rowspan=\"10\">-hetero</th><th>ConViT [6]</th><td>80.45</td><td>91.56</td><td>35.44</td><td>82.37</td><td>77.46</td><td>93.75</td><td>60.11</td><td>81.02</td><td>79.34</td><td>85.42</td><td>54.79</td><td>76.55</td></tr><tr><th>Swin [20]</th><td>72.18</td><td>83.54</td><td>69.37</td><td>81.85</td><td>80.34</td><td>83.45</td><td>76.18</td><td>80.34</td><td>79.89</td><td>87.11</td><td>50.68</td><td>75.28</td></tr><tr><th>DualStage [4]</th><td>75.18</td><td>67.08</td><td>77.18</td><td>80.22</td><td>80.70</td><td>88.69</td><td>75.71</td><td>77.15</td><td>81.79</td><td>92.56</td><td>37.50</td><td>72.63</td></tr><tr><th>DENet [10]</th><td>78.94</td><td>84.81</td><td>77.50</td><td>84.12</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>AGCNN [19]</th><td>66.16</td><td>62.81</td><td>79.74</td><td>80.94</td><td>82.29</td><td>93.41</td><td>70.58</td><td>82.85</td><td>76.08</td><td>78.98</td><td>64.38</td><td>77.38</td></tr><tr><th>ColNet [36]</th><td>61.40</td><td>97.46</td><td>52.50</td><td>82.78</td><td>84.93</td><td>91.19</td><td>73.82</td><td>84.43</td><td>82.33</td><td>93.91</td><td>34.72</td><td>77.95</td></tr><tr><th>MagNet [14]</th><td>70.42</td><td>70.31</td><td>70.88</td><td>75.44</td><td>78.15</td><td>87.74</td><td>77.36</td><td>75.68</td><td>79.34</td><td>90.50</td><td>33.33</td><td>69.67</td></tr><tr><th>CMSNET [37]</th><td>60.15</td><td>77.21</td><td>55.93</td><td>78.17</td><td>81.75</td><td>84.60</td><td>78.14</td><td>76.71</td><td>72.82</td><td>73.73</td><td>69.01</td><td>76.39</td></tr><tr><th>L2T-KT [32]</th><td>79.95</td><td>85.00</td><td>59.49</td><td>84.98</td><td>83.89</td><td>94.04</td><td>76.18</td><td>84.29</td><td>82.06</td><td>88.51</td><td>55.56</td><td>80.75</td></tr><tr><th>SeATrans</th><td>80.70</td><td>80.62</td><td>78.48</td><td>87.61</td><td>84.60</td><td>90.26</td><td>80.45</td><td>86.23</td><td>84.42</td><td>87.20</td><td>57.35</td><td>83.16</td></tr></tbody></table>", "caption": "Table 2: Comparing with SOTA segmentation-assisted diagnosis methods. Accuracy, specificity, sensitivity and AUC (%) are measured on three different diagnosis tasks.", "list_citation_info": ["[4] Bajwa, M.N., Malik, M.I., Siddiqui, S.A., Dengel, A., Shafait, F., Neumeier, W., Ahmed, S.: Two-stage framework for optic disc localization and glaucoma classification in retinal fundus images using deep learning. BMC medical informatics and decision making 19(1), 1\u201316 (2019)", "[36] Zhou, Y., He, X., Huang, L., Liu, L., Zhu, F., Cui, S., Shao, L.: Collaborative learning of semi-supervised segmentation and classification for medical images. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2079\u20132088 (2019)", "[19] Li, L., Xu, M., Wang, X., Jiang, L., Liu, H.: Attention based glaucoma detection: A large-scale database and cnn model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10571\u201310580 (2019)", "[37] Zhou, Y., Chen, H., Li, Y., Liu, Q., Xu, X., Wang, S., Yap, P.T., Shen, D.: Multi-task learning for segmentation and classification of tumors in 3d automated breast ultrasound images. Medical Image Analysis 70, 101918 (2021)", "[10] Fu, H., Cheng, J., Xu, Y., Zhang, C., Wong, D.W.K., Liu, J., Cao, X.: Disc-aware ensemble network for glaucoma screening from fundus image. IEEE transactions on medical imaging 37(11), 2493\u20132501 (2018)", "[20] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10012\u201310022 (2021)", "[6] d\u2019Ascoli, S., Touvron, H., Leavitt, M., Morcos, A., Biroli, G., Sagun, L.: Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697 (2021)", "[32] Wu, J., Yu, S., Chen, W., Ma, K., Fu, R., Liu, H., Di, X., Zheng, Y.: Leveraging undiagnosed data for glaucoma classification with teacher-student learning. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 731\u2013740. Springer (2020)", "[14] Gupta, S., Punn, N.S., Sonbhadra, S.K., Agarwal, S.: Mag-net: Multi-task attention guided network for brain tumor segmentation and classification. In: International Conference on Big Data Analytics. pp. 3\u201315. Springer (2021)"]}], "citation_info_to_title": {"[10] Fu, H., Cheng, J., Xu, Y., Zhang, C., Wong, D.W.K., Liu, J., Cao, X.: Disc-aware ensemble network for glaucoma screening from fundus image. IEEE transactions on medical imaging 37(11), 2493\u20132501 (2018)": "Disc-aware ensemble network for glaucoma screening from fundus image", "[19] Li, L., Xu, M., Wang, X., Jiang, L., Liu, H.: Attention based glaucoma detection: A large-scale database and cnn model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10571\u201310580 (2019)": "Attention based glaucoma detection: A large-scale database and cnn model", "[6] d\u2019Ascoli, S., Touvron, H., Leavitt, M., Morcos, A., Biroli, G., Sagun, L.: Convit: Improving vision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697 (2021)": "Convit: Improving vision transformers with soft convolutional inductive biases", "[4] Bajwa, M.N., Malik, M.I., Siddiqui, S.A., Dengel, A., Shafait, F., Neumeier, W., Ahmed, S.: Two-stage framework for optic disc localization and glaucoma classification in retinal fundus images using deep learning. BMC medical informatics and decision making 19(1), 1\u201316 (2019)": "Two-stage framework for optic disc localization and glaucoma classification in retinal fundus images using deep learning", "[37] Zhou, Y., Chen, H., Li, Y., Liu, Q., Xu, X., Wang, S., Yap, P.T., Shen, D.: Multi-task learning for segmentation and classification of tumors in 3d automated breast ultrasound images. Medical Image Analysis 70, 101918 (2021)": "Multi-task learning for segmentation and classification of tumors in 3D automated breast ultrasound images", "[14] Gupta, S., Punn, N.S., Sonbhadra, S.K., Agarwal, S.: Mag-net: Multi-task attention guided network for brain tumor segmentation and classification. In: International Conference on Big Data Analytics. pp. 3\u201315. Springer (2021)": "Mag-net: Multi-task attention guided network for brain tumor segmentation and classification", "[36] Zhou, Y., He, X., Huang, L., Liu, L., Zhu, F., Cui, S., Shao, L.: Collaborative learning of semi-supervised segmentation and classification for medical images. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2079\u20132088 (2019)": "Collaborative learning of semi-supervised segmentation and classification for medical images", "[20] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10012\u201310022 (2021)": "Swin transformer: Hierarchical vision transformer using shifted windows", "[32] Wu, J., Yu, S., Chen, W., Ma, K., Fu, R., Liu, H., Di, X., Zheng, Y.: Leveraging undiagnosed data for glaucoma classification with teacher-student learning. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 731\u2013740. Springer (2020)": "Leveraging undiagnosed data for glaucoma classification with teacher-student learning"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}