{"title": "TexPose: Neural Texture Learning for Self-Supervised 6D Object Pose Estimation", "abstract": "In this paper, we introduce neural texture learning for 6D object pose\nestimation from synthetic data and a few unlabelled real images. Our major\ncontribution is a novel learning scheme which removes the drawbacks of previous\nworks, namely the strong dependency on co-modalities or additional refinement.\nThese have been previously necessary to provide training signals for\nconvergence. We formulate such a scheme as two sub-optimisation problems on\ntexture learning and pose learning. We separately learn to predict realistic\ntexture of objects from real image collections and learn pose estimation from\npixel-perfect synthetic data. Combining these two capabilities allows then to\nsynthesise photorealistic novel views to supervise the pose estimator with\naccurate geometry. To alleviate pose noise and segmentation imperfection\npresent during the texture learning phase, we propose a surfel-based\nadversarial training loss together with texture regularisation from synthetic\ndata. We demonstrate that the proposed approach significantly outperforms the\nrecent state-of-the-art methods without ground-truth pose annotations and\ndemonstrates substantial generalisation improvements towards unseen scenes.\nRemarkably, our scheme improves the adopted pose estimators substantially even\nwhen initialised with much inferior performance.", "authors": ["Hanzhi Chen", "Fabian Manhardt", "Nassir Navab", "Benjamin Busam"], "published_date": "2022_12_25", "pdf_url": "http://arxiv.org/pdf/2212.12902v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Supervision</th><td colspan=\"5\">Syn</td><td colspan=\"3\">Syn + Real</td><td colspan=\"6\">Syn + Self</td></tr><tr><th colspan=\"15\">Supervision signals required in real domain</th></tr><tr><th>Depth</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>-</td></tr><tr><th>Pose Refiner</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td></tr><tr><th>Weak labels</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td></tr><tr><th>GT Pose labels</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th rowspan=\"2\">Methods</th><td>AAE</td><td>MHP</td><td>DPODv2</td><td>GDR-LB**</td><td>DeepIM\\dagger</td><td>DPODv2</td><td>GDR-UB</td><td>SO-Pose</td><td>Self6D</td><td>Sock et al.</td><td>DSC</td><td>Self6D++</td><td>Self6D++\\ddagger</td><td rowspan=\"2\">Ours</td></tr><tr><td>[45]</td><td>[29]</td><td>[42]</td><td>[48]</td><td>[26]</td><td>[42]</td><td>[48]</td><td>[10]</td><td>[47]</td><td>[44]</td><td>[54]</td><td>[46]</td><td>[46]</td></tr><tr><th>Ape</th><td>4.0</td><td>11.9</td><td>62.1</td><td>50.9</td><td>85.8</td><td>80.0</td><td>85.0</td><td>-</td><td>38.9</td><td>37.6</td><td>31.2</td><td>76.0</td><td>75.4</td><td>80.9</td></tr><tr><th>Benchvise</th><td>20.9</td><td>66.2</td><td>88.3</td><td>99.4</td><td>93.1</td><td>99.7</td><td>99.8</td><td>-</td><td>75.2</td><td>78.6</td><td>83.0</td><td>91.6</td><td>94.9</td><td>99</td></tr><tr><th>Camera</th><td>30.5</td><td>22.4</td><td>92.5</td><td>89.2</td><td>99.1</td><td>99.2</td><td>96.5</td><td>-</td><td>36.9</td><td>65.6</td><td>49.6</td><td>97.1</td><td>97.0</td><td>94.8</td></tr><tr><th>Can</th><td>35.9</td><td>59.8</td><td>96.6</td><td>97.2</td><td>99.8</td><td>99.6</td><td>99.3</td><td>-</td><td>65.6</td><td>65.6</td><td>56.5</td><td>99.8</td><td>99.5</td><td>99.7</td></tr><tr><th>Cat</th><td>17.9</td><td>26.9</td><td>86.1</td><td>79.9</td><td>98.7</td><td>95.1</td><td>93.0</td><td>-</td><td>57.9</td><td>52.5</td><td>57.9</td><td>85.6</td><td>86.6</td><td>92.6</td></tr><tr><th>Driller</th><td>24.0</td><td>44.6</td><td>90.1</td><td>98.7</td><td>100.0</td><td>98.9</td><td>100.0</td><td>-</td><td>67.0</td><td>48.8</td><td>73.7</td><td>98.8</td><td>98.9</td><td>97.4</td></tr><tr><th>Duck</th><td>4.9</td><td>8.3</td><td>54.8</td><td>24.6</td><td>61.9</td><td>79.5</td><td>65.3</td><td>-</td><td>19.6</td><td>35.1</td><td>31.3</td><td>56.5</td><td>68.3</td><td>83.4</td></tr><tr><th>Eggbox*</th><td>81.0</td><td>55.7</td><td>98.6</td><td>81.1</td><td>93.5</td><td>99.6</td><td>99.9</td><td>-</td><td>99.0</td><td>89.2</td><td>96.0</td><td>91.0</td><td>99.0</td><td>94.9</td></tr><tr><th>Glue*</th><td>45.5</td><td>54.6</td><td>95.4</td><td>81.2</td><td>93.3</td><td>99.8</td><td>98.1</td><td>-</td><td>94.1</td><td>64.5</td><td>63.4</td><td>92.2</td><td>96.1</td><td>93.4</td></tr><tr><th>Holep.</th><td>17.6</td><td>15.5</td><td>27.0</td><td>41.9</td><td>32.1</td><td>72.3</td><td>73.4</td><td>-</td><td>16.2</td><td>41.5</td><td>38.8</td><td>35.4</td><td>41.9</td><td>79.3</td></tr><tr><th>Iron</th><td>32.0</td><td>60.8</td><td>98.2</td><td>98.8</td><td>100.0</td><td>99.4</td><td>86.9</td><td>-</td><td>77.9</td><td>80.9</td><td>61.9</td><td>99.5</td><td>99.4</td><td>99.8</td></tr><tr><th>Lamp</th><td>60.5</td><td>-</td><td>91.0</td><td>98.9</td><td>99.1</td><td>96.3</td><td>99.6</td><td>-</td><td>68.2</td><td>70.7</td><td>64.7</td><td>97.4</td><td>98.9</td><td>98.3</td></tr><tr><th>Phone</th><td>33.8</td><td>34.4</td><td>74.3</td><td>64.3</td><td>94.8</td><td>96.8</td><td>86.3</td><td>-</td><td>50.1</td><td>60.5</td><td>54.4</td><td>91.8</td><td>94.3</td><td>78.9</td></tr><tr><th>Average</th><td>31.4</td><td>38.8</td><td>81.2</td><td>77.4</td><td>88.0</td><td>93.5</td><td>91.0</td><td>96.0</td><td>58.9</td><td>60.6</td><td>58.6</td><td>85.6</td><td>88.5</td><td>91.7</td></tr></tbody></table>", "caption": "Table 1: Evaluation results on LineMOD dataset. *: objects with symmetry. **: used as our pretrained pose estimator before self-supervision, consistent with Self6D++. \\dagger: re-implemented version from [46], used as supervision source for Self6D++. \\ddagger: variant with depth supervision", "list_citation_info": ["[45] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In Proceedings of the european conference on computer vision (ECCV), pages 699\u2013715, 2018.", "[48] Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji. Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 16611\u201316621. Computer Vision Foundation / IEEE, 2021.", "[29] Fabian Manhardt, Diego Martin Arroyo, Christian Rupprecht, Benjamin Busam, Tolga Birdal, Nassir Navab, and Federico Tombari. Explaining the ambiguity of object detection and 6d pose from visual data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6841\u20136850, 2019.", "[26] Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. Deepim: Deep iterative matching for 6d pose estimation. Int. J. Comput. Vis., 128(3):657\u2013678, 2020.", "[10] Yan Di, Fabian Manhardt, Gu Wang, Xiangyang Ji, Nassir Navab, and Federico Tombari. So-pose: Exploiting self-occlusion for direct 6d pose estimation. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 12376\u201312385. IEEE, 2021.", "[44] Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, and Tae-Kyun Kim. Introducing pose consistency and warp-alignment for self-supervised 6d object pose estimation in color images. In Vitomir Struc and Francisco G\u00f3mez Fern\u00e1ndez, editors, 8th International Conference on 3D Vision, 3DV 2020, Virtual Event, Japan, November 25-28, 2020, pages 291\u2013300. IEEE, 2020.", "[54] Zongxin Yang, Xin Yu, and Yi Yang. Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 3907\u20133916. Computer Vision Foundation / IEEE, 2021.", "[47] Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Federico Tombari. Self6d: Self-supervised monocular 6d object pose estimation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of Lecture Notes in Computer Science, pages 108\u2013125. Springer, 2020.", "[42] Ivan Shugurov, Sergey Zakharov, and Slobodan Ilic. Dpodv2: Dense correspondence-based 6 dof pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[46] Gu Wang, Fabian Manhardt, Xingyu Liu, Xiangyang Ji, and Federico Tombari. Occlusion-aware self-supervised monocular 6D object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021."]}, {"table": "<table><thead><tr><th>Supervision</th><th>Syn</th><th colspan=\"6\">Syn + Self</th></tr><tr><th rowspan=\"2\">Methods</th><th>GDR**</th><th>Self6D</th><th>Sock et al.</th><th>DSC</th><th>Self6D++</th><th>Self6D++\\ddagger</th><th rowspan=\"2\">Ours</th></tr><tr><th>[48]</th><th>[47]</th><th>[44]</th><th>[54]</th><th>[46]</th><th>[46]</th></tr></thead><tbody><tr><th>Ape</th><th>44.0</th><td>13.7</td><td>12.0</td><td>9.1</td><td>57.7</td><td>59.4</td><td>60.5</td></tr><tr><th>Can</th><th>83.9</th><td>43.2</td><td>27.5</td><td>21.1</td><td>95.0</td><td>96.5</td><td>93.4</td></tr><tr><th>Cat</th><th>49.1</th><td>18.7</td><td>12.0</td><td>26.0</td><td>52.6</td><td>60.8</td><td>56.1</td></tr><tr><th>Driller</th><th>88.5</th><td>32.5</td><td>20.5</td><td>33.5</td><td>90.5</td><td>92.0</td><td>92.5</td></tr><tr><th>Duck</th><th>15.0</th><td>14.4</td><td>23.0</td><td>12.2</td><td>26.7</td><td>30.6</td><td>55.5</td></tr><tr><th>Eggbox*</th><th>33.9</th><td>57.8</td><td>25.1</td><td>39.4</td><td>45.0</td><td>51.1</td><td>46.0</td></tr><tr><th>Glue*</th><th>75.0</th><td>54.3</td><td>27.0</td><td>37.0</td><td>87.1</td><td>88.6</td><td>82.8</td></tr><tr><th>Holep.</th><th>34.0</th><td>22.0</td><td>35.0</td><td>20.4</td><td>23.5</td><td>38.5</td><td>46.5</td></tr><tr><th>Average</th><th>52.9</th><td>32.1</td><td>22.8</td><td>24.8</td><td>59.8</td><td>64.7</td><td>66.7</td></tr></tbody></table>", "caption": "Table 2: Evaluation results on Occluded LineMOD dataset. *: objects with symmetry. **: used as our pretrained pose estimator before self-supervision, consistent with Self6D++. \\dagger:re-implemented version from [46], used as supervision source for Self6D++. \\ddagger: variant with depth supervision.", "list_citation_info": ["[48] Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji. Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 16611\u201316621. Computer Vision Foundation / IEEE, 2021.", "[46] Gu Wang, Fabian Manhardt, Xingyu Liu, Xiangyang Ji, and Federico Tombari. Occlusion-aware self-supervised monocular 6D object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.", "[54] Zongxin Yang, Xin Yu, and Yi Yang. Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 3907\u20133916. Computer Vision Foundation / IEEE, 2021.", "[47] Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Federico Tombari. Self6d: Self-supervised monocular 6d object pose estimation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of Lecture Notes in Computer Science, pages 108\u2013125. Springer, 2020.", "[44] Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, and Tae-Kyun Kim. Introducing pose consistency and warp-alignment for self-supervised 6d object pose estimation in color images. In Vitomir Struc and Francisco G\u00f3mez Fern\u00e1ndez, editors, 8th International Conference on 3D Vision, 3DV 2020, Virtual Event, Japan, November 25-28, 2020, pages 291\u2013300. IEEE, 2020."]}, {"table": "<table><thead><tr><th>Supervision</th><th>Syn</th><th colspan=\"4\">Syn + Self</th></tr><tr><th>Method</th><th>GDR** \\dagger [48]</th><th>Sock et al. [44]</th><th>Self6D++ \\dagger [46]</th><th>Ours \\dagger</th><th>Ours \\ddagger</th></tr></thead><tbody><tr><th>Benchvise</th><th>88.8</th><td>57.3</td><td>75.7</td><td>93.1</td><td>92.9</td></tr><tr><th>Driller</th><th>92.8</th><td>46.6</td><td>89.4</td><td>94.8</td><td>94.2</td></tr><tr><th>Phone</th><th>78.7</th><td>41.5</td><td>76.8</td><td>79.3</td><td>81.2</td></tr><tr><th>Average</th><th>86.8</th><td>52.0</td><td>80.6</td><td>89.1</td><td>89.4</td></tr></tbody></table>", "caption": "Table 3: Evaluation results on HomebrewedDB dataset. **: used as our pretrained pose estimator before self-supervision, consistent with Self6D++. \\dagger: Pre-process testing images with the same intrinsics as LineMOD, different from [46] as they directly feed raw images without scaling. \\ddagger: Use raw images for re-adaptation.", "list_citation_info": ["[44] Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, and Tae-Kyun Kim. Introducing pose consistency and warp-alignment for self-supervised 6d object pose estimation in color images. In Vitomir Struc and Francisco G\u00f3mez Fern\u00e1ndez, editors, 8th International Conference on 3D Vision, 3DV 2020, Virtual Event, Japan, November 25-28, 2020, pages 291\u2013300. IEEE, 2020.", "[48] Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji. Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 16611\u201316621. Computer Vision Foundation / IEEE, 2021.", "[46] Gu Wang, Fabian Manhardt, Xingyu Liu, Xiangyang Ji, and Federico Tombari. Occlusion-aware self-supervised monocular 6D object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021."]}], "citation_info_to_title": {"[26] Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. Deepim: Deep iterative matching for 6d pose estimation. Int. J. Comput. Vis., 128(3):657\u2013678, 2020.": "Deepim: Deep Iterative Matching for 6D Pose Estimation", "[42] Ivan Shugurov, Sergey Zakharov, and Slobodan Ilic. Dpodv2: Dense correspondence-based 6 dof pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.": "Dpodv2: Dense correspondence-based 6 dof pose estimation", "[10] Yan Di, Fabian Manhardt, Gu Wang, Xiangyang Ji, Nassir Navab, and Federico Tombari. So-pose: Exploiting self-occlusion for direct 6d pose estimation. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 12376\u201312385. IEEE, 2021.": "So-pose: Exploiting self-occlusion for direct 6d pose estimation", "[47] Gu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, and Federico Tombari. Self6d: Self-supervised monocular 6d object pose estimation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of Lecture Notes in Computer Science, pages 108\u2013125. Springer, 2020.": "Self6d: Self-supervised monocular 6d object pose estimation", "[45] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In Proceedings of the european conference on computer vision (ECCV), pages 699\u2013715, 2018.": "Implicit 3D Orientation Learning for 6D Object Detection from RGB Images", "[44] Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, and Tae-Kyun Kim. Introducing pose consistency and warp-alignment for self-supervised 6d object pose estimation in color images. In Vitomir Struc and Francisco G\u00f3mez Fern\u00e1ndez, editors, 8th International Conference on 3D Vision, 3DV 2020, Virtual Event, Japan, November 25-28, 2020, pages 291\u2013300. IEEE, 2020.": "Introducing pose consistency and warp-alignment for self-supervised 6d object pose estimation in color images", "[48] Gu Wang, Fabian Manhardt, Federico Tombari, and Xiangyang Ji. Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 16611\u201316621. Computer Vision Foundation / IEEE, 2021.": "Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation", "[54] Zongxin Yang, Xin Yu, and Yi Yang. Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 3907\u20133916. Computer Vision Foundation / IEEE, 2021.": "Dsc-posenet: Learning 6dof object pose estimation via dual-scale consistency", "[46] Gu Wang, Fabian Manhardt, Xingyu Liu, Xiangyang Ji, and Federico Tombari. Occlusion-aware self-supervised monocular 6D object pose estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.": "Occlusion-aware self-supervised monocular 6D object pose estimation", "[29] Fabian Manhardt, Diego Martin Arroyo, Christian Rupprecht, Benjamin Busam, Tolga Birdal, Nassir Navab, and Federico Tombari. Explaining the ambiguity of object detection and 6d pose from visual data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6841\u20136850, 2019.": "Explaining the ambiguity of object detection and 6d pose from visual data"}, "source_title_to_arxiv_id": {"Dpodv2: Dense correspondence-based 6 dof pose estimation": "2207.02805"}}