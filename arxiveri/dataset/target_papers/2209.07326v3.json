{"title": "A Continual Development Methodology for Large-scale Multitask Dynamic ML Systems", "abstract": "The traditional Machine Learning (ML) methodology requires to fragment the\ndevelopment and experimental process into disconnected iterations whose\nfeedback is used to guide design or tuning choices. This methodology has\nmultiple efficiency and scalability disadvantages, such as leading to spend\nsignificant resources into the creation of multiple trial models that do not\ncontribute to the final solution.The presented work is based on the intuition\nthat defining ML models as modular and extensible artefacts allows to introduce\na novel ML development methodology enabling the integration of multiple design\nand evaluation iterations into the continuous enrichment of a single unbounded\nintelligent system. We define a novel method for the generation of dynamic\nmultitask ML models as a sequence of extensions and generalizations. We first\nanalyze the capabilities of the proposed method by using the standard ML\nempirical evaluation methodology. Finally, we propose a novel continuous\ndevelopment methodology that allows to dynamically extend a pre-existing\nmultitask large-scale ML system while analyzing the properties of the proposed\nmethod extensions. This results in the generation of an ML model capable of\njointly solving 124 image classification tasks achieving state of the art\nquality with improved size and compute cost.", "authors": ["Andrea Gesmundo"], "published_date": "2022_09_15", "pdf_url": "http://arxiv.org/pdf/2209.07326v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td colspan=\"2\">Accuracy (%)</td><td colspan=\"3\">Splits</td><td></td></tr><tr><td>Name</td><td>Val.</td><td>Test</td><td>Train</td><td>Val.</td><td>Test</td><td>Reference</td></tr><tr><td colspan=\"7\">Task-set A</td></tr><tr><td>imagenet2012</td><td>78.54</td><td>86.66</td><td>train</td><td>imagenet_v2:test{}^{[1]}</td><td>val</td><td>(Russakovsky et al., 2015)</td></tr><tr><td>cifar100</td><td>96.88</td><td>94.94</td><td>train[:98%]</td><td>train[98%:]</td><td>test</td><td>(Krizhevsky, 2009)</td></tr><tr><td>cifar10</td><td>98.81</td><td>99.48</td><td>train</td><td>cifar10_1:test{}^{[2]}</td><td>test</td><td>(Krizhevsky, 2009)</td></tr><tr><td colspan=\"7\">VTAB-full benchmark{}^{[3][4]}</td></tr><tr><td>caltech101</td><td>98.67</td><td>95.94</td><td>train[:2754]</td><td>train[2754:]</td><td>test</td><td>(Fei-Fei et al., 2004)</td></tr><tr><td>dtd</td><td>82.63</td><td>82.23</td><td>train</td><td>val</td><td>test</td><td>(Cimpoi et al., 2014)</td></tr><tr><td>oxford_flowers102</td><td>99.79</td><td>99.48</td><td>train</td><td>val</td><td>test</td><td>(Nilsback &amp; Zisserman, 2008)</td></tr><tr><td>oxford_iiit_pet</td><td>98.09</td><td>95.50</td><td>train[:2944]</td><td>train[2944:]</td><td>test</td><td>(Parkhi et al., 2012)</td></tr><tr><td>sun397</td><td>84.71</td><td>84.32</td><td>train</td><td>val</td><td>test</td><td>(Xiao et al., 2010)</td></tr><tr><td>svhn_cropped</td><td>97.47</td><td>97.50</td><td>train[:65931]</td><td>train[65931:]</td><td>test</td><td>(Netzer et al., 2011)</td></tr><tr><td>patch_camelyon</td><td>92.82</td><td>91.14</td><td>train</td><td>val</td><td>test</td><td>(Veeling et al., 2018)</td></tr><tr><td>eurosat/rgb</td><td>99.27</td><td>99.22</td><td>train[:16200]</td><td>train[16200:21600]</td><td>train[21600:]</td><td>(Helber et al., 2019)</td></tr><tr><td>resisc45</td><td>97.80</td><td>97.21</td><td>train[:18900]</td><td>train[18900:25200]</td><td>train[25200:]</td><td>(Cheng et al., 2017)</td></tr><tr><td colspan=\"2\">diabetic_retinopathy_detection/\u2026</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>    btgraham-300</td><td>85.22</td><td>83.75</td><td>train</td><td>val</td><td>test</td><td>(Kaggle &amp; EyePacs, 2015)</td></tr><tr><td>clevr/count_cylinders{}^{[5]}</td><td>99.65</td><td>99.47</td><td>train[:63000]</td><td>train[63000:]</td><td>val</td><td>(Johnson et al., 2017)</td></tr><tr><td>clevr/count_all</td><td>99.97</td><td>99.90</td><td>train[:63000]</td><td>train[63000:]</td><td>val</td><td>(Johnson et al., 2017)</td></tr><tr><td>clevr/closest_object_distance</td><td>94.63</td><td>94.05</td><td>train[:63000]</td><td>train[63000:]</td><td>val</td><td>(Johnson et al., 2017)</td></tr><tr><td>dmlab</td><td>77.01</td><td>76.92</td><td>train</td><td>val</td><td>test</td><td>(Zhai et al., 2019)</td></tr><tr><td>dsprites/label_x_position</td><td>99.99</td><td>99.98</td><td>train[:589824]</td><td>train[589824:663552]</td><td>train[663552:]</td><td>(Klindt et al., 2021)</td></tr><tr><td>dsprites/label_orientation</td><td>96.78</td><td>96.44</td><td>train[:589824]</td><td>train[589824:663552]</td><td>train[663552:]</td><td>(Klindt et al., 2021)</td></tr><tr><td>kitti/closest_object_distance{}^{[5]}</td><td>83.57</td><td>78.48</td><td>train</td><td>val</td><td>test</td><td>(Geiger et al., 2012)</td></tr><tr><td>kitti/count_vehicles{}^{[5]}</td><td>92.14</td><td>76.93</td><td>train</td><td>val</td><td>test</td><td>(Geiger et al., 2012)</td></tr><tr><td>kitti/closest_vehicle_distance</td><td>89.76</td><td>82.28</td><td>train</td><td>val</td><td>test</td><td>(Geiger et al., 2012)</td></tr><tr><td>smallnorb/label_category{}^{[5]}</td><td>99.45</td><td>99.38</td><td>train</td><td>test[:50%]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td>smallnorb/label_lighting{}^{[5]}</td><td>99.93</td><td>99.89</td><td>train</td><td>test[:50%]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td>smallnorb/label_azimuth</td><td>35.40</td><td>34.44</td><td>train</td><td>test[:50%]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td>smallnorb/label_elevation</td><td>96.22</td><td>96.26</td><td>train</td><td>test[:50%]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td colspan=\"7\">Continues in Table 5 \u2026</td></tr></tbody></table>", "caption": "Table 4: Datasets details (part 1 of 3).For each dataset used in the experiments, this table reports:1) accuracy achieved on the test and validations sets by the large scale multitask model generated by the experiments described in Section 52) dataset name indicative of the Tensorflow Datasets Catalogs identification string and linking to the corresponding catalog page,3) train, validation and test data splits, represented with the standard Tensorflow Datasets format (\"validation\" has been abbreviated as \"val\").4) corresponding scientific publication reference.Datasets are listed in the order of introduction into the system.<br/>                                                                                                                        Notes:<br/>                                                                                                                        [1] The test split of the imagenet_v2 dataset is used as validation set for imagenet2012. <br/>                                                                                                                        [2] The test split of the cifar10_1 dataset is used as validation set for cifar10.<br/>                                                                                                                        [3] The VTAB-full benchmark also includes the cifar100 task. Cifar100 has been introduced to the system as part of the initial benchmark. In the VTAB-full results tables we refer to the top 1 test accuracy achieved in the latest cifar100 training iteration without retraining it as part of the VTAB-full active training iteration.<br/>                                                                                                                        [4] The definition for the VTAB standard and additional tasks has been sourced from https://github.com/google-research/task_adaptation/tree/master/task_adaptation/data.<br/>                                                                                                                        [5] VTAB additional task, not included in the standard scoring set. These tasks were added to further scale the system and analyze transfer across related tasks.", "list_citation_info": ["Zhai et al. (2019) Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark. ArXiv, abs/1910.04867, 2019.", "Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.", "LeCun et al. (2004) Yann LeCun, Fu Jie Huang, and L\u00e9on Bottou. Learning methods for generic object recognition with invariance to pose and lighting. Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., 2:II\u2013104 Vol.2, 2004.", "Klindt et al. (2021) David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan M. Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. ArXiv, abs/2007.10930, 2021.", "Veeling et al. (2018) Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. ArXiv, abs/1806.03962, 2018.", "Cheng et al. (2017) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105:1865\u20131883, 2017.", "Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1988\u20131997, 2017.", "Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3485\u20133492, 2010.", "Kaggle & EyePacs (2015) Kaggle and EyePacs. Kaggle diabetic retinopathy detection. https://www.kaggle.com/c/diabetic-retinopathy-detection/data, 2015.", "Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606\u20133613, 2014.", "Geiger et al. (2012) Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3354\u20133361, 2012.", "Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211\u2013252, 2015.", "Nilsback & Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722\u2013729, 2008.", "Fei-Fei et al. (2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. 2004.", "Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas R. Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12:2217\u20132226, 2019.", "Parkhi et al. (2012) Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3498\u20133505, 2012.", "Netzer et al. (2011) Yuval Netzer, Tao Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. 2011."]}, {"table": "<table><tbody><tr><td></td><td colspan=\"2\">Accuracy (%)</td><td colspan=\"3\">Splits</td><td></td></tr><tr><td>Name</td><td>Val.</td><td>Test</td><td>Train</td><td>Val.</td><td>Test</td><td>Reference</td></tr><tr><td colspan=\"7\">\u2026Continues from Table 4</td></tr><tr><td colspan=\"7\">Visual domain decathlon benchmark</td></tr><tr><td colspan=\"2\">visual_domain_decathlon/\u2026</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>    imagenet12</td><td>89.47</td><td>89.69</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    svhn</td><td>98.75</td><td>98.57</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    cifar100</td><td>97.79</td><td>98.00</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    gtsrb</td><td>100</td><td>99.95</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    daimlerpedcls</td><td>100</td><td>100</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    omniglot</td><td>88.70</td><td>87.99</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    ucf101</td><td>85.96</td><td>87.70</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    aircraft</td><td>72.95</td><td>70.01</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    dtd</td><td>73.11</td><td>75.00</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td>    vgg-flowers</td><td>99.37</td><td>99.41</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Bilen et al., 2017)</td></tr><tr><td colspan=\"7\">Multitask Character Classification Benchmark</td></tr><tr><td>emnist/digits</td><td>99.86</td><td>99.81</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Cohen et al., 2017)</td></tr><tr><td>emnist/letters</td><td>96.57</td><td>95.03</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Cohen et al., 2017)</td></tr><tr><td>kmnist</td><td>99.79</td><td>98.44</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Clanuwat et al., 2018)</td></tr><tr><td>mnist</td><td>99.83</td><td>99.72</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(LeCun et al., 1998)</td></tr><tr><td>omniglot</td><td>99.89</td><td>99.90</td><td>train</td><td>small1</td><td>small2</td><td>(Lake et al., 2015)</td></tr><tr><td>cmaterdb/bangla</td><td>99.89</td><td>99.00</td><td>train[20%:]</td><td>train[:20%]</td><td>test</td><td>(Das et al., 2012b; a)</td></tr><tr><td>cmaterdb/devanagari</td><td>100</td><td>98.20</td><td>train[20%:]</td><td>train[:20%]</td><td>test</td><td>(Das et al., 2012b; a)</td></tr><tr><td>cmaterdb/telugu</td><td>100</td><td>99.40</td><td>train[20%:]</td><td>train[:20%]</td><td>test</td><td>(Das et al., 2012b; a)</td></tr><tr><td colspan=\"7\">VTAB 1k benchmark{}^{[4]}</td></tr><tr><td>caltech101</td><td>98.97</td><td>89.88</td><td>train[:800]</td><td>train[2754:2954]</td><td>test</td><td>(Fei-Fei et al., 2004)</td></tr><tr><td>cifar100</td><td>96.92</td><td>92.85</td><td>train[:800]</td><td>train[45000:45200]</td><td>test</td><td>(Krizhevsky, 2009)</td></tr><tr><td>cifar10</td><td>100</td><td>99.23</td><td>train[:800]</td><td>train[45000:45200]</td><td>test</td><td>(Krizhevsky, 2009)</td></tr><tr><td>dtd</td><td>82.05</td><td>77.87</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Cimpoi et al., 2014)</td></tr><tr><td>oxford_flowers102</td><td>100</td><td>99.33</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Nilsback &amp; Zisserman, 2008)</td></tr><tr><td>oxford_iiit_pet</td><td>97.44</td><td>93.51</td><td>train[:800]</td><td>train[2944:3144]</td><td>test</td><td>(Parkhi et al., 2012)</td></tr><tr><td>sun397</td><td>66.15</td><td>60.93</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Xiao et al., 2010)</td></tr><tr><td>svhn_cropped</td><td>98.00</td><td>97.47</td><td>train[:800]</td><td>train[65931:66131]</td><td>test</td><td>(Netzer et al., 2011)</td></tr><tr><td>patch_camelyon</td><td>96.41</td><td>91.55</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Veeling et al., 2018)</td></tr><tr><td>eurosat/rgb</td><td>99.49</td><td>98.56</td><td>train[:800]</td><td>train[16200:16400]</td><td>train[21600:]</td><td>(Helber et al., 2019)</td></tr><tr><td>resisc45</td><td>97.50</td><td>95.33</td><td>train[:800]</td><td>train[18900:19100]</td><td>train[25200:]</td><td>(Cheng et al., 2017)</td></tr><tr><td colspan=\"2\">diabetic_retinopathy_detection/\u2026</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>    btgraham-300</td><td>88.50</td><td>82.77</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Kaggle &amp; EyePacs, 2015)</td></tr><tr><td>clevr/count_cylinders{}^{[5]}</td><td>99.49</td><td>99.01</td><td>train[:800]</td><td>train[63000:63200]</td><td>val</td><td>(Johnson et al., 2017)</td></tr><tr><td>clevr/count_all</td><td>100</td><td>99.88</td><td>train[:800]</td><td>train[63000:63200]</td><td>val</td><td>(Johnson et al., 2017)</td></tr><tr><td>clevr/closest_object_distance</td><td>92.00</td><td>90.64</td><td>train[:800]</td><td>train[63000:63200]</td><td>val</td><td>(Johnson et al., 2017)</td></tr><tr><td>dmlab</td><td>77.44</td><td>74.71</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Zhai et al., 2019)</td></tr><tr><td>dsprites/label_x_position</td><td>100</td><td>99.43</td><td>train[:800]</td><td>train[589824:590024]</td><td>train[663552:]</td><td>(Klindt et al., 2021)</td></tr><tr><td>dsprites/label_orientation</td><td>97.50</td><td>96.30</td><td>train[:800]</td><td>train[589824:590024]</td><td>train[663552:]</td><td>(Klindt et al., 2021)</td></tr><tr><td>kitti/closest_object_distance{}^{[5]}</td><td>84.50</td><td>78.34</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Geiger et al., 2012)</td></tr><tr><td>kitti/count_vehicles{}^{[5]}</td><td>93.85</td><td>69.34</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Geiger et al., 2012)</td></tr><tr><td>kitti/closest_vehicle_distance</td><td>88.00</td><td>82.14</td><td>train[:800]</td><td>val[:200]</td><td>test</td><td>(Geiger et al., 2012)</td></tr><tr><td>smallnorb/label_category{}^{[5]}</td><td>100</td><td>97.67</td><td>train[:800]</td><td>test[:200]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td>smallnorb/label_lighting{}^{[5]}</td><td>100</td><td>98.30</td><td>train[:800]</td><td>test[:200]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td>smallnorb/label_azimuth</td><td>37.00</td><td>33.75</td><td>train[:800]</td><td>test[:200]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td>smallnorb/label_elevation</td><td>92.50</td><td>92.83</td><td>train[:800]</td><td>test[:200]</td><td>test[50%:]</td><td>(LeCun et al., 2004)</td></tr><tr><td colspan=\"7\">Continues in Table 6 \u2026</td></tr></tbody></table>", "caption": "Table 5: Datasets details (part 2 of 3).", "list_citation_info": ["Das et al. (2012b) N. Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, and Dipak Kumar Basu. A genetic algorithm based region sampling for selection of local features in handwritten digit recognition application. Appl. Soft Comput., 12:1592\u20131606, 2012b.", "Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3485\u20133492, 2010.", "Cohen et al. (2017) Gregory Cohen, Saeed Afshar, Jonathan C. Tapson, and Andr\u00e9 van Schaik. Emnist: Extending mnist to handwritten letters. 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921\u20132926, 2017.", "Geiger et al. (2012) Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3354\u20133361, 2012.", "Nilsback & Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722\u2013729, 2008.", "Zhai et al. (2019) Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark. ArXiv, abs/1910.04867, 2019.", "Bilen et al. (2017) Hakan Bilen, Sylvestre Rebuffi, and Tomas Jakab. Visual domain decathlon. 2017.", "Klindt et al. (2021) David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan M. Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. ArXiv, abs/2007.10930, 2021.", "Veeling et al. (2018) Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. ArXiv, abs/1806.03962, 2018.", "Parkhi et al. (2012) Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3498\u20133505, 2012.", "Netzer et al. (2011) Yuval Netzer, Tao Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. 2011.", "Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.", "Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1988\u20131997, 2017.", "Kaggle & EyePacs (2015) Kaggle and EyePacs. Kaggle diabetic retinopathy detection. https://www.kaggle.com/c/diabetic-retinopathy-detection/data, 2015.", "LeCun et al. (2004) Yann LeCun, Fu Jie Huang, and L\u00e9on Bottou. Learning methods for generic object recognition with invariance to pose and lighting. Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., 2:II\u2013104 Vol.2, 2004.", "Lake et al. (2015) Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350:1332 \u2013 1338, 2015.", "Fei-Fei et al. (2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. 2004.", "Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas R. Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12:2217\u20132226, 2019.", "Cheng et al. (2017) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105:1865\u20131883, 2017.", "Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606\u20133613, 2014.", "Clanuwat et al. (2018) Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature. ArXiv, abs/1812.01718, 2018.", "LeCun et al. (1998) Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86:2278\u20132324, 1998."]}, {"table": "<table><tbody><tr><td></td><th colspan=\"2\">Accuracy (%)</th><th colspan=\"3\">Splits</th><td></td></tr><tr><th>Name</th><th>Val.</th><th>Test</th><th>Train</th><th>Val.</th><th>Test</th><th>Reference</th></tr><tr><th colspan=\"7\">\u2026Continues from Table 5</th></tr><tr><th colspan=\"7\">Task-set B</th></tr><tr><td>beans</td><td>100</td><td>94.53</td><td>train</td><td>val</td><td>test</td><td>(Makerere, 2020)</td></tr><tr><td>binary_alpha_digits</td><td>91.43</td><td>85.71</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>-</td></tr><tr><td>caltech_birds2010</td><td>98.00</td><td>89.48</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Welinder et al., 2010)</td></tr><tr><td>caltech_birds2011</td><td>93.00</td><td>90.94</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Welinder et al., 2010)</td></tr><tr><td>cars196</td><td>89.23</td><td>87.18</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Krause et al., 2013)</td></tr><tr><td>cassava</td><td>92.31</td><td>91.78</td><td>train</td><td>val</td><td>test</td><td>(Mwebaze et al., 2019)</td></tr><tr><td>cats_vs_dogs</td><td>100</td><td>99.83</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Elson et al., 2007)</td></tr><tr><td>citrus_leaves</td><td>100</td><td>93.33</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Rauf et al., 2019)</td></tr><tr><td>colorectal_histology</td><td>99.17</td><td>98.40</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Kather et al., 2016)</td></tr><tr><td colspan=\"2\">controlled_noisy_web_labels/\u2026</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>    mini_imagenet_red</td><td>96.35</td><td>95.04</td><td>train_00</td><td>val[:50%]</td><td>val[50%:]</td><td>(Jiang et al., 2020)</td></tr><tr><td>    mini_imagenet_blue</td><td>96.35</td><td>95.24</td><td>train_00</td><td>val[:50%]</td><td>val[50%:]</td><td>(Jiang et al., 2020)</td></tr><tr><td colspan=\"2\">curated_breast_imaging_ddsm/\u2026</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>    patches</td><td>70.66</td><td>67.49</td><td>train</td><td>val</td><td>test</td><td>(Clark et al., 2013)</td></tr><tr><td>cycle_gan/\u2026</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>    apple2orange</td><td>100</td><td>98.83</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    summer2winter</td><td>95.71</td><td>90.49</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    horse2zebra</td><td>100</td><td>99.23</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    monet2photo</td><td>100</td><td>100</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    cezanne2photo</td><td>100</td><td>100</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    ukiyoe2photo</td><td>100</td><td>99.70</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    vangogh2photo</td><td>100</td><td>100</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    maps</td><td>100</td><td>100</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    cityscapes</td><td>100</td><td>100</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    facades</td><td>100</td><td>100</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>    iphone2dslr_flower</td><td>98.99</td><td>94.66</td><td>trainA+B[10%:]</td><td>trainA+B[:10%]</td><td>testA+B</td><td>(Zhu et al., 2017)</td></tr><tr><td>deep_weeds</td><td>98.79</td><td>98.06</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Olsen et al., 2019)</td></tr><tr><td>domainnet/real</td><td>90.90</td><td>90.25</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Peng et al., 2019)</td></tr><tr><td>domainnet/painting</td><td>82.62</td><td>82.11</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Peng et al., 2019)</td></tr><tr><td>domainnet/clipart</td><td>87.16</td><td>85.35</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Peng et al., 2019)</td></tr><tr><td>domainnet/quickdraw</td><td>74.34</td><td>73.78</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Peng et al., 2019)</td></tr><tr><td>domainnet/infograph</td><td>56.28</td><td>55.28</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Peng et al., 2019)</td></tr><tr><td>domainnet/sketch</td><td>78.70</td><td>78.51</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Peng et al., 2019)</td></tr><tr><td>food101</td><td>94.58</td><td>91.47</td><td>train[5%:]</td><td>val</td><td>train[:5%]</td><td>(Bossard et al., 2014)</td></tr><tr><td>horses_or_humans</td><td>100</td><td>99.61</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Moroney, 2019a)</td></tr><tr><td>i_naturalist2017</td><td>77.27</td><td>77.71</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Horn et al., 2018)</td></tr><tr><td>i_naturalist2018</td><td>80.90</td><td>80.97</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Horn et al., 2018)</td></tr><tr><td>imagenet_a</td><td>87.78</td><td>84.53</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Hendrycks et al., 2021b)</td></tr><tr><td>imagenet_lt</td><td>87.26</td><td>82.50</td><td>train</td><td>val</td><td>test</td><td>(Liu et al., 2019c)</td></tr><tr><td>imagenet_r</td><td>91.13</td><td>89.87</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Hendrycks et al., 2021a)</td></tr><tr><td>imagenet_sketch</td><td>89.54</td><td>88.60</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Wang et al., 2019a)</td></tr><tr><td>imagenette</td><td>99.92</td><td>100</td><td>train[5%:]</td><td>val</td><td>train[:5%]</td><td>(Howard, 2019a)</td></tr><tr><td>imagewang</td><td>97.32</td><td>99.59</td><td>train[5%:]</td><td>val</td><td>train[:5%]</td><td>(Howard, 2019b)</td></tr><tr><td>malaria</td><td>98.17</td><td>97.46</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Rajaraman et al., 2018)</td></tr><tr><td>pet_finder</td><td>62.40</td><td>60.73</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>-</td></tr><tr><td>places365_small</td><td>58.99</td><td>59.15</td><td>train</td><td>val[:50%]</td><td>val[50%:]</td><td>(Zhou et al., 2018)</td></tr><tr><td>plant_village</td><td>100</td><td>99.89</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Hughes &amp; Salath\u00e9, 2015)</td></tr><tr><td>plantae_k</td><td>99.05</td><td>90.74</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Kour &amp; Arora, 2019)</td></tr><tr><td>quickdraw_bitmap</td><td>78.35</td><td>77.59</td><td>train[20k:]</td><td>train[10k:20k]</td><td>train[:10k]</td><td>(Ha &amp; Eck, 2018)</td></tr><tr><td>rock_paper_scissors</td><td>100</td><td>97.04</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Moroney, 2019b)</td></tr><tr><td>siscore/rotation</td><td>100</td><td>100</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Djolonga et al., 2021)</td></tr><tr><td>siscore/size</td><td>99.93</td><td>99.94</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Djolonga et al., 2021)</td></tr><tr><td>siscore/location</td><td>99.99</td><td>99.95</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Djolonga et al., 2021)</td></tr><tr><td>stanford_dogs</td><td>95.17</td><td>93.50</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Khosla et al., 2012)</td></tr><tr><td>stanford_online_products</td><td>90.00</td><td>89.47</td><td>train</td><td>test[:10k]</td><td>test[10k:]</td><td>(Song et al., 2016)</td></tr><tr><td>stl10</td><td>100</td><td>99.64</td><td>train[5%:]</td><td>train[:5%]</td><td>test</td><td>(Coates et al., 2011)</td></tr><tr><td>tf_flowers</td><td>99.45</td><td>97.83</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>-</td></tr><tr><td>uc_merced</td><td>100</td><td>100</td><td>train[10%:]</td><td>train[5%:10%]</td><td>train[:5%]</td><td>(Yang &amp; Newsam, 2010)</td></tr></tbody></table>", "caption": "Table 6: Datasets details (part 3 of 3).", "list_citation_info": ["Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Lixuan Zhu, Samyak Parajuli, Mike Guo, Dawn Xiaodong Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 8320\u20138329, 2021a.", "Kather et al. (2016) Jakob Nikolas Kather, Cleo-Aron Weis, Francesco Bianconi, Susanne Maria Melchers, Lothar Rudi Schad, Timo Gaiser, Alexander Marx, and Frank G. Z\u00f6llner. Multi-class texture analysis in colorectal cancer histology. Scientific Reports, 6, 2016.", "Bossard et al. (2014) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative components with random forests. In ECCV, 2014.", "Howard (2019a) Jeremy Howard. imagenette, 2019a. URL https://github.com/fastai/imagenette/.", "Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2242\u20132251, 2017.", "Liu et al. (2019c) Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-scale long-tailed recognition in an open world. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2532\u20132541, 2019c.", "Jiang et al. (2020) Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In ICML, 2020.", "Moroney (2019b) Laurence Moroney. Rock, paper, scissors dataset, 2019b. URL http://laurencemoroney.com/rock-paper-scissors-dataset.", "Olsen et al. (2019) Alex Olsen, Dmitry A. Konovalov, Bronson W Philippa, Peter V. Ridd, Jake C. Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James C. Whinney, Brendan Calvert, Mostafa Rahimi Azghadi, and Ronald D. White. Deepweeds: A multiclass weed species image dataset for deep learning. Scientific Reports, 9, 2019.", "Howard (2019b) Jeremy Howard. Imagewang, 2019b. URL https://github.com/fastai/imagenette/.", "Rajaraman et al. (2018) Sivaramakrishnan Rajaraman, Sameer Kiran Antani, Mahdieh Poostchi, Kamolrat Silamut, Md Amir Hossain, Richard James Maude, Stefan Jaeger, and George R. Thoma. Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images. PeerJ, 6, 2018.", "Makerere (2020) AI Lab Makerere. Bean disease dataset, January 2020. URL https://github.com/AI-Lab-Makerere/ibean.", "Djolonga et al. (2021) Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan I. Moldovan, Sylvan Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convolutional neural networks. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16453\u201316463, 2021.", "Rauf et al. (2019) Hafiz Tayyab Rauf, Basharat Ali Saleem, Muhammad Ikram Ullah Lali, Muhammad Attique Khan, Muhammad Usman Sharif, and Syed Ahmad Chan Bukhari. A citrus fruits and leaves dataset for detection and classification of citrus diseases through machine learning. Data in Brief, 26, 2019.", "Horn et al. (2018) Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. The inaturalist species classification and detection dataset. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8769\u20138778, 2018.", "Song et al. (2016) Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4004\u20134012, 2016.", "Zhou et al. (2018) Bolei Zhou, \u00c0gata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:1452\u20131464, 2018.", "Welinder et al. (2010) Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge J. Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010.", "Coates et al. (2011) Adam Coates, A. Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, 2011.", "Peng et al. (2019) Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1406\u20131415, 2019.", "Ha & Eck (2018) David R Ha and Douglas Eck. A neural representation of sketch drawings. ArXiv, abs/1704.03477, 2018.", "Yang & Newsam (2010) Yi Yang and S. Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In GIS \u201910, 2010.", "Mwebaze et al. (2019) Ernest Mwebaze, Timnit Gebru, Andrea Frome, Solomon Nsumba, and Tusubira Francis Jeremy. icassava 2019fine-grained visual categorization challenge. ArXiv, abs/1908.02900, 2019.", "Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Xiaodong Song. Natural adversarial examples. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15257\u201315266, 2021b.", "Hughes & Salath\u00e9 (2015) David P. Hughes and Marcel Salath\u00e9. An open access repository of images on plant health to enable the development of mobile disease diagnostics through machine learning and crowdsourcing. ArXiv, abs/1511.08060, 2015.", "Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. 2013 IEEE International Conference on Computer Vision Workshops, pp. 554\u2013561, 2013.", "Khosla et al. (2012) Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization : Stanford dogs. 2012.", "Wang et al. (2019a) Haohan Wang, Songwei Ge, Eric P. Xing, and Zachary Chase Lipton. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019a.", "Elson et al. (2007) Jeremy Elson, John R. Douceur, Jon Howell, and Jared Saul. Asirra: a captcha that exploits interest-aligned manual image categorization. In CCS \u201907, 2007.", "Clark et al. (2013) Kenneth W. Clark, Bruce A. Vendt, Kirk E. Smith, John B. Freymann, Justin S. Kirby, Paul Koppel, Stephen M. Moore, Stanley R. Phillips, David R. Maffitt, Michael Pringle, Lawrence Tarbox, and Fred W. Prior. The cancer imaging archive (tcia): Maintaining and operating a public information repository. Journal of Digital Imaging, 26:1045\u20131057, 2013.", "Kour & Arora (2019) Vippon Preet Kour and Sakshi Arora. Plantaek: A leaf database of native plants of jammu and kashmir. 2019.", "Moroney (2019a) Laurence Moroney. Horses or humans dataset, feb 2019a. URL http://laurencemoroney.com/horses-or-humans-dataset."]}], "citation_info_to_title": {"Veeling et al. (2018) Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. ArXiv, abs/1806.03962, 2018.": "Rotation Equivariant CNNs for Digital Pathology", "Rauf et al. (2019) Hafiz Tayyab Rauf, Basharat Ali Saleem, Muhammad Ikram Ullah Lali, Muhammad Attique Khan, Muhammad Usman Sharif, and Syed Ahmad Chan Bukhari. A citrus fruits and leaves dataset for detection and classification of citrus diseases through machine learning. Data in Brief, 26, 2019.": "A citrus fruits and leaves dataset for detection and classification of citrus diseases through machine learning", "Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1988\u20131997, 2017.": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "Kaggle & EyePacs (2015) Kaggle and EyePacs. Kaggle diabetic retinopathy detection. https://www.kaggle.com/c/diabetic-retinopathy-detection/data, 2015.": "Kaggle diabetic retinopathy detection", "Horn et al. (2018) Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. The inaturalist species classification and detection dataset. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8769\u20138778, 2018.": "The Inaturalist Species Classification and Detection Dataset", "Zhai et al. (2019) Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. The visual task adaptation benchmark. ArXiv, abs/1910.04867, 2019.": "The Visual Task Adaptation Benchmark", "Zhou et al. (2018) Bolei Zhou, \u00c0gata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:1452\u20131464, 2018.": "Places: A 10 million image database for scene recognition", "Parkhi et al. (2012) Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3498\u20133505, 2012.": "Cats and dogs", "Coates et al. (2011) Adam Coates, A. Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, 2011.": "An analysis of single-layer networks in unsupervised feature learning", "Olsen et al. (2019) Alex Olsen, Dmitry A. Konovalov, Bronson W Philippa, Peter V. Ridd, Jake C. Wood, Jamie Johns, Wesley Banks, Benjamin Girgenti, Owen Kenny, James C. Whinney, Brendan Calvert, Mostafa Rahimi Azghadi, and Ronald D. White. Deepweeds: A multiclass weed species image dataset for deep learning. Scientific Reports, 9, 2019.": "Deepweeds: A multiclass weed species image dataset for deep learning", "Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Xiaodong Song. Natural adversarial examples. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15257\u201315266, 2021b.": "Natural adversarial examples", "Bilen et al. (2017) Hakan Bilen, Sylvestre Rebuffi, and Tomas Jakab. Visual domain decathlon. 2017.": "Visual Domain Decathlon", "Xiao et al. (2010) Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 3485\u20133492, 2010.": "Sun database: Large-scale scene recognition from abbey to zoo", "Mwebaze et al. (2019) Ernest Mwebaze, Timnit Gebru, Andrea Frome, Solomon Nsumba, and Tusubira Francis Jeremy. icassava 2019fine-grained visual categorization challenge. ArXiv, abs/1908.02900, 2019.": "icassava 2019: Fine-Grained Visual Categorization Challenge", "Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.": "Learning multiple layers of features from tiny images", "Moroney (2019a) Laurence Moroney. Horses or humans dataset, feb 2019a. URL http://laurencemoroney.com/horses-or-humans-dataset.": "Moroney (2019a) Horses or humans dataset", "Lake et al. (2015) Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350:1332 \u2013 1338, 2015.": "Human-level concept learning through probabilistic program induction", "Djolonga et al. (2021) Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan I. Moldovan, Sylvan Gelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convolutional neural networks. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16453\u201316463, 2021.": "On robustness and transferability of convolutional neural networks", "Rajaraman et al. (2018) Sivaramakrishnan Rajaraman, Sameer Kiran Antani, Mahdieh Poostchi, Kamolrat Silamut, Md Amir Hossain, Richard James Maude, Stefan Jaeger, and George R. Thoma. Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images. PeerJ, 6, 2018.": "Pre-trained Convolutional Neural Networks as Feature Extractors Toward Improved Malaria Parasite Detection in Thin Blood Smear Images", "Nilsback & Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722\u2013729, 2008.": "Automated Flower Classification Over a Large Number of Classes", "Kather et al. (2016) Jakob Nikolas Kather, Cleo-Aron Weis, Francesco Bianconi, Susanne Maria Melchers, Lothar Rudi Schad, Timo Gaiser, Alexander Marx, and Frank G. Z\u00f6llner. Multi-class texture analysis in colorectal cancer histology. Scientific Reports, 6, 2016.": "Multi-class texture analysis in colorectal cancer histology", "Yang & Newsam (2010) Yi Yang and S. Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In GIS \u201910, 2010.": "Bag-of-visual-words and spatial extensions for land-use classification", "LeCun et al. (2004) Yann LeCun, Fu Jie Huang, and L\u00e9on Bottou. Learning methods for generic object recognition with invariance to pose and lighting. Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., 2:II\u2013104 Vol.2, 2004.": "Learning methods for generic object recognition with invariance to pose and lighting", "Bossard et al. (2014) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative components with random forests. In ECCV, 2014.": "Food-101 - mining discriminative components with random forests", "Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Lixuan Zhu, Samyak Parajuli, Mike Guo, Dawn Xiaodong Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 8320\u20138329, 2021a.": "The many faces of robustness: A critical analysis of out-of-distribution generalization", "Wang et al. (2019a) Haohan Wang, Songwei Ge, Eric P. Xing, and Zachary Chase Lipton. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019a.": "Learning Robust Global Representations by Penalizing Local Predictive Power", "Peng et al. (2019) Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1406\u20131415, 2019.": "Moment Matching for Multi-Source Domain Adaptation", "Elson et al. (2007) Jeremy Elson, John R. Douceur, Jon Howell, and Jared Saul. Asirra: a captcha that exploits interest-aligned manual image categorization. In CCS \u201907, 2007.": "Asirra: a captcha that exploits interest-aligned manual image categorization", "Geiger et al. (2012) Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3354\u20133361, 2012.": "Are we ready for autonomous driving? The KITTI Vision Benchmark Suite", "LeCun et al. (1998) Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86:2278\u20132324, 1998.": "Gradient-based learning applied to document recognition", "Klindt et al. (2021) David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan M. Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. ArXiv, abs/2007.10930, 2021.": "Towards nonlinear disentanglement in natural data with temporal sparse coding", "Howard (2019a) Jeremy Howard. imagenette, 2019a. URL https://github.com/fastai/imagenette/.": "Jeremy Howards Imagenette (2019a)", "Cimpoi et al. (2014) Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606\u20133613, 2014.": "Describing textures in the wild", "Jiang et al. (2020) Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In ICML, 2020.": "Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels", "Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211\u2013252, 2015.": "Imagenet large scale visual recognition challenge", "Howard (2019b) Jeremy Howard. Imagewang, 2019b. URL https://github.com/fastai/imagenette/.": "Imagewang (2019b)", "Liu et al. (2019c) Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-scale long-tailed recognition in an open world. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2532\u20132541, 2019c.": "Large-scale long-tailed recognition in an open world", "Makerere (2020) AI Lab Makerere. Bean disease dataset, January 2020. URL https://github.com/AI-Lab-Makerere/ibean.": "Makerere (2020) AI Lab Makerere Bean disease dataset", "Clark et al. (2013) Kenneth W. Clark, Bruce A. Vendt, Kirk E. Smith, John B. Freymann, Justin S. Kirby, Paul Koppel, Stephen M. Moore, Stanley R. Phillips, David R. Maffitt, Michael Pringle, Lawrence Tarbox, and Fred W. Prior. The cancer imaging archive (tcia): Maintaining and operating a public information repository. Journal of Digital Imaging, 26:1045\u20131057, 2013.": "The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository", "Welinder et al. (2010) Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge J. Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010.": "Caltech-ucsd birds 200", "Cheng et al. (2017) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105:1865\u20131883, 2017.": "Remote Sensing Image Scene Classification: Benchmark and State of the Art", "Clanuwat et al. (2018) Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature. ArXiv, abs/1812.01718, 2018.": "Deep Learning for Classical Japanese Literature", "Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas R. Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12:2217\u20132226, 2019.": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification", "Zhu et al. (2017) Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 2242\u20132251, 2017.": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "Hughes & Salath\u00e9 (2015) David P. Hughes and Marcel Salath\u00e9. An open access repository of images on plant health to enable the development of mobile disease diagnostics through machine learning and crowdsourcing. ArXiv, abs/1511.08060, 2015.": "An open access repository of images on plant health to enable the development of mobile disease diagnostics through machine learning and crowdsourcing", "Khosla et al. (2012) Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization : Stanford dogs. 2012.": "Novel dataset for fine-grained image categorization: Stanford dogs", "Netzer et al. (2011) Yuval Netzer, Tao Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images with unsupervised feature learning. 2011.": "Reading digits in natural images with unsupervised feature learning", "Das et al. (2012b) N. Das, Ram Sarkar, Subhadip Basu, Mahantapas Kundu, Mita Nasipuri, and Dipak Kumar Basu. A genetic algorithm based region sampling for selection of local features in handwritten digit recognition application. Appl. Soft Comput., 12:1592\u20131606, 2012b.": "A genetic algorithm based region sampling for selection of local features in handwritten digit recognition application", "Song et al. (2016) Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4004\u20134012, 2016.": "Deep metric learning via lifted structured feature embedding", "Kour & Arora (2019) Vippon Preet Kour and Sakshi Arora. Plantaek: A leaf database of native plants of jammu and kashmir. 2019.": "Plantaek: A leaf database of native plants of Jammu and Kashmir", "Cohen et al. (2017) Gregory Cohen, Saeed Afshar, Jonathan C. Tapson, and Andr\u00e9 van Schaik. Emnist: Extending mnist to handwritten letters. 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921\u20132926, 2017.": "Emnist: Extending mnist to handwritten letters", "Ha & Eck (2018) David R Ha and Douglas Eck. A neural representation of sketch drawings. ArXiv, abs/1704.03477, 2018.": "A neural representation of sketch drawings", "Fei-Fei et al. (2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. 2004.": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories", "Moroney (2019b) Laurence Moroney. Rock, paper, scissors dataset, 2019b. URL http://laurencemoroney.com/rock-paper-scissors-dataset.": "Rock, paper, scissors dataset", "Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. 2013 IEEE International Conference on Computer Vision Workshops, pp. 554\u2013561, 2013.": "3D Object Representations for Fine-Grained Categorization"}, "source_title_to_arxiv_id": {"Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels": "1911.09781", "Deep Learning for Classical Japanese Literature": "1812.01718"}}