{"title": "HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection", "abstract": "Besides standard cameras, autonomous vehicles typically include multiple\nadditional sensors, such as lidars and radars, which help acquire richer\ninformation for perceiving the content of the driving scene. While several\nrecent works focus on fusing certain pairs of sensors - such as camera and\nlidar or camera and radar - by using architectural components specific to the\nexamined setting, a generic and modular sensor fusion architecture is missing\nfrom the literature. In this work, we focus on 2D object detection, a\nfundamental high-level task which is defined on the 2D image domain, and\npropose HRFuser, a multi-resolution sensor fusion architecture that scales\nstraightforwardly to an arbitrary number of input modalities. The design of\nHRFuser is based on state-of-the-art high-resolution networks for image-only\ndense prediction and incorporates a novel multi-window cross-attention block as\nthe means to perform fusion of multiple modalities at multiple resolutions.\nEven though cameras alone provide very informative features for 2D detection,\nwe demonstrate via extensive experiments on the nuScenes and Seeing Through Fog\ndatasets that our model effectively leverages complementary features from\nadditional modalities, substantially improving upon camera-only performance and\nconsistently outperforming state-of-the-art fusion methods for 2D detection\nboth in normal and adverse conditions. The source code will be made publicly\navailable.", "authors": ["Tim Broedermann", "Christos Sakaridis", "Dengxin Dai", "Luc Van Gool"], "published_date": "2022_06_30", "pdf_url": "http://arxiv.org/pdf/2206.15157v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>Modalities</td><td>AP</td><td>AP{}_{0.5}</td><td>AP{}_{0.75}</td><td>AP{}_{m}</td><td>AP{}_{l}</td><td>AR</td></tr><tr><th>HRNetV2p-w18 [76]</th><td>C</td><td>32.4</td><td>56.6</td><td>33.5</td><td>21.0</td><td>43.7</td><td>43.4</td></tr><tr><th>HRFormer-T [88]</th><td>C</td><td>34.3</td><td>59.6</td><td>35.6</td><td>23.2</td><td>45.5</td><td>43.9</td></tr><tr><th>HRFormer-B [88]</th><td>C</td><td>33.8</td><td>59.4</td><td>34.6</td><td>22.4</td><td>45.1</td><td>43.1</td></tr><tr><th>Radar-Camera Fusion[46]*</th><td>CR</td><td>35.6</td><td>60.5</td><td>37.4</td><td>-</td><td>-</td><td>42.1</td></tr><tr><th>HRFuser-w18 (HRNet)</th><td>CRL</td><td>36.7</td><td>63.1</td><td>38.1</td><td>24.9</td><td>48.6</td><td>47.0</td></tr><tr><th>HRFuser-T (HRFormer)</th><td>CRL</td><td>38.3</td><td>65.3</td><td>40.1</td><td>26.8</td><td>49.9</td><td>48.3</td></tr><tr><th>HRFuser-S (HRFormer)</th><td>CRL</td><td>38.5</td><td>65.6</td><td>40.2</td><td>27.2</td><td>49.9</td><td>48.1</td></tr><tr><th>HRFuser-B (HRFormer)</th><td>CRL</td><td>38.8</td><td>66.0</td><td>41.0</td><td>26.9</td><td>50.7</td><td>48.6</td></tr><tr><th>CRF-Net [48]</th><td>CR</td><td>27.0</td><td>42.7</td><td>29.0</td><td>22.7</td><td>35.6</td><td>31.3</td></tr><tr><th>HRFuser-T (HRFormer)</th><td>CRL</td><td>34.6</td><td>62.0</td><td>34.7</td><td>26.0</td><td>48.5</td><td>45.8</td></tr></tbody></table>", "caption": "Table 1: Comparison of 2D detection methods on nuScenes evaluated on 6 classes: car, truck, bus, bicycle, motorcycle and pedestrian.The first group of rows shows results on the standard nuScenes validation set. The second group of rows shows results using the split from [48] for training and evaluation. All entries including the word \u201cHRFuser\u201d constitute versions of our method and state the camera-only model upon which they build in parentheses. All HR* methods use a Cascade R-CNN head and have undergone hyper-parameter tuning. C: camera, R: radar, L: lidar, (*): results taken directly from the respective paper.", "list_citation_info": ["Wang et al. [2021b] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(10):3349\u20133364, 2021b.", "Nobis et al. [2019] Felix Nobis, Maximilian Geisslinger, Markus Weber, Johannes Betz, and Markus Lienkamp. A deep learning-based radar and camera sensor fusion architecture for object detection. In Sensor Data Fusion: Trends, Solutions, Applications (SDF), 2019.", "Yuan et al. [2021] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. HRFormer: High-resolution transformer for dense prediction. arXiv e-prints, abs/2110.09408, 2021.", "Nabati and Qi [2020] Ramin Nabati and Hairong Qi. Radar-camera sensor fusion for joint object detection and distance estimation in autonomous vehicles. arXiv e-prints, abs/2009.08428, 2020."]}, {"table": "<table><thead><tr><th>Weather</th><th colspan=\"3\">clear</th><th colspan=\"3\">light fog</th><th colspan=\"3\">dense fog</th><th colspan=\"3\">snow/rain</th></tr></thead><tbody><tr><th>Difficulty</th><td>easy</td><td>mod.</td><td>hard</td><td>easy</td><td>mod.</td><td>hard</td><td>easy</td><td>mod.</td><td>hard</td><td>easy</td><td>mod.</td><td>hard</td></tr><tr><th>Deep Entropy Fusion [2]*</th><th>89.84</th><th>85.57</th><th>79.46</th><th>90.54</th><th>87.99</th><th>84.90</th><th>87.68</th><th>81.49</th><th>76.69</th><th>88.99</th><th>83.71</th><th>77.85</th></tr><tr><th>HRFuser-T</th><td>90.15</td><td>87.10</td><td>79.48</td><td>90.60</td><td>89.34</td><td>86.50</td><td>87.93</td><td>80.27</td><td>78.21</td><td>90.05</td><td>85.35</td><td>78.09</td></tr></tbody></table>", "caption": "Table 2: Comparison of 2D detection methods on the STF test sets. HRFuser-T builds upon HRFormer. (*): results taken directly from the respective paper.", "list_citation_info": ["Bijelic et al. [2020] Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Kraus, Werner Ritter, Klaus Dietmayer, and Felix Heide. Seeing through fog without seeing fog: Deep multimodal sensor fusion in unseen adverse weather. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020."]}, {"table": "<table><tbody><tr><td>Method (Fusion Type)</td><td>AP</td><td>AP{}_{0.5}</td><td>AP{}_{0.75}</td><td>AP{}_{m}</td><td>AP{}_{l}</td><td>AR</td></tr><tr><td>HRFormer-T</td><td>26.5</td><td>49.9</td><td>25.3</td><td>18.2</td><td>37.0</td><td>26.8</td></tr><tr><td>HRFormer-T (Early)</td><td>27.7</td><td>51.6</td><td>26.5</td><td>18.4</td><td>38.8</td><td>38.9</td></tr><tr><td>HRFuser-T (Addition)</td><td>30.8</td><td>56.4</td><td>30.5</td><td>22.0</td><td>41.9</td><td>42.0</td></tr><tr><td>HRFuser-T (MWCA{}_{\\text{onlyHighRes}})</td><td>30.5</td><td>56.1</td><td>29.7</td><td>21.8</td><td>41.4</td><td>41.5</td></tr><tr><td>HRFuser-T (MWCA)</td><td>31.5</td><td>57.4</td><td>31.1</td><td>22.7</td><td>42.5</td><td>42.3</td></tr><tr><td>HRFuser-T (PVTv2-CA [79])</td><td>29.8</td><td>54.3</td><td>29.4</td><td>20.1</td><td>41.3</td><td>40.9</td></tr><tr><td>HRFuser-T (PVTv2-Li-CA [79])</td><td>29.5</td><td>54.2</td><td>28.6</td><td>19.9</td><td>41.0</td><td>40.6</td></tr></tbody></table>", "caption": "Table 3: Ablations of fusion strategies on nuScenes. Early: early fusion by direct concatenation of all 3 input modalities, Addition: fusion at multiple levels and resolutions between secondary branches and camera branch via an addition block, MWCA: our MWCA fusion block, MWCA{}_{\\text{onlyHighRes}}: our fusion block included only in the highest resolution of the camera branch.", "list_citation_info": ["Wang et al. [2022] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, pages 1\u201310, 2022."]}], "citation_info_to_title": {"Wang et al. [2021b] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(10):3349\u20133364, 2021b.": "Deep high-resolution representation learning for visual recognition", "Nabati and Qi [2020] Ramin Nabati and Hairong Qi. Radar-camera sensor fusion for joint object detection and distance estimation in autonomous vehicles. arXiv e-prints, abs/2009.08428, 2020.": "Radar-camera sensor fusion for joint object detection and distance estimation in autonomous vehicles", "Bijelic et al. [2020] Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Kraus, Werner Ritter, Klaus Dietmayer, and Felix Heide. Seeing through fog without seeing fog: Deep multimodal sensor fusion in unseen adverse weather. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "Seeing through fog without seeing fog: Deep multimodal sensor fusion in unseen adverse weather", "Wang et al. [2022] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, pages 1\u201310, 2022.": "Pvt v2: Improved baselines with pyramid vision transformer", "Nobis et al. [2019] Felix Nobis, Maximilian Geisslinger, Markus Weber, Johannes Betz, and Markus Lienkamp. A deep learning-based radar and camera sensor fusion architecture for object detection. In Sensor Data Fusion: Trends, Solutions, Applications (SDF), 2019.": "A deep learning-based radar and camera sensor fusion architecture for object detection", "Yuan et al. [2021] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. HRFormer: High-resolution transformer for dense prediction. arXiv e-prints, abs/2110.09408, 2021.": "HRFormer: High-resolution transformer for dense prediction"}, "source_title_to_arxiv_id": {"Pvt v2: Improved baselines with pyramid vision transformer": "2106.13797"}}