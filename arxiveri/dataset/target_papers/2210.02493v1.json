{"title": "Depth Is All You Need for Monocular 3D Detection", "abstract": "A key contributor to recent progress in 3D detection from single images is\nmonocular depth estimation. Existing methods focus on how to leverage depth\nexplicitly, by generating pseudo-pointclouds or providing attention cues for\nimage features. More recent works leverage depth prediction as a pretraining\ntask and fine-tune the depth representation while training it for 3D detection.\nHowever, the adaptation is insufficient and is limited in scale by manual\nlabels. In this work, we propose to further align depth representation with the\ntarget domain in unsupervised fashions. Our methods leverage commonly available\nLiDAR or RGB videos during training time to fine-tune the depth representation,\nwhich leads to improved 3D detectors. Especially when using RGB videos, we show\nthat our two-stage training by first generating pseudo-depth labels is critical\nbecause of the inconsistency in loss distribution between the two tasks. With\neither type of reference data, our multi-task learning approach improves over\nthe state of the art on both KITTI and NuScenes, while matching the test-time\ncomplexity of its single task sub-network.", "authors": ["Dennis Park", "Jie Li", "Dian Chen", "Vitor Guizilini", "Adrien Gaidon"], "published_date": "2022_10_05", "pdf_url": "http://arxiv.org/pdf/2210.02493v1", "list_table_and_caption": [{"table": "<table><tr><td>Methods</td><td>Depth Sup.</td><td>Backbone</td><td>AP[%]\\uparrow</td><td>ATE[m]\\downarrow</td><td>ASE[1-IoU]\\downarrow</td><td>AOE[rad]\\downarrow</td><td>NDS\\uparrow</td></tr><tr><td>MonoDIS [40]</td><td>-</td><td>R34</td><td>30.4</td><td>0.74</td><td>0.26</td><td>0.55</td><td>0.38</td></tr><tr><td>FCOS3D [3]</td><td>-</td><td>R101</td><td>35.8</td><td>0.69</td><td>0.25</td><td>0.45</td><td>0.43</td></tr><tr><td>PGD[42]</td><td>-</td><td>R101</td><td>37.0</td><td>0.66</td><td>0.25</td><td>0.49</td><td>0.43</td></tr><tr><td>DD3D  [1]</td><td>-</td><td>V2-99</td><td>41.8</td><td>0.57</td><td>0.25</td><td>0.37</td><td>0.48</td></tr><tr><td>DETR3D [43]</td><td>-</td><td>V2-99</td><td>41.2</td><td>0.64</td><td>0.26</td><td>0.39</td><td>0.48</td></tr><tr><td>BEVDet{}^{*} [44]</td><td>-</td><td>V2-99</td><td>42.4</td><td>0.52</td><td>0.24</td><td>0.37</td><td>0.49</td></tr><tr><td>BEVFormer-S{}^{*} [45]</td><td>-</td><td>V2-99</td><td>43.5</td><td>0.59</td><td>0.25</td><td>0.40</td><td>0.50</td></tr><tr><td>PETR{}^{*} [46]</td><td>-</td><td>V2-99</td><td>44.1</td><td>0.59</td><td>0.25</td><td>0.38</td><td>0.50</td></tr><tr><td>DD3Dv2-selfsup</td><td>Video</td><td>V2-99</td><td>43.1</td><td>0.57</td><td>0.25</td><td>0.38</td><td>0.48</td></tr><tr><td>DD3Dv2</td><td>LiDAR</td><td>V2-99</td><td>46.1</td><td>0.52</td><td>0.24</td><td>0.36</td><td>0.51</td></tr></table><br/>", "caption": "TABLE I: nuScenes detection test set evaluation. We present summary metrics of the benchmark. * denotes results reported on the benchmark that do not have associated publications at the time of writing. The bold and underline denote the best of all and the best excluding concurrent work, respectively. Note that PointPillars [47] is a Lidar-based detector.", "list_citation_info": ["[40] A. Simonelli, S. R. Bulo, L. Porzi, M. L. Antequera, and P. Kontschieder, \u201cDisentangling monocular 3d object detection: From single to multi-class recognition,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[43] Y. Wang, V. Guizilini, T. Zhang, Y. Wang, H. Zhao, , and J. M. Solomon, \u201cDetr3d: 3d object detection from multi-view images via 3d-to-2d queries,\u201d in The Conference on Robot Learning (CoRL), 2021.", "[3] T. Wang, X. Zhu, J. Pang, and D. Lin, \u201cFcos3d: Fully convolutional one-stage monocular 3d object detection,\u201d arXiv preprint arXiv:2104.10956, 2021.", "[46] Y. Liu, T. Wang, X. Zhang, and J. Sun, \u201cPetr: Position embedding transformation for multi-view 3d object detection,\u201d arXiv preprint arXiv:2203.05625, 2022.", "[42] T. Wang, Z. Xinge, J. Pang, and D. Lin, \u201cProbabilistic and geometric depth: Detecting objects in perspective,\u201d in Conference on Robot Learning. PMLR, 2022, pp. 1475\u20131485.", "[44] J. Huang, G. Huang, Z. Zhu, and D. Du, \u201cBevdet: High-performance multi-camera 3d object detection in bird-eye-view,\u201d arXiv preprint arXiv:2112.11790, 2021.", "[45] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, and J. Dai, \u201cBevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers,\u201d arXiv preprint arXiv:2203.17270, 2022.", "[47] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointpillars: Fast encoders for object detection from point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12\u2009697\u201312\u2009705.", "[1] D. Park, R. Ambrus, V. Guizilini, J. Li, and A. Gaidon, \u201cIs pseudo-lidar needed for monocular 3d object detection?\u201d in IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><tr><td></td><td></td><td colspan=\"6\">Car</td></tr><tr><td></td><td></td><td colspan=\"3\">BEV AP</td><td colspan=\"3\">3D AP</td></tr><tr><td rowspan=\"-2\">Methods</td><td rowspan=\"-2\">Depth Sup.</td><td>Easy</td><td>Med</td><td>Hard</td><td>Easy</td><td>Med</td><td>Hard</td></tr><tr><td>SMOKE [27]</td><td>-</td><td>20.83</td><td>14.49</td><td>12.75</td><td>14.03</td><td>9.76</td><td>7.84</td></tr><tr><td>MonoPair [48]</td><td>-</td><td>19.28</td><td>14.83</td><td>12.89</td><td>13.04</td><td>9.99</td><td>8.65</td></tr><tr><td>AM3D [26]</td><td>LiDAR</td><td>25.03</td><td>17.32</td><td>14.91</td><td>16.50</td><td>10.74</td><td>9.52</td></tr><tr><td>PatchNet\\dagger  [12]</td><td>LiDAR</td><td>22.97</td><td>16.86</td><td>14.97</td><td>15.68</td><td>11.12</td><td>10.17</td></tr><tr><td>RefinedMPL [49]</td><td></td><td>28.08</td><td>17.60</td><td>13.95</td><td>18.09</td><td>11.14</td><td>8.96</td></tr><tr><td>D4LCN [50]</td><td>LiDAR</td><td>22.51</td><td>16.02</td><td>12.55</td><td>16.65</td><td>11.72</td><td>9.51</td></tr><tr><td>Kinematic3D [51]</td><td>Video</td><td>26.99</td><td>17.52</td><td>13.10</td><td>19.07</td><td>12.72</td><td>9.17</td></tr><tr><td>Demystifying [5]</td><td>LiDAR</td><td>-</td><td>-</td><td>-</td><td>23.66</td><td>13.25</td><td>11.23</td></tr><tr><td>CaDDN [30]</td><td>LiDAR</td><td>27.94</td><td>18.91</td><td>17.19</td><td>19.17</td><td>13.41</td><td>11.46</td></tr><tr><td>MonoEF [52]</td><td>Video</td><td>29.03</td><td>19.70</td><td>17.26</td><td>21.29</td><td>13.87</td><td>11.71</td></tr><tr><td>MonoFlex [53]</td><td>-</td><td>28.23</td><td>19.75</td><td>16.89</td><td>19.94</td><td>13.89</td><td>12.07</td></tr><tr><td>GUPNet [54]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>20.11</td><td>14.20</td><td>11.77</td></tr><tr><td>PGD [42]</td><td>-</td><td>30.56</td><td>23.67</td><td>20.84</td><td>24.35</td><td>18.34</td><td>16.90</td></tr><tr><td>DD3D [1]</td><td>-</td><td>30.98</td><td>22.56</td><td>20.03</td><td>23.22</td><td>16.34</td><td>14.20</td></tr><tr><td>MonoDTR{}^{\\star} [33]</td><td>LiDAR</td><td>28.59</td><td>20.38</td><td>17.14</td><td>21.99</td><td>15.39</td><td>12.73</td></tr><tr><td>PS-fld{}^{\\star\\dagger} [55]</td><td>LiDAR</td><td>32.64</td><td>23.76</td><td>20.64</td><td>23.74</td><td>17.74</td><td>15.14</td></tr><tr><td>MonoDDE{}^{\\star} [56]</td><td>-</td><td>33.58</td><td>23.46</td><td>20.37</td><td>23.74</td><td>17.14</td><td>15.10</td></tr><tr><td>Ours</td><td>LiDAR</td><td>35.70</td><td>24.67</td><td>21.73</td><td>26.36</td><td>17.61</td><td>15.32</td></tr></table><br/>", "caption": "TABLE II: KITTI-3D test set evaluation on Car. We report AP|_{R_{40}} metrics.{}^{\\star} indicates concurrent works. {}^{\\dagger} indicates the usage of the KITTI-depth dataset, with a known information leakage between training and validation splits [5]. Bold and underline denote the best of all and the best excluding concurrent work.", "list_citation_info": ["[50] M. Ding, Y. Huo, H. Yi, Z. Wang, J. Shi, Z. Lu, and P. Luo, \u201cLearning depth-guided convolutions for monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 1000\u20131001.", "[55] Y.-N. Chen, H. Dai, and Y. Ding, \u201cPseudo-stereo for monocular 3d object detection in autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.", "[48] Y. Chen, L. Tai, K. Sun, and M. Li, \u201cMonopair: Monocular 3d object detection using pairwise spatial relationships,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009093\u201312\u2009102.", "[26] X. Ma, Z. Wang, H. Li, P. Zhang, W. Ouyang, and X. Fan, \u201cAccurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6851\u20136860.", "[49] J. M. U. Vianney, S. Aich, and B. Liu, \u201cRefinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving,\u201d arXiv preprint arXiv:1911.09712, 2019.", "[30] C. Reading, A. Harakeh, J. Chae, and S. L. Waslander, \u201cCategorical depth distributionnetwork for monocular 3d object detection,\u201d CVPR, 2021.", "[42] T. Wang, Z. Xinge, J. Pang, and D. Lin, \u201cProbabilistic and geometric depth: Detecting objects in perspective,\u201d in Conference on Robot Learning. PMLR, 2022, pp. 1475\u20131485.", "[27] Z. Liu, Z. Wu, and R. T\u00f3th, \u201cSmoke: single-stage monocular 3d object detection via keypoint estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 996\u2013997.", "[33] K.-C. Huang, T.-H. Wu, H.-T. Su, and W. H. Hsu, \u201cMonodtr: Monocular 3d object detection with depth-aware transformer,\u201d in CVPR, 2022.", "[51] G. Brazil, G. Pons-Moll, X. Liu, and B. Schiele, \u201cKinematic 3d object detection in monocular video,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 135\u2013152.", "[56] Z. Li, Z. Qu, Y. Zhou, J. Liu, H. Wang, and L. Jiang, \u201cDiversity matters: Fully exploiting depth clues for reliable monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2791\u20132800.", "[53] Y. Zhang, J. Lu, and J. Zhou, \u201cObjects are different: Flexible monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3289\u20133298.", "[52] Y. Zhou, Y. He, H. Zhu, C. Wang, H. Li, and Q. Jiang, \u201cMonocular 3d object detection: An extrinsic parameter free approach,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7556\u20137566.", "[5] A. Simonelli, S. R. Bul\u00f2, L. Porzi, P. Kontschieder, and E. Ricci, \u201cDemystifying pseudo-lidar for monocular 3d object detection,\u201d arXiv preprint arXiv:2012.05796, 2020.", "[1] D. Park, R. Ambrus, V. Guizilini, J. Li, and A. Gaidon, \u201cIs pseudo-lidar needed for monocular 3d object detection?\u201d in IEEE/CVF International Conference on Computer Vision (ICCV), 2021.", "[12] X. Ma, S. Liu, Z. Xia, H. Zhang, X. Zeng, and W. Ouyang, \u201cRethinking pseudo-lidar representation,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 311\u2013327.", "[54] Y. Lu, X. Ma, L. Yang, T. Zhang, Y. Liu, Q. Chu, J. Yan, and W. Ouyang, \u201cGeometry uncertainty projection network for monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 3111\u20133121."]}, {"table": "<table><tr><td></td><td colspan=\"6\">Pedestrian</td><td colspan=\"6\">Cyclist</td></tr><tr><td></td><td colspan=\"3\">BEV AP</td><td colspan=\"3\">3D AP</td><td colspan=\"3\">BEV AP</td><td colspan=\"3\">3D AP</td></tr><tr><td rowspan=\"-3\">Methods</td><td>Easy</td><td>Med</td><td>Hard</td><td>Easy</td><td>Med</td><td>Hard</td><td>Easy</td><td>Med</td><td>Hard</td><td>Easy</td><td>Med</td><td>Hard</td></tr><tr><td>M3D-RPN [57]</td><td>5.65</td><td>4.05</td><td>3.29</td><td>4.92</td><td>3.48</td><td>2.94</td><td>1.25</td><td>0.81</td><td>0.78</td><td>0.94</td><td>0.65</td><td>0.47</td></tr><tr><td>MonoPSR [23]</td><td>7.24</td><td>4.56</td><td>4.11</td><td>6.12</td><td>4.00</td><td>3.30</td><td>9.87</td><td>5.78</td><td>4.57</td><td>8.37</td><td>4.74</td><td>3.68</td></tr><tr><td>CaDDN [30]</td><td>14.72</td><td>9.41</td><td>8.17</td><td>12.87</td><td>8.14</td><td>6.76</td><td>9.67</td><td>5.38</td><td>4.75</td><td>7.00</td><td>3.41</td><td>3.30</td></tr><tr><td>DD3D</td><td>15.90</td><td>10.85</td><td>8.05</td><td>13.91</td><td>9.30</td><td>8.05</td><td>3.20</td><td>1.99</td><td>1.79</td><td>2.39</td><td>1.52</td><td>1.31</td></tr><tr><td>MonoDTR{}^{\\star} [33]</td><td>16.66</td><td>10.59</td><td>9.00</td><td>15.33</td><td>10.18</td><td>8.61</td><td>5.84</td><td>4.11</td><td>3.48</td><td>5.05</td><td>3.27</td><td>3.19</td></tr><tr><td>MonoDDE{}^{*} [56]</td><td>12.38</td><td>8.41</td><td>7.16</td><td>11.13</td><td>7.32</td><td>6.67</td><td>6.68</td><td>4.36</td><td>3.76</td><td>5.94</td><td>3.78</td><td>3.33</td></tr><tr><td>PS-fld{}^{\\star\\dagger} [55]</td><td>19.03</td><td>12.23</td><td>10.53</td><td>16.95</td><td>10.82</td><td>9.26</td><td>12.80</td><td>7.29</td><td>6.05</td><td>11.22</td><td>6.18</td><td>5.21</td></tr><tr><td>Ours</td><td>17.74</td><td>12.16</td><td>10.49</td><td>16.25</td><td>10.82</td><td>9.24</td><td>10.67</td><td>7.02</td><td>5.78</td><td>8.79</td><td>5.68</td><td>4.75</td></tr></table><br/>", "caption": "TABLE III: KITTI-3D test set evaluation on Pedestrian and Cyclist. {}^{\\star} indicates concurrent works. {}^{\\dagger} indicates the usage of the KITTI-depth dataset. Bold and underline denote the best of all and the best excluding concurrent work.", "list_citation_info": ["[23] J. Ku, A. D. Pon, and S. L. Waslander, \u201cMonocular 3d object detection leveraging accurate proposals and shape reconstruction,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 11\u2009867\u201311\u2009876.", "[55] Y.-N. Chen, H. Dai, and Y. Ding, \u201cPseudo-stereo for monocular 3d object detection in autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.", "[30] C. Reading, A. Harakeh, J. Chae, and S. L. Waslander, \u201cCategorical depth distributionnetwork for monocular 3d object detection,\u201d CVPR, 2021.", "[33] K.-C. Huang, T.-H. Wu, H.-T. Su, and W. H. Hsu, \u201cMonodtr: Monocular 3d object detection with depth-aware transformer,\u201d in CVPR, 2022.", "[57] G. Brazil and X. Liu, \u201cM3d-rpn: Monocular 3d region proposal network for object detection,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9287\u20139296.", "[56] Z. Li, Z. Qu, Y. Zhou, J. Liu, H. Wang, and L. Jiang, \u201cDiversity matters: Fully exploiting depth clues for reliable monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2791\u20132800."]}, {"table": "<table><tbody><tr><td>ID</td><th> Approach </th><th> Extra Data </th><th> PseudoLabels </th><th> DepthLoss </th><th> Detection AccuracyNDS \\uparrow (mAP [\\%]\\uparrow) </th><th> Depth AccuracyAbs. Rel \\downarrow </th></tr><tr><td>E1</td><td>Detection Only</td><td>-</td><td>-</td><td>L1</td><td>41.2 (35.8)</td><td>-</td></tr><tr><td>E2</td><td>DD3Dv2</td><td>LiDAR</td><td>-</td><td>L1</td><td>45.6 (39.1)</td><td>0.20</td></tr><tr><td>E3</td><td>Self-supervised</td><td>Video</td><td>-</td><td>SSIM</td><td>42.8 (36.4)</td><td>0.51</td></tr><tr><td>E4</td><td>\u2003+ ignore close</td><td>Video</td><td>-</td><td>SSIM</td><td>42.9 (37.5)</td><td>0.54</td></tr><tr><td>E5</td><td>DD3Dv2-selfsup</td><td>Video</td><td>\\surd</td><td>L1</td><td>43.2 (37.7)</td><td>0.51 \\rightarrow 0.52</td></tr><tr><td>E6</td><td>\u2003+ ignore close</td><td>Video</td><td>\\surd</td><td>L1</td><td>43.7 (36.9)</td><td>0.54 \\rightarrow 0.54</td></tr></tbody></table>", "caption": "TABLE IV: We provide an ablation analysis on crucial design choices of both architecture and training strategies. We show how LiDAR supervision improves on top of single-task training (E2 vs. E1). In E3 and E4, we employ a single-stage training strategy using video frames as depicted in Figure 2(a). In E5 and E6, we employ a two-stage training strategy by generating pseudo-labels first as depicted in Figure 2(b). \u201cignore close\u201d indicate a small trick to ignore closest depth estimation in self-supervised training. All methods start from a single initial model pretrained by large-scale depth supervision available from [1].", "list_citation_info": ["[1] D. Park, R. Ambrus, V. Guizilini, J. Li, and A. Gaidon, \u201cIs pseudo-lidar needed for monocular 3d object detection?\u201d in IEEE/CVF International Conference on Computer Vision (ICCV), 2021."]}, {"table": "<table><thead><tr><th> Backbone </th><th> Multi-task </th><th> Pretrained Dataset </th><th> Pretrained Task </th><th> NDS \\uparrow </th><th> mAP [\\%]\\uparrow </th></tr></thead><tbody><tr><td>V2-99</td><td>-</td><td>DDAD15M</td><td>Depth Est.</td><td>41.2</td><td>35.8</td></tr><tr><td>V2-99</td><td>\\surd</td><td>DDAD15M</td><td>Depth Est.</td><td>45.6 (+4.4)</td><td>39.1 (+3.3)</td></tr><tr><td>V2-99</td><td>-</td><td>COCO</td><td>2D Det.</td><td>40.8</td><td>34.0</td></tr><tr><td>V2-99</td><td>\\surd</td><td>COCO</td><td>2D Det.</td><td>43.1 (+2.3)</td><td>36.2(+2.2)</td></tr></tbody></table>", "caption": "TABLE V: We analyzed the relationship between the pretraining backbone and proposed the in-domain multi-task representation learning using depth supervision. We compare the same backbone training on COCO [58] on 2D detection. (Released by  [59].) The multi-task training paradigm is consistently improving over the detection-only case. It is also noticeable that geometry-aware backbones (pretrained on depth estimation) achieve more significant improvement than object-aware backbones (COCO).", "list_citation_info": ["[58] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in European conference on computer vision. Springer, 2014, pp. 740\u2013755.", "[59] Y. Lee and J. Park, \u201cCentermask: Real-time anchor-free instance segmentation,\u201d 2020."]}], "citation_info_to_title": {"[56] Z. Li, Z. Qu, Y. Zhou, J. Liu, H. Wang, and L. Jiang, \u201cDiversity matters: Fully exploiting depth clues for reliable monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 2791\u20132800.": "Diversity matters: Fully exploiting depth clues for reliable monocular 3d object detection", "[50] M. Ding, Y. Huo, H. Yi, Z. Wang, J. Shi, Z. Lu, and P. Luo, \u201cLearning depth-guided convolutions for monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 1000\u20131001.": "Learning depth-guided convolutions for monocular 3d object detection", "[26] X. Ma, Z. Wang, H. Li, P. Zhang, W. Ouyang, and X. Fan, \u201cAccurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6851\u20136860.": "Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving", "[59] Y. Lee and J. Park, \u201cCentermask: Real-time anchor-free instance segmentation,\u201d 2020.": "Centermask: Real-time anchor-free instance segmentation", "[27] Z. Liu, Z. Wu, and R. T\u00f3th, \u201cSmoke: single-stage monocular 3d object detection via keypoint estimation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 996\u2013997.": "Smoke: Single-Stage Monocular 3D Object Detection via Keypoint Estimation", "[52] Y. Zhou, Y. He, H. Zhu, C. Wang, H. Li, and Q. Jiang, \u201cMonocular 3d object detection: An extrinsic parameter free approach,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 7556\u20137566.": "Monocular 3d object detection: An extrinsic parameter free approach", "[44] J. Huang, G. Huang, Z. Zhu, and D. Du, \u201cBevdet: High-performance multi-camera 3d object detection in bird-eye-view,\u201d arXiv preprint arXiv:2112.11790, 2021.": "Bevdet: High-performance multi-camera 3d object detection in bird-eye-view", "[46] Y. Liu, T. Wang, X. Zhang, and J. Sun, \u201cPetr: Position embedding transformation for multi-view 3d object detection,\u201d arXiv preprint arXiv:2203.05625, 2022.": "Petr: Position Embedding Transformation for Multi-View 3D Object Detection", "[45] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, and J. Dai, \u201cBevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers,\u201d arXiv preprint arXiv:2203.17270, 2022.": "Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers", "[48] Y. Chen, L. Tai, K. Sun, and M. Li, \u201cMonopair: Monocular 3d object detection using pairwise spatial relationships,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009093\u201312\u2009102.": "Monopair: Monocular 3d object detection using pairwise spatial relationships", "[30] C. Reading, A. Harakeh, J. Chae, and S. L. Waslander, \u201cCategorical depth distributionnetwork for monocular 3d object detection,\u201d CVPR, 2021.": "Categorical depth distribution network for monocular 3D object detection", "[58] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in European conference on computer vision. Springer, 2014, pp. 740\u2013755.": "Microsoft coco: Common objects in context", "[3] T. Wang, X. Zhu, J. Pang, and D. Lin, \u201cFcos3d: Fully convolutional one-stage monocular 3d object detection,\u201d arXiv preprint arXiv:2104.10956, 2021.": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection", "[55] Y.-N. Chen, H. Dai, and Y. Ding, \u201cPseudo-stereo for monocular 3d object detection in autonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.": "Pseudo-stereo for monocular 3d object detection in autonomous driving", "[51] G. Brazil, G. Pons-Moll, X. Liu, and B. Schiele, \u201cKinematic 3d object detection in monocular video,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 135\u2013152.": "Kinematic 3d object detection in monocular video", "[47] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointpillars: Fast encoders for object detection from point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12\u2009697\u201312\u2009705.": "Pointpillars: Fast encoders for object detection from point clouds", "[5] A. Simonelli, S. R. Bul\u00f2, L. Porzi, P. Kontschieder, and E. Ricci, \u201cDemystifying pseudo-lidar for monocular 3d object detection,\u201d arXiv preprint arXiv:2012.05796, 2020.": "Demystifying pseudo-lidar for monocular 3d object detection", "[54] Y. Lu, X. Ma, L. Yang, T. Zhang, Y. Liu, Q. Chu, J. Yan, and W. Ouyang, \u201cGeometry uncertainty projection network for monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 3111\u20133121.": "Geometry uncertainty projection network for monocular 3d object detection", "[53] Y. Zhang, J. Lu, and J. Zhou, \u201cObjects are different: Flexible monocular 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3289\u20133298.": "Flexible Monocular 3D Object Detection", "[57] G. Brazil and X. Liu, \u201cM3d-rpn: Monocular 3d region proposal network for object detection,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9287\u20139296.": "M3d-rpn: Monocular 3d region proposal network for object detection", "[12] X. Ma, S. Liu, Z. Xia, H. Zhang, X. Zeng, and W. Ouyang, \u201cRethinking pseudo-lidar representation,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 311\u2013327.": "Rethinking pseudo-lidar representation", "[43] Y. Wang, V. Guizilini, T. Zhang, Y. Wang, H. Zhao, , and J. M. Solomon, \u201cDetr3d: 3d object detection from multi-view images via 3d-to-2d queries,\u201d in The Conference on Robot Learning (CoRL), 2021.": "Detr3d: 3d object detection from multi-view images via 3d-to-2d queries", "[42] T. Wang, Z. Xinge, J. Pang, and D. Lin, \u201cProbabilistic and geometric depth: Detecting objects in perspective,\u201d in Conference on Robot Learning. PMLR, 2022, pp. 1475\u20131485.": "Probabilistic and Geometric Depth: Detecting Objects in Perspective", "[1] D. Park, R. Ambrus, V. Guizilini, J. Li, and A. Gaidon, \u201cIs pseudo-lidar needed for monocular 3d object detection?\u201d in IEEE/CVF International Conference on Computer Vision (ICCV), 2021.": "Is pseudo-lidar needed for monocular 3d object detection?", "[23] J. Ku, A. D. Pon, and S. L. Waslander, \u201cMonocular 3d object detection leveraging accurate proposals and shape reconstruction,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 11\u2009867\u201311\u2009876.": "Monocular 3d object detection leveraging accurate proposals and shape reconstruction", "[49] J. M. U. Vianney, S. Aich, and B. Liu, \u201cRefinedmpl: Refined monocular pseudolidar for 3d object detection in autonomous driving,\u201d arXiv preprint arXiv:1911.09712, 2019.": "Refinedmpl: Refined Monocular Pseudolidar for 3D Object Detection in Autonomous Driving", "[33] K.-C. Huang, T.-H. Wu, H.-T. Su, and W. H. Hsu, \u201cMonodtr: Monocular 3d object detection with depth-aware transformer,\u201d in CVPR, 2022.": "Monodtr: Monocular 3d object detection with depth-aware transformer", "[40] A. Simonelli, S. R. Bulo, L. Porzi, M. L. Antequera, and P. Kontschieder, \u201cDisentangling monocular 3d object detection: From single to multi-class recognition,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.": "Disentangling Monocular 3D Object Detection: From Single to Multi-Class Recognition"}, "source_title_to_arxiv_id": {"Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers": "2203.17270", "Categorical depth distribution network for monocular 3D object detection": "2103.01100", "Detr3d: 3d object detection from multi-view images via 3d-to-2d queries": "2110.06922", "Monocular 3d object detection leveraging accurate proposals and shape reconstruction": "1904.01690"}}