{"title": "Real-time 3D Single Object Tracking with Transformer", "abstract": "LiDAR-based 3D single object tracking is a challenging issue in robotics and\nautonomous driving. Currently, existing approaches usually suffer from the\nproblem that objects at long distance often have very sparse or\npartially-occluded point clouds, which makes the features extracted by the\nmodel ambiguous. Ambiguous features will make it hard to locate the target\nobject and finally lead to bad tracking results. To solve this problem, we\nutilize the powerful Transformer architecture and propose a\nPoint-Track-Transformer (PTT) module for point cloud-based 3D single object\ntracking task. Specifically, PTT module generates fine-tuned attention features\nby computing attention weights, which guides the tracker focusing on the\nimportant features of the target and improves the tracking ability in complex\nscenarios. To evaluate our PTT module, we embed PTT into the dominant method\nand construct a novel 3D SOT tracker named PTT-Net. In PTT-Net, we embed PTT\ninto the voting stage and proposal generation stage, respectively. PTT module\nin the voting stage could model the interactions among point patches, which\nlearns context-dependent features. Meanwhile, PTT module in the proposal\ngeneration stage could capture the contextual information between object and\nbackground. We evaluate our PTT-Net on KITTI and NuScenes datasets.\nExperimental results demonstrate the effectiveness of PTT module and the\nsuperiority of PTT-Net, which surpasses the baseline by a noticeable margin,\n~10% in the Car category. Meanwhile, our method also has a significant\nperformance improvement in sparse scenarios. In general, the combination of\ntransformer and tracking pipeline enables our PTT-Net to achieve\nstate-of-the-art performance on both two datasets. Additionally, PTT-Net could\nrun in real-time at 40FPS on NVIDIA 1080Ti GPU. Our code is open-sourced for\nthe research community at https://github.com/shanjiayao/PTT.", "authors": ["Jiayao Shan", "Sifan Zhou", "Yubo Cui", "Zheng Fang"], "published_date": "2022_09_02", "pdf_url": "http://arxiv.org/pdf/2209.00860v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Module</th><th>Modality</th><td>3D Success</td><td>3D Precision</td><td>FPS</td></tr><tr><th>AVOD-Tracking[63]</th><th>RGB+LiDAR</th><td>63.1</td><td>69.7</td><td>-</td></tr><tr><th>F-Siamese[44]</th><th>RGB+LIDAR</th><td>37.1</td><td>50.6</td><td>-</td></tr><tr><th>SC3D[16]</th><th>LiDAR only</th><td>41.3</td><td>57.9</td><td>1.8</td></tr><tr><th>ETP2D-3D[42]</th><th>LiDAR only</th><td>36.3</td><td>51.0</td><td>-</td></tr><tr><th>P2B[17]</th><th>LiDAR only</th><td>56.2</td><td>72.8</td><td>45.5</td></tr><tr><th>3D-SiamRPN[19]</th><th>LiDAR only</th><td>58.2</td><td>76.2</td><td>20.8</td></tr><tr><th>PTT-Net(Ours)</th><th>LiDAR only</th><td>67.8</td><td>81.8</td><td>40.0</td></tr></tbody></table>", "caption": "TABLE I: Performance comparison on KITTI for the car category. Red and blue mean the performance score is ranked first and second respectively.", "list_citation_info": ["[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020.", "[63] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. Waslander, \u201cJoint 3d proposal generation and object detection from view aggregation,\u201d in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 1\u20138.", "[19] Z. Fang, S. Zhou, Y. Cui, and S. Scherer, \u201c3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud,\u201d IEEE Sensors Journal, vol. 21, no. 4, pp. 4995\u20135011, 2021.", "[44] H. Zou, J. Cui, X. Kong, C. Zhang, Y. Liu, F. Wen, and W. Li, \u201cF-siamese tracker: A frustum-based double siamese network for 3d single object tracking,\u201d IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8133\u20138139, 2020.", "[42] J. Zarzar, S. Giancola, and B. Ghanem, \u201cEfficient tracking proposals using 2d-3d siamese networks on lidar,\u201d arXiv preprint arXiv:1903.10168, 2019.", "[16] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1359\u20131368."]}, {"table": "<table><thead><tr><th></th><th>Dataset</th><th colspan=\"5\">KITTI</th><th colspan=\"5\">NuScenes</th></tr><tr><th></th><th>Category</th><th>Car</th><th>Pedestrian</th><th>Van</th><th>Cyclist</th><th>Mean</th><th>Car</th><th>Truck</th><th>Trailer</th><th>Bus</th><th>Mean</th></tr><tr><th></th><th>Frame Number</th><th>6424</th><th>6088</th><th>1248</th><th>308</th><th>14068</th><th>64159</th><th>13587</th><th>3352</th><th>2953</th><th>84051</th></tr></thead><tbody><tr><th rowspan=\"6\">3D Success</th><th>SC3D[16]</th><td>41.3</td><td>18.2</td><td>40.4</td><td>41.5</td><td>31.2</td><td>22.31</td><td>30.67</td><td>35.28</td><td>29.35</td><td>24.43</td></tr><tr><th>P2B[17]</th><td>56.2</td><td>28.7</td><td>40.8</td><td>32.1</td><td>42.4</td><td>38.81</td><td>42.95</td><td>48.96</td><td>32.95</td><td>39.68</td></tr><tr><th>F-Siamese[44]</th><td>37.1</td><td>16.2</td><td>-</td><td>47.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>3D-SiamRPN[19]</th><td>58.2</td><td>35.2</td><td>45.6</td><td>36.1</td><td>46.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BAT[64]</th><td>65.4</td><td>45.7</td><td>52.4</td><td>33.7</td><td>55.0</td><td>40.73</td><td>45.34</td><td>52.59</td><td>35.44</td><td>41.76</td></tr><tr><th>PTT-Net(Ours)</th><td>67.8</td><td>44.9</td><td>43.6</td><td>37.2</td><td>55.1</td><td>41.22</td><td>50.23</td><td>61.66</td><td>43.86</td><td>43.58</td></tr><tr><th rowspan=\"5\">3D Precision</th><th>SC3D[16]</th><td>57.9</td><td>37.8</td><td>47.0</td><td>70.4</td><td>48.5</td><td>21.93</td><td>27.73</td><td>28.12</td><td>24.08</td><td>23.19</td></tr><tr><th>P2B[17]</th><td>72.8</td><td>49.6</td><td>48.4</td><td>44.7</td><td>60.0</td><td>43.18</td><td>41.59</td><td>40.05</td><td>27.41</td><td>42.24</td></tr><tr><th>F-Siamese[44]</th><td>50.6</td><td>32.2</td><td>-</td><td>77.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>3D-SiamRPN[19]</th><td>76.2</td><td>56.2</td><td>52.8</td><td>49.0</td><td>64.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BAT[64]</th><td>78.9</td><td>74.5</td><td>67.0</td><td>45.4</td><td>75.2</td><td>43.29</td><td>42.58</td><td>44.89</td><td>28.01</td><td>42.70</td></tr><tr><th></th><th>PTT-Net(Ours)</th><td>81.8</td><td>72.0</td><td>52.5</td><td>47.3</td><td>74.2</td><td>45.26</td><td>48.56</td><td>56.05</td><td>39.96</td><td>46.04</td></tr></tbody></table>", "caption": "TABLE II: Extensive comparisons with different categories on KITTI(left) and NuScenes(right) dataset. Red and blue mean the performance score is ranked first and second respectively. And frame number indicates the instance number of each category.", "list_citation_info": ["[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020.", "[64] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, \u201cBox-aware feature enhancement for single object tracking on point clouds,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 13\u2009199\u201313\u2009208.", "[19] Z. Fang, S. Zhou, Y. Cui, and S. Scherer, \u201c3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud,\u201d IEEE Sensors Journal, vol. 21, no. 4, pp. 4995\u20135011, 2021.", "[44] H. Zou, J. Cui, X. Kong, C. Zhang, Y. Liu, F. Wen, and W. Li, \u201cF-siamese tracker: A frustum-based double siamese network for 3d single object tracking,\u201d IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8133\u20138139, 2020.", "[16] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1359\u20131368."]}, {"table": "<table><thead><tr><th>Source of</th><th colspan=\"4\">Success</th><th colspan=\"4\">Precision</th></tr><tr><th>template points</th><th><p>PTT-Net</p></th><th><p>3D-SiamRPN[19]</p></th><th><p>P2B[17]</p></th><th><p>SC3D [16]</p></th><th><p>PTT-Net</p></th><th><p>3D-SiamRPN[19]</p></th><th><p>P2B[17]</p></th><th><p>SC3D [16]</p></th></tr></thead><tbody><tr><th><p>The First GT</p></th><td>62.9</td><td><p>57.2</p></td><td><p>46.7</p></td><td><p>31.6</p></td><td>76.5</td><td><p>75.0</p></td><td><p>59.7</p></td><td><p>44.4</p></td></tr><tr><th><p>Previous result</p></th><td>64.9</td><td><p>-</p></td><td><p>53.1</p></td><td><p>25.7</p></td><td>77.5</td><td><p>-</p></td><td><p>68.9</p></td><td><p>35.1</p></td></tr><tr><th><p>First &amp; Preivous</p></th><td>67.8</td><td><p>58.2</p></td><td><p>56.2</p></td><td><p>34.9</p></td><td>81.8</td><td><p>76.2</p></td><td><p>72.8</p></td><td><p>49.8</p></td></tr><tr><th><p>All previous</p></th><td>59.8</td><td><p>-</p></td><td><p>51.4</p></td><td><p>41.3</p></td><td>74.5</td><td><p>-</p></td><td><p>66.8</p></td><td><p>57.9</p></td></tr></tbody></table>", "caption": "TABLE III: Different ways for template generation. \u201cFirst &amp; Previous\" denotes \u201cThe first ground truth (GT) and Previous result\". ", "list_citation_info": ["[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020.", "[19] Z. Fang, S. Zhou, Y. Cui, and S. Scherer, \u201c3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud,\u201d IEEE Sensors Journal, vol. 21, no. 4, pp. 4995\u20135011, 2021.", "[16] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1359\u20131368."]}, {"table": "<table><tbody><tr><td rowspan=\"2\"></td><th colspan=\"3\">Success</th><th colspan=\"3\">Precesion</th></tr><tr><th><p>SC3D[16]</p></th><th><p>P2B[17]</p></th><th><p>PTT-Net(Ours)</p></th><th><p>SC3D[16]</p></th><th><p>P2B[17]</p></th><th><p>PTT-Net(Ours)</p></th></tr><tr><td><p>Previous Result</p></td><td><p>41.3</p></td><td><p>56.2</p></td><td>67.8</td><td><p>57.9</p></td><td><p>72.8</p></td><td>81.8</td></tr><tr><td><p>Previous GT</p></td><td><p>64.6</p></td><td>82.4</td><td><p>75.9</p></td><td><p>74.5</p></td><td>90.1</td><td><p>88.9</p></td></tr><tr><td><p>Current GT</p></td><td><p>76.9</p></td><td>84.0</td><td><p>76.1</p></td><td><p>81.3</p></td><td>90.3</td><td><p>89.1</p></td></tr></tbody></table>", "caption": "TABLE IV: Different ways for search area generation. ", "list_citation_info": ["[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020.", "[16] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1359\u20131368."]}, {"table": "<table><thead><tr><th> Ablation</th><th>3D Success</th><th>3D Precision</th></tr></thead><tbody><tr><td>baseline[17]</td><td>56.2</td><td>72.8</td></tr><tr><td>Only PTT in Vote</td><td>62.1</td><td>76.9</td></tr><tr><td>Only PTT in Prop</td><td>65.7</td><td>78.9</td></tr><tr><td>PTT in all (PTT-Net)</td><td>67.8</td><td>81.8</td></tr></tbody></table>", "caption": "TABLE VI: Different embedded positions of PTT module.", "list_citation_info": ["[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020."]}], "citation_info_to_title": {"[44] H. Zou, J. Cui, X. Kong, C. Zhang, Y. Liu, F. Wen, and W. Li, \u201cF-siamese tracker: A frustum-based double siamese network for 3d single object tracking,\u201d IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 8133\u20138139, 2020.": "F-siamese tracker: A frustum-based double siamese network for 3d single object tracking", "[63] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. Waslander, \u201cJoint 3d proposal generation and object detection from view aggregation,\u201d in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 1\u20138.": "Joint 3d proposal generation and object detection from view aggregation", "[42] J. Zarzar, S. Giancola, and B. Ghanem, \u201cEfficient tracking proposals using 2d-3d siamese networks on lidar,\u201d arXiv preprint arXiv:1903.10168, 2019.": "Efficient tracking proposals using 2d-3d siamese networks on lidar", "[19] Z. Fang, S. Zhou, Y. Cui, and S. Scherer, \u201c3d-siamrpn: An end-to-end learning method for real-time 3d single object tracking using raw point cloud,\u201d IEEE Sensors Journal, vol. 21, no. 4, pp. 4995\u20135011, 2021.": "3D-SiamRPN: An End-to-End Learning Method for Real-Time 3D Single Object Tracking Using Raw Point Cloud", "[16] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1359\u20131368.": "Leveraging shape completion for 3d siamese tracking", "[64] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, \u201cBox-aware feature enhancement for single object tracking on point clouds,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 13\u2009199\u201313\u2009208.": "Box-aware feature enhancement for single object tracking on point clouds", "[17] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6328\u20136337, 2020.": "P2b: Point-to-box network for 3d object tracking in point clouds"}, "source_title_to_arxiv_id": {"Joint 3d proposal generation and object detection from view aggregation": "1712.02294"}}