{"title": "MAFormer: A Transformer Network with Multi-scale Attention Fusion for Visual Recognition", "abstract": "Vision Transformer and its variants have demonstrated great potential in\nvarious computer vision tasks. But conventional vision transformers often focus\non global dependency at a coarse level, which suffer from a learning challenge\non global relationships and fine-grained representation at a token level. In\nthis paper, we introduce Multi-scale Attention Fusion into transformer\n(MAFormer), which explores local aggregation and global feature extraction in a\ndual-stream framework for visual recognition. We develop a simple but effective\nmodule to explore the full potential of transformers for visual representation\nby learning fine-grained and coarse-grained features at a token level and\ndynamically fusing them. Our Multi-scale Attention Fusion (MAF) block consists\nof: i) a local window attention branch that learns short-range interactions\nwithin windows, aggregating fine-grained local features; ii) global feature\nextraction through a novel Global Learning with Down-sampling (GLD) operation\nto efficiently capture long-range context information within the whole image;\niii) a fusion module that self-explores the integration of both features via\nattention. Our MAFormer achieves state-of-the-art performance on common vision\ntasks. In particular, MAFormer-L achieves 85.9$\\%$ Top-1 accuracy on ImageNet,\nsurpassing CSWin-B and LV-ViT-L by 1.7$\\%$ and 0.6$\\%$ respectively. On MSCOCO,\nMAFormer outperforms the prior art CSWin by 1.7$\\%$ mAPs on object detection\nand 1.4$\\%$ on instance segmentation with similar-sized parameters,\ndemonstrating the potential to be a general backbone network.", "authors": ["Yunhao Wang", "Huixin Sun", "Xiaodi Wang", "Bin Zhang", "Chao Li", "Ying Xin", "Baochang Zhang", "Errui Ding", "Shumin Han"], "published_date": "2022_08_31", "pdf_url": "http://arxiv.org/pdf/2209.01620v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>Params</th><th>Attention in Local Aggregation</th><th>Global Feature Extraction</th><th>Top1 (%)</th></tr></thead><tbody><tr><td>Swin-T</td><td>29M</td><td>Shifted Window liu2021swin </td><td>None</td><td>81.3</td></tr><tr><td>CSWin-T</td><td>23M</td><td>Cross-shaped dong2021CSWin </td><td>None</td><td>82.7</td></tr><tr><td>MAFormer-S</td><td>23M</td><td>Shifted Window liu2021swin </td><td>GLD</td><td>83.4</td></tr><tr><td>MAFormer-S</td><td>23M</td><td>Cross-shaped dong2021CSWin </td><td>Convolution</td><td>83.4</td></tr><tr><td>MAFormer-S</td><td>23M</td><td>Cross-shaped dong2021CSWin </td><td>GLD</td><td>83.7</td></tr></tbody></table>", "caption": "Table 2: Ablation study of different local aggregation and global feature representation modules.", "list_citation_info": ["(10) X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "(24) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021."]}, {"table": "<table><thead><tr><th>Framework</th><th>Params(M)</th><th>Dual Stream Design</th><th>Top1 (%)</th></tr></thead><tbody><tr><td>DS-Net mao2021dual </td><td>23M</td><td>Co-Attention from Convolution and Self-attention</td><td>82.3</td></tr><tr><td>MAFormer-S</td><td>23M</td><td>Local-Enhanced Fusion Attention</td><td>83.5</td></tr><tr><td>MAFormer-S</td><td>23M</td><td>Multi-scale Fusion Attention</td><td>83.7</td></tr></tbody></table>", "caption": "Table 3: Accuracy of MAFormer-S using different structure design.", "list_citation_info": ["(27) M. Mao, R. Zhang, H. Zheng, T. Ma, Y. Peng, E. Ding, B. Zhang, S. Han, et al. Dual-stream network for visual recognition. Advances in Neural Information Processing Systems, 34, 2021."]}, {"table": "<table><thead><tr><th>Models</th><th>Train Size</th><th>Test Size</th><th>Params(M)</th><th>FLOPs(G)</th><th>Top1(\\%)</th></tr></thead><tbody><tr><th>DeiT-S  2020Training </th><td>224^{2}</td><td>224^{2}</td><td>22</td><td>4.6</td><td>79.8</td></tr><tr><th>Swin-T  liu2021swin </th><td>224^{2}</td><td>224^{2}</td><td>29</td><td>4.5</td><td>81.3</td></tr><tr><th>CrossViT-15  2021CrossViT </th><td>224^{2}</td><td>224^{2}</td><td>27</td><td>5.8</td><td>81.5</td></tr><tr><th>CoAtNet-0  dai2021coatnet </th><td>224^{2}</td><td>224^{2}</td><td>25</td><td>4.6</td><td>81.6</td></tr><tr><th>Focal-T  2021Focal </th><td>224^{2}</td><td>224^{2}</td><td>29</td><td>4.9</td><td>82.2</td></tr><tr><th>DS-Net-S  mao2021dual </th><td>224^{2}</td><td>224^{2}</td><td>23</td><td>3.5</td><td>82.3</td></tr><tr><th>Shuffle-T  huang2021shuffle </th><td>224^{2}</td><td>224^{2}</td><td>29</td><td>4.6</td><td>82.5</td></tr><tr><th>CSWin-T  dong2021CSWin </th><td>224^{2}</td><td>224^{2}</td><td>23</td><td>4.3</td><td>82.7</td></tr><tr><th>MAFormer-S</th><td>224^{2}</td><td>224^{2}</td><td>23</td><td>4.5</td><td>83.0</td></tr><tr><th>LV-ViT-S\\dagger  jiang2021all </th><td>224^{2}</td><td>224^{2}</td><td>26</td><td>6.6</td><td>83.3</td></tr><tr><th>MAFormer-S\\dagger</th><td>224^{2}</td><td>224^{2}</td><td>23</td><td>4.5</td><td>83.7</td></tr><tr><th>CrossViT-18  2021CrossViT </th><td>224^{2}</td><td>224^{2}</td><td>44</td><td>9.5</td><td>82.8</td></tr><tr><th>Swin-S  liu2021swin </th><td>224^{2}</td><td>224^{2}</td><td>50</td><td>8.7</td><td>83.0</td></tr><tr><th>DS-Net-B  mao2021dual </th><td>224^{2}</td><td>224^{2}</td><td>49</td><td>8.4</td><td>83.1</td></tr><tr><th>Twins-SVT-B  chu2021twins </th><td>224^{2}</td><td>224^{2}</td><td>56</td><td>8.3</td><td>83.2</td></tr><tr><th>CoAtNet-1  dai2021coatnet </th><td>224^{2}</td><td>224^{2}</td><td>42</td><td>8.4</td><td>83.3</td></tr><tr><th>Shuffle-S  huang2021shuffle </th><td>224^{2}</td><td>224^{2}</td><td>50</td><td>8.9</td><td>83.5</td></tr><tr><th>Focal-S  2021Focal </th><td>224^{2}</td><td>224^{2}</td><td>51</td><td>9.1</td><td>83.5</td></tr><tr><th>CSWin-S  dong2021CSWin </th><td>224^{2}</td><td>224^{2}</td><td>35</td><td>8.9</td><td>83.6</td></tr><tr><th>LV-ViT-M\\dagger  jiang2021all </th><td>224^{2}</td><td>224^{2}</td><td>56</td><td>16</td><td>84.1</td></tr><tr><th>MAFormer-B\\dagger</th><td>224^{2}</td><td>224^{2}</td><td>53</td><td>9.8</td><td>85.0</td></tr><tr><th>DeiT-B  2020Training </th><td>224^{2}</td><td>224^{2}</td><td>86</td><td>17.5</td><td>81.8</td></tr><tr><th>CrossViT-B  2021CrossViT </th><td>224^{2}</td><td>224^{2}</td><td>105</td><td>21.2</td><td>82.2</td></tr><tr><th>Swin-B  liu2021swin </th><td>224^{2}</td><td>224^{2}</td><td>88</td><td>15.4</td><td>83.5</td></tr><tr><th>Focal-B  2021Focal </th><td>224^{2}</td><td>224^{2}</td><td>90</td><td>16.0</td><td>83.8</td></tr><tr><th>Shuffle-B  huang2021shuffle </th><td>224^{2}</td><td>224^{2}</td><td>88</td><td>15.6</td><td>84</td></tr><tr><th>CSWin-B  dong2021CSWin </th><td>224^{2}</td><td>224^{2}</td><td>78</td><td>15.0</td><td>84.2</td></tr><tr><th>CoAtNet-3  dai2021coatnet </th><td>224^{2}</td><td>224^{2}</td><td>168</td><td>34.7</td><td>84.5</td></tr><tr><th>CaiT-M36  2021Going </th><td>224^{2}</td><td>384^{2}</td><td>271</td><td>247.8</td><td>85.1</td></tr><tr><th>LV-ViT-L\\dagger  jiang2021all </th><td>288^{2}</td><td>288^{2}</td><td>150</td><td>59.0</td><td>85.3</td></tr><tr><th>MAFormer-L\\dagger</th><td>224^{2}</td><td>224^{2}</td><td>105</td><td>22.6</td><td>85.9</td></tr></tbody></table>", "caption": "Table 4: Comparison with the state-of-the-art on ImageNet-1K. \\dagger indicates with Token Labeling jiang2021all .", "list_citation_info": ["(24) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.", "(36) H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.", "(10) X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "(47) J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.", "(16) Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021.", "(27) M. Mao, R. Zhang, H. Zheng, T. Ma, Y. Peng, E. Ding, B. Zhang, S. Han, et al. Dual-stream network for visual recognition. Advances in Neural Information Processing Systems, 34, 2021.", "(18) Z.-H. Jiang, Q. Hou, L. Yuan, D. Zhou, Y. Shi, X. Jin, A. Wang, and J. Feng. All tokens matter: Token labeling for training better vision transformers. Advances in Neural Information Processing Systems, 34, 2021.", "(6) X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting spatial attention design in vision transformers. arXiv e-prints, 2021.", "(37) H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32\u201342, 2021.", "(8) Z. Dai, H. Liu, Q. Le, and M. Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34, 2021.", "(3) C.-F. R. Chen, Q. Fan, and R. Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pages 357\u2013366, 2021."]}, {"table": "\\resizebox<p>100mm!BackboneParamsFLOPsMask R-CNN 1x schedule(M)(G)AP^{b}AP^{b}_{50}AP^{b}_{75}AP^{m}AP^{m}_{50}AP^{m}_{75}Res50  he2016deep 4426038.058.641.434.455.136.7PVT-S  wang2021pyramid 4424540.462.943.837.860.140.3ViL-S  zhang2021multi 4521844.967.149.341.64.244.1TwinsP-S  chu2021twins 4424542.965.847.140.462.742.9Twins-S  chu2021twins 4422843.466.047.340.363.243.4Swin-T  liu2021swin 4826442.264.646.239.164.642.0CSWin-T  dong2021CSWin 4227946.768.651.342.265.645.4MAFormer-S4125647.069.551.642.766.546.1Res101  he2016deep 6333640.461.144.236.457.738.8X101-32  xie2017aggregated 6334041.962.545.937.559.440.2PVT-M  wang2021pyramid 6430242.064.445.639.061.642.1ViL-M  zhang2021multi 6026143.4\u2013\u201339.7\u2013\u2013TwinsP-B  chu2021twins 6430244.666.748.940.963.844.2Twins-B  chu2021twins 7634045.267.649.341.564.544.8Swin-S  liu2021swin 6935444.866.648.940.963.444.2CSWin-S  dong2021CSWin 5434247.970.152.643.267.146.2MAFormer-B7135449.671.454.744.668.648.4X101-64  xie2017aggregated 10149342.863.847.338.460.641.3PVT-L  wang2021pyramid 8136442.965.046.639.561.942.5ViL-B  zhang2021multi 7636545.1\u2013\u201341.0\u2013\u2013TwinsP-L  chu2021twins 8136445.4\u2013\u201341.5\u2013\u2013Twins-L  chu2021twins 11147445.9\u2013\u201341.6\u2013\u2013Swin-B  liu2021swin 10749646.9\u2013\u201342.3\u2013\u2013CSWin-B  dong2021CSWin 9752648.770.453.943.967.847.3MAFormer-L12260950.772.455.645.469.749.2</p>", "caption": "Table 5: Object detection and instance segmentation performance on the COCO val2017 with the Mask R-CNN framework. The FLOPs (G) are measured at resolution 800x1280, and the models are pretrained on the ImageNet-1K.", "list_citation_info": ["(13) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "(24) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.", "(41) W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.", "(10) X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "(45) S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.", "(53) P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.", "(6) X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting spatial attention design in vision transformers. arXiv e-prints, 2021."]}, {"table": "\\resizebox<p>100mm!BackboneParamsFLOPsCascade R-CNN 3x schedule(M)(G)AP^{b}AP^{b}_{50}AP^{b}_{75}AP^{m}AP^{m}_{50}AP^{m}_{75}Res50  he2016deep 8273946.364.350.540.161.743.4Swin-T  liu2021swin 8674550.569.354.943.766.647.1CSWin-T  dong2021CSWin 8075752.571.557.145.368.848.9MAFormer-S8073352.671.357.345.768.949.8X101-32  xie2017aggregated 10181948.166.552.441.663.945.2Swin-S  liu2021swin 10783851.870.456.344.767.948.5CSWin-S  dong2021CSWin 9282053.772.258.446.469.650.6MAFormer-B10983354.472.859.246.870.451.0X101-64  xie2017aggregated 14097248.366.452.341.764.045.1Swin-B  liu2021swin 14598251.970.956.545.068.448.7CSWin-B  dong2021CSWin 135100553.972.658.546.470.050.4MAFormer-L160108854.773.259.447.371.251.3</p>", "caption": "Table 6: Object detection and instance segmentation performance on the COCO val2017 with the Cascade R-CNN framework. The FLOPs (G) are measured at resolution 800x1280, and the models are pretrained on the ImageNet-1K.", "list_citation_info": ["(13) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "(10) X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "(24) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.", "(45) S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017."]}, {"table": "\\resizebox<p>120mm!ModelsSemantic FPN 80KUPerNet 160k\\#Params(M)FLOPs(G)mIoU(\\%)\\#Params(M)FLOPs(G)mIoU(\\%)MS mIoU(\\%)Res50 he2016deep 2918336.7----Twins-S  2021Twins 2814443.25490146.247.1TwinsP-S  2021Twins 2816244.35591946.247.5Swin-T  liu2021swin 3218241.56094544.545.8Focal-T  2021Focal ---6299845.847.0Shuffle-T  huang2021shuffle ---6094946.647.6MAFormer-S2817047.95292948.348.6Res101 he2016deep 4826038.8861029-44.9TwinsP-B  2021Twins 4822044.97497747.148.4Twins-B  2021Twins 6026145.389102047.748.9Swin-S  liu2021swin 5327445.281103847.649.5Focal-S  2021Focal ---85113048.050.0Shuffle-S  huang2021shuffle ---81104448.449.6Swin-B  liu2021swin 9144246.0121118848.149.1MAFormer-B5527449.882103151.151.6</p>", "caption": "Table 7: Comparison with previous best results on ADE20K semantic segmentation. UPerNet: learning rate of 6 \u00d7 10{{}^{-5}}, a weight decay of 0.01, a scheduler that uses linear learning rate decay, and a linear warmup of 1,500 iterations. Semantic FPN: learning rate of 2 \u00d7 10{{}^{-4}}, a weight decay of 1 \u00d7 10{{}^{-4}}, a scheduler that uses Cosine Annealing learning rate decay, and a linear warmup of 1,000 iterations. The FLOPs are measured at resolution 2048\u00d7512.", "list_citation_info": ["(13) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "(24) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.", "(47) J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.", "(16) Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021.", "(7) X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34:9355\u20139366, 2021."]}], "citation_info_to_title": {"(41) W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions", "(8) Z. Dai, H. Liu, Q. Le, and M. Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34, 2021.": "Coatnet: Marrying Convolution and Attention for All Data Sizes", "(27) M. Mao, R. Zhang, H. Zheng, T. Ma, Y. Peng, E. Ding, B. Zhang, S. Han, et al. Dual-stream network for visual recognition. Advances in Neural Information Processing Systems, 34, 2021.": "Dual-stream network for visual recognition", "(7) X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34:9355\u20139366, 2021.": "Twins: Revisiting the design of spatial attention in vision transformers", "(24) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "(16) Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu. Shuffle transformer: Rethinking spatial shuffle for vision transformer. arXiv preprint arXiv:2106.03650, 2021.": "Shuffle transformer: Rethinking spatial shuffle for vision transformer", "(18) Z.-H. Jiang, Q. Hou, L. Yuan, D. Zhou, Y. Shi, X. Jin, A. Wang, and J. Feng. All tokens matter: Token labeling for training better vision transformers. Advances in Neural Information Processing Systems, 34, 2021.": "All tokens matter: Token labeling for training better vision transformers", "(37) H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32\u201342, 2021.": "Going deeper with image transformers", "(6) X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen. Twins: Revisiting spatial attention design in vision transformers. arXiv e-prints, 2021.": "Twins: Revisiting spatial attention design in vision transformers", "(53) P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.": "Multi-scale vision longformer: A new vision transformer for high-resolution image encoding", "(47) J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.": "Focal self-attention for local-global interactions in vision transformers", "(13) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.": "Deep Residual Learning for Image Recognition", "(36) H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021.": "Training Data-Efficient Image Transformers & Distillation Through Attention", "(10) X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows", "(3) C.-F. R. Chen, Q. Fan, and R. Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pages 357\u2013366, 2021.": "Crossvit: Cross-attention multi-scale vision transformer for image classification", "(45) S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.": "Aggregated residual transformations for deep neural networks"}, "source_title_to_arxiv_id": {"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions": "2102.12122", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "All tokens matter: Token labeling for training better vision transformers": "2104.10858"}}