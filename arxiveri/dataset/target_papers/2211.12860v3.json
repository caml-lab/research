{"title": "DETRs with Collaborative Hybrid Assignments Training", "abstract": "In this paper, we provide the observation that too few queries assigned as\npositive samples in DETR with one-to-one set matching leads to sparse\nsupervisions on the encoder's output which considerably hurt the discriminative\nfeature learning of the encoder and vice visa for attention learning in the\ndecoder. To alleviate this, we present a novel collaborative hybrid assignments\ntraining scheme, namely Co-DETR, to learn more efficient and effective\nDETR-based detectors from versatile label assignment manners. This new training\nscheme can easily enhance the encoder's learning ability in end-to-end\ndetectors by training the multiple parallel auxiliary heads supervised by\none-to-many label assignments such as ATSS, FCOS, and Faster RCNN. In addition,\nwe conduct extra customized positive queries by extracting the positive\ncoordinates from these auxiliary heads to improve the training efficiency of\npositive samples in the decoder. In inference, these auxiliary heads are\ndiscarded and thus our method introduces no additional parameters and\ncomputational cost to the original detector while requiring no hand-crafted\nnon-maximum suppression (NMS). We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach on DETR variants, including DAB-DETR,\nDeformable-DETR, and DINO-Deformable-DETR. Specifically, we improve the basic\nDeformable-DETR by 5.8% in 12-epoch training and 3.2% in 36-epoch training. The\nstate-of-the-art DINO-Deformable-DETR with Swin-L can still be improved from\n58.5% to 59.5%. Surprisingly, incorporated with the large-scale backbone\nMixMIM-g with 1-Billion parameters, we achieve the 64.5% mAP on MS COCO\ntest-dev, achieving superior performance with much fewer extra data sizes.\nCodes will be available at https://github.com/Sense-X/Co-DETR.", "authors": ["Zhuofan Zong", "Guanglu Song", "Yu Liu"], "published_date": "2022_11_22", "pdf_url": "http://arxiv.org/pdf/2211.12860v3", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\"> Head i</td><td rowspan=\"2\">Loss \\mathcal{L}_{i}</td><td colspan=\"3\">Assignment \\mathcal{A}_{i}</td></tr><tr><td>{\\{pos\\}}, {\\{neg\\}} Generation</td><td>\\mathbf{P}_{i} Generation</td><td>\\mathbf{B}_{i}^{\\{pos\\}} Generation</td></tr><tr><td rowspan=\"2\">Faster-RCNN [24]</td><td>cls: CE loss,</td><td>\\{pos\\}: IoU(proposal, gt)&gt;0.5</td><td>\\{pos\\}: gt labels, offset(proposal, gt)</td><td>positive proposals</td></tr><tr><td>reg: GIoU loss</td><td>\\{reg\\}: IoU(proposal, gt)&lt;0.5</td><td>\\{neg\\}: gt labels</td><td>(x_{1},y_{1},x_{2},y_{2})</td></tr><tr><td rowspan=\"2\">ATSS [34]</td><td>cls: Focal loss</td><td>\\{pos\\}:IoU(anchor, gt)&gt;(mean+std)</td><td>\\{pos\\}: gt labels, offset(anchor, gt), centerness</td><td>positive anchors</td></tr><tr><td>reg: GIoU, BCE loss</td><td>\\{neg\\}: IoU(anchor, gt)&lt;(mean+std)</td><td>\\{neg\\}: gt labels</td><td>(x_{1},y_{1},x_{2},y_{2})</td></tr><tr><td rowspan=\"2\">RetinaNet [17]</td><td>cls: Focal loss</td><td>\\{pos\\}: IoU(anchor, gt)&gt;0.5</td><td>\\{pos\\}: gt labels, offset(anchor, gt)</td><td>positive anchors</td></tr><tr><td>reg: GIoU Loss</td><td>\\{neg\\}: IoU(anchor, gt)&lt;0.4</td><td>\\{neg\\}: gt labels</td><td>(x_{1},y_{1},x_{2},y_{2})</td></tr><tr><td rowspan=\"2\">FCOS [28]</td><td>cls: Focal Loss</td><td>\\{pos\\}: points inside gt center area</td><td>\\{pos\\}: gt labels, ltrb distance, centerness</td><td>FCOS point (cx,cy)</td></tr><tr><td>reg: GIoU, BCE loss</td><td>\\{neg\\}: points outside gt center area</td><td>\\{neg\\}: gt labels</td><td>w=h=8\\times 2^{2+j}</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Detailed information of auxiliary heads. The auxiliary heads include Faster-RCNN [24], ATSS [34], RetinaNet [17], and FCOS [28]. If not otherwise specified, we follow the original implementations, e.g., anchor generation.", "list_citation_info": ["[28] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627\u20139636, 2019.", "[34] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759\u20139768, 2020.", "[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.", "[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015."]}, {"table": "<table><tr><td>Method</td><td>K</td><td>#epochs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><td>Conditional DETR-C5 [23]</td><td>0</td><td>36</td><td>39.4</td><td>60.2</td><td>41.6</td></tr><tr><td>Conditional DETR-C5 [23]</td><td>1</td><td>36</td><td>41.5\\mathbf{(+2.1)}</td><td>61.7</td><td>44.0</td></tr><tr><td>Conditional DETR-C5 [23]</td><td>2</td><td>36</td><td>41.8\\mathbf{(+2.4)}</td><td>61.9</td><td>44.0</td></tr><tr><td>DAB-DETR-C5 [20]</td><td>0</td><td>36</td><td>41.2</td><td>62.0</td><td>43.9</td></tr><tr><td>DAB-DETR-C5 [20]</td><td>1</td><td>36</td><td>43.1\\mathbf{(+1.9)}</td><td>63.7</td><td>45.7</td></tr><tr><td>DAB-DETR-C5 [20]</td><td>2</td><td>36</td><td>43.5\\mathbf{(+2.3)}</td><td>64.2</td><td>46.3</td></tr><tr><td>Deformable-DETR [35]</td><td>0</td><td>12</td><td>37.1</td><td>55.5</td><td>40.0</td></tr><tr><td>Deformable-DETR [35]</td><td>1</td><td>12</td><td>42.3\\mathbf{(+5.2)}</td><td>60.5</td><td>46.1</td></tr><tr><td>Deformable-DETR [35]</td><td>2</td><td>12</td><td>42.9\\mathbf{(+5.8)}</td><td>61.3</td><td>47.0</td></tr><tr><td>Deformable-DETR [35]</td><td>0</td><td>36</td><td>43.3</td><td>62.3</td><td>47.1</td></tr><tr><td>Deformable-DETR [35]</td><td>1</td><td>36</td><td>46.8\\mathbf{(+3.5)}</td><td>65.1</td><td>51.5</td></tr><tr><td>Deformable-DETR [35]</td><td>2</td><td>36</td><td>46.5\\mathbf{(+3.2)}</td><td>65.0</td><td>50.8</td></tr><tr><td>Deformable-DETR++ [35]</td><td>0</td><td>12</td><td>47.1</td><td>65.4</td><td>51.2</td></tr><tr><td>Deformable-DETR++ [35]</td><td>1</td><td>12</td><td>48.7\\mathbf{(+1.6)}</td><td>66.6</td><td>53.2</td></tr><tr><td>Deformable-DETR++ [35]</td><td>2</td><td>12</td><td>49.5\\mathbf{(+2.4)}</td><td>67.6</td><td>54.3</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>0</td><td>12</td><td>48.4</td><td>66.2</td><td>52.7</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>1</td><td>12</td><td>49.2\\mathbf{(+0.8)}</td><td>67.1</td><td>54.1</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>2</td><td>12</td><td>49.7\\mathbf{(+1.3)}</td><td>67.5</td><td>54.4</td></tr></table>", "caption": "Table 2: Object detection results on COCO val. All results of baselines are reproduced using mmdetection.", "list_citation_info": ["[35] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[20] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.", "[23] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3651\u20133660, 2021.", "[12] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. arXiv preprint arXiv:2207.13080, 2022."]}, {"table": "<table><tr><td>Method</td><td>Backbone</td><td>Multi-scale</td><td>#query</td><td>#epochs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><td>Conditional-DETR [23]</td><td>R50</td><td>\u2717</td><td>300</td><td>108</td><td>43.0</td><td>64.0</td><td>45.7</td><td>22.7</td><td>46.7</td><td>61.5</td></tr><tr><td>Anchor-DETR [30]</td><td>R50</td><td>\u2717</td><td>300</td><td>50</td><td>42.1</td><td>63.1</td><td>44.9</td><td>22.3</td><td>46.2</td><td>60.0</td></tr><tr><td>DAB-DETR [20]</td><td>R50</td><td>\u2717</td><td>900</td><td>50</td><td>45.7</td><td>66.2</td><td>49.0</td><td>26.1</td><td>49.4</td><td>63.1</td></tr><tr><td>AdaMixer [7]</td><td>R50</td><td>\u2713</td><td>300</td><td>36</td><td>47.0</td><td>66.0</td><td>51.1</td><td>30.1</td><td>50.2</td><td>61.8</td></tr><tr><td>AdaMixer [7]</td><td>R101</td><td>\u2713</td><td>300</td><td>36</td><td>48.0</td><td>67.0</td><td>52.4</td><td>30.0</td><td>51.2</td><td>63.7</td></tr><tr><td>AdaMixer [7]</td><td>Swin-S</td><td>\u2713</td><td>300</td><td>36</td><td>51.3</td><td>71.2</td><td>55.7</td><td>34.2</td><td>54.6</td><td>67.3</td></tr><tr><td>Deformable-DETR [35]</td><td>R50</td><td>\u2713</td><td>300</td><td>50</td><td>46.9</td><td>65.6</td><td>51.0</td><td>29.6</td><td>50.1</td><td>61.6</td></tr><tr><td>DN-Deformable-DETR [14]</td><td>R50</td><td>\u2713</td><td>300</td><td>50</td><td>48.6</td><td>67.4</td><td>52.7</td><td>31.0</td><td>52.0</td><td>63.7</td></tr><tr><td>DINO-Deformable-DETR{}^{\\dagger} [33]</td><td>R50</td><td>\u2713</td><td>900</td><td>12</td><td>47.9</td><td>65.3</td><td>52.1</td><td>31.2</td><td>50.9</td><td>61.9</td></tr><tr><td>DINO-Deformable-DETR{}^{\\dagger} [33]</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>58.0</td><td>76.7</td><td>63.4</td><td>41.3</td><td>61.9</td><td>73.7</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>R50</td><td>\u2713</td><td>300</td><td>12</td><td>48.7</td><td>66.4</td><td>52.9</td><td>31.2</td><td>51.5</td><td>63.5</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>R101</td><td>\u2713</td><td>300</td><td>12</td><td>49.4</td><td>67.2</td><td>53.7</td><td>31.9</td><td>53.1</td><td>64.2</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>Swin-T</td><td>\u2713</td><td>300</td><td>36</td><td>53.2</td><td>71.5</td><td>58.2</td><td>35.9</td><td>56.4</td><td>68.2</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>Swin-S</td><td>\u2713</td><td>300</td><td>36</td><td>54.4</td><td>72.9</td><td>59.4</td><td>36.9</td><td>58.3</td><td>69.5</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>300</td><td>36</td><td>57.1</td><td>76.2</td><td>62.5</td><td>39.7</td><td>61.4</td><td>73.4</td></tr><tr><td>\\mathcal{H}-Deformable-DETR{}^{\\dagger}{}^{\\ddagger} [12]</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>57.9</td><td>76.8</td><td>63.6</td><td>42.4</td><td>61.9</td><td>73.4</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>R50</td><td>\u2713</td><td>300</td><td>12</td><td>49.5</td><td>67.6</td><td>54.3</td><td>32.4</td><td>52.7</td><td>63.7</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>R101</td><td>\u2713</td><td>300</td><td>12</td><td>50.1</td><td>67.9</td><td>54.9</td><td>33.0</td><td>53.9</td><td>64.6</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-B (IN-22K)</td><td>\u2713</td><td>300</td><td>12</td><td>55.5</td><td>74.1</td><td>60.7</td><td>37.9</td><td>59.6</td><td>72.2</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>300</td><td>12</td><td>56.9</td><td>75.5</td><td>62.6</td><td>40.1</td><td>61.2</td><td>73.3</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-T</td><td>\u2713</td><td>300</td><td>36</td><td>54.1</td><td>72.4</td><td>59.0</td><td>37.8</td><td>57.3</td><td>68.7</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-S</td><td>\u2713</td><td>300</td><td>36</td><td>55.3</td><td>73.6</td><td>60.9</td><td>39.0</td><td>59.0</td><td>70.1</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-B (IN-22K)</td><td>\u2713</td><td>300</td><td>36</td><td>57.5</td><td>76.0</td><td>63.4</td><td>41.3</td><td>61.7</td><td>73.2</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>58.3</td><td>76.7</td><td>64.3</td><td>42.1</td><td>62.2</td><td>73.9</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR{}^{\\dagger}</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>\\mathbf{58.5}</td><td>\\mathbf{77.1}</td><td>\\mathbf{64.5}</td><td>\\mathbf{42.4}</td><td>\\mathbf{62.4}</td><td>\\mathbf{74.0}</td></tr><tr><td>\\mathcal{C}o-\\mathcal{H}-Deformable-DETR{}^{\\dagger}</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>\\mathbf{58.7}</td><td>\\mathbf{77.4}</td><td>\\mathbf{64.5}</td><td>\\mathbf{41.9}</td><td>\\mathbf{62.7}</td><td>\\mathbf{74.9}</td></tr><tr><td colspan=\"11\">{\\dagger}: 300 predictions for evaluation. {}^{\\ddagger}: improved hyperparameters.</td></tr></table>", "caption": "Table 4: Comparison to the state-of-the-art DETR variants on COCO val.", "list_citation_info": ["[12] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. arXiv preprint arXiv:2207.13080, 2022.", "[20] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.", "[35] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[14] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13619\u201313627, 2022.", "[7] Ziteng Gao, Limin Wang, Bing Han, and Sheng Guo. Adamixer: A fast-converging query-based object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5364\u20135373, 2022.", "[33] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.", "[30] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 2567\u20132575, 2022.", "[23] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3651\u20133660, 2021."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">#Params</td><td>Image</td><td>Detection</td><td>val</td><td>test-dev</td></tr><tr><td>pre-training data</td><td>pre-training data</td><td>\\text{AP}^{box}</td><td>\\text{AP}^{box}</td></tr><tr><td>HTC++ [2]</td><td>SwinV2-G [21]</td><td>3.0B</td><td>IN-22K-ext (70M)</td><td>Objects365</td><td>62.5</td><td>63.1</td></tr><tr><td>DINO [33]</td><td>Swin-L [22]</td><td>218M</td><td>IN-22K (14M)</td><td>Objects365</td><td>63.2</td><td>63.3</td></tr><tr><td>BEIT3 [29]</td><td>ViT-g [6]</td><td>1.9B</td><td>IN-22K (14M) + Image-Text Pairs (35M) + Texts (160GB)</td><td>Objects365</td><td>-</td><td>63.7</td></tr><tr><td>FD [31]</td><td>SwinV2-G [21]</td><td>3.0B</td><td>IN-22K-ext (70M)</td><td>Objects365</td><td>-</td><td>64.2</td></tr><tr><td>DINO [33]</td><td>FocalNet-H [32]</td><td>746M</td><td>IN-22K (14M)</td><td>Objects365</td><td>64.2</td><td>64.3</td></tr><tr><td>Group DETR v2 [4]</td><td>ViT-Huge [6]</td><td>629M</td><td>IN-1K (1M)</td><td>Objects365</td><td>-</td><td>64.5</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>MixMIM-g</td><td>1.0B</td><td>IN-\\mathbf{1}K (\\mathbf{1}M)</td><td>Objects365</td><td>64.4</td><td>64.5</td></tr></table>", "caption": "Table 5: Comparison to the state-of-the-art on COCO test-dev with fewer extra data sizes.", "list_citation_info": ["[31] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.", "[4] Qiang Chen, Jian Wang, Chuchu Han, Shan Zhang, Zexian Li, Xiaokang Chen, Jiahui Chen, Xiaodi Wang, Shuming Han, Gang Zhang, et al. Group detr v2: Strong object detector with encoder-decoder pretraining. arXiv preprint arXiv:2211.03594, 2022.", "[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv, abs/2010.11929, 2021.", "[29] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.", "[21] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12009\u201312019, 2022.", "[33] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.", "[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ArXiv, abs/2103.14030, 2021.", "[32] Jianwei Yang, Chunyuan Li, and Jianfeng Gao. Focal modulation networks. arXiv preprint arXiv:2203.11926, 2022.", "[2] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974\u20134983, 2019."]}, {"table": "<table><tr><td>Method</td><td>#epochs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><td>Deformable-DETR</td><td>36</td><td>43.3</td><td>62.3</td><td>47.1</td></tr><tr><td>Deformable-DETR + RetinaNet [17]</td><td>36</td><td>46.1</td><td>64.2</td><td>50.1</td></tr><tr><td>Deformable-DETR + Faster-RCNN [24]</td><td>36</td><td>46.3</td><td>64.7</td><td>50.5</td></tr><tr><td>Deformable-DETR + Mask-RCNN [9]</td><td>36</td><td>46.5</td><td>65.0</td><td>50.6</td></tr><tr><td>Deformable-DETR + FCOS [28]</td><td>36</td><td>46.5</td><td>64.8</td><td>50.7</td></tr><tr><td>Deformable-DETR + PAA [13]</td><td>36</td><td>46.5</td><td>64.6</td><td>50.7</td></tr><tr><td>Deformable-DETR + GFL [15]</td><td>36</td><td>46.5</td><td>65.0</td><td>51.0</td></tr><tr><td>Deformable-DETR + ATSS [34]</td><td>36</td><td>\\mathbf{46.8}</td><td>\\mathbf{65.1}</td><td>\\mathbf{51.5}</td></tr></table>", "caption": "Table 8: Detection performance of our approach with various one-to-many heads on COCO val.", "list_citation_info": ["[28] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627\u20139636, 2019.", "[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.", "[13] Kang Kim and Hee Seok Lee. Probabilistic anchor assignment with iou prediction for object detection. In European Conference on Computer Vision, pages 355\u2013371. Springer, 2020.", "[15] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002\u201321012, 2020.", "[9] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.", "[34] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759\u20139768, 2020.", "[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015."]}, {"table": "<table><tr><td>Method</td><td>Backbone</td><td>Multi-scale</td><td>#query</td><td>#epochs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><td>Conditional-DETR [23]</td><td>R50</td><td>\u2717</td><td>300</td><td>108</td><td>43.0</td><td>64.0</td><td>45.7</td><td>22.7</td><td>46.7</td><td>61.5</td></tr><tr><td>Anchor-DETR [30]</td><td>R50</td><td>\u2717</td><td>300</td><td>50</td><td>42.1</td><td>63.1</td><td>44.9</td><td>22.3</td><td>46.2</td><td>60.0</td></tr><tr><td>DAB-DETR [20]</td><td>R50</td><td>\u2717</td><td>900</td><td>50</td><td>45.7</td><td>66.2</td><td>49.0</td><td>26.1</td><td>49.4</td><td>63.1</td></tr><tr><td>AdaMixer [7]</td><td>R50</td><td>\u2713</td><td>300</td><td>36</td><td>47.0</td><td>66.0</td><td>51.1</td><td>30.1</td><td>50.2</td><td>61.8</td></tr><tr><td>AdaMixer [7]</td><td>R101</td><td>\u2713</td><td>300</td><td>36</td><td>48.0</td><td>67.0</td><td>52.4</td><td>30.0</td><td>51.2</td><td>63.7</td></tr><tr><td>AdaMixer [7]</td><td>Swin-S</td><td>\u2713</td><td>300</td><td>36</td><td>51.3</td><td>71.2</td><td>55.7</td><td>34.2</td><td>54.6</td><td>67.3</td></tr><tr><td>Deformable-DETR [35]</td><td>R50</td><td>\u2713</td><td>300</td><td>50</td><td>46.9</td><td>65.6</td><td>51.0</td><td>29.6</td><td>50.1</td><td>61.6</td></tr><tr><td>DN-Deformable-DETR [14]</td><td>R50</td><td>\u2713</td><td>300</td><td>50</td><td>48.6</td><td>67.4</td><td>52.7</td><td>31.0</td><td>52.0</td><td>63.7</td></tr><tr><td>DINO-Deformable-DETR{}^{\\dagger} [33]</td><td>R50</td><td>\u2713</td><td>900</td><td>12</td><td>47.9</td><td>65.3</td><td>52.1</td><td>31.2</td><td>50.9</td><td>61.9</td></tr><tr><td>DINO-Deformable-DETR{}^{\\dagger} [33]</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>58.0</td><td>76.7</td><td>63.4</td><td>41.3</td><td>61.9</td><td>73.7</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>R50</td><td>\u2713</td><td>300</td><td>12</td><td>48.7</td><td>66.4</td><td>52.9</td><td>31.2</td><td>51.5</td><td>63.5</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>R101</td><td>\u2713</td><td>300</td><td>12</td><td>49.4</td><td>67.2</td><td>53.7</td><td>31.9</td><td>53.1</td><td>64.2</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>Swin-T</td><td>\u2713</td><td>300</td><td>36</td><td>53.2</td><td>71.5</td><td>58.2</td><td>35.9</td><td>56.4</td><td>68.2</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>Swin-S</td><td>\u2713</td><td>300</td><td>36</td><td>54.4</td><td>72.9</td><td>59.4</td><td>36.9</td><td>58.3</td><td>69.5</td></tr><tr><td>\\mathcal{H}-Deformable-DETR [12]</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>300</td><td>36</td><td>57.1</td><td>76.2</td><td>62.5</td><td>39.7</td><td>61.4</td><td>73.4</td></tr><tr><td>\\mathcal{H}-Deformable-DETR{}^{\\dagger}{}^{\\ddagger} [12]</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>57.9</td><td>76.8</td><td>63.6</td><td>42.4</td><td>61.9</td><td>73.4</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>R50</td><td>\u2713</td><td>300</td><td>12</td><td>48.7</td><td>66.6</td><td>53.2</td><td>30.5</td><td>52.2</td><td>62.8</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>R101</td><td>\u2713</td><td>300</td><td>12</td><td>49.6</td><td>67.6</td><td>54.0</td><td>31.7</td><td>53.2</td><td>64.3</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-B (IN-22K)</td><td>\u2713</td><td>300</td><td>12</td><td>55.2</td><td>74.0</td><td>60.6</td><td>38.1</td><td>59.3</td><td>71.3</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>300</td><td>12</td><td>56.4</td><td>75.4</td><td>62.0</td><td>38.9</td><td>60.5</td><td>72.9</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-T</td><td>\u2713</td><td>300</td><td>36</td><td>53.9</td><td>72.0</td><td>59.2</td><td>37.8</td><td>57.3</td><td>68.6</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-S</td><td>\u2713</td><td>300</td><td>36</td><td>55.0</td><td>73.5</td><td>60.5</td><td>38.5</td><td>58.8</td><td>70.2</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-B (IN-22K)</td><td>\u2713</td><td>300</td><td>36</td><td>57.0</td><td>75.7</td><td>62.3</td><td>40.3</td><td>60.9</td><td>73.2</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>58.1</td><td>76.6</td><td>63.8</td><td>41.3</td><td>62.1</td><td>74.0</td></tr><tr><td>\\mathcal{C}o-Deformable-DETR{}^{\\dagger}</td><td>Swin-L (IN-22K)</td><td>\u2713</td><td>900</td><td>36</td><td>\\mathbf{58.3}</td><td>\\mathbf{77.0}</td><td>\\mathbf{64.0}</td><td>\\mathbf{42.1}</td><td>\\mathbf{62.3}</td><td>\\mathbf{74.0}</td></tr><tr><td colspan=\"11\">{\\dagger}: 300 predictions for evaluation. {}^{\\ddagger}: improved hyperparameters.</td></tr></table>", "caption": "Table 14: Comparison to the state-of-the-art DETR variants on COCO val. We report the results of \\mathcal{C}o-Deformable-DETR with K=1.", "list_citation_info": ["[12] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. arXiv preprint arXiv:2207.13080, 2022.", "[20] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.", "[35] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.", "[14] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13619\u201313627, 2022.", "[7] Ziteng Gao, Limin Wang, Bing Han, and Sheng Guo. Adamixer: A fast-converging query-based object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5364\u20135373, 2022.", "[33] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.", "[30] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 2567\u20132575, 2022.", "[23] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3651\u20133660, 2021."]}], "citation_info_to_title": {"[31] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.": "Contrastive learning rivals masked image modeling in fine-tuning via feature distillation", "[15] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002\u201321012, 2020.": "Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection", "[13] Kang Kim and Hee Seok Lee. Probabilistic anchor assignment with iou prediction for object detection. In European Conference on Computer Vision, pages 355\u2013371. Springer, 2020.": "Probabilistic Anchor Assignment with IOU Prediction for Object Detection", "[7] Ziteng Gao, Limin Wang, Bing Han, and Sheng Guo. Adamixer: A fast-converging query-based object detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5364\u20135373, 2022.": "Adamixer: A fast-converging query-based object detector", "[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv, abs/2010.11929, 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.": "Focal loss for dense object detection", "[35] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.": "Faster r-cnn: Towards real-time object detection with region proposal networks", "[32] Jianwei Yang, Chunyuan Li, and Jianfeng Gao. Focal modulation networks. arXiv preprint arXiv:2203.11926, 2022.": "Focal Modulation Networks", "[33] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection", "[28] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627\u20139636, 2019.": "FCOS: Fully Convolutional One-Stage Object Detection", "[14] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13619\u201313627, 2022.": "Dn-detr: Accelerate detr training by introducing query denoising", "[23] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3651\u20133660, 2021.": "Conditional DETR for Fast Training Convergence", "[21] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12009\u201312019, 2022.": "Swin Transformer V2: Scaling up Capacity and Resolution", "[29] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks", "[34] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9759\u20139768, 2020.": "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection", "[12] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. arXiv preprint arXiv:2207.13080, 2022.": "Detrs with hybrid matching", "[2] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974\u20134983, 2019.": "Hybrid Task Cascade for Instance Segmentation", "[4] Qiang Chen, Jian Wang, Chuchu Han, Shan Zhang, Zexian Li, Xiaokang Chen, Jiahui Chen, Xiaodi Wang, Shuming Han, Gang Zhang, et al. Group detr v2: Strong object detector with encoder-decoder pretraining. arXiv preprint arXiv:2211.03594, 2022.": "Group detr v2: Strong object detector with encoder-decoder pretraining", "[20] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.": "Dab-detr: Dynamic anchor boxes are better queries for detr", "[30] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 2567\u20132575, 2022.": "Anchor detr: Query design for transformer-based detector", "[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ArXiv, abs/2103.14030, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[9] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.": "Mask R-CNN"}, "source_title_to_arxiv_id": {"Contrastive learning rivals masked image modeling in fine-tuning via feature distillation": "2205.14141", "Adamixer: A fast-converging query-based object detector": "2203.16507", "Conditional DETR for Fast Training Convergence": "2108.06152", "Swin Transformer V2: Scaling up Capacity and Resolution": "2111.09883", "Detrs with hybrid matching": "2207.13080", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}