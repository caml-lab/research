{"title": "Offline Visual Representation Learning for Embodied Navigation", "abstract": "How should we learn visual representations for embodied agents that must see\nand move? The status quo is tabula rasa in vivo, i.e. learning visual\nrepresentations from scratch while also learning to move, potentially augmented\nwith auxiliary tasks (e.g. predicting the action taken between two successive\nobservations). In this paper, we show that an alternative 2-stage strategy is\nfar more effective: (1) offline pretraining of visual representations with\nself-supervised learning (SSL) using large-scale pre-rendered images of indoor\nenvironments (Omnidata), and (2) online finetuning of visuomotor\nrepresentations on specific tasks with image augmentations under long learning\nschedules. We call this method Offline Visual Representation Learning (OVRL).\nWe conduct large-scale experiments - on 3 different 3D datasets (Gibson, HM3D,\nMP3D), 2 tasks (ImageNav, ObjectNav), and 2 policy learning algorithms (RL, IL)\n- and find that the OVRL representations lead to significant across-the-board\nimprovements in state of art, on ImageNav from 29.2% to 54.2% (+25% absolute,\n86% relative) and on ObjectNav from 18.1% to 23.2% (+5.1% absolute, 28%\nrelative). Importantly, both results were achieved by the same visual encoder\ngeneralizing to datasets that were not seen during pretraining. While the\nbenefits of pretraining sometimes diminish (or entirely disappear) with long\nfinetuning schedules, we find that OVRL's performance gains continue to\nincrease (not decrease) as the agent is trained for 2 billion frames of\nexperience.", "authors": ["Karmesh Yadav", "Ram Ramrakhya", "Arjun Majumdar", "Vincent-Pierre Berges", "Sachit Kuhar", "Dhruv Batra", "Alexei Baevski", "Oleksandr Maksymets"], "published_date": "2022_04_27", "pdf_url": "http://arxiv.org/pdf/2204.13226v1", "list_table_and_caption": [{"table": "<table><tr><td></td><td></td><td></td><td></td><td></td><td colspan=\"2\">Test</td></tr><tr><td></td><td>Method</td><td> PretrainingDataset </td><td> TestSplit </td><td>Camera(s)</td><td>SPL (\\uparrow)</td><td>SR (\\uparrow)</td></tr><tr><td>1)</td><td>Scratch</td><td>\u2717</td><td>A</td><td>1 RGB</td><td>9.3\\pm 1.1%</td><td>17.9\\pm 2.0%</td></tr><tr><td>2)</td><td>ZER (ResNet9) [2]</td><td>\u2717</td><td>A</td><td>1 RGB</td><td>21.6%</td><td>29.2%</td></tr><tr><td>3)</td><td>ZER (ResNet50){}^{*}</td><td>\u2717</td><td>A</td><td>1 RGB</td><td>18.8\\pm 2.3%</td><td>27.7\\pm 1.7%</td></tr><tr><td>4)</td><td>CRL [13]</td><td>MP3D</td><td>PointNav</td><td>1 RGB</td><td>3.2%</td><td>5.8%</td></tr><tr><td>5)</td><td>CRL{}^{*}</td><td>Gibson</td><td>A</td><td>1 RGB</td><td>10.2\\pm 1.6%</td><td>20.4\\pm 2.8%</td></tr><tr><td>6)</td><td>OVRL (Ours)</td><td>OSD</td><td>A</td><td>1 RGB</td><td>26.9\\pm 0.9%</td><td>41.3\\pm 1.0%</td></tr><tr><td>7)</td><td>OVRL+ZER-Reward (Ours)</td><td>OSD</td><td>A</td><td>1 RGB</td><td>27.0\\pm 2.5%</td><td>54.2\\pm 1.4%</td></tr><tr><td>\\hdashline8)</td><td>Mem-Aug RL [30]</td><td>\u2717</td><td>A</td><td>4 RGB</td><td>56.0%</td><td>69.0%</td></tr><tr><td>9)</td><td>OVRL (Ours)</td><td>OSD</td><td>A</td><td>4 RGB</td><td>62.5\\pm 1.3%</td><td>79.8\\pm 0.7%</td></tr><tr><td>\\hdashline10)</td><td>NRNS [19]</td><td>\u2717</td><td>B</td><td>1 RGBD</td><td>12.4%</td><td>24.0%</td></tr><tr><td>11)</td><td>OVRL (Ours)</td><td>OSD</td><td>B</td><td>1 RGB</td><td>28.4\\pm 1.7%</td><td>45.5\\pm 2.7%</td></tr></table>", "caption": "Table 1: ImageNav performance of OVRL and baselines. (* Reimplementation)", "list_citation_info": ["[2] Al-Halah, Z., Ramakrishnan, S.K., Grauman, K.: Zero experience required: Plug & play modular transfer learning for semantic visual navigation. arXiv preprint arXiv:2202.02440 (2022)", "[30] Mezghani, L., Sukhbaatar, S., Lavril, T., Maksymets, O., Batra, D., Bojanowski, P., Alahari, K.: Memory-augmented reinforcement learning for image-goal navigation. arXiv preprint arXiv:2101.05181 (2021)", "[19] Hahn, M., Chaplot, D.S., Tulsiani, S., Mukadam, M., Rehg, J.M., Gupta, A.: No rl, no simulation: Learning to navigate without navigating. In: Advances in Neural Information Processing Systems (NeurIPS) (2021)", "[13] Du, Y., Gan, C., Isola, P.: Curious representation learning for embodied intelligence. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10408\u201310417 (2021)"]}, {"table": "<table><tr><td></td><td></td><td></td><td></td><td colspan=\"2\">Val</td><td colspan=\"2\">Test</td></tr><tr><td></td><td><p>Method</p></td><td>Pretraining Dataset</td><td>Camera(s)</td><td>SPL (\\uparrow)</td><td>SR (\\uparrow)</td><td>SPL (\\uparrow)</td><td>SR (\\uparrow)</td></tr><tr><td>12)</td><td><p>EmbCLIP [24]</p></td><td>WebImageText</td><td>1 RGB</td><td>-</td><td>-</td><td>\\mathbf{7.8\\%}</td><td>18.1\\%</td></tr><tr><td>13)</td><td><p>Habitat-Web{}^{*}</p></td><td>\u2717</td><td>1 RGB</td><td>4.6\\%</td><td>17.4\\%</td><td>4.9\\%</td><td>15.1\\%</td></tr><tr><td>14)</td><td><p>OVRL (ours)</p></td><td>OSD</td><td>1 RGB</td><td>7.0\\%</td><td>25.3\\%</td><td>6.8\\%</td><td>\\mathbf{21.4\\%}</td></tr><tr><td>\\hdashline15)</td><td><p>DDPPO [47]</p></td><td>\u2717</td><td>1 RGBD</td><td>1.8%</td><td>8.0%</td><td>2.1%</td><td>6.2%</td></tr><tr><td>16)</td><td><p>Habitat-Web [37]</p></td><td>\u2717</td><td>1 RGBD</td><td>6.1\\%</td><td>22.7\\%</td><td>6.1\\%</td><td>17.0\\%</td></tr><tr><td>17)</td><td><p>Habitat-Web{}^{*}</p></td><td>\u2717</td><td>1 RGBD</td><td>5.9\\%</td><td>24.2\\%</td><td>6.1\\%</td><td>17.9\\%</td></tr><tr><td>18)</td><td><p>OVRL (ours)</p></td><td>OSD</td><td>1 RGBD</td><td>\\mathbf{7.4\\%}</td><td>\\mathbf{28.6\\%}</td><td>\\mathbf{7.6\\%}</td><td>\\mathbf{23.2\\%}</td></tr></table>", "caption": "Table 2: ObjectNav results on MP3D val and test. (* Reimplementation)", "list_citation_info": ["[24] Khandelwal, A., Weihs, L., Mottaghi, R., Kembhavi, A.: Simple but effective: Clip embeddings for embodied ai. CVPR (2022)", "[37] Ramrakhya, R., Undersander, E., Batra, D., Das, A.: Habitat-web: Learning embodied object-search from human demonstrations at scale. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2022)", "[47] Wijmans, E., Kadian, A., Morcos, A., Lee, S., Essa, I., Parikh, D., Savva, M., Batra, D.: DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. In: International Conference on Learning Representations (ICLR) (2020)"]}, {"table": "<table><tr><td></td><td></td><td colspan=\"2\">Val Accuracy</td></tr><tr><td></td><td>Method</td><td>Top 1</td><td>Top 5</td></tr><tr><td>38)</td><td>ImageNav from Scratch</td><td>32.0%</td><td>62.7%</td></tr><tr><td>39)</td><td>CRL[13]</td><td>21.2%</td><td>48.8%</td></tr><tr><td>40)</td><td>CRL{}^{*}</td><td>42.1%</td><td>74.0%</td></tr><tr><td>41)</td><td>ImageNet-DINO</td><td>48.3%</td><td>80.4%</td></tr><tr><td>42)</td><td>OVRL-Frozen</td><td>50.9%</td><td>82.9%</td></tr><tr><td>43)</td><td>OVRL</td><td>42.9%</td><td>76.0%</td></tr><tr><td>44)</td><td>ImageNet-SL</td><td>54.7%</td><td>85.7%</td></tr></table>", "caption": "Table 7: OVRL\u2019s visual encoder compared against baselines on the classification task using the Places dataset. (* Reimplementation)", "list_citation_info": ["[13] Du, Y., Gan, C., Isola, P.: Curious representation learning for embodied intelligence. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10408\u201310417 (2021)"]}], "citation_info_to_title": {"[2] Al-Halah, Z., Ramakrishnan, S.K., Grauman, K.: Zero experience required: Plug & play modular transfer learning for semantic visual navigation. arXiv preprint arXiv:2202.02440 (2022)": "Zero experience required: Plug & play modular transfer learning for semantic visual navigation", "[37] Ramrakhya, R., Undersander, E., Batra, D., Das, A.: Habitat-web: Learning embodied object-search from human demonstrations at scale. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2022)": "Habitat-web: Learning embodied object-search from human demonstrations at scale", "[19] Hahn, M., Chaplot, D.S., Tulsiani, S., Mukadam, M., Rehg, J.M., Gupta, A.: No rl, no simulation: Learning to navigate without navigating. In: Advances in Neural Information Processing Systems (NeurIPS) (2021)": "No rl, no simulation: Learning to navigate without navigating", "[47] Wijmans, E., Kadian, A., Morcos, A., Lee, S., Essa, I., Parikh, D., Savva, M., Batra, D.: DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames. In: International Conference on Learning Representations (ICLR) (2020)": "DD-PPO: Learning near-perfect pointgoal navigators from 25 billion frames", "[30] Mezghani, L., Sukhbaatar, S., Lavril, T., Maksymets, O., Batra, D., Bojanowski, P., Alahari, K.: Memory-augmented reinforcement learning for image-goal navigation. arXiv preprint arXiv:2101.05181 (2021)": "Memory-augmented reinforcement learning for image-goal navigation", "[13] Du, Y., Gan, C., Isola, P.: Curious representation learning for embodied intelligence. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 10408\u201310417 (2021)": "Curious representation learning for embodied intelligence", "[24] Khandelwal, A., Weihs, L., Mottaghi, R., Kembhavi, A.: Simple but effective: Clip embeddings for embodied ai. CVPR (2022)": "Simple but effective: Clip embeddings for embodied ai"}, "source_title_to_arxiv_id": {"Habitat-web: Learning embodied object-search from human demonstrations at scale": "2204.03514", "No rl, no simulation: Learning to navigate without navigating": "2110.09470"}}