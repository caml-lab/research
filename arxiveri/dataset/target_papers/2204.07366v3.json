{"title": "ResT V2: Simpler, Faster and Stronger", "abstract": "This paper proposes ResTv2, a simpler, faster, and stronger multi-scale\nvision Transformer for visual recognition. ResTv2 simplifies the EMSA structure\nin ResTv1 (i.e., eliminating the multi-head interaction part) and employs an\nupsample operation to reconstruct the lost medium- and high-frequency\ninformation caused by the downsampling operation. In addition, we explore\ndifferent techniques for better apply ResTv2 backbones to downstream tasks. We\nfound that although combining EMSAv2 and window attention can greatly reduce\nthe theoretical matrix multiply FLOPs, it may significantly decrease the\ncomputation density, thus causing lower actual speed. We comprehensively\nvalidate ResTv2 on ImageNet classification, COCO detection, and ADE20K semantic\nsegmentation. Experimental results show that the proposed ResTv2 can outperform\nthe recently state-of-the-art backbones by a large margin, demonstrating the\npotential of ResTv2 as solid backbones. The code and models will be made\npublicly available at \\url{https://github.com/wofmanaf/ResT}", "authors": ["Qing-Long Zhang", "Yu-Bin Yang"], "published_date": "2022_04_15", "pdf_url": "http://arxiv.org/pdf/2204.07366v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td>Image Size</td><td>Params</td><td>FLOPs</td><td>Throughput</td><td>Top-1 (%)</td><td>Top-5 (%)</td></tr><tr><td>RegNetY-4G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>224^{2}</td><td>21M</td><td>4.0G</td><td>1156</td><td>79.4</td><td>94.7</td></tr><tr><td>ConvNeXt-T liu2022convnet </td><td>224^{2}</td><td>29M</td><td>4.5G</td><td>775</td><td>82.1</td><td>95.9</td></tr><tr><td>Swin-T DBLP:conf/iccv/LiuL00W0LG21 </td><td>224^{2}</td><td>28M</td><td>4.5G</td><td>755</td><td>81.3</td><td>95.5</td></tr><tr><td>Focal-T yang2021focal </td><td>224^{2}</td><td>29M</td><td>4.9G</td><td>319</td><td>82.2</td><td>95.9</td></tr><tr><td>ResTv1-B zhang2021rest </td><td>224^{2}</td><td>30M</td><td>4.3G</td><td>673</td><td>81.6</td><td>95.7</td></tr><tr><td>ResTv2-T</td><td>224^{2}</td><td>30M</td><td>4.1G</td><td>826</td><td>82.3</td><td>95.5</td></tr><tr><td>ResTv2-T</td><td>384^{2}</td><td>30M</td><td>12.7G</td><td>319</td><td>83.7</td><td>96.6</td></tr><tr><td>RegNetY-8G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>224^{2}</td><td>39M</td><td>8.0G</td><td>591</td><td>79.9</td><td>94.9</td></tr><tr><td>ResTv2-S</td><td>224^{2}</td><td>41M</td><td>6.0G</td><td>687</td><td>83.2</td><td>96.1</td></tr><tr><td>ResTv2-S</td><td>384^{2}</td><td>41M</td><td>18.4G</td><td>256</td><td>84.5</td><td>96.7</td></tr><tr><td>ConvNeXt-S liu2022convnet </td><td>224^{2}</td><td>50M</td><td>8.7G</td><td>447</td><td>83.1</td><td>96.4</td></tr><tr><td>Swin-S DBLP:conf/iccv/LiuL00W0LG21 </td><td>224^{2}</td><td>50M</td><td>8.7G</td><td>437</td><td>83.2</td><td>96.2</td></tr><tr><td>Focal-S yang2021focal </td><td>224^{2}</td><td>51M</td><td>9.4G</td><td>192</td><td>83.6</td><td>96.2</td></tr><tr><td>ResTv1-L zhang2021rest </td><td>224^{2}</td><td>52M</td><td>7.9G</td><td>429</td><td>83.6</td><td>96.3</td></tr><tr><td>ResTv2-B</td><td>224^{2}</td><td>56M</td><td>7.9G</td><td>582</td><td>83.7</td><td>96.3</td></tr><tr><td>ResTv2-B</td><td>384^{2}</td><td>56M</td><td>24.3G</td><td>210</td><td>85.1</td><td>97.2</td></tr><tr><td>RegNetY-16G DBLP:conf/cvpr/RadosavovicKGHD20 </td><td>224^{2}</td><td>84M</td><td>15.9G</td><td>334</td><td>80.4</td><td>95.1</td></tr><tr><td>ConvNeXt-B liu2022convnet </td><td>224^{2}</td><td>89M</td><td>15.4G</td><td>292</td><td>83.8</td><td>96.7</td></tr><tr><td>Swin-B DBLP:conf/iccv/LiuL00W0LG21 </td><td>224^{2}</td><td>88M</td><td>15.4G</td><td>278</td><td>83.5</td><td>96.5</td></tr><tr><td>Focal-B yang2021focal </td><td>224^{2}</td><td>90M</td><td>16.4G</td><td>138</td><td>84.0</td><td>96.5</td></tr><tr><td>ResTv2-L</td><td>224^{2}</td><td>87M</td><td>13.8G</td><td>415</td><td>84.2</td><td>96.5</td></tr><tr><td>ConvNeXt-B liu2022convnet </td><td>384^{2}</td><td>89M</td><td>45.0G</td><td>96</td><td>85.1</td><td>97.3</td></tr><tr><td>Swin-B DBLP:conf/iccv/LiuL00W0LG21 </td><td>384^{2}</td><td>88M</td><td>47.1G</td><td>85</td><td>84.5</td><td>97.0</td></tr><tr><td>ResTv2-L</td><td>384^{2}</td><td>87M</td><td>42.4G</td><td>141</td><td>85.4</td><td>97.1</td></tr></tbody></table>", "caption": "Table 1: Classification accuracy on ImageNet-1k. Inference throughput (images / s) is measured on a V100 GPU, following zhang2021rest .", "list_citation_info": ["(24) Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, 2022.", "(30) Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, pages 10425\u201310433. IEEE, 2020.", "(23) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 9992\u201310002. IEEE, 2021.", "(41) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, NeurIPS 2021, 2021.", "(47) Qinglong Zhang and Yu bin Yang. Rest: An efficient transformer for visual recognition. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, NeurIPS 2021, 2021."]}, {"table": "<table><thead><tr><th>Backbones</th><th>{\\rm AP^{box}}</th><th>{\\rm AP^{mask}}</th><th>Params.</th><th>FLOPs</th><th>FPS</th></tr></thead><tbody><tr><td>ResNet-50 DBLP:conf/cvpr/HeZRS16 </td><td>41.0</td><td>37.1</td><td>44.2M</td><td>260G</td><td>24.1</td></tr><tr><td>ConvNeXt-T liu2022convnet </td><td>46.2</td><td>41.7</td><td>48.1M</td><td>262G</td><td>23.4</td></tr><tr><td>Swin-T DBLP:conf/iccv/LiuL00W0LG21 </td><td>46.0</td><td>41.6</td><td>47.8M</td><td>264G</td><td>21.8</td></tr><tr><td>ResTv2-T</td><td>47.6</td><td>43.2</td><td>49.9M</td><td>253G</td><td>25.0</td></tr><tr><td>ResNet-101 DBLP:conf/cvpr/HeZRS16 </td><td>42.8</td><td>38.5</td><td>63.2M</td><td>336G</td><td>13.5</td></tr><tr><td>Swin-S DBLP:conf/iccv/LiuL00W0LG21 </td><td>48.5</td><td>43.3</td><td>69.1M</td><td>354G</td><td>17.4</td></tr><tr><td>ResTv2-S</td><td>48.1</td><td>43.3</td><td>60.7M</td><td>290G</td><td>21.3</td></tr><tr><td>ResTv2-B</td><td>48.7</td><td>43.9</td><td>75.5M</td><td>328G</td><td>18.3</td></tr></tbody></table>", "caption": "Table 4: COCO object detection and segmentation results usingMask-RCNN. We measure FPS on one V100 GPU. FLOPs are calculated with image size (1280, 800).", "list_citation_info": ["(15) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, pages 770\u2013778. IEEE Computer Society, 2016.", "(23) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 9992\u201310002. IEEE, 2021.", "(24) Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, 2022."]}, {"table": "<table><tbody><tr><td>Backbones</td><td>input crop.</td><td>mIoU</td><td>Params.</td><td>FLOPs</td><td>FPS</td></tr><tr><td>ResNet-50 DBLP:conf/cvpr/HeZRS16 </td><td>512^{2}</td><td>42.8</td><td>66.5M</td><td>952G</td><td>23.4</td></tr><tr><td>ConvNeXt-T liu2022convnet </td><td>512^{2}</td><td>46.7</td><td>60.2M</td><td>939G</td><td>19.9</td></tr><tr><td>Swin-T DBLP:conf/iccv/LiuL00W0LG21 </td><td>512^{2}</td><td>45.8</td><td>59.9 M</td><td>941G</td><td>21.1</td></tr><tr><td>ResTv2-T</td><td>512^{2}</td><td>47.3</td><td>62.1M</td><td>977G</td><td>22.4</td></tr><tr><td>ResNet-101 DBLP:conf/cvpr/HeZRS16 </td><td>512^{2}</td><td>44.9</td><td>85.5M</td><td>1029G</td><td>20.3</td></tr><tr><td>ConvNeXt-S liu2022convnet </td><td>512^{2}</td><td>49.0</td><td>81.9M</td><td>1027G</td><td>15.3</td></tr><tr><td>Swin-S DBLP:conf/iccv/LiuL00W0LG21 </td><td>512^{2}</td><td>49.2</td><td>81.3M</td><td>1038G</td><td>14.7</td></tr><tr><td>ResTv2-S</td><td>512^{2}</td><td>49.2</td><td>72.9M</td><td>1035G</td><td>20.0</td></tr><tr><td>ResTv2-B</td><td>512^{2}</td><td>49.6</td><td>87.6M</td><td>1095G</td><td>19.2</td></tr></tbody></table>", "caption": "Table 5: ADE20K validation results using UperNet. Following Swin, we report mIoU results with multiscale testing. FLOPs are based on input sizes of (2048, 512).", "list_citation_info": ["(15) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, pages 770\u2013778. IEEE Computer Society, 2016.", "(23) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 9992\u201310002. IEEE, 2021.", "(24) Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, 2022."]}, {"table": "<table><thead><tr><th>Methods</th><th>Params</th><th>FLOPs</th><th>Throughput</th><th>Top-1 (%)</th></tr></thead><tbody><tr><td>PVT-TinyDBLP:conf/iccv/WangX0FSLL0021 </td><td>13.27M</td><td>1.94G</td><td>1144</td><td>70.46</td></tr><tr><td>PVT-MSA-Tiny{}^{\\ast}</td><td>11.36M</td><td>4.81G</td><td>617</td><td>72.38</td></tr><tr><td>PVT-EMSAv2-Tiny{}^{\\dagger}</td><td>13.95M</td><td>1.95G</td><td>947</td><td>72.53</td></tr><tr><td>ResTv1-Litezhang2021rest </td><td>10.50M</td><td>1.44G</td><td>1091</td><td>74.108</td></tr><tr><td>ResTv1-MSA-Lite{}^{\\ast}</td><td>10.48M</td><td>4.37G</td><td>625</td><td>75.06</td></tr><tr><td>ResTv1-EMSAv2-Lite{}^{\\dagger}</td><td>10.66M</td><td>1.45G</td><td>926</td><td>75.04</td></tr></tbody></table>", "caption": "Table 6: Results of different MSAs. \\dagger means the implementation of adding an upsample operation for SRADBLP:conf/iccv/WangX0FSLL0021  or EMSAzhang2021rest , and\\ast means replacing SRA or EMSA with the standard MSA, leaving other components unchanged. Inference throughput (images / s) is measured on a V100 GPU, following zhang2021rest .", "list_citation_info": ["(35) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 548\u2013558. IEEE, 2021.", "(47) Qinglong Zhang and Yu bin Yang. Rest: An efficient transformer for visual recognition. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, NeurIPS 2021, 2021."]}, {"table": "<table><thead><tr><th>Methods</th><th>head_dim</th><th>Params</th><th>FLOPs</th><th>Throughput</th><th>Top-1 (%)</th></tr></thead><tbody><tr><td>ResTv2-Lite</td><td>64</td><td>10.66M</td><td>1.45G</td><td>945</td><td>74.54</td></tr><tr><td>+MHIM</td><td>64</td><td>10.66M</td><td>1.45G</td><td>926(-19)</td><td>75.04(+0.5)</td></tr><tr><td>ResTv2-Tiny</td><td>96</td><td>30.43 M</td><td>4.10G</td><td>826</td><td>80.33</td></tr><tr><td>+MHIM</td><td>96</td><td>30.44M</td><td>4.10G</td><td>792(-34)</td><td>80.47 (+0.14)</td></tr></tbody></table>", "caption": "Table 7: Results of short for MHIM. ResTv2-Lite is a shallow ResTv2 variant with blocks number={2, 2, 2, 2} and C=64. Inference throughput (images / s) is measured on a V100 GPU, following zhang2021rest .", "list_citation_info": ["(47) Qinglong Zhang and Yu bin Yang. Rest: An efficient transformer for visual recognition. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, NeurIPS 2021, 2021."]}, {"table": "<table><tbody><tr><th>configure</th><td>pre-training</td><td>fine-tuning</td></tr><tr><th>input crop.</th><td>224^{2}</td><td>384^{2}</td></tr><tr><th>optimizer</th><td>AdamW</td><td>AdamW</td></tr><tr><th>base learning rate</th><td>1.5e-4</td><td>1.5e-5</td></tr><tr><th>weight decay</th><td>0.05</td><td>1e-8</td></tr><tr><th>optimizer momentum</th><td>\\beta_{1}=0.9,\\beta_{2}=0.999</td><td>\\beta_{1}=0.9,\\beta_{2}=0.999</td></tr><tr><th>batch size</th><td>2048</td><td>512</td></tr><tr><th>training epoch</th><td>300</td><td>30</td></tr><tr><th>learning rate schedule</th><td>cosine decay</td><td>cosine decay</td></tr><tr><th>warmup epochs</th><td>50</td><td>N/A</td></tr><tr><th>warmup schedule</th><td>linear</td><td>N/A</td></tr><tr><th>RandAugment DBLP:conf/nips/CubukZS020 </th><td>(9, 0.5)</td><td>(9, 0.5)</td></tr><tr><th>label smoothing DBLP:conf/cvpr/SzegedyVISW16 </th><td>0.1</td><td>0.1</td></tr><tr><th>Mixup DBLP:conf/iclr/ZhangCDL18 </th><td>0.8</td><td>N/A</td></tr><tr><th>Cutmix DBLP:conf/iccv/YunHCOYC19 </th><td>1.0</td><td>N/A</td></tr><tr><th>stochastic depth DBLP:conf/eccv/HuangSLSW16 </th><td>0.1/0.2/0.3/0.5</td><td>0.1/0.2/0.3/0.5</td></tr><tr><th>gradient clip</th><td>1.0</td><td>1.0</td></tr><tr><th>EMA polyak1992acceleration </th><td>0.9999</td><td>N/A</td></tr></tbody></table>", "caption": "Table 8: ImageNet-1k training and fine-tuning settings. Mutiple stochastic depth rates (e.g., 0.1/0.2/0.3/0.5) are for each model (e.g., ResTv2-T/S/B/L) respectively.", "list_citation_info": ["(46) Hongyi Zhang, Moustapha Ciss\u00e9, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018. OpenReview.net, 2018.", "(32) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, pages 2818\u20132826. IEEE Computer Society, 2016.", "(44) Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix: Regularization strategy to train strong classifiers with localizable features. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, pages 6022\u20136031. IEEE, 2019.", "(29) Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838\u2013855, 1992.", "(8) Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, NeurIPS 2020, 2020.", "(16) Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In 14th European Conference on Computer Vision, ECCV 2016, volume 9908, pages 646\u2013661. Springer, 2016."]}], "citation_info_to_title": {"(16) Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In 14th European Conference on Computer Vision, ECCV 2016, volume 9908, pages 646\u2013661. Springer, 2016.": "Deep networks with stochastic depth", "(8) Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Advances in Neural Information Processing Systems, NeurIPS 2020, 2020.": "Randaugment: Practical automated data augmentation with a reduced search space", "(24) Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, 2022.": "A convnet for the 2020s", "(44) Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix: Regularization strategy to train strong classifiers with localizable features. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, pages 6022\u20136031. IEEE, 2019.": "Cutmix: Regularization strategy to train strong classifiers with localizable features", "(35) Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 548\u2013558. IEEE, 2021.": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "(15) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, pages 770\u2013778. IEEE Computer Society, 2016.": "Deep residual learning for image recognition", "(47) Qinglong Zhang and Yu bin Yang. Rest: An efficient transformer for visual recognition. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, NeurIPS 2021, 2021.": "Rest: An efficient transformer for visual recognition", "(41) Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, NeurIPS 2021, 2021.": "Focal attention for long-range interactions in vision transformers", "(29) Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838\u2013855, 1992.": "Acceleration of stochastic approximation by averaging", "(46) Hongyi Zhang, Moustapha Ciss\u00e9, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018. OpenReview.net, 2018.": "mixup: Beyond Empirical Risk Minimization", "(23) Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, pages 9992\u201310002. IEEE, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "(30) Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, pages 10425\u201310433. IEEE, 2020.": "Designing network design spaces", "(32) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, pages 2818\u20132826. IEEE Computer Society, 2016.": "Rethinking the inception architecture for computer vision"}, "source_title_to_arxiv_id": {"A convnet for the 2020s": "2201.03545", "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions": "2102.12122", "Rest: An efficient transformer for visual recognition": "2105.13677", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}