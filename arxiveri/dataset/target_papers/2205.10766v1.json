{"title": "Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey", "abstract": "Multi-object tracking (MOT) aims to associate target objects across video\nframes in order to obtain entire moving trajectories. With the advancement of\ndeep neural networks and the increasing demand for intelligent video analysis,\nMOT has gained significantly increased interest in the computer vision\ncommunity. Embedding methods play an essential role in object location\nestimation and temporal identity association in MOT. Unlike other computer\nvision tasks, such as image classification, object detection,\nre-identification, and segmentation, embedding methods in MOT have large\nvariations, and they have never been systematically analyzed and summarized. In\nthis survey, we first conduct a comprehensive overview with in-depth analysis\nfor embedding methods in MOT from seven different perspectives, including\npatch-level embedding, single-frame embedding, cross-frame joint embedding,\ncorrelation embedding, sequential embedding, tracklet embedding, and\ncross-track relational embedding. We further summarize the existing widely used\nMOT datasets and analyze the advantages of existing state-of-the-art methods\naccording to their embedding strategies. Finally, some critical yet\nunder-investigated areas and future research directions are discussed.", "authors": ["Gaoang Wang", "Mingli Song", "Jenq-Neng Hwang"], "published_date": "2022_05_22", "pdf_url": "http://arxiv.org/pdf/2205.10766v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>Year</th><th>#Track</th><th>#Box</th><th>Annotation</th></tr></thead><tbody><tr><th>KITTI [211]</th><th>2012</th><td>-</td><td>-</td><td>2D box/3D box</td></tr><tr><th>MOT15 [213]</th><th>2015</th><td>1,221</td><td>101.3K</td><td>2D box/3D box</td></tr><tr><th>DukeMTMCT [117]</th><th>2016</th><td>6.7K</td><td>-</td><td>2D box</td></tr><tr><th>MOT16-17 [143]</th><th>2017</th><td>1,331</td><td>300.4K</td><td>2D box</td></tr><tr><th>PathTrack [124]</th><th>2017</th><td>16.3K</td><td>-</td><td>2D box</td></tr><tr><th>UA-DETRAC [214]</th><th>2017</th><td>8.2K</td><td>1.2M</td><td>2D box</td></tr><tr><th>PoseTrack [215]</th><th>2018</th><td>-</td><td>153.6K</td><td>Pose</td></tr><tr><th>VisDrone [223]</th><th>2018</th><td>-</td><td>1.8M</td><td>2D box</td></tr><tr><th>BDD100K [219]</th><th>2018</th><td>160K</td><td>4M</td><td>2D box/Mask</td></tr><tr><th>MOTS [37]</th><th>2019</th><td>228</td><td>26K</td><td>Mask</td></tr><tr><th>KITTI MOTS [37]</th><th>2019</th><td>-</td><td>-</td><td>Mask</td></tr><tr><th>CityFlow [20]</th><th>2019</th><td>666</td><td>230K</td><td>2D box</td></tr><tr><th>MOT20 [144]</th><th>2020</th><td>3,833</td><td>2.1M</td><td>2D box</td></tr><tr><th>Waymo [218]</th><th>2020</th><td>-</td><td>12.6M</td><td>2D box/3D box</td></tr><tr><th>nuScenes [217]</th><th>2020</th><td>-</td><td>1.4M</td><td>3D box</td></tr></tbody></table>", "caption": "TABLE II: Statistics of MOT datasets. We use \u201c-\u201d to represent the missing information that is not released from the official site of the dataset.", "list_citation_info": ["[223] L. Wen, P. Zhu, D. Du, X. Bian, H. Ling, Q. Hu, J. Zheng, T. Peng, X. Wang, Y. Zhang et al., \u201cVisdrone-mot2019: The vision meets drone multiple object tracking challenge results,\u201d in ICCV Workshops, 2019, pp. 0\u20130.", "[143] A. Milan, L. Leal-Taix\u00e9, I. Reid, S. Roth, and K. Schindler, \u201cMot16: A benchmark for multi-object tracking,\u201d arXiv preprint arXiv:1603.00831, 2016.", "[217] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in CVPR, 2020, pp. 11\u2009621\u201311\u2009631.", "[20] Z. Tang, M. Naphade, M.-Y. Liu, X. Yang, S. Birchfield, S. Wang, R. Kumar, D. Anastasiu, and J.-N. Hwang, \u201cCityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification,\u201d in CVPR, 2019, pp. 8797\u20138806.", "[213] L. Leal-Taix\u00e9, A. Milan, I. Reid, S. Roth, and K. Schindler, \u201cMotchallenge 2015: Towards a benchmark for multi-target tracking,\u201d arXiv preprint arXiv:1504.01942, 2015.", "[144] P. Dendorfer, H. Rezatofighi, A. Milan, J. Shi, D. Cremers, I. Reid, S. Roth, K. Schindler, and L. Leal-Taix\u00e9, \u201cMot20: A benchmark for multi object tracking in crowded scenes,\u201d arXiv preprint arXiv:2003.09003, 2020.", "[214] L. Wen, D. Du, Z. Cai, Z. Lei, M.-C. Chang, H. Qi, J. Lim, M.-H. Yang, and S. Lyu, \u201cUa-detrac: A new benchmark and protocol for multi-object detection and tracking,\u201d CVIU, vol. 193, p. 102907, 2020.", "[218] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine et al., \u201cScalability in perception for autonomous driving: Waymo open dataset,\u201d in CVPR, 2020, pp. 2446\u20132454.", "[37] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger, and B. Leibe, \u201cMots: Multi-object tracking and segmentation,\u201d in CVPR, 2019, pp. 7942\u20137951.", "[211] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? the kitti vision benchmark suite,\u201d in CVPR, 2012, pp. 3354\u20133361.", "[124] S. Manen, M. Gygli, D. Dai, and L. Van Gool, \u201cPathtrack: Fast trajectory annotation with path supervision,\u201d in ICCV, 2017, pp. 290\u2013299.", "[219] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell, \u201cBdd100k: A diverse driving dataset for heterogeneous multitask learning,\u201d in CVPR, 2020, pp. 2636\u20132645.", "[117] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, \u201cPerformance measures and a data set for multi-target, multi-camera tracking,\u201d in ECCV, 2016, pp. 17\u201335.", "[215] M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall, and B. Schiele, \u201cPosetrack: A benchmark for human pose estimation and tracking,\u201d in CVPR, 2018, pp. 5167\u20135176."]}, {"table": "<table><tbody><tr><td>Emb. Method</td><td>Ref.</td><td>Year</td><td>Venue</td><td>Det.</td><td>MOTA</td><td>IDF1</td><td>HOTA</td></tr><tr><td rowspan=\"11\">Patch</td><td><p>[102]</p></td><td><p>2017</p></td><td><p>CVPRW</p></td><td><p>Pub.</p></td><td><p>50.0</p></td><td><p>51.3</p></td><td><p>41.3</p></td></tr><tr><td><p>[105]</p></td><td><p>2018</p></td><td><p>CoRR</p></td><td><p>Pub.</p></td><td><p>51.5</p></td><td><p>46.9</p></td><td><p>38.5</p></td></tr><tr><td><p>[103]</p></td><td><p>2018</p></td><td><p>ICME</p></td><td><p>Pub.</p></td><td><p>50.9</p></td><td><p>52.7</p></td><td><p>41.2</p></td></tr><tr><td><p>[100]</p></td><td><p>2018</p></td><td><p>AVSS</p></td><td><p>Pub.</p></td><td><p>48.3</p></td><td><p>51.1</p></td><td><p>40.3</p></td></tr><tr><td><p>[72]</p></td><td><p>2019</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>54.7</p></td><td><p>62.3</p></td><td><p>47.1</p></td></tr><tr><td><p>[125]</p></td><td><p>2019</p></td><td><p>Access</p></td><td><p>Pub.</p></td><td><p>48.6</p></td><td><p>47.9</p></td><td><p>38.4</p></td></tr><tr><td><p>[68]</p></td><td><p>2020</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>61.7</p></td><td><p>58.1</p></td><td><p>46.9</p></td></tr><tr><td><p>[98]</p></td><td><p>2020</p></td><td><p>ICRAI</p></td><td><p>Pub.</p></td><td><p>49.7</p></td><td><p>51.5</p></td><td><p>39.0</p></td></tr><tr><td><p>[70]</p></td><td><p>2021</p></td><td><p>J-VCIR</p></td><td><p>Pub.</p></td><td><p>46.8</p></td><td><p>54.1</p></td><td><p>41.5</p></td></tr><tr><td><p>[71]</p></td><td><p>2021</p></td><td><p>IVC</p></td><td><p>Priv.</p></td><td><p>77.0</p></td><td><p>72.0</p></td><td><p>59.7</p></td></tr><tr><td><p>[106]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>73.3</p></td><td><p>73.2</p></td><td><p>59.8</p></td></tr><tr><td rowspan=\"5\">S-Fr</td><td><p>[15]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>Pub.</p></td><td><p>56.3</p></td><td><p>55.1</p></td><td><p>44.8</p></td></tr><tr><td><p>[33]</p></td><td><p>2021</p></td><td><p>AI</p></td><td><p>Pub.</p></td><td><p>60.1</p></td><td><p>58.8</p></td><td><p>47.2</p></td></tr><tr><td><p>[35]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>73.8</p></td><td><p>74.7</p></td><td><p>61.0</p></td></tr><tr><td><p>[34]</p></td><td><p>2021</p></td><td><p>IJCV</p></td><td><p>Priv.</p></td><td><p>73.7</p></td><td><p>72.3</p></td><td><p>59.3</p></td></tr><tr><td><p>[132]</p></td><td><p>2022</p></td><td><p>Neuroc.</p></td><td><p>Priv.</p></td><td><p>73.5</p></td><td><p>70.2</p></td><td><p>58.7</p></td></tr><tr><td rowspan=\"8\">X-Fr</td><td><p>[12]</p></td><td><p>2020</p></td><td><p>ECCV</p></td><td><p>Pub.</p></td><td><p>61.5</p></td><td><p>59.6</p></td><td><p>48.2</p></td></tr><tr><td><p>[146]</p></td><td><p>2019</p></td><td><p>T-PAMI</p></td><td><p>Priv.</p></td><td><p>52.4</p></td><td><p>49.5</p></td><td><p>39.3</p></td></tr><tr><td><p>[14]</p></td><td><p>2020</p></td><td><p>CVPR</p></td><td><p>Priv.</p></td><td><p>63.0</p></td><td><p>58.6</p></td><td><p>48.0</p></td></tr><tr><td><p>[13]</p></td><td><p>2020</p></td><td><p>ECCV</p></td><td><p>Priv.</p></td><td><p>66.6</p></td><td><p>57.4</p></td><td><p>49.0</p></td></tr><tr><td><p>[12]</p></td><td><p>2020</p></td><td><p>ECCV</p></td><td><p>Priv.</p></td><td><p>67.8</p></td><td><p>64.7</p></td><td><p>52.2</p></td></tr><tr><td><p>[5]</p></td><td><p>2021</p></td><td><p>ICRA</p></td><td><p>Priv.</p></td><td><p>73.2</p></td><td><p>66.5</p></td><td><p>55.2</p></td></tr><tr><td><p>[147]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>73.2</p></td><td><p>62.2</p></td><td><p>54.5</p></td></tr><tr><td><p>[159]</p></td><td><p>2022</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>73.6</p></td><td><p>65.2</p></td><td><p>53.9</p></td></tr><tr><td rowspan=\"5\">Corre</td><td><p>[73]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>Pub.</p></td><td><p>52.0</p></td><td><p>48.7</p></td><td><p>-</p></td></tr><tr><td><p>[163]</p></td><td><p>2020</p></td><td><p>AAAI</p></td><td><p>Pub.</p></td><td><p>49.5</p></td><td><p>51.8</p></td><td><p>41.5</p></td></tr><tr><td><p>[170]</p></td><td><p>2020</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>75.2</p></td><td><p>63.5</p></td><td><p>54.1</p></td></tr><tr><td><p>[165]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>Priv.</p></td><td><p>76.5</p></td><td><p>73.6</p></td><td><p>60.7</p></td></tr><tr><td><p>[78]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>Priv.</p></td><td><p>68.7</p></td><td><p>66.3</p></td><td><p>53.9</p></td></tr><tr><td rowspan=\"6\">Seq</td><td><p>[101]</p></td><td><p>2019</p></td><td><p>Neuroc.</p></td><td><p>Pub.</p></td><td><p>45.1</p></td><td><p>43.2</p></td><td><p>-</p></td></tr><tr><td><p>[177]</p></td><td><p>2021</p></td><td><p>ICCV</p></td><td><p>Pub.</p></td><td><p>73.1</p></td><td><p>67.2</p></td><td><p>54.2</p></td></tr><tr><td><p>[173]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>67.8</p></td><td><p>61.4</p></td><td><p>50.0</p></td></tr><tr><td><p>[179]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>Pub.</p></td><td><p>51.5</p></td><td><p>54.9</p></td><td><p>41.3</p></td></tr><tr><td><p>[176]</p></td><td><p>2021</p></td><td><p>ICCV</p></td><td><p>Priv.</p></td><td><p>74.9</p></td><td><p>75.0</p></td><td><p>62.0</p></td></tr><tr><td><p>[177]</p></td><td><p>2021</p></td><td><p>ICCV</p></td><td><p>Priv.</p></td><td><p>73.8</p></td><td><p>68.9</p></td><td><p>55.5</p></td></tr><tr><td rowspan=\"13\">Tracklet</td><td><p>[191]</p></td><td><p>2018</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>54.1</p></td><td><p>48.4</p></td><td><p>46.8</p></td></tr><tr><td><p>[199]</p></td><td><p>2018</p></td><td><p>ECCV</p></td><td><p>Pub.</p></td><td><p>48.2</p></td><td><p>55.7</p></td><td><p>42.5</p></td></tr><tr><td><p>[180]</p></td><td><p>2018</p></td><td><p>ECCV</p></td><td><p>Pub.</p></td><td><p>47.5</p></td><td><p>51.9</p></td><td><p>41.0</p></td></tr><tr><td><p>[194]</p></td><td><p>2019</p></td><td><p>MM</p></td><td><p>Pub.</p></td><td><p>51.9</p></td><td><p>58.1</p></td><td><p>44.9</p></td></tr><tr><td><p>[197]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>Pub.</p></td><td><p>50.9</p></td><td><p>56.0</p></td><td><p>42.6</p></td></tr><tr><td><p>[193]</p></td><td><p>2020</p></td><td><p>T-IP</p></td><td><p>Pub.</p></td><td><p>54.9</p></td><td><p>63.1</p></td><td><p>48.4</p></td></tr><tr><td><p>[183]</p></td><td><p>2020</p></td><td><p>PR</p></td><td><p>Pub.</p></td><td><p>54.2</p></td><td><p>52.6</p></td><td><p>41.5</p></td></tr><tr><td><p>[188]</p></td><td><p>2020</p></td><td><p>T-CSVT</p></td><td><p>Pub.</p></td><td><p>53.1</p></td><td><p>53.7</p></td><td><p>42.2</p></td></tr><tr><td><p>[186]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>Pub.</p></td><td><p>59.0</p></td><td><p>66.8</p></td><td><p>51.5</p></td></tr><tr><td><p>[198]</p></td><td><p>2021</p></td><td><p>NeurIPS</p></td><td><p>Pub.</p></td><td><p>56.8</p></td><td><p>58.3</p></td><td><p>46.4</p></td></tr><tr><td><p>[195]</p></td><td><p>2021</p></td><td><p>IS</p></td><td><p>Pub.</p></td><td><p>50.3</p></td><td><p>53.5</p></td><td><p>42.0</p></td></tr><tr><td><p>[109]</p></td><td><p>2021</p></td><td><p>J-VCIR</p></td><td><p>Pub.</p></td><td><p>45.4</p></td><td><p>39.9</p></td><td><p>34.0</p></td></tr><tr><td><p>[182]</p></td><td><p>2022</p></td><td><p>T-MM</p></td><td><p>Pub.</p></td><td><p>61.5</p></td><td><p>63.3</p></td><td><p>50.5</p></td></tr><tr><td rowspan=\"6\">X-Track</td><td><p>[6]</p></td><td><p>2020</p></td><td><p>CVPR</p></td><td><p>Pub.</p></td><td><p>58.8</p></td><td><p>61.7</p></td><td><p>49.0</p></td></tr><tr><td><p>[202]</p></td><td><p>2020</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>57.3</p></td><td><p>56.3</p></td><td><p>45.4</p></td></tr><tr><td><p>[201]</p></td><td><p>2020</p></td><td><p>IJCAI</p></td><td><p>Pub.</p></td><td><p>56.4</p></td><td><p>57.8</p></td><td><p>45.7</p></td></tr><tr><td><p>[137]</p></td><td><p>2020</p></td><td><p>CVPR</p></td><td><p>Pub.</p></td><td><p>53.7</p></td><td><p>53.8</p></td><td><p>42.4</p></td></tr><tr><td><p>[209]</p></td><td><p>2020</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>76.2</p></td><td><p>68.0</p></td><td><p>57.9</p></td></tr><tr><td><p>[5]</p></td><td><p>2021</p></td><td><p>ICRA</p></td><td><p>Priv.</p></td><td><p>73.2</p></td><td><p>66.5</p></td><td><p>55.2</p></td></tr></tbody></table>", "caption": "TABLE III: Summary of embedding methods on MOT17 benchmark.", "list_citation_info": ["[182] G. Wang, Y. Wang, R. Gu, W. Hu, and J.-N. Hwang, \u201cSplit and connect: A universal tracklet booster for multi-object tracking,\u201d IEEE TMM, 2022.", "[132] Q. Liu, D. Chen, Q. Chu, L. Yuan, B. Liu, L. Zhang, and N. Yu, \u201cOnline multi-object tracking with unsupervised re-identification learning and occlusion estimation,\u201d arXiv preprint arXiv:2201.01297, 2022.", "[173] A. Girbau, X. Gir\u00f3-i Nieto, I. Rius, and F. Marqu\u00e9s, \u201cMultiple object tracking with mixture density networks for trajectory estimation,\u201d arXiv preprint arXiv:2106.10950, 2021.", "[68] S. Karthik, A. Prabhu, and V. Gandhi, \u201cSimple unsupervised multi-object tracking,\u201d arXiv preprint arXiv:2006.02609, 2020.", "[35] E. Yu, Z. Li, S. Han, and H. Wang, \u201cRelationtrack: Relation-aware multiple object tracking with decoupled representation,\u201d arXiv preprint arXiv:2105.04322, 2021.", "[201] Q. Liu, Q. Chu, B. Liu, and N. Yu, \u201cGsm: Graph similarity model for multi-object tracking.\u201d in IJCAI, 2020, pp. 530\u2013536.", "[193] Y. Zhang, H. Sheng, Y. Wu, S. Wang, W. Lyu, W. Ke, and Z. Xiong, \u201cLong-term tracking with deep tracklet association,\u201d IEEE TIP, vol. 29, pp. 6694\u20136706, 2020.", "[188] J. Xiang, G. Xu, C. Ma, and J. Hou, \u201cEnd-to-end learning deep crf models for multi-object tracking deep crf models,\u201d IEEE TCSVT, vol. 31, no. 1, pp. 275\u2013288, 2020.", "[5] Y. Wang, K. Kitani, and X. Weng, \u201cJoint object detection and multi-object tracking with graph neural networks,\u201d in ICRA, 2021, pp. 13\u2009708\u201313\u2009715.", "[78] J. Pang, L. Qiu, X. Li, H. Chen, Q. Li, T. Darrell, and F. Yu, \u201cQuasi-dense similarity learning for multiple object tracking,\u201d in CVPR, 2021, pp. 164\u2013173.", "[159] X. Chen, S. M. Iranmanesh, and K.-C. Lien, \u201cPatchtrack: Multiple object tracking using frame patches,\u201d arXiv preprint arXiv:2201.00080, 2022.", "[101] M. Babaee, Z. Li, and G. Rigoll, \u201cA dual cnn\u2013rnn for multiple people tracking,\u201d Neurocomputing, vol. 368, pp. 69\u201383, 2019.", "[34] Y. Zhang, C. Wang, X. Wang, W. Zeng, and W. Liu, \u201cFairmot: On the fairness of detection and re-identification in multiple object tracking,\u201d IJCV, vol. 129, no. 11, pp. 3069\u20133087, 2021.", "[98] Y. Ye, X. Ke, and Z. Yu, \u201cA cost matrix optimization method based on spatial constraints under hungarian algorithm,\u201d in ICRAI, 2020, pp. 134\u2013139.", "[102] J. Chen, H. Sheng, Y. Zhang, and Z. Xiong, \u201cEnhancing detection model for multiple hypothesis tracking,\u201d in CVPR Workshops, 2017, pp. 18\u201327.", "[170] P. Sun, Y. Jiang, R. Zhang, E. Xie, J. Cao, X. Hu, T. Kong, Z. Yuan, C. Wang, and P. Luo, \u201cTranstrack: Multiple-object tracking with transformer,\u201d arXiv preprint arXiv:2012.15460, 2020.", "[186] P. Dai, R. Weng, W. Choi, C. Zhang, Z. He, and W. Ding, \u201cLearning a proposal classifier for multiple object tracking,\u201d in CVPR, 2021, pp. 2443\u20132452.", "[165] Q. Wang, Y. Zheng, P. Pan, and Y. Xu, \u201cMultiple object tracking with correlation learning,\u201d in CVPR, 2021, pp. 3876\u20133886.", "[177] P. Tokmakov, J. Li, W. Burgard, and A. Gaidon, \u201cLearning to track with object permanence,\u201d arXiv preprint arXiv:2103.14258, 2021.", "[71] F. Yang, X. Chang, S. Sakti, Y. Wu, and S. Nakamura, \u201cRemot: A model-agnostic refinement for multiple object tracking,\u201d IVC, vol. 106, p. 104091, 2021.", "[13] J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang, and Y. Fu, \u201cChained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking,\u201d in ECCV, 2020, pp. 145\u2013161.", "[209] C. Shan, C. Wei, B. Deng, J. Huang, X.-S. Hua, X. Cheng, and K. Liang, \u201cTracklets predicting based adaptive graph tracking,\u201d arXiv preprint arXiv:2010.09015, 2020.", "[199] J. Zhu, H. Yang, N. Liu, M. Kim, W. Zhang, and M.-H. Yang, \u201cOnline multi-object tracking with dual matching attention networks,\u201d in ECCV, 2018, pp. 366\u2013382.", "[146] S. Sun, N. Akhtar, H. Song, A. Mian, and M. Shah, \u201cDeep affinity network for multiple object tracking,\u201d IEEE TPAMI, vol. 43, no. 1, pp. 104\u2013119, 2019.", "[195] Y.-C. Yoon, D. Y. Kim, Y.-m. Song, K. Yoon, and M. Jeon, \u201cOnline multiple pedestrians tracking using deep temporal appearance matching association,\u201d Information Sciences, vol. 561, pp. 326\u2013351, 2021.", "[125] Q. Liu, B. Liu, Y. Wu, W. Li, and N. Yu, \u201cReal-time online multi-object tracking in compressed domain,\u201d IEEE Access, vol. 7, pp. 76\u2009489\u201376\u2009499, 2019.", "[197] J. Xu, Y. Cao, Z. Zhang, and H. Hu, \u201cSpatial-temporal relation networks for multi-object tracking,\u201d in ICCV, 2019, pp. 3988\u20133998.", "[180] C. Kim, F. Li, and J. M. Rehg, \u201cMulti-object tracking with neural gating using bilinear lstm,\u201d in ECCV, 2018, pp. 200\u2013215.", "[105] H. Shen, L. Huang, C. Huang, and W. Xu, \u201cTracklet association tracker: An end-to-end learning-based association approach for multi-object tracking,\u201d arXiv preprint arXiv:1808.01562, 2018.", "[194] G. Wang, Y. Wang, H. Zhang, R. Gu, and J.-N. Hwang, \u201cExploit the connectivity: Multi-object tracking with trackletnet,\u201d in ACM MM, 2019, pp. 482\u2013490.", "[163] Q. Chu, W. Ouyang, B. Liu, F. Zhu, and N. Yu, \u201cDasot: A unified framework integrating data association and single object tracking for online multi-object tracking,\u201d in AAAI, vol. 34, no. 07, 2020, pp. 10\u2009672\u201310\u2009679.", "[179] C. Kim, L. Fuxin, M. Alotaibi, and J. M. Rehg, \u201cDiscriminative appearance modeling with multi-track pooling for real-time multi-object tracking,\u201d in CVPR, 2021, pp. 9553\u20139562.", "[103] L. Chen, H. Ai, Z. Zhuang, and C. Shang, \u201cReal-time multiple people tracking with deeply learned candidate selection and person re-identification,\u201d in ICME, 2018, pp. 1\u20136.", "[6] G. Bras\u00f3 and L. Leal-Taix\u00e9, \u201cLearning a neural solver for multiple object tracking,\u201d in CVPR, 2020, pp. 6247\u20136257.", "[198] F. Bastani, S. He, and S. Madden, \u201cSelf-supervised multi-object tracking with cross-input consistency,\u201d NeurIPS, vol. 34, 2021.", "[202] I. Papakis, A. Sarkar, and A. Karpatne, \u201cGcnnmatch: Graph convolutional neural networks for multi-object tracking via sinkhorn normalization,\u201d arXiv preprint arXiv:2010.00067, 2020.", "[176] S. Wang, H. Sheng, Y. Zhang, Y. Wu, and Z. Xiong, \u201cA general recurrent tracking framework without real data,\u201d in ICCV, 2021, pp. 13\u2009219\u201313\u2009228.", "[191] M. Babaee, A. Athar, and G. Rigoll, \u201cMultiple people tracking using hierarchical deep tracklet re-identification,\u201d arXiv preprint arXiv:1811.04091, 2018.", "[33] J. Yang, H. Ge, J. Yang, Y. Tong, and S. Su, \u201cOnline multi-object tracking using multi-function integration and tracking simulation training,\u201d AI, pp. 1\u201321, 2021.", "[106] W. Li, Y. Xiong, S. Yang, M. Xu, Y. Wang, and W. Xia, \u201cSemi-tcl: Semi-supervised track contrastive representation learning,\u201d arXiv preprint arXiv:2107.02396, 2021.", "[137] Y. Xu, A. Osep, Y. Ban, R. Horaud, L. Leal-Taix\u00e9, and X. Alameda-Pineda, \u201cHow to train your deep multi-object tracker,\u201d in CVPR, 2020, pp. 6787\u20136796.", "[15] P. Bergmann, T. Meinhardt, and L. Leal-Taixe, \u201cTracking without bells and whistles,\u201d in ICCV, 2019, pp. 941\u2013951.", "[109] N. L. Baisa, \u201cRobust online multi-target visual tracking using a hisp filter with discriminative deep appearance learning,\u201d JVCIR, vol. 77, p. 102952, 2021.", "[147] Y. Xu, Y. Ban, G. Delorme, C. Gan, D. Rus, and X. Alameda-Pineda, \u201cTranscenter: Transformers with dense queries for multiple-object tracking,\u201d arXiv preprint arXiv:2103.15145, 2021.", "[12] X. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl, \u201cTracking objects as points,\u201d in ECCV, 2020, pp. 474\u2013490.", "[183] J. Peng, T. Wang, W. Lin, J. Wang, J. See, S. Wen, and E. Ding, \u201cTpm: Multiple object tracking with tracklet-plane matching,\u201d Pattern Recognition, vol. 107, p. 107480, 2020.", "[72] W. Feng, Z. Hu, W. Wu, J. Yan, and W. Ouyang, \u201cMulti-object tracking with multiple cues and switcher-aware classification,\u201d arXiv preprint arXiv:1901.06129, 2019.", "[73] P. Chu and H. Ling, \u201cFamnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking,\u201d in ICCV, 2019, pp. 6172\u20136181.", "[70] N. L. Baisa, \u201cOcclusion-robust online multi-object visual tracking using a gm-phd filter with cnn-based re-identification,\u201d JVCIR, vol. 80, p. 103279, 2021.", "[100] Y.-c. Yoon, A. Boragule, Y.-m. Song, K. Yoon, and M. Jeon, \u201cOnline multi-object tracking with historical appearance matching and scene adaptive detection filtering,\u201d in AVSS, 2018, pp. 1\u20136.", "[14] B. Pang, Y. Li, Y. Zhang, M. Li, and C. Lu, \u201cTubetk: Adopting tubes to track multi-object in a one-step training model,\u201d in CVPR, 2020, pp. 6308\u20136318."]}, {"table": "<table><tbody><tr><td>Emb. Method</td><td>Ref.</td><td>Year</td><td>Venue</td><td>Det.</td><td>MOTA</td><td>IDF1</td><td>HOTA</td></tr><tr><td rowspan=\"4\">Patch</td><td><p>[68]</p></td><td><p>2020</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>53.6</p></td><td><p>50.6</p></td><td><p>41.7</p></td></tr><tr><td><p>[70]</p></td><td><p>2021</p></td><td><p>J-VCIR</p></td><td><p>Pub.</p></td><td><p>44.7</p></td><td><p>43.5</p></td><td><p>35.6</p></td></tr><tr><td><p>[71]</p></td><td><p>2021</p></td><td><p>IVC</p></td><td><p>Priv.</p></td><td><p>77.4</p></td><td><p>73.1</p></td><td><p>61.2</p></td></tr><tr><td><p>[106]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>65.2</p></td><td><p>70.1</p></td><td><p>55.3</p></td></tr><tr><td rowspan=\"5\">S-Fr</td><td><p>[15]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>Pub.</p></td><td><p>52.6</p></td><td><p>52.7</p></td><td><p>42.1</p></td></tr><tr><td><p>[33]</p></td><td><p>2021</p></td><td><p>AI</p></td><td><p>Pub.</p></td><td><p>59.3</p></td><td><p>59.1</p></td><td><p>47.1</p></td></tr><tr><td><p>[35]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>67.2</p></td><td><p>70.5</p></td><td><p>56.5</p></td></tr><tr><td><p>[34]</p></td><td><p>2021</p></td><td><p>IJCV</p></td><td><p>Priv.</p></td><td><p>61.8</p></td><td><p>67.3</p></td><td><p>54.6</p></td></tr><tr><td><p>[132]</p></td><td><p>2022</p></td><td><p>Neuroc.</p></td><td><p>Priv.</p></td><td><p>68.6</p></td><td><p>69.4</p></td><td><p>56.2</p></td></tr><tr><td rowspan=\"2\">X-Fr</td><td><p>[147]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>61.0</p></td><td><p>49.8</p></td><td><p>43.5</p></td></tr><tr><td><p>[5]</p></td><td><p>2021</p></td><td><p>ICRA</p></td><td><p>Priv.</p></td><td><p>67.1</p></td><td><p>67.5</p></td><td><p>53.6</p></td></tr><tr><td>Corre</td><td><p>[170]</p></td><td><p>2020</p></td><td><p>ArXiv</p></td><td><p>Priv.</p></td><td><p>65.0</p></td><td><p>59.4</p></td><td><p>48.9</p></td></tr><tr><td rowspan=\"2\">Tracklet</td><td><p>[186]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>Pub.</p></td><td><p>56.3</p></td><td><p>62.5</p></td><td><p>49.0</p></td></tr><tr><td><p>[182]</p></td><td><p>2022</p></td><td><p>T-MM</p></td><td><p>Pub.</p></td><td><p>54.6</p></td><td><p>53.4</p></td><td><p>42.5</p></td></tr><tr><td rowspan=\"2\">X-Track</td><td><p>[6]</p></td><td><p>2020</p></td><td><p>CVPR</p></td><td><p>Pub.</p></td><td><p>57.6</p></td><td><p>59.1</p></td><td><p>46.8</p></td></tr><tr><td><p>[202]</p></td><td><p>2020</p></td><td><p>ArXiv</p></td><td><p>Pub.</p></td><td><p>54.5</p></td><td><p>49.0</p></td><td><p>40.2</p></td></tr></tbody></table>", "caption": "TABLE IV: Summary of embedding methods on MOT20 benchmark.", "list_citation_info": ["[182] G. Wang, Y. Wang, R. Gu, W. Hu, and J.-N. Hwang, \u201cSplit and connect: A universal tracklet booster for multi-object tracking,\u201d IEEE TMM, 2022.", "[15] P. Bergmann, T. Meinhardt, and L. Leal-Taixe, \u201cTracking without bells and whistles,\u201d in ICCV, 2019, pp. 941\u2013951.", "[132] Q. Liu, D. Chen, Q. Chu, L. Yuan, B. Liu, L. Zhang, and N. Yu, \u201cOnline multi-object tracking with unsupervised re-identification learning and occlusion estimation,\u201d arXiv preprint arXiv:2201.01297, 2022.", "[6] G. Bras\u00f3 and L. Leal-Taix\u00e9, \u201cLearning a neural solver for multiple object tracking,\u201d in CVPR, 2020, pp. 6247\u20136257.", "[68] S. Karthik, A. Prabhu, and V. Gandhi, \u201cSimple unsupervised multi-object tracking,\u201d arXiv preprint arXiv:2006.02609, 2020.", "[71] F. Yang, X. Chang, S. Sakti, Y. Wu, and S. Nakamura, \u201cRemot: A model-agnostic refinement for multiple object tracking,\u201d IVC, vol. 106, p. 104091, 2021.", "[35] E. Yu, Z. Li, S. Han, and H. Wang, \u201cRelationtrack: Relation-aware multiple object tracking with decoupled representation,\u201d arXiv preprint arXiv:2105.04322, 2021.", "[202] I. Papakis, A. Sarkar, and A. Karpatne, \u201cGcnnmatch: Graph convolutional neural networks for multi-object tracking via sinkhorn normalization,\u201d arXiv preprint arXiv:2010.00067, 2020.", "[5] Y. Wang, K. Kitani, and X. Weng, \u201cJoint object detection and multi-object tracking with graph neural networks,\u201d in ICRA, 2021, pp. 13\u2009708\u201313\u2009715.", "[34] Y. Zhang, C. Wang, X. Wang, W. Zeng, and W. Liu, \u201cFairmot: On the fairness of detection and re-identification in multiple object tracking,\u201d IJCV, vol. 129, no. 11, pp. 3069\u20133087, 2021.", "[70] N. L. Baisa, \u201cOcclusion-robust online multi-object visual tracking using a gm-phd filter with cnn-based re-identification,\u201d JVCIR, vol. 80, p. 103279, 2021.", "[33] J. Yang, H. Ge, J. Yang, Y. Tong, and S. Su, \u201cOnline multi-object tracking using multi-function integration and tracking simulation training,\u201d AI, pp. 1\u201321, 2021.", "[106] W. Li, Y. Xiong, S. Yang, M. Xu, Y. Wang, and W. Xia, \u201cSemi-tcl: Semi-supervised track contrastive representation learning,\u201d arXiv preprint arXiv:2107.02396, 2021.", "[170] P. Sun, Y. Jiang, R. Zhang, E. Xie, J. Cao, X. Hu, T. Kong, Z. Yuan, C. Wang, and P. Luo, \u201cTranstrack: Multiple-object tracking with transformer,\u201d arXiv preprint arXiv:2012.15460, 2020.", "[186] P. Dai, R. Weng, W. Choi, C. Zhang, Z. He, and W. Ding, \u201cLearning a proposal classifier for multiple object tracking,\u201d in CVPR, 2021, pp. 2443\u20132452.", "[147] Y. Xu, Y. Ban, G. Delorme, C. Gan, D. Rus, and X. Alameda-Pineda, \u201cTranscenter: Transformers with dense queries for multiple-object tracking,\u201d arXiv preprint arXiv:2103.15145, 2021."]}, {"table": "<table><thead><tr><th>Emb. Method</th><th>Ref.</th><th>Year</th><th>Venue</th><th>Mod.</th><th>Obj.</th><th>MOTA</th><th>HOTA</th></tr></thead><tbody><tr><td rowspan=\"5\"><p>Patch</p></td><td><p>[128]</p></td><td><p>2019</p></td><td><p>ArXiv</p></td><td><p>V</p></td><td><p>C</p></td><td><p>75.8</p></td><td><p>60.9</p></td></tr><tr><td><p>[237]</p></td><td><p>2020</p></td><td><p>IROS</p></td><td><p>V&amp;L</p></td><td><p>C</p></td><td><p>85.1</p></td><td><p>69.6</p></td></tr><tr><td><p>[238]</p></td><td><p>2020</p></td><td><p>ACCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>87.8</p></td><td><p>68.5</p></td></tr><tr><td><p>[238]</p></td><td><p>2020</p></td><td><p>ACCV</p></td><td><p>V</p></td><td><p>P</p></td><td><p>68.0</p></td><td><p>50.9</p></td></tr><tr><td><p>[237]</p></td><td><p>2020</p></td><td><p>IROS</p></td><td><p>V&amp;L</p></td><td><p>P</p></td><td><p>45.3</p></td><td><p>34.2</p></td></tr><tr><td><p>S-Fr</p></td><td><p>[28]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>84.3</p></td><td><p>73.2</p></td></tr><tr><td rowspan=\"8\"><p>X-Fr</p></td><td><p>[239]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>V&amp;L</p></td><td><p>C</p></td><td><p>83.2</p></td><td><p>62.1</p></td></tr><tr><td><p>[240]</p></td><td><p>2020</p></td><td><p>RAL</p></td><td><p>L</p></td><td><p>C</p></td><td><p>67.6</p></td><td><p>57.2</p></td></tr><tr><td><p>[12]</p></td><td><p>2020</p></td><td><p>ECCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>88.8</p></td><td><p>73.0</p></td></tr><tr><td><p>[241]</p></td><td><p>2021</p></td><td><p>IJCAI</p></td><td><p>L</p></td><td><p>C</p></td><td><p>91.7</p></td><td><p>80.9</p></td></tr><tr><td><p>[27]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>V</p></td><td><p>C</p></td><td><p>88.4</p></td><td><p>74.2</p></td></tr><tr><td><p>[242]</p></td><td><p>2021</p></td><td><p>RAL</p></td><td><p>L</p></td><td><p>C</p></td><td><p>84.5</p></td><td><p>72.2</p></td></tr><tr><td><p>[243]</p></td><td><p>2021</p></td><td><p>IROS</p></td><td><p>V&amp;L</p></td><td><p>C</p></td><td><p>85.4</p></td><td><p>70.7</p></td></tr><tr><td><p>[12]</p></td><td><p>2020</p></td><td><p>ECCV</p></td><td><p>V</p></td><td><p>P</p></td><td><p>53.8</p></td><td><p>40.4</p></td></tr><tr><td rowspan=\"5\"><p>Corre</p></td><td><p>[73]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>75.9</p></td><td><p>52.6</p></td></tr><tr><td><p>[244]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>V</p></td><td><p>C</p></td><td><p>85.9</p></td><td><p>72.8</p></td></tr><tr><td><p>[78]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>V</p></td><td><p>C</p></td><td><p>84.9</p></td><td><p>68.5</p></td></tr><tr><td><p>[78]</p></td><td><p>2021</p></td><td><p>CVPR</p></td><td><p>V</p></td><td><p>P</p></td><td><p>55.6</p></td><td><p>41.1</p></td></tr><tr><td><p>[244]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>V</p></td><td><p>P</p></td><td><p>51.8</p></td><td><p>41.1</p></td></tr><tr><td rowspan=\"3\"><p>Seq</p></td><td><p>[28]</p></td><td><p>2019</p></td><td><p>ICCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>84.3</p></td><td><p>73.2</p></td></tr><tr><td><p>[177]</p></td><td><p>2021</p></td><td><p>ICCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>91.3</p></td><td><p>78.0</p></td></tr><tr><td><p>[177]</p></td><td><p>2021</p></td><td><p>ICCV</p></td><td><p>V</p></td><td><p>P</p></td><td><p>66.0</p></td><td><p>48.6</p></td></tr><tr><td><p>Tracklet</p></td><td><p>[40]</p></td><td><p>2021</p></td><td><p>ICCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>87.6</p></td><td><p>73.1</p></td></tr><tr><td rowspan=\"4\"><p>X-Track</p></td><td><p>[40]</p></td><td><p>2021</p></td><td><p>ICCV</p></td><td><p>V</p></td><td><p>C</p></td><td><p>87.6</p></td><td><p>73.1</p></td></tr><tr><td><p>[208]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>V</p></td><td><p>C</p></td><td><p>87.3</p></td><td><p>72.3</p></td></tr><tr><td><p>[6]</p></td><td><p>2020</p></td><td><p>CVPR</p></td><td><p>V</p></td><td><p>P</p></td><td><p>46.2</p></td><td><p>45.3</p></td></tr><tr><td><p>[208]</p></td><td><p>2021</p></td><td><p>ArXiv</p></td><td><p>V</p></td><td><p>P</p></td><td><p>52.1</p></td><td><p>39.4</p></td></tr></tbody></table>", "caption": "TABLE V: Summary of embedding methods on KITTI MOT benchmark.", "list_citation_info": ["[27] M. Chaabane, P. Zhang, J. R. Beveridge, and S. O\u2019Hara, \u201cDeft: Detection embeddings for tracking,\u201d arXiv preprint arXiv:2102.02267, 2021.", "[237] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatofighi, R. Mart\u00edn-Mart\u00edn, and S. Savarese, \u201cJrmot: A real-time 3d multi-object tracker and a new large-scale dataset,\u201d in IROS, 2020, pp. 10\u2009335\u201310\u2009342.", "[238] D. Mykheievskyi, D. Borysenko, and V. Porokhonskyy, \u201cLearning local feature descriptors for multiple object tracking,\u201d in ACCV, 2020.", "[241] H. Wu, Q. Li, C. Wen, X. Li, X. Fan, and C. Wang, \u201cTracklet proposal network for multi-object tracking on point clouds,\u201d in IJCAI, 2021, pp. 1165\u20131171.", "[244] H.-N. Hu, Y.-H. Yang, T. Fischer, T. Darrell, F. Yu, and M. Sun, \u201cMonocular quasi-dense 3d object tracking,\u201d arXiv preprint arXiv:2103.07351, 2021.", "[6] G. Bras\u00f3 and L. Leal-Taix\u00e9, \u201cLearning a neural solver for multiple object tracking,\u201d in CVPR, 2020, pp. 6247\u20136257.", "[177] P. Tokmakov, J. Li, W. Burgard, and A. Gaidon, \u201cLearning to track with object permanence,\u201d arXiv preprint arXiv:2103.14258, 2021.", "[240] S. Wang, Y. Sun, C. Liu, and M. Liu, \u201cPointtracknet: An end-to-end network for 3-d object detection and tracking from point clouds,\u201d IEEE RAL, vol. 5, no. 2, pp. 3206\u20133212, 2020.", "[12] X. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl, \u201cTracking objects as points,\u201d in ECCV, 2020, pp. 474\u2013490.", "[243] K. Huang and Q. Hao, \u201cJoint multi-object detection and tracking with camera-lidar fusion for autonomous driving,\u201d in IROS, 2021, pp. 6983\u20136989.", "[239] W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, and C. C. Loy, \u201cRobust multi-modality multi-object tracking,\u201d in ICCV, 2019, pp. 2365\u20132374.", "[28] H.-N. Hu, Q.-Z. Cai, D. Wang, J. Lin, M. Sun, P. Krahenbuhl, T. Darrell, and F. Yu, \u201cJoint monocular 3d vehicle detection and tracking,\u201d in ICCV, 2019, pp. 5390\u20135399.", "[40] G. Wang, R. Gu, Z. Liu, W. Hu, M. Song, and J.-N. Hwang, \u201cTrack without appearance: Learn box and tracklet embedding with local and global motion patterns for vehicle tracking,\u201d in ICCV, 2021, pp. 9876\u20139886.", "[128] E. Baser, V. Balasubramanian, P. Bhattacharyya, and K. Czarnecki, \u201cFantrack: 3d multi-object tracking with feature association network,\u201d in IV, 2019, pp. 1426\u20131433.", "[73] P. Chu and H. Ling, \u201cFamnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking,\u201d in ICCV, 2019, pp. 6172\u20136181.", "[78] J. Pang, L. Qiu, X. Li, H. Chen, Q. Li, T. Darrell, and F. Yu, \u201cQuasi-dense similarity learning for multiple object tracking,\u201d in CVPR, 2021, pp. 164\u2013173.", "[208] A. Rangesh, P. Maheshwari, M. Gebre, S. Mhatre, V. Ramezani, and M. M. Trivedi, \u201cTrackmpnn: A message passing graph neural architecture for multi-object tracking,\u201d arXiv preprint arXiv:2101.04206, 2021.", "[242] S. Wang, P. Cai, L. Wang, and M. Liu, \u201cDitnet: End-to-end 3d object detection and track id assignment in spatio-temporal world,\u201d IEEE RAL, vol. 6, no. 2, pp. 3397\u20133404, 2021."]}], "citation_info_to_title": {"[173] A. Girbau, X. Gir\u00f3-i Nieto, I. Rius, and F. Marqu\u00e9s, \u201cMultiple object tracking with mixture density networks for trajectory estimation,\u201d arXiv preprint arXiv:2106.10950, 2021.": "Multiple object tracking with mixture density networks for trajectory estimation", "[194] G. Wang, Y. Wang, H. Zhang, R. Gu, and J.-N. Hwang, \u201cExploit the connectivity: Multi-object tracking with trackletnet,\u201d in ACM MM, 2019, pp. 482\u2013490.": "Exploit the connectivity: Multi-object tracking with trackletnet", "[165] Q. Wang, Y. Zheng, P. Pan, and Y. Xu, \u201cMultiple object tracking with correlation learning,\u201d in CVPR, 2021, pp. 3876\u20133886.": "Multiple object tracking with correlation learning", "[106] W. Li, Y. Xiong, S. Yang, M. Xu, Y. Wang, and W. Xia, \u201cSemi-tcl: Semi-supervised track contrastive representation learning,\u201d arXiv preprint arXiv:2107.02396, 2021.": "Semi-tcl: Semi-supervised Track Contrastive Representation Learning", "[209] C. Shan, C. Wei, B. Deng, J. Huang, X.-S. Hua, X. Cheng, and K. Liang, \u201cTracklets predicting based adaptive graph tracking,\u201d arXiv preprint arXiv:2010.09015, 2020.": "Tracklets Predicting Based Adaptive Graph Tracking", "[73] P. Chu and H. Ling, \u201cFamnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking,\u201d in ICCV, 2019, pp. 6172\u20136181.": "Famnet: Joint learning of feature, affinity and multi-dimensional assignment for online multiple object tracking", "[211] A. Geiger, P. Lenz, and R. Urtasun, \u201cAre we ready for autonomous driving? the kitti vision benchmark suite,\u201d in CVPR, 2012, pp. 3354\u20133361.": "Are we ready for autonomous driving? The KITTI Vision Benchmark Suite", "[238] D. Mykheievskyi, D. Borysenko, and V. Porokhonskyy, \u201cLearning local feature descriptors for multiple object tracking,\u201d in ACCV, 2020.": "Learning Local Feature Descriptors for Multiple Object Tracking", "[219] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell, \u201cBdd100k: A diverse driving dataset for heterogeneous multitask learning,\u201d in CVPR, 2020, pp. 2636\u20132645.": "Bdd100k: A diverse driving dataset for heterogeneous multitask learning", "[132] Q. Liu, D. Chen, Q. Chu, L. Yuan, B. Liu, L. Zhang, and N. Yu, \u201cOnline multi-object tracking with unsupervised re-identification learning and occlusion estimation,\u201d arXiv preprint arXiv:2201.01297, 2022.": "Online multi-object tracking with unsupervised re-identification learning and occlusion estimation", "[237] A. Shenoi, M. Patel, J. Gwak, P. Goebel, A. Sadeghian, H. Rezatofighi, R. Mart\u00edn-Mart\u00edn, and S. Savarese, \u201cJrmot: A real-time 3d multi-object tracker and a new large-scale dataset,\u201d in IROS, 2020, pp. 10\u2009335\u201310\u2009342.": "Jrmot: A real-time 3d multi-object tracker and a new large-scale dataset", "[143] A. Milan, L. Leal-Taix\u00e9, I. Reid, S. Roth, and K. Schindler, \u201cMot16: A benchmark for multi-object tracking,\u201d arXiv preprint arXiv:1603.00831, 2016.": "Mot16: A benchmark for multi-object tracking", "[146] S. Sun, N. Akhtar, H. Song, A. Mian, and M. Shah, \u201cDeep affinity network for multiple object tracking,\u201d IEEE TPAMI, vol. 43, no. 1, pp. 104\u2013119, 2019.": "Deep affinity network for multiple object tracking", "[101] M. Babaee, Z. Li, and G. Rigoll, \u201cA dual cnn\u2013rnn for multiple people tracking,\u201d Neurocomputing, vol. 368, pp. 69\u201383, 2019.": "A dual cnn-rnn for multiple people tracking", "[218] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine et al., \u201cScalability in perception for autonomous driving: Waymo open dataset,\u201d in CVPR, 2020, pp. 2446\u20132454.": "Scalability in perception for autonomous driving: Waymo open dataset", "[34] Y. Zhang, C. Wang, X. Wang, W. Zeng, and W. Liu, \u201cFairmot: On the fairness of detection and re-identification in multiple object tracking,\u201d IJCV, vol. 129, no. 11, pp. 3069\u20133087, 2021.": "Fairmot: On the fairness of detection and re-identification in multiple object tracking", "[244] H.-N. Hu, Y.-H. Yang, T. Fischer, T. Darrell, F. Yu, and M. Sun, \u201cMonocular quasi-dense 3d object tracking,\u201d arXiv preprint arXiv:2103.07351, 2021.": "Monocular quasi-dense 3d object tracking", "[72] W. Feng, Z. Hu, W. Wu, J. Yan, and W. Ouyang, \u201cMulti-object tracking with multiple cues and switcher-aware classification,\u201d arXiv preprint arXiv:1901.06129, 2019.": "Multi-object tracking with multiple cues and switcher-aware classification", "[137] Y. Xu, A. Osep, Y. Ban, R. Horaud, L. Leal-Taix\u00e9, and X. Alameda-Pineda, \u201cHow to train your deep multi-object tracker,\u201d in CVPR, 2020, pp. 6787\u20136796.": "How to train your deep multi-object tracker", "[180] C. Kim, F. Li, and J. M. Rehg, \u201cMulti-object tracking with neural gating using bilinear lstm,\u201d in ECCV, 2018, pp. 200\u2013215.": "Multi-object tracking with neural gating using bilinear lstm", "[13] J. Peng, C. Wang, F. Wan, Y. Wu, Y. Wang, Y. Tai, C. Wang, J. Li, F. Huang, and Y. Fu, \u201cChained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking,\u201d in ECCV, 2020, pp. 145\u2013161.": "Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking", "[215] M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall, and B. Schiele, \u201cPosetrack: A benchmark for human pose estimation and tracking,\u201d in CVPR, 2018, pp. 5167\u20135176.": "Posetrack: A benchmark for human pose estimation and tracking", "[197] J. Xu, Y. Cao, Z. Zhang, and H. Hu, \u201cSpatial-temporal relation networks for multi-object tracking,\u201d in ICCV, 2019, pp. 3988\u20133998.": "Spatial-temporal relation networks for multi-object tracking", "[159] X. Chen, S. M. Iranmanesh, and K.-C. Lien, \u201cPatchtrack: Multiple object tracking using frame patches,\u201d arXiv preprint arXiv:2201.00080, 2022.": "Patchtrack: Multiple Object Tracking Using Frame Patches", "[27] M. Chaabane, P. Zhang, J. R. Beveridge, and S. O\u2019Hara, \u201cDeft: Detection embeddings for tracking,\u201d arXiv preprint arXiv:2102.02267, 2021.": "Deft: Detection embeddings for tracking", "[35] E. Yu, Z. Li, S. Han, and H. Wang, \u201cRelationtrack: Relation-aware multiple object tracking with decoupled representation,\u201d arXiv preprint arXiv:2105.04322, 2021.": "Relationtrack: Relation-aware multiple object tracking with decoupled representation", "[188] J. Xiang, G. Xu, C. Ma, and J. Hou, \u201cEnd-to-end learning deep crf models for multi-object tracking deep crf models,\u201d IEEE TCSVT, vol. 31, no. 1, pp. 275\u2013288, 2020.": "End-to-end learning deep crf models for multi-object tracking deep crf models", "[177] P. Tokmakov, J. Li, W. Burgard, and A. Gaidon, \u201cLearning to track with object permanence,\u201d arXiv preprint arXiv:2103.14258, 2021.": "Learning to track with object permanence", "[105] H. Shen, L. Huang, C. Huang, and W. Xu, \u201cTracklet association tracker: An end-to-end learning-based association approach for multi-object tracking,\u201d arXiv preprint arXiv:1808.01562, 2018.": "Tracklet association tracker: An end-to-end learning-based association approach for multi-object tracking", "[186] P. Dai, R. Weng, W. Choi, C. Zhang, Z. He, and W. Ding, \u201cLearning a proposal classifier for multiple object tracking,\u201d in CVPR, 2021, pp. 2443\u20132452.": "Learning a proposal classifier for multiple object tracking", "[208] A. Rangesh, P. Maheshwari, M. Gebre, S. Mhatre, V. Ramezani, and M. M. Trivedi, \u201cTrackmpnn: A message passing graph neural architecture for multi-object tracking,\u201d arXiv preprint arXiv:2101.04206, 2021.": "Trackmpnn: A message passing graph neural architecture for multi-object tracking", "[28] H.-N. Hu, Q.-Z. Cai, D. Wang, J. Lin, M. Sun, P. Krahenbuhl, T. Darrell, and F. Yu, \u201cJoint monocular 3d vehicle detection and tracking,\u201d in ICCV, 2019, pp. 5390\u20135399.": "Joint Monocular 3D Vehicle Detection and Tracking", "[239] W. Zhang, H. Zhou, S. Sun, Z. Wang, J. Shi, and C. C. Loy, \u201cRobust multi-modality multi-object tracking,\u201d in ICCV, 2019, pp. 2365\u20132374.": "Robust multi-modality multi-object tracking", "[243] K. Huang and Q. Hao, \u201cJoint multi-object detection and tracking with camera-lidar fusion for autonomous driving,\u201d in IROS, 2021, pp. 6983\u20136989.": "Joint multi-object detection and tracking with camera-lidar fusion for autonomous driving", "[124] S. Manen, M. Gygli, D. Dai, and L. Van Gool, \u201cPathtrack: Fast trajectory annotation with path supervision,\u201d in ICCV, 2017, pp. 290\u2013299.": "Pathtrack: Fast trajectory annotation with path supervision", "[214] L. Wen, D. Du, Z. Cai, Z. Lei, M.-C. Chang, H. Qi, J. Lim, M.-H. Yang, and S. Lyu, \u201cUa-detrac: A new benchmark and protocol for multi-object detection and tracking,\u201d CVIU, vol. 193, p. 102907, 2020.": "UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking", "[117] E. Ristani, F. Solera, R. Zou, R. Cucchiara, and C. Tomasi, \u201cPerformance measures and a data set for multi-target, multi-camera tracking,\u201d in ECCV, 2016, pp. 17\u201335.": "Performance measures and a data set for multi-target, multi-camera tracking", "[240] S. Wang, Y. Sun, C. Liu, and M. Liu, \u201cPointtracknet: An end-to-end network for 3-d object detection and tracking from point clouds,\u201d IEEE RAL, vol. 5, no. 2, pp. 3206\u20133212, 2020.": "Pointtracknet: An end-to-end network for 3-d object detection and tracking from point clouds", "[147] Y. Xu, Y. Ban, G. Delorme, C. Gan, D. Rus, and X. Alameda-Pineda, \u201cTranscenter: Transformers with dense queries for multiple-object tracking,\u201d arXiv preprint arXiv:2103.15145, 2021.": "Transcenter: Transformers with dense queries for multiple-object tracking", "[179] C. Kim, L. Fuxin, M. Alotaibi, and J. M. Rehg, \u201cDiscriminative appearance modeling with multi-track pooling for real-time multi-object tracking,\u201d in CVPR, 2021, pp. 9553\u20139562.": "Discriminative Appearance Modeling with Multi-Track Pooling for Real-Time Multi-Object Tracking", "[163] Q. Chu, W. Ouyang, B. Liu, F. Zhu, and N. Yu, \u201cDasot: A unified framework integrating data association and single object tracking for online multi-object tracking,\u201d in AAAI, vol. 34, no. 07, 2020, pp. 10\u2009672\u201310\u2009679.": "Dasot: A unified framework integrating data association and single object tracking for online multi-object tracking", "[15] P. Bergmann, T. Meinhardt, and L. Leal-Taixe, \u201cTracking without bells and whistles,\u201d in ICCV, 2019, pp. 941\u2013951.": "Tracking without bells and whistles", "[201] Q. Liu, Q. Chu, B. Liu, and N. Yu, \u201cGsm: Graph similarity model for multi-object tracking.\u201d in IJCAI, 2020, pp. 530\u2013536.": "Gsm: Graph Similarity Model for Multi-Object Tracking", "[242] S. Wang, P. Cai, L. Wang, and M. Liu, \u201cDitnet: End-to-end 3d object detection and track id assignment in spatio-temporal world,\u201d IEEE RAL, vol. 6, no. 2, pp. 3397\u20133404, 2021.": "Ditnet: End-to-end 3d object detection and track id assignment in spatio-temporal world", "[37] P. Voigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar, A. Geiger, and B. Leibe, \u201cMots: Multi-object tracking and segmentation,\u201d in CVPR, 2019, pp. 7942\u20137951.": "Mots: Multi-object tracking and segmentation", "[202] I. Papakis, A. Sarkar, and A. Karpatne, \u201cGcnnmatch: Graph convolutional neural networks for multi-object tracking via sinkhorn normalization,\u201d arXiv preprint arXiv:2010.00067, 2020.": "GCNNMATCH: Graph Convolutional Neural Networks for Multi-Object Tracking via Sinkhorn Normalization", "[144] P. Dendorfer, H. Rezatofighi, A. Milan, J. Shi, D. Cremers, I. Reid, S. Roth, K. Schindler, and L. Leal-Taix\u00e9, \u201cMot20: A benchmark for multi object tracking in crowded scenes,\u201d arXiv preprint arXiv:2003.09003, 2020.": "Mot20: A benchmark for multi object tracking in crowded scenes", "[193] Y. Zhang, H. Sheng, Y. Wu, S. Wang, W. Lyu, W. Ke, and Z. Xiong, \u201cLong-term tracking with deep tracklet association,\u201d IEEE TIP, vol. 29, pp. 6694\u20136706, 2020.": "Long-term tracking with deep tracklet association", "[71] F. Yang, X. Chang, S. Sakti, Y. Wu, and S. Nakamura, \u201cRemot: A model-agnostic refinement for multiple object tracking,\u201d IVC, vol. 106, p. 104091, 2021.": "Remot: A model-agnostic refinement for multiple object tracking", "[102] J. Chen, H. Sheng, Y. Zhang, and Z. Xiong, \u201cEnhancing detection model for multiple hypothesis tracking,\u201d in CVPR Workshops, 2017, pp. 18\u201327.": "Enhancing detection model for multiple hypothesis tracking", "[98] Y. Ye, X. Ke, and Z. Yu, \u201cA cost matrix optimization method based on spatial constraints under hungarian algorithm,\u201d in ICRAI, 2020, pp. 134\u2013139.": "A cost matrix optimization method based on spatial constraints under Hungarian algorithm", "[78] J. Pang, L. Qiu, X. Li, H. Chen, Q. Li, T. Darrell, and F. Yu, \u201cQuasi-dense similarity learning for multiple object tracking,\u201d in CVPR, 2021, pp. 164\u2013173.": "Quasi-dense similarity learning for multiple object tracking", "[182] G. Wang, Y. Wang, R. Gu, W. Hu, and J.-N. Hwang, \u201cSplit and connect: A universal tracklet booster for multi-object tracking,\u201d IEEE TMM, 2022.": "Split and Connect: A Universal Tracklet Booster for Multi-Object Tracking", "[68] S. Karthik, A. Prabhu, and V. Gandhi, \u201cSimple unsupervised multi-object tracking,\u201d arXiv preprint arXiv:2006.02609, 2020.": "Simple unsupervised multi-object tracking", "[5] Y. Wang, K. Kitani, and X. Weng, \u201cJoint object detection and multi-object tracking with graph neural networks,\u201d in ICRA, 2021, pp. 13\u2009708\u201313\u2009715.": "Joint object detection and multi-object tracking with graph neural networks", "[223] L. Wen, P. Zhu, D. Du, X. Bian, H. Ling, Q. Hu, J. Zheng, T. Peng, X. Wang, Y. Zhang et al., \u201cVisdrone-mot2019: The vision meets drone multiple object tracking challenge results,\u201d in ICCV Workshops, 2019, pp. 0\u20130.": "Visdrone-mot2019: The vision meets drone multiple object tracking challenge results", "[103] L. Chen, H. Ai, Z. Zhuang, and C. Shang, \u201cReal-time multiple people tracking with deeply learned candidate selection and person re-identification,\u201d in ICME, 2018, pp. 1\u20136.": "Real-time multiple people tracking with deeply learned candidate selection and person re-identification", "[217] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal dataset for autonomous driving,\u201d in CVPR, 2020, pp. 11\u2009621\u201311\u2009631.": "nuscenes: A multimodal dataset for autonomous driving", "[199] J. Zhu, H. Yang, N. Liu, M. Kim, W. Zhang, and M.-H. Yang, \u201cOnline multi-object tracking with dual matching attention networks,\u201d in ECCV, 2018, pp. 366\u2013382.": "Online multi-object tracking with dual matching attention networks", "[70] N. L. Baisa, \u201cOcclusion-robust online multi-object visual tracking using a gm-phd filter with cnn-based re-identification,\u201d JVCIR, vol. 80, p. 103279, 2021.": "Occlusion-robust online multi-object visual tracking using a gm-phd filter with cnn-based re-identification", "[40] G. Wang, R. Gu, Z. Liu, W. Hu, M. Song, and J.-N. Hwang, \u201cTrack without appearance: Learn box and tracklet embedding with local and global motion patterns for vehicle tracking,\u201d in ICCV, 2021, pp. 9876\u20139886.": "Track without appearance: Learn box and tracklet embedding with local and global motion patterns for vehicle tracking", "[128] E. Baser, V. Balasubramanian, P. Bhattacharyya, and K. Czarnecki, \u201cFantrack: 3d multi-object tracking with feature association network,\u201d in IV, 2019, pp. 1426\u20131433.": "Fantrack: 3D Multi-Object Tracking with Feature Association Network", "[33] J. Yang, H. Ge, J. Yang, Y. Tong, and S. Su, \u201cOnline multi-object tracking using multi-function integration and tracking simulation training,\u201d AI, pp. 1\u201321, 2021.": "Online multi-object tracking using multi-function integration and tracking simulation training", "[213] L. Leal-Taix\u00e9, A. Milan, I. Reid, S. Roth, and K. Schindler, \u201cMotchallenge 2015: Towards a benchmark for multi-target tracking,\u201d arXiv preprint arXiv:1504.01942, 2015.": "Motchallenge 2015: Towards a benchmark for multi-target tracking", "[6] G. Bras\u00f3 and L. Leal-Taix\u00e9, \u201cLearning a neural solver for multiple object tracking,\u201d in CVPR, 2020, pp. 6247\u20136257.": "Learning a neural solver for multiple object tracking", "[12] X. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl, \u201cTracking objects as points,\u201d in ECCV, 2020, pp. 474\u2013490.": "Tracking objects as points", "[100] Y.-c. Yoon, A. Boragule, Y.-m. Song, K. Yoon, and M. Jeon, \u201cOnline multi-object tracking with historical appearance matching and scene adaptive detection filtering,\u201d in AVSS, 2018, pp. 1\u20136.": "Online multi-object tracking with historical appearance matching and scene adaptive detection filtering", "[183] J. Peng, T. Wang, W. Lin, J. Wang, J. See, S. Wen, and E. Ding, \u201cTpm: Multiple object tracking with tracklet-plane matching,\u201d Pattern Recognition, vol. 107, p. 107480, 2020.": "Tpm: Multiple object tracking with tracklet-plane matching", "[198] F. Bastani, S. He, and S. Madden, \u201cSelf-supervised multi-object tracking with cross-input consistency,\u201d NeurIPS, vol. 34, 2021.": "Self-supervised multi-object tracking with cross-input consistency", "[195] Y.-C. Yoon, D. Y. Kim, Y.-m. Song, K. Yoon, and M. Jeon, \u201cOnline multiple pedestrians tracking using deep temporal appearance matching association,\u201d Information Sciences, vol. 561, pp. 326\u2013351, 2021.": "Online multiple pedestrians tracking using deep temporal appearance matching association", "[125] Q. Liu, B. Liu, Y. Wu, W. Li, and N. Yu, \u201cReal-time online multi-object tracking in compressed domain,\u201d IEEE Access, vol. 7, pp. 76\u2009489\u201376\u2009499, 2019.": "Real-time online multi-object tracking in compressed domain", "[14] B. Pang, Y. Li, Y. Zhang, M. Li, and C. Lu, \u201cTubetk: Adopting tubes to track multi-object in a one-step training model,\u201d in CVPR, 2020, pp. 6308\u20136318.": "Tubetk: Adopting tubes to track multi-object in a one-step training model", "[241] H. Wu, Q. Li, C. Wen, X. Li, X. Fan, and C. Wang, \u201cTracklet proposal network for multi-object tracking on point clouds,\u201d in IJCAI, 2021, pp. 1165\u20131171.": "Tracklet proposal network for multi-object tracking on point clouds", "[20] Z. Tang, M. Naphade, M.-Y. Liu, X. Yang, S. Birchfield, S. Wang, R. Kumar, D. Anastasiu, and J.-N. Hwang, \u201cCityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification,\u201d in CVPR, 2019, pp. 8797\u20138806.": "Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification", "[109] N. L. Baisa, \u201cRobust online multi-target visual tracking using a hisp filter with discriminative deep appearance learning,\u201d JVCIR, vol. 77, p. 102952, 2021.": "Robust online multi-target visual tracking using a hisp filter with discriminative deep appearance learning", "[170] P. Sun, Y. Jiang, R. Zhang, E. Xie, J. Cao, X. Hu, T. Kong, Z. Yuan, C. Wang, and P. Luo, \u201cTranstrack: Multiple-object tracking with transformer,\u201d arXiv preprint arXiv:2012.15460, 2020.": "Transtrack: Multiple-object tracking with transformer", "[176] S. Wang, H. Sheng, Y. Zhang, Y. Wu, and Z. Xiong, \u201cA general recurrent tracking framework without real data,\u201d in ICCV, 2021, pp. 13\u2009219\u201313\u2009228.": "A general recurrent tracking framework without real data", "[191] M. Babaee, A. Athar, and G. Rigoll, \u201cMultiple people tracking using hierarchical deep tracklet re-identification,\u201d arXiv preprint arXiv:1811.04091, 2018.": "Multiple people tracking using hierarchical deep tracklet re-identification"}, "source_title_to_arxiv_id": {"Multiple object tracking with mixture density networks for trajectory estimation": "2106.10950", "Semi-tcl: Semi-supervised Track Contrastive Representation Learning": "2107.02396", "Patchtrack: Multiple Object Tracking Using Frame Patches": "2201.00080", "Joint multi-object detection and tracking with camera-lidar fusion for autonomous driving": "2108.04602", "Split and Connect: A Universal Tracklet Booster for Multi-Object Tracking": "2105.02426", "Track without appearance: Learn box and tracklet embedding with local and global motion patterns for vehicle tracking": "2108.06029"}}