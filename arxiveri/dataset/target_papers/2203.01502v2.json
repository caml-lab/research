{"title": "NeW CRFs: Neural Window Fully-connected CRFs for Monocular Depth Estimation", "abstract": "Estimating the accurate depth from a single image is challenging since it is\ninherently ambiguous and ill-posed. While recent works design increasingly\ncomplicated and powerful networks to directly regress the depth map, we take\nthe path of CRFs optimization. Due to the expensive computation, CRFs are\nusually performed between neighborhoods rather than the whole graph. To\nleverage the potential of fully-connected CRFs, we split the input into windows\nand perform the FC-CRFs optimization within each window, which reduces the\ncomputation complexity and makes FC-CRFs feasible. To better capture the\nrelationships between nodes in the graph, we exploit the multi-head attention\nmechanism to compute a multi-head potential function, which is fed to the\nnetworks to output an optimized depth map. Then we build a bottom-up-top-down\nstructure, where this neural window FC-CRFs module serves as the decoder, and a\nvision transformer serves as the encoder. The experiments demonstrate that our\nmethod significantly improves the performance across all metrics on both the\nKITTI and NYUv2 datasets, compared to previous methods. Furthermore, the\nproposed method can be directly applied to panorama images and outperforms all\nprevious panorama methods on the MatterPort3D dataset. Project page:\nhttps://weihaosky.github.io/newcrfs.", "authors": ["Weihao Yuan", "Xiaodong Gu", "Zuozhuo Dai", "Siyu Zhu", "Ping Tan"], "published_date": "2022_03_03", "pdf_url": "http://arxiv.org/pdf/2203.01502v2", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>cap</th><th>Abs Rel \\downarrow</th><th>Sq Rel \\downarrow</th><th>RMSE \\downarrow</th><th>RMSE{}_{log} \\downarrow</th><th>\\delta&lt;1.25 \\uparrow</th><th>\\delta&lt;1.25^{2} \\uparrow</th><th>\\delta&lt;1.25^{3} \\uparrow</th></tr></thead><tbody><tr><th>Eigen et al. [6]</th><th>   0-80m</th><td>0.190</td><td>1.515</td><td>7.156</td><td>0.270</td><td>0.692</td><td>0.899</td><td>0.967</td></tr><tr><th>Liu et al. [20]</th><th>0-80m</th><td>0.217</td><td>-</td><td>7.046</td><td>-</td><td>0.656</td><td>0.881</td><td>0.958</td></tr><tr><th>Xu et al. [38]</th><th>0-80m</th><td>0.122</td><td>0.897</td><td>4.677</td><td>-</td><td>0.818</td><td>0.954</td><td>0.985</td></tr><tr><th>DORN [7]</th><th>0-80m</th><td>0.072</td><td>0.307</td><td>2.727</td><td>0.120</td><td>0.932</td><td>0.984</td><td>0.995</td></tr><tr><th>Yin et al. [39]</th><th>0-80m</th><td>0.072</td><td>-</td><td>3.258</td><td>0.117</td><td>0.938</td><td>0.990</td><td>0.998</td></tr><tr><th>BTS [17]</th><th>0-80m</th><td>0.059</td><td>0.241</td><td>2.756</td><td>0.096</td><td>0.956</td><td>0.993</td><td>0.998</td></tr><tr><th>PackNet-SAN [10]</th><th>0-80m</th><td>0.062</td><td>-</td><td>2.888</td><td>-</td><td>0.955</td><td>-</td><td>-</td></tr><tr><th>Adabin [2]</th><th>0-80m</th><td>0.058</td><td>0.190</td><td>2.360</td><td>0.088</td><td>0.964</td><td>0.995</td><td>\\mathbf{0.999}</td></tr><tr><th>DPT* [28]</th><th>0-80m</th><td>0.062</td><td>-</td><td>2.573</td><td>0.092</td><td>0.959</td><td>0.995</td><td>\\mathbf{0.999}</td></tr><tr><th>PWA [18]</th><th>0-80m</th><td>0.060</td><td>0.221</td><td>2.604</td><td>0.093</td><td>0.958</td><td>0.994</td><td>\\mathbf{0.999}</td></tr><tr><th>Ours</th><th>0-80m</th><td>\\mathbf{0.052}</td><td>\\mathbf{0.155}</td><td>\\mathbf{2.129}</td><td>\\mathbf{0.079}</td><td>\\mathbf{0.974}</td><td>\\mathbf{0.997}</td><td>\\mathbf{0.999}</td></tr></tbody></table>", "caption": "Table 1: Quantitative results on the Eigen split of KITTI dataset. Seven widely used metrics are reported. \u201cAbs Rel\u201d error is the main ranking metric. Note that the \u201cSq Rel\u201d error is calculated in a different way here. \u201c*\u201d means using additional data for training.", "list_citation_info": ["[20] Fayao Liu, Chunhua Shen, and Guosheng Lin. Deep convolutional neural fields for depth estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5162\u20135170, 2015.", "[17] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[7] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[39] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5684\u20135693, 2019.", "[2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4009\u20134018, 2021.", "[28] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the International Conference on Computer Vision, pages 12179\u201312188, 2021.", "[38] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured attention guided convolutional neural fields for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3917\u20133925, 2018.", "[18] Sihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and Junmo Kim. Patch-wise attention network for monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1873\u20131881, 2021.", "[10] Vitor Guizilini, Rares Ambrus, Wolfram Burgard, and Adrien Gaidon. Sparse auxiliary networks for unified monocular depth prediction and completion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11078\u201311088, 2021.", "[6] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in Neural Information Processing Systems, pages 2366\u20132374, 2014."]}, {"table": "<table><tbody><tr><th>Method</th><th>dataset</th><td>SILog \\downarrow</td><td>Abs Rel \\downarrow</td><td>Sq Rel \\downarrow</td><td>iRMSE \\downarrow</td><td>RMSE \\downarrow</td><td>\\delta&lt;1.25 \\uparrow</td><td>\\delta&lt;1.25^{2} \\uparrow</td><td>\\delta&lt;1.25^{3} \\uparrow</td></tr><tr><th>DORN [7]</th><th>val</th><td>12.22</td><td>11.78</td><td>3.03</td><td>11.68</td><td>3.80</td><td>0.913</td><td>0.985</td><td>0.995</td></tr><tr><th>BTS [17]</th><th>val</th><td>10.67</td><td>7.51</td><td>1.59</td><td>8.10</td><td>3.37</td><td>0.938</td><td>0.987</td><td>0.996</td></tr><tr><th>BA-Full [1]</th><th>val</th><td>10.64</td><td>8.25</td><td>1.81</td><td>8.47</td><td>3.30</td><td>0.938</td><td>0.988</td><td>0.997</td></tr><tr><th>Ours</th><th>val</th><td>\\mathbf{8.31}</td><td>\\mathbf{5.54}</td><td>\\mathbf{0.89}</td><td>\\mathbf{6.34}</td><td>\\mathbf{2.55}</td><td>\\mathbf{0.968}</td><td>\\mathbf{0.995}</td><td>\\mathbf{0.998}</td></tr><tr><th>DORN [7]</th><th>online test</th><td>11.77</td><td>8.78</td><td>2.23</td><td>12.98</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BTS [17]</th><th>online test</th><td>11.67</td><td>9.04</td><td>2.21</td><td>12.23</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BA-Full [1]</th><th>online test</th><td>11.61</td><td>9.38</td><td>2.29</td><td>12.23</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PackNet-SAN [10]</th><th>online test</th><td>11.54</td><td>9.12</td><td>2.35</td><td>12.38</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PWA [18]</th><th>online test</th><td>11.45</td><td>9.05</td><td>2.30</td><td>12.32</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Ours</th><th>  online test</th><td>\\mathbf{10.39}</td><td>\\mathbf{8.37}</td><td>\\mathbf{1.83}</td><td>\\mathbf{11.03}</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Quantitative results on the official split of KITTI dataset. Eight widely used metrics are reported for the validation set while only four metrics are available from the online evaluation server for the test set. \u201cSILog\u201d error is the main ranking metric. Our method ranks 1st among all submissions on the KITTI depth prediction online benchmark at the submission time of this paper.", "list_citation_info": ["[17] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[7] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[18] Sihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and Junmo Kim. Patch-wise attention network for monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1873\u20131881, 2021.", "[10] Vitor Guizilini, Rares Ambrus, Wolfram Burgard, and Adrien Gaidon. Sparse auxiliary networks for unified monocular depth prediction and completion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11078\u201311088, 2021.", "[1] Shubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Islam, Mannat Kaur, and Bingbing Liu. Bidirectional attention network for monocular depth estimation. In Proceedings of the IEEE International Conference on Robotics and Automation, pages 11746\u201311752, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Abs Rel \\downarrow</th><th>Sq Rel \\downarrow</th><th>RMSE \\downarrow</th><th>RMSE{}_{log} \\downarrow</th><th>log10 \\downarrow</th><th>\\delta&lt;1.25 \\uparrow</th><th>\\delta&lt;1.25^{2} \\uparrow</th><th>\\delta&lt;1.25^{3} \\uparrow</th></tr></thead><tbody><tr><th>Liu et al. [20]</th><td>0.230</td><td>-</td><td>0.824</td><td>-</td><td>0.095</td><td>0.614</td><td>0.883</td><td>0.971</td></tr><tr><th>Xu et al. [38]</th><td>0.125</td><td>-</td><td>0.593</td><td>-</td><td>0.057</td><td>0.806</td><td>0.952</td><td>0.986</td></tr><tr><th>DORN [7]</th><td>0.115</td><td>-</td><td>0.509</td><td>-</td><td>0.051</td><td>0.828</td><td>0.965</td><td>0.992</td></tr><tr><th>Yin et al. [39]</th><td>0.108</td><td>-</td><td>0.416</td><td>-</td><td>0.048</td><td>0.875</td><td>0.976</td><td>0.994</td></tr><tr><th>BTS [17]</th><td>0.110</td><td>0.066</td><td>0.392</td><td>0.142</td><td>0.047</td><td>0.885</td><td>0.978</td><td>0.994</td></tr><tr><th>DAV [12]</th><td>0.108</td><td>-</td><td>0.412</td><td>-</td><td>-</td><td>0.882</td><td>0.980</td><td>0.996</td></tr><tr><th>PackNet-SAN* [10]</th><td>0.106</td><td>-</td><td>0.393</td><td>-</td><td>-</td><td>0.892</td><td>0.979</td><td>0.995</td></tr><tr><th>Adabin [2]</th><td>0.103</td><td>-</td><td>0.364</td><td>-</td><td>0.044</td><td>0.903</td><td>0.984</td><td>0.997</td></tr><tr><th>DPT* [28]</th><td>0.110</td><td>-</td><td>0.357</td><td>-</td><td>0.045</td><td>0.904</td><td>0.988</td><td>\\mathbf{0.998}</td></tr><tr><th>PWA [18]</th><td>0.105</td><td>-</td><td>0.374</td><td>-</td><td>0.045</td><td>0.892</td><td>0.985</td><td>0.997</td></tr><tr><th>Ours</th><td>\\mathbf{0.095}</td><td>\\mathbf{0.045}</td><td>\\mathbf{0.334}</td><td>\\mathbf{0.119}</td><td>\\mathbf{0.041}</td><td>\\mathbf{0.922}</td><td>\\mathbf{0.992}</td><td>\\mathbf{0.998}</td></tr></tbody></table>", "caption": "Table 3: Quantitative results on NYUv2. \u201cAbs Rel\u201d and \u201cRMSE\u201d are the main ranking metrics. \u201c*\u201d means using additional data.", "list_citation_info": ["[17] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[20] Fayao Liu, Chunhua Shen, and Guosheng Lin. Deep convolutional neural fields for depth estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5162\u20135170, 2015.", "[7] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[39] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5684\u20135693, 2019.", "[2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4009\u20134018, 2021.", "[28] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the International Conference on Computer Vision, pages 12179\u201312188, 2021.", "[38] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured attention guided convolutional neural fields for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3917\u20133925, 2018.", "[12] Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, and Janne Heikkil\u00e4. Guiding monocular depth estimation using depth-attention volume. In Proceedings of the European Conference on Computer Vision, pages 581\u2013597. Springer, 2020.", "[18] Sihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and Junmo Kim. Patch-wise attention network for monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1873\u20131881, 2021.", "[10] Vitor Guizilini, Rares Ambrus, Wolfram Burgard, and Adrien Gaidon. Sparse auxiliary networks for unified monocular depth prediction and completion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11078\u201311088, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th>Abs Rel \\downarrow</th><th>Abs \\downarrow</th><th>RMSE \\downarrow</th><th>RMSE{}_{log} \\downarrow</th><th>\\delta&lt;1.25 \\uparrow</th><th>\\delta&lt;1.25^{2} \\uparrow</th><th>\\delta&lt;1.25^{3} \\uparrow</th></tr></thead><tbody><tr><th>OmniDepth [46]</th><td>0.2901</td><td>0.4838</td><td>0.7643</td><td>0.1450</td><td>0.6830</td><td>0.8794</td><td>0.9429</td></tr><tr><th>BiFuse [36]</th><td>0.2048</td><td>0.3470</td><td>0.6259</td><td>0.1134</td><td>0.8452</td><td>0.9319</td><td>0.9632</td></tr><tr><th>SliceNet [25]</th><td>0.1764</td><td>0.3296</td><td>0.6133</td><td>0.1045</td><td>0.8716</td><td>0.9483</td><td>0.9716</td></tr><tr><th>HoHoNet [33]</th><td>0.1488</td><td>0.2862</td><td>0.5138</td><td>0.0871</td><td>0.8786</td><td>0.9519</td><td>0.9771</td></tr><tr><th>UniFuse [14]</th><td>0.1063</td><td>0.2814</td><td>0.4941</td><td>0.0701</td><td>0.8897</td><td>0.9623</td><td>0.9831</td></tr><tr><th>Ours</th><td>\\mathbf{0.0906}</td><td>\\mathbf{0.2252}</td><td>\\mathbf{0.4778}</td><td>\\mathbf{0.0638}</td><td>\\mathbf{0.9197}</td><td>\\mathbf{0.9761}</td><td>\\mathbf{0.9909}</td></tr><tr><th>Ours*</th><td>\\mathbf{0.0793}</td><td>\\mathbf{0.1970}</td><td>\\mathbf{0.4279}</td><td>\\mathbf{0.0575}</td><td>\\mathbf{0.9376}</td><td>\\mathbf{0.9812}</td><td>\\mathbf{0.9933}</td></tr></tbody></table>", "caption": "Table 4: Quantitative results on the Matterport3D dataset. \u201c*\u201d means using additional data for training.", "list_citation_info": ["[46] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras. Omnidepth: Dense depth estimation for indoors spherical panoramas. In Proceedings of the European Conference on Computer Vision, pages 448\u2013465, 2018.", "[33] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet: 360 indoor holistic understanding with latent horizontal features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2573\u20132582, 2021.", "[14] Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui Huang. Unifuse: Unidirectional fusion for 360 panorama depth estimation. IEEE Robotics and Automation Letters, 6(2):1519\u20131526, 2021.", "[25] Giovanni Pintore, Marco Agus, Eva Almansa, Jens Schneider, and Enrico Gobbetti. Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11536\u201311545, 2021.", "[36] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via bi-projection fusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 462\u2013471, 2020."]}], "citation_info_to_title": {"[20] Fayao Liu, Chunhua Shen, and Guosheng Lin. Deep convolutional neural fields for depth estimation from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5162\u20135170, 2015.": "Deep Convolutional Neural Fields for Depth Estimation from a Single Image", "[39] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5684\u20135693, 2019.": "Enforcing geometric constraints of virtual normal for depth prediction", "[6] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in Neural Information Processing Systems, pages 2366\u20132374, 2014.": "Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network", "[25] Giovanni Pintore, Marco Agus, Eva Almansa, Jens Schneider, and Enrico Gobbetti. Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11536\u201311545, 2021.": "Slicenet: Deep Dense Depth Estimation from a Single Indoor Panorama using a Slice-based Representation", "[10] Vitor Guizilini, Rares Ambrus, Wolfram Burgard, and Adrien Gaidon. Sparse auxiliary networks for unified monocular depth prediction and completion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11078\u201311088, 2021.": "Sparse auxiliary networks for unified monocular depth prediction and completion", "[12] Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, and Janne Heikkil\u00e4. Guiding monocular depth estimation using depth-attention volume. In Proceedings of the European Conference on Computer Vision, pages 581\u2013597. Springer, 2020.": "Guiding Monocular Depth Estimation Using Depth-Attention Volume", "[28] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the International Conference on Computer Vision, pages 12179\u201312188, 2021.": "Vision Transformers for Dense Prediction", "[7] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.": "Deep ordinal regression network for monocular depth estimation", "[1] Shubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Islam, Mannat Kaur, and Bingbing Liu. Bidirectional attention network for monocular depth estimation. In Proceedings of the IEEE International Conference on Robotics and Automation, pages 11746\u201311752, 2021.": "Bidirectional attention network for monocular depth estimation", "[38] Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured attention guided convolutional neural fields for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3917\u20133925, 2018.": "Structured attention guided convolutional neural fields for monocular depth estimation", "[2] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4009\u20134018, 2021.": "Adabins: Depth estimation using adaptive bins", "[36] Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, and Yi-Hsuan Tsai. Bifuse: Monocular 360 depth estimation via bi-projection fusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 462\u2013471, 2020.": "Bifuse: Monocular 360 depth estimation via bi-projection fusion", "[18] Sihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and Junmo Kim. Patch-wise attention network for monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1873\u20131881, 2021.": "Patch-wise attention network for monocular depth estimation", "[46] Nikolaos Zioulis, Antonis Karakottas, Dimitrios Zarpalas, and Petros Daras. Omnidepth: Dense depth estimation for indoors spherical panoramas. In Proceedings of the European Conference on Computer Vision, pages 448\u2013465, 2018.": "Omnidepth: Dense depth estimation for indoors spherical panoramas", "[33] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Hohonet: 360 indoor holistic understanding with latent horizontal features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2573\u20132582, 2021.": "Hohonet: 360 indoor holistic understanding with latent horizontal features", "[17] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.": "From big to small: Multi-scale local planar guidance for monocular depth estimation", "[14] Hualie Jiang, Zhe Sheng, Siyu Zhu, Zilong Dong, and Rui Huang. Unifuse: Unidirectional fusion for 360 panorama depth estimation. IEEE Robotics and Automation Letters, 6(2):1519\u20131526, 2021.": "Unifuse: Unidirectional Fusion for 360 Panorama Depth Estimation"}, "source_title_to_arxiv_id": {"Sparse auxiliary networks for unified monocular depth prediction and completion": "2103.16690", "Vision Transformers for Dense Prediction": "2102.12122", "Adabins: Depth estimation using adaptive bins": "2011.14141"}}