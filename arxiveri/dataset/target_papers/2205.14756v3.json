{"title": "EfficientViT: Lightweight Multi-Scale Attention for On-Device Semantic Segmentation", "abstract": "Semantic segmentation enables many appealing real-world applications, such as\ncomputational photography, autonomous driving, etc. However, the vast\ncomputational cost makes deploying state-of-the-art semantic segmentation\nmodels on edge devices with limited hardware resources difficult. This work\npresents EfficientViT, a new family of semantic segmentation models with a\nnovel lightweight multi-scale attention for on-device semantic segmentation.\nUnlike prior semantic segmentation models that rely on heavy self-attention,\nhardware-inefficient large-kernel convolution, or complicated topology\nstructure to obtain good performances, our lightweight multi-scale attention\nachieves a global receptive field and multi-scale learning (two critical\nfeatures for semantic segmentation models) with only lightweight and\nhardware-efficient operations. As such, EfficientViT delivers remarkable\nperformance gains over previous state-of-the-art semantic segmentation models\nacross popular benchmark datasets with significant speedup on the mobile\nplatform. Without performance loss on Cityscapes, our EfficientViT provides up\nto 15x and 9.3x mobile latency reduction over SegFormer and SegNeXt,\nrespectively. Maintaining the same mobile latency, EfficientViT provides +7.4\nmIoU gain on ADE20K over SegNeXt. Code:\nhttps://github.com/mit-han-lab/efficientvit.", "authors": ["Han Cai", "Junyan Li", "Muyan Hu", "Chuang Gan", "Song Han"], "published_date": "2022_05_29", "pdf_url": "http://arxiv.org/pdf/2205.14756v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td colspan=\"2\">Models</td><td>Params</td><td>MACs</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><td rowspan=\"10\">CNN-based</td><td>MobileDet-DSP [62]</td><td>9.2M</td><td>3.2G</td><td>29.1</td><td>-</td><td>-</td></tr><tr><td>RetinaNet+ResNet50 [63]</td><td>34M</td><td>97G</td><td>39.2</td><td>-</td><td>-</td></tr><tr><td>EfficientDet-D0 [38]</td><td>3.9M</td><td>2.5G</td><td>34.3</td><td>-</td><td>-</td></tr><tr><td>EfficientDet-D1 [38]</td><td>6.6M</td><td>6.1G</td><td>40.2</td><td>-</td><td>-</td></tr><tr><td>YOLOv4-Tiny [64]</td><td>6.1M</td><td>3.5G</td><td>21.7</td><td>40.2</td><td>-</td></tr><tr><td>YOLOX-Tiny [58]</td><td>5.1M</td><td>3.2G</td><td>32.8</td><td>-</td><td>-</td></tr><tr><td>YOLOv5s</td><td>7.2M</td><td>8.3G</td><td>37.2</td><td>56.0</td><td>-</td></tr><tr><td>YOLOX-s [58]</td><td>9.0M</td><td>13.4G</td><td>40.5</td><td>-</td><td>-</td></tr><tr><td>PP-PicoDet-L{}^{\\dagger} [61]</td><td>3.3M</td><td>4.5G</td><td>40.9</td><td>57.6</td><td>-</td></tr><tr><td>RetinaNet+PVT-Tiny [5]</td><td>23.0M</td><td>221G</td><td>36.7</td><td>56.9</td><td>38.9</td></tr><tr><td>ViT-based</td><td>RetinaNet+ConT-M [65]</td><td>27.0M</td><td>217G</td><td>37.9</td><td>58.1</td><td>40.2</td></tr><tr><td></td><td>RetinaNet+MobileFormer [40]</td><td>17.9M</td><td>168G</td><td>38.0</td><td>58.3</td><td>40.3</td></tr><tr><td></td><td>EfficientViT-Det-r416 (ours)</td><td>10.6M</td><td>2.1G</td><td>38.1</td><td>55.6</td><td>40.3</td></tr><tr><td>ViT-based</td><td>EfficientViT-Det-r512 (ours)</td><td>10.6M</td><td>3.2G</td><td>40.7</td><td>58.6</td><td>43.6</td></tr><tr><td></td><td>EfficientViT-Det-r608 (ours)</td><td>10.6M</td><td>4.4G</td><td>42.6</td><td>60.5</td><td>45.3</td></tr></tbody></table>", "caption": "Table 1: EfficientViT outperforms state-of-the-art one-stage detectors on COCO (val2017). \u2018r608\u2019 denotes the input resolution is 608x608. {}^{\\dagger} denotes the best result we find for CNN-based mobile object detection, which is achieved with a bunch of additional techniques (e.g., neural architecture search, ghost module, CSP, Cycle-EMA, etc.). Compared with this strong baseline (PP-PicoDet-L [61]), EfficientViT provides 1.7 higher AP with slightly lower MACs. ", "list_citation_info": ["[63] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.", "[64] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.", "[38] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10781\u201310790, 2020.", "[40] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2022.", "[58] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.", "[5] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 568\u2013578, 2021.", "[61] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng Cui, Wei Ji, Qingqing Dang, Kaipeng Deng, Guanzhong Wang, Yuning Du, et al. Pp-picodet: A better real-time object detector on mobile devices. arXiv preprint arXiv:2111.00902, 2021.", "[65] Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming Wu, and Chuang Zhang. Contnet: Why not use convolution and transformer at the same time? arXiv preprint arXiv:2104.13497, 2021.", "[62] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin, Gabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans, Mingxing Tan, Vikas Singh, and Bo Chen. Mobiledets: Searching for object detection architectures for mobile accelerators. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3825\u20133834, 2021."]}, {"table": "<table><tbody><tr><th colspan=\"2\">Models</th><th>Backbone</th><td>Params</td><td>MACs</td><td>mIoU</td></tr><tr><th rowspan=\"4\">CNN-based</th><th>FCN [72]</th><th>MobileNetV2</th><td>9.8M</td><td>158.6G</td><td>61.5</td></tr><tr><th>Fast-SCNN [21]</th><th>-</th><td>1.1M</td><td>6.9G</td><td>68.0</td></tr><tr><th>PSPNet [73]</th><th>MobileNetV2</th><td>13.7M</td><td>211.7G</td><td>70.2</td></tr><tr><th>DeepLabV3+ [74]</th><th>MobileNetV2</th><td>15.4M</td><td>277.7G</td><td>75.2</td></tr><tr><th rowspan=\"2\">ViT-based</th><th>SegFormer [9]</th><th>MiT-B0</th><td>3.8M</td><td>62.8G</td><td>76.2</td></tr><tr><th>NASViT [42]</th><th>-</th><td>-</td><td>-</td><td>76.1</td></tr><tr><th></th><th>EfficientViT-Seg-r1408 (ours)</th><th>EfficientViT-Base</th><td>6.5M</td><td>9.1G</td><td>76.1</td></tr><tr><th>ViT-based</th><th>EfficientViT-Seg-r1536 (ours)</th><th>EfficientViT-Base</th><td>6.5M</td><td>10.8G</td><td>77.0</td></tr><tr><th></th><th>EfficientViT-Seg-r2048 (ours)</th><th>EfficientViT-Base</th><td>6.5M</td><td>19.1G</td><td>78.7</td></tr></tbody></table>", "caption": "Table 2: Results on Cityscapes semantic segmentation. \u2018r2048\u2019 denotes the input resolution is 1024x2048. Unlike SegFormer [9] that runs inference on 1024x1024 sliding windows, we directly run inference on high-resolution images (1024x2048), thanks to the high efficiency. This brings significant performance improvements. With 19.1G MACs, EfficientViT provides 78.7 mIoU, surpassing SegFormer by 2.5 mIoU while requiring 3.3\\times fewer MACs. ", "list_citation_info": ["[72] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015.", "[9] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34, 2021.", "[21] Rudra PK Poudel, Stephan Liwicki, and Roberto Cipolla. Fast-scnn: Fast semantic segmentation network. arXiv preprint arXiv:1902.04502, 2019.", "[74] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.", "[73] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881\u20132890, 2017.", "[42] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, qiang liu, and Vikas Chandra. NASVit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training. In International Conference on Learning Representations, 2022."]}, {"table": "<table><tbody><tr><th colspan=\"2\" rowspan=\"2\">Models</th><td rowspan=\"2\">Params</td><td rowspan=\"2\">MACs</td><td colspan=\"2\">Accuracy</td></tr><tr><td>Top1 (%)</td><td>Top5 (%)</td></tr><tr><th rowspan=\"9\">CNN-based</th><th>MobileNetV2 [22]</th><td>3.4M</td><td>300M</td><td>72.0</td><td>-</td></tr><tr><th>ShuffleNetV2 1.5x [75]</th><td>-</td><td>299M</td><td>72.6</td><td>-</td></tr><tr><th>FBNet-B [76]</th><td>4.5M</td><td>295M</td><td>74.1</td><td>-</td></tr><tr><th>ProxylessNAS-Mobile [77]</th><td>4.1M</td><td>320M</td><td>74.6</td><td>92.2</td></tr><tr><th>MnasNet-A1 [78]</th><td>3.9M</td><td>312M</td><td>75.2</td><td>92.5</td></tr><tr><th>MobileNetV3-Large 1.25x [48]</th><td>7.5M</td><td>356M</td><td>76.6</td><td>-</td></tr><tr><th>EfficientNet-B0 [39]</th><td>5.3M</td><td>390M</td><td>77.1</td><td>93.3</td></tr><tr><th>EfficientNetV2-B0 [79]</th><td>7.4M</td><td>700M</td><td>78.7</td><td>-</td></tr><tr><th>EfficientNet-B1 [39]</th><td>7.8M</td><td>700M</td><td>79.1</td><td>94.4</td></tr><tr><th rowspan=\"13\">ViT-based</th><th>T2T-ViT-7 [80]</th><td>4.3M</td><td>1.2G</td><td>71.7</td><td>-</td></tr><tr><th>QuadTree-B-b0 [34]</th><td>3.5M</td><td>0.7G</td><td>72.0</td><td>-</td></tr><tr><th>ConViT-Tiny [81]</th><td>6.0M</td><td>1.0G</td><td>73.1</td><td>-</td></tr><tr><th>PVT-Tiny [5]</th><td>13.2M</td><td>1.9G</td><td>75.1</td><td>-</td></tr><tr><th>CeiT-T [82]</th><td>6.4M</td><td>1.2G</td><td>76.4</td><td>93.4</td></tr><tr><th>ViL-Tiny-RPB [46]</th><td>6.7M</td><td>1.3G</td><td>76.7</td><td></td></tr><tr><th>Swin-1G [4] {}^{{\\ddagger}}</th><td>7.3M</td><td>1.0G</td><td>77.3</td><td>-</td></tr><tr><th>HVT-S-1 [83]</th><td>22.1M</td><td>2.4G</td><td>78.0</td><td>93.8</td></tr><tr><th>PiT-XS [84]</th><td>10.6M</td><td>1.4G</td><td>78.1</td><td>-</td></tr><tr><th>CoaT Tiny [47]</th><td>5.5M</td><td>4.4G</td><td>78.3</td><td></td></tr><tr><th>HRFormer-T [10]</th><td>8.0M</td><td>1.8G</td><td>78.5</td><td></td></tr><tr><th>MobileViT-XS [41]</th><td>2.3M</td><td>700M</td><td>74.8</td><td>-</td></tr><tr><th>MobileFormer w/o DY-ReLU [40]</th><td>10.1M</td><td>290M</td><td>76.8</td><td>93.2</td></tr><tr><th rowspan=\"3\">ViT-based</th><th>EfficientViT-Base-r192 (ours)</th><td>7.9M</td><td>304M</td><td>77.7</td><td>93.6</td></tr><tr><th>EfficientViT-Base-r224 (ours)</th><td>7.9M</td><td>406M</td><td>78.6</td><td>94.2</td></tr><tr><th>EfficientViT-Base-r224-w1.2 (ours)</th><td>10.9M</td><td>584M</td><td>79.7</td><td>94.8</td></tr></tbody></table>", "caption": "Table 3: Results on ImageNet classification. \u2018r224\u2019 denotes the input resolution is 224x224. \u2018w1.2\u2019 denotes the width multiplier [22] is 1.2. {}^{{\\ddagger}} denotes the result is from [40]. While EfficientViT is not specifically designed for image classification, it still provides highly competitive performances on ImageNet. With 584M MACs, EfficientViT achieves 79.7% ImageNet top1 accuracy, outperforming EfficientNet-B1 by 0.6% while saving the computational cost by 1.2\\times. It demonstrates the strong capacity of EfficientViT in visual feature learning. ", "list_citation_info": ["[39] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019.", "[47] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9981\u20139990, 2021.", "[75] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, 2018.", "[83] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable vision transformers with hierarchical pooling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 377\u2013386, 2021.", "[5] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 568\u2013578, 2021.", "[81] St\u00e9phane d\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In International Conference on Machine Learning, pages 2286\u20132296. PMLR, 2021.", "[77] Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In ICLR, 2019.", "[34] Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan. Quadtree attention for vision transformers. In International Conference on Learning Representations, 2022.", "[82] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 579\u2013588, 2021.", "[4] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.", "[46] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998\u20133008, 2021.", "[80] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 558\u2013567, 2021.", "[10] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. Hrformer: High-resolution vision transformer for dense predict. Advances in Neural Information Processing Systems, 34, 2021.", "[84] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11936\u201311945, 2021.", "[40] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2022.", "[41] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. In International Conference on Learning Representations, 2022.", "[79] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International Conference on Machine Learning, pages 10096\u201310106. PMLR, 2021.", "[76] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR, 2019.", "[78] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.", "[48] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV, 2019.", "[22] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018."]}], "citation_info_to_title": {"[76] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR, 2019.": "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search", "[74] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "[4] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[77] Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task and hardware. In ICLR, 2019.": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "[61] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng Cui, Wei Ji, Qingqing Dang, Kaipeng Deng, Guanzhong Wang, Yuning Du, et al. Pp-picodet: A better real-time object detector on mobile devices. arXiv preprint arXiv:2111.00902, 2021.": "Pp-picodet: A better real-time object detector on mobile devices", "[63] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.": "Focal loss for dense object detection", "[10] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. Hrformer: High-resolution vision transformer for dense predict. Advances in Neural Information Processing Systems, 34, 2021.": "Hrformer: High-resolution vision transformer for dense predict", "[41] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer. In International Conference on Learning Representations, 2022.": "Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer", "[5] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 568\u2013578, 2021.": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "[39] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, 2019.": "Efficientnet: Rethinking Model Scaling for Convolutional Neural Networks", "[80] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 558\u2013567, 2021.": "Tokens-to-token vit: Training vision transformers from scratch on imagenet", "[48] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV, 2019.": "Searching for MobileNetV3", "[34] Shitao Tang, Jiahui Zhang, Siyu Zhu, and Ping Tan. Quadtree attention for vision transformers. In International Conference on Learning Representations, 2022.": "Quadtree Attention for Vision Transformers", "[84] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11936\u201311945, 2021.": "Rethinking spatial dimensions of vision transformers", "[46] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2998\u20133008, 2021.": "Multi-scale vision longformer: A new vision transformer for high-resolution image encoding", "[75] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, 2018.": "Shufflenet v2: Practical guidelines for efficient cnn architecture design", "[9] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34, 2021.": "Segformer: Simple and efficient design for semantic segmentation with transformers", "[21] Rudra PK Poudel, Stephan Liwicki, and Roberto Cipolla. Fast-scnn: Fast semantic segmentation network. arXiv preprint arXiv:1902.04502, 2019.": "Fast-SCNN: Fast Semantic Segmentation Network", "[58] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.": "Yolox: Exceeding YOLO Series in 2021", "[62] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin, Gabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans, Mingxing Tan, Vikas Singh, and Bo Chen. Mobiledets: Searching for object detection architectures for mobile accelerators. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3825\u20133834, 2021.": "Mobiledets: Searching for object detection architectures for mobile accelerators", "[40] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2022.": "Mobile-former: Bridging mobilenet and transformer", "[65] Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming Wu, and Chuang Zhang. Contnet: Why not use convolution and transformer at the same time? arXiv preprint arXiv:2104.13497, 2021.": "Why not use convolution and transformer at the same time?", "[79] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International Conference on Machine Learning, pages 10096\u201310106. PMLR, 2021.": "Efficientnetv2: Smaller models and faster training", "[81] St\u00e9phane d\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In International Conference on Machine Learning, pages 2286\u20132296. PMLR, 2021.": "Convit: Improving vision transformers with soft convolutional inductive biases", "[78] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.": "Mnasnet: Platform-aware neural architecture search for mobile", "[82] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 579\u2013588, 2021.": "Incorporating Convolution Designs into Visual Transformers", "[42] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, qiang liu, and Vikas Chandra. NASVit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training. In International Conference on Learning Representations, 2022.": "NASVit: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict Aware Supernet Training", "[22] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.": "Mobilenetv2: Inverted residuals and linear bottlenecks", "[72] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015.": "Fully Convolutional Networks for Semantic Segmentation", "[38] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10781\u201310790, 2020.": "Efficientdet: Scalable and efficient object detection", "[83] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable vision transformers with hierarchical pooling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 377\u2013386, 2021.": "Scalable vision transformers with hierarchical pooling", "[64] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020.": "Yolov4: Optimal speed and accuracy of object detection", "[73] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881\u20132890, 2017.": "Pyramid scene parsing network", "[47] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9981\u20139990, 2021.": "Co-scale conv-attentional image transformers"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware": "1812.00332", "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions": "2102.12122", "Quadtree Attention for Vision Transformers": "2201.02767", "Scalable vision transformers with hierarchical pooling": "2103.10619"}}