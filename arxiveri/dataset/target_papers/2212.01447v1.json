{"title": "Compound Tokens: Channel Fusion for Vision-Language Representation Learning", "abstract": "We present an effective method for fusing visual-and-language representations\nfor several question answering tasks including visual question answering and\nvisual entailment. In contrast to prior works that concatenate unimodal\nrepresentations or use only cross-attention, we compose multimodal\nrepresentations via channel fusion. By fusing on the channels, the model is\nable to more effectively align the tokens compared to standard methods. These\nmultimodal representations, which we call compound tokens are generated with\ncross-attention transformer layers. First, vision tokens are used as queries to\nretrieve compatible text tokens through cross-attention. We then chain the\nvision tokens and the queried text tokens along the channel dimension. We call\nthe resulting representations compound tokens. A second group of compound\ntokens are generated using an analogous process where the text tokens serve as\nqueries to the cross-attention layer. We concatenate all the compound tokens\nfor further processing with multimodal encoder. We demonstrate the\neffectiveness of compound tokens using an encoder-decoder vision-language model\ntrained end-to-end in the open-vocabulary setting. Compound Tokens achieve\nhighly competitive performance across a range of question answering tasks\nincluding GQA, VQA2.0, and SNLI-VE.", "authors": ["Maxwell Mbabilla Aladago", "AJ Piergiovanni"], "published_date": "2022_12_02", "pdf_url": "http://arxiv.org/pdf/2212.01447v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Fusion Method</th><th>L</th><th>Params (\\times 10^{6})</th><th>RES</th><th>GFlops</th><th>SNLI-VE</th><th>GQA</th></tr></thead><tbody><tr><td>Merged Attention</td><td>12</td><td>332.94</td><td>384\\times 384</td><td>34.89</td><td>79.81</td><td>78.07</td></tr><tr><td>Co-Attention</td><td>12</td><td>361.26</td><td>384\\times 384</td><td>29.61</td><td>80.20</td><td>77.75</td></tr><tr><td>Compound Tokens (Ours)</td><td>10</td><td>325.82</td><td>384\\times 384</td><td>32.90</td><td>80.52</td><td>78.21</td></tr><tr><td>Co-Tokenization</td><td>12</td><td>392.14</td><td>384\\times 384</td><td>57.78</td><td>80.79</td><td>81.07</td></tr><tr><td>Compound Tokens (Ours)</td><td>10</td><td>325.82</td><td>384\\times 384</td><td>32.90</td><td>80.52</td><td>78.21</td></tr></tbody></table>", "caption": "Table 3: Comparisons with other Fusion models without Vision-Language Pretraining: We extend the models to include a multimodal encoder with 12 self-attention layers in merged attention to match the typical setting in previous works. Compound Tokens outperform merged attention and Co-Attention with fewer parameters than both methods and fewer flops than merged attention. Co-Attention and merged attention are from  Dou et al. (2022) while Co-Tokenization is from Piergiovanni et al. (2022b). The results here are our implementations of the these methods. Params shows the number of parameters in the entire model (not just the fusion module); RES is the image resolution and L is the total number of transformer blocks in the multimodal encoder: Compound Tokens uses two cross-attention blocks before the multimodal encoder.", "list_citation_info": ["Dou et al. (2022) Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. An empirical study of training end-to-end vision-and-language transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. URL https://arxiv.org/abs/2111.02387.", "Piergiovanni et al. (2022b) AJ Piergiovanni, Kairo Morton, Weicheng Kuo, Michael S. Ryoo, and Anelia Angelova. Video question answering with iterative video-text co-tokenization. In ECCV, 2022b. URL https://arxiv.org/abs/2208.00934."]}, {"table": "<table><thead><tr><th>Approach</th><th>Params</th><th>GFlops{}^{*}</th><th>VQA</th><th>SNLI-VE</th><th>GQA</th></tr></thead><tbody><tr><td>SimVLM{}_{Huge} (Wang et al., 2022b)</td><td>1.5B</td><td>890</td><td>80.34</td><td>86.32</td><td>-</td></tr><tr><td>VisualBERT (Li et al., 2019)</td><td></td><td></td><td>66.70</td><td>75.69</td><td>-</td></tr><tr><td>UNITER (Chen et al., 2020)</td><td></td><td></td><td>73.82</td><td>79.39</td><td>-</td></tr><tr><td>LXMERT (Tan &amp; Bansal, 2019)</td><td></td><td></td><td>69.90</td><td>-</td><td>60.00</td></tr><tr><td>ALBEF (Li et al., 2021a)</td><td>418M</td><td>122</td><td>75.84</td><td>80.91</td><td>-</td></tr><tr><td>METER (Dou et al., 2022)</td><td>336M</td><td>130</td><td>77.68</td><td>80.61</td><td>-</td></tr><tr><td>BLIP (Li et al., 2022)</td><td>475M</td><td>122</td><td>77.54</td><td>-</td><td>-</td></tr><tr><td>12-in-1 (Lu et al., 2020)</td><td></td><td></td><td>71.30</td><td>-</td><td>60.50</td></tr><tr><td>VinVL (Zhang et al., 2021)</td><td></td><td></td><td>75.95</td><td>-</td><td>65.05</td></tr><tr><td>VL-T5 (Cho et al., 2021)</td><td></td><td></td><td>70.30</td><td>-</td><td>60.80</td></tr><tr><td>CFR (Nguyen et al., 2022)</td><td></td><td></td><td>69.80</td><td>-</td><td>73.60</td></tr><tr><td>Compound Tokens (Ours)</td><td>340M</td><td>36</td><td>70.62</td><td>82.87</td><td>82.43</td></tr></tbody></table>", "caption": "Table 5: Comparison with SOTA: Compound Tokens outperforms all other models on SNLI-VE and GQA in an open-vocabulary evaluation except SimVLM (Wang et al., 2022b) which used a privat dataset of 1.5B samples. For VQA, we present the results in the closed-vocabulary setting for fair comparisons with the other methods: our open-set evaluation is significantly worse than the closed-set evaluation model on this task. The best values among the models besides SimVLM are in bold. The second best values are underlined. *The flops are based on our calculations. Our model is extremely more efficient than the rest partly because we use a short text sequence length of 32 and a ResNet-50 backbone that produces 49 visual tokens. ", "list_citation_info": ["Lu et al. (2020) Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "Zhang et al. (2021) Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5579\u20135588, June 2021.", "Tan & Bansal (2019) Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 2019.", "Li et al. (2021a) Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 9694\u20139705. Curran Associates, Inc., 2021a. URL https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf.", "Nguyen et al. (2022) Binh X. Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D, and Anh Nguyen Tran. Coarse-to-fine reasoning for visual question answering. In Multimodal Learning and Applications (MULA) Workshop, CVPR, 2022.", "Li et al. (2022) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. URL https://arxiv.org/pdf/2201.12086.pdf.", "Dou et al. (2022) Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. An empirical study of training end-to-end vision-and-language transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. URL https://arxiv.org/abs/2111.02387.", "Chen et al. (2020) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020.", "Wang et al. (2022b) Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In International Conference on Learning Representations (ICLR), 2022b. URL https://arxiv.org/abs/2108.10904.", "Cho et al. (2021) Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1931\u20131942. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/cho21a.html.", "Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. In Arxiv, 2019."]}], "citation_info_to_title": {"Zhang et al. (2021) Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5579\u20135588, June 2021.": "Vinvl: Revisiting visual representations in vision-language models", "Wang et al. (2022b) Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In International Conference on Learning Representations (ICLR), 2022b. URL https://arxiv.org/abs/2108.10904.": "Simvlm: Simple visual language model pretraining with weak supervision", "Cho et al. (2021) Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1931\u20131942. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/cho21a.html.": "Unifying vision-and-language tasks via text generation", "Piergiovanni et al. (2022b) AJ Piergiovanni, Kairo Morton, Weicheng Kuo, Michael S. Ryoo, and Anelia Angelova. Video question answering with iterative video-text co-tokenization. In ECCV, 2022b. URL https://arxiv.org/abs/2208.00934.": "Video question answering with iterative video-text co-tokenization", "Tan & Bansal (2019) Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 2019.": "Lxmert: Learning cross-modality encoder representations from transformers", "Li et al. (2022) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022. URL https://arxiv.org/pdf/2201.12086.pdf.": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "Dou et al. (2022) Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. An empirical study of training end-to-end vision-and-language transformers. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. URL https://arxiv.org/abs/2111.02387.": "An empirical study of training end-to-end vision-and-language transformers", "Chen et al. (2020) Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020.": "Uniter: Universal image-text representation learning", "Li et al. (2021a) Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 9694\u20139705. Curran Associates, Inc., 2021a. URL https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf.": "Align before fuse: Vision and language representation learning with momentum distillation", "Li et al. (2019) Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. In Arxiv, 2019.": "Visualbert: A simple and performant baseline for vision and language", "Nguyen et al. (2022) Binh X. Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D, and Anh Nguyen Tran. Coarse-to-fine reasoning for visual question answering. In Multimodal Learning and Applications (MULA) Workshop, CVPR, 2022.": "Coarse-to-fine reasoning for visual question answering", "Lu et al. (2020) Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.": "12-in-1: Multi-task vision and language representation learning"}, "source_title_to_arxiv_id": {"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation": "2201.12086", "An empirical study of training end-to-end vision-and-language transformers": "2111.02387", "Align before fuse: Vision and language representation learning with momentum distillation": "2107.07651"}}