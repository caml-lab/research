{"title": "Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets", "abstract": "While parameter efficient tuning (PET) methods have shown great potential\nwith transformer architecture on Natural Language Processing (NLP) tasks, their\neffectiveness is still under-studied with large-scale ConvNets on Computer\nVision (CV) tasks. This paper proposes Conv-Adapter, a PET module designed for\nConvNets. Conv-Adapter is light-weight, domain-transferable, and\narchitecture-agnostic with generalized performance on different tasks. When\ntransferring on downstream tasks, Conv-Adapter learns tasks-specific feature\nmodulation to the intermediate representations of backbone while keeping the\npre-trained parameters frozen. By introducing only a tiny amount of learnable\nparameters, e.g., only 3.5% full fine-tuning parameters of ResNet50,\nConv-Adapter outperforms previous PET baseline methods and achieves comparable\nor surpasses the performance of full fine-tuning on 23 classification tasks of\nvarious domains. It also presents superior performance on few-shot\nclassifications, with an average margin of 3.39%. Beyond classification,\nConv-Adapter can generalize to detection and segmentation tasks with more than\n50% reduction of parameters but comparable performance to the traditional full\nfine-tuning.", "authors": ["Hao Chen", "Ran Tao", "Han Zhang", "Yidong Wang", "Wei Ye", "Jindong Wang", "Guosheng Hu", "Marios Savvides"], "published_date": "2022_08_15", "pdf_url": "http://arxiv.org/pdf/2208.07463v3", "list_table_and_caption": [{"table": "<table><thead><tr><th>Backbone</th><th>Pre-trained Objective</th><th>Pre-trained Dataset</th><th># Param (M)</th><th>Feature Dim</th><th>Model</th></tr></thead><tbody><tr><td>ResNet50 (He et al. 2016)</td><td>Supervised</td><td>ImageNet-1k</td><td>23.5</td><td>2,048</td><td>checkpoint</td></tr><tr><td>ResNet50 (He et al. 2016)</td><td>Supervised</td><td>ImageNet-21k</td><td>23.5</td><td>2,048</td><td>checkpoint</td></tr><tr><td>ResNet50 BiT-M (Kolesnikov et al. 2019)</td><td>Supervised</td><td>ImageNet-21k</td><td>23.5</td><td>2,048</td><td>checkpoint</td></tr><tr><td>ConvNext-B (Liu et al. 2022b)</td><td>Supervised</td><td>ImageNet-1k</td><td>87.6</td><td>1,024</td><td>checkpoint</td></tr><tr><td>ConvNext-B (Liu et al. 2022b)</td><td>Supervised</td><td>ImageNet-21k</td><td>87.6</td><td>1,024</td><td>checkpoint</td></tr><tr><td>ConvNext-L (Liu et al. 2022b)</td><td>Supervised</td><td>ImageNet-21k</td><td>196.2</td><td>1,536</td><td>checkpoint</td></tr><tr><td>Swin-B (Liu et al. 2021b)</td><td>Supervised</td><td>ImageNet-21k</td><td>86.7</td><td>1,024</td><td>checkpoint</td></tr><tr><td>Swin-L (Liu et al. 2021b)</td><td>Supervised</td><td>ImageNet-21k</td><td>194.9</td><td>1,536</td><td>checkpoint</td></tr><tr><td>ResNet50 (Radford et al. 2021)</td><td>CLIP</td><td>CLIP</td><td>38.3</td><td>1,024</td><td>checkpoint</td></tr><tr><td>ResNet50x4 (Radford et al. 2021)</td><td>CLIP</td><td>CLIP</td><td>87.1</td><td>640</td><td>checkpoint</td></tr><tr><td>ResNet50 (He et al. 2020)</td><td>MoCov3</td><td>ImageNet-1k</td><td>23.5</td><td>2,048</td><td>checkpoint</td></tr></tbody></table>", "caption": "Table 7: Specification of pre-trained models used in experiments. ", "list_citation_info": ["He et al. (2020) He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).", "Liu et al. (2022b) Liu, Z.; Mao, H.; Wu, C.-Y.; Feichtenhofer, C.; Darrell, T.; and Xie, S. 2022b. A ConvNet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).", "Kolesnikov et al. (2019) Kolesnikov, A.; Beyer, L.; Zhai, X.; Puigcerver, J.; Yung, J.; Gelly, S.; and Houlsby, N. 2019. Big Transfer (BiT): General Visual Representation Learning. arXiv:1912.11370.", "He et al. (2016) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "Liu et al. (2021b) Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021b. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).", "Radford et al. (2021) Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020."]}, {"table": "<table><thead><tr><th></th><th>All Backbones</th></tr></thead><tbody><tr><th>Optimizer</th><td>AdamW (Loshchilov and Hutter 2017)</td></tr><tr><th>LR Range</th><td>[1e-3, 5e-4, 1e-4, 5e-5, 1e-5]</td></tr><tr><th>WD Range</th><td>[1e-2, 1e-3, 1e-4, 0]</td></tr><tr><th>LR schedule</th><td>cosine</td></tr><tr><th>Total Epochs</th><td>100</td></tr><tr><th>Warmup</th><td>10</td></tr></tbody></table>", "caption": "Table 8: Hyper-parameter range for grid-search on image classification tasks of FGVC and VTAB-1k.", "list_citation_info": ["Loshchilov and Hutter (2017) Loshchilov, I.; and Hutter, F. 2017. Decoupled Weight Decay Regularization. arXiv:1711.05101."]}], "citation_info_to_title": {"Liu et al. (2022b) Liu, Z.; Mao, H.; Wu, C.-Y.; Feichtenhofer, C.; Darrell, T.; and Xie, S. 2022b. A ConvNet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).": "A ConvNet for the 2020s", "Loshchilov and Hutter (2017) Loshchilov, I.; and Hutter, F. 2017. Decoupled Weight Decay Regularization. arXiv:1711.05101.": "Decoupled Weight Decay Regularization", "He et al. (2020) He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).": "Momentum Contrast for Unsupervised Visual Representation Learning", "Radford et al. (2021) Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021. Learning Transferable Visual Models From Natural Language Supervision. arXiv:2103.00020.": "Learning Transferable Visual Models From Natural Language Supervision", "Liu et al. (2021b) Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021b. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "Kolesnikov et al. (2019) Kolesnikov, A.; Beyer, L.; Zhai, X.; Puigcerver, J.; Yung, J.; Gelly, S.; and Houlsby, N. 2019. Big Transfer (BiT): General Visual Representation Learning. arXiv:1912.11370.": "Big Transfer (BiT): General Visual Representation Learning", "He et al. (2016) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).": "Deep Residual Learning for Image Recognition"}, "source_title_to_arxiv_id": {"A ConvNet for the 2020s": "2201.03545", "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows": "2103.14030"}}