{"title": "Multi-Query Video Retrieval", "abstract": "Retrieving target videos based on text descriptions is a task of great\npractical value and has received increasing attention over the past few years.\nDespite recent progress, imperfect annotations in existing video retrieval\ndatasets have posed significant challenges on model evaluation and development.\nIn this paper, we tackle this issue by focusing on the less-studied setting of\nmulti-query video retrieval, where multiple descriptions are provided to the\nmodel for searching over the video archive. We first show that multi-query\nretrieval task effectively mitigates the dataset noise introduced by imperfect\nannotations and better correlates with human judgement on evaluating retrieval\nabilities of current models. We then investigate several methods which leverage\nmultiple queries at training time, and demonstrate that the multi-query\ninspired training can lead to superior performance and better generalization.\nWe hope further investigation in this direction can bring new insights on\nbuilding systems that perform better in real-world video retrieval\napplications.", "authors": ["Zeyu Wang", "Yu Wu", "Karthik Narasimhan", "Olga Russakovsky"], "published_date": "2022_01_10", "pdf_url": "http://arxiv.org/pdf/2201.03639v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"5\">MSVD [55] (Frozen)</td><td colspan=\"5\">Vatex [50] (Frozen)</td></tr><tr><th></th><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td><td>MnR \\downarrow</td><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td><td>MnR \\downarrow</td></tr><tr><th>Baseline</th><td>39.6</td><td>70.5</td><td>80.7</td><td>2.0</td><td>13.9</td><td>26.7</td><td>56.9</td><td>70.4</td><td>4.0</td><td>26.4</td></tr><tr><th>RA</th><td>42.5</td><td>75.5</td><td>85.9</td><td>2.0</td><td>6.2</td><td>37.1</td><td>69.8</td><td>82.1</td><td>2.0</td><td>11.7</td></tr><tr><th>SA</th><td>53.2</td><td>85.3</td><td>93.0</td><td>1.0</td><td>3.7</td><td>40.6</td><td>72.2</td><td>83.8</td><td>2.0</td><td>10.0</td></tr><tr><th>MF</th><td>55.8</td><td>86.5</td><td>93.8</td><td>1.0</td><td>3.7</td><td>49.0</td><td>79.2</td><td>88.3</td><td>2.0</td><td>8.1</td></tr><tr><th>TS-WF</th><td>56.5</td><td>87.1</td><td>93.9</td><td>1.0</td><td>3.7</td><td>49.7</td><td>79.7</td><td>88.7</td><td>1.8</td><td>7.9</td></tr><tr><th>LG-WF</th><td>56.4</td><td>86.3</td><td>93.4</td><td>1.0</td><td>3.9</td><td>49.5</td><td>79.4</td><td>88.5</td><td>1.9</td><td>8.2</td></tr><tr><th>CG-WF</th><td>56.7</td><td>86.8</td><td>93.8</td><td>1.0</td><td>3.8</td><td>49.4</td><td>79.3</td><td>88.5</td><td>1.9</td><td>8.6</td></tr></tbody></table>", "caption": "Table 3: Performance of different multi-query retrieval methods on MSVD and VATEX datasets with Frozen [8] backbone. The baseline is trained and evaluated with one query. Others are evaluated with five-query input. RA, SA are trained with one query. MF, TS-WF, LG-WF, and CG-WF are trained with five-query input. All numbers are the average over 100 evaluations with different query samples. Recall numbers are reported in percent.", "list_citation_info": ["[8] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv:2104.00650 (2021)", "[50] Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.F., Wang, W.Y.: VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In: ICCV (2019)", "[55] Xu, J., Mei, T., Yao, T., Rui, Y.: MSR-VTT: A large video description dataset for bridging video and language. In: CVPR (2016)"]}], "citation_info_to_title": {"[8] Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. arXiv:2104.00650 (2021)": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "[55] Xu, J., Mei, T., Yao, T., Rui, Y.: MSR-VTT: A large video description dataset for bridging video and language. In: CVPR (2016)": "MSR-VTT: A large video description dataset for bridging video and language", "[50] Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.F., Wang, W.Y.: VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In: ICCV (2019)": "VATEX: A large-scale, high-quality multilingual dataset for video-and-language research"}, "source_title_to_arxiv_id": {"Frozen in time: A joint video and image encoder for end-to-end retrieval": "2104.00650"}}