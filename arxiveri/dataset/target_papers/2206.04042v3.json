{"title": "Learning Ego 3D Representation as Ray Tracing", "abstract": "A self-driving perception model aims to extract 3D semantic representations\nfrom multiple cameras collectively into the bird's-eye-view (BEV) coordinate\nframe of the ego car in order to ground downstream planner. Existing perception\nmethods often rely on error-prone depth estimation of the whole scene or\nlearning sparse virtual 3D representations without the target geometry\nstructure, both of which remain limited in performance and/or capability. In\nthis paper, we present a novel end-to-end architecture for ego 3D\nrepresentation learning from an arbitrary number of unconstrained camera views.\nInspired by the ray tracing principle, we design a polarized grid of \"imaginary\neyes\" as the learnable ego 3D representation and formulate the learning process\nwith the adaptive attention mechanism in conjunction with the 3D-to-2D\nprojection. Critically, this formulation allows extracting rich 3D\nrepresentation from 2D images without any depth supervision, and with the\nbuilt-in geometry structure consistent w.r.t. BEV. Despite its simplicity and\nversatility, extensive experiments on standard BEV visual tasks (e.g.,\ncamera-based 3D object detection and BEV segmentation) show that our model\noutperforms all state-of-the-art alternatives significantly, with an extra\nadvantage in computational efficiency from multi-task learning.", "authors": ["Jiachen Lu", "Zheyuan Zhou", "Xiatian Zhu", "Hang Xu", "Li Zhang"], "published_date": "2022_06_08", "pdf_url": "http://arxiv.org/pdf/2206.04042v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Methods</td><td>mATE\\downarrow</td><td>mASE\\downarrow</td><td>mAOE\\downarrow</td><td>mAVE\\downarrow</td><td>mAAE\\downarrow</td><td>mAP\\uparrow</td><td>NDS\\uparrow</td></tr><tr><td>FCOS3D[31]</td><td><p>0.790</p></td><td>0.261</td><td><p>0.499</p></td><td><p>1.286</p></td><td>0.167</td><td><p>0.298</p></td><td><p>0.377</p></td></tr><tr><td>DETR3D[34]</td><td><p>0.860</p></td><td><p>0.278</p></td><td>0.327</td><td>0.967</td><td><p>0.235</p></td><td><p>0.303</p></td><td><p>0.374</p></td></tr><tr><td>PGD[30]</td><td><p>0.732</p></td><td><p>0.263</p></td><td><p>0.423</p></td><td><p>1.285</p></td><td><p>0.172</p></td><td><p>0.336</p></td><td><p>0.409</p></td></tr><tr><td>Ego3RT(Ours)</td><td>0.714</td><td><p>0.275</p></td><td><p>0.421</p></td><td><p>0.988</p></td><td><p>0.292</p></td><td>0.355</td><td>0.409</td></tr><tr><td>FCOS3D{\\dagger}[31]</td><td><p>0.754</p></td><td>0.260</td><td><p>0.486</p></td><td><p>1.331</p></td><td>0.158</td><td><p>0.321</p></td><td><p>0.395</p></td></tr><tr><td>DETR3D{\\dagger}[34]</td><td><p>0.765</p></td><td><p>0.267</p></td><td><p>0.392</p></td><td><p>0.876</p></td><td><p>0.211</p></td><td><p>0.347</p></td><td><p>0.422</p></td></tr><tr><td>PGD{\\dagger}[30]</td><td><p>0.667</p></td><td><p>0.264</p></td><td><p>0.435</p></td><td><p>1.276</p></td><td><p>0.177</p></td><td><p>0.358</p></td><td><p>0.425</p></td></tr><tr><td>Ego3RT(Ours){\\dagger}</td><td>0.657</td><td><p>0.268</p></td><td>0.391</td><td>0.850</td><td><p>0.206</p></td><td>0.375</td><td>0.450</td></tr><tr><td>Ego3RT(Ours){\\ddagger}</td><td>0.582</td><td><p>0.272</p></td><td>0.316</td><td>0.683</td><td><p>0.202</p></td><td>0.478</td><td>0.534</td></tr></tbody></table>", "caption": "Table 1: Comparison of different paradigms on the nuScenes val set.FCOS3D{\\dagger} is trained with 1x learning schedule, depth weight 0.2 and is finetuned on another FCOS3D checkpoint.PGD{\\dagger} is trained with 2x learning schedule, depth weight 0.2 on another PGD checkpoint.DETR3D{\\dagger} and Ego3RT{\\dagger} are initialized from the same pretrained FCOS3D checkpoint.Ego3RT{\\ddagger} is initialized from the pretrained DD3D checkpoint.", "list_citation_info": ["[34] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning (2022)", "[31] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d object detection. In: CVPR (2021)", "[30] Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting objects in perspective. In: Conference on Robot Learning (2022)"]}, {"table": "<table><tbody><tr><td>Methods</td><td>mATE\\downarrow</td><td>mASE\\downarrow</td><td>mAOE\\downarrow</td><td>mAVE\\downarrow</td><td>mAAE\\downarrow</td><td>mAP\\uparrow</td><td>NDS\\uparrow</td></tr><tr><td>MonoDIS</td><td><p>0.738</p></td><td><p>0.263</p></td><td><p>0.546</p></td><td><p>1.553</p></td><td><p>0.134</p></td><td><p>0.304</p></td><td><p>0.384</p></td></tr><tr><td>CenterNet [41]</td><td><p>0.658</p></td><td><p>0.255</p></td><td><p>0.629</p></td><td><p>1.629</p></td><td><p>0.142</p></td><td><p>0.338</p></td><td><p>0.400</p></td></tr><tr><td>FCOS3D[31]</td><td><p>0.690</p></td><td><p>0.249</p></td><td><p>0.452</p></td><td><p>1.434</p></td><td>0.124</td><td><p>0.358</p></td><td><p>0.428</p></td></tr><tr><td>PGD[30]</td><td><p>0.626</p></td><td>0.245</td><td>0.451</td><td><p>1.509</p></td><td><p>0.127</p></td><td><p>0.386</p></td><td>0.448</td></tr><tr><td>Ego3RT(Ours)</td><td>0.599</td><td><p>0.268</p></td><td><p>0.470</p></td><td>1.169</td><td><p>0.172</p></td><td>0.389</td><td><p>0.443</p></td></tr><tr><td>DD3D{\\ddagger}[21]</td><td><p>0.572</p></td><td>0.249</td><td>0.368</td><td><p>1.014</p></td><td>0.124</td><td><p>0.418</p></td><td><p>0.477</p></td></tr><tr><td>DETR3D{\\ddagger}[34]</td><td><p>0.641</p></td><td><p>0.255</p></td><td><p>0.394</p></td><td>0.845</td><td><p>0.133</p></td><td><p>0.412</p></td><td>0.479</td></tr><tr><td>Ego3RT(Ours){\\ddagger}</td><td>0.549</td><td><p>0.264</p></td><td><p>0.433</p></td><td><p>1.014</p></td><td><p>0.145</p></td><td>0.425</td><td><p>0.473</p></td></tr></tbody></table>", "caption": "Table 2: Comparisons to top-performing works on the nuScenes test set.{\\ddagger} represents that the method uses external data other than nuScenes 3D box annotations.DD3D{\\ddagger} uses extra data for depth estimation.DETR3D{\\ddagger} and Ego3RT{\\ddagger} are initialized from the pre-trained DD3D checkpoint.", "list_citation_info": ["[31] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d object detection. In: CVPR (2021)", "[34] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning (2022)", "[41] Zhou, X., Wang, D., Kr\u00e4henb\u00fchl, P.: Objects as points. arXiv preprint (2019)", "[30] Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting objects in perspective. In: Conference on Robot Learning (2022)", "[21] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is pseudo-lidar needed for monocular 3d object detection? In: ICCV (2021)"]}, {"table": "<table><tbody><tr><th>Method</th><th>multi?</th><td>Drivable</td><td>Crossing</td><td>Walkway</td><td>Carpark</td><td>Divider</td></tr><tr><th>VED [17]</th><th>\u2717</th><td>54.7</td><td>12.0</td><td>20.7</td><td>13.5</td><td>-</td></tr><tr><th>VPN [20]</th><th>\u2717</th><td>58.0</td><td>27.3</td><td>29.4</td><td>12.3</td><td>-</td></tr><tr><th>PON [25]</th><th>\u2717</th><td>60.4</td><td>28.0</td><td>31.0</td><td>18.4</td><td>-</td></tr><tr><th>OFT [26]</th><th>\u2717</th><td>62.4</td><td>30.9</td><td>34.5</td><td>23.5</td><td>-</td></tr><tr><th>LSF [7]</th><th>\u2717</th><td>61.1</td><td>33.5</td><td>37.8</td><td>25.4</td><td>-</td></tr><tr><th>Image2Map [27]</th><th>\u2717</th><td>74.5</td><td>36.6</td><td>35.9</td><td>31.3</td><td>-</td></tr><tr><th>OFT [26]</th><th>\u2713</th><td>71.7</td><td>-</td><td>-</td><td>-</td><td>18.0</td></tr><tr><th>LSS [22]</th><th>\u2713</th><td>72.9</td><td>-</td><td>-</td><td>-</td><td>20.0</td></tr><tr><th>Ego3RT(Ours)</th><th>\u2713</th><td>79.6</td><td>48.3</td><td>52.0</td><td>50.3</td><td>47.5</td></tr><tr><th>Ego3RT(Ours) \\lx@paragraphsign</th><th>\u2713</th><td>74.6</td><td>33.0</td><td>42.6</td><td>44.1</td><td>36.6</td></tr></tbody></table>", "caption": "Table 3: Comparison of BEV semantic segmentation IoU on the nuScenes val set. Multi means wether generate a full surrounded BEV segmentation map from multi-view images. \u201c-\u201d represents the unprovided result. Single-task version Ego3RT uses EfficientNet-B0 [29] as the image backbone to align with OFT [26] and LSS [22].Multi-task version Ego3RT\\lx@paragraphsign means we only train the segmentation head with the pretrained detection model frozen.", "list_citation_info": ["[25] Roddick, T., Cipolla, R.: Predicting semantic map representations from images using pyramid occupancy networks. In: CVPR (2020)", "[7] Dwivedi, I., Malla, S., Chen, Y.T., Dariush, B.: Bird\u2019s eye view segmentation using lifted 2d semantic features. In: BMVC (2021)", "[26] Roddick, T., Kendall, A., Cipolla, R.: Orthographic feature transform for monocular 3d object detection. In: BMVC (2019)", "[17] Lu, C., van de Molengraft, M.J.G., Dubbelman, G.: Monocular Semantic Occupancy Grid Mapping With Convolutional Variational Encoder-Decoder Networks. IEEE Robotics and Automation Letters (2019)", "[27] Saha, A., Maldonado, O.M., Russell, C., Bowden, R.: Translating images into maps. arXiv preprint (2021)", "[29] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: ICML (2019)", "[20] Pan, B., Sun, J., Leung, H.Y.T., Andonian, A., Zhou, B.: Cross-view semantic segmentation for sensing surroundings. IEEE Robotics and Automation Letters (2020)", "[22] Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In: ECCV (2020)"]}, {"table": "<table><thead><tr><th>Methods</th><th>overlap?</th><th>mATE\\downarrow</th><th>mASE\\downarrow</th><th>mAOE\\downarrow</th><th>mAVE\\downarrow</th><th>mAAE\\downarrow</th><th>mAP\\uparrow</th><th>NDS\\uparrow</th></tr></thead><tbody><tr><td>FCOS3D[31]</td><td><p>\u2717</p></td><td><p>0.747</p></td><td>0.260</td><td><p>0.487</p></td><td><p>1.351</p></td><td>0.156</td><td><p>0.320</p></td><td><p>0.395</p></td></tr><tr><td>PGD[30]</td><td><p>\u2717</p></td><td><p>0.658</p></td><td><p>0.263</p></td><td><p>0.425</p></td><td><p>1.290</p></td><td><p>0.178</p></td><td><p>0.357</p></td><td><p>0.426</p></td></tr><tr><td>DETR3D[34]</td><td><p>\u2717</p></td><td><p>0.769</p></td><td><p>0.267</p></td><td>0.390</td><td><p>0.893</p></td><td><p>0.215</p></td><td><p>0.343</p></td><td><p>0.419</p></td></tr><tr><td>Ego3RT(Ours)</td><td><p>\u2717</p></td><td>0.655</td><td><p>0.267</p></td><td><p>0.395</p></td><td>0.854</td><td><p>0.208</p></td><td>0.371</td><td>0.448</td></tr><tr><td>FCOS3D[31]</td><td><p>\u2713</p></td><td><p>0.816</p></td><td><p>0.272</p></td><td><p>0.571</p></td><td><p>1.084</p></td><td>0.173</td><td><p>0.229</p></td><td><p>0.329</p></td></tr><tr><td>PGD[30]</td><td><p>\u2713</p></td><td><p>0.768</p></td><td><p>0.274</p></td><td><p>0.495</p></td><td><p>1.090</p></td><td><p>0.186</p></td><td><p>0.255</p></td><td><p>0.354</p></td></tr><tr><td>DETR3D[34]</td><td><p>\u2713</p></td><td><p>0.807</p></td><td><p>0.273</p></td><td><p>0.453</p></td><td>0.788</td><td><p>0.184</p></td><td><p>0.268</p></td><td><p>0.384</p></td></tr><tr><td>Ego3RT(Ours)</td><td><p>\u2713</p></td><td>0.671</td><td>0.268</td><td>0.347</td><td><p>0.797</p></td><td><p>0.212</p></td><td>0.298</td><td>0.420</td></tr></tbody></table>", "caption": "Table 5: Comparisons of detection performance in non-overlap region and overlap region.FCOS3D is trained with 1x learning schedule, depth weight 0.2 and is finetuned on another FCOS3D checkpoint.PGD is trained with 2x learning schedule, depth weight 0.2 and is finetuned on another PGD checkpoint.DETR3D and Ego3RTare initialized from a same pretrained FCOS3D checkpoint.", "list_citation_info": ["[34] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning (2022)", "[31] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d object detection. In: CVPR (2021)", "[30] Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting objects in perspective. In: Conference on Robot Learning (2022)"]}, {"table": "<table><tbody><tr><th>Methods</th><td>Resolution</td><td>Eyes density</td><td>FFN</td><td>#Blocks</td><td>FPS\\uparrow</td><td>mAP\\uparrow</td><td>NDS\\uparrow</td></tr><tr><th>FCOS3D[31]</th><td>1600\\times 900</td><td>-</td><td>-</td><td>-</td><td><p>2.0</p></td><td><p>0.321</p></td><td><p>0.395</p></td></tr><tr><th>PGD [30]</th><td>1600\\times 900</td><td>-</td><td>-</td><td>-</td><td><p>1.5</p></td><td><p>0.358</p></td><td><p>0.425</p></td></tr><tr><th>DETR3D [34]</th><td>1600\\times 900</td><td>-</td><td>-</td><td>-</td><td>3.0</td><td><p>0.347</p></td><td><p>0.422</p></td></tr><tr><th>Ego3RT(Ours)</th><td>1600\\times 900^{\\star}</td><td>80\\times 256</td><td>1024</td><td>8</td><td><p>1.7</p></td><td>0.375</td><td>0.450</td></tr><tr><th>Ego3RT(Ours)</th><td>1280\\times 768</td><td>72\\times 192</td><td>1024</td><td>8</td><td><p>2.3</p></td><td><p>0.372</p></td><td><p>0.438</p></td></tr><tr><th>Ego3RT(Ours)</th><td>1280\\times 768</td><td>64\\times 128</td><td>256</td><td>2</td><td>3.0</td><td><p>0.355</p></td><td><p>0.423</p></td></tr></tbody></table>", "caption": "Table 10: Comparison of the efficiency and the performance of different configurations of Ego3RT and the other methods.\u201cFPS\u201d is a metric for efficiency standing for frames per second.\u201cResolution\u201d represents input image shape.\u201cFFN\u201d represents the channel expansion dimension of FFN in Back tracing decoder.\u201cBlocks\u201d notes the number of blocks in BEV encoder.\u201c\\star\u201d means we test the speed at 1600\\times 900.", "list_citation_info": ["[34] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning (2022)", "[31] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d object detection. In: CVPR (2021)", "[30] Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting objects in perspective. In: Conference on Robot Learning (2022)"]}], "citation_info_to_title": {"[26] Roddick, T., Kendall, A., Cipolla, R.: Orthographic feature transform for monocular 3d object detection. In: BMVC (2019)": "Orthographic Feature Transform for Monocular 3D Object Detection", "[22] Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In: ECCV (2020)": "Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d", "[41] Zhou, X., Wang, D., Kr\u00e4henb\u00fchl, P.: Objects as points. arXiv preprint (2019)": "Objects as Points", "[21] Park, D., Ambrus, R., Guizilini, V., Li, J., Gaidon, A.: Is pseudo-lidar needed for monocular 3d object detection? In: ICCV (2021)": "Is pseudo-lidar needed for monocular 3d object detection?", "[25] Roddick, T., Cipolla, R.: Predicting semantic map representations from images using pyramid occupancy networks. In: CVPR (2020)": "Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks", "[7] Dwivedi, I., Malla, S., Chen, Y.T., Dariush, B.: Bird\u2019s eye view segmentation using lifted 2d semantic features. In: BMVC (2021)": "Bird\u2019s Eye View Segmentation Using Lifted 2D Semantic Features", "[20] Pan, B., Sun, J., Leung, H.Y.T., Andonian, A., Zhou, B.: Cross-view semantic segmentation for sensing surroundings. IEEE Robotics and Automation Letters (2020)": "Cross-view semantic segmentation for sensing surroundings", "[17] Lu, C., van de Molengraft, M.J.G., Dubbelman, G.: Monocular Semantic Occupancy Grid Mapping With Convolutional Variational Encoder-Decoder Networks. IEEE Robotics and Automation Letters (2019)": "Monocular Semantic Occupancy Grid Mapping With Convolutional Variational Encoder-Decoder Networks", "[27] Saha, A., Maldonado, O.M., Russell, C., Bowden, R.: Translating images into maps. arXiv preprint (2021)": "Translating images into maps", "[29] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: ICML (2019)": "Efficientnet: Rethinking Model Scaling for Convolutional Neural Networks", "[34] Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d object detection from multi-view images via 3d-to-2d queries. In: Conference on Robot Learning (2022)": "Detr3d: 3D Object Detection from Multi-View Images via 3D-to-2D Queries", "[31] Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monocular 3d object detection. In: CVPR (2021)": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection", "[30] Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting objects in perspective. In: Conference on Robot Learning (2022)": "Probabilistic and Geometric Depth: Detecting Objects in Perspective"}, "source_title_to_arxiv_id": {"Detr3d: 3D Object Detection from Multi-View Images via 3D-to-2D Queries": "2110.06922"}}