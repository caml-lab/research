{"title": "Few-Shot Head Swapping in the Wild", "abstract": "The head swapping task aims at flawlessly placing a source head onto a target\nbody, which is of great importance to various entertainment scenarios. While\nface swapping has drawn much attention, the task of head swapping has rarely\nbeen explored, particularly under the few-shot setting. It is inherently\nchallenging due to its unique needs in head modeling and background blending.\nIn this paper, we present the Head Swapper (HeSer), which achieves few-shot\nhead swapping in the wild through two delicately designed modules. Firstly, a\nHead2Head Aligner is devised to holistically migrate pose and expression\ninformation from the target to the source head by examining multi-scale\ninformation. Secondly, to tackle the challenges of skin color variations and\nhead-background mismatches in the swapping procedure, a Head2Scene Blender is\nintroduced to simultaneously modify facial skin color and fill mismatched gaps\nin the background around the head. Particularly, seamless blending is achieved\nwith the help of a Semantic-Guided Color Reference Creation procedure and a\nBlending UNet. Extensive experiments demonstrate that the proposed method\nproduces superior head swapping results in a variety of scenes.", "authors": ["Changyong Shu", "Hemao Wu", "Hang Zhou", "Jiaming Liu", "Zhibin Hong", "Changxing Ding", "Junyu Han", "Jingtuo Liu", "Errui Ding", "Jingdong Wang"], "published_date": "2022_04_27", "pdf_url": "http://arxiv.org/pdf/2204.13100v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>ID \\uparrow</th><th>Exp \\uparrow</th><th>Skin Color \\uparrow</th><th>Inpainting \\uparrow</th><th>Holistic \\uparrow</th></tr></thead><tbody><tr><th>FaceSwap [4]</th><td>1.22</td><td>3.02</td><td>3.36</td><td>\u2013</td><td>1.42</td></tr><tr><th>Deepfacelab [27]</th><td>2.16</td><td>1.47</td><td>1.53</td><td>0.05</td><td>2.26</td></tr><tr><th>Ours</th><td>4.12</td><td>3.01</td><td>2.61</td><td>4.95</td><td>3.82</td></tr></tbody></table>", "caption": "Table 1: Average score from the user study, rating from 0 to 5.", "list_citation_info": ["[27] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um, Mr Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, et al. Deepfacelab: Integrated, flexible and extensible face-swapping framework. arXiv preprint arXiv:2005.05535, 2020.", "[4] Renwang Chen, Xuanhong Chen, and Yanhao. Ge. Simswap: An efficient framework for high fidelity face swapping. In ACM Int. Conf. Multimedia, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><td>E_{ID} \\downarrow</td><td>E_{P} \\downarrow</td><td>SSIM \\uparrow</td><td>LPIPS \\downarrow</td><td>PSNR \\uparrow</td></tr><tr><th>FOMM [32]</th><td>0.49</td><td>0.275</td><td>0.76</td><td>0.18</td><td>30.92</td></tr><tr><th>LPD [7]</th><td>0.71</td><td>0.063</td><td>0.52</td><td>0.50</td><td>28.84</td></tr><tr><th>Siarohin et al [33]</th><td>0.71</td><td>0.137</td><td>0.73</td><td>0.20</td><td>30.01</td></tr><tr><th>Ours</th><td>0.53</td><td>0.026</td><td>0.77</td><td>0.19</td><td>31.33</td></tr><tr><th>LPD (32-shot + ft)</th><td>0.23</td><td>0.024</td><td>0.62</td><td>0.36</td><td>29.46</td></tr><tr><th>Ours (32-shot + ft)</th><td>0.22</td><td>0.024</td><td>0.89</td><td>0.12</td><td>33.26</td></tr></tbody></table>", "caption": "Table 2: Quantitative comparison on face reenactment.", "list_citation_info": ["[32] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. neurips, 2019.", "[33] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In CVPR, 2021.", "[7] Burkov Egor, Pasechnik Igor, Grigorev Artur, and Lempitsky Victor. Neural head reenactment with latent pose descriptors. In IEEE Conf. Comput. Vis. Pattern Recog., 2020."]}], "citation_info_to_title": {"[27] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um, Mr Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, et al. Deepfacelab: Integrated, flexible and extensible face-swapping framework. arXiv preprint arXiv:2005.05535, 2020.": "Deepfacelab: Integrated, flexible and extensible face-swapping framework", "[7] Burkov Egor, Pasechnik Igor, Grigorev Artur, and Lempitsky Victor. Neural head reenactment with latent pose descriptors. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.": "Neural head reenactment with latent pose descriptors", "[32] Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. neurips, 2019.": "First Order Motion Model for Image Animation", "[4] Renwang Chen, Xuanhong Chen, and Yanhao. Ge. Simswap: An efficient framework for high fidelity face swapping. In ACM Int. Conf. Multimedia, 2020.": "Simswap: An efficient framework for high fidelity face swapping", "[33] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In CVPR, 2021.": "Motion Representations for Articulated Animation"}, "source_title_to_arxiv_id": {"First Order Motion Model for Image Animation": "2003.00196", "Motion Representations for Articulated Animation": "2104.11280"}}