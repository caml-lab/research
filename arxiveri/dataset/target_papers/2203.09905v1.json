{"title": "Learning Affordance Grounding from Exocentric Images", "abstract": "Affordance grounding, a task to ground (i.e., localize) action possibility\nregion in objects, which faces the challenge of establishing an explicit link\nwith object parts due to the diversity of interactive affordance. Human has the\nability that transform the various exocentric interactions to invariant\negocentric affordance so as to counter the impact of interactive diversity. To\nempower an agent with such ability, this paper proposes a task of affordance\ngrounding from exocentric view, i.e., given exocentric human-object interaction\nand egocentric object images, learning the affordance knowledge of the object\nand transferring it to the egocentric image using only the affordance label as\nsupervision. To this end, we devise a cross-view knowledge transfer framework\nthat extracts affordance-specific features from exocentric interactions and\nenhances the perception of affordance regions by preserving affordance\ncorrelation. Specifically, an Affordance Invariance Mining module is devised to\nextract specific clues by minimizing the intra-class differences originated\nfrom interaction habits in exocentric images. Besides, an Affordance\nCo-relation Preserving strategy is presented to perceive and localize\naffordance by aligning the co-relation matrix of predicted results between the\ntwo views. Particularly, an affordance grounding dataset named AGD20K is\nconstructed by collecting and labeling over 20K images from 36 affordance\ncategories. Experimental results demonstrate that our method outperforms the\nrepresentative models in terms of objective metrics and visual quality. Code:\ngithub.com/lhc1224/Cross-View-AG.", "authors": ["Hongchen Luo", "Wei Zhai", "Jing Zhang", "Yang Cao", "Dacheng Tao"], "published_date": "2022_03_18", "pdf_url": "http://arxiv.org/pdf/2203.09905v1", "list_table_and_caption": [{"table": "<table><tr><td> Dataset</td><td>Year</td><td>HQ</td><td>Part</td><td>\\sharpObj.</td><td>\\sharpAff.</td><td>\\sharpImg.</td></tr><tr><td> UMD [32]</td><td>2015</td><td>\u2717</td><td>\u2713</td><td>17</td><td>7</td><td>30,000</td></tr><tr><td>[43]</td><td>2017</td><td>\u2717</td><td>\u2713</td><td>17</td><td>7</td><td>3,090</td></tr><tr><td>IIT-AFF [35]</td><td>2017</td><td>\u2717</td><td>\u2713</td><td>10</td><td>9</td><td>8,835</td></tr><tr><td>ADE-Aff [4]</td><td>2018</td><td>\u2713</td><td>\u2713</td><td>150</td><td>7</td><td>10,000</td></tr><tr><td>PAD [29]</td><td>2021</td><td>\u2713</td><td>\u2717</td><td>72</td><td>31</td><td>4,002</td></tr><tr><td>AGD20k (Ours)</td><td>2021</td><td>\u2713</td><td>\u2713</td><td>50</td><td>36</td><td>23,816</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 1: Statistics of related datasets and the proposed AGD20K dataset. Part: part-level annotation. HQ: high-quality annotation. \\sharpObj: number of object classes. \\sharpAff: number of affordance classes. \\sharpImg: number of images.", "list_citation_info": ["[43] Johann Sawatzky, Abhilash Srikantha, and Juergen Gall. Weakly supervised affordance detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.", "[29] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. One-shot object affordance detection. arXiv preprint arXiv:2108.03658, 2021.", "[35] Anh Nguyen, Dimitrios Kanoulas, Darwin G Caldwell, and Nikos G Tsagarakis. Object-based affordances detection with convolutional neural networks and dense conditional random fields. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5908\u20135915. IEEE, 2017.", "[4] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja Fidler. Learning to act properly: Predicting and explaining affordances from images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 975\u2013983, 2018.", "[32] Austin Myers, Ching L Teo, Cornelia Ferm\u00fcller, and Yiannis Aloimonos. Affordance detection of tool parts from geometric features. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 1374\u20131381. IEEE, 2015."]}, {"table": "<table><tr><td> </td><td colspan=\"3\">Seen</td><td colspan=\"3\">Unseen</td></tr><tr><td>Methods</td><td>\\text{KLD}\\downarrow</td><td>\\text{SIM}\\uparrow</td><td>\\text{NSS}\\uparrow</td><td>\\text{KLD}\\downarrow</td><td>\\text{SIM}\\uparrow</td><td>\\text{NSS}\\uparrow</td></tr><tr><td> Mlnet [5]</td><td>5.197{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 70.4\\%}</td><td>0.280{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 19.3\\%}</td><td>0.596{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 55.5\\%}</td><td>5.012{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 64.3\\%}</td><td>0.263{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 8.4\\%}</td><td>0.595{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 39.3\\%}</td></tr><tr><td>DeepGazeII [21]</td><td>1.858{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 17.2\\%}</td><td>0.280{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 19.3\\%}</td><td>0.623{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 48.8\\%}</td><td>1.990{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 10.2\\%}</td><td>0.256{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 11.3\\%}</td><td>0.597{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 38.9\\%}</td></tr><tr><td>EgoGaze [17]</td><td>4.185{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 63.2\\%}</td><td>0.227{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 47.1\\%}</td><td>0.333{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 178.\\%}</td><td>4.285{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 58.3\\%}</td><td>0.211{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 35.1\\%}</td><td>0.350{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 137.\\%}</td></tr><tr><td>EIL [30]</td><td>1.931{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 20.4\\%}</td><td>0.285{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 17.2\\%}</td><td>0.522{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 77.6\\%}</td><td>2.167{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 17.5\\%}</td><td>0.227{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 25.6\\%}</td><td>0.330{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 151.\\%}</td></tr><tr><td>SPA [36]</td><td>5.528{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 72.2\\%}</td><td>0.221{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 51.1\\%}</td><td>0.357{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 160.\\%}</td><td>7.425{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 75.9\\%}</td><td>0.169{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 68.6\\%}</td><td>0.262{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 216.\\%}</td></tr><tr><td>TS-CAM [9]</td><td>1.842{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 16.5\\%}</td><td>0.260{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 28.5\\%}</td><td>0.336{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 176.\\%}</td><td>2.104{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 15.1\\%}</td><td>0.201{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 41.8\\%}</td><td>0.151{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 449.\\%}</td></tr><tr><td>Hotspots [33]</td><td>1.773{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 13.3\\%}</td><td>0.278{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 20.1\\%}</td><td>0.615{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 50.7\\%}</td><td>1.994{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 10.4\\%}</td><td>0.237{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 20.3\\%}</td><td>0.577{\\color[rgb]{0.91,0.33,0.5}\\scriptstyle~{}\\diamond 43.7\\%}</td></tr><tr><td>Ours</td><td>\\bm{1.538}_{\\pm 0.017}</td><td>\\bm{0.334}_{\\pm 0.001}</td><td>\\bm{0.927}_{\\pm 0.007}</td><td>\\bm{1.787}_{\\pm 0.017}</td><td>\\bm{0.285}_{\\pm 0.002}</td><td>\\bm{0.829}_{\\pm 0.014}</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 2: The results of different methods on AGD20k. The best results are in bold. \u201cSeen\u201d means that the training set and the test set contain the same object categories, while \u201cUnseen\u201d means that the object categories in the training set and the test set do not overlap. The {\\color[rgb]{0.91,0.33,0.5}\\diamond} defines the relative improvement of our method over other methods. \u201cDark red\u201d, \u201cOrange\u201d and \u201cPurple\u201d represent saliency detection, weakly supervised object localization and affordance grounding models, respectively.", "list_citation_info": ["[30] Jinjie Mai, Meng Yang, and Wenfeng Luo. Erasing integrated learning: A simple yet effective approach for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8766\u20138775, 2020.", "[5] Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara. A deep multi-level network for saliency prediction. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 3488\u20133493. IEEE, 2016.", "[9] Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han, Bolei Zhou, and Qixiang Ye. Ts-cam: Token semantic coupled attention map for weakly supervised object localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2886\u20132895, October 2021.", "[33] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman. Grounded human-object interaction hotspots from video. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), pages 8688\u20138697, 2019.", "[17] Yifei Huang, Minjie Cai, Zhenqiang Li, and Yoichi Sato. Predicting gaze in egocentric video by learning task-dependent attention transition. In Proceedings of the European Conference on Computer Vision (ECCV), pages 754\u2013769, 2018.", "[36] Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei Yuan, Feiyue Huang, and Changsheng Xu. Unveiling the potential of structure preserving for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11642\u201311651, 2021.", "[21] Matthias K\u00fcmmerer, Thomas SA Wallis, and Matthias Bethge. Deepgaze ii: Reading fixations from deep features trained on object recognition. arXiv preprint arXiv:1610.01563, 2016."]}, {"table": "<table><tr><td colspan=\"3\"> Classes</td><td>Hold</td><td>Swing</td><td>Drink with</td><td>Lie on</td><td>Brush with</td></tr><tr><td> Mlnet [5]</td><td>6.762</td><td>9.248</td><td>4.497</td><td>4.767</td><td>6.215</td></tr><tr><td>DeepGazeII [21]</td><td>2.071</td><td>2.478</td><td>2.067</td><td>1.602</td><td>2.385</td></tr><tr><td>EgoGaze [17]</td><td>4.671</td><td>6.723</td><td>4.268</td><td>2.921</td><td>5.135</td></tr><tr><td>EIL [30]</td><td>2.008</td><td>2.486</td><td>2.254</td><td>1.377</td><td>3.003</td></tr><tr><td>SPA [36]</td><td>3.006</td><td>6.720</td><td>7.683</td><td>4.006</td><td>8.043</td></tr><tr><td>TS-CAM [9]</td><td>1.628</td><td>2.420</td><td>2.300</td><td>1.370</td><td>2.642</td></tr><tr><td>Hotspots [33]</td><td>1.770</td><td>2.178</td><td>1.942</td><td>1.566</td><td>2.154</td></tr><tr><td>Ours</td><td>1.594</td><td>2.161</td><td>1.748</td><td>1.039</td><td>2.040</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 4: Different classes.The KLD results of different methods on some representative affordance categories.", "list_citation_info": ["[30] Jinjie Mai, Meng Yang, and Wenfeng Luo. Erasing integrated learning: A simple yet effective approach for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8766\u20138775, 2020.", "[5] Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara. A deep multi-level network for saliency prediction. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 3488\u20133493. IEEE, 2016.", "[9] Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han, Bolei Zhou, and Qixiang Ye. Ts-cam: Token semantic coupled attention map for weakly supervised object localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2886\u20132895, October 2021.", "[33] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman. Grounded human-object interaction hotspots from video. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), pages 8688\u20138697, 2019.", "[17] Yifei Huang, Minjie Cai, Zhenqiang Li, and Yoichi Sato. Predicting gaze in egocentric video by learning task-dependent attention transition. In Proceedings of the European Conference on Computer Vision (ECCV), pages 754\u2013769, 2018.", "[36] Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei Yuan, Feiyue Huang, and Changsheng Xu. Unveiling the potential of structure preserving for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11642\u201311651, 2021.", "[21] Matthias K\u00fcmmerer, Thomas SA Wallis, and Matthias Bethge. Deepgaze ii: Reading fixations from deep features trained on object recognition. arXiv preprint arXiv:1610.01563, 2016."]}, {"table": "<table><tr><td> </td><td>Method</td><td>Source</td><td>\\text{KLD}\\downarrow</td><td>\\text{SIM}\\uparrow</td><td>\\text{NSS}\\uparrow</td></tr><tr><td rowspan=\"7\"> Seen</td><td rowspan=\"2\"> EIL [30]</td><td>Exo</td><td>1.931</td><td>0.285</td><td>0.522</td></tr><tr><td>Exo&amp;Ego</td><td>2.156</td><td>0.321</td><td>0.747</td></tr><tr><td rowspan=\"2\">SPA [36]</td><td>Exo</td><td>5.528</td><td>0.221</td><td>0.357</td></tr><tr><td>Exo&amp;Ego</td><td>4.312</td><td>0.252</td><td>0.494</td></tr><tr><td rowspan=\"2\">TS-CAM [9]</td><td>Exo</td><td>1.842</td><td>0.260</td><td>0.336</td></tr><tr><td>Exo&amp;Ego</td><td>1.707</td><td>0.290</td><td>0.622</td></tr><tr><td>Ours</td><td>Exo&amp;Ego</td><td>1.538</td><td>0.334</td><td>0.927</td></tr><tr><td rowspan=\"7\"> Unseen</td><td rowspan=\"2\">EIL [30]</td><td>Exo</td><td>2.167</td><td>0.227</td><td>0.330</td></tr><tr><td>Exo&amp;Ego</td><td>2.029</td><td>0.256</td><td>0.529</td></tr><tr><td rowspan=\"2\">SPA [36]</td><td>Exo</td><td>7.425</td><td>0.169</td><td>0.262</td></tr><tr><td>Exo&amp;Ego</td><td>6.174</td><td>0.209</td><td>0.433</td></tr><tr><td rowspan=\"2\">TS-CAM [9]</td><td>Exo</td><td>2.104</td><td>0.201</td><td>0.151</td></tr><tr><td>Exo&amp;Ego</td><td>2.002</td><td>0.228</td><td>0.305</td></tr><tr><td>Ours</td><td>Exo&amp;Ego</td><td>\\bm{1.787}</td><td>\\bm{0.285}</td><td>\\bm{0.829}</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "Table 5: Different sources. \u201cExo\u201d means that training only uses exocentric images, \u201cExo&amp;Ego\u201d means that training uses both exocentric and egocentric images.", "list_citation_info": ["[30] Jinjie Mai, Meng Yang, and Wenfeng Luo. Erasing integrated learning: A simple yet effective approach for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8766\u20138775, 2020.", "[36] Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei Yuan, Feiyue Huang, and Changsheng Xu. Unveiling the potential of structure preserving for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11642\u201311651, 2021.", "[9] Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han, Bolei Zhou, and Qixiang Ye. Ts-cam: Token semantic coupled attention map for weakly supervised object localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2886\u20132895, October 2021."]}], "citation_info_to_title": {"[36] Xingjia Pan, Yingguo Gao, Zhiwen Lin, Fan Tang, Weiming Dong, Haolei Yuan, Feiyue Huang, and Changsheng Xu. Unveiling the potential of structure preserving for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11642\u201311651, 2021.": "Unveiling the potential of structure preserving for weakly supervised object localization", "[4] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja Fidler. Learning to act properly: Predicting and explaining affordances from images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 975\u2013983, 2018.": "Learning to act properly: Predicting and explaining affordances from images", "[9] Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han, Bolei Zhou, and Qixiang Ye. Ts-cam: Token semantic coupled attention map for weakly supervised object localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2886\u20132895, October 2021.": "Ts-cam: Token semantic coupled attention map for weakly supervised object localization", "[30] Jinjie Mai, Meng Yang, and Wenfeng Luo. Erasing integrated learning: A simple yet effective approach for weakly supervised object localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8766\u20138775, 2020.": "Erasing integrated learning: A simple yet effective approach for weakly supervised object localization", "[5] Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, and Rita Cucchiara. A deep multi-level network for saliency prediction. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 3488\u20133493. IEEE, 2016.": "A deep multi-level network for saliency prediction", "[17] Yifei Huang, Minjie Cai, Zhenqiang Li, and Yoichi Sato. Predicting gaze in egocentric video by learning task-dependent attention transition. In Proceedings of the European Conference on Computer Vision (ECCV), pages 754\u2013769, 2018.": "Predicting gaze in egocentric video by learning task-dependent attention transition", "[29] Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, and Dacheng Tao. One-shot object affordance detection. arXiv preprint arXiv:2108.03658, 2021.": "One-shot object affordance detection", "[35] Anh Nguyen, Dimitrios Kanoulas, Darwin G Caldwell, and Nikos G Tsagarakis. Object-based affordances detection with convolutional neural networks and dense conditional random fields. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5908\u20135915. IEEE, 2017.": "Object-based affordances detection with convolutional neural networks and dense conditional random fields", "[43] Johann Sawatzky, Abhilash Srikantha, and Juergen Gall. Weakly supervised affordance detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.": "Weakly supervised affordance detection", "[32] Austin Myers, Ching L Teo, Cornelia Ferm\u00fcller, and Yiannis Aloimonos. Affordance detection of tool parts from geometric features. In 2015 IEEE International Conference on Robotics and Automation (ICRA), pages 1374\u20131381. IEEE, 2015.": "Affordance detection of tool parts from geometric features", "[21] Matthias K\u00fcmmerer, Thomas SA Wallis, and Matthias Bethge. Deepgaze ii: Reading fixations from deep features trained on object recognition. arXiv preprint arXiv:1610.01563, 2016.": "Deepgaze II: Reading Fixations from Deep Features Trained on Object Recognition", "[33] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman. Grounded human-object interaction hotspots from video. In Proceedings of the IEEE International Conference on Computer Vision (CVPR), pages 8688\u20138697, 2019.": "Grounded human-object interaction hotspots from video"}, "source_title_to_arxiv_id": {"Unveiling the potential of structure preserving for weakly supervised object localization": "2103.04523", "One-shot object affordance detection": "2108.03658"}}