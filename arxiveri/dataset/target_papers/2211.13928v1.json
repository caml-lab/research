{"title": "UperFormer: A Multi-scale Transformer-based Decoder for Semantic Segmentation", "abstract": "While a large number of recent works on semantic segmentation focus on\ndesigning and incorporating a transformer-based encoder, much less attention\nand vigor have been devoted to transformer-based decoders. For such a task\nwhose hallmark quest is pixel-accurate prediction, we argue that the decoder\nstage is just as crucial as that of the encoder in achieving superior\nsegmentation performance, by disentangling and refining the high-level cues and\nworking out object boundaries with pixel-level precision. In this paper, we\npropose a novel transformer-based decoder called UperFormer, which is\nplug-and-play for hierarchical encoders and attains high quality segmentation\nresults regardless of encoder architecture. UperFormer is equipped with\ncarefully designed multi-head skip attention units and novel upsampling\noperations. Multi-head skip attention is able to fuse multi-scale features from\nbackbones with those in decoders. The upsampling operation, which incorporates\nfeature from encoder, can be more friendly for object localization. It brings a\n0.4% to 3.2% increase compared with traditional upsampling methods. By\ncombining UperFormer with Swin Transformer (Swin-T), a fully transformer-based\nsymmetric network is formed for semantic segmentation tasks. Extensive\nexperiments show that our proposed approach is highly effective and\ncomputationally efficient. On Cityscapes dataset, we achieve state-of-the-art\nperformance. On the more challenging ADE20K dataset, our best model yields a\nsingle-scale mIoU of 50.18, and a multi-scale mIoU of 51.8, which is on-par\nwith the current state-of-art model, while we drastically cut the number of\nFLOPs by 53.5%. Our source code and models are publicly available at:\nhttps://github.com/shiwt03/UperFormer", "authors": ["Jing Xu", "Wentao Shi", "Pan Gao", "Zhengwei Wang", "Qizhu Li"], "published_date": "2022_11_25", "pdf_url": "http://arxiv.org/pdf/2211.13928v1", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>Backbone</td><td>Pretrain</td><td>Optimizer</td><td> LearningRate </td><td>Dataset</td><td> BatchSize </td><td>Iterations</td><td> ImageSize </td></tr><tr><td>UperNet*</td><td>ResNet-101</td><td>ImgNet1K[9]</td><td>SGD[29]</td><td>0.01\u2020</td><td rowspan=\"4\">ADE20K</td><td rowspan=\"4\">16</td><td rowspan=\"4\">160000</td><td rowspan=\"4\">512\\times 512</td></tr><tr><td>UperFormer</td><td>ResNet-101</td><td>ImgNet1K</td><td>AdamW[27]</td><td>6\\times 10^{-5}</td></tr><tr><td>UperNet*</td><td>Swin-T-Base</td><td>ImgNet22K</td><td>AdamW</td><td>6\\times 10^{-5}\u2020</td></tr><tr><td>UperFormer</td><td>Swin-T-Base</td><td>ImgNet22K</td><td>AdamW</td><td>6\\times 10^{-5}</td></tr><tr><td>UperNet*</td><td>ResNet-101</td><td>ImgNet1K</td><td>SGD</td><td>0.01\u2020</td><td rowspan=\"3\">Cityscapes</td><td rowspan=\"3\">8</td><td rowspan=\"3\">80000</td><td rowspan=\"3\">768\\times 768</td></tr><tr><td>UperFormer</td><td>ResNet-101</td><td>ImgNet1K</td><td>AdamW</td><td>6\\times 10^{-5}</td></tr><tr><td>UperFormer</td><td>Swin-T-Base</td><td>ImgNet22K</td><td>AdamW</td><td>6\\times 10^{-5}</td></tr></table><p>*To make fair comparison with UperNet, we remove the FCN auxiliary head from these models and trained them by ourselves.\u2020The learning rates are in accordance with the configurations in their papers.</p>", "caption": "Table 1: Detailed Training Configurations", "list_citation_info": ["[29] Aatila Mustapha, Lachgar Mohamed, and Kartit Ali. An overview of gradient descent algorithm optimization in machine learning: Application in the ophthalmology field. In Communications in Computer and Information Science, volume 1207 CCIS, pages 349 \u2013 359, Marrakesh, Morocco, 2020. Comparative studies;Gradient descent algorithms;Harvard;Normal equations;On-machines;Optimization algorithms;.", "[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, United states, 2019. Classification datasets;Generalization performance;Gradient algorithm;Learning rates;Loss functions;Simple modifications;Stochastic gradient descent;Weight decay regularization;.", "[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. computer vision and pattern recognition, 2009."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">Params</td><td colspan=\"3\">ADE20K</td><td colspan=\"3\">Cityscapes</td></tr><tr><td>FLOPs**</td><td>mIoU</td><td> mIoU(ms+flip)* </td><td>FLOPs</td><td>mIoU</td><td> mIoU(ms+flip) </td></tr><tr><td>FPN[23]</td><td>ResNet-101[16]</td><td>47.51M</td><td>65.04G</td><td>39.35</td><td>40.72</td><td>145.72G</td><td>75.80</td><td>77.40</td></tr><tr><td>UperNet[37]</td><td>ResNet-101</td><td>83.11M</td><td>257.37G</td><td>40.66</td><td>40.44</td><td>576.55G</td><td>78.84</td><td>79.9</td></tr><tr><td>UperFormer(Ours)\u2020</td><td>ResNet-101</td><td>247.86M</td><td>248.3G</td><td>43.18</td><td>44.08</td><td>523.01G</td><td>79.94</td><td>-</td></tr><tr><td>FCN[26]</td><td>ResNet-101-D8</td><td>68.59M</td><td>275.69G</td><td>39.91</td><td>41.40</td><td>632.49G</td><td>75.52</td><td>76.61</td></tr><tr><td>DeepLabV3+[5]</td><td>ResNet-101-D8</td><td>62.68M</td><td>255.14G</td><td>45.47</td><td>46.35</td><td>583.18G</td><td>80.65</td><td>81.47</td></tr><tr><td>DMnet[15]</td><td>ResNet-101-D8</td><td>72.27M</td><td>273.64G</td><td>45.42</td><td>46.76</td><td>627.67G</td><td>79.19</td><td>80.65</td></tr><tr><td>PSPNet[43]</td><td>ResNet-101-D8</td><td>68.07M</td><td>256.44G</td><td>44.39</td><td>45.35</td><td>588.21G</td><td>79.77</td><td>81.06</td></tr><tr><td>PSANet[44]</td><td>ResNet-101-D8</td><td>73.06M</td><td>272.48G</td><td>43.74</td><td>45.38</td><td>637.68G</td><td>79.69</td><td>80.89</td></tr><tr><td>SETR-PUP[45]</td><td>ViT-Large</td><td>317.29M</td><td>270.55G</td><td>48.24</td><td>49.99</td><td>588.62G</td><td>79.21</td><td>81.02</td></tr><tr><td>SegFormer[39]</td><td>MiT-B5</td><td>82.01M</td><td>52.45G</td><td>49.13</td><td>50.22</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UperNet[37]</td><td>Swin-T-Base[25]</td><td>121.42M</td><td>299.81G</td><td>50.21</td><td>52.02</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UperFormer(Ours)</td><td>Swin-T-Base</td><td>138.94M</td><td>139.53G</td><td>50.18</td><td>51.8</td><td>285.85G</td><td>81.44</td><td>82.79</td></tr></table><p>*ms+flip means we use multi-scaled and flipped data when testing. On ADE20K, the 512\\times 512 images are augmented to 512\\times 512 and 1024\\times 1024. On Cityscapes, the 2048\\times 1024 images are augmented to 2048\\times 1024 and 4096\\times 2048.<br/>**On ADE20K, the FLOPs are calculated with input images of 512\\times 512. On Cityscapes, the FLOPs are calculated with input images of 768\\times 768.<br/>\u2020The maximum output channels of ResNet are 2048, while that of Swin-T are 1024. So the Params and FLOPs of UperFormer+ResNet are quite larger than UperFormer+Swin-T, but the FLOPs are still lower than most of the models.</p>", "caption": "Table 5: Comparison of segmentation results of different methods on ADE20K and Cityscapes dataset. ", "list_citation_info": ["[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv: Computer Vision and Pattern Recognition, 2015.", "[44] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In Proceedings of the European Conference on Computer Vision (ECCV), pages 267\u2013283, 2018.", "[26] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.", "[23] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. arXiv: Computer Vision and Pattern Recognition, 2016.", "[43] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.", "[39] Enze Xie, Wenhai Wang, Zhiding Yu, Animashree Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. arXiv: Computer Vision and Pattern Recognition, 2021.", "[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.", "[45] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. computer vision and pattern recognition, 2021.", "[37] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. european conference on computer vision, 2018.", "[15] Junjun He, Zhongying Deng, and Yu Qiao. Dynamic multi-scale filters for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.", "[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. international conference on computer vision, 2021."]}], "citation_info_to_title": {"[15] Junjun He, Zhongying Deng, and Yu Qiao. Dynamic multi-scale filters for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.": "Dynamic multi-scale filters for semantic segmentation", "[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv: Computer Vision and Pattern Recognition, 2015.": "Deep Residual Learning for Image Recognition", "[37] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. european conference on computer vision, 2018.": "Unified Perceptual Parsing for Scene Understanding", "[44] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In Proceedings of the European Conference on Computer Vision (ECCV), pages 267\u2013283, 2018.": "Psanet: Point-wise spatial attention network for scene parsing", "[27] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, United states, 2019. Classification datasets;Generalization performance;Gradient algorithm;Learning rates;Loss functions;Simple modifications;Stochastic gradient descent;Weight decay regularization;.": "Decoupled weight decay regularization", "[23] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. arXiv: Computer Vision and Pattern Recognition, 2016.": "Feature Pyramid Networks for Object Detection", "[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. computer vision and pattern recognition, 2009.": "Imagenet: A large-scale hierarchical image database", "[43] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.": "Pyramid Scene Parsing Network", "[29] Aatila Mustapha, Lachgar Mohamed, and Kartit Ali. An overview of gradient descent algorithm optimization in machine learning: Application in the ophthalmology field. In Communications in Computer and Information Science, volume 1207 CCIS, pages 349 \u2013 359, Marrakesh, Morocco, 2020. Comparative studies;Gradient descent algorithms;Harvard;Normal equations;On-machines;Optimization algorithms;.": "An overview of gradient descent algorithm optimization in machine learning: Application in the ophthalmology field", "[26] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.": "Fully Convolutional Networks for Semantic Segmentation", "[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. international conference on computer vision, 2021.": "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows", "[45] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. computer vision and pattern recognition, 2021.": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers", "[39] Enze Xie, Wenhai Wang, Zhiding Yu, Animashree Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. arXiv: Computer Vision and Pattern Recognition, 2021.": "Segformer: Simple and efficient design for semantic segmentation with transformers"}, "source_title_to_arxiv_id": {"Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows": "2103.14030", "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers": "2012.15840"}}