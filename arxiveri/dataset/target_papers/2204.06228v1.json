{"title": "Do You Really Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization", "abstract": "Due to its high societal impact, deepfake detection is getting active\nattention in the computer vision community. Most deepfake detection methods\nrely on identity, facial attribute and adversarial perturbation based\nspatio-temporal modifications at the whole video or random locations, while\nkeeping the meaning of the content intact. However, a sophisticated deepfake\nmay contain only a small segment of video/audio manipulation, through which the\nmeaning of the content can be, for example, completely inverted from sentiment\nperspective. To address this gap, we introduce a content driven audio-visual\ndeepfake dataset, termed as Localized Audio Visual DeepFake (LAV-DF),\nexplicitly designed for the task of learning temporal forgery localization.\nSpecifically, the content driven audio-visual manipulations are performed at\nstrategic locations in order to change the sentiment polarity of the whole\nvideo. Our baseline method for benchmarking the proposed dataset is a 3DCNN\nmodel, termed as Boundary Aware Temporal Forgery Detection (BA-TFD), which is\nguided via contrastive, boundary matching and frame classification loss\nfunctions. Our extensive quantitative analysis demonstrates the strong\nperformance of the proposed method for both task of temporal forgery\nlocalization and deepfake detection.", "authors": ["Zhixi Cai", "Kalin Stefanov", "Abhinav Dhall", "Munawar Hayat"], "published_date": "2022_04_13", "pdf_url": "http://arxiv.org/pdf/2204.06228v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>Year</th><th>Tasks</th><th>Manipulated</th><th>Method</th><th>#Subjects</th><th>#Real</th><th>#Fake</th><th>#Total</th></tr><tr><th></th><th></th><th></th><th>Modality</th><th>Manipulation</th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><th>DF-TIMIT (Korshunov and Marcel, 2018)</th><th>2018</th><td>Cla</td><td>V</td><td>FS</td><td>43</td><td>320</td><td>640</td><td>960</td></tr><tr><th>UADFV (Yang et al., 2019)</th><th>2019</th><td>Cla</td><td>V</td><td>FS</td><td>49</td><td>49</td><td>49</td><td>98</td></tr><tr><th>FaceForensics++ (Rossler et al., 2019)</th><th>2019</th><td>Cla</td><td>V</td><td>FS/RE</td><td>-</td><td>1,000</td><td>4,000</td><td>5,000</td></tr><tr><th>Google DFD (Nick and Andrew, 2019)</th><th>2019</th><td>Cla</td><td>V</td><td>FS</td><td>-</td><td>363</td><td>3,068</td><td>3,431</td></tr><tr><th>DFDC (Dolhansky et al., 2020)</th><th>2020</th><td>Cla</td><td>AV</td><td>FS</td><td>960</td><td>23,654</td><td>104,500</td><td>128,154</td></tr><tr><th>DeeperForensics (Jiang et al., 2020)</th><th>2020</th><td>Cla</td><td>V</td><td>FS</td><td>100</td><td>50,000</td><td>10,000</td><td>60,000</td></tr><tr><th>Celeb-DF (Li et al., 2020)</th><th>2020</th><td>Cla</td><td>V</td><td>FS</td><td>59</td><td>590</td><td>5,639</td><td>6,229</td></tr><tr><th>WildDeepfake (Zi et al., 2020)</th><th>2021</th><td>Cla</td><td>-</td><td>-</td><td>-</td><td>3,805</td><td>3,509</td><td>7,314</td></tr><tr><th>FakeAVCeleb (Khalid et al., 2021b)</th><th>2021</th><td>Cla</td><td>AV</td><td>RE</td><td>600+</td><td>570</td><td>25,000+</td><td>25,500+</td></tr><tr><th>ForgeryNet (He et al., 2021)</th><th>2021</th><td>SL/TFL/Cla</td><td>V</td><td>Random FS/RE</td><td>5400+</td><td>99,630</td><td>121,617</td><td>221,247</td></tr><tr><th>LAV-DF (Ours)</th><th>2022</th><td>TFL/Cla</td><td>AV</td><td>Content driven RE</td><td>153</td><td>36,431</td><td>99,873</td><td>136,304</td></tr></tbody></table>", "caption": "Table 1. Quantitative comparison of LAV-DF with previous public deepfake datasets. Cla: Classification, SL: Spatial Localization, TFL: Temporal Forgery Localization, FS: Face Swapping, and RE: ReEnactment.", "list_citation_info": ["Dolhansky et al. (2020) Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset. arXiv:2006.07397 [cs] (Oct. 2020). arXiv: 2006.07397.", "Nick and Andrew (2019) Dufou Nick and Jigsaw Andrew. 2019. Contributing Data to Deepfake Detection Research.", "Jiang et al. (2020) Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. 2020. DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2889\u20132898.", "Khalid et al. (2021b) Hasam Khalid, Shahroz Tariq, and Simon S. Woo. 2021b. FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. arXiv:2108.05080 [cs] (Aug. 2021). arXiv: 2108.05080.", "Zi et al. (2020) Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. 2020. WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 2382\u20132390. https://doi.org/10.1145/3394171.3413769", "He et al. (2021) Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. 2021. ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4360\u20134369.", "Yang et al. (2019) Xin Yang, Yuezun Li, and Siwei Lyu. 2019. Exposing Deep Fakes Using Inconsistent Head Poses. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 8261\u20138265. https://doi.org/10.1109/ICASSP.2019.8683164 ISSN: 2379-190X.", "Li et al. (2020) Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3207\u20133216.", "Rossler et al. (2019) Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1\u201311.", "Korshunov and Marcel (2018) Pavel Korshunov and Sebastien Marcel. 2018. DeepFakes: a New Threat to Face Recognition? Assessment and Detection. arXiv:1812.08685 [cs] (Dec. 2018). arXiv: 1812.08685."]}, {"table": "<table><tbody><tr><th>Model</th><td>AP@0.5</td><td>AP@0.75</td><td>AP@0.95</td><td>AR@100</td><td>AR@50</td><td>AR@20</td><td>AR@10</td></tr><tr><th>MDS (Chugh et al., 2020)</th><td>12.78</td><td>01.62</td><td>00.00</td><td>37.88</td><td>36.71</td><td>34.39</td><td>32.15</td></tr><tr><th>AGT (Nawhal and Mori, 2021)</th><td>17.85</td><td>09.42</td><td>00.11</td><td>43.15</td><td>34.23</td><td>24.59</td><td>16.71</td></tr><tr><th>BMN (Lin et al., 2019)</th><td>24.01</td><td>07.61</td><td>00.07</td><td>53.26</td><td>41.24</td><td>31.60</td><td>26.93</td></tr><tr><th>BMN (I3D)</th><td>10.56</td><td>01.66</td><td>00.00</td><td>48.49</td><td>44.39</td><td>37.13</td><td>31.55</td></tr><tr><th>AVFusion (Bagchi et al., 2021)</th><td>65.38</td><td>23.89</td><td>00.11</td><td>62.98</td><td>59.26</td><td>54.80</td><td>52.11</td></tr><tr><th>BA-TFD (visual-only)</th><td>58.55</td><td>28.60</td><td>00.16</td><td>62.49</td><td>58.77</td><td>53.86</td><td>50.29</td></tr><tr><th>BA-TFD</th><td>76.90</td><td>38.50</td><td>00.25</td><td>66.90</td><td>64.08</td><td>60.77</td><td>58.42</td></tr></tbody></table>", "caption": "Table 2. Temporal forgery localization results on the full set (please see Section 5) of LAV-DF. The BA-TFD (visual-only) is the output from video boundary matching layer in Figure 4, showing the performance when using only the video modality.", "list_citation_info": ["Bagchi et al. (2021) Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, and Ravi Kiran Sarvadevabhatla. 2021. Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. arXiv:2106.14118 [cs] (Aug. 2021). arXiv: 2106.14118 version: 3.", "Chugh et al. (2020) Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian. 2020. Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 439\u2013447. https://doi.org/10.1145/3394171.3413700", "Lin et al. (2019) Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. 2019. BMN: Boundary-Matching Network for Temporal Action Proposal Generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3889\u20133898.", "Nawhal and Mori (2021) Megha Nawhal and Greg Mori. 2021. Activity Graph Transformer for Temporal Action Localization. arXiv:2101.08540 [cs] (Jan. 2021). arXiv: 2101.08540."]}, {"table": "<table><tbody><tr><th>Model</th><td>AP@0.5</td><td>AP@0.75</td><td>AP@0.95</td><td>AR@100</td><td>AR@50</td><td>AR@20</td><td>AR@10</td></tr><tr><th>MDS (Chugh et al., 2020)</th><td>23.43</td><td>03.48</td><td>00.00</td><td>58.53</td><td>56.68</td><td>53.16</td><td>49.67</td></tr><tr><th>AGT (Nawhal and Mori, 2021)</th><td>15.69</td><td>10.69</td><td>00.15</td><td>49.11</td><td>40.31</td><td>31.70</td><td>23.13</td></tr><tr><th>BMN (Lin et al., 2019)</th><td>32.32</td><td>11.38</td><td>00.14</td><td>59.69</td><td>48.17</td><td>39.01</td><td>34.17</td></tr><tr><th>BMN (I3D)</th><td>28.10</td><td>05.47</td><td>00.01</td><td>55.49</td><td>54.44</td><td>52.14</td><td>47.72</td></tr><tr><th>AVFusion (Bagchi et al., 2021)</th><td>62.01</td><td>22.77</td><td>00.11</td><td>61.98</td><td>58.08</td><td>53.31</td><td>50.52</td></tr><tr><th>BA-TFD (visual-only)</th><td>83.55</td><td>41.88</td><td>00.24</td><td>65.79</td><td>62.30</td><td>57.95</td><td>55.34</td></tr><tr><th>BA-TFD</th><td>85.20</td><td>47.06</td><td>00.29</td><td>67.34</td><td>64.52</td><td>61.19</td><td>59.32</td></tr></tbody></table>", "caption": "Table 3. Temporal forgery localization results on the subset (please see Section 5) of LAV-DF. The BA-TFD (visual-only) is the output from video boundary matching layer in Figure 4, showing the performance when using only the video modality.", "list_citation_info": ["Bagchi et al. (2021) Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, and Ravi Kiran Sarvadevabhatla. 2021. Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. arXiv:2106.14118 [cs] (Aug. 2021). arXiv: 2106.14118 version: 3.", "Chugh et al. (2020) Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian. 2020. Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 439\u2013447. https://doi.org/10.1145/3394171.3413700", "Lin et al. (2019) Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. 2019. BMN: Boundary-Matching Network for Temporal Action Proposal Generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3889\u20133898.", "Nawhal and Mori (2021) Megha Nawhal and Greg Mori. 2021. Activity Graph Transformer for Temporal Action Localization. arXiv:2101.08540 [cs] (Jan. 2021). arXiv: 2101.08540."]}, {"table": "<p>. DatasetLAV-DFModelMDS (Chugh et al., 2020)EfficientViT (Coccomini et al., 2021)BA-TFDAUC0.8280.9650.990</p>", "caption": "Table 5. Classification results on the full set of LAV-DF.", "list_citation_info": ["Chugh et al. (2020) Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian. 2020. Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 439\u2013447. https://doi.org/10.1145/3394171.3413700", "Coccomini et al. (2021) Davide Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio Falchi. 2021. Combining EfficientNet and Vision Transformers for Video Deepfake Detection. arXiv:2107.02612 [cs] (July 2021). arXiv: 2107.02612 version: 1."]}, {"table": "<table><thead><tr><th>Dataset</th><th colspan=\"5\">DFDC</th></tr></thead><tbody><tr><th>Model</th><td>Meso4(Afchar et al., 2018)</td><td>FWA(Li and Lyu, 2019)</td><td>Siamese(Mittal et al., 2020)</td><td>MDS (Chugh et al., 2020)</td><td>BA-TFD</td></tr><tr><th>AUC</th><td>0.753</td><td>0.727</td><td>0.844</td><td>0.916</td><td>0.846</td></tr></tbody></table>", "caption": "Table 6. Classification results on a subset set of DFDC.", "list_citation_info": ["Afchar et al. (2018) Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: a Compact Facial Video Forgery Detection Network. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS). 1\u20137. https://doi.org/10.1109/WIFS.2018.8630761 ISSN: 2157-4774.", "Li and Lyu (2019) Yuezun Li and Siwei Lyu. 2019. Exposing DeepFake Videos By Detecting Face Warping Artifacts. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 7.", "Chugh et al. (2020) Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian. 2020. Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 439\u2013447. https://doi.org/10.1145/3394171.3413700", "Mittal et al. (2020) Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. 2020. Emotions Don\u2019t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 2823\u20132832. https://doi.org/10.1145/3394171.3413570"]}], "citation_info_to_title": {"Mittal et al. (2020) Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. 2020. Emotions Don\u2019t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 2823\u20132832. https://doi.org/10.1145/3394171.3413570": "Emotions Don\u2019t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues", "Lin et al. (2019) Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. 2019. BMN: Boundary-Matching Network for Temporal Action Proposal Generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 3889\u20133898.": "BMN: Boundary-Matching Network for Temporal Action Proposal Generation", "Coccomini et al. (2021) Davide Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio Falchi. 2021. Combining EfficientNet and Vision Transformers for Video Deepfake Detection. arXiv:2107.02612 [cs] (July 2021). arXiv: 2107.02612 version: 1.": "Combining EfficientNet and Vision Transformers for Video Deepfake Detection", "Afchar et al. (2018) Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: a Compact Facial Video Forgery Detection Network. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS). 1\u20137. https://doi.org/10.1109/WIFS.2018.8630761 ISSN: 2157-4774.": "MesoNet: a Compact Facial Video Forgery Detection Network", "Li and Lyu (2019) Yuezun Li and Siwei Lyu. 2019. Exposing DeepFake Videos By Detecting Face Warping Artifacts. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). 7.": "Exposing DeepFake Videos By Detecting Face Warping Artifacts", "Jiang et al. (2020) Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. 2020. DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2889\u20132898.": "DeeperForensics-10: A Large-Scale Dataset for Real-World Face Forgery Detection", "Yang et al. (2019) Xin Yang, Yuezun Li, and Siwei Lyu. 2019. Exposing Deep Fakes Using Inconsistent Head Poses. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 8261\u20138265. https://doi.org/10.1109/ICASSP.2019.8683164 ISSN: 2379-190X.": "Exposing Deep Fakes Using Inconsistent Head Poses", "He et al. (2021) Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. 2021. ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4360\u20134369.": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis", "Bagchi et al. (2021) Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, and Ravi Kiran Sarvadevabhatla. 2021. Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. arXiv:2106.14118 [cs] (Aug. 2021). arXiv: 2106.14118 version: 3.": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization", "Dolhansky et al. (2020) Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset. arXiv:2006.07397 [cs] (Oct. 2020). arXiv: 2006.07397.": "The DeepFake Detection Challenge (DFDC) Dataset", "Korshunov and Marcel (2018) Pavel Korshunov and Sebastien Marcel. 2018. DeepFakes: a New Threat to Face Recognition? Assessment and Detection. arXiv:1812.08685 [cs] (Dec. 2018). arXiv: 1812.08685.": "DeepFakes: a New Threat to Face Recognition? Assessment and Detection", "Khalid et al. (2021b) Hasam Khalid, Shahroz Tariq, and Simon S. Woo. 2021b. FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. arXiv:2108.05080 [cs] (Aug. 2021). arXiv: 2108.05080.": "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset", "Rossler et al. (2019) Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1\u201311.": "FaceForensics++: Learning to Detect Manipulated Facial Images", "Nawhal and Mori (2021) Megha Nawhal and Greg Mori. 2021. Activity Graph Transformer for Temporal Action Localization. arXiv:2101.08540 [cs] (Jan. 2021). arXiv: 2101.08540.": "Activity Graph Transformer for Temporal Action Localization", "Nick and Andrew (2019) Dufou Nick and Jigsaw Andrew. 2019. Contributing Data to Deepfake Detection Research.": "Contributing Data to Deepfake Detection Research", "Li et al. (2020) Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3207\u20133216.": "Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics", "Zi et al. (2020) Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. 2020. WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 2382\u20132390. https://doi.org/10.1145/3394171.3413769": "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection", "Chugh et al. (2020) Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian. 2020. Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization. In Proceedings of the 28th ACM International Conference on Multimedia (MM \u201920). Association for Computing Machinery, New York, NY, USA, 439\u2013447. https://doi.org/10.1145/3394171.3413700": "Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization"}, "source_title_to_arxiv_id": {"Activity Graph Transformer for Temporal Action Localization": "2101.08540"}}