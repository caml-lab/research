{"title": "Retrieval-Augmented Multimodal Language Modeling", "abstract": "Recent multimodal models such as DALL-E and CM3 have achieved remarkable\nprogress in text-to-image and image-to-text generation. However, these models\nstore all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\nmodel parameters, requiring increasingly larger models and training data to\ncapture more knowledge. To integrate knowledge in a more scalable and modular\nway, we propose a retrieval-augmented multimodal model, which enables a base\nmultimodal model (generator) to refer to relevant knowledge fetched by a\nretriever from external memory (e.g., multimodal documents on the web).\nSpecifically, we implement a retriever using the pretrained CLIP model and a\ngenerator using the CM3 Transformer architecture, and train this model using\nthe LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3),\nis the first multimodal model that can retrieve and generate mixtures of text\nand images. We show that RA-CM3 significantly outperforms baseline multimodal\nmodels such as DALL-E and CM3 on both image and caption generation tasks (12\nFID and 17 CIDEr improvements on MS-COCO), while requiring much less compute\nfor training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel\ncapabilities such as knowledge-intensive image generation and multimodal\nin-context learning.", "authors": ["Michihiro Yasunaga", "Armen Aghajanyan", "Weijia Shi", "Rich James", "Jure Leskovec", "Percy Liang", "Mike Lewis", "Luke Zettlemoyer", "Wen-tau Yih"], "published_date": "2022_11_22", "pdf_url": "http://arxiv.org/pdf/2211.12561v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Approach</th><th>Model type</th><th>Image generation</th><th>Text generation</th><th>Retrieval</th><th>In-context learning</th></tr></thead><tbody><tr><th>DALL-E, Parti <p>(Ramesh et al.; Yu et al.)</p></th><td>Autoregressive</td><td>\u2714</td><td></td><td></td><td></td></tr><tr><th>DALL-E 2, Imagen <p>(Ramesh et al.; Saharia et al.)</p></th><td>Diffusion</td><td>\u2714</td><td></td><td></td><td></td></tr><tr><th>Re-Imagen <p>(Chen et al.)</p></th><td>Diffusion</td><td>\u2714</td><td></td><td>\u2714</td><td></td></tr><tr><th>Flamingo, MetaLM <p>(Alayrac et al.; Hao et al.)</p></th><td>Autoregressive</td><td></td><td>\u2714</td><td></td><td>\u2714</td></tr><tr><th>MuRAG <p>(Chen et al.)</p></th><td>Autoregressive</td><td></td><td> \u2009\u2714{}^{\\dagger}</td><td>\u2714</td><td></td></tr><tr><th>CM3 <p>(Aghajanyan et al.)</p></th><td>Autoregressive</td><td>\u2714</td><td>\u2714</td><td></td><td></td></tr><tr><th>RA-CM3 (Ours)</th><td>Autoregressive</td><td>\u2714</td><td>\u2714</td><td>\u2714</td><td>\u2714</td></tr></tbody></table>", "caption": "Table 1: Comparison with other multimodal models. Our RA-CM3 is the first retrieval-augmented model that can perform both image and text generation. RA-CM3 also exhibits strong in-context learning abilities thanks to the proposed retrieval-augmented training (\u00a73.3). {}^{\\dagger}Focus on question answering.", "list_citation_info": ["Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.", "Chen et al. (2022a) Chen, W., Hu, H., Chen, X., Verga, P., and Cohen, W. W. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. In Empirical Methods in Natural Language Processing (EMNLP), 2022a.", "Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.", "Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021.", "Aghajanyan et al. (2022) Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., and Zettlemoyer, L. CM3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.", "Chen et al. (2022b) Chen, W., Hu, H., Saharia, C., and Cohen, W. W. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022b."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Approach</th><td rowspan=\"2\">Model type</td><td colspan=\"2\">MS-COCO FID (\\downarrow)</td></tr><tr><td>Not finetuned</td><td>Finetuned</td></tr><tr><th>Retrieval Baseline</th><td>-</td><td>17.97</td><td>-</td></tr><tr><th>KNN-Diffusion <p>(Ashual et al., 2022)</p></th><td>Diffusion</td><td>16.66</td><td>-</td></tr><tr><th>Stable Diffusion <p>(Rombach et al., 2022)</p></th><td>Diffusion</td><td>12.63</td><td>-</td></tr><tr><th>GLIDE <p>(Nichol et al., 2021)</p></th><td>Diffusion</td><td>12.24</td><td>-</td></tr><tr><th>DALL-E 2 <p>(Ramesh et al., 2022)</p></th><td>Diffusion</td><td>10.39</td><td>-</td></tr><tr><th>Imagen <p>(Saharia et al., 2022)</p></th><td>Diffusion</td><td>7.27</td><td>-</td></tr><tr><th>Re-Imagen <p>(Chen et al., 2022b)</p></th><td>Diffusion</td><td>6.88</td><td>5.25</td></tr><tr><th>DALL-E (12B) <p>(Ramesh et al., 2021)</p></th><td><p>Autoregressive</p></td><td>\\sim28</td><td>-</td></tr><tr><th>CogView (4B) <p>(Ding et al., 2021)</p></th><td><p>Autoregressive</p></td><td>27.1</td><td>-</td></tr><tr><th>CogView2 (6B) <p>(Ding et al., 2022)</p></th><td><p>Autoregressive</p></td><td>24.0</td><td>17.7</td></tr><tr><th>Parti (20B) <p>(Yu et al., 2022)</p></th><td><p>Autoregressive</p></td><td>7.23</td><td>3.22</td></tr><tr><th>Vanilla CM3</th><td><p>Autoregressive</p></td><td>29.5</td><td>-</td></tr><tr><th>RA-CM3 (2.7B) (Ours)</th><td><p>Autoregressive</p></td><td>15.7</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Caption-to-image generation performance on MS-COCO.Our retrieval-augmented CM3 significantly outperforms the baseline CM3 with no retrieval, as well as other models such as DALL-E (12B parameters). Moreover, our model achieves strong performance with much less training compute than existing models; see Figure 2 for details.", "list_citation_info": ["Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.", "Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.", "Nichol et al. (2021) Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.", "Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.", "Ashual et al. (2022) Ashual, O., Sheynin, S., Polyak, A., Singer, U., Gafni, O., Nachmani, E., and Taigman, Y. Knn-diffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022.", "Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021.", "Ding et al. (2022) Ding, M., Zheng, W., Hong, W., and Tang, J. Cogview2: Faster and better text-to-image generation via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.", "Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.", "Chen et al. (2022b) Chen, W., Hu, H., Saharia, C., and Cohen, W. W. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022b.", "Ding et al. (2021) Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview: Mastering text-to-image generation via transformers. In Neural Information Processing Systems (NeurIPS), 2021."]}, {"table": "<table><tbody><tr><td>Approach</td><td>CIDEr (\\uparrow)</td></tr><tr><td>Retrieval Baseline</td><td>84.1</td></tr><tr><td>Ground Truth (upper bound)</td><td>108.3</td></tr><tr><td>DALL-E<sup>Small</sup>\u2009<sup>5</sup><sup>5</sup>5https://github.com/lucidrains/DALLE-pytorch</td><td>20.2</td></tr><tr><td>ruDALL-E-XL<sup>6</sup><sup>6</sup>6https://rudalle.ru</td><td>38.7</td></tr><tr><td>minDALL-E <p>(Kim et al., 2021)</p></td><td>48.0</td></tr><tr><td>X-LXMERT <p>(Cho et al., 2020)</p></td><td>55.8</td></tr><tr><td>Parti <p>(Yu et al., 2022)</p></td><td>83.9</td></tr><tr><td>Flamingo (3B; 4-shot) <p>(Alayrac et al., 2022)</p></td><td>85</td></tr><tr><td>Flamingo (80B; 4-shot) <p>(Alayrac et al., 2022)</p></td><td>103</td></tr><tr><td>Vanilla CM3</td><td>71.9</td></tr><tr><td>RA-CM3 (2.7B) (Ours)</td><td>89.1</td></tr></tbody></table>", "caption": "Table 3: Image-to-caption generation performance on MS-COCO (with no finetuning).Our retrieval-augmented CM3 significantly outperforms the baseline CM3 with no retrieval. Moreover, our model outperforms other strong models such as Parti (20B parameters) and Flamingo (3B; 4-shot), despite using just \\sim3B parameters and 2-shot in-context examples.", "list_citation_info": ["Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.", "Cho et al. (2020) Cho, J., Lu, J., Schwenk, D., Hajishirzi, H., and Kembhavi, A. X-lxmert: Paint, caption and answer questions with multi-modal transformers. arXiv preprint arXiv:2009.11278, 2020.", "Kim et al. (2021) Kim, S., Cho, S., Kim, C., Lee, D., and Baek, W. mindall-e on conceptual captions. https://github.com/kakaobrain/minDALL-E, 2021.", "Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022."]}], "citation_info_to_title": {"Cho et al. (2020) Cho, J., Lu, J., Schwenk, D., Hajishirzi, H., and Kembhavi, A. X-lxmert: Paint, caption and answer questions with multi-modal transformers. arXiv preprint arXiv:2009.11278, 2020.": "X-lxmert: Paint, caption and answer questions with multi-modal transformers", "Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.": "Hierarchical text-conditional image generation with clip latents", "Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.": "High-resolution image synthesis with latent diffusion models", "Aghajanyan et al. (2022) Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko, D., Joshi, M., Ghosh, G., Lewis, M., and Zettlemoyer, L. CM3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.": "CM3: A causal masked multimodal model of the internet", "Kim et al. (2021) Kim, S., Cho, S., Kim, C., Lee, D., and Baek, W. mindall-e on conceptual captions. https://github.com/kakaobrain/minDALL-E, 2021.": "mindall-e on conceptual captions", "Chen et al. (2022b) Chen, W., Hu, H., Saharia, C., and Cohen, W. W. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022b.": "Re-imagen: Retrieval-augmented text-to-image generator", "Ding et al. (2021) Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview: Mastering text-to-image generation via transformers. In Neural Information Processing Systems (NeurIPS), 2021.": "Cogview: Mastering Text-to-Image Generation via Transformers", "Yu et al. (2022) Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.": "Scaling autoregressive models for content-rich text-to-image generation", "Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021.": "Zero-shot text-to-image generation", "Ding et al. (2022) Ding, M., Zheng, W., Hong, W., and Tang, J. Cogview2: Faster and better text-to-image generation via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.": "Cogview2: Faster and better text-to-image generation via hierarchical transformers", "Nichol et al. (2021) Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models", "Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.": "Photorealistic text-to-image diffusion models with deep language understanding", "Ashual et al. (2022) Ashual, O., Sheynin, S., Polyak, A., Singer, U., Gafni, O., Nachmani, E., and Taigman, Y. Knn-diffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022.": "Knn-diffusion: Image generation via large-scale retrieval", "Alayrac et al. (2022) Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.": "Flamingo: a visual language model for few-shot learning", "Chen et al. (2022a) Chen, W., Hu, H., Chen, X., Verga, P., and Cohen, W. W. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. In Empirical Methods in Natural Language Processing (EMNLP), 2022a.": "Murag: Multimodal retrieval-augmented generator for open question answering over images and text"}, "source_title_to_arxiv_id": {"Hierarchical text-conditional image generation with clip latents": "2204.06125"}}