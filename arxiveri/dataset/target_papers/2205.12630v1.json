{"title": "Multimodal Knowledge Alignment with Reinforcement Learning", "abstract": "Large language models readily adapt to novel settings, even without\ntask-specific training data. Can their zero-shot capacity be extended to\nmultimodal inputs? In this work, we propose ESPER which extends language-only\nzero-shot models to unseen multimodal tasks, like image and audio captioning.\nOur key novelty is to use reinforcement learning to align multimodal inputs to\nlanguage model generations without direct supervision: for example, in the\nimage case our reward optimization relies only on cosine similarity derived\nfrom CLIP, and thus requires no additional explicitly paired (image, caption)\ndata. Because the parameters of the language model are left unchanged, the\nmodel maintains its capacity for zero-shot generalization. Experiments\ndemonstrate that ESPER outperforms baselines and prior work on a variety of\nzero-shot tasks; these include a new benchmark we collect+release, ESP dataset,\nwhich tasks models with generating several diversely-styled captions for each\nimage.", "authors": ["Youngjae Yu", "Jiwan Chung", "Heeseung Yun", "Jack Hessel", "JaeSung Park", "Ximing Lu", "Prithviraj Ammanabrolu", "Rowan Zellers", "Ronan Le Bras", "Gunhee Kim", "Yejin Choi"], "published_date": "2022_05_25", "pdf_url": "http://arxiv.org/pdf/2205.12630v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><th>Style</th><td>B@4</td><td>M</td><td>C</td><td>Time (sec/image)</td></tr><tr><th>Pseudo-Align Laina et al. (2019)</th><th>\\checkmark</th><td>5.2</td><td>15.5</td><td>29.4</td><td>-</td></tr><tr><th>RSA Honda et al. (2021)</th><th>\\checkmark</th><td>7.6</td><td>13.5</td><td>31.8</td><td>-</td></tr><tr><th>Unpaired Laina et al. (2019)</th><th>\\checkmark</th><td>19.3</td><td>20.1</td><td>63.6</td><td>-</td></tr><tr><th>CLIP-Infer Tewel et al. (2021)</th><th></th><td>2.6</td><td>11.5</td><td>14.6</td><td>65s</td></tr><tr><th>CLIP-Infer-Style</th><th>\\checkmark</th><td>7.0</td><td>15.4</td><td>34.5</td><td>65s</td></tr><tr><th>CLIP-Retrieval</th><th>\\checkmark</th><td>4.8</td><td>11.2</td><td>13.4</td><td>0.37s</td></tr><tr><th><img/>\u2009ESPER-Free (GPT-2)</th><th></th><td>6.3</td><td>13.3</td><td>29.1</td><td>0.65s</td></tr><tr><th><img/>\u2009ESPER-Style (GPT-2)</th><th>\\checkmark</th><td>21.9</td><td>21.9</td><td>78.2</td><td>0.65s</td></tr></tbody></table>", "caption": "Table 1: Unpaired captioning experiments in COCO test split. B@4 denotes Bleu-4, M METEOR and C CIDEr score. Running time entails the whole time for each process needed to infer caption for an image, including image loading and feature extraction. We use greedy decoding for all results in this table.", "list_citation_info": ["Laina et al. (2019) Iro Laina, Christian Rupprecht, and Nassir Navab. 2019. Towards Unsupervised Image Captioning with Shared Multimodal Embeddings. In ICCV.", "Tewel et al. (2021) Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. 2021. Zero-shot image-to-text generation for visual-semantic arithmetic. In CVPR.", "Honda et al. (2021) Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, and Yuji Matsumoto. 2021. Removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning. In EACL."]}, {"table": "<table><thead><tr><th colspan=\"5\">News</th></tr></thead><tbody><tr><th><p>Model</p></th><th>Zero-shot</th><td>B@4</td><td>M</td><td>C</td></tr><tr><th><p>Show Attend Tell</p></th><th></th><td>0.7</td><td>4.1</td><td>12.2</td></tr><tr><th><p>Text-Only</p></th><th>\\checkmark</th><td>0.2</td><td>2.7</td><td>1.3</td></tr><tr><th><p><img/>\u2009ESPER-Style</p></th><th>\\checkmark</th><td>0.8</td><td>4.4</td><td>4.6</td></tr><tr><th><p><img/>\u2009ESPER-MLP</p></th><th></th><td>1.3</td><td>4.8</td><td>15.7</td></tr><tr><th colspan=\"5\">Dialog</th></tr><tr><th><p>Model</p></th><th>Zero-shot</th><td>NDCG</td><td>MRR</td><td>R@1</td></tr><tr><th><p>ViLBERT</p></th><th>\\checkmark</th><td>11.6</td><td>6.9</td><td>2.6</td></tr><tr><th><p>ViLBERT-Head</p></th><th></th><td>19.7</td><td>9.8</td><td>3.4</td></tr><tr><th><p>Text-Only</p></th><th>\\checkmark</th><td>19.3</td><td>18.3</td><td>5.7</td></tr><tr><th><p><img/>\u2009ESPER-Style</p></th><th>\\checkmark</th><td>22.3</td><td>25.7</td><td>14.6</td></tr></tbody></table>", "caption": "Table 4: Downstream task evaluation in (VisualNews Liu et al. (2021a) test split and VisDial Das et al. (2017) validation split. NDCG denotes Normalized Discounted Cumulative Gain, MRR Mean Reciprocal Rank and R@1 Recall at top 1.All our results on VisDial are evaluated with the official server.", "list_citation_info": ["Das et al. (2017) Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 326\u2013335.", "Liu et al. (2021a) Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. 2021a. Visual news: Benchmark and challenges in news image captioning. In EMNLP."]}], "citation_info_to_title": {"Das et al. (2017) Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 326\u2013335.": "Visual Dialog", "Tewel et al. (2021) Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. 2021. Zero-shot image-to-text generation for visual-semantic arithmetic. In CVPR.": "Zero-shot image-to-text generation for visual-semantic arithmetic", "Honda et al. (2021) Ukyo Honda, Yoshitaka Ushiku, Atsushi Hashimoto, Taro Watanabe, and Yuji Matsumoto. 2021. Removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning. In EACL.": "Removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning", "Laina et al. (2019) Iro Laina, Christian Rupprecht, and Nassir Navab. 2019. Towards Unsupervised Image Captioning with Shared Multimodal Embeddings. In ICCV.": "Towards Unsupervised Image Captioning with Shared Multimodal Embeddings", "Liu et al. (2021a) Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. 2021a. Visual news: Benchmark and challenges in news image captioning. In EMNLP.": "Visual News: Benchmark and Challenges in News Image Captioning"}, "source_title_to_arxiv_id": {"Zero-shot image-to-text generation for visual-semantic arithmetic": "2111.14447", "Removing word-level spurious alignment between images and pseudo-captions in unsupervised image captioning": "2104.13872"}}