{"title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model", "abstract": "Effective scaling and a flexible task interface enable large language models\nto excel at many tasks. PaLI (Pathways Language and Image model) extends this\napproach to the joint modeling of language and vision. PaLI generates text\nbased on visual and textual inputs, and with this interface performs many\nvision, language, and multimodal tasks, in many languages. To train PaLI, we\nmake use of large pretrained encoder-decoder language models and Vision\nTransformers (ViTs). This allows us to capitalize on their existing\ncapabilities and leverage the substantial cost of training them. We find that\njoint scaling of the vision and language components is important. Since\nexisting Transformers for language are much larger than their vision\ncounterparts, we train the largest ViT to date (ViT-e) to quantify the benefits\nfrom even larger-capacity vision models. To train PaLI, we create a large\nmultilingual mix of pretraining tasks, based on a new image-text training set\ncontaining 10B images and texts in over 100 languages. PaLI achieves\nstate-of-the-art in multiple vision and language tasks (such as captioning,\nvisual question-answering, scene-text understanding), while retaining a simple,\nmodular, and scalable design.", "authors": ["Xi Chen", "Xiao Wang", "Soravit Changpinyo", "AJ Piergiovanni", "Piotr Padlewski", "Daniel Salz", "Sebastian Goodman", "Adam Grycner", "Basil Mustafa", "Lucas Beyer", "Alexander Kolesnikov", "Joan Puigcerver", "Nan Ding", "Keran Rong", "Hassan Akbari", "Gaurav Mishra", "Linting Xue", "Ashish Thapliyal", "James Bradbury", "Weicheng Kuo", "Mojtaba Seyedhosseini", "Chao Jia", "Burcu Karagol Ayan", "Carlos Riquelme", "Andreas Steiner", "Anelia Angelova", "Xiaohua Zhai", "Neil Houlsby", "Radu Soricut"], "published_date": "2022_09_14", "pdf_url": "http://arxiv.org/pdf/2209.06794v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><th colspan=\"3\">Image Captioning</th><th colspan=\"4\">Visual Question Answering</th></tr><tr><td></td><th><p>COCO</p></th><th><p>NoCaps{}^{*}</p></th><th><p>TextCaps{}^{*}</p></th><th><p>VQAv2{}^{*}</p></th><th><p>TextVQA{}^{*}</p></th><th><p>VizWiz-QA{}^{*}</p></th><th><p>OKVQA</p></th></tr><tr><td><p>GIT2</p></td><td><p>145.0</p></td><td>124.8</td><td><p>145.0</p></td><td><p>81.9</p></td><td><p>67.3</p></td><td><p>70.1</p></td><td><p>-</p></td></tr><tr><td><p>Flamingo</p></td><td><p>138.1</p></td><td><p>-</p></td><td><p>-</p></td><td><p>82.1</p></td><td><p>54.1</p></td><td><p>65.4</p></td><td><p>57.8</p></td></tr><tr><td><p>BEiT-3</p></td><td><p>147.6</p></td><td><p>-</p></td><td><p>-</p></td><td><p>\\textcolorgray84.0</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>CoCa</p></td><td><p>143.6</p></td><td><p>120.6</p></td><td><p>-</p></td><td><p>\\textcolorgray82.3</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>\\NAME(Ours)</p></td><td>149.1</td><td><p>124.4</p></td><td>160.4</td><td>84.3</td><td>73.1</td><td>73.3</td><td>64.5</td></tr></tbody></table>", "caption": "Table 1: \\NAMEmodel results on image-language tasks. Test set results are reported where possible. COCO result is on the Karpathy-test Karpathy and Fei-Fei (2015) Benchmarks labeled with \u201c*\u201d are evaluated on public server. VQA tasks are evaluated in the open-vocabulary generation setting, which is more challenging than the closed-vocabulary classification setting (numbers shown in gray). See Section 4 for all results. CIDEr scores Vedantam et al. (2015) are reported for the image captioning tasks and VQA accuracy Antol et al. (2015) for the VQA tasks.", "list_citation_info": ["Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575.", "Karpathy and Fei-Fei (2015) Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128\u20133137.", "Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433."]}, {"table": "<table><thead><tr><th>Model</th><th>en</th><th>fr</th><th>hi</th><th>iw</th><th>ro</th><th>th</th><th>zh</th><th>35-lang avg.</th></tr><tr><th>Thapliyal et al. (2022)</th><th>57.6</th><th>40.9</th><th>20.6</th><th>16.1</th><th>13.9</th><th>35.5</th><th>19.8</th><th>28.9</th></tr></thead><tbody><tr><th>\\NAME-3B</th><td>92.8</td><td>68.6</td><td>30.3</td><td>39.2</td><td>30.3</td><td>65.9</td><td>32.2</td><td>47.0</td></tr><tr><th>\\NAME-17B</th><td>98.1</td><td>75.5</td><td>31.3</td><td>46.8</td><td>35.8</td><td>72.1</td><td>36.5</td><td>53.4</td></tr></tbody></table>", "caption": "Table 5: CIDEr scores on image captioning for the Crossmodal-3600 benchmark, covering seven diverse languages (English, French, Hindi, Hebrew, Romanian, Thai, and Chinese), as well as the average of the 35 languages covered by the benchmark.", "list_citation_info": ["Thapliyal et al. (2022) Ashish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"2\">VQAv2</td><td>OKVQA</td><td colspan=\"2\">TextVQA</td><td colspan=\"2\">VizWiz-QA</td></tr><tr><th>Method</th><td>test-dev</td><td>test-std</td><td>val</td><td>val</td><td>test</td><td>test-dev</td><td>test</td></tr><tr><th>SimVLM</th><td>\\textcolorgray80.03</td><td>\\textcolorgray80.34</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CoCa</th><td>\\textcolorgray82.3</td><td>\\textcolorgray82.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>GIT</th><td>78.56</td><td>78.81</td><td>-</td><td>59.93</td><td>59.75</td><td>68.0</td><td>67.5</td></tr><tr><th>GIT2</th><td>81.74</td><td>81.92</td><td>-</td><td>68.38</td><td>67.27</td><td>70.97</td><td>70.1</td></tr><tr><th>OFA</th><td>\\textcolorgray82.0</td><td>\\textcolorgray82.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Flamingo</th><td>82.0</td><td>82.1</td><td>57.8{}^{*}</td><td>57.1</td><td>54.1</td><td>65.7</td><td>65.4</td></tr><tr><th>BEiT-3</th><td>\\textcolorgray84.2</td><td>\\textcolorgray84.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>KAT</th><td>-</td><td>-</td><td>54.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Mia</th><td>-</td><td>-</td><td>-</td><td>-</td><td>\\textcolorgray73.67{}^{\\dagger}</td><td>-</td><td>-</td></tr><tr><th>\\NAME-3B</th><td>79.3</td><td>-</td><td>52.4</td><td>58.75</td><td>-</td><td>66.4</td><td>-</td></tr><tr><th>\\NAME-15B</th><td>80.8</td><td>-</td><td>56.5</td><td>64.12</td><td>-</td><td>70.0</td><td>-</td></tr><tr><th>\\NAME-17B</th><td>84.3</td><td>84.3</td><td>64.5</td><td>71.81</td><td>73.06</td><td>74.4</td><td>73.3</td></tr></tbody></table>", "caption": "Table 6: VQA Accuracy results on VQAv2, OKVQA, TextVQA, and VizWiz-QA.\\NAMEmodels are evaluated in the open-vocabulary generation setting, and still outperform previous models that use closed-vocabulary classification evaluations (SimVLM, CoCa, BEiT3, OFA Wang et al. (2022b)).The result on OKVQA by Flamingo (with \u201c*\u201d) is obtained in a 32-shot learning setup. Mia Qiao et al. (2021) (with \u201c\\dagger\u201d) is the winning model of TextVQA Challenge 2021, based on fine-tuning T5-XL Raffel et al. (2020). Numbers shown in gray are from models using closed-vocabulary classification.", "list_citation_info": ["Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.", "Wang et al. (2022b) Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022b. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052.", "Qiao et al. (2021) Yixuan Qiao, Hao Chen, Jun Wang, Yihao Chen, Xianbin Ye, Ziliang Li, Xianbiao Qi, Peng Gao, and Guotong Xie. 2021. Winner team Mia at TextVQA challenge 2021: Vision-and-language representation learning with pre-trained sequence-to-sequence model. arXiv preprint arXiv:2106.15332."]}, {"table": "<table><thead><tr><th>Model</th><th>SuperGLUE</th><th>XNLI</th><th>XQuAD</th><th>TyDiQA-GoldP</th></tr><tr><th>Method</th><th>FT</th><th>ZS</th><th>ZS</th><th>ZS</th></tr><tr><th>Metric</th><th>Avg. Score</th><th>Accuracy</th><th>F1/EM</th><th>F1/EM</th></tr></thead><tbody><tr><th>mT5-\\mathrm{XXL} Xue et al. (2021)</th><td>89.2</td><td>85.0</td><td>82.5 / 66.8</td><td>80.8 / 65.9</td></tr><tr><th>mT5-\\mathrm{XXL} (our setting)</th><td>89.3</td><td>84.5</td><td>82.6 / 66.6</td><td>81.6 / 66.3</td></tr><tr><th>\\NAME-17B</th><td>88.2</td><td>84.9</td><td>81.8 / 66.0</td><td>81.2 / 66.5</td></tr></tbody></table>", "caption": "Table 8: Results on SuperGLUE and three XTREME tasks. The first row is the result reported by mT5 Xue et al. (2021) and ByT5 Xue et al. (2022) paper.The second row is our repetition using the publicly available mT5-XXL checkpoint, which is also the starting point for \\NAME-17B.The third row results are using the trained \\NAME-17B model.", "list_citation_info": ["Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498.", "Xue et al. (2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291\u2013306."]}, {"table": "<table><thead><tr><th>Model (ImageNet data)</th><th>INet</th><th>INet-R</th><th>INet-A</th><th>INet-Sketch</th><th>INet-v2</th><th>ObjNet</th></tr></thead><tbody><tr><td>GIT2 (full dataset)</td><td>89.22</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Flamingo-80B (1-shot)</td><td>71.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Flamingo-80B (5-shot)</td><td>77.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>\\NAME-3B (0-shot)</td><td>70.06</td><td>80.15</td><td>37.92</td><td>61.11</td><td>62.55</td><td>38.87</td></tr><tr><td>\\NAME-15B (0-shot)</td><td>70.27</td><td>81.21</td><td>41.16</td><td>61.03</td><td>62.81</td><td>39.51</td></tr><tr><td>\\NAME-17B (0-shot)</td><td>72.11</td><td>81.97</td><td>44.70</td><td>63.83</td><td>64.46</td><td>42.62</td></tr></tbody></table>", "caption": "Table 9: Top 1 accuracy results of 0-shot image classification on ImageNet Deng et al. (2009), ImageNet-R Hendrycks et al. (2021a), ImageNet-A Hendrycks et al. (2021b), ImageNet-Sketch Wang et al. (2019b), Imagenet-v2 Recht et al. (2019) and ObjectNet Barbu et al. (2019).", "list_citation_info": ["Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021b. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271.", "Wang et al. (2019b) Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. 2019b. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506\u201310518.", "Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021a. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349.", "Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.", "Barbu et al. (2019) Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Joshua Tenenbaum, and Boris Katz. 2019. ObjectNet: a large-scale bias-controlled dataset for pushing the limits of object recognition models. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9453\u20139463.", "Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning, pages 5389\u20135400."]}, {"table": "<table><thead><tr><th>Model</th><th>INet</th><th>INet-v2</th><th>INet-R</th><th>INet-A</th><th>ObjNet</th><th>ReaL</th><th>VTAB-N</th></tr></thead><tbody><tr><th>CLIP Radford et al. (2021)</th><td>76.2</td><td>70.1</td><td>88.9</td><td>77.2</td><td>72.3</td><td>-</td><td>73.9</td></tr><tr><th>ALIGN Jia et al. (2021)</th><td>76.4</td><td>70.1</td><td>92.2</td><td>75.8</td><td>72.2</td><td>-</td><td>-</td></tr><tr><th>BASIC Pham et al. (2021)</th><td>85.7</td><td>80.6</td><td>95.7</td><td>85.6</td><td>78.9</td><td>-</td><td>-</td></tr><tr><th>CoCa Yu et al. (2022)</th><td>86.3</td><td>80.7</td><td>96.5</td><td>90.2</td><td>82.7</td><td>-</td><td>-</td></tr><tr><th>LiT ViT-g Zhai et al. (2022b)</th><td>85.2</td><td>79.8</td><td>94.9</td><td>81.8</td><td>82.5</td><td>88.6</td><td>74.7</td></tr><tr><th>LiT ViT-e (ours)</th><td>85.4</td><td>80.6</td><td>96.1</td><td>88.0</td><td>84.9</td><td>88.4</td><td>76.9</td></tr></tbody></table>", "caption": "Table 12: Zero-shot transfer results of ViT-e on ImageNet, OOD test sets and VTAB-Natural datasets.", "list_citation_info": ["Yu et al. (2022) Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. CoCa: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917.", "Pham et al. (2021) Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. 2021. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050.", "Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916.", "Zhai et al. (2022b) Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. 2022b. LiT: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18123\u201318133.", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Model</th><td colspan=\"5\">Component</td><td colspan=\"3\">Result</td></tr><tr><td>WebLI</td><td>Text dataset</td><td>CC3M</td><td>VQA</td><td>Other</td><td>TextVQA</td><td>VQAv2 (val)</td><td>XM-3600{}_{@224}</td></tr><tr><th rowspan=\"2\">\\NAME-3B</th><td>\u2713</td><td>\u2713</td><td></td><td></td><td></td><td>57.2{}_{@378}</td><td>78.4{}_{@378}</td><td>91.7 (EN) / 33.6 (6L)</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>58.8{}_{@378}</td><td>79.3{}_{@378}</td><td>92.8 (EN) / 44.4 (6L)</td></tr><tr><th rowspan=\"3\">\\NAME-15B</th><td>\u2713</td><td></td><td>\u2713</td><td></td><td></td><td>-</td><td>74.8{}_{@224}</td><td>87.1 (EN) / 47.1 (6L)</td></tr><tr><td>\u2713</td><td></td><td>\u2713</td><td>\u2713</td><td></td><td>-</td><td>76.3{}_{@224}</td><td>-</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>-</td><td>77.8{}_{@224}</td><td>96.8 (EN) / 49.0 (6L)</td></tr></tbody></table>", "caption": "Table 13: Advantage of our comprehensive pretraining mixture over mixtures with subsets of components on both VQA and captioning tasks. Results labeled with \"@378\" are obtained with image resolution 378\\times378. Other results are all from resolution 224\\times224. \u201cOther\u201d refers to VQG, Object-Aware (OA) and detection components. The text-only data is a subset of 100M examples from the dataset used in PaLM Chowdhery et al. (2022).", "list_citation_info": ["Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311."]}, {"table": "<table><tr><td colspan=\"2\">Model Summary</td></tr><tr><td>Model Architecture</td><td><p>PaLI is a multimodal sequence-to-sequence Transformer Vaswani et al. (2017) model derived from the T5 Raffel et al. (2020) encoder-decoder architecture. It takes text tokens and ViT Dosovitskiy et al. (2021) dense image embeddings as inputs to an encoder and autoregressively predicts discrete text tokens with a decoder.</p></td></tr><tr><td>Input(s)</td><td><p>A pair of image and text.</p></td></tr><tr><td>Output(s)</td><td><p>Generated text.</p></td></tr><tr><td colspan=\"2\">Usage</td></tr><tr><td>Application</td><td><p>The model is for research prototype and the current version is not available for the public.</p></td></tr><tr><td>Known Caveats</td><td><p>No.</p></td></tr><tr><td colspan=\"2\">System Type</td></tr><tr><td>System Description</td><td><p>This is a standalone model.</p></td></tr><tr><td>Upstream Dependencies</td><td><p>No.</p></td></tr><tr><td>Downstream Dependencies</td><td><p>No.</p></td></tr><tr><td colspan=\"2\">Implementation Frameworks</td></tr><tr><td>Hardware &amp; Software</td><td><p>Hardware: TPU v4 (Jouppi et al., 2020).</p><p>Software: T5X Roberts et al. (2022), JAX (Bradbury et al., 2018), Flaxformer Heek et al. (2020)</p><p>Details are reported in Section 3.4.</p></td></tr><tr><td>Compute Requirements</td><td><p>Reported in Section 3.4.</p></td></tr><tr><td colspan=\"2\">Model Characteristics</td></tr><tr><td>Model Initialization</td><td><p>The model is initialized from pre-trained language (mT5) Xue et al. (2021) and Vision Transformer (ViT) Zhai et al. (2022a); Dosovitskiy et al. (2021) checkpoints.</p></td></tr><tr><td>Model Status</td><td><p>This is a static model trained on an offline dataset.</p></td></tr><tr><td>Model Stats</td><td><p>The largest PaLI model has 17B parameters, which consists of a 13B parameter mT5-XXL model and a 4B parameter ViT-e model. We have also trained 3B and 15B parameter models.</p></td></tr><tr><td colspan=\"2\">Data Overview</td></tr><tr><td>Training dataset</td><td><p>The model is pre-trained on the following mixture of datasets: WebLI (Table 16), a subset of PaLM/GLaM Dataset Du et al. (2022); Chowdhery et al. (2022), CC3M-35L Sharma et al. (2018), VQ2A-CC3M-35L Changpinyo et al. (2022a), Open Images Kuznetsova et al. (2020), Visual Genome Krishna et al. (2017) and Object365 Shao et al. (2019).Details are reported in Section 3.3.</p></td></tr><tr><td>Evaluation and Fine-tuning Dataset</td><td><p>\u2022Vision + language tasks\u2013Image captioning (English):COCO Chen et al. (2015),NoCaps Agrawal et al. (2019),TextCaps Sidorov et al. (2020)\u2013Image captioning (multilingual): Crossmodal-3600 Thapliyal et al. (2022)\u2013Visual question answering (English):VQAv2 Goyal et al. (2017),OKVQA Gui et al. (2021),TextVQA Singh et al. (2019),VizWiz-QA Gurari et al. (2018)\u2013Visual question answering (multilingual): xGQA Pfeiffer et al. (2022),MaXM Changpinyo et al. (2022b)\u2022Vision-only tasks\u2013Image classification (fine-tuning): ImageNet Deng et al. (2009), ImageNet-V2 Recht et al. (2019), ObjectNet Barbu et al. (2019),ReaL Beyer et al. (2020)\u2013Image classification (zero-shot): ImageNet Deng et al. (2009), ImageNet-V2 Recht et al. (2019), ImageNet-R Hendrycks et al. (2021a), ImageNet-A Hendrycks et al. (2021b), ImageNet-Sketch Wang et al. (2019b), ObjectNet Barbu et al. (2019),ReaL Beyer et al. (2020),VTAB Zhai et al. (2019)\u2022Language-only tasks\u2013Natural language inference (English): SuperGLUE Wang et al. (2019a)\u2013Natural language inference (multilingual): XNLI Conneau et al. (2018)\u2013Question Answering (multilingual): XQuAD Artetxe et al. (2020), TyDiQA Clark et al. (2020)</p></td></tr><tr><td colspan=\"2\">Evaluation Results</td></tr><tr><td>Evaluation Results</td><td><p>Reported in Section 4.</p></td></tr><tr><td colspan=\"2\">Model Usage &amp; Limitations</td></tr><tr><td>Sensitive Use</td><td><p>The model is capable of open-ended text generations. This model should not be used for any of the unacceptable language model use cases, e.g., generation of toxic speech.</p></td></tr><tr><td>Known Limitations</td><td><p>Reported in Section 4.8.</p></td></tr><tr><td>Ethical Considerations &amp; Risks</td><td><p>Reported in Section 5.</p></td></tr></table>", "caption": "Table 15: PaLI model card.", "list_citation_info": ["Thapliyal et al. (2022) Ashish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522.", "Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs.", "Changpinyo et al. (2022a) Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, and Radu Soricut. 2022a. All you may need for VQA are image captions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1947\u20131963.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 2556\u20132565.", "Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.", "Wang et al. (2019a) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019a. SuperGLUE: a stickier benchmark for general-purpose language understanding systems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 3266\u20133280.", "Sidorov et al. (2020) Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. 2020. TextCaps: a dataset for image captioning with reading comprehension. In European conference on computer vision, pages 742\u2013758.", "Clark et al. (2020) Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454\u2013470.", "Du et al. (2022) Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. GLaM: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547\u20135569.", "Roberts et al. (2022) Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. 2022. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189.", "Shao et al. (2019) Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. 2019. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430\u20138439.", "Barbu et al. (2019) Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Joshua Tenenbaum, and Boris Katz. 2019. ObjectNet: a large-scale bias-controlled dataset for pushing the limits of object recognition models. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9453\u20139463.", "Jouppi et al. (2020) Norman P Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, Cliff Young, and David Patterson. 2020. A domain-specific supercomputer for training deep neural networks. Communications of the ACM, 63(7):67\u201378.", "Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021,.", "Pfeiffer et al. (2022) Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-Martin Steitz, Stefan Roth, Ivan Vuli\u0107, and Iryna Gurevych. 2022. xGQA: Cross-lingual visual question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2497\u20132511.", "Changpinyo et al. (2022b) Soravit Changpinyo, Linting Xue, Idan Szpektor, Ashish V. Thapliyal, Julien Amelot, Xi Chen, and Radu Soricut. 2022b. Towards multi-lingual visual question answering. arXiv preprint arXiv:2209.05401.", "Beyer et al. (2020) Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. 2020. Are we done with ImageNet? arXiv preprint arXiv:2006.07159.", "Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021b. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271.", "Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021a. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349.", "Gurari et al. (2018) Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. VizWiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608\u20133617.", "Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.", "Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2015. Microsoft COCO Captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.", "Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning, pages 5389\u20135400.", "Zhai et al. (2022a) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022a. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113.", "Gui et al. (2021) Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021. KAT: A knowledge augmented transformer for vision-and-language. arXiv preprint arXiv:2112.08614.", "Kuznetsova et al. (2020) Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. 2020. The Open Images dataset v4. International Journal of Computer Vision, 128(7):1956\u20131981.", "Heek et al. (2020) Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. 2020. Flax: A neural network library and ecosystem for JAX.", "Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000\u20136010.", "Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498.", "Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards VQA models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326.", "Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913.", "Zhai et al. (2019) Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. 2019. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867.", "Agrawal et al. (2019) Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. 2019. nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8948\u20138957.", "Wang et al. (2019b) Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. 2019b. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506\u201310518.", "Conneau et al. (2018) Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485.", "Artetxe et al. (2020) Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637.", "Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual Genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373."]}, {"table": "<table><tbody><tr><th></th><td></td><td>TextCaps</td><td>TextVQA</td><td colspan=\"2\">VizWiz-QA</td></tr><tr><th>Method</th><td>OCR input?</td><td>test</td><td>test</td><td>test-dev</td><td>test-std</td></tr><tr><th>TAP Yang et al. (2021)</th><td>Yes</td><td>103.2</td><td>53.97</td><td>-</td><td>-</td></tr><tr><th>GIT</th><td>No</td><td>138.2</td><td>59.75</td><td>68.0</td><td>67.5</td></tr><tr><th>GIT2</th><td>No</td><td>145.0</td><td>67.27</td><td>71.0</td><td>70.1</td></tr><tr><th>\\NAME</th><td>No</td><td>135.4</td><td>58.80</td><td>71.6</td><td>70.7</td></tr><tr><th>\\NAME</th><td>Yes</td><td>160.4</td><td>73.06</td><td>74.4</td><td>73.3</td></tr></tbody></table>", "caption": "Table 19: Results on TextCaps, TextVQA and VizWiz-QA with and without detected OCR as input for \\NAME", "list_citation_info": ["Yang et al. (2021) Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo Luo. 2021. TAP: Text-aware pre-training for Text-VQA and Text-Caption. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8751\u20138761."]}, {"table": "<table><thead><tr><th>Model</th><th>INet</th><th>INet-R</th><th>INet-A</th><th>INet-sketch</th><th>INet-v2</th><th>ObjNet</th></tr></thead><tbody><tr><th>\\NAME-3B</th><td>84.31</td><td>90.05</td><td>55.04</td><td>76.47</td><td>78.49</td><td>53.71</td></tr><tr><th>\\NAME-15B</th><td>84.78</td><td>90.91</td><td>59.00</td><td>76.81</td><td>79.54</td><td>55.29</td></tr><tr><th>\\NAME</th><td>86.18</td><td>91.51</td><td>62.72</td><td>79.30</td><td>80.71</td><td>58.35</td></tr></tbody></table>", "caption": "Table 21: Top 5 accuracy results of Zero-shot image classification on ImageNet Deng et al. (2009), ImageNet-R Hendrycks et al. (2021a), ImageNet-A Hendrycks et al. (2021b), ImageNet-Sketch Wang et al. (2019b), ImageNet-v2 Recht et al. (2019) and ObjectNet Barbu et al. (2019).", "list_citation_info": ["Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021b. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271.", "Wang et al. (2019b) Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. 2019b. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506\u201310518.", "Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021a. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349.", "Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.", "Barbu et al. (2019) Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Joshua Tenenbaum, and Boris Katz. 2019. ObjectNet: a large-scale bias-controlled dataset for pushing the limits of object recognition models. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9453\u20139463.", "Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning, pages 5389\u20135400."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Language</th><th colspan=\"3\">Image-to-text</th><th colspan=\"3\">Text-to-image</th></tr><tr><th>LiT ViT-g</th><th>LiT ViT-e</th><th>LiT ViT-e (multilingual)</th><th>LiT ViT-g</th><th>LiT ViT-e</th><th>LiT ViT-e (multilingual)</th></tr></thead><tbody><tr><th>ar</th><td>5.28</td><td>26.58</td><td>39.69</td><td>2.80</td><td>18.46</td><td>32.60</td></tr><tr><th>bn</th><td>0.00</td><td>0.11</td><td>5.67</td><td>0.00</td><td>0.06</td><td>3.31</td></tr><tr><th>cs</th><td>18.19</td><td>39.25</td><td>44.03</td><td>11.24</td><td>27.35</td><td>35.24</td></tr><tr><th>da</th><td>26.44</td><td>48.92</td><td>50.75</td><td>14.07</td><td>34.43</td><td>38.48</td></tr><tr><th>de</th><td>37.83</td><td>58.42</td><td>58.53</td><td>23.61</td><td>43.25</td><td>46.50</td></tr><tr><th>el</th><td>1.56</td><td>13.47</td><td>29.03</td><td>0.39</td><td>5.46</td><td>20.92</td></tr><tr><th>en</th><td>51.22</td><td>51.78</td><td>42.11</td><td>46.24</td><td>47.07</td><td>40.63</td></tr><tr><th>es</th><td>41.81</td><td>57.50</td><td>55.22</td><td>30.29</td><td>47.71</td><td>46.55</td></tr><tr><th>fa</th><td>3.78</td><td>18.39</td><td>44.50</td><td>1.57</td><td>10.74</td><td>35.58</td></tr><tr><th>fi</th><td>14.14</td><td>29.42</td><td>32.64</td><td>6.59</td><td>16.91</td><td>21.80</td></tr><tr><th>fil</th><td>10.94</td><td>16.39</td><td>15.53</td><td>4.18</td><td>8.66</td><td>10.04</td></tr><tr><th>fr</th><td>38.28</td><td>57.06</td><td>52.61</td><td>28.02</td><td>45.20</td><td>43.47</td></tr><tr><th>hi</th><td>0.47</td><td>7.33</td><td>13.14</td><td>0.08</td><td>2.90</td><td>7.42</td></tr><tr><th>hr</th><td>15.86</td><td>34.47</td><td>38.31</td><td>8.80</td><td>22.72</td><td>29.55</td></tr><tr><th>hu</th><td>15.11</td><td>31.17</td><td>44.67</td><td>8.45</td><td>20.52</td><td>35.49</td></tr><tr><th>id</th><td>24.11</td><td>43.72</td><td>46.33</td><td>12.99</td><td>32.08</td><td>36.75</td></tr><tr><th>it</th><td>39.69</td><td>57.47</td><td>54.53</td><td>27.07</td><td>46.79</td><td>44.76</td></tr><tr><th>iw</th><td>1.75</td><td>9.11</td><td>38.67</td><td>0.86</td><td>3.99</td><td>29.39</td></tr><tr><th>ja</th><td>3.61</td><td>11.67</td><td>35.47</td><td>1.20</td><td>4.91</td><td>27.24</td></tr><tr><th>ko</th><td>1.78</td><td>6.00</td><td>36.11</td><td>0.35</td><td>3.14</td><td>25.95</td></tr><tr><th>mi</th><td>0.58</td><td>0.92</td><td>0.33</td><td>0.19</td><td>0.30</td><td>0.22</td></tr><tr><th>nl</th><td>37.47</td><td>51.67</td><td>52.14</td><td>27.26</td><td>44.08</td><td>43.79</td></tr><tr><th>no</th><td>26.53</td><td>49.69</td><td>49.17</td><td>14.61</td><td>35.59</td><td>37.35</td></tr><tr><th>pl</th><td>19.67</td><td>42.03</td><td>51.42</td><td>12.00</td><td>31.13</td><td>43.72</td></tr><tr><th>pt</th><td>33.92</td><td>50.81</td><td>49.19</td><td>23.58</td><td>42.97</td><td>42.73</td></tr><tr><th>quz</th><td>5.08</td><td>6.83</td><td>4.31</td><td>1.85</td><td>1.89</td><td>1.90</td></tr><tr><th>ro</th><td>17.94</td><td>30.08</td><td>37.75</td><td>10.15</td><td>20.06</td><td>28.82</td></tr><tr><th>ru</th><td>12.00</td><td>26.22</td><td>50.64</td><td>5.76</td><td>17.19</td><td>41.11</td></tr><tr><th>sv</th><td>25.50</td><td>51.00</td><td>53.22</td><td>15.11</td><td>38.80</td><td>40.66</td></tr><tr><th>sw</th><td>4.47</td><td>7.75</td><td>6.42</td><td>1.58</td><td>4.17</td><td>3.41</td></tr><tr><th>te</th><td>0.06</td><td>0.03</td><td>1.92</td><td>0.03</td><td>0.03</td><td>1.42</td></tr><tr><th>th</th><td>1.89</td><td>7.22</td><td>22.00</td><td>0.79</td><td>3.71</td><td>16.06</td></tr><tr><th>tr</th><td>10.72</td><td>31.28</td><td>39.50</td><td>4.73</td><td>20.42</td><td>31.47</td></tr><tr><th>uk</th><td>7.67</td><td>19.94</td><td>39.53</td><td>3.38</td><td>10.40</td><td>30.81</td></tr><tr><th>vi</th><td>3.08</td><td>11.44</td><td>27.08</td><td>0.98</td><td>6.22</td><td>21.28</td></tr><tr><th>zh</th><td>4.53</td><td>11.11</td><td>33.61</td><td>1.67</td><td>5.60</td><td>28.24</td></tr><tr><th>avg</th><td>15.64</td><td>28.23</td><td>35.99</td><td>9.79</td><td>20.14</td><td>28.46</td></tr></tbody></table>", "caption": "Table 22: Image-to-text and text-to-image zero-shot retrieval results on all 36 languages of Crossmodal-3600. Models are trained following LiT Zhai et al. (2022b) method with diverse visual backbones (ViT-g or ViT-e) and datasets (English or multilingual).", "list_citation_info": ["Zhai et al. (2022b) Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. 2022b. LiT: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18123\u201318133."]}], "citation_info_to_title": {"Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.": "Palm: Scaling language modeling with pathways", "Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021,.": "An image is worth 16x16 words: Transformers for image recognition at scale", "Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. 2021a. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349.": "The many faces of robustness: A critical analysis of out-of-distribution generalization", "Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916.": "Scaling up visual and vision-language representation learning with noisy text supervision", "Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. 2021b. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15262\u201315271.": "Natural adversarial examples", "Yu et al. (2022) Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022. CoCa: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917.": "CoCa: Contrastive captioners are image-text foundation models", "Changpinyo et al. (2022a) Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, and Radu Soricut. 2022a. All you may need for VQA are image captions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1947\u20131963.": "All you may need for VQA are image captions", "Gurari et al. (2018) Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. 2018. VizWiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608\u20133617.": "VizWiz grand challenge: Answering visual questions from blind people", "Pham et al. (2021) Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. 2021. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050.": "Combined Scaling for Zero-Shot Transfer Learning", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 2556\u20132565.": "Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "Barbu et al. (2019) Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Joshua Tenenbaum, and Boris Katz. 2019. ObjectNet: a large-scale bias-controlled dataset for pushing the limits of object recognition models. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 9453\u20139463.": "ObjectNet: a large-scale bias-controlled dataset for pushing the limits of object recognition models", "Zhai et al. (2022a) Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. 2022a. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113.": "Scaling Vision Transformers", "Jouppi et al. (2020) Norman P Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, Cliff Young, and David Patterson. 2020. A domain-specific supercomputer for training deep neural networks. Communications of the ACM, 63(7):67\u201378.": "A domain-specific supercomputer for training deep neural networks", "Kuznetsova et al. (2020) Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. 2020. The Open Images dataset v4. International Journal of Computer Vision, 128(7):1956\u20131981.": "The Open Images dataset v4", "Chen et al. (2015) Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2015. Microsoft COCO Captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.": "Microsoft COCO Captions: Data collection and evaluation server", "Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual Genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373.": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations", "Xue et al. (2021) Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498.": "mT5: A massively multilingual pre-trained text-to-text transformer", "Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.": "ImageNet: A large-scale hierarchical image database", "Agrawal et al. (2019) Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. 2019. nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8948\u20138957.": "Nocaps: Novel Object Captioning at Scale", "Xue et al. (2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291\u2013306.": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "Zhai et al. (2022b) Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. 2022b. LiT: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18123\u201318133.": "LiT: Zero-shot transfer with locked-image text tuning", "Du et al. (2022) Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. GLaM: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547\u20135569.": "GLaM: Efficient scaling of language models with mixture-of-experts", "Wang et al. (2019a) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019a. SuperGLUE: a stickier benchmark for general-purpose language understanding systems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 3266\u20133280.": "SuperGLUE: a stickier benchmark for general-purpose language understanding systems", "Gui et al. (2021) Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021. KAT: A knowledge augmented transformer for vision-and-language. arXiv preprint arXiv:2112.08614.": "KAT: A knowledge augmented transformer for vision-and-language", "Conneau et al. (2018) Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485.": "XNLI: Evaluating cross-lingual sentence representations", "Heek et al. (2020) Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. 2020. Flax: A neural network library and ecosystem for JAX.": "Flax: A neural network library and ecosystem for JAX", "Beyer et al. (2020) Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. 2020. Are we done with ImageNet? arXiv preprint arXiv:2006.07159.": "Are we done with ImageNet?", "Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards VQA models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326.": "Towards VQA models that can read", "Thapliyal et al. (2022) Ashish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. 2022. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. arXiv preprint arXiv:2205.12522.": "Crossmodal-3600: A massively multilingual multimodal evaluation dataset", "Yang et al. (2021) Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo Luo. 2021. TAP: Text-aware pre-training for Text-VQA and Text-Caption. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8751\u20138761.": "TAP: Text-aware pre-training for Text-VQA and Text-Caption", "Changpinyo et al. (2022b) Soravit Changpinyo, Linting Xue, Idan Szpektor, Ashish V. Thapliyal, Julien Amelot, Xi Chen, and Radu Soricut. 2022b. Towards multi-lingual visual question answering. arXiv preprint arXiv:2209.05401.": "Towards multi-lingual visual question answering", "Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433.": "VQA: Visual question answering", "Artetxe et al. (2020) Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637.": "On the cross-lingual transferability of monolingual representations", "Shao et al. (2019) Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. 2019. Objects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8430\u20138439.": "Objects365: A large-scale, high-quality dataset for object detection", "Zhai et al. (2019) Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. 2019. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867.": "A large-scale study of representation learning with the visual task adaptation benchmark", "Wang et al. (2022b) Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022b. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052.": "Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework", "Pfeiffer et al. (2022) Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-Martin Steitz, Stefan Roth, Ivan Vuli\u0107, and Iryna Gurevych. 2022. xGQA: Cross-lingual visual question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2497\u20132511.": "xGQA: Cross-lingual visual question answering", "Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000\u20136010.": "Attention is all you need", "Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913.": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "Sidorov et al. (2020) Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. 2020. TextCaps: a dataset for image captioning with reading comprehension. In European conference on computer vision, pages 742\u2013758.": "TextCaps: a dataset for image captioning with reading comprehension", "Vedantam et al. (2015) Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575.": "CIDEr: Consensus-based image description evaluation", "Clark et al. (2020) Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454\u2013470.": "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763.": "Learning transferable visual models from natural language supervision", "Wang et al. (2019b) Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. 2019b. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems, pages 10506\u201310518.": "Learning Robust Global Representations by Penalizing Local Predictive Power", "Karpathy and Fei-Fei (2015) Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128\u20133137.": "Deep visual-semantic alignments for generating image descriptions", "Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.": "Exploring the limits of transfer learning with a unified text-to-text transformer", "Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning, pages 5389\u20135400.": "Do ImageNet classifiers generalize to ImageNet?", "Bradbury et al. (2018) James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs.": "JAX: Composable Transformations of Python+NumPy Programs", "Roberts et al. (2022) Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. 2022. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189.": "Scaling up models and data with t5x and seqio", "Qiao et al. (2021) Yixuan Qiao, Hao Chen, Jun Wang, Yihao Chen, Xianbin Ye, Ziliang Li, Xianbiao Qi, Peng Gao, and Guotong Xie. 2021. Winner team Mia at TextVQA challenge 2021: Vision-and-language representation learning with pre-trained sequence-to-sequence model. arXiv preprint arXiv:2106.15332.": "Winner team Mia at TextVQA challenge 2021: Vision-and-language representation learning with pre-trained sequence-to-sequence model"}, "source_title_to_arxiv_id": {"Palm: Scaling language modeling with pathways": "2204.02311", "All you may need for VQA are image captions": "2205.01883", "Combined Scaling for Zero-Shot Transfer Learning": "2111.10050", "GLaM: Efficient scaling of language models with mixture-of-experts": "2112.06905"}}