{"title": "On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization", "abstract": "Combining the visual modality with pretrained language models has been\nsurprisingly effective for simple descriptive tasks such as image captioning.\nMore general text generation however remains elusive. We take a step back and\nask: How do these models work for more complex generative tasks, i.e.\nconditioning on both text and images? Are multimodal models simply visually\nadapted language models, or do they combine they reason jointly over\nmodalities?\n  We investigate these questions in the context of self-rationalization\n(jointly generating task labels/answers and free-text explanations) of three\ntasks: (i) visual question answering in VQA-X, (ii) visual commonsense\nreasoning in VCR, and (iii) visual-textual entailment in e-SNLI-VE. We show\nthat recent unimodal advances, CLIP image representations and scaling of\nlanguage models, do not consistently improve self-rationalization in multimodal\ntasks. We find that no single model type works universally best across tasks,\ndatasets, and finetuning data sizes. Our findings motivate the need for novel\ngeneral backbones approach that move text generation from images and text\nbeyond image captioning.", "authors": ["Shruti Palaskar", "Akshita Bhagia", "Yonatan Bisk", "Florian Metze", "Alan W Black", "Ana Marasovi\u0107"], "published_date": "2022_05_24", "pdf_url": "http://arxiv.org/pdf/2205.11686v2", "list_table_and_caption": [{"table": "<table><tr><td></td><td>Wanted Model Properties for Multimodal Self-Rationalization</td><td><p>Visually Adapted PLMs (\u00a72.2)</p></td><td><p>Joint VL Models (\u00a72.1)</p></td><td><p>Comb. Models (\u00a72.3)</p></td></tr><tr><td>1</td><td>Designed for some text generation task (e.g., image captioning, language modeling)</td><td><p> \u2713 </p></td><td><p> Some </p></td><td><p> Some </p></td></tr><tr><td>2</td><td>Offered in larger sizes (related to better text generation performance)</td><td><p> \u2713 </p></td><td></td><td></td></tr><tr><td>3</td><td>Large textual pretraining data (related to capturing world/commonsense knowledge)</td><td><p> \u2713 </p></td><td></td><td><p> \u2713 </p></td></tr><tr><td>4</td><td>Easy plug-and-playing with the latest pretrained LMs and image representations</td><td><p> \u2713 </p></td><td></td><td></td></tr><tr><td>5</td><td>Tight coupling between modalities</td><td></td><td><p> \u2713 </p></td><td><p> \u2713 </p></td></tr><tr><td>6</td><td>Expected to be robust when multimodal training data is limited</td><td></td><td><p> \u2713 </p></td><td><p> \u2713 </p></td></tr></table>", "caption": "Table 1: A comparison between: training vision and language (VL) jointly from scratch, adapting pretrained language models (PLM) to visual features, and models that somewhat combine these two approaches, w.r.t. desired model properties for self-rationalization. Text generation typically improves with model size Brown et al. (2020), incl. self-rationalization Marasovi\u0107 et al. (2022). Due to huge pretraining corpora, PLMs have been shown to capture some world Petroni et al. (2019) and commonsense knowledge Davison et al. (2019) which is beneficial for self-rationalization as the task often requires inferring relevant information from the inputs (see Fig. 1).", "list_citation_info": ["Davison et al. (2019) Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173\u20131178, Hong Kong, China. Association for Computational Linguistics.", "Petroni et al. (2019) Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China. Association for Computational Linguistics.", "Marasovi\u0107 et al. (2022) Ana Marasovi\u0107, Iz Beltagy, Doug Downey, and Matthew E. Peters. 2022. Few-shot self-rationalization with natural language prompts. In Findings of the Association for Computational Linguistics: NAACL 2022.", "Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc."]}, {"table": "<table><tr><td>Model</td><td>Backbone</td><td><p>Backbone PTObjectives </p></td><td>Backbone PT Data</td><td><p>Continued PTDatasets </p></td></tr><tr><td>VLP</td><td><p>BERT / UniLM</p></td><td><p>MLM</p></td><td><p>Wiki, BookCorpus</p></td><td><p>Conceptual Captions</p></td></tr><tr><td>VA-T5-CLIP</td><td rowspan=\"4\">T5</td><td rowspan=\"4\">Fill-in-the-span-style denoising objectives + multitask learning</td><td rowspan=\"4\">C4 + a suite of annotated datasets for classification, QA, translation, etc.</td><td rowspan=\"3\">None</td></tr><tr><td>VA-T5-Captions</td></tr><tr><td>VA-T5-Objects</td></tr><tr><td>VL-T5</td><td rowspan=\"2\">MS COCO, Visual Genome, VQA v2.0, GQA, Visual7W</td></tr><tr><td>VL-BART</td><td><p>BART</p></td><td><p>Reconstruct text corrupted with anarbitrary noising function</p></td><td><p>Wiki, BookCorpus, Stories, CCNews, OpenWebText</p></td></tr><tr><td>Model</td><td><p>Continued PTObjectives </p></td><td>Img. Feat. Model</td><td><p>Img. Feat. ModelBackbone </p></td><td><p>Img. Feat. ModelPT Data </p></td></tr><tr><td>VLP</td><td><p>MLM, LM</p></td><td><p>Faster R-CNN</p></td><td><p>ResNeXt-101 FPN</p></td><td><p>Visual Genome</p></td></tr><tr><td>VA-T5-CLIP</td><td rowspan=\"3\">None</td><td><p>CLIP</p></td><td><p>Vision Transformer</p></td><td><p>New unavail. data</p></td></tr><tr><td>VA-T5-Captions</td><td><p>VL-T5</p></td><td><p>See VL-T5 rows</p></td><td><p>See VL-T5 rows</p></td></tr><tr><td>VA-T5-Objects</td><td><p>Faster R-CNN</p></td><td><p>ResNeXt-101 FPN</p></td><td><p>Visual Genome</p></td></tr><tr><td>VL-T5</td><td rowspan=\"2\">MLM, VQA, image-text matching, visual grounding, grounded captioning</td><td rowspan=\"2\">Faster R-CNN</td><td rowspan=\"2\">ResNeXt-101 FPN</td><td rowspan=\"2\">Visual Genome</td></tr><tr><td>VL-BART</td></tr></table>", "caption": "Table 2: Model specifications. PT stands for \u201cpretraining\u201d, MLM for \u201cmasked language modeling\u201d, \u201cImg. Feat.\u201d for \u201cImage Features\u201d. Sources: BERT Devlin et al. (2019), UniLM Dong et al. (2019), BookCorpus Zhu et al. (2015), T5 Raffel et al. (2020), C4 Raffel et al. (2020) is made publicly available by Dodge et al. (2021), BART Lewis et al. (2020), MS COCO Lin et al. (2014), VQA v2 Goyal et al. (2017), GQA Hudson and Manning (2019), Visual7W Zhu et al. (2016), Stories Trinh and Le (2018), CCNews, OpenWebText Gokaslan and Cohen (2019), Conceptual Captions Sharma et al. (2018), Faster R-CNN Ren et al. (2015), Visual Genome Krishna et al. (2017), ResNeXt-101 FPN Xie et al. (2017), Vision Transformer Dosovitskiy et al. (2021).", "list_citation_info": ["Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus.", "Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.", "Zhu et al. (2016) Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question answering in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.", "Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:1137\u20131149.", "Dodge et al. (2021) Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.", "Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.", "Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 19\u201327. IEEE Computer Society.", "Trinh and Le (2018) Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning. CoRR, abs/1806.02847.", "Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.", "Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32\u201373.", "Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13042\u201313054.", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, Melbourne, Australia. Association for Computational Linguistics.", "Xie et al. (2017) Saining Xie, Ross B. Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. 2017. Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5987\u20135995. IEEE Computer Society.", "Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.", "Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV).", "Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition."]}, {"table": "<table><tr><td>Dataset</td><td>Image Sources</td></tr><tr><td>VQA-X</td><td>VQA v2.0</td></tr><tr><td>E-SNLI-VE</td><td>SNLI / Flickr</td></tr><tr><td>VCR</td><td>movie clips / Fandango</td></tr><tr><td>MS COCO</td><td>Flickr</td></tr><tr><td>YFCC100M</td><td>Flickr</td></tr><tr><td>Visual Genome</td><td>YFCC100M + MS COCO</td></tr><tr><td>VQA v2.0</td><td>MS COCO</td></tr><tr><td>GQA</td><td>Visual Genome</td></tr><tr><td>Visual7W</td><td>MS COCO</td></tr></table>", "caption": "Table 3: Image sources. MS COCO Lin et al. (2014), SNLI Bowman et al. (2015), movie clips Rohrbach et al. (2017), YFCC100M Thomee et al. (2016).", "list_citation_info": ["Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV).", "Rohrbach et al. (2017) Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, Hugo Larochelle, Aaron C. Courville, and Bernt Schiele. 2017. Movie description. Int. J. Comput. Vis., 123(1):94\u2013120.", "Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Computational Linguistics.", "Thomee et al. (2016) Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016. YFCC100M: the new data in multimedia research. Commun. ACM, 59(2):64\u201373."]}], "citation_info_to_title": {"Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Proceedings of the European Conference on Computer Vision (ECCV).": "Microsoft COCO: Common Objects in Context", "Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.": "Language models are few-shot learners", "Rohrbach et al. (2017) Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, Hugo Larochelle, Aaron C. Courville, and Bernt Schiele. 2017. Movie description. Int. J. Comput. Vis., 123(1):94\u2013120.": "Movie Description", "Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "Ren et al. (2015) Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:1137\u20131149.": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "Petroni et al. (2019) Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China. Association for Computational Linguistics.": "Language models as knowledge bases?", "Hudson and Manning (2019) Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.": "Gqa: A new dataset for real-world visual reasoning and compositional question answering", "Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13042\u201313054.": "Unified Language Model Pre-Training for Natural Language Understanding and Generation", "Davison et al. (2019) Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173\u20131178, Hong Kong, China. Association for Computational Linguistics.": "Commonsense knowledge mining from pretrained models", "Gokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus.": "Openwebtext Corpus", "Trinh and Le (2018) Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning. CoRR, abs/1806.02847.": "A Simple Method for Commonsense Reasoning", "Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 19\u201327. IEEE Computer Society.": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.": "An image is worth 16x16 words: Transformers for image recognition at scale", "Thomee et al. (2016) Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016. YFCC100M: the new data in multimedia research. Commun. ACM, 59(2):64\u201373.": "YFCC100M: The New Data in Multimedia Research", "Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32\u201373.": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations", "Sharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, Melbourne, Australia. Association for Computational Linguistics.": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Computational Linguistics.": "A large annotated corpus for learning natural language inference", "Zhu et al. (2016) Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. 2016. Visual7w: Grounded question answering in images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.": "Visual7w: Grounded question answering in images", "Marasovi\u0107 et al. (2022) Ana Marasovi\u0107, Iz Beltagy, Doug Downey, and Matthew E. Peters. 2022. Few-shot self-rationalization with natural language prompts. In Findings of the Association for Computational Linguistics: NAACL 2022.": "Few-shot self-rationalization with natural language prompts", "Dodge et al. (2021) Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus", "Lewis et al. (2020) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.": "BERT: Pre-training of deep bidirectional transformers for language understanding", "Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.": "Exploring the limits of transfer learning with a unified text-to-text transformer", "Xie et al. (2017) Saining Xie, Ross B. Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. 2017. Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5987\u20135995. IEEE Computer Society.": "Aggregated residual transformations for deep neural networks"}, "source_title_to_arxiv_id": {"Few-shot self-rationalization with natural language prompts": "2111.08284", "Documenting large webtext corpora: A case study on the colossal clean crawled corpus": "2104.08758"}}