{"title": "DiffRF: Rendering-Guided 3D Radiance Field Diffusion", "abstract": "We introduce DiffRF, a novel approach for 3D radiance field synthesis based\non denoising diffusion probabilistic models. While existing diffusion-based\nmethods operate on images, latent codes, or point cloud data, we are the first\nto directly generate volumetric radiance fields. To this end, we propose a 3D\ndenoising model which directly operates on an explicit voxel grid\nrepresentation. However, as radiance fields generated from a set of posed\nimages can be ambiguous and contain artifacts, obtaining ground truth radiance\nfield samples is non-trivial. We address this challenge by pairing the\ndenoising formulation with a rendering loss, enabling our model to learn a\ndeviated prior that favours good image quality instead of trying to replicate\nfitting errors like floating artifacts. In contrast to 2D-diffusion models, our\nmodel learns multi-view consistent priors, enabling free-view synthesis and\naccurate shape generation. Compared to 3D GANs, our diffusion-based approach\nnaturally enables conditional generation such as masked completion or\nsingle-view 3D synthesis at inference time.", "authors": ["Norman M\u00fcller", "Yawar Siddiqui", "Lorenzo Porzi", "Samuel Rota Bul\u00f2", "Peter Kontschieder", "Matthias Nie\u00dfner"], "published_date": "2022_12_02", "pdf_url": "http://arxiv.org/pdf/2212.01206v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>FID \u2193</td><td>IS \\uparrow</td><td>COV \\uparrow</td><td>MMD \u2193</td></tr><tr><th>\\pi-GAN [5]</th><td>68.16</td><td>2.428</td><td>35.18</td><td>8.176</td></tr><tr><th>EG3D [6]</th><td>27.03</td><td>3.641</td><td>46.37</td><td>5.864</td></tr><tr><th>DiffRF w/o 2D</th><td>34.42</td><td>3.274</td><td>53.34</td><td>4.305</td></tr><tr><th>DiffRF w/o refine</th><td>27.39</td><td>3.674</td><td>52.72</td><td>4.272</td></tr><tr><th>DiffRF</th><td>25.64</td><td>3.768</td><td>52.41</td><td>4.264</td></tr></tbody></table>", "caption": "Table 1: Quantitative comparison of unconditional generation on the PhotoShape Chairs [41] dataset. Our method achieves a better image and geometric quality than state-of-the-art GAN-based approaches. Both the 2D rendering loss and the CNN refinement are crucial to our method, as indicated by the drop in quality without them. MMD scores are multiplied by 10^{3}.", "list_citation_info": ["[6] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022.", "[41] Keunhong Park, Konstantinos Rematas, Ali Farhadi, and Steven M. Seitz. Photoshape: Photorealistic materials for large-scale shape collections. ACM Trans. Graph., 37(6), Nov. 2018.", "[5] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In arXiv, 2020."]}, {"table": "<table><tbody><tr><th>Method</th><td>FID \u2193</td><td>IS \\uparrow</td><td>COV \\uparrow</td><td>MMD \u2193</td></tr><tr><th>\\pi-GAN [5]</th><td>65.40</td><td>2.851</td><td>44.11</td><td>8.393</td></tr><tr><th>EG3D [6]</th><td>52.71</td><td>3.332</td><td>52.94</td><td>6.417</td></tr><tr><th>DiffRF w/o 2D</th><td>60.15</td><td>2.918</td><td>63.73</td><td>4.921</td></tr><tr><th>DiffRF w/o refine</th><td>53.84</td><td>3.254</td><td>62.75</td><td>4.874</td></tr><tr><th>DiffRF</th><td>51.67</td><td>3.324</td><td>61.76</td><td>4.863</td></tr></tbody></table>", "caption": "Table 2: Quantitative comparison of unconditional generation on the ABO Tables [9] dataset. Our method achieves a better image and geometric quality than state-of-the-art GAN-based approaches, with both 2D rendering loss and CNN refinement being important. MMD scores are multiplied by 10^{3}.", "list_citation_info": ["[6] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022.", "[9] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21126\u201321136, 2022.", "[5] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In arXiv, 2020."]}], "citation_info_to_title": {"[6] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022.": "Efficient geometry-aware 3D generative adversarial networks", "[41] Keunhong Park, Konstantinos Rematas, Ali Farhadi, and Steven M. Seitz. Photoshape: Photorealistic materials for large-scale shape collections. ACM Trans. Graph., 37(6), Nov. 2018.": "Photoshape: Photorealistic materials for large-scale shape collections", "[5] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In arXiv, 2020.": "pi-gan: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis", "[9] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21126\u201321136, 2022.": "Abo: Dataset and benchmarks for real-world 3d object understanding"}, "source_title_to_arxiv_id": {"Abo: Dataset and benchmarks for real-world 3d object understanding": "2110.06199"}}