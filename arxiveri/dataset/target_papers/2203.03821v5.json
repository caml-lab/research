{"title": "CF-ViT: A General Coarse-to-Fine Method for Vision Transformer", "abstract": "Vision Transformers (ViT) have made many breakthroughs in computer vision\ntasks. However, considerable redundancy arises in the spatial dimension of an\ninput image, leading to massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve computational burden\nwhile retaining performance in this paper. Our proposed CF-ViT is motivated by\ntwo important observations in modern ViT models: (1) The coarse-grained patch\nsplitting can locate informative regions of an input image. (2) Most images can\nbe well recognized by a ViT model in a small-length token sequence. Therefore,\nour CF-ViT implements network inference in a two-stage manner. At coarse\ninference stage, an input image is split into a small-length patch sequence for\na computationally economical classification. If not well recognized, the\ninformative patches are identified and further re-split in a fine-grained\ngranularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For\nexample, without any compromise on performance, CF-ViT reduces 53% FLOPs of\nLV-ViT, and also achieves 2.01x throughput.", "authors": ["Mengzhao Chen", "Mingbao Lin", "Ke Li", "Yunhang Shen", "Yongjian Wu", "Fei Chao", "Rongrong Ji"], "published_date": "2022_03_08", "pdf_url": "http://arxiv.org/pdf/2203.03821v5", "list_table_and_caption": [{"table": "<table><thead><tr><th>No. of token</th><th>14\\times14</th><th>7\\times7</th></tr></thead><tbody><tr><th>Accuracy</th><td>79.8%</td><td>73.2%</td></tr><tr><th>FLOPs</th><td>4.60G</td><td>1.10G</td></tr></tbody></table>", "caption": "Table 1: Accuracy and FLOPs of Deit-S (Touvron et al. 2021a) on ImageNet with different No. of patches as inputs.", "list_citation_info": ["Touvron et al. (2021a) Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.; and J\u00e9gou, H. 2021a. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (ICML), 10347\u201310357."]}, {"table": "<table><tbody><tr><td>Model</td><td>Top-1 Acc.(%)</td><td>FLOPs(G)</td></tr><tr><td colspan=\"3\">DeiT-S</td></tr><tr><td>Baseline (Touvron et al. 2021a)</td><td>79.8</td><td>4.6</td></tr><tr><td>DynamicViT (Rao et al. 2021)</td><td>79.3</td><td>2.9</td></tr><tr><td>IA-RED{}^{2} (Pan et al. 2021a)</td><td>79.1</td><td>3.2</td></tr><tr><td>PS-ViT (Tang et al. 2021)</td><td>79.4</td><td>2.6</td></tr><tr><td>EVIT (Liang et al. 2022)</td><td>79.5</td><td>3.0</td></tr><tr><td>Evo-ViT (Xu et al. 2022)</td><td>79.4</td><td>3.0</td></tr><tr><td>CF-ViT(\\eta=0.5)(Ours)</td><td>79.8</td><td>1.8</td></tr><tr><td>CF-ViT(\\eta=0.75)(Ours)</td><td>80.7</td><td>2.6</td></tr><tr><td colspan=\"3\">LV-ViT-S</td></tr><tr><td>Baseline (Jiang et al. 2021)</td><td>83.3</td><td>6.6</td></tr><tr><td>DynamicViT (Rao et al. 2021)</td><td>83.0</td><td>4.6</td></tr><tr><td>EVIT (Liang et al. 2022)</td><td>83.0</td><td>4.7</td></tr><tr><td>SiT (Zong et al. 2021)</td><td>83.2</td><td>4.0</td></tr><tr><td>CF-ViT(\\eta=0.63)(Ours)</td><td>83.3</td><td>3.1</td></tr><tr><td>CF-ViT(\\eta=0.75)(Ours)</td><td>83.5</td><td>4.0</td></tr></tbody></table>", "caption": "Table 3:  Comparisons between existing token slimming based ViT compression methods and our CF-ViT. ", "list_citation_info": ["Jiang et al. (2021) Jiang, Z.-H.; Hou, Q.; Yuan, L.; Zhou, D.; Shi, Y.; Jin, X.; Wang, A.; and Feng, J. 2021. All tokens matter: Token labeling for training better vision transformers. In Advances in Neural Information Processing Systems (NeurIPS).", "Pan et al. (2021a) Pan, B.; Panda, R.; Jiang, Y.; Wang, Z.; Feris, R.; and Oliva, A. 2021a. IA-RED22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT: Interpretability-Aware Redundancy Reduction for Vision Transformers. In Advances in Neural Information Processing Systems (NeurIPS).", "Touvron et al. (2021a) Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.; and J\u00e9gou, H. 2021a. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (ICML), 10347\u201310357.", "Rao et al. (2021) Rao, Y.; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh, C.-J. 2021. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In Advances in Neural Information Processing Systems (NeurIPS).", "Xu et al. (2022) Xu, Y.; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.; Zhang, L.; Xu, C.; and Sun, X. 2022. Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).", "Zong et al. (2021) Zong, Z.; Li, K.; Song, G.; Wang, Y.; Qiao, Y.; Leng, B.; and Liu, Y. 2021. Self-slimmed Vision Transformer. arXiv preprint arXiv:2111.12624.", "Tang et al. (2021) Tang, Y.; Han, K.; Wang, Y.; Xu, C.; Guo, J.; Xu, C.; and Tao, D. 2021. Patch slimming for efficient vision transformers. In Advances in Neural Information Processing Systems (NeurIPS).", "Liang et al. (2022) Liang, Y.; GE, C.; Tong, Z.; Song, Y.; Wang, J.; and Xie, P. 2022. EviT: Expediting Vision Transformers via Token Reorganizations. In International Conference on Learning Representations (ICLR)."]}], "citation_info_to_title": {"Rao et al. (2021) Rao, Y.; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh, C.-J. 2021. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In Advances in Neural Information Processing Systems (NeurIPS).": "Dynamicvit: Efficient Vision Transformers with Dynamic Token Sparsification", "Xu et al. (2022) Xu, Y.; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.; Zhang, L.; Xu, C.; and Sun, X. 2022. Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer", "Zong et al. (2021) Zong, Z.; Li, K.; Song, G.; Wang, Y.; Qiao, Y.; Leng, B.; and Liu, Y. 2021. Self-slimmed Vision Transformer. arXiv preprint arXiv:2111.12624.": "Self-slimmed Vision Transformer", "Touvron et al. (2021a) Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.; and J\u00e9gou, H. 2021a. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (ICML), 10347\u201310357.": "Training data-efficient image transformers & distillation through attention", "Tang et al. (2021) Tang, Y.; Han, K.; Wang, Y.; Xu, C.; Guo, J.; Xu, C.; and Tao, D. 2021. Patch slimming for efficient vision transformers. In Advances in Neural Information Processing Systems (NeurIPS).": "Patch Slimming for Efficient Vision Transformers", "Liang et al. (2022) Liang, Y.; GE, C.; Tong, Z.; Song, Y.; Wang, J.; and Xie, P. 2022. EviT: Expediting Vision Transformers via Token Reorganizations. In International Conference on Learning Representations (ICLR).": "EviT: Expediting Vision Transformers via Token Reorganizations", "Jiang et al. (2021) Jiang, Z.-H.; Hou, Q.; Yuan, L.; Zhou, D.; Shi, Y.; Jin, X.; Wang, A.; and Feng, J. 2021. All tokens matter: Token labeling for training better vision transformers. In Advances in Neural Information Processing Systems (NeurIPS).": "All tokens matter: Token labeling for training better vision transformers", "Pan et al. (2021a) Pan, B.; Panda, R.; Jiang, Y.; Wang, Z.; Feris, R.; and Oliva, A. 2021a. IA-RED22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT: Interpretability-Aware Redundancy Reduction for Vision Transformers. In Advances in Neural Information Processing Systems (NeurIPS).": "IA-RED22{}^{2}start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT: Interpretability-Aware Redundancy Reduction for Vision Transformers"}, "source_title_to_arxiv_id": {"Self-slimmed Vision Transformer": "2111.12624", "All tokens matter: Token labeling for training better vision transformers": "2104.10858"}}