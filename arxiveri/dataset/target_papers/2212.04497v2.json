{"title": "UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation", "abstract": "Owing to the success of transformer models, recent works study their\napplicability in 3D medical segmentation tasks. Within the transformer models,\nthe self-attention mechanism is one of the main building blocks that strives to\ncapture long-range dependencies. However, the self-attention operation has\nquadratic complexity which proves to be a computational bottleneck, especially\nin volumetric medical imaging, where the inputs are 3D with numerous slices. In\nthis paper, we propose a 3D medical image segmentation approach, named UNETR++,\nthat offers both high-quality segmentation masks as well as efficiency in terms\nof parameters, compute cost, and inference speed. The core of our design is the\nintroduction of a novel efficient paired attention (EPA) block that efficiently\nlearns spatial and channel-wise discriminative features using a pair of\ninter-dependent branches based on spatial and channel attention. Our spatial\nattention formulation is efficient having linear complexity with respect to the\ninput sequence length. To enable communication between spatial and\nchannel-focused branches, we share the weights of query and key mapping\nfunctions that provide a complimentary benefit (paired attention), while also\nreducing the overall network parameters. Our extensive evaluations on five\nbenchmarks, Synapse, BTCV, ACDC, BRaTs, and Decathlon-Lung, reveal the\neffectiveness of our contributions in terms of both efficiency and accuracy. On\nSynapse, our UNETR++ sets a new state-of-the-art with a Dice Score of 87.2%,\nwhile being significantly efficient with a reduction of over 71% in terms of\nboth parameters and FLOPs, compared to the best method in the literature. Code:\nhttps://github.com/Amshaker/unetr_plus_plus.", "authors": ["Abdelrahman Shaker", "Muhammad Maaz", "Hanoona Rasheed", "Salman Khan", "Ming-Hsuan Yang", "Fahad Shahbaz Khan"], "published_date": "2022_12_08", "pdf_url": "http://arxiv.org/pdf/2212.04497v2", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Methods</th><th rowspan=\"2\">Params</th><th rowspan=\"2\">FLOPs</th><th rowspan=\"2\">Spl</th><th rowspan=\"2\">RKid</th><th rowspan=\"2\">LKid</th><th rowspan=\"2\">Gal</th><th rowspan=\"2\">Liv</th><th rowspan=\"2\">Sto</th><th rowspan=\"2\">Aor</th><th rowspan=\"2\">Pan</th><th colspan=\"2\">Average</th></tr><tr><th>HD95 \\downarrow</th><th>DSC \\uparrow</th></tr></thead><tbody><tr><th>U-Net [27]</th><td>-</td><td>-</td><td>86.67</td><td>68.60</td><td>77.77</td><td>69.72</td><td>93.43</td><td>75.58</td><td>89.07</td><td>53.98</td><td>-</td><td>76.85</td></tr><tr><th>TransUNet [5]</th><td>96.07</td><td>88.91</td><td>85.08</td><td>77.02</td><td>81.87</td><td>63.16</td><td>94.08</td><td>75.62</td><td>87.23</td><td>55.86</td><td>31.69</td><td>77.49</td></tr><tr><th>Swin-UNet [3]</th><td>-</td><td>-</td><td>90.66</td><td>79.61</td><td>83.28</td><td>66.53</td><td>94.29</td><td>76.60</td><td>85.47</td><td>56.58</td><td>21.55</td><td>79.13</td></tr><tr><th>UNETR [13]</th><td>92.49</td><td>75.76</td><td>85.00</td><td>84.52</td><td>85.60</td><td>56.30</td><td>94.57</td><td>70.46</td><td>89.80</td><td>60.47</td><td>18.59</td><td>78.35</td></tr><tr><th>MISSFormer [15]</th><td>-</td><td>-</td><td>91.92</td><td>82.00</td><td>85.21</td><td>68.65</td><td>94.41</td><td>80.81</td><td>86.99</td><td>65.67</td><td>18.20</td><td>81.96</td></tr><tr><th>Swin-UNETR [12]</th><td>62.83</td><td>384.2</td><td>95.37</td><td>86.26</td><td>86.99</td><td>66.54</td><td>95.72</td><td>77.01</td><td>91.12</td><td>68.80</td><td>10.55</td><td>83.48</td></tr><tr><th>nnFormer [34]</th><td>150.5</td><td>213.4</td><td>90.51</td><td>86.25</td><td>86.57</td><td>70.17</td><td>96.84</td><td>86.83</td><td>92.04</td><td>83.35</td><td>10.63</td><td>86.57</td></tr><tr><th>UNETR++</th><td>42.96</td><td>47.98</td><td>95.77</td><td>87.18</td><td>87.54</td><td>71.25</td><td>96.42</td><td>86.01</td><td>92.52</td><td>81.10</td><td>7.53</td><td>87.22</td></tr></tbody></table>", "caption": "Table 1: State-of-the-art comparison on the abdominal multi-organ Synapse dataset. We report both the segmentation performance (DSC, HD95) and model complexity (parameters and FLOPs).Our proposed UNETR++ achieves favorable segmentation performance against existing methods, while being considerably reducing the model complexity. Best results are in bold. Abbreviations stand for: Spl: spleen, RKid: right kidney, LKid: left kidney, Gal: gallbladder, Liv: liver, Sto: stomach, Aor: aorta, Pan: pancreas. Best results are in bold.", "list_citation_info": ["[3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European Conference on Computer Vision Workshops, 2022.", "[5] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.", "[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015.", "[12] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI Brainlesion Workshop, 2022.", "[34] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201, 2021.", "[15] Xiaohong Huang, Zhifang Deng, Dandan Li, and Xueguang Yuan. Missformer: An effective medical image segmentation transformer. arXiv preprint arXiv:2109.07162, 2021.", "[13] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022."]}, {"table": "<table><thead><tr><th>Methods</th><th>Spl</th><th>RKid</th><th>LKid</th><th>Gal</th><th>Eso</th><th>Liv</th><th>Sto</th><th>Aor</th><th>IVC</th><th>PSV</th><th>Pan</th><th>RAG</th><th>LAG</th><th>Avg</th></tr></thead><tbody><tr><th>nnUNet [16]</th><td>95.95</td><td>88.35</td><td>93.02</td><td>70.13</td><td>76.72</td><td>96.51</td><td>86.79</td><td>88.93</td><td>82.89</td><td>78.51</td><td>79.60</td><td>73.26</td><td>68.35</td><td>83.16</td></tr><tr><th>TransBTS [31]</th><td>94.55</td><td>89.20</td><td>90.97</td><td>68.38</td><td>75.61</td><td>96.44</td><td>83.52</td><td>88.55</td><td>82.48</td><td>74.21</td><td>76.02</td><td>67.23</td><td>67.03</td><td>81.31</td></tr><tr><th>UNETR [13]</th><td>90.48</td><td>82.51</td><td>86.05</td><td>58.23</td><td>71.21</td><td>94.64</td><td>72.06</td><td>86.57</td><td>76.51</td><td>70.37</td><td>66.06</td><td>66.25</td><td>63.04</td><td>76.00</td></tr><tr><th>Swin-UNETR [12]</th><td>94.59</td><td>88.97</td><td>92.39</td><td>65.37</td><td>75.43</td><td>95.61</td><td>75.57</td><td>88.28</td><td>81.61</td><td>76.30</td><td>74.52</td><td>68.23</td><td>66.02</td><td>80.44</td></tr><tr><th>nnFormer [34]</th><td>94.58</td><td>88.62</td><td>93.68</td><td>65.29</td><td>76.22</td><td>96.17</td><td>83.59</td><td>89.09</td><td>80.80</td><td>75.97</td><td>77.87</td><td>70.20</td><td>66.05</td><td>81.62</td></tr><tr><th>UNETR++</th><td>94.94</td><td>91.90</td><td>93.62</td><td>70.75</td><td>77.18</td><td>95.95</td><td>85.15</td><td>89.28</td><td>83.14</td><td>76.91</td><td>77.42</td><td>72.56</td><td>68.17</td><td>83.28</td></tr></tbody></table>", "caption": "Table 3: State-of-the-art comparison on the BTCV test set for multi-organ segmentation. All results are obtained using a single model accuracy and without any ensemble, pre-training or additional custom data. Our UNETR++ achieves favorable segmentation performance against existing 3D image segmentation methods. Abbreviations are as follows: Spl: spleen, RKid: right kidney, LKid: left kidney, Gal: gallbladder, Eso: esophagus, Liv: liver, Sto: stomach, Aor: aorta, IVC: the inferior vena cava, PSV: portal and splenic veins, Pan: pancreas, RAG: right adrenal gland, LAG: left adrenal gland. Results are obtained from BTCV leaderboard. Best results are in bold.", "list_citation_info": ["[16] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein. nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203\u2013211, 2021.", "[12] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI Brainlesion Workshop, 2022.", "[34] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201, 2021.", "[31] Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, and Jiangyun Li. Transbts: Multimodal brain tumor segmentation using transformer. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 2021.", "[13] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022."]}, {"table": "<table><thead><tr><th>Methods</th><th>RV</th><th>Myo</th><th>LV</th><th>Average</th></tr></thead><tbody><tr><th>TransUNet [5]</th><td>88.86</td><td>84.54</td><td>95.73</td><td>89.71</td></tr><tr><th>Swin-UNet [3]</th><td>88.55</td><td>85.62</td><td>95.83</td><td>90.00</td></tr><tr><th>UNETR [13]</th><td>88.49</td><td>82.04</td><td>91.62</td><td>87.38</td></tr><tr><th>MISSFormer [15]</th><td>86.36</td><td>85.75</td><td>91.59</td><td>87.90</td></tr><tr><th>nnFormer [34]</th><td>91.18</td><td>86.24</td><td>94.07</td><td>90.50</td></tr><tr><th>UNETR++</th><td>91.47</td><td>86.47</td><td>94.25</td><td>90.73</td></tr></tbody></table>", "caption": "Table 4: State-of-the-art comparison on ACDC. We report the performance on right ventricle (RV), left ventricle (LV) and myocardium (MYO) along with mean results using DSC metric. Our UNETR++ obtains better results compared to existing methods by achieving a mean DSC of 90.73%. Best results are in bold.", "list_citation_info": ["[3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European Conference on Computer Vision Workshops, 2022.", "[5] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.", "[34] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201, 2021.", "[15] Xiaohong Huang, Zhifang Deng, Dandan Li, and Xueguang Yuan. Missformer: An effective medical image segmentation transformer. arXiv preprint arXiv:2109.07162, 2021.", "[13] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022."]}], "citation_info_to_title": {"[12] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI Brainlesion Workshop, 2022.": "Swin UNetr: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images", "[5] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306, 2021.": "Transunet: Transformers make strong encoders for medical image segmentation", "[31] Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, and Jiangyun Li. Transbts: Multimodal brain tumor segmentation using transformer. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 2021.": "Transbts: Multimodal Brain Tumor Segmentation Using Transformer", "[16] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein. nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203\u2013211, 2021.": "nnu-net: a self-configuring method for deep learning-based biomedical image segmentation", "[27] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015.": "U-net: Convolutional networks for biomedical image segmentation", "[3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European Conference on Computer Vision Workshops, 2022.": "Swin-UNet: Unet-like Pure Transformer for Medical Image Segmentation", "[15] Xiaohong Huang, Zhifang Deng, Dandan Li, and Xueguang Yuan. Missformer: An effective medical image segmentation transformer. arXiv preprint arXiv:2109.07162, 2021.": "Missformer: An effective medical image segmentation transformer", "[34] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201, 2021.": "nnformer: Interleaved transformer for volumetric segmentation", "[13] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022.": "Unetr: Transformers for 3d medical image segmentation"}, "source_title_to_arxiv_id": {"Transunet: Transformers make strong encoders for medical image segmentation": "2102.04306", "Swin-UNet: Unet-like Pure Transformer for Medical Image Segmentation": "2105.05537", "nnformer: Interleaved transformer for volumetric segmentation": "2109.03201"}}