{"title": "Globally Consistent Video Depth and Pose Estimation with Efficient Test-Time Training", "abstract": "Dense depth and pose estimation is a vital prerequisite for various video\napplications. Traditional solutions suffer from the robustness of sparse\nfeature tracking and insufficient camera baselines in videos. Therefore, recent\nmethods utilize learning-based optical flow and depth prior to estimate dense\ndepth. However, previous works require heavy computation time or yield\nsub-optimal depth results. We present GCVD, a globally consistent method for\nlearning-based video structure from motion (SfM) in this paper. GCVD integrates\na compact pose graph into the CNN-based optimization to achieve globally\nconsistent estimation from an effective keyframe selection mechanism. It can\nimprove the robustness of learning-based methods with flow-guided keyframes and\nwell-established depth prior. Experimental results show that GCVD outperforms\nthe state-of-the-art methods on both depth and pose estimation. Besides, the\nruntime experiments reveal that it provides strong efficiency in both short-\nand long-term videos with global consistency provided.", "authors": ["Yao-Chih Lee", "Kuan-Wei Tseng", "Guan-Sheng Chen", "Chu-Song Chen"], "published_date": "2022_08_04", "pdf_url": "http://arxiv.org/pdf/2208.02709v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><th rowspan=\"3\">knownpose?</th><td colspan=\"4\">Depth Metrics</td><td colspan=\"3\">Pose Metrics</td></tr><tr><td rowspan=\"2\"><p>AbsRel\\downarrow</p></td><td rowspan=\"2\"><p>SqRel\\downarrow</p></td><td rowspan=\"2\"><p>RMSE\\downarrow</p></td><td><p>\\delta&lt;</p></td><td><p>ATE</p></td><td><p>RPE Trans</p></td><td><p>RPE Rot</p></td></tr><tr><td><p>1.25\\uparrow</p></td><td><p>(m)\\downarrow</p></td><td><p>(m) \\downarrow</p></td><td><p>(deg) \\downarrow</p></td></tr><tr><th>DPSNet\\dagger (Im et al. 2019)</th><th>\u2713</th><td><p>0.199</p></td><td><p>0.142</p></td><td><p>0.438</p></td><td><p>0.710</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>CNMNet\\dagger (Long et al. 2020)</th><th>\u2713</th><td><p>0.161</p></td><td><p>0.083</p></td><td><p>0.361</p></td><td><p>0.766</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>NeuralRecon\\dagger (Sun et al. 2021)</th><th>\u2713</th><td><p>0.155</p></td><td><p>0.104</p></td><td><p>0.347</p></td><td><p>0.820</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><th>DeepV2D (Teed and Deng 2019)</th><th></th><td><p>0.162</p></td><td><p>0.092</p></td><td><p>0.380</p></td><td><p>0.767</p></td><td><p>0.471</p></td><td><p>1.018</p></td><td><p>60.979</p></td></tr><tr><th>DROID-SLAM (Teed and Deng 2021)</th><th></th><td><p>0.209</p></td><td><p>0.132</p></td><td><p>0.462</p></td><td><p>0.665</p></td><td><p>0.463</p></td><td><p>0.928</p></td><td><p>40.143</p></td></tr><tr><th>Deep3D (Lee et al. 2021)</th><th></th><td><p>0.172</p></td><td><p>0.105</p></td><td><p>0.406</p></td><td><p>0.748</p></td><td><p>0.310</p></td><td><p>0.306</p></td><td><p>8.665</p></td></tr><tr><th>CVD2 (Kopf, Rong, and Huang 2021)</th><th></th><td><p>0.154</p></td><td><p>0.085</p></td><td><p>0.379</p></td><td><p>0.795</p></td><td><p>0.375</p></td><td><p>0.517</p></td><td><p>31.102</p></td></tr><tr><th>Ours (GCVD)</th><th></th><td>0.124</td><td>0.054</td><td>0.307</td><td>0.858</td><td>0.249</td><td>0.257</td><td>8.155</td></tr></tbody></table>", "caption": "Table 1: Quantitative evaluations of depth and pose on 7-Scenes dataset (Shotton et al. 2013). The standard depth evaluation measures per-frame errors and accuracy metrics. The pose evaluation computes the per-sequence pose errors.\\daggerWe also refer to the approaches of multi-view stereo and 3D reconstruction with known camera pose and compare with the results reported in NeuralRecon (Sun et al. 2021).", "list_citation_info": ["Sun et al. (2021) Sun, J.; Xie, Y.; Chen, L.; Zhou, X.; and Bao, H. 2021. NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video. In CVPR.", "Teed and Deng (2021) Teed, Z.; and Deng, J. 2021. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. NeurIPS.", "Lee et al. (2021) Lee, Y.-C.; Tseng, K.-W.; Chen, Y.-T.; Chen, C.-C.; Chen, C.-S.; and Hung, Y.-P. 2021. 3D Video Stabilization With Depth Estimation by CNN-Based Optimization. In CVPR.", "Long et al. (2020) Long, X.; Liu, L.; Theobalt, C.; and Wang, W. 2020. Occlusion-aware depth estimation with adaptive normal constraints. In ECCV.", "Shotton et al. (2013) Shotton, J.; Glocker, B.; Zach, C.; Izadi, S.; Criminisi, A.; and Fitzgibbon, A. 2013. Scene coordinate regression forests for camera relocalization in RGB-D images. In CVPR.", "Kopf, Rong, and Huang (2021) Kopf, J.; Rong, X.; and Huang, J.-B. 2021. Robust consistent video depth estimation. In CVPR.", "Im et al. (2019) Im, S.; Jeon, H.-G.; Lin, S.; and Kweon, I. S. 2019. Dpsnet: End-to-end deep plane sweep stereo. arXiv preprint arXiv:1905.00538.", "Teed and Deng (2019) Teed, Z.; and Deng, J. 2019. DeepV2D: Video to Depth with Differentiable Structure from Motion. In International Conference on Learning Representations."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Sequence</th><th colspan=\"4\">Pose Error (ATE in meters) \\downarrow</th><th colspan=\"3\">Depth Error (Abs Rel) \\downarrow</th></tr><tr><th>COLMAP</th><th><p>Deep3D</p></th><th><p>CVD2</p></th><th><p>GCVD</p></th><th><p>Deep3D</p></th><th><p>CVD2</p></th><th><p>GCVD</p></th></tr></thead><tbody><tr><th>fr1_desk</th><th>0.019</th><td><p>0.580</p></td><td><p>0.273</p></td><td><p>0.229</p></td><td><p>0.1940</p></td><td><p>0.1090</p></td><td><p>0.0940</p></td></tr><tr><th>fr1_desk2</th><th>0.027</th><td><p>0.611</p></td><td><p>0.314</p></td><td><p>0.156</p></td><td><p>0.2282</p></td><td><p>0.1139</p></td><td><p>0.1305</p></td></tr><tr><th>fr2_desk</th><th>0.818</th><td><p>0.260</p></td><td><p>0.613</p></td><td><p>0.422</p></td><td><p>0.0973</p></td><td><p>0.1544</p></td><td><p>0.1130</p></td></tr><tr><th>fr3_cabinet</th><th>failed</th><td><p>1.225</p></td><td><p>0.444</p></td><td><p>0.850</p></td><td><p>0.4613</p></td><td><p>0.1236</p></td><td><p>0.1832</p></td></tr><tr><th>fr3_nstr_tex_near_loop</th><th>0.015</th><td><p>0.694</p></td><td><p>0.491</p></td><td><p>0.399</p></td><td><p>0.2437</p></td><td><p>0.0615</p></td><td><p>0.0352</p></td></tr><tr><th>fr3_sitting_static</th><th>0.033</th><td><p>0.006</p></td><td><p>0.029</p></td><td><p>0.006</p></td><td><p>0.2368</p></td><td><p>0.1260</p></td><td><p>0.1243</p></td></tr><tr><th>fr3_str_tex_far</th><th>0.008</th><td><p>0.089</p></td><td><p>0.169</p></td><td><p>0.131</p></td><td><p>0.0511</p></td><td><p>0.0742</p></td><td><p>0.0810</p></td></tr><tr><th>Mean</th><th>0.153</th><td><p>0.495</p></td><td><p>0.333</p></td><td>0.313</td><td><p>0.2160</p></td><td><p>0.1089</p></td><td>0.1087</td></tr></tbody></table>", "caption": "Table 2: Quantitative evaluation on TUM RGB-D (Sturm et al. 2012). We present the depth and pose errors of each sequence and compare with Deep3D (Lee et al. 2021), and CVD2 (Kopf, Rong, and Huang 2021). We additionally provide the pose results of COLMAP (Schonberger and Frahm 2016) as reference.", "list_citation_info": ["Kopf, Rong, and Huang (2021) Kopf, J.; Rong, X.; and Huang, J.-B. 2021. Robust consistent video depth estimation. In CVPR.", "Sturm et al. (2012) Sturm, J.; Engelhard, N.; Endres, F.; Burgard, W.; and Cremers, D. 2012. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS).", "Lee et al. (2021) Lee, Y.-C.; Tseng, K.-W.; Chen, Y.-T.; Chen, C.-C.; Chen, C.-S.; and Hung, Y.-P. 2021. 3D Video Stabilization With Depth Estimation by CNN-Based Optimization. In CVPR.", "Schonberger and Frahm (2016) Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-from-motion revisited. In CVPR."]}, {"table": "<table><tbody><tr><td colspan=\"5\">Ablation settings</td><td colspan=\"2\">Errors \\downarrow</td></tr><tr><td rowspan=\"2\"><p>KF</p></td><td><p>layer</p></td><td><p>mesh</p></td><td><p>grad.</p></td><td rowspan=\"2\"><p>PGO</p></td><td>Depth</td><td>Pose</td></tr><tr><td><p>norm</p></td><td><p>deform.</p></td><td><p>loss</p></td><td>AbsRel</td><td>ATE</td></tr><tr><td><p>\u2713</p></td><td></td><td></td><td></td><td></td><td><p>0.208</p></td><td><p>0.555</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td></td><td></td><td><p>0.137</p></td><td><p>0.332</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td></td><td><p>0.132</p></td><td><p>0.327</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>0.125</p></td><td><p>0.277</p></td></tr><tr><td></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td></td><td><p>0.127</p></td><td><p>0.308</p></td></tr><tr><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td><p>\u2713</p></td><td>0.124</td><td>0.249</td></tr></tbody></table>", "caption": "Table 3: Ablation studies on 7-Scenes (Shotton et al. 2013). We validate the effectiveness of the keyframe strategy (KF), the added layer normalization and mesh deformation in the depth component, depth gradient loss, and pose graph optimization (PGO) in our pipeline.", "list_citation_info": ["Shotton et al. (2013) Shotton, J.; Glocker, B.; Zach, C.; Izadi, S.; Criminisi, A.; and Fitzgibbon, A. 2013. Scene coordinate regression forests for camera relocalization in RGB-D images. In CVPR."]}, {"table": "<table><tbody><tr><th></th><td><p>COLMAP</p></td><td><p>ORB-</p></td><td><p>Deep3D</p></td><td><p>CVD2</p></td><td><p>Our</p></td></tr><tr><th rowspan=\"-2\">Sequence</th><td></td><td><p>SLAM2</p></td><td></td><td></td><td><p>GCVD</p></td></tr><tr><th>fr1_desk</th><td><p>0.019</p></td><td><p>0.013</p></td><td><p>0.580</p></td><td><p>0.273</p></td><td><p>0.229</p></td></tr><tr><th>fr1_desk2</th><td><p>0.027</p></td><td><p>failed</p></td><td><p>0.611</p></td><td><p>0.314</p></td><td><p>0.156</p></td></tr><tr><th>fr2_desk</th><td><p>0.818</p></td><td><p>0.009</p></td><td><p>0.260</p></td><td><p>0.613</p></td><td><p>0.422</p></td></tr><tr><th>fr3_cabinet</th><td><p>failed</p></td><td><p>failed</p></td><td><p>1.225</p></td><td><p>0.444</p></td><td><p>0.850</p></td></tr><tr><th>fr3_nstr_tex_near_loop</th><td><p>0.015</p></td><td><p>0.010</p></td><td><p>0.694</p></td><td><p>0.491</p></td><td><p>0.399</p></td></tr><tr><th>fr3_sitting_static</th><td><p>0.033</p></td><td><p>0.023</p></td><td><p>0.006</p></td><td><p>0.029</p></td><td><p>0.006</p></td></tr><tr><th>fr3_str_tex_far</th><td><p>0.008</p></td><td><p>0.009</p></td><td><p>0.089</p></td><td><p>0.169</p></td><td><p>0.131</p></td></tr><tr><th>Mean</th><td></td><td></td><td></td><td></td><td></td></tr><tr><th>(exclude fr1_desk2,fr3_cabinet)</th><td rowspan=\"-2\"><p>0.179</p></td><td rowspan=\"-2\"><p>0.013</p></td><td rowspan=\"-2\"><p>0.326</p></td><td rowspan=\"-2\"><p>0.315</p></td><td rowspan=\"-2\"><p>0.237</p></td></tr></tbody></table>", "caption": "Table 6: Comparison of Absolute Trajectory Error (ATE) with traditional COLMAP (Schonberger and Frahm 2016) and ORB-SLAM2 (Mur-Artal, Montiel, and Tardos 2015) on TUM RGBD dataset (Sturm et al. 2012). We only show the pose results since both COLMAP and ORB-SLAM2 perform 3D reconstruction with sparse point clouds instead of dense depths.", "list_citation_info": ["Sturm et al. (2012) Sturm, J.; Engelhard, N.; Endres, F.; Burgard, W.; and Cremers, D. 2012. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS).", "Mur-Artal, Montiel, and Tardos (2015) Mur-Artal, R.; Montiel, J. M. M.; and Tardos, J. D. 2015. ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE Transactions on Robotics.", "Schonberger and Frahm (2016) Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-from-motion revisited. In CVPR."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Sequence</th><th><p>MH</p></th><th><p>MH</p></th><th><p>MH</p></th><th><p>MH</p></th><th><p>MH</p></th><th><p>V1</p></th><th><p>V1</p></th><th><p>V1</p></th><th><p>V2</p></th><th><p>V2</p></th><th><p>V2</p></th><th rowspan=\"2\">Mean</th></tr><tr><th><p>01</p></th><th><p>02</p></th><th><p>03</p></th><th><p>04</p></th><th><p>05</p></th><th><p>01</p></th><th><p>02</p></th><th><p>03</p></th><th><p>01</p></th><th><p>02</p></th><th><p>03</p></th></tr></thead><tbody><tr><th>Deep3D</th><td><p>3.24</p></td><td><p>3.82</p></td><td><p>2.82</p></td><td><p>4.26</p></td><td><p>5.06</p></td><td><p>1.53</p></td><td><p>1.57</p></td><td><p>1.32</p></td><td><p>1.97</p></td><td><p>1.87</p></td><td><p>1.73</p></td><td><p>2.65</p></td></tr><tr><th>CVD2</th><td><p>1.48</p></td><td><p>1.32</p></td><td><p>2.63</p></td><td><p>4.20</p></td><td><p>4.31</p></td><td><p>0.94</p></td><td><p>1.63</p></td><td><p>1.19</p></td><td><p>1.43</p></td><td><p>1.56</p></td><td><p>1.74</p></td><td><p>2.04</p></td></tr><tr><th>Our GCVD</th><td><p>1.33</p></td><td><p>1.72</p></td><td><p>1.99</p></td><td><p>3.78</p></td><td><p>4.59</p></td><td><p>1.11</p></td><td><p>1.07</p></td><td><p>1.33</p></td><td><p>0.96</p></td><td><p>1.88</p></td><td><p>1.46</p></td><td>1.93</td></tr></tbody></table>", "caption": "Table 7: Pose evaluation on EuRoC dataset (Burri et al. 2016). The per-sequence absolute trajectory errors (ATE) are reported in meters.", "list_citation_info": ["Burri et al. (2016) Burri, M.; Nikolic, J.; Gohl, P.; Schneider, T.; Rehder, J.; Omari, S.; Achtelik, M. W.; and Siegwart, R. 2016. The EuRoC micro aerial vehicle datasets. The International Journal of Robotics Research (IJRR)."]}], "citation_info_to_title": {"Sturm et al. (2012) Sturm, J.; Engelhard, N.; Endres, F.; Burgard, W.; and Cremers, D. 2012. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS).": "A Benchmark for the Evaluation of RGB-D SLAM Systems", "Teed and Deng (2021) Teed, Z.; and Deng, J. 2021. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. NeurIPS.": "Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras", "Long et al. (2020) Long, X.; Liu, L.; Theobalt, C.; and Wang, W. 2020. Occlusion-aware depth estimation with adaptive normal constraints. In ECCV.": "Occlusion-aware depth estimation with adaptive normal constraints", "Shotton et al. (2013) Shotton, J.; Glocker, B.; Zach, C.; Izadi, S.; Criminisi, A.; and Fitzgibbon, A. 2013. Scene coordinate regression forests for camera relocalization in RGB-D images. In CVPR.": "Scene coordinate regression forests for camera relocalization in RGB-D images", "Kopf, Rong, and Huang (2021) Kopf, J.; Rong, X.; and Huang, J.-B. 2021. Robust consistent video depth estimation. In CVPR.": "Robust consistent video depth estimation", "Teed and Deng (2019) Teed, Z.; and Deng, J. 2019. DeepV2D: Video to Depth with Differentiable Structure from Motion. In International Conference on Learning Representations.": "DeepV2D: Video to Depth with Differentiable Structure from Motion", "Mur-Artal, Montiel, and Tardos (2015) Mur-Artal, R.; Montiel, J. M. M.; and Tardos, J. D. 2015. ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE Transactions on Robotics.": "ORB-SLAM: a versatile and accurate monocular SLAM system", "Burri et al. (2016) Burri, M.; Nikolic, J.; Gohl, P.; Schneider, T.; Rehder, J.; Omari, S.; Achtelik, M. W.; and Siegwart, R. 2016. The EuRoC micro aerial vehicle datasets. The International Journal of Robotics Research (IJRR).": "The EuRoC micro aerial vehicle datasets", "Lee et al. (2021) Lee, Y.-C.; Tseng, K.-W.; Chen, Y.-T.; Chen, C.-C.; Chen, C.-S.; and Hung, Y.-P. 2021. 3D Video Stabilization With Depth Estimation by CNN-Based Optimization. In CVPR.": "3D Video Stabilization With Depth Estimation by CNN-Based Optimization", "Im et al. (2019) Im, S.; Jeon, H.-G.; Lin, S.; and Kweon, I. S. 2019. Dpsnet: End-to-end deep plane sweep stereo. arXiv preprint arXiv:1905.00538.": "Dpsnet: End-to-end deep plane sweep stereo", "Schonberger and Frahm (2016) Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-from-motion revisited. In CVPR.": "Structure-from-motion revisited", "Sun et al. (2021) Sun, J.; Xie, Y.; Chen, L.; Zhou, X.; and Bao, H. 2021. NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video. In CVPR.": "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video"}, "source_title_to_arxiv_id": {"Robust consistent video depth estimation": "2012.05901", "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video": "2104.00681"}}