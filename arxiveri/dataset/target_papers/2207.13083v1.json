{"title": "Task Agnostic and Post-hoc Unseen Distribution Detection", "abstract": "Despite the recent advances in out-of-distribution(OOD) detection, anomaly\ndetection, and uncertainty estimation tasks, there do not exist a task-agnostic\nand post-hoc approach. To address this limitation, we design a novel\nclustering-based ensembling method, called Task Agnostic and Post-hoc Unseen\nDistribution Detection (TAPUDD) that utilizes the features extracted from the\nmodel trained on a specific task. Explicitly, it comprises of TAP-Mahalanobis,\nwhich clusters the training datasets' features and determines the minimum\nMahalanobis distance of the test sample from all clusters. Further, we propose\nthe Ensembling module that aggregates the computation of iterative\nTAP-Mahalanobis for a different number of clusters to provide reliable and\nefficient cluster computation. Through extensive experiments on synthetic and\nreal-world datasets, we observe that our approach can detect unseen samples\neffectively across diverse tasks and performs better or on-par with the\nexisting baselines. To this end, we eliminate the necessity of determining the\noptimal value of the number of clusters and demonstrate that our method is more\nviable for large-scale classification tasks.", "authors": ["Radhika Dua", "Seongjun Yang", "Yixuan Li", "Edward Choi"], "published_date": "2022_07_26", "pdf_url": "http://arxiv.org/pdf/2207.13083v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Brightness</th><td colspan=\"7\">Baselines</td><td colspan=\"3\">Ours (Task-Agnostic)</td></tr><tr><th></th><td>MSP [22]</td><td>ODIN [31]</td><td>Energy [33]</td><td>MB [29]</td><td>KL [21]</td><td>MOS [26]</td><td>Gram [5]</td><td>TAP-MOS</td><td>TAP-MB</td><td>TAPUDD</td></tr><tr><th></th><td></td><td></td><td></td><td></td><td></td><td>(K = 8)</td><td></td><td>(K = 8)</td><td>(K = 8)</td><td>(Average)</td></tr><tr><th>0.0</th><td>88.7\\pm4.8</td><td>88.7\\pm4.8</td><td>88.2\\pm5.3</td><td>99.9\\pm0.1</td><td>26.3\\pm32.8</td><td>89.3\\pm5.5</td><td>99.3\\pm1.4</td><td>63.1\\pm26</td><td>99.9\\pm0.1</td><td>100.0\\pm0.1</td></tr><tr><th>0.2</th><td>66.1\\pm3.5</td><td>66.1\\pm3.5</td><td>66.0\\pm3.7</td><td>87.5\\pm4.5</td><td>44.5\\pm3</td><td>65.9\\pm3.2</td><td>61.0\\pm3.3</td><td>63.6\\pm10</td><td>86.8\\pm4.7</td><td>87.3\\pm5.2</td></tr><tr><th>0.4</th><td>56.3\\pm1.4</td><td>56.4\\pm1.4</td><td>56.2\\pm1.7</td><td>70.5\\pm3.8</td><td>46.9\\pm1.2</td><td>56.4\\pm1.1</td><td>53.4\\pm1.1</td><td>56.3\\pm4.9</td><td>69.6\\pm3.7</td><td>70.1\\pm4.5</td></tr><tr><th>0.6</th><td>52.4\\pm0.8</td><td>52.4\\pm0.8</td><td>52.3\\pm0.9</td><td>59.9\\pm2.5</td><td>48.2\\pm1</td><td>52.5\\pm0.8</td><td>51.4\\pm0.5</td><td>52.7\\pm2.4</td><td>59.3\\pm2.5</td><td>59.4\\pm2.7</td></tr><tr><th>0.8</th><td>50.4\\pm0.4</td><td>50.4\\pm0.4</td><td>50.4\\pm0.4</td><td>52.2\\pm1.4</td><td>48.8\\pm0.6</td><td>50.5\\pm0.3</td><td>50.2\\pm0.5</td><td>50.5\\pm1.2</td><td>52.0\\pm1.7</td><td>52.0\\pm1.6</td></tr><tr><th>1.0</th><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td></tr><tr><th>1.2</th><td>51.7\\pm0.4</td><td>51.7\\pm0.4</td><td>51.7\\pm0.4</td><td>55.4\\pm1.6</td><td>49.2\\pm0.5</td><td>51.7\\pm0.5</td><td>51.1\\pm0.6</td><td>51.2\\pm0.9</td><td>56.1\\pm1.5</td><td>56.0\\pm1.5</td></tr><tr><th>1.4</th><td>55.8\\pm0.8</td><td>55.8\\pm0.8</td><td>55.8\\pm0.8</td><td>62.9\\pm2.1</td><td>48.2\\pm1.2</td><td>55.8\\pm0.8</td><td>53.6\\pm1.1</td><td>53.6\\pm1.7</td><td>63.7\\pm2.0</td><td>63.5\\pm2.1</td></tr><tr><th>1.6</th><td>59.7\\pm1.3</td><td>59.7\\pm1.3</td><td>59.8\\pm1.4</td><td>70.2\\pm2.7</td><td>47.5\\pm1.7</td><td>59.6\\pm1.1</td><td>55.9\\pm1.2</td><td>55.8\\pm2.4</td><td>70.9\\pm2.8</td><td>70.7\\pm2.9</td></tr><tr><th>1.8</th><td>63.1\\pm2.0</td><td>63.1\\pm2.1</td><td>63.2\\pm2.2</td><td>76.5\\pm2.9</td><td>48.3\\pm2.4</td><td>62.8\\pm1.7</td><td>58.1\\pm1.5</td><td>57.0\\pm4.0</td><td>76.9\\pm3.4</td><td>76.6\\pm3.5</td></tr><tr><th>2.0</th><td>65.5\\pm3.2</td><td>65.6\\pm3.2</td><td>65.7\\pm3.5</td><td>81.6\\pm2.7</td><td>49.8\\pm2.9</td><td>65.1\\pm2.6</td><td>60.5\\pm1.8</td><td>57.7\\pm6.0</td><td>81.8\\pm3.7</td><td>81.4\\pm3.8</td></tr><tr><th>2.5</th><td>69.5\\pm6.5</td><td>69.5\\pm6.5</td><td>69.6\\pm6.8</td><td>90.4\\pm2.5</td><td>51.6\\pm4.9</td><td>69.0\\pm5.5</td><td>65.4\\pm4.4</td><td>60.0\\pm9.0</td><td>89.9\\pm3.8</td><td>89.6\\pm4.1</td></tr><tr><th>3.0</th><td>72.5\\pm8.7</td><td>72.5\\pm8.7</td><td>72.6\\pm9.0</td><td>94.8\\pm1.8</td><td>51.3\\pm5.4</td><td>72.0\\pm7.6</td><td>69.6\\pm5.9</td><td>63.0\\pm11.5</td><td>93.9\\pm3.8</td><td>93.6\\pm4.0</td></tr><tr><th>3.5</th><td>73.7\\pm9.7</td><td>73.7\\pm9.7</td><td>73.6\\pm10</td><td>96.8\\pm1.3</td><td>52.0\\pm6.1</td><td>73.0\\pm8.8</td><td>72.2\\pm6.8</td><td>64.3\\pm13.6</td><td>95.5\\pm3.8</td><td>95.4\\pm3.7</td></tr><tr><th>4.0</th><td>75.8\\pm9.5</td><td>75.8\\pm9.5</td><td>75.7\\pm9.8</td><td>97.8\\pm0.8</td><td>50.5\\pm7.2</td><td>75.3\\pm8.8</td><td>75.1\\pm7.9</td><td>66.3\\pm14.4</td><td>96.5\\pm3.6</td><td>96.5\\pm3.2</td></tr><tr><th>4.5</th><td>78.1\\pm7.9</td><td>78.1\\pm7.9</td><td>78.0\\pm8.3</td><td>98.5\\pm0.5</td><td>47.4\\pm8.2</td><td>77.8\\pm7.5</td><td>78.0\\pm7.1</td><td>68.7\\pm13.7</td><td>97.3\\pm3.0</td><td>97.4\\pm2.4</td></tr><tr><th>5.0</th><td>79.9\\pm6.4</td><td>79.9\\pm6.4</td><td>79.8\\pm6.9</td><td>98.8\\pm0.4</td><td>44.9\\pm8.4</td><td>79.8\\pm6.1</td><td>80.4\\pm6.6</td><td>70.3\\pm12.8</td><td>97.9\\pm2.5</td><td>98.0\\pm1.7</td></tr><tr><th>5.5</th><td>81.4\\pm5.6</td><td>81.4\\pm5.6</td><td>81.3\\pm6.2</td><td>99.0\\pm0.4</td><td>44.1\\pm8.7</td><td>81.3\\pm5.4</td><td>82.4\\pm6.6</td><td>71.3\\pm12.7</td><td>98.2\\pm2.2</td><td>98.4\\pm1.2</td></tr><tr><th>6.0</th><td>82.5\\pm5.1</td><td>82.5\\pm5.1</td><td>82.5\\pm5.6</td><td>99.1\\pm0.4</td><td>43.6\\pm8.6</td><td>82.4\\pm4.9</td><td>83.9\\pm6.3</td><td>71.9\\pm12.8</td><td>98.5\\pm1.9</td><td>98.7\\pm0.9</td></tr><tr><th>6.5</th><td>83.2\\pm4.9</td><td>83.2\\pm4.9</td><td>83.2\\pm5.4</td><td>99.2\\pm0.4</td><td>44.3\\pm8.2</td><td>83.1\\pm4.6</td><td>85.0\\pm6.2</td><td>72.2\\pm13.1</td><td>98.7\\pm1.7</td><td>98.9\\pm0.7</td></tr><tr><th>Average</th><td>67.8</td><td>67.8</td><td>67.8</td><td>82.1</td><td>46.9</td><td>67.7</td><td>66.8</td><td>61.0</td><td>81.7</td><td>82.0</td></tr></tbody></table>", "caption": "Table 1: NAS detection performance in binary classification task for NAS shift of brightness in RSNA boneage dataset measured by AUROC. Highlighted row presents the performance on ID data. MB and TAP-MB refers to Mahalanobis and TAP-Mahalanobis, respectively. Our task-agnostic approach significantly outperforms all baselines (except MB) and is comparable to MB. Note that MB is task-specific and cannot be used in tasks other than classification.", "list_citation_info": ["[5] Sastry Shama Chandramouli and Oore Sageev. Detecting out-of-distribution examples with gram matrices. In Proceedings of the International Conference on Machine Learning (ICML), 2020.", "[33] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.", "[22] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.", "[26] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8706\u20138715, 2021.", "[21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Xiaodong Song. A benchmark for anomaly segmentation. ArXiv, abs/1911.11132, 2019.", "[31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.", "[29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018."]}, {"table": "<table><tbody><tr><th>Brightness</th><td colspan=\"3\">Baselines</td><td colspan=\"3\">Ours (Task-Agnostic)</td></tr><tr><th></th><td>DE [28]</td><td>MC Dropout [14]</td><td>\\text{SWAG}^{*} [36]</td><td>TAP-MOS</td><td>TAP-MB</td><td>TAPUDD</td></tr><tr><th></th><td></td><td></td><td></td><td>(K = 8)</td><td>(K = 8)</td><td>(Average)</td></tr><tr><th>0.0</th><td>100.0\\pmNA</td><td>6.9\\pmNA</td><td>99.9\\pmNA</td><td>57.8\\pm31.5</td><td>100.0\\pm0.1</td><td>100.0\\pm0.0</td></tr><tr><th>0.2</th><td>57.0\\pmNA</td><td>45.5\\pmNA</td><td>51.4\\pmNA</td><td>68.7\\pm18.0</td><td>87.9\\pm6.1</td><td>88.8\\pm6.7</td></tr><tr><th>0.4</th><td>51.3\\pmNA</td><td>50.8\\pmNA</td><td>49.8\\pmNA</td><td>70.7\\pm16.4</td><td>64.5\\pm6.9</td><td>66.6\\pm5.0</td></tr><tr><th>0.6</th><td>50.7\\pmNA</td><td>49.7\\pmNA</td><td>49.5\\pmNA</td><td>65.3\\pm11.5</td><td>54.6\\pm4.4</td><td>55.1\\pm2.5</td></tr><tr><th>0.8</th><td>50.5\\pmNA</td><td>49.9\\pmNA</td><td>49.7\\pmNA</td><td>57.7\\pm6.0</td><td>48.9\\pm1.7</td><td>49.2\\pm1.0</td></tr><tr><th>1.0</th><td>50.0\\pmNA</td><td>49.8\\pmNA</td><td>50.0\\pmNA</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td></tr><tr><th>1.2</th><td>50.3\\pmNA</td><td>48.5\\pmNA</td><td>50.8\\pmNA</td><td>48.7\\pm4.0</td><td>57.6\\pm1.8</td><td>57.8\\pm1.9</td></tr><tr><th>1.4</th><td>54.5\\pmNA</td><td>46.7\\pmNA</td><td>55.8\\pmNA</td><td>50.8\\pm8.0</td><td>68.4\\pm3.4</td><td>68.4\\pm3.4</td></tr><tr><th>1.6</th><td>58.6\\pmNA</td><td>44.5\\pmNA</td><td>63.5\\pmNA</td><td>55.4\\pm11.3</td><td>78.7\\pm3.6</td><td>78.6\\pm3.7</td></tr><tr><th>1.8</th><td>64.9\\pmNA</td><td>41.6\\pmNA</td><td>71.6\\pmNA</td><td>62.1\\pm14.5</td><td>86.4\\pm3.5</td><td>86.3\\pm3.6</td></tr><tr><th>2.0</th><td>75.8\\pmNA</td><td>38.4\\pmNA</td><td>79.3\\pmNA</td><td>67.3\\pm16.8</td><td>91.9\\pm3.0</td><td>91.7\\pm3.2</td></tr><tr><th>2.5</th><td>95.6\\pmNA</td><td>31.1\\pmNA</td><td>89.8\\pmNA</td><td>76.2\\pm16.4</td><td>97.5\\pm1.5</td><td>97.4\\pm1.4</td></tr><tr><th>3.0</th><td>98.4\\pmNA</td><td>25.8\\pmNA</td><td>90.7\\pmNA</td><td>82.8\\pm13.5</td><td>99.0\\pm0.6</td><td>99.0\\pm0.5</td></tr><tr><th>3.5</th><td>99.3\\pmNA</td><td>21.7\\pmNA</td><td>93.7\\pmNA</td><td>88.1\\pm10.2</td><td>99.4\\pm0.3</td><td>99.4\\pm0.3</td></tr><tr><th>4.0</th><td>99.8\\pmNA</td><td>18.0\\pmNA</td><td>96.4\\pmNA</td><td>90.7\\pm6.8</td><td>99.6\\pm0.3</td><td>99.6\\pm0.2</td></tr><tr><th>4.5</th><td>100.0\\pmNA</td><td>14.9\\pmNA</td><td>97.4\\pmNA</td><td>91.7\\pm4.4</td><td>99.7\\pm0.2</td><td>99.7\\pm0.1</td></tr><tr><th>5.0</th><td>100.0\\pmNA</td><td>11.7\\pmNA</td><td>98.1\\pmNA</td><td>91.7\\pm3.8</td><td>99.8\\pm0.1</td><td>99.7\\pm0.1</td></tr><tr><th>5.5</th><td>100.0\\pmNA</td><td>9.7\\pmNA</td><td>98.5\\pmNA</td><td>91.0\\pm4.5</td><td>99.8\\pm0.1</td><td>99.8\\pm0.2</td></tr><tr><th>6.0</th><td>100.0\\pmNA</td><td>7.9\\pmNA</td><td>98.7\\pmNA</td><td>89.7\\pm5.7</td><td>99.8\\pm0.1</td><td>99.8\\pm0.2</td></tr><tr><th>6.5</th><td>100.0\\pmNA</td><td>7.0\\pmNA</td><td>98.9\\pmNA</td><td>88.3\\pm7.0</td><td>99.8\\pm0.2</td><td>99.8\\pm0.3</td></tr><tr><th>Average</th><td>77.8</td><td>31.0</td><td>76.7</td><td>72.2</td><td>84.2</td><td>84.3</td></tr></tbody></table>", "caption": "Table 2: NAS detection performance in regression task (age prediction) for NAS shift of brightness in RSNA boneage dataset measured by AUROC. Highlighted row presents the performance on the ID dataset. DE, MC Dropout, TAP-MB, and NA denotes Deep Ensemble, Monte Carlo Dropout, TAP-Mahalanobis, and Not Applicable respectively. \\text{SWAG}^{*} = SWAG + Deep Ensemble.", "list_citation_info": ["[36] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 13153\u201313164, 2019.", "[14] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the International Conference on Machine Learning (ICML), 2016.", "[28] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NeurIPS), 2017."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"2\">iNaturalist</td><td colspan=\"2\">SUN</td><td colspan=\"2\">Places</td><td colspan=\"2\">Textures</td><td colspan=\"2\">Average</td></tr><tr><td>AUROC \\uparrow</td><td>FPR95 \\downarrow</td><td>AUROC \\uparrow</td><td>FPR95 \\downarrow</td><td>AUROC \\uparrow</td><td>FPR95 \\downarrow</td><td>AUROC \\uparrow</td><td>FPR95 \\downarrow</td><td>AUROC \\uparrow</td><td>FPR95 \\downarrow</td></tr><tr><th>Expected Results</th><td>Low</td><td>High</td><td>Low</td><td>High</td><td>Low</td><td>High</td><td>High</td><td>Low</td><td>\u2013</td><td>\u2013</td></tr><tr><th>MSP [22]</th><td>87.70</td><td>63.52</td><td>78.22</td><td>80.01</td><td>76.67</td><td>81.31</td><td>74.46</td><td>82.70</td><td>79.26</td><td>76.88</td></tr><tr><th>ODIN [31]</th><td>89.49</td><td>62.61</td><td>83.83</td><td>71.89</td><td>80.60</td><td>76.51</td><td>76.29</td><td>81.31</td><td>82.55</td><td>73.08</td></tr><tr><th>Mahalanobis [29]</th><td>59.60</td><td>95.62</td><td>67.96</td><td>91.58</td><td>66.48</td><td>92.05</td><td>74.96</td><td>51.54</td><td>67.25</td><td>82.70</td></tr><tr><th>Energy [33]</th><td>88.64</td><td>64.35</td><td>85.25</td><td>65.30</td><td>81.31</td><td>72.77</td><td>75.78</td><td>80.60</td><td>82.75</td><td>70.76</td></tr><tr><th>KL Matching [21]</th><td>93.06</td><td>27.24</td><td>78.74</td><td>67.56</td><td>76.53</td><td>72.67</td><td>87.07</td><td>49.47</td><td>83.85</td><td>54.23</td></tr><tr><th>MOS [26]</th><td>98.15</td><td>9.23</td><td>92.01</td><td>40.38</td><td>89.05</td><td>49.49</td><td>81.27</td><td>60.30</td><td>90.12</td><td>39.85</td></tr><tr><th>TAPUDD (Average)</th><td>70.00</td><td>84.46</td><td>70.47</td><td>79.52</td><td>66.97</td><td>84.72</td><td>97.59</td><td>10.85</td><td>76.26</td><td>64.88</td></tr></tbody></table>", "caption": "Table 3:  OOD detection performance in the large-scale classification task.\\uparrow indicates larger values are better, while \\downarrow indicates smaller values are better.Ideally, all methods should follow the expected results obtained from our analysis (described in first row in green color).However, as highlighted in green color, only Mahalanobis and our proposed approach follow the expected results. This highlights the failure of existing baselines, including MSP, ODIN, Energy, KL Matching, and MOS.Further, amongst all methods following the expected results (highlighted in green color), our approach is highly sensitive to OOD samples and significantly outperforms the baselines. ", "list_citation_info": ["[33] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.", "[22] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.", "[26] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8706\u20138715, 2021.", "[21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Xiaodong Song. A benchmark for anomaly segmentation. ArXiv, abs/1911.11132, 2019.", "[31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.", "[29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018."]}, {"table": "<table><tbody><tr><th>OOD</th><td colspan=\"6\">Baselines</td><td colspan=\"2\">Ours (Task-Agnostic)</td></tr><tr><th>Dataset</th><td>MSP [22]</td><td>ODIN [31]</td><td>Energy [33]</td><td>MB [29]</td><td>KL [21]</td><td>Gram [5]</td><td>TAP-MB</td><td>TAPUDD</td></tr><tr><th></th><td></td><td></td><td></td><td></td><td></td><td></td><td>(K = 8)</td><td>(Average)</td></tr><tr><th>LSUN (R)</th><td>91.0</td><td>94.1</td><td>92.8</td><td>99.7</td><td>70.3</td><td>99.9</td><td>96.3</td><td>96.4</td></tr><tr><th>LSUN (C)</th><td>91.9</td><td>91.2</td><td>93.9</td><td>96.7</td><td>81.9</td><td>97.8</td><td>94.5</td><td>94.2</td></tr><tr><th>TinyImgNet (R)</th><td>91.0</td><td>94.0</td><td>92.4</td><td>99.5</td><td>73.8</td><td>99.7</td><td>93.8</td><td>94.3</td></tr><tr><th>TinyImgNet (C)</th><td>91.4</td><td>93.1</td><td>93.0</td><td>98.6</td><td>74.1</td><td>99.2</td><td>94.3</td><td>94.5</td></tr><tr><th>SVHN</th><td>89.9</td><td>96.7</td><td>91.2</td><td>99.1</td><td>85.5</td><td>99.5</td><td>92.8</td><td>93.4</td></tr><tr><th>CIFAR100</th><td>86.4</td><td>85.8</td><td>87.1</td><td>88.2</td><td>69.2</td><td>79.0</td><td>88.2</td><td>88.9</td></tr></tbody></table>", "caption": "Table 5: Comparison of OOD Detection Performance of Resnet34 model trained on CIFAR10 on diverse OOD datasets measured by AUROC. The hyperparameters of ODIN and the hyperparameters and parameters of Mahalanobis are tuned using a random sample of the OOD dataset.MB and TAP-MB refers to Mahalanobis and TAP-Mahalanobis, respectively.", "list_citation_info": ["[5] Sastry Shama Chandramouli and Oore Sageev. Detecting out-of-distribution examples with gram matrices. In Proceedings of the International Conference on Machine Learning (ICML), 2020.", "[33] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.", "[22] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.", "[21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Xiaodong Song. A benchmark for anomaly segmentation. ArXiv, abs/1911.11132, 2019.", "[31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.", "[29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018."]}, {"table": "<table><tbody><tr><th>Method</th><td>iNaturalist</td><td>SUN</td><td>Places</td><td>Textures</td><td>Average</td></tr><tr><th>Expected</th><td>Low</td><td>Low</td><td>Low</td><td>High</td><td>\u2013</td></tr><tr><th>MSP [22]</th><td>97.26</td><td>94.41</td><td>94.12</td><td>95.65</td><td>95.36</td></tr><tr><th>ODIN [31]</th><td>97.80</td><td>96.23</td><td>95.33</td><td>96.11</td><td>96.37</td></tr><tr><th>Mahalanobis [29]</th><td>87.35</td><td>90.32</td><td>90.25</td><td>92.52</td><td>90.11</td></tr><tr><th>Energy [33]</th><td>97.62</td><td>96.55</td><td>95.47</td><td>96.04</td><td>96.42</td></tr><tr><th>KL Matching [21]</th><td>97.98</td><td>94.11</td><td>93.62</td><td>97.96</td><td>95.92</td></tr><tr><th>MOS [26]</th><td>99.62</td><td>98.17</td><td>97.36</td><td>96.68</td><td>97.96</td></tr><tr><th>TAPUDD (Average)</th><td>91.87</td><td>91.02</td><td>90.08</td><td>99.68</td><td>93.16</td></tr></tbody></table>", "caption": "Table 7: OOD detection performance comparison between TAPUDD method and baselines measured by AUPR. Ideally, all methods should follow the expected results obtained from our analysis (described in first row in green color) conducted in Section 4.4 of the main paper.However, as highlighted in green color, only Mahalanobis and our proposed approach follow the expected results. This highlights the failure of existing baselines, including MSP, ODIN, Energy, KL Matching, and MOS.Further, amongst all methods following the expected results (highlighted in green color), our approach is highly sensitive to OOD samples and significantly outperforms the baselines. ", "list_citation_info": ["[33] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.", "[22] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.", "[26] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8706\u20138715, 2021.", "[21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Xiaodong Song. A benchmark for anomaly segmentation. ArXiv, abs/1911.11132, 2019.", "[31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.", "[29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018."]}, {"table": "<table><tbody><tr><th>Brightness</th><td colspan=\"7\">Baselines</td><td colspan=\"3\">Ours (Task-Agnostic)</td></tr><tr><th></th><td>MSP [22]</td><td>ODIN [31]</td><td>Energy [33]</td><td>MB [29]</td><td>KL [21]</td><td>MOS [26]</td><td>Gram [5]</td><td>TAP-MOS</td><td>TAP-MB</td><td>TAPUDD</td></tr><tr><th></th><td></td><td></td><td></td><td></td><td></td><td>(K = 8)</td><td></td><td>(K = 8)</td><td>(K = 8)</td><td>(Average)</td></tr><tr><th>0.0</th><td>94.1\\pm2.6</td><td>94.1\\pm2.6</td><td>93.9\\pm2.8</td><td>100.0\\pm0.0</td><td>51.4\\pm23.3</td><td>94.5\\pm2.9</td><td>99.7\\pm0.7</td><td>63.1\\pm14.3</td><td>100.0\\pm0.1</td><td>100.0\\pm0.0</td></tr><tr><th>0.2</th><td>68.2\\pm4.3</td><td>68.0\\pm4.5</td><td>67.8\\pm4.8</td><td>89.0\\pm4.6</td><td>44.9\\pm1.8</td><td>67.5\\pm4.5</td><td>73.9\\pm1.5</td><td>67.4\\pm9.0</td><td>89.3\\pm3.5</td><td>89.7\\pm4.0</td></tr><tr><th>0.4</th><td>57.6\\pm2.2</td><td>56.6\\pm2.3</td><td>56.4\\pm2.6</td><td>71.7\\pm4.0</td><td>47.4\\pm0.7</td><td>56.7\\pm1.8</td><td>71.0\\pm1.2</td><td>57.5\\pm4.3</td><td>72.6\\pm3.7</td><td>73.2\\pm4.4</td></tr><tr><th>0.6</th><td>53.9\\pm2.0</td><td>52.2\\pm1.4</td><td>52.1\\pm1.3</td><td>59.9\\pm2.7</td><td>48.8\\pm0.7</td><td>52.5\\pm1.2</td><td>70.3\\pm1.4</td><td>53.2\\pm2.3</td><td>60.8\\pm2.6</td><td>60.9\\pm2.9</td></tr><tr><th>0.8</th><td>52.3\\pm1.7</td><td>50.2\\pm0.7</td><td>50.2\\pm0.7</td><td>52.4\\pm1.3</td><td>49.6\\pm0.4</td><td>50.4\\pm0.6</td><td>70.0\\pm1.4</td><td>50.7\\pm1.3</td><td>52.5\\pm1.4</td><td>52.5\\pm1.5</td></tr><tr><th>1.0</th><td>52.2\\pm1.4</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>69.9\\pm1.4</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td></tr><tr><th>1.2</th><td>53.4\\pm1.0</td><td>51.4\\pm0.7</td><td>51.5\\pm0.7</td><td>55.0\\pm1.8</td><td>48.9\\pm0.5</td><td>51.4\\pm0.7</td><td>70.2\\pm1.4</td><td>50.9\\pm0.4</td><td>56.2\\pm1.6</td><td>56.1\\pm1.6</td></tr><tr><th>1.4</th><td>56.6\\pm0.9</td><td>55.2\\pm1.3</td><td>55.2\\pm1.3</td><td>62.8\\pm2.6</td><td>47.7\\pm0.7</td><td>55.1\\pm1.2</td><td>71.0\\pm1.3</td><td>53.0\\pm0.8</td><td>64.2\\pm2.2</td><td>64.0\\pm2.2</td></tr><tr><th>1.6</th><td>60.4\\pm1.6</td><td>59.3\\pm2.2</td><td>59.4\\pm2.3</td><td>70.8\\pm3.1</td><td>46.9\\pm0.9</td><td>59.0\\pm1.8</td><td>71.8\\pm1.2</td><td>55.1\\pm1.5</td><td>72.2\\pm2.5</td><td>72.0\\pm2.6</td></tr><tr><th>1.8</th><td>63.9\\pm2.8</td><td>63.2\\pm3.4</td><td>63.4\\pm3.6</td><td>77.6\\pm3.2</td><td>46.9\\pm1.3</td><td>62.5\\pm2.9</td><td>72.7\\pm1.2</td><td>56.5\\pm3.5</td><td>78.5\\pm2.8</td><td>78.3\\pm3.0</td></tr><tr><th>2.0</th><td>66.5\\pm4.4</td><td>66.0\\pm5.1</td><td>66.3\\pm5.2</td><td>83.0\\pm3.0</td><td>47.4\\pm1.6</td><td>65.0\\pm4.5</td><td>73.6\\pm1.3</td><td>57.6\\pm5.4</td><td>83.6\\pm2.7</td><td>83.3\\pm2.8</td></tr><tr><th>2.5</th><td>71.0\\pm7.5</td><td>70.5\\pm8.4</td><td>70.7\\pm8.5</td><td>91.6\\pm2.5</td><td>48.4\\pm3.4</td><td>69.4\\pm7.2</td><td>75.7\\pm2.2</td><td>60.3\\pm7.6</td><td>91.3\\pm2.9</td><td>91.1\\pm3.0</td></tr><tr><th>3.0</th><td>75.1\\pm9.5</td><td>74.7\\pm10.3</td><td>74.9\\pm10.4</td><td>95.7\\pm1.6</td><td>48.5\\pm4.5</td><td>73.9\\pm9.2</td><td>77.8\\pm3.2</td><td>63.8\\pm8.1</td><td>95.0\\pm2.8</td><td>94.9\\pm2.9</td></tr><tr><th>3.5</th><td>76.5\\pm10.2</td><td>76.3\\pm10.7</td><td>76.4\\pm10.8</td><td>97.5\\pm1.1</td><td>49.4\\pm5.5</td><td>75.4\\pm9.7</td><td>79.2\\pm4.0</td><td>65.9\\pm8.9</td><td>96.5\\pm2.7</td><td>96.5\\pm2.7</td></tr><tr><th>4.0</th><td>78.8\\pm9.9</td><td>78.8\\pm10.0</td><td>78.7\\pm10.4</td><td>98.4\\pm0.6</td><td>48.6\\pm6.2</td><td>78.1\\pm9.3</td><td>80.9\\pm5.0</td><td>67.9\\pm9.4</td><td>97.4\\pm2.5</td><td>97.3\\pm2.4</td></tr><tr><th>4.5</th><td>81.0\\pm8.3</td><td>81.1\\pm8.3</td><td>81.0\\pm8.8</td><td>98.9\\pm0.4</td><td>46.5\\pm6.4</td><td>80.6\\pm7.9</td><td>82.5\\pm4.8</td><td>69.5\\pm9.5</td><td>98.0\\pm2.1</td><td>98.0\\pm1.8</td></tr><tr><th>5.0</th><td>82.9\\pm6.6</td><td>82.9\\pm6.6</td><td>82.8\\pm7.2</td><td>99.1\\pm0.3</td><td>45.0\\pm6.0</td><td>82.7\\pm6.3</td><td>84.1\\pm4.7</td><td>70.3\\pm9.8</td><td>98.4\\pm1.8</td><td>98.5\\pm1.3</td></tr><tr><th>5.5</th><td>84.4\\pm5.4</td><td>84.4\\pm5.5</td><td>84.3\\pm6.1</td><td>99.3\\pm0.3</td><td>44.5\\pm6.0</td><td>84.3\\pm5.3</td><td>85.5\\pm4.8</td><td>70.7\\pm10.3</td><td>98.7\\pm1.6</td><td>98.8\\pm1.0</td></tr><tr><th>6.0</th><td>85.7\\pm4.7</td><td>85.7\\pm4.7</td><td>85.6\\pm5.3</td><td>99.4\\pm0.3</td><td>44.2\\pm5.8</td><td>85.5\\pm4.6</td><td>86.6\\pm4.8</td><td>71.1\\pm10.9</td><td>98.9\\pm1.4</td><td>99.0\\pm0.7</td></tr><tr><th>6.5</th><td>86.5\\pm4.3</td><td>86.5\\pm4.3</td><td>86.4\\pm4.8</td><td>99.4\\pm0.3</td><td>44.4\\pm5.5</td><td>86.3\\pm4.2</td><td>87.5\\pm4.8</td><td>71.2\\pm11.4</td><td>99.0\\pm1.2</td><td>99.1\\pm0.6</td></tr><tr><th>Average</th><td>70.1</td><td>69.4</td><td>69.4</td><td>82.6</td><td>47.5</td><td>69.0</td><td>77.7</td><td>62.0</td><td>81.7</td><td>81.9</td></tr></tbody></table>", "caption": "Table 8: NAS detection performance in binary classification task (gender prediction) for NAS shift of brightness in RSNA boneage dataset measured by AUPR. Highlighted row presents the performance on in-distribution dataset. MB and TAP-MB refers to Mahalanobis and TAP-Mahalanobis, respectively.", "list_citation_info": ["[5] Sastry Shama Chandramouli and Oore Sageev. Detecting out-of-distribution examples with gram matrices. In Proceedings of the International Conference on Machine Learning (ICML), 2020.", "[33] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.", "[22] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.", "[26] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8706\u20138715, 2021.", "[21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Xiaodong Song. A benchmark for anomaly segmentation. ArXiv, abs/1911.11132, 2019.", "[31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.", "[29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018."]}, {"table": "<table><tbody><tr><th>Brightness</th><td colspan=\"7\">Baselines</td><td colspan=\"3\">Ours (Task-Agnostic)</td></tr><tr><th></th><td>MSP [22]</td><td>ODIN [31]</td><td>Energy [33]</td><td>MB [29]</td><td>KL [21]</td><td>MOS [26]</td><td>Gram [5]</td><td>TAP-MOS</td><td>TAP-MB</td><td>TAPUDD</td></tr><tr><th></th><td></td><td></td><td></td><td></td><td></td><td>(K = 8)</td><td></td><td>(K = 8)</td><td>(K = 8)</td><td>(Average)</td></tr><tr><th>0.0</th><td>90.0\\pm31.6</td><td>90.0\\pm31.6</td><td>90.0\\pm31.6</td><td>0.0\\pm0.0</td><td>90\\pm31.6</td><td>80\\pm42.2</td><td>0.0\\pm0.0</td><td>100.0\\pm0.0</td><td>0.0\\pm0.0</td><td>0.0\\pm0.0</td></tr><tr><th>0.2</th><td>90.4\\pm1.6</td><td>90.4\\pm1.6</td><td>90.6\\pm2.5</td><td>61.4\\pm14.8</td><td>92.1\\pm1.3</td><td>90.7\\pm1.5</td><td>88.5\\pm2.7</td><td>86.0\\pm9.2</td><td>65.6\\pm11.9</td><td>62.5\\pm15.1</td></tr><tr><th>0.4</th><td>93.9\\pm1.0</td><td>93.9\\pm1.0</td><td>93.9\\pm1.1</td><td>86.4\\pm5.9</td><td>94.5\\pm1.0</td><td>94.0\\pm0.9</td><td>93.6\\pm0.9</td><td>91.6\\pm5.0</td><td>89.1\\pm3.3</td><td>88.5\\pm4.2</td></tr><tr><th>0.6</th><td>94.3\\pm0.8</td><td>94.3\\pm0.8</td><td>94.3\\pm0.7</td><td>92.8\\pm2.0</td><td>94.5\\pm1.1</td><td>94.4\\pm1.0</td><td>94.2\\pm1.2</td><td>93.4\\pm2.6</td><td>93.3\\pm1.3</td><td>93.4\\pm1.2</td></tr><tr><th>0.8</th><td>95.0\\pm0.4</td><td>95.0\\pm0.4</td><td>95.0\\pm0.4</td><td>94.7\\pm0.8</td><td>95.5\\pm0.7</td><td>95.2\\pm0.5</td><td>94.9\\pm0.6</td><td>94.8\\pm1.2</td><td>95.4\\pm0.7</td><td>95.2\\pm0.5</td></tr><tr><th>1.0</th><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td></tr><tr><th>1.2</th><td>94.8\\pm0.3</td><td>94.8\\pm0.3</td><td>94.5\\pm0.3</td><td>93.5\\pm1.0</td><td>94.7\\pm1.3</td><td>94.8\\pm0.4</td><td>94.6\\pm0.7</td><td>95.0\\pm0.8</td><td>93.5\\pm0.9</td><td>93.3\\pm0.6</td></tr><tr><th>1.4</th><td>93.3\\pm0.7</td><td>93.3\\pm0.7</td><td>93.0\\pm0.8</td><td>90.0\\pm1.8</td><td>93.5\\pm1.6</td><td>93.6\\pm0.9</td><td>93.0\\pm0.8</td><td>94.4\\pm1.9</td><td>89.9\\pm2.2</td><td>89.1\\pm2.1</td></tr><tr><th>1.6</th><td>92.2\\pm0.6</td><td>92.2\\pm0.6</td><td>91.8\\pm0.8</td><td>83.5\\pm2.3</td><td>93.1\\pm0.9</td><td>92.1\\pm1</td><td>90.8\\pm1.6</td><td>93.4\\pm2.6</td><td>84.1\\pm3.9</td><td>83.4\\pm3.7</td></tr><tr><th>1.8</th><td>91.1\\pm1.2</td><td>91.1\\pm1.2</td><td>90.5\\pm1.3</td><td>77.1\\pm3.3</td><td>92.1\\pm1.1</td><td>91.0\\pm1.4</td><td>88.5\\pm2.4</td><td>92.5\\pm3.6</td><td>77.3\\pm6.1</td><td>76.8\\pm5.4</td></tr><tr><th>2.0</th><td>90.2\\pm1.1</td><td>90.2\\pm1.1</td><td>89.5\\pm1.2</td><td>70.5\\pm5.6</td><td>91.6\\pm1.4</td><td>90.3\\pm1.2</td><td>86.6\\pm3.4</td><td>92.5\\pm4.3</td><td>70.1\\pm8.7</td><td>69.9\\pm8.4</td></tr><tr><th>2.5</th><td>87.4\\pm2.5</td><td>87.4\\pm2.5</td><td>87.0\\pm2.6</td><td>51.6\\pm11.3</td><td>89.8\\pm2.1</td><td>87.7\\pm2.5</td><td>80.5\\pm5.3</td><td>91.5\\pm5.6</td><td>49.8\\pm16.1</td><td>50.5\\pm15.0</td></tr><tr><th>3.0</th><td>86.2\\pm4.9</td><td>86.2\\pm4.9</td><td>85.7\\pm4.6</td><td>32.5\\pm12.4</td><td>88.8\\pm3.9</td><td>86.5\\pm4.9</td><td>76.0\\pm7.9</td><td>89.6\\pm8.1</td><td>33.5\\pm21.1</td><td>34.3\\pm18.7</td></tr><tr><th>3.5</th><td>84.6\\pm6.9</td><td>84.7\\pm6.9</td><td>84.7\\pm6.3</td><td>19.0\\pm10.2</td><td>87.8\\pm5.7</td><td>85.5\\pm6.7</td><td>72.4\\pm11.9</td><td>87.9\\pm12.3</td><td>25.4\\pm23.7</td><td>25.3\\pm20.9</td></tr><tr><th>4.0</th><td>83.6\\pm8.2</td><td>83.6\\pm8.1</td><td>83.5\\pm7.4</td><td>10.8\\pm6.9</td><td>86.5\\pm6.8</td><td>84.1\\pm8.1</td><td>68.2\\pm14.3</td><td>85.1\\pm16</td><td>19.8\\pm24.5</td><td>19.0\\pm20.9</td></tr><tr><th>4.5</th><td>81.9\\pm7.4</td><td>81.9\\pm7.4</td><td>81.7\\pm7.6</td><td>5.4\\pm3.6</td><td>84.9\\pm6.4</td><td>82.3\\pm7.6</td><td>64.7\\pm13.5</td><td>83.6\\pm16.8</td><td>14.8\\pm22.4</td><td>14.1\\pm18.1</td></tr><tr><th>5.0</th><td>80.5\\pm7.4</td><td>80.5\\pm7.3</td><td>80.0\\pm7.8</td><td>2.7\\pm1.9</td><td>83.9\\pm6.2</td><td>81.2\\pm7.2</td><td>61.7\\pm13.4</td><td>82.7\\pm15.9</td><td>11.4\\pm19.9</td><td>10.0\\pm13.9</td></tr><tr><th>5.5</th><td>78.6\\pm8.1</td><td>78.6\\pm8.1</td><td>78.2\\pm8.6</td><td>1.8\\pm1.7</td><td>82.3\\pm6.9</td><td>79.3\\pm8.0</td><td>58.3\\pm13.9</td><td>82.8\\pm14.8</td><td>9.2\\pm18.0</td><td>7.1\\pm10.4</td></tr><tr><th>6.0</th><td>77.6\\pm8.9</td><td>77.6\\pm8.9</td><td>77.0\\pm9.4</td><td>1.3\\pm1.6</td><td>81.7\\pm7.7</td><td>78.6\\pm8.8</td><td>56.1\\pm14.6</td><td>83.7\\pm14</td><td>7.5\\pm16.3</td><td>5.1\\pm7.6</td></tr><tr><th>6.5</th><td>76.5\\pm9.7</td><td>76.6\\pm9.6</td><td>76.1\\pm10.3</td><td>1.0\\pm1.5</td><td>80.3\\pm8</td><td>77.7\\pm9.2</td><td>54.2\\pm15.5</td><td>84.2\\pm13.5</td><td>6.3\\pm14.9</td><td>3.6\\pm5.9</td></tr><tr><th>Average</th><td>87.9</td><td>87.9</td><td>87.6</td><td>48.6</td><td>89.6</td><td>87.7</td><td>75.6</td><td>62.0</td><td>81.7</td><td>46.5</td></tr></tbody></table>", "caption": "Table 9: NAS detection performance in binary classification task (gender prediction) for NAS shift of brightness in RSNA boneage dataset measured by FPR95. Highlighted row presents the performance on in-distribution dataset. MB and TAP-MB refers to Mahalanobis and TAP-Mahalanobis, respectively.", "list_citation_info": ["[5] Sastry Shama Chandramouli and Oore Sageev. Detecting out-of-distribution examples with gram matrices. In Proceedings of the International Conference on Machine Learning (ICML), 2020.", "[33] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.", "[22] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.", "[26] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8706\u20138715, 2021.", "[21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Xiaodong Song. A benchmark for anomaly segmentation. ArXiv, abs/1911.11132, 2019.", "[31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.", "[29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018."]}, {"table": "<table><tbody><tr><th>Brightness</th><td colspan=\"3\">Baselines</td><td colspan=\"3\">Ours (Task-Agnostic)</td></tr><tr><th></th><td>DE [28]</td><td>MC Dropout [14]</td><td>\\text{SWAG}^{*} [36]</td><td>TAP-MOS</td><td>TAP-MB</td><td>TAPUDD</td></tr><tr><th></th><td></td><td></td><td></td><td>(K = 8)</td><td>(K = 8)</td><td>(Average)</td></tr><tr><th>0.0</th><td>100.0\\pmNA</td><td>34.9\\pmNA</td><td>100.0\\pmNA</td><td>74.4\\pm20.8</td><td>99.8\\pm0.4</td><td>100.0\\pm0.0</td></tr><tr><th>0.2</th><td>53.9\\pmNA</td><td>48.4\\pmNA</td><td>51.4\\pmNA</td><td>69.2\\pm17.2</td><td>89.6\\pm12.8</td><td>89.6\\pm6.1</td></tr><tr><th>0.4</th><td>50.0\\pmNA</td><td>51.0\\pmNA</td><td>49.4\\pmNA</td><td>69.2\\pm16.9</td><td>75.6\\pm15.8</td><td>68.1\\pm4.8</td></tr><tr><th>0.6</th><td>50.1\\pmNA</td><td>50.4\\pmNA</td><td>49.2\\pmNA</td><td>64.3\\pm12.3</td><td>58.0\\pm8.4</td><td>56.3\\pm2.8</td></tr><tr><th>0.8</th><td>50.2\\pmNA</td><td>50.1\\pmNA</td><td>49.8\\pmNA</td><td>57.3\\pm6.4</td><td>51.5\\pm2.3</td><td>50.2\\pm0.9</td></tr><tr><th>1.0</th><td>50.0\\pmNA</td><td>49.7\\pmNA</td><td>50.0\\pmNA</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td><td>50.0\\pm0.0</td></tr><tr><th>1.2</th><td>50.4\\pmNA</td><td>48.7\\pmNA</td><td>50.8\\pmNA</td><td>49.3\\pm3.6</td><td>50.2\\pm0.5</td><td>56.2\\pm1.1</td></tr><tr><th>1.4</th><td>53.6\\pmNA</td><td>47.9\\pmNA</td><td>54.8\\pmNA</td><td>51.0\\pm6.8</td><td>51.1\\pm1.2</td><td>65.2\\pm2.5</td></tr><tr><th>1.6</th><td>55.5\\pmNA</td><td>46.7\\pmNA</td><td>62.1\\pmNA</td><td>55.2\\pm10.1</td><td>52.0\\pm2.1</td><td>75.1\\pm3.0</td></tr><tr><th>1.8</th><td>60.3\\pmNA</td><td>45.4\\pmNA</td><td>74.0\\pmNA</td><td>61.5\\pm13.5</td><td>53.0\\pm3.1</td><td>83.2\\pm4.2</td></tr><tr><th>2.0</th><td>69.9\\pmNA</td><td>43.9\\pmNA</td><td>83.2\\pmNA</td><td>67.2\\pm15.9</td><td>55.0\\pm4.6</td><td>89.3\\pm4.2</td></tr><tr><th>2.5</th><td>94.4\\pmNA</td><td>40.1\\pmNA</td><td>92.3\\pmNA</td><td>76.8\\pm16.0</td><td>61.0\\pm9.3</td><td>96.3\\pm2.0</td></tr><tr><th>3.0</th><td>98.4\\pmNA</td><td>37.4\\pmNA</td><td>92.7\\pmNA</td><td>83.4\\pm13.2</td><td>64.8\\pm11.7</td><td>98.5\\pm0.7</td></tr><tr><th>3.5</th><td>99.3\\pmNA</td><td>35.6\\pmNA</td><td>94.8\\pmNA</td><td>88.5\\pm9.7</td><td>68.5\\pm13.2</td><td>99.1\\pm0.4</td></tr><tr><th>4.0</th><td>99.8\\pmNA</td><td>34.3\\pmNA</td><td>97.2\\pmNA</td><td>90.8\\pm6.6</td><td>71.3\\pm13.3</td><td>99.4\\pm0.3</td></tr><tr><th>4.5</th><td>100.0\\pmNA</td><td>33.4\\pmNA</td><td>98.0\\pmNA</td><td>91.5\\pm4.6</td><td>73.8\\pm12.3</td><td>99.6\\pm0.3</td></tr><tr><th>5.0</th><td>100.0\\pmNA</td><td>32.4\\pmNA</td><td>98.6\\pmNA</td><td>91.4\\pm4.2</td><td>77.1\\pm10.7</td><td>99.6\\pm0.3</td></tr><tr><th>5.5</th><td>100.0\\pmNA</td><td>32.0\\pmNA</td><td>98.9\\pmNA</td><td>90.7\\pm4.9</td><td>80.2\\pm9.0</td><td>99.6\\pm0.3</td></tr><tr><th>6.0</th><td>100.0\\pmNA</td><td>31.7\\pmNA</td><td>99.0\\pmNA</td><td>89.7\\pm5.8</td><td>82.6\\pm8.3</td><td>99.6\\pm0.4</td></tr><tr><th>6.5</th><td>100.0\\pmNA</td><td>31.5\\pmNA</td><td>99.2\\pmNA</td><td>88.6\\pm6.7</td><td>84.3\\pm7.9</td><td>99.6\\pm0.5</td></tr><tr><th>Average</th><td>76.8</td><td>41.3</td><td>77.3</td><td>73.0</td><td>67.5</td><td>83.7</td></tr></tbody></table>", "caption": "Table 10: NAS detection performance in regression task (age prediction) for NAS shift of brightness in RSNA boneage dataset measured by AUPR. Highlighted row presents the performance on in-distribution dataset. DE and TAP-MB denotes Deep Ensemble and TAP-Mahalanobis, respectively. \\text{SWAG}^{*} = SWAG + Deep Ensemble.", "list_citation_info": ["[36] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 13153\u201313164, 2019.", "[14] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the International Conference on Machine Learning (ICML), 2016.", "[28] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NeurIPS), 2017."]}, {"table": "<table><tbody><tr><th>Brightness</th><td colspan=\"3\">Baselines</td><td colspan=\"3\">Ours (Task-Agnostic)</td></tr><tr><th></th><td>DE [28]</td><td>MC Dropout [14]</td><td>\\text{SWAG}^{*} [36]</td><td>TAP-MOS</td><td>TAP-MB</td><td>TAPUDD</td></tr><tr><th></th><td></td><td></td><td></td><td>(K = 8)</td><td>(K = 8)</td><td>(Average)</td></tr><tr><th>0.0</th><td>0.0\\pmNA</td><td>100.0\\pmNA</td><td>0.0\\pmNA</td><td>80.0\\pm42.2</td><td>0.0\\pm0.0</td><td>0.0\\pm0.0</td></tr><tr><th>0.2</th><td>91.9\\pmNA</td><td>99.2\\pmNA</td><td>94.0\\pmNA</td><td>74.9\\pm21.6</td><td>51.2\\pm27.4</td><td>58.9\\pm18.7</td></tr><tr><th>0.4</th><td>94.6\\pmNA</td><td>96.1\\pmNA</td><td>94.7\\pmNA</td><td>71.4\\pm23.4</td><td>69.1\\pm18.4</td><td>90.5\\pm2.5</td></tr><tr><th>0.6</th><td>94.7\\pmNA</td><td>95.6\\pmNA</td><td>95.0\\pmNA</td><td>84.0\\pm9.6</td><td>88.9\\pm5.4</td><td>94.4\\pm1.0</td></tr><tr><th>0.8</th><td>95.0\\pmNA</td><td>95.6\\pmNA</td><td>95.1\\pmNA</td><td>91.8\\pm2.8</td><td>94.1\\pm1.5</td><td>95.4\\pm0.4</td></tr><tr><th>1.0</th><td>95.0\\pmNA</td><td>94.5\\pmNA</td><td>95.0\\pmNA</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td><td>95.0\\pm0.0</td></tr><tr><th>1.2</th><td>94.7\\pmNA</td><td>95.5\\pmNA</td><td>94.1\\pmNA</td><td>95.2\\pm1.2</td><td>94.5\\pm1.4</td><td>92.6\\pm2.1</td></tr><tr><th>1.4</th><td>89.2\\pmNA</td><td>95.6\\pmNA</td><td>93.7\\pmNA</td><td>92.3\\pm5.6</td><td>93.8\\pm2.2</td><td>86.0\\pm5.8</td></tr><tr><th>1.6</th><td>78.8\\pmNA</td><td>97.5\\pmNA</td><td>88.6\\pmNA</td><td>87.4\\pm9.8</td><td>92.9\\pm3.1</td><td>74.9\\pm9.7</td></tr><tr><th>1.8</th><td>69.7\\pmNA</td><td>98.7\\pmNA</td><td>87.5\\pmNA</td><td>79.8\\pm16.8</td><td>90.3\\pm5.3</td><td>62.1\\pm14</td></tr><tr><th>2.0</th><td>53.3\\pmNA</td><td>99.1\\pmNA</td><td>81.7\\pmNA</td><td>73.5\\pm24.1</td><td>88.2\\pm7.0</td><td>49.2\\pm18.8</td></tr><tr><th>2.5</th><td>14.9\\pmNA</td><td>100\\pmNA</td><td>60.3\\pmNA</td><td>63.2\\pm31.1</td><td>81.4\\pm12.6</td><td>23.6\\pm16.1</td></tr><tr><th>3.0</th><td>6.9\\pmNA</td><td>100\\pmNA</td><td>53.6\\pmNA</td><td>54.5\\pm29.9</td><td>76.7\\pm15.4</td><td>8.8\\pm6.8</td></tr><tr><th>3.5</th><td>2.8\\pmNA</td><td>100.0\\pmNA</td><td>35.2\\pmNA</td><td>43.5\\pm25.6</td><td>73.0\\pm17.9</td><td>3.9\\pm3.0</td></tr><tr><th>4.0</th><td>0.8\\pmNA</td><td>100.0\\pmNA</td><td>20.2\\pmNA</td><td>36.6\\pm20.2</td><td>69.6\\pm19.0</td><td>2.4\\pm2.1</td></tr><tr><th>4.5</th><td>0.0\\pmNA</td><td>100.0\\pmNA</td><td>13.4\\pmNA</td><td>33.7\\pm17.2</td><td>67.3\\pm17.6</td><td>1.3\\pm1.6</td></tr><tr><th>5.0</th><td>0.0\\pmNA</td><td>100.0\\pmNA</td><td>8.2\\pmNA</td><td>32.8\\pm17.5</td><td>65.4\\pm16.2</td><td>1.2\\pm1.9</td></tr><tr><th>5.5</th><td>0.1\\pmNA</td><td>100.0\\pmNA</td><td>5.7\\pmNA</td><td>34.6\\pm20.2</td><td>61.6\\pm16.0</td><td>1.6\\pm3.3</td></tr><tr><th>6.0</th><td>0.0\\pmNA</td><td>100.0\\pmNA</td><td>4.5\\pmNA</td><td>37.7\\pm23.0</td><td>57.7\\pm16.6</td><td>2.1\\pm4.5</td></tr><tr><th>6.5</th><td>0.0\\pmNA</td><td>100.0\\pmNA</td><td>3.1\\pmNA</td><td>41.8\\pm25.9</td><td>54.1\\pm17.6</td><td>2.5\\pm5.5</td></tr><tr><th>Average</th><td>44.1</td><td>98.4</td><td>56.2</td><td>65.2</td><td>73.2</td><td>42.3</td></tr></tbody></table>", "caption": "Table 11: NAS detection performance in regression task (age prediction) for NAS shift of brightness in RSNA boneage dataset measured by FPR95. Highlighted row presents the performance on in-distribution dataset. DE and TAP-MB denotes Deep Ensemble and TAP-Mahalanobis, respectively. \\text{SWAG}^{*} = SWAG + Deep Ensemble.", "list_citation_info": ["[36] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 13153\u201313164, 2019.", "[14] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the International Conference on Machine Learning (ICML), 2016.", "[28] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NeurIPS), 2017."]}], "citation_info_to_title": {"[14] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proceedings of the International Conference on Machine Learning (ICML), 2016.": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "[21] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Xiaodong Song. A benchmark for anomaly segmentation. ArXiv, abs/1911.11132, 2019.": "A benchmark for anomaly segmentation", "[33] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.": "Energy-based out-of-distribution detection", "[31] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2018.": "Enhancing the reliability of out-of-distribution image detection in neural networks", "[29] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.": "A simple unified framework for detecting out-of-distribution samples and adversarial attacks", "[28] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NeurIPS), 2017.": "Simple and scalable predictive uncertainty estimation using deep ensembles", "[36] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. In Advances in Neural Information Processing Systems (NeurIPS), pages 13153\u201313164, 2019.": "A simple baseline for Bayesian uncertainty in deep learning", "[5] Sastry Shama Chandramouli and Oore Sageev. Detecting out-of-distribution examples with gram matrices. In Proceedings of the International Conference on Machine Learning (ICML), 2020.": "Detecting out-of-distribution examples with gram matrices", "[26] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pages 8706\u20138715, 2021.": "Mos: Towards scaling out-of-distribution detection for large semantic space", "[22] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.": "A baseline for detecting misclassified and out-of-distribution examples in neural networks"}, "source_title_to_arxiv_id": {"Mos: Towards scaling out-of-distribution detection for large semantic space": "2105.01879"}}