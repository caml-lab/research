{"title": "Gaze-enhanced Crossmodal Embeddings for Emotion Recognition", "abstract": "Emotional expressions are inherently multimodal -- integrating facial\nbehavior, speech, and gaze -- but their automatic recognition is often limited\nto a single modality, e.g. speech during a phone call. While previous work\nproposed crossmodal emotion embeddings to improve monomodal recognition\nperformance, despite its importance, an explicit representation of gaze was not\nincluded. We propose a new approach to emotion recognition that incorporates an\nexplicit representation of gaze in a crossmodal emotion embedding framework. We\nshow that our method outperforms the previous state of the art for both\naudio-only and video-only emotion classification on the popular One-Minute\nGradual Emotion Recognition dataset. Furthermore, we report extensive ablation\nexperiments and provide detailed insights into the performance of different\nstate-of-the-art gaze representations and integration strategies. Our results\nnot only underline the importance of gaze for emotion recognition but also\ndemonstrate a practical and highly effective approach to leveraging gaze\ninformation for this task.", "authors": ["Ahmed Abdou", "Ekta Sood", "Philipp M\u00fcller", "Andreas Bulling"], "published_date": "2022_04_30", "pdf_url": "http://arxiv.org/pdf/2205.00129v1", "list_table_and_caption": [{"table": "<table><thead><tr><th><p>Base feature</p></th><th><p>Statistical functionals</p></th><th># Features</th></tr></thead><tbody><tr><td><p>gaze angle x, gaze angle y, \\Delta gaze angle x, \\Delta gaze angle y, pupil diameter mm</p></td><td><p>min, max, mean, median, quartile 1, quartile 3, std, IQR 1-2, IQR 2-3, IQR 1-3, LR intercept, LR slope</p></td><td>60</td></tr><tr><td><p>\\Delta pupil diameter mm</p></td><td><p>min, max, mean, quartile 1, quartile 3, std, IQR 1-2, IQR 2-3, IQR 1-3, LR intercept, LR slope</p></td><td>11</td></tr><tr><td><p>eye blink intensity</p></td><td><p>max, mean, median, quartile 3, std, IQR 1-2, IQR 2-3, IQR 1-3, LR intercept, LR slope</p></td><td>10</td></tr><tr><td><p>pupil dilation, pupil constriction</p></td><td><p>time ratio, mean time, max time, total time</p></td><td>8</td></tr><tr><td><p>gaze approach</p></td><td><p>time ratio, mean time, max time, median time</p></td><td>4</td></tr><tr><td><p>eyes closed, gaze fixation</p></td><td><p>time ratio, min time, max time, mean time, median time</p></td><td>10</td></tr></tbody></table>", "caption": "Table 1. Base features from OpenFace output and the corresponding statistical features. These 103 features are subset of the feature set in (O\u2019Dwyer et al., 2019). LR refers to a linear regression fitted to the time series of feature values in the window. Time ratio is the proportion of time during which a binary feature is detected in the analysis window.IQR denotes the interquartile range, i.e. IQR 2-3 refers to the difference between third and second quartile.", "list_citation_info": ["O\u2019Dwyer et al. (2019) Jonny O\u2019Dwyer, Niall Murray, and Ronan Flynn. 2019. Eye-based Continuous Affect Prediction. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, 137\u2013143."]}, {"table": "<table><tbody><tr><th><p>Approach \\downarrow \u2014\u2014 Test Modality \\rightarrow</p></th><td>Audio</td><td>Video</td></tr><tr><th><p>OMG baseline (Barros et al., 2018)</p></th><td>33.0</td><td>37.0</td></tr><tr><th><p>EmoBed (Han et al., 2019)</p></th><td>41.7</td><td>43.9</td></tr><tr><th>monomodal</th><td></td><td></td></tr><tr><th><p>     no gaze</p></th><td>41.3 (0.15)</td><td>42.8 (0.24)</td></tr><tr><th><p>     averaging, model-level fusion</p></th><td>-</td><td>43.8 (0.21)</td></tr><tr><th><p>     windowing, early fusion</p></th><td>-</td><td>43.0 (0.23)</td></tr><tr><th><p>     windowing, model-level fusion</p></th><td>-</td><td>43.7 (0.20)</td></tr><tr><th>crossmodal triplet training</th><td></td><td></td></tr><tr><th><p>     no gaze</p></th><td>42.9 (0.17)</td><td>43.5 (0.25)</td></tr><tr><th><p>     averaging, model-level fusion</p></th><td>42.6 (0.19)</td><td>45.0 (0.22)</td></tr><tr><th><p>     windowing, early fusion</p></th><td>43.4 (0.17)</td><td>43.7 (0.26)</td></tr><tr><th><p>     windowing, model-level fusion</p></th><td>42.8 (0.15)</td><td>44.5 (0.20)</td></tr></tbody></table>", "caption": "Table 2. F1 scores for different approaches and test modalities on the OMG (Barros et al., 2018) dataset. Scores are averaged across 20 runs with different random intialisations and corresponding standard errors are shown in brackets.", "list_citation_info": ["Han et al. (2019) Jing Han, Zixing Zhang, Zhao Ren, and Bjoern W Schuller. 2019. Emobed: Strengthening monomodal emotion recognition via training with crossmodal emotion embeddings. IEEE Transactions on Affective Computing (2019).", "Barros et al. (2018) Pablo Barros, Nikhil Churamani, Egor Lakomkin, Henrique Siqueira, Alexander Sutherland, and Stefan Wermter. 2018. The omg-emotion behavior dataset. In 2018 International Joint Conference on Neural Networks (IJCNN). IEEE, 1\u20137."]}, {"table": "<table><tbody><tr><th><p>Gaze Features \\downarrow \u2014\u2014 Test Modality \\rightarrow</p></th><td>Audio</td><td>Video</td></tr><tr><th><p>O\u2019Dwyer et al. (2019)</p></th><td></td><td></td></tr><tr><th><p>     averaging, model-level fusion</p></th><td>42.6 (0.19)</td><td>45.0 (0.22)</td></tr><tr><th><p>     windowing, early fusion</p></th><td>43.4 (0.17)</td><td>43.7 (0.26)</td></tr><tr><th><p>     windowing, model-level fusion</p></th><td>42.8 (0.15)</td><td>44.5 (0.20)</td></tr><tr><th><p>Van Huynh et al. (2019)</p></th><td></td><td></td></tr><tr><th><p>     averaging, model-level fusion</p></th><td>43.1 (0.16)</td><td>44.8 (0.24)</td></tr><tr><th><p>     windowing, early fusion</p></th><td>43.2 (0.18)</td><td>43.6 (0.28)</td></tr><tr><th><p>     windowing, model-level fusion</p></th><td>42.8 (0.18)</td><td>43.5 (0.23)</td></tr></tbody></table>", "caption": "Table 4. F1 scores of different gaze feature representations in the crossmodal triplet training formulation on the OMG (Barros et al., 2018) dataset.Standard errors across 20 runs are shown in brackets.", "list_citation_info": ["Van Huynh et al. (2019) Thong Van Huynh, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim, and In-Seop Na. 2019. Emotion recognition by integrating eye movement analysis and facial expression model. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing. 166\u2013169.", "O\u2019Dwyer et al. (2019) Jonny O\u2019Dwyer, Niall Murray, and Ronan Flynn. 2019. Eye-based Continuous Affect Prediction. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, 137\u2013143.", "Barros et al. (2018) Pablo Barros, Nikhil Churamani, Egor Lakomkin, Henrique Siqueira, Alexander Sutherland, and Stefan Wermter. 2018. The omg-emotion behavior dataset. In 2018 International Joint Conference on Neural Networks (IJCNN). IEEE, 1\u20137."]}], "citation_info_to_title": {"Han et al. (2019) Jing Han, Zixing Zhang, Zhao Ren, and Bjoern W Schuller. 2019. Emobed: Strengthening monomodal emotion recognition via training with crossmodal emotion embeddings. IEEE Transactions on Affective Computing (2019).": "Emobed: Strengthening Monomodal Emotion Recognition via Training with Crossmodal Emotion Embeddings", "Van Huynh et al. (2019) Thong Van Huynh, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim, and In-Seop Na. 2019. Emotion recognition by integrating eye movement analysis and facial expression model. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing. 166\u2013169.": "Emotion recognition by integrating eye movement analysis and facial expression model", "O\u2019Dwyer et al. (2019) Jonny O\u2019Dwyer, Niall Murray, and Ronan Flynn. 2019. Eye-based Continuous Affect Prediction. In 2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, 137\u2013143.": "Eye-based Continuous Affect Prediction", "Barros et al. (2018) Pablo Barros, Nikhil Churamani, Egor Lakomkin, Henrique Siqueira, Alexander Sutherland, and Stefan Wermter. 2018. The omg-emotion behavior dataset. In 2018 International Joint Conference on Neural Networks (IJCNN). IEEE, 1\u20137.": "The OMG-Emotion Behavior Dataset"}, "source_title_to_arxiv_id": {"The OMG-Emotion Behavior Dataset": "1803.05434"}}