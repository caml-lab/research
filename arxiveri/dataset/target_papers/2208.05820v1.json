{"title": "Hybrid Transformer Network for Deepfake Detection", "abstract": "Deepfake media is becoming widespread nowadays because of the easily\navailable tools and mobile apps which can generate realistic looking deepfake\nvideos/images without requiring any technical knowledge. With further advances\nin this field of technology in the near future, the quantity and quality of\ndeepfake media is also expected to flourish, while making deepfake media a\nlikely new practical tool to spread mis/disinformation. Because of these\nconcerns, the deepfake media detection tools are becoming a necessity. In this\nstudy, we propose a novel hybrid transformer network utilizing early feature\nfusion strategy for deepfake video detection. Our model employs two different\nCNN networks, i.e., (1) XceptionNet and (2) EfficientNet-B4 as feature\nextractors. We train both feature extractors along with the transformer in an\nend-to-end manner on FaceForensics++, DFDC benchmarks. Our model, while having\nrelatively straightforward architecture, achieves comparable results to other\nmore advanced state-of-the-art approaches when evaluated on FaceForensics++ and\nDFDC benchmarks. Besides this, we also propose novel face cut-out\naugmentations, as well as random cut-out augmentations. We show that the\nproposed augmentations improve the detection performance of our model and\nreduce overfitting. In addition to that, we show that our model is capable of\nlearning from considerably small amount of data.", "authors": ["Sohail Ahmed Khan", "Duc-Tien Dang-Nguyen"], "published_date": "2022_08_11", "pdf_url": "http://arxiv.org/pdf/2208.05820v1", "list_table_and_caption": [{"table": "<table><tr><td>Approach</td><td>Deepfakes</td><td>Face2Face</td><td>FaceSwap</td><td>NeuralTextures</td><td>Pristine</td><td>Cumulative</td></tr><tr><td>Steg. Features + SVM (Fridrich and Kodovsk\u00fd, 2012)</td><td>68.80%</td><td>67.69%</td><td>70.12%</td><td>69.21%</td><td>72.98%</td><td>70.97%</td></tr><tr><td>Cozzolino et al. (Cozzolino et al., 2017)</td><td>75.51%</td><td>86.34%</td><td>76.81%</td><td>75.34%</td><td>78.41%</td><td>78.45%</td></tr><tr><td>Bayar and Stamm (Bayar and Stamm, 2016)</td><td>90.25%</td><td>93.96%</td><td>87.74%</td><td>83.69%</td><td>77.02%</td><td>82.97%</td></tr><tr><td>Afchar et al. (Afchar et al., 2018)</td><td>89.55%</td><td>88.60%</td><td>81.24%</td><td>76.62%</td><td>82.19%</td><td>83.10%</td></tr><tr><td>Rossler et al. (R\u00f6ssler et al., 2019)</td><td>97.49%</td><td>97.69%</td><td>96.79%</td><td>92.19%</td><td>95.41%</td><td>95.73%</td></tr><tr><td>Qi et al. (Qi et al., 2020)</td><td>99.70%</td><td>98.90%</td><td>97.80%</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Ours (Face cut-out)</td><td>97.85%</td><td>97.85%</td><td>96.42%</td><td>90.71%</td><td>95.00%</td><td>95.57%</td></tr><tr><td>Ours (Random cut-out)</td><td>98.57%</td><td>98.57%</td><td>97.85%</td><td>92.14%</td><td>97.85%</td><td>97.00%</td></tr></table>", "caption": "Table 1. Performance (accuracy) comparison of a number of different deepfake detection baseline models on FaceForensics++ dataset. Each of the mentioned model was trained on all subsets of the FaceForensics++ dataset at once. Best results are highlighted.", "list_citation_info": ["R\u00f6ssler et al. (2019) Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV) (2019), 1\u201311.", "Bayar and Stamm (2016) Belhassen Bayar and Matthew C. Stamm. 2016. A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer. Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security (2016).", "Fridrich and Kodovsk\u00fd (2012) Jessica J. Fridrich and Jan Kodovsk\u00fd. 2012. Rich Models for Steganalysis of Digital Images. IEEE Transactions on Information Forensics and Security 7 (2012), 868\u2013882.", "Qi et al. (2020) Hua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, L. Ma, Wei Feng, Yang Liu, and Jianjun Zhao. 2020. DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms. In Proceedings of the 28th ACM International Conference on Multimedia.", "Cozzolino et al. (2017) Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. 2017. Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection. In Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security.", "Afchar et al. (2018) Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: a Compact Facial Video Forgery Detection Network. In Proceedings of IEEE International Workshop on Information Forensics and Security (WIFS). IEEE. {https://arxiv.org/abs/1809.00888}"]}, {"table": "<table><tr><td>Model</td><td>DF</td><td>F2F</td><td>FS</td><td>NT</td><td>Agg.</td></tr><tr><td>Rossler et al. (R\u00f6ssler et al., 2019)</td><td>92.48%</td><td>91.33%</td><td>92.63%</td><td>85.98%</td><td>90.60%</td></tr><tr><td>Ours (Random cut-out)</td><td>95.00%</td><td>95.00%</td><td>95.71%</td><td>90.00%</td><td>93.92%</td></tr></table>", "caption": "Table 4. We compare the performance (accuracy) of our model with the XceptionNet proposed by Rossler et al. in (R\u00f6ssler et al., 2019). In this table, DF refers to Deepfakes, F2F refers to Face2Face, FS refers to FaceSwap, NT refers to NeuralTextures.", "list_citation_info": ["R\u00f6ssler et al. (2019) Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV) (2019), 1\u201311."]}, {"table": "<table><tr><td>Approach</td><td>Train</td><td>Validation</td><td>Test</td></tr><tr><td>Rossler et al. (R\u00f6ssler et al., 2019)</td><td>388K</td><td>70K</td><td>70K</td></tr><tr><td>Zhu et al. (Zhu et al., 2021)</td><td>360K</td><td>70K</td><td>70K</td></tr><tr><td>Ours</td><td>200K</td><td>40K</td><td>11.2K</td></tr></table>", "caption": "Table 5. Number of training and validation images used by different deepfake detection techniques. The number of train, validation, and test sets of other studies in this table are rough estimates, as the the authors do not specify exact number of images they used to train their models.", "list_citation_info": ["Zhu et al. (2021) Xiangyu Zhu, Hao Wang, Hongyan Fei, Zhen Lei, and S. Li. 2021. Face Forgery Detection by 3D Decomposition. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2928\u20132938.", "R\u00f6ssler et al. (2019) Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV) (2019), 1\u201311."]}, {"table": "<table><tr><td>Approach</td><td>Dataset</td><td>Train Size</td><td>Accuracy</td></tr><tr><td>Mittal et al. (Mittal et al., 2020)</td><td>DFDC</td><td>-</td><td>84.40%</td></tr><tr><td>Wodajo et al. (Wodajo and Atnafu, 2021)</td><td>DFDC</td><td>112K</td><td>91.50%</td></tr><tr><td>Bondi et al. (Bondi et al., 2020)</td><td>DFDC</td><td>-</td><td>92.20%</td></tr><tr><td>Ours (Random cut-out)</td><td>DFDC</td><td>48K</td><td>98.24%</td></tr></table>", "caption": "Table 6. Performance (accuracy) comparison of a number of different deepfake detection baseline models on DFDC dataset. Best results are highlighted.", "list_citation_info": ["Mittal et al. (2020) Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. 2020. Emotions Don\u2019t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues. In Proceedings of the 28th ACM International Conference on Multimedia. ACM, 2823\u20132832. {https://arxiv.org/abs/2003.06711}", "Bondi et al. (2020) L. Bondi, Edoardo Daniele Cannas, Paolo Bestagini, and Stefano Tubaro. 2020. Training Strategies and Data Augmentations in CNN-based DeepFake Video Detection. Proceedings of the IEEE International Workshop on Information Forensics and Security (WIFS) (2020), 1\u20136.", "Wodajo and Atnafu (2021) Deressa Wodajo and Solomon Atnafu. 2021. Deepfake Video Detection Using Convolutional Vision Transformer. ArXiv abs/2102.11126 (2021)."]}], "citation_info_to_title": {"Mittal et al. (2020) Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. 2020. Emotions Don\u2019t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues. In Proceedings of the 28th ACM International Conference on Multimedia. ACM, 2823\u20132832. {https://arxiv.org/abs/2003.06711}": "Emotions Don\u2019t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues", "Qi et al. (2020) Hua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, L. Ma, Wei Feng, Yang Liu, and Jianjun Zhao. 2020. DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms. In Proceedings of the 28th ACM International Conference on Multimedia.": "DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms", "Bayar and Stamm (2016) Belhassen Bayar and Matthew C. Stamm. 2016. A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer. Proceedings of the 4th ACM Workshop on Information Hiding and Multimedia Security (2016).": "A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer", "R\u00f6ssler et al. (2019) Andreas R\u00f6ssler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. Proceedings of IEEE/CVF International Conference on Computer Vision (ICCV) (2019), 1\u201311.": "FaceForensics++: Learning to Detect Manipulated Facial Images", "Cozzolino et al. (2017) Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. 2017. Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection. In Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security.": "Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection", "Fridrich and Kodovsk\u00fd (2012) Jessica J. Fridrich and Jan Kodovsk\u00fd. 2012. Rich Models for Steganalysis of Digital Images. IEEE Transactions on Information Forensics and Security 7 (2012), 868\u2013882.": "Rich Models for Steganalysis of Digital Images", "Wodajo and Atnafu (2021) Deressa Wodajo and Solomon Atnafu. 2021. Deepfake Video Detection Using Convolutional Vision Transformer. ArXiv abs/2102.11126 (2021).": "Deepfake Video Detection Using Convolutional Vision Transformer", "Zhu et al. (2021) Xiangyu Zhu, Hao Wang, Hongyan Fei, Zhen Lei, and S. Li. 2021. Face Forgery Detection by 3D Decomposition. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2928\u20132938.": "Face Forgery Detection by 3D Decomposition", "Afchar et al. (2018) Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: a Compact Facial Video Forgery Detection Network. In Proceedings of IEEE International Workshop on Information Forensics and Security (WIFS). IEEE. {https://arxiv.org/abs/1809.00888}": "MesoNet: a Compact Facial Video Forgery Detection Network", "Bondi et al. (2020) L. Bondi, Edoardo Daniele Cannas, Paolo Bestagini, and Stefano Tubaro. 2020. Training Strategies and Data Augmentations in CNN-based DeepFake Video Detection. Proceedings of the IEEE International Workshop on Information Forensics and Security (WIFS) (2020), 1\u20136.": "Training Strategies and Data Augmentations in CNN-based DeepFake Video Detection"}, "source_title_to_arxiv_id": {"Deepfake Video Detection Using Convolutional Vision Transformer": "2102.11126"}}