{"title": "VIPriors 2: Visual Inductive Priors for Data-Efficient Deep Learning Challenges", "abstract": "The second edition of the \"VIPriors: Visual Inductive Priors for\nData-Efficient Deep Learning\" challenges featured five data-impaired\nchallenges, where models are trained from scratch on a reduced number of\ntraining samples for various key computer vision tasks. To encourage new and\ncreative ideas on incorporating relevant inductive biases to improve the data\nefficiency of deep learning models, we prohibited the use of pre-trained\ncheckpoints and other transfer learning techniques. The provided baselines are\noutperformed by a large margin in all five challenges, mainly thanks to\nextensive data augmentation policies, model ensembling, and data efficient\nnetwork architectures.", "authors": ["Attila Lengyel", "Robert-Jan Bruintjes", "Marcos Baptista Rios", "Osman Semih Kayhan", "Davide Zambrano", "Nergis Tomen", "Jan van Gemert"], "published_date": "2022_01_21", "pdf_url": "http://arxiv.org/pdf/2201.08625v1", "list_table_and_caption": [{"table": "<table><tr><td>Ranking</td><td>Teams</td><td>Encoder architectures</td><td>Data augmentation</td><td>Methods</td><td>Main metric</td></tr><tr><td colspan=\"2\">Classification</td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>Sun et al. </td><td>ResNeSt [73]</td><td>AutoAugment [16], MixUp [74],CutMix [70] </td><td>BlurPool [76],stochastic depth [31] </td><td>75.5</td></tr><tr><td>2</td><td>J. Wang et al. </td><td>ResNeSt [73], TResNet [51],RexNet [26], RegNet [49],Inception-ResNet [56] </td><td>AutoAugment [16], MixUp [74],CutMix [70], label smoothing [57] </td><td>DSB-Focalloss </td><td>75.2</td></tr><tr><td>2\\,\\dagger</td><td>Guo et al. </td><td>EfficientNet-b5/b6/b7 [58],DSK-ResNeXt101 [4, 67],ResNet-152 [29], SEResNet-152 [67] </td><td>AutoAugment [16], MixUp [74],CutMix [70], label smoothing [57],dropout [54], random erasing [79] </td><td>Contrastive Regularization,Mean Teacher [59],Symmetric Cross Entropy [64] </td><td>74.3</td></tr><tr><td>3 &amp; J</td><td>T. Wang et al. </td><td>ResNeSt-101/200 [73],SEResNeXt-101 [67] </td><td>HorizontalFlip, FiveCrop, TenCrop,label smoothing [57] </td><td>Iterative Partition-basedInvariant Risk Minimization </td><td>71.6</td></tr><tr><td colspan=\"2\">Object detection</td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>Lu et al. </td><td>YOLO 4-5 [2, 33]</td><td>Mosaic [2], MixUp [74],random color-jittering </td><td>Weighted Boxes Fusion [52] </td><td>30.5</td></tr><tr><td>1\\,\\mathsection</td><td>Zhang et al. </td><td>Cascade RCNN [5],DCN [19] </td><td>Multi-scale augmentation, TTA </td><td>MoCo v2 [12], Soft-NMS [3],Class-specific IoU thresholds </td><td>30.4</td></tr><tr><td>2</td><td>Niu et al. </td><td>Swin-T [40]</td><td>Hierarchical labeling</td><td>FPN [38], Soft-NMS [3],pseudo labeling </td><td>30.4</td></tr><tr><td>2\\,\\mathsection</td><td>Luo et al. </td><td>Cascade RCNN [5],DCN [19] </td><td>Albu, Top-Bottom Cut</td><td>GCNet [7], SimSiam [13],Soft-NMS [3] </td><td>30.1</td></tr><tr><td colspan=\"2\">Instance segmentation</td><td></td><td></td><td></td><td></td></tr><tr><td>1 &amp; J</td><td>Yunusov et al. [71] </td><td>CBSwin-T [37] </td><td>Location-aware MixUp [74],RandAugment [18],GridMask [11],Random scaling </td><td>Hybrid Task Cascade [9] </td><td>47.7</td></tr><tr><td>2</td><td>Yan et al. [68] </td><td>ResNet-101 [29] </td><td>Random brightness, color jitter,saturation, sharpening, blurring,noise, pixel shuffle, pixelization,filtering, hue transform </td><td>Cascade R-CNN [6]Switchable atrous convs. [46]Group normalization [65] </td><td>40.2</td></tr><tr><td>3</td><td>Chen et al. </td><td>Swin [40] </td><td>HorizontalFlip,Random scale and crop </td><td>Cascade Mask-RCNN [5]</td><td>36.6</td></tr><tr><td>4</td><td>Chen, Zheng </td><td>ResNet-50 [29]</td><td>Instaboost [23] </td><td>SCNet [61], Seesaw Loss [63],Deformable Convolutions [19] </td><td>18.5</td></tr><tr><td colspan=\"2\">Action recognition</td><td></td><td></td><td></td><td></td></tr><tr><td>1 &amp; J</td><td>Dave et al. </td><td>R3D[27], I3D[8], MViT[22]</td><td></td><td>TCLR[20]</td><td>0.74</td></tr><tr><td>1</td><td>Wu et al. </td><td>TPN[69], Slowfast (slow path)[24]</td><td>MixUp [74], CutMix [70]</td><td></td><td>0.66</td></tr><tr><td>2</td><td>Gao et al. </td><td>Swin [43], TPN [69], X3D[25],R2+1D[60], TimesFormer[1],Slowfast[24] </td><td></td><td></td><td>0.73</td></tr><tr><td colspan=\"2\">Re-identification</td><td></td><td></td><td></td><td></td></tr><tr><td>1</td><td>Liu et al. </td><td>ResNet [29], ResNetSt [73],SE-ResNetXt [67](24 models) </td><td>Difficult sample mining [53],Random Erasing [79],Local Grayscale Transform,affine transformations,pixel padding, random flip. </td><td>Triplet loss and circle loss,with augmentation test,re-ranking [78],query expansion [14] </td><td>96.5</td></tr><tr><td>2 &amp; J</td><td>Chen et al. </td><td>ResNet-IBN [29],SE ResNet-IBN [30] (5 models) </td><td>Video temporal mining,Random Erasing [79],pixel padding, random flip </td><td>Cross-entropy and Triplet loss,with augmentation test,re-ranking [78],6x schedule [28]. </td><td>96.4</td></tr><tr><td>3</td><td>Qi et al. </td><td>ResNet-IBN [29],23OSNet on Stronger Baseline </td><td>Random Erasing [79],color jitter, random flip,AutoAugment [16] </td><td>Cross-entropy and Triplet loss,re-ranking [78],query expansion [14] </td><td>94.2</td></tr><tr><td>4</td><td>Zheng et al. </td><td>ResNet-IBN [29]w/ spatial and channel attention </td><td>Random Erasing and Patch [79],color jitter, random flip,AutoAugment [16] </td><td>Cross-entropy,Triplet loss and circle loss </td><td>84.8</td></tr></table>", "caption": "TABLE I: Overview of challenge submissions. Bold-faced methods are contributions by the competitors.", "list_citation_info": ["[24] Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 6201\u20136210 (2019). https://doi.org/10.1109/ICCV.2019.00630", "[37] Liang, T., Chu, X., Liu, Y., Wang, Y., Tang, Z., Chu, W., Chen, J., Ling, H.: Cbnetv2: A composite backbone network architecture for object detection. arXiv preprint arXiv:2107.00420 (2021)", "[29] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)", "[52] Roman Solovyev, W.W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes from different object detection models. Image and Vision Computing 107, 104117 (2021)", "[71] Yunusov, J., Rakhmatov, S., Namozov, A., Gaybulayev, A., Kim, T.H.: Instance segmentation challenge track technical report, vipriors workshop at iccv 2021: Task-specific copy-paste data augmentation method for instance segmentation (2021)", "[54] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15(56), 1929\u20131958 (2014), http://jmlr.org/papers/v15/srivastava14a.html", "[23] Fang, H.S., Sun, J., Wang, R., Gou, M., Li, Y.L., Lu, C.: Instaboost: Boosting instance segmentation via probability map guided copy-pasting. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 682\u2013691 (2019)", "[59] Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780 (2017)", "[5] Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6154\u20136162 (2018)", "[26] Han, D., Yun, S., Heo, B., Yoo, Y.: Rethinking channel dimensions for efficient model design. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 732\u2013741 (2021)", "[6] Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2018)", "[53] Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors with online hard example mining. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016)", "[78] Zhong, Z., Zheng, L., Cao, D., Li, S.: Re-ranking person re-identification with k-reciprocal encoding. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1318\u20131327 (2017)", "[2] Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speed and accuracy of object detection (2020)", "[20] Dave, I.R., Gupta, R., Rizve, M.N., Shah, M.: TCLR: temporal contrastive learning for video representation. arXiv preprint arXiv:2101.07974 (2021)", "[65] Wu, Y., He, K.: Group normalization. In: ECCV (2018)", "[14] Chum, O., Philbin, J., Sivic, J., Isard, M., Zisserman, A.: Total recall: Automatic query expansion with a generative feature model for object retrieval. In: 2007 IEEE 11th International Conference on Computer Vision. pp. 1\u20138. IEEE (2007)", "[28] He, K., Girshick, R., Doll\u00e1r, P.: Rethinking imagenet pre-training. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4918\u20134927 (2019)", "[25] Feichtenhofer, C.: X3d: Expanding architectures for efficient video recognition. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 200\u2013210 (2020). https://doi.org/10.1109/CVPR42600.2020.00028", "[30] Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7132\u20137141 (2018)", "[18] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.: Randaugment: Practical automated data augmentation with a reduced search space. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 18613\u201318624. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf", "[70] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6023\u20136032 (2019)", "[4] Bruintjes, R.J., Lengyel, A., Rios, M.B., Kayhan, O.S., van Gemert, J.: Vipriors 1: Visual inductive priors for data-efficient deep learning challenges. arXiv preprint arXiv:2103.03768 (2021)", "[31] Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth. In: European conference on computer vision. pp. 646\u2013661. Springer (2016)", "[60] Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look at spatiotemporal convolutions for action recognition. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6450\u20136459 (2018). https://doi.org/10.1109/CVPR.2018.00675", "[19] Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolutional networks. In: Proceedings of the IEEE international conference on computer vision. pp. 764\u2013773 (2017)", "[40] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) (2021)", "[13] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)", "[68] Yan, B., Qi, F., Cao, L., Wang, H.: The second place solution for iccv2021 vipriors instance segmentation challenge (2021)", "[1] Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095 (2021)", "[7] Cao, Y., Xu, J., Lin, S., Wei, F., Hu, H.: Gcnet: Non-local networks meet squeeze-excitation networks and beyond. In: Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. pp. 0\u20130 (2019)", "[49] Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Doll\u00e1r, P.: Designing network design spaces. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10428\u201310436 (2020)", "[67] Xie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1492\u20131500 (2017)", "[9] Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Shi, J., Ouyang, W., Loy, C.C., Lin, D.: Hybrid task cascade for instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)", "[3] Bodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-nms\u2013improving object detection with one line of code. In: Proceedings of the IEEE international conference on computer vision. pp. 5561\u20135569 (2017)", "[43] Niu, J., Gu, Y., Nie, L., You, C.: Semi-supervised transformer with fpn for bikes parts detection (2021)", "[73] Zhang, H., Wu, C., Zhang, Z., Zhu, Y., Lin, H., Zhang, Z., Sun, Y., He, T., Mueller, J., Manmatha, R., Li, M., Smola, A.: Resnest: Split-attention networks (2020)", "[27] Hara, K., Kataoka, H., Satoh, Y.: Towards good practice for action recognition with spatiotemporal 3d convolutions. In: 2018 24th International Conference on Pattern Recognition (ICPR). pp. 2516\u20132521 (2018). https://doi.org/10.1109/ICPR.2018.8546325", "[8] Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the kinetics dataset. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4724\u20134733 (2017). https://doi.org/10.1109/CVPR.2017.502", "[38] Lin, T.Y., Doll\u00e1r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection (2017)", "[12] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)", "[57] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818\u20132826 (2016)", "[76] Zhang, R.: Making convolutional networks shift-invariant again. In: International conference on machine learning. pp. 7324\u20137334. PMLR (2019)", "[11] Chen, P., Liu, S., Zhao, H., Jia, J.: Gridmask data augmentation (2020)", "[22] Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.: Multiscale vision transformers. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021)", "[69] Yang, C., Xu, Y., Shi, J., Dai, B., Zhou, B.: Temporal pyramid network for action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)", "[58] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning. pp. 6105\u20136114. PMLR (2019)", "[79] Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmentation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 13001\u201313008 (2020)", "[63] Wang, J., Zhang, W., Zang, Y., Cao, Y., Pang, J., Gong, T., Chen, K., Liu, Z., Loy, C.C., Lin, D.: Seesaw loss for long-tailed instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9695\u20139704 (June 2021)", "[64] Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., Bailey, J.: Symmetric cross entropy for robust learning with noisy labels. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 322\u2013330 (2019)", "[16] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501 (2018)", "[74] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=r1Ddp1-Rb", "[61] Vu, T., Haeyong, K., Yoo, C.D.: Scnet: Training inference sample consistency for instance segmentation. In: AAAI (2021)", "[46] Qiao, S., Chen, L.C., Yuille, A.: Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution (2020)", "[51] Ridnik, T., Lawen, H., Noy, A., Ben Baruch, E., Sharir, G., Friedman, I.: Tresnet: High performance gpu-dedicated architecture. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1400\u20131409 (2021)", "[56] Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: Thirty-first AAAI conference on artificial intelligence (2017)"]}], "citation_info_to_title": {"[59] Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780 (2017)": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results", "[2] Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M.: Yolov4: Optimal speed and accuracy of object detection (2020)": "Yolov4: Optimal speed and accuracy of object detection", "[11] Chen, P., Liu, S., Zhao, H., Jia, J.: Gridmask data augmentation (2020)": "Gridmask Data Augmentation", "[51] Ridnik, T., Lawen, H., Noy, A., Ben Baruch, E., Sharir, G., Friedman, I.: Tresnet: High performance gpu-dedicated architecture. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1400\u20131409 (2021)": "Tresnet: High performance gpu-dedicated architecture", "[24] Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 6201\u20136210 (2019). https://doi.org/10.1109/ICCV.2019.00630": "Slowfast networks for video recognition", "[9] Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Shi, J., Ouyang, W., Loy, C.C., Lin, D.: Hybrid task cascade for instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019)": "Hybrid task cascade for instance segmentation", "[16] Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501 (2018)": "Autoaugment: Learning augmentation policies from data", "[27] Hara, K., Kataoka, H., Satoh, Y.: Towards good practice for action recognition with spatiotemporal 3d convolutions. In: 2018 24th International Conference on Pattern Recognition (ICPR). pp. 2516\u20132521 (2018). https://doi.org/10.1109/ICPR.2018.8546325": "Towards good practice for action recognition with spatiotemporal 3d convolutions", "[28] He, K., Girshick, R., Doll\u00e1r, P.: Rethinking imagenet pre-training. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4918\u20134927 (2019)": "Rethinking imagenet pre-training", "[38] Lin, T.Y., Doll\u00e1r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection (2017)": "Feature Pyramid Networks for Object Detection", "[49] Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Doll\u00e1r, P.: Designing network design spaces. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10428\u201310436 (2020)": "Designing network design spaces", "[23] Fang, H.S., Sun, J., Wang, R., Gou, M., Li, Y.L., Lu, C.: Instaboost: Boosting instance segmentation via probability map guided copy-pasting. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 682\u2013691 (2019)": "Instaboost: Boosting instance segmentation via probability map guided copy-pasting", "[70] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6023\u20136032 (2019)": "Cutmix: Regularization strategy to train strong classifiers with localizable features", "[25] Feichtenhofer, C.: X3d: Expanding architectures for efficient video recognition. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 200\u2013210 (2020). https://doi.org/10.1109/CVPR42600.2020.00028": "X3d: Expanding architectures for efficient video recognition", "[67] Xie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1492\u20131500 (2017)": "Aggregated residual transformations for deep neural networks", "[53] Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors with online hard example mining. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016)": "Training region-based object detectors with online hard example mining", "[30] Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7132\u20137141 (2018)": "Squeeze-and-excitation networks", "[7] Cao, Y., Xu, J., Lin, S., Wei, F., Hu, H.: Gcnet: Non-local networks meet squeeze-excitation networks and beyond. In: Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. pp. 0\u20130 (2019)": "Gcnet: Non-local networks meet squeeze-excitation networks and beyond", "[78] Zhong, Z., Zheng, L., Cao, D., Li, S.: Re-ranking person re-identification with k-reciprocal encoding. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1318\u20131327 (2017)": "Re-ranking person re-identification with k-reciprocal encoding", "[20] Dave, I.R., Gupta, R., Rizve, M.N., Shah, M.: TCLR: temporal contrastive learning for video representation. arXiv preprint arXiv:2101.07974 (2021)": "TCLR: Temporal Contrastive Learning for Video Representation", "[74] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=r1Ddp1-Rb": "mixup: Beyond Empirical Risk Minimization", "[12] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)": "Improved Baselines with Momentum Contrastive Learning", "[19] Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolutional networks. In: Proceedings of the IEEE international conference on computer vision. pp. 764\u2013773 (2017)": "Deformable Convolutional Networks", "[46] Qiao, S., Chen, L.C., Yuille, A.: Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution (2020)": "Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution", "[1] Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095 (2021)": "Is space-time attention all you need for video understanding?", "[14] Chum, O., Philbin, J., Sivic, J., Isard, M., Zisserman, A.: Total recall: Automatic query expansion with a generative feature model for object retrieval. In: 2007 IEEE 11th International Conference on Computer Vision. pp. 1\u20138. IEEE (2007)": "Total recall: Automatic query expansion with a generative feature model for object retrieval", "[3] Bodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-nms\u2013improving object detection with one line of code. In: Proceedings of the IEEE international conference on computer vision. pp. 5561\u20135569 (2017)": "Soft-nms\u2013improving object detection with one line of code", "[76] Zhang, R.: Making convolutional networks shift-invariant again. In: International conference on machine learning. pp. 7324\u20137334. PMLR (2019)": "Making convolutional networks shift-invariant again", "[71] Yunusov, J., Rakhmatov, S., Namozov, A., Gaybulayev, A., Kim, T.H.: Instance segmentation challenge track technical report, vipriors workshop at iccv 2021: Task-specific copy-paste data augmentation method for instance segmentation (2021)": "Instance Segmentation Challenge Track Technical Report", "[61] Vu, T., Haeyong, K., Yoo, C.D.: Scnet: Training inference sample consistency for instance segmentation. In: AAAI (2021)": "Scnet: Training Inference Sample Consistency for Instance Segmentation", "[73] Zhang, H., Wu, C., Zhang, Z., Zhu, Y., Lin, H., Zhang, Z., Sun, Y., He, T., Mueller, J., Manmatha, R., Li, M., Smola, A.: Resnest: Split-attention networks (2020)": "Resnest: Split-attention networks", "[5] Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 6154\u20136162 (2018)": "Cascade r-cnn: Delving into high quality object detection", "[40] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. International Conference on Computer Vision (ICCV) (2021)": "Swin transformer: Hierarchical vision transformer using shifted windows", "[8] Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the kinetics dataset. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4724\u20134733 (2017). https://doi.org/10.1109/CVPR.2017.502": "Quo vadis, action recognition? a new model and the kinetics dataset", "[18] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.: Randaugment: Practical automated data augmentation with a reduced search space. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 18613\u201318624. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper/2020/file/d85b63ef0ccb114d0a3bb7b7d808028f-Paper.pdf": "Randaugment: Practical automated data augmentation with a reduced search space", "[58] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning. pp. 6105\u20136114. PMLR (2019)": "Efficientnet: Rethinking Model Scaling for Convolutional Neural Networks", "[52] Roman Solovyev, W.W., Gabruseva, T.: Weighted boxes fusion: Ensembling boxes from different object detection models. Image and Vision Computing 107, 104117 (2021)": "Weighted Boxes Fusion: Ensembling Boxes from Different Object Detection Models", "[79] Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmentation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 34, pp. 13001\u201313008 (2020)": "Random Erasing Data Augmentation", "[13] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)": "Exploring Simple Siamese Representation Learning", "[64] Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., Bailey, J.: Symmetric cross entropy for robust learning with noisy labels. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 322\u2013330 (2019)": "Symmetric Cross Entropy for Robust Learning with Noisy Labels", "[63] Wang, J., Zhang, W., Zang, Y., Cao, Y., Pang, J., Gong, T., Chen, K., Liu, Z., Loy, C.C., Lin, D.: Seesaw loss for long-tailed instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9695\u20139704 (June 2021)": "Seesaw loss for long-tailed instance segmentation", "[60] Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look at spatiotemporal convolutions for action recognition. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6450\u20136459 (2018). https://doi.org/10.1109/CVPR.2018.00675": "A closer look at spatiotemporal convolutions for action recognition", "[6] Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2018)": "Cascade r-cnn: Delving into high quality object detection", "[57] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818\u20132826 (2016)": "Rethinking the inception architecture for computer vision", "[69] Yang, C., Xu, Y., Shi, J., Dai, B., Zhou, B.: Temporal pyramid network for action recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)": "Temporal Pyramid Network for Action Recognition", "[65] Wu, Y., He, K.: Group normalization. In: ECCV (2018)": "Group Normalization", "[29] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)": "Deep Residual Learning for Image Recognition", "[43] Niu, J., Gu, Y., Nie, L., You, C.: Semi-supervised transformer with fpn for bikes parts detection (2021)": "Semi-supervised transformer with fpn for bikes parts detection", "[31] Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth. In: European conference on computer vision. pp. 646\u2013661. Springer (2016)": "Deep Networks with Stochastic Depth", "[54] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15(56), 1929\u20131958 (2014), http://jmlr.org/papers/v15/srivastava14a.html": "Dropout: A simple way to prevent neural networks from overfitting", "[68] Yan, B., Qi, F., Cao, L., Wang, H.: The second place solution for iccv2021 vipriors instance segmentation challenge (2021)": "The second place solution for ICCV2021 VIPriors Instance Segmentation Challenge", "[56] Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: Thirty-first AAAI conference on artificial intelligence (2017)": "Inception-v4, inception-resnet and the impact of residual connections on learning", "[26] Han, D., Yun, S., Heo, B., Yoo, Y.: Rethinking channel dimensions for efficient model design. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 732\u2013741 (2021)": "Rethinking channel dimensions for efficient model design", "[4] Bruintjes, R.J., Lengyel, A., Rios, M.B., Kayhan, O.S., van Gemert, J.: Vipriors 1: Visual inductive priors for data-efficient deep learning challenges. arXiv preprint arXiv:2103.03768 (2021)": "Vipriors 1: Visual inductive priors for data-efficient deep learning challenges", "[37] Liang, T., Chu, X., Liu, Y., Wang, Y., Tang, Z., Chu, W., Chen, J., Ling, H.: Cbnetv2: A composite backbone network architecture for object detection. arXiv preprint arXiv:2107.00420 (2021)": "Cbnetv2: A Composite Backbone Network Architecture for Object Detection", "[22] Fan, H., Xiong, B., Mangalam, K., Li, Y., Yan, Z., Malik, J., Feichtenhofer, C.: Multiscale vision transformers. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021)": "Multiscale vision transformers"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}