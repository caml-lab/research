{"title": "Resolving Semantic Confusions for Improved Zero-Shot Detection", "abstract": "Zero-shot detection (ZSD) is a challenging task where we aim to recognize and\nlocalize objects simultaneously, even when our model has not been trained with\nvisual samples of a few target (\"unseen\") classes. Recently, methods employing\ngenerative models like GANs have shown some of the best results, where\nunseen-class samples are generated based on their semantics by a GAN trained on\nseen-class data, enabling vanilla object detectors to recognize unseen objects.\nHowever, the problem of semantic confusion still remains, where the model is\nsometimes unable to distinguish between semantically-similar classes. In this\nwork, we propose to train a generative model incorporating a triplet loss that\nacknowledges the degree of dissimilarity between classes and reflects them in\nthe generated samples. Moreover, a cyclic-consistency loss is also enforced to\nensure that generated visual samples of a class highly correspond to their own\nsemantics. Extensive experiments on two benchmark ZSD datasets - MSCOCO and\nPASCAL-VOC - demonstrate significant gains over the current ZSD methods,\nreducing semantic confusion and improving detection for the unseen classes.", "authors": ["Sandipan Sarma", "Sushil Kumar", "Arijit Sur"], "published_date": "2022_12_12", "pdf_url": "http://arxiv.org/pdf/2212.06097v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Metric</td><td>Method</td><td>ZSD</td><td colspan=\"3\">GZSD</td></tr><tr><td></td><td></td><td></td><td>Seen</td><td>Unseen</td><td>HM</td></tr><tr><td rowspan=\"7\">mAP</td><td>PL [Rahman et al.(2018a)Rahman, Khan, andBarnes]</td><td>12.40</td><td>34.07</td><td>12.40</td><td>18.18</td></tr><tr><td>BLC [Zheng et al.(2020)Zheng, Huang, Han, Huang, andCui]</td><td>14.70</td><td>36.00</td><td>13.10</td><td>19.20</td></tr><tr><td>ACS-ZSD [Mao et al.(2020)Mao, Wang, Yu, Zheng, and Li]</td><td>15.34</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SUZOD[Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, andKhan]</td><td>17.30</td><td>37.40</td><td>17.30</td><td>23.65</td></tr><tr><td>ZSDTR [Zheng and Cui(2021)]</td><td>13.20</td><td>40.55</td><td>13.22</td><td>20.16</td></tr><tr><td>ContrastZSD [Yan et al.(2022)Yan, Chang, Luo, Liu, Zhang, andZheng]</td><td>18.60</td><td>40.20</td><td>16.50</td><td>23.40</td></tr><tr><td>Ours</td><td>20.10</td><td>37.40</td><td>20.10</td><td>26.15</td></tr><tr><td rowspan=\"7\">RE@100</td><td>PL [Rahman et al.(2018a)Rahman, Khan, andBarnes]</td><td>37.72</td><td>36.38</td><td>37.16</td><td>36.76</td></tr><tr><td>BLC [Zheng et al.(2020)Zheng, Huang, Han, Huang, andCui]</td><td>54.68</td><td>56.39</td><td>51.65</td><td>53.92</td></tr><tr><td>ACS-ZSD[Mao et al.(2020)Mao, Wang, Yu, Zheng, and Li]</td><td>47.83</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SUZOD[Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, andKhan]</td><td>61.40</td><td>58.60</td><td>60.80</td><td>59.67</td></tr><tr><td>ZSDTR [Zheng and Cui(2021)]</td><td>60.30</td><td>69.12</td><td>59.45</td><td>61.12</td></tr><tr><td>ContrastZSD [Yan et al.(2022)Yan, Chang, Luo, Liu, Zhang, andZheng]</td><td>59.50</td><td>62.90</td><td>58.60</td><td>60.70</td></tr><tr><td>Ours</td><td>65.10</td><td>58.60</td><td>64.00</td><td>61.18</td></tr></tbody></table>", "caption": "Table 1: ZSD and GZSD performance of various methods on MSCOCO in terms of mAP and Recall@100 (RE@100) at an IoU threshold of 0.5. HM denotes the harmonic mean of seen and unseen results for GZSD. The best results and second-best results are shown in red and blue respectively. Results are shown for the 65/15 split of MSCOCO.", "list_citation_info": ["[Zheng et al.(2020)Zheng, Huang, Han, Huang, and Cui] Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, and Li Cui. Background learnable cascade for zero-shot object detection. In Proceedings of the Asian Conference on Computer Vision, 2020.", "[Zheng and Cui(2021)] Ye Zheng and Li Cui. Zero-shot object detection with transformers. In 2021 IEEE International Conference on Image Processing (ICIP), pages 444\u2013448. IEEE, 2021.", "[Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, and Khan] Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection. In Proceedings of the Asian Conference on Computer Vision, 2020.", "[Yan et al.(2022)Yan, Chang, Luo, Liu, Zhang, and Zheng] Caixia Yan, Xiaojun Chang, Minnan Luo, Huan Liu, Xiaoqin Zhang, and Qinghua Zheng. Semantics-guided contrastive network for zero-shot object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.", "[Rahman et al.(2018a)Rahman, Khan, and Barnes] Shafin Rahman, Salman Khan, and Nick Barnes. Polarity loss for zero-shot object detection. arXiv preprint arXiv:1811.08982, 2018a.", "[Mao et al.(2020)Mao, Wang, Yu, Zheng, and Li] Qiaomei Mao, Chong Wang, Shenghao Yu, Ye Zheng, and Yuqi Li. Zero-shot object detection with attributes-based category similarity. IEEE Transactions on Circuits and Systems II: Express Briefs, 67(5):921\u2013925, 2020."]}, {"table": "<p>MethodOverallaeroplanetrainp.metercatbearsuitcasefrisbeesnowbrd.forksandwichhot dogtoiletmousetoasterhair drierPL[Rahman et al.(2018a)Rahman, Khan, andBarnes]12.4020.048.20.6328.313.812.421.815.18.98.50.875.70.041.70.03ACS-ZSD[Mao et al.(2020)Mao, Wang, Yu, Zheng, and Li]15.348.7225.56.5940.854.09.5510.626.816.411.04.997.836.211.320.0SUZOD[Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, andKhan]17.3017.846.30.763.141.010.50.730.216.517.60.013.41.60.40.2Ours20.1022.953.30.664.954.313.21.231.215.722.60.017.52.70.70.2</p>", "caption": "Table 2:  Class-wise average precisions (APs) on unseen classes (ZSD) from MSCOCO with an IoU threshold of 0.5. The best results and second-best results are shown in red and blue.", "list_citation_info": ["[Rahman et al.(2018a)Rahman, Khan, and Barnes] Shafin Rahman, Salman Khan, and Nick Barnes. Polarity loss for zero-shot object detection. arXiv preprint arXiv:1811.08982, 2018a.", "[Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, and Khan] Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection. In Proceedings of the Asian Conference on Computer Vision, 2020.", "[Mao et al.(2020)Mao, Wang, Yu, Zheng, and Li] Qiaomei Mao, Chong Wang, Shenghao Yu, Ye Zheng, and Yuqi Li. Zero-shot object detection with attributes-based category similarity. IEEE Transactions on Circuits and Systems II: Express Briefs, 67(5):921\u2013925, 2020."]}, {"table": "<p>Methodseenunseenaeroplanebicyclebirdboatbottlebuscatchaircowd.tablehorsemotorbikepersonp.plantsheeptvmonitorcardogsofatrainHRE [Demirel et al.(2018)Demirel, Cinbis, andIkizler-Cinbis]65.654.270.073.076.054.042.086.064.040.054.075.080.080.075.034.069.079.055.082.055.026.0PL [Rahman et al.(2018a)Rahman, Khan, andBarnes]63.562.174.471.267.050.150.867.684.744.868.639.674.976.079.539.661.666.163.787.253.244.1SAN [Rahman et al.(2018b)Rahman, Khan, andPorikli]69.657.671.478.574.961.448.276.089.151.178.461.684.276.876.942.571.071.756.285.362.626.4BLC [Zheng et al.(2020)Zheng, Huang, Han, Huang, andCui]75.155.278.583.277.667.770.175.687.455.977.571.285.282.877.656.177.178.543.786.060.830.1SUZOD [Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, andKhan]74.763.180.684.278.766.767.674.091.561.175.063.482.685.784.947.876.974.855.292.359.046.1Ours74.762.780.484.178.766.667.673.891.461.175.063.783.185.584.947.376.774.755.692.657.545.0</p>", "caption": "Table 3: mAP (in %) for PASCAL-VOC dataset. The unseen classes are shown in italic. The best and second-best results are shown in red and blue.", "list_citation_info": ["[Zheng et al.(2020)Zheng, Huang, Han, Huang, and Cui] Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, and Li Cui. Background learnable cascade for zero-shot object detection. In Proceedings of the Asian Conference on Computer Vision, 2020.", "[Rahman et al.(2018b)Rahman, Khan, and Porikli] Shafin Rahman, Salman Khan, and Fatih Porikli. Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts. In Asian Conference on Computer Vision, pages 547\u2013563. Springer, 2018b.", "[Demirel et al.(2018)Demirel, Cinbis, and Ikizler-Cinbis] Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis. Zero-shot object detection by hybrid region embedding. In BMVC, 2018.", "[Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, and Khan] Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection. In Proceedings of the Asian Conference on Computer Vision, 2020.", "[Rahman et al.(2018a)Rahman, Khan, and Barnes] Shafin Rahman, Salman Khan, and Nick Barnes. Polarity loss for zero-shot object detection. arXiv preprint arXiv:1811.08982, 2018a."]}], "citation_info_to_title": {"[Yan et al.(2022)Yan, Chang, Luo, Liu, Zhang, and Zheng] Caixia Yan, Xiaojun Chang, Minnan Luo, Huan Liu, Xiaoqin Zhang, and Qinghua Zheng. Semantics-guided contrastive network for zero-shot object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.": "Semantics-guided contrastive network for zero-shot object detection", "[Rahman et al.(2018a)Rahman, Khan, and Barnes] Shafin Rahman, Salman Khan, and Nick Barnes. Polarity loss for zero-shot object detection. arXiv preprint arXiv:1811.08982, 2018a.": "Polarity loss for zero-shot object detection", "[Rahman et al.(2018b)Rahman, Khan, and Porikli] Shafin Rahman, Salman Khan, and Fatih Porikli. Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts. In Asian Conference on Computer Vision, pages 547\u2013563. Springer, 2018b.": "Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts", "[Demirel et al.(2018)Demirel, Cinbis, and Ikizler-Cinbis] Berkan Demirel, Ramazan Gokberk Cinbis, and Nazli Ikizler-Cinbis. Zero-shot object detection by hybrid region embedding. In BMVC, 2018.": "Zero-shot object detection by hybrid region embedding", "[Zheng and Cui(2021)] Ye Zheng and Li Cui. Zero-shot object detection with transformers. In 2021 IEEE International Conference on Image Processing (ICIP), pages 444\u2013448. IEEE, 2021.": "Zero-shot object detection with transformers", "[Hayat et al.(2020)Hayat, Hayat, Rahman, Khan, Zamir, and Khan] Nasir Hayat, Munawar Hayat, Shafin Rahman, Salman Khan, Syed Waqas Zamir, and Fahad Shahbaz Khan. Synthesizing the unseen for zero-shot object detection. In Proceedings of the Asian Conference on Computer Vision, 2020.": "Synthesizing the unseen for zero-shot object detection", "[Mao et al.(2020)Mao, Wang, Yu, Zheng, and Li] Qiaomei Mao, Chong Wang, Shenghao Yu, Ye Zheng, and Yuqi Li. Zero-shot object detection with attributes-based category similarity. IEEE Transactions on Circuits and Systems II: Express Briefs, 67(5):921\u2013925, 2020.": "Zero-shot object detection with attributes-based category similarity", "[Zheng et al.(2020)Zheng, Huang, Han, Huang, and Cui] Ye Zheng, Ruoran Huang, Chuanqi Han, Xi Huang, and Li Cui. Background learnable cascade for zero-shot object detection. In Proceedings of the Asian Conference on Computer Vision, 2020.": "Background Learnable Cascade for Zero-Shot Object Detection"}, "source_title_to_arxiv_id": {"Polarity loss for zero-shot object detection": "1811.08982", "Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts": "1803.06049"}}