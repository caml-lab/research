{"title": "CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains", "abstract": "Unsupervised Domain Adaptation demonstrates great potential to mitigate\ndomain shifts by transferring models from labeled source domains to unlabeled\ntarget domains. While Unsupervised Domain Adaptation has been applied to a wide\nvariety of complex vision tasks, only few works focus on lane detection for\nautonomous driving. This can be attributed to the lack of publicly available\ndatasets. To facilitate research in these directions, we propose CARLANE, a\n3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE\nencompasses the single-target datasets MoLane and TuLane and the multi-target\ndataset MuLane. These datasets are built from three different domains, which\ncover diverse scenes and contain a total of 163K unique images, 118K of which\nare annotated. In addition we evaluate and report systematic baselines,\nincluding our own method, which builds upon Prototypical Cross-domain\nSelf-supervised Learning. We find that false positive and false negative rates\nof the evaluated domain adaptation methods are high compared to those of fully\nsupervised baselines. This affirms the need for benchmarks such as CARLANE to\nfurther strengthen research in Unsupervised Domain Adaptation for lane\ndetection. CARLANE, all evaluated models and the corresponding implementations\nare publicly available at https://carlanebenchmark.github.io.", "authors": ["Julian Gebele", "Bonifaz Stuhr", "Johann Haselberger"], "published_date": "2022_06_16", "pdf_url": "http://arxiv.org/pdf/2206.08083v3", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th>domain</th><th>total images</th><th>train</th><th>validation</th><th>test</th><th>lanes</th></tr></thead><tbody><tr><td rowspan=\"2\">MoLane</td><td>CARLA simulation</td><td>84,000</td><td>80,000</td><td>4,000</td><td>-</td><td>\\leq 2</td></tr><tr><td>model vehicle</td><td>46,843</td><td>43,843*</td><td>2,000</td><td>1,000</td><td>\\leq 2</td></tr><tr><td rowspan=\"2\">TuLane</td><td>CARLA simulation</td><td>26,400</td><td>24,000</td><td>2,400</td><td>-</td><td>\\leq 4</td></tr><tr><td>TuSimple TuSimple2017 </td><td>6,408</td><td>3,268</td><td>358</td><td>2,782</td><td>\\leq 4</td></tr><tr><td rowspan=\"2\">MuLane</td><td>CARLA simulation</td><td>52,800</td><td>48,000</td><td>4,800</td><td>-</td><td>\\leq 4</td></tr><tr><td>model vehicle + TuSimple TuSimple2017 </td><td>12,536</td><td>6,536**</td><td>4,000</td><td>2,000</td><td>\\leq 4</td></tr></tbody></table>", "caption": "Table 1: Dataset overview. Unlabeled images denoted by *, partially labeled images denoted by ** ", "list_citation_info": ["(20) TuSimple, \u201cTuSimple-benchmark.\u201d https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection. Accessed: 2021-11-16."]}, {"table": "<table><thead><tr><th>Method</th><th>Initial Learning Rate</th><th>Scheduler</th><th>Batch Size</th><th>Epochs</th><th>Losses</th><th>Other Changes</th></tr></thead><tbody><tr><td>UFLD-SO</td><td>4e^{-4}</td><td>Cosine Annealing</td><td>4</td><td>150</td><td>cls, sim, aux</td><td>-</td></tr><tr><td>DANN</td><td>1e^{-5}, C: 1e^{-3}</td><td>\\frac{1e^{-5}}{(1+10p)^{0.75}}</td><td>4</td><td>30</td><td>cls, sim, aux, adv Ganin2016 </td><td>C: 3 fc layers (1024-1024-2)</td></tr><tr><td>ADDA</td><td>1e^{-6}, D: 1e^{-3}</td><td>Constant</td><td>16</td><td>30</td><td>map Tzeng2017ADDA , adv Tzeng2017ADDA </td><td>D: 3 fc layers (500-500-2)</td></tr><tr><td>SGADA</td><td>1e^{-6}, D: 1e^{-3}</td><td>Constant</td><td>16</td><td>15</td><td>map Tzeng2017ADDA , adv Tzeng2017ADDA , pseudo: 0.25</td><td>Pseudo label selection</td></tr><tr><td rowspan=\"2\">SGPCS</td><td rowspan=\"2\">4e^{-4}</td><td rowspan=\"2\">Cosine Annealing</td><td rowspan=\"2\">16</td><td rowspan=\"2\">10</td><td>in-domain yue2021prototypical , cross-domain yue2021prototypical </td><td rowspan=\"2\">-</td></tr><tr><td>cls, sim, aux, pseudo: 0.25</td></tr><tr><td>UFLD-TO</td><td>4e^{-4}</td><td>Cosine Annealing</td><td>4</td><td>300</td><td>cls, sim, aux</td><td>-</td></tr></tbody></table>", "caption": "Table 2: Optimized hyperparameters to achieve the reported results. C denotes domain classifier parameters, D denotes domain discriminator parameters, adv the adversarial loss from Tzeng2017ADDA  and cls the classifier loss, sim the similarity loss and aux the auxiliary loss from qin2020ultra . Loss weights are set to 1.0 unless stated otherwise.", "list_citation_info": ["(22) X. Yue, Z. Zheng, S. Zhang, Y. Gao, T. Darrell, K. Keutzer, and A. Sangiovanni-Vincentelli, \u201cPrototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.", "(13) E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, \u201cAdversarial Discriminative Domain Adaptation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.", "(12) Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky, \u201cDomain-Adversarial Training of Neural Networks,\u201d Journal of Machine Learning Research, vol. 17, no. 59, pp. 1\u201335, 2016.", "(2) Z. Qin, H. Wang, and X. Li, \u201cUltra Fast Structure-aware Deep Lane Detection,\u201d in The European Conference on Computer Vision (ECCV), 2020."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">ResNet-18</td><td colspan=\"2\">MoLane</td><td colspan=\"3\">TuLane</td><td colspan=\"3\">MuLane</td></tr><tr><td>LA</td><td>FP &amp; FN</td><td>LA</td><td>FP</td><td>FN</td><td>LA</td><td>FP</td><td>FN</td></tr><tr><td>UFLD-SO</td><td>89.39</td><td>25.25</td><td>87.43</td><td>34.21</td><td>23.48</td><td>88.02</td><td>50.24</td><td>26.08</td></tr><tr><td>DANN Ganin2016 </td><td>87.65\\pm0.48</td><td>29.97\\pm1.21</td><td>88.74\\pm0.32</td><td>32.71\\pm0.52</td><td>21.64\\pm0.65</td><td>86.01\\pm0.67</td><td>55.33\\pm1.22</td><td>36.30\\pm1.90</td></tr><tr><td>ADDA Tzeng2017ADDA </td><td>92.85\\pm0.17</td><td>10.61\\pm0.77</td><td>90.72\\pm0.15</td><td>29.73\\pm0.36</td><td>17.67\\pm0.42</td><td>89.83\\pm0.33</td><td>46.79\\pm0.43</td><td>20.57\\pm0.63</td></tr><tr><td>SGADA sgada2021 </td><td>93.82\\pm0.10</td><td>7.13\\pm0.22</td><td>91.70\\pm0.13</td><td>28.42\\pm0.34</td><td>16.10\\pm0.43</td><td>90.71\\pm0.10</td><td>45.13\\pm0.32</td><td>17.26\\pm0.36</td></tr><tr><td>SGPCS (ours)</td><td>93.94\\pm0.04</td><td>7.16\\pm0.16</td><td>91.55\\pm0.13</td><td>28.52\\pm0.21</td><td>16.16\\pm0.26</td><td>91.57\\pm0.22</td><td>45.49\\pm0.63</td><td>17.39\\pm0.88</td></tr><tr><td>UFLD-TO</td><td>97.35</td><td>0.50</td><td>94.97</td><td>18.05</td><td>3.84</td><td>96.57</td><td>34.06</td><td>2.49</td></tr><tr><td>ResNet-34</td><td>LA</td><td>FP &amp; FN</td><td>LA</td><td>FP</td><td>FN</td><td>LA</td><td>FP</td><td>FN</td></tr><tr><td>UFLD-SO</td><td>90.35</td><td>22.25</td><td>89.42</td><td>32.35</td><td>21.19</td><td>89.17</td><td>48.86</td><td>23.67</td></tr><tr><td>DANN Ganin2016 </td><td>90.91\\pm0.42</td><td>19.73\\pm1.51</td><td>91.06\\pm0.14</td><td>30.17\\pm0.20</td><td>18.54\\pm0.25</td><td>88.76\\pm0.22</td><td>48.93\\pm0.47</td><td>24.16\\pm0.89</td></tr><tr><td>ADDA Tzeng2017ADDA </td><td>92.39\\pm0.26</td><td>12.17\\pm0.84</td><td>91.39\\pm0.16</td><td>28.76\\pm0.30</td><td>16.63\\pm0.36</td><td>90.22\\pm0.39</td><td>45.84\\pm0.54</td><td>19.49\\pm0.90</td></tr><tr><td>SGADA sgada2021 </td><td>93.31\\pm0.10</td><td>9.41\\pm0.16</td><td>92.04\\pm0.09</td><td>28.18\\pm0.20</td><td>15.99\\pm0.24</td><td>91.63\\pm0.03</td><td>44.18\\pm0.12</td><td>16.23\\pm0.16</td></tr><tr><td>SGPCS (ours)</td><td>93.53\\pm0.25</td><td>8.24\\pm0.91</td><td>93.29\\pm0.18</td><td>25.68\\pm0.48</td><td>12.73\\pm0.59</td><td>91.55\\pm0.17</td><td>44.75\\pm0.28</td><td>16.41\\pm0.44</td></tr><tr><td>UFLD-TO</td><td>97.21</td><td>0.30</td><td>94.43</td><td>20.74</td><td>7.20</td><td>96.54</td><td>33.76</td><td>2.03</td></tr></tbody></table>", "caption": "Table 3: Performance on the test set. Lane accuracy (LA), false positives (FP), and false negatives (FN) are reported in %.", "list_citation_info": ["(13) E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, \u201cAdversarial Discriminative Domain Adaptation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.", "(12) Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky, \u201cDomain-Adversarial Training of Neural Networks,\u201d Journal of Machine Learning Research, vol. 17, no. 59, pp. 1\u201335, 2016.", "(21) I. B. Akkaya, F. Altinel, and U. Halici, \u201cSelf-Training Guided Adversarial Domain Adaptation for Thermal Imagery,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 4322\u20134331, June 2021."]}, {"table": "<table><tbody><tr><th>Dataset</th><td><p>Ego Vehicle</p></td><td><p>Camera Position</p></td><td><p>Lane Deviation</p></td><td><p>Traffic</p></td><td><p>Pedestrians</p></td><td><p>World Objects</p></td><td><p>Daytime</p></td><td><p>Weather</p></td><td><p>City</p></td><td><p>Rural</p></td><td><p>Highway</p></td><td><p>Terrain</p></td><td><p>Lane Topology</p></td><td><p>Road Appearance</p></td></tr><tr><th>Garnett2019 </th><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>Garnett2020 </th><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>SimuLanes2022 </th><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr><tr><th>ours</th><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody></table>", "caption": "Table 5: Comparison of applied variations for the collection of the synthetic datasets.", "list_citation_info": ["(5) C. Hu, S. Hudson, M. Ethier, M. Al-Sharman, D. Rayside, and W. Melek, \u201cSim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving,\u201d 2022.", "(19) N. Garnett, R. Uziel, N. Efrat, and D. Levi, \u201cSynthetic-to-Real Domain Adaptation for Lane Detection,\u201d in ACCV, 2020.", "(4) N. Garnett, R. Cohen, T. Pe\u2019er, R. Lahav, and D. Levi, \u201c3D-LaneNet: End-to-End 3D Multiple Lane Detection,\u201d in ICCV, pp. 1013 \u2013 1021, 2019."]}], "citation_info_to_title": {"(4) N. Garnett, R. Cohen, T. Pe\u2019er, R. Lahav, and D. Levi, \u201c3D-LaneNet: End-to-End 3D Multiple Lane Detection,\u201d in ICCV, pp. 1013 \u2013 1021, 2019.": "3D-LaneNet: End-to-End 3D Multiple Lane Detection", "(21) I. B. Akkaya, F. Altinel, and U. Halici, \u201cSelf-Training Guided Adversarial Domain Adaptation for Thermal Imagery,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 4322\u20134331, June 2021.": "Self-Training Guided Adversarial Domain Adaptation for Thermal Imagery", "(12) Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. March, and V. Lempitsky, \u201cDomain-Adversarial Training of Neural Networks,\u201d Journal of Machine Learning Research, vol. 17, no. 59, pp. 1\u201335, 2016.": "Domain-Adversarial Training of Neural Networks", "(22) X. Yue, Z. Zheng, S. Zhang, Y. Gao, T. Darrell, K. Keutzer, and A. Sangiovanni-Vincentelli, \u201cPrototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2021.": "Prototypical Cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation", "(2) Z. Qin, H. Wang, and X. Li, \u201cUltra Fast Structure-aware Deep Lane Detection,\u201d in The European Conference on Computer Vision (ECCV), 2020.": "Ultra Fast Structure-aware Deep Lane Detection", "(19) N. Garnett, R. Uziel, N. Efrat, and D. Levi, \u201cSynthetic-to-Real Domain Adaptation for Lane Detection,\u201d in ACCV, 2020.": "Synthetic-to-Real Domain Adaptation for Lane Detection", "(13) E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, \u201cAdversarial Discriminative Domain Adaptation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.": "Adversarial Discriminative Domain Adaptation", "(5) C. Hu, S. Hudson, M. Ethier, M. Al-Sharman, D. Rayside, and W. Melek, \u201cSim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving,\u201d 2022.": "Sim-to-Real Domain Adaptation for Lane Detection and Classification in Autonomous Driving", "(20) TuSimple, \u201cTuSimple-benchmark.\u201d https://github.com/TuSimple/tusimple-benchmark/tree/master/doc/lane_detection. Accessed: 2021-11-16.": "TuSimple-benchmark"}, "source_title_to_arxiv_id": {"Self-Training Guided Adversarial Domain Adaptation for Thermal Imagery": "2106.07165"}}