{"title": "Integrating Language Guidance into Vision-based Deep Metric Learning", "abstract": "Deep Metric Learning (DML) proposes to learn metric spaces which encode\nsemantic similarities as embedding space distances. These spaces should be\ntransferable to classes beyond those seen during training. Commonly, DML\nmethods task networks to solve contrastive ranking tasks defined over binary\nclass assignments. However, such approaches ignore higher-level semantic\nrelations between the actual classes. This causes learned embedding spaces to\nencode incomplete semantic context and misrepresent the semantic relation\nbetween classes, impacting the generalizability of the learned metric space. To\ntackle this issue, we propose a language guidance objective for visual\nsimilarity learning. Leveraging language embeddings of expert- and\npseudo-classnames, we contextualize and realign visual representation spaces\ncorresponding to meaningful language semantics for better semantic consistency.\nExtensive experiments and ablations provide a strong motivation for our\nproposed approach and show language guidance offering significant,\nmodel-agnostic improvements for DML, achieving competitive and state-of-the-art\nresults on all benchmarks. Code available at\nhttps://github.com/ExplainableML/LanguageGuidance_for_DML.", "authors": ["Karsten Roth", "Oriol Vinyals", "Zeynep Akata"], "published_date": "2022_03_16", "pdf_url": "http://arxiv.org/pdf/2203.08543v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Benchmarks \\rightarrow</td><td colspan=\"3\">CUB200 [108]</td><td colspan=\"3\">CARS196 [56]</td><td colspan=\"3\">SOP [73]</td></tr><tr><td>Methods \\downarrow</td><td>R@1</td><td>R@2</td><td>NMI</td><td>R@1</td><td>R@2</td><td>NMI</td><td>R@1</td><td>R@10</td><td>NMI</td></tr><tr><td colspan=\"10\">ResNet50, 128 dim.</td></tr><tr><td>Margin [115]</td><td>63.6</td><td>74.4</td><td>69.0</td><td>79.6</td><td>86.5</td><td>69.1</td><td>72.7</td><td>86.2</td><td>90.7</td></tr><tr><td>Div&amp;Conq [92]</td><td>65.9</td><td>76.6</td><td>69.6</td><td>84.6</td><td>90.7</td><td>70.3</td><td>75.9</td><td>88.4</td><td>90.2</td></tr><tr><td>MIC [88]</td><td>66.1</td><td>76.8</td><td>69.7</td><td>82.6</td><td>89.1</td><td>68.4</td><td>77.2</td><td>89.4</td><td>90.0</td></tr><tr><td>PADS [89]</td><td>67.3</td><td>78.0</td><td>69.9</td><td>83.5</td><td>89.7</td><td>68.8</td><td>76.5</td><td>89.0</td><td>89.9</td></tr><tr><td>S2SD [90]</td><td>68.9 \\pm 0.3</td><td>79.0 \\pm 0.3</td><td>72.1 \\pm 0.4</td><td>87.6 \\pm 0.2</td><td>92.7 \\pm 0.2</td><td>72.3 \\pm 0.2</td><td>80.2 \\pm 0.2</td><td>91.5 \\pm 0.1</td><td>90.9 \\pm 0.1</td></tr><tr><td>Multisimilarity+PLG</td><td>67.8 \\pm 0.2</td><td>78.2 \\pm 0.2</td><td>70.1 \\pm 0.1</td><td>86.0 \\pm 0.3</td><td>91.4 \\pm 0.1</td><td>72.4 \\pm 0.2</td><td>77.9 \\pm 0.1</td><td>89.9 \\pm 0.2</td><td>90.2 \\pm 0.2</td></tr><tr><td>S2SD+PLG</td><td>71.1 \\pm 0.1</td><td>80.6 \\pm 0.2</td><td>73.0 \\pm 0.2</td><td>89.1 \\pm 0.2</td><td>93.8 \\pm 0.2</td><td>73.1 \\pm 0.3</td><td>80.6 \\pm 0.1</td><td>91.8 \\pm 0.2</td><td>90.9 \\pm 0.1</td></tr><tr><td colspan=\"10\">ResNet50, 512 dim.</td></tr><tr><td>EPSHN [118]</td><td>64.9</td><td>75.3</td><td>-</td><td>82.7</td><td>89.3</td><td>-</td><td>78.3</td><td>90.7</td><td>-</td></tr><tr><td>NormSoft [121]</td><td>61.3</td><td>73.9</td><td>-</td><td>84.2</td><td>90.4</td><td>-</td><td>78.2</td><td>90.6</td><td>-</td></tr><tr><td>DiVA [66]</td><td>69.2</td><td>79.3</td><td>71.4</td><td>87.6</td><td>92.9</td><td>72.2</td><td>79.6</td><td>91.2</td><td>90.6</td></tr><tr><td>DCML-MDW [124]</td><td>68.4</td><td>77.9</td><td>71.8</td><td>85.2</td><td>91.8</td><td>73.9</td><td>79.8</td><td>90.8</td><td>90.8</td></tr><tr><td>IB-DML [95]{}^{a,b}</td><td>70.3</td><td>80.3</td><td>74.0</td><td>88.1</td><td>93.3</td><td>74.8</td><td>81.4</td><td>91.3</td><td>92.6</td></tr><tr><td>Multisimilarity+PLG</td><td>69.6 \\pm 0.4</td><td>79.5 \\pm 0.2</td><td>70.7 \\pm 0.1</td><td>87.1 \\pm 0.2</td><td>92.3 \\pm 0.3</td><td>73.0 \\pm 0.2</td><td>79.0 \\pm 0.1</td><td>91.0 \\pm 0.1</td><td>90.0 \\pm 0.1</td></tr><tr><td>S2SD+PLG</td><td>71.4 \\pm 0.3</td><td>81.1 \\pm 0.2</td><td>73.5 \\pm 0.3</td><td>90.2 \\pm 0.3</td><td>94.4 \\pm 0.2</td><td>72.4 \\pm 0.3</td><td>81.3 \\pm 0.2</td><td>92.3 \\pm 0.2</td><td>91.1 \\pm 0.2</td></tr><tr><td colspan=\"10\">Inception-BN, 512 dim.</td></tr><tr><td>Group [29]</td><td>65.5</td><td>77.0</td><td>69.0</td><td>85.6</td><td>91.2</td><td>72.7</td><td>75.1</td><td>87.5</td><td>90.8</td></tr><tr><td>Multisimilarity{}^{c} [110]</td><td>65.7</td><td>77.0</td><td>-</td><td>84.1</td><td>90.4</td><td>-</td><td>78.2</td><td>90.5</td><td>-</td></tr><tr><td>DR-MS [27]</td><td>66.1</td><td>77.0</td><td>-</td><td>85.0</td><td>90.5</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ProxyGML [125]</td><td>66.6</td><td>77.6</td><td>69.8</td><td>85.5</td><td>91.8</td><td>72.4</td><td>78.0</td><td>90.6</td><td>90.2</td></tr><tr><td>ProxyAnchor [50]{}^{d}</td><td>68.4</td><td>79.2</td><td>-</td><td>86.8</td><td>91.6</td><td>-</td><td>79.1</td><td>90.8</td><td>-</td></tr><tr><td>Multisimilarity+PLG</td><td>69.2 \\pm 0.2</td><td>79.7 \\pm 0.1</td><td>70.6 \\pm 0.3</td><td>86.2 \\pm 0.2</td><td>91.5 \\pm 0.3</td><td>70.8 \\pm 0.3</td><td>78.6 \\pm 0.2</td><td>90.7 \\pm 0.1</td><td>90.0 \\pm 0.2</td></tr><tr><td>S2SD+PLG</td><td>70.4 \\pm 0.2</td><td>80.5 \\pm 0.2</td><td>71.9 \\pm 0.3</td><td>88.1 \\pm 0.3</td><td>92.9 \\pm 0.1</td><td>71.4 \\pm 0.3</td><td>79.4 \\pm 0.1</td><td>91.2 \\pm 0.1</td><td>90.4 \\pm 0.2</td></tr></tbody></table>", "caption": "Table 1: State-of-the-art. Bold: best results per literature setup. The results showcase competitive and state-of-the-art performance with little hyperparameter tuning.While a separation into backbones and embedding dimensions provides a fairer comparison, we note some pipeline changes that improve performance independently and should be taken into account when comparing: {}^{a}: Better optimizer (RAdam [61] instead of Adam), {}^{b}: Larger input images, {}^{c}: Larger batchsizes on SOP, {}^{d}: Combination of pooling operations in backbone.", "list_citation_info": ["[29] Ismail Elezi, Sebastiano Vascon, Alessandro Torcinovich, Marcello Pelillo, and Laura Leal-Taix\u00e9. The group loss for deep metric learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision \u2013 ECCV 2020, pages 277\u2013294, Cham, 2020. Springer International Publishing.", "[66] Timo Milbich, Karsten Roth, Homanga Bharadhwaj, Samarth Sinha, Yoshua Bengio, Bj\u00f6rn Ommer, and Joseph Paul Cohen. Diva: Diverse visual feature aggregation for deep metric learning. CoRR, abs/2004.13458, 2020.", "[73] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4004\u20134012, 2016.", "[95] Jenny Denise Seidenschwarz, Ismail Elezi, and Laura Leal-Taix\u00e9. Learning intra-batch connections for deep metric learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9410\u20139421. PMLR, 18\u201324 Jul 2021.", "[27] Ujjal Kr Dutta, Mehrtash Harandi, and Chellu Chandra Sekhar. Unsupervised deep metric learning via orthogonality based probabilistic loss. IEEE Transactions on Artificial Intelligence, 1(1):74\u201384, 2020.", "[88] Karsten Roth, Biagio Brattoli, and Bjorn Ommer. Mic: Mining interclass characteristics for improved metric learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 8000\u20138009, 2019.", "[61] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations, 2020.", "[56] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 554\u2013561, 2013.", "[121] Andrew Zhai and Hao-Yu Wu. Making classification competitive for deep metric learning. CoRR, abs/1811.12649, 2018.", "[50] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.", "[108] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.", "[110] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss with general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[125] Yuehua Zhu, Muli Yang, Cheng Deng, and Wei Liu. Fewer is more: A deep graph metric learning perspective using fewer proxies. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17792\u201317803. Curran Associates, Inc., 2020.", "[118] Hong Xuan, Abby Stylianou, and Robert Pless. Improved embeddings with easy positive triplet mining. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), March 2020.", "[90] Karsten Roth, Timo Milbich, Bjorn Ommer, Joseph Paul Cohen, and Marzyeh Ghassemi. Simultaneous similarity-based self-distillation for deep metric learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9095\u20139106. PMLR, 18\u201324 Jul 2021.", "[115] Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling matters in deep embedding learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 2840\u20132848, 2017.", "[92] Artsiom Sanakoyeu, Vadim Tschernezki, Uta Buchler, and Bjorn Ommer. Divide and conquer the embedding space for metric learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[124] Wenzhao Zheng, Chengkun Wang, Jiwen Lu, and Jie Zhou. Deep compositional metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9320\u20139329, June 2021.", "[89] Karsten Roth, Timo Milbich, and Bjorn Ommer. Pads: Policy-adapted sampling for visual similarity learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020."]}, {"table": "<table><tbody><tr><th>Benchmarks\\rightarrow</th><td>CUB200</td><td>CARS196</td><td>SOP({}^{*})</td></tr><tr><th>Approaches \\downarrow</th><td>R@1</td><td>R@1</td><td>R@1</td></tr><tr><th>Multisimilarity</th><td>62.8 \\pm 0.2</td><td>81.6 \\pm 0.3</td><td>76.0 \\pm 0.1</td></tr><tr><th>+ELG</th><td>67.3 \\pm 0.2</td><td>85.3 \\pm 0.1</td><td>76.0 \\pm 0.2</td></tr><tr><th>+PLG Top-5</th><td>67.1 \\pm 0.4</td><td>85.4 \\pm 0.2</td><td>76.4 \\pm 0.1</td></tr><tr><th>Margin, \\beta=1.2</th><td>62.7 \\pm 0.6</td><td>79.4 \\pm 0.5</td><td>78.0 \\pm 0.3</td></tr><tr><th>+ELG</th><td>65.3 \\pm 0.5</td><td>83.2 \\pm 0.5</td><td>77.8 \\pm 0.1</td></tr><tr><th>+PLG Top-5</th><td>65.2 \\pm 0.5</td><td>83.4 \\pm 0.4</td><td>78.3 \\pm 0.2</td></tr><tr><th>Multisimilarity + S2SD</th><td>67.7 \\pm 0.3</td><td>86.5 \\pm 0.1</td><td>77.7 \\pm 0.2</td></tr><tr><th>+ELG</th><td>68.9 \\pm 0.4</td><td>88.2 \\pm 0.2</td><td>77.8 \\pm 0.1</td></tr><tr><th>+PLG Top-5</th><td>69.0 \\pm 0.4</td><td>88.4 \\pm 0.3</td><td>78.0 \\pm 0.1</td></tr></tbody></table>", "caption": "Table 2: Relative comparison. We follow protocols proposed in [91]<sup>5</sup><sup>5</sup>footnotemark: 5, with no learning rate scheduling, to ensure exact comparability. The results show significant improvements when language-guidance is applied. ({}^{*}) For SOP, only 12 superlabels are given for 11,318 training classes, with very few samples per class. This limits the benefits of language guidance.", "list_citation_info": ["[91] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8242\u20138252. PMLR, 13\u201318 Jul 2020."]}, {"table": "<table><tbody><tr><td>Benchmarks\\rightarrow</td><td colspan=\"2\">CUB200-2011</td><td colspan=\"2\">CARS196</td></tr><tr><td rowspan=\"2\">Models \\downarrow</td><td rowspan=\"2\">R@1</td><td>mAP</td><td rowspan=\"2\">R@1</td><td>mAP</td></tr><tr><td>@1000</td><td>@1000</td></tr><tr><td>Baseline</td><td>62.8 \\pm 0.2</td><td>31.1 \\pm 0.3</td><td>81.6 \\pm 0.3</td><td>31.7 \\pm 0.1</td></tr><tr><td>+ CLIP-L [83]</td><td>67.3 \\pm 0.2</td><td>34.8 \\pm 0.2</td><td>85.3 \\pm 0.1</td><td>32.7 \\pm 0.2</td></tr><tr><td colspan=\"5\">(a) Language Models</td></tr><tr><td>+ BERT [23]</td><td>66.9 \\pm 0.3</td><td>33.5 \\pm 0.2</td><td>84.9 \\pm 0.1</td><td>32.3 \\pm 0.1</td></tr><tr><td>+ Roberta-L [64]</td><td>67.3 \\pm 0.2</td><td>33.9 \\pm 0.3</td><td>85.1 \\pm 0.2</td><td>32.4 \\pm 0.2</td></tr><tr><td>+ Reformer [54]</td><td>66.7 \\pm 0.1</td><td>33.1 \\pm 0.1</td><td>85.5 \\pm 0.2</td><td>32.0 \\pm 0.2</td></tr><tr><td>+ GPT2 [84]</td><td>67.0 \\pm 0.3</td><td>33.7 \\pm 0.1</td><td>84.8 \\pm 0.4</td><td>32.4 \\pm 0.1</td></tr><tr><td>+ Top 3</td><td>67.5 \\pm 0.2</td><td>34.5 \\pm 0.3</td><td>85.6 \\pm 0.3</td><td>32.5 \\pm 0.3</td></tr><tr><td colspan=\"5\">(b) Word Embeddings</td></tr><tr><td>+ FastText [5]</td><td>66.9 \\pm 0.3</td><td>33.7 \\pm 0.3</td><td>85.2 \\pm 0.1</td><td>32.7 \\pm 0.1</td></tr><tr><td>+ GloVe [79]</td><td>66.1 \\pm 0.2</td><td>33.1 \\pm 0.3</td><td>85.0 \\pm 0.2</td><td>32.1 \\pm 0.3</td></tr><tr><td colspan=\"5\">(c) Word Hierarchies</td></tr><tr><td>+ WordNet [31]</td><td>63.7 \\pm 0.2</td><td>31.3 \\pm 0.2</td><td>82.5 \\pm 0.3</td><td>31.0 \\pm 0.2</td></tr><tr><td>+ HierMatch [2]</td><td>64.8 \\pm 0.3</td><td>32.2 \\pm 0.1</td><td>82.8 \\pm 0.2</td><td>31.8 \\pm 0.1</td></tr></tbody></table>", "caption": "Table 3: Model guidance quality. Performance improves regardless of the exact language model and even with large-scale pretrained word embeddings s.a. FastText [5] and GloVe [79]. However, less transferable word hierarchies fall short in comparison.", "list_citation_info": ["[84] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019.", "[5] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.", "[2] Bj\u00f6rn Barz and Joachim Denzler. Hierarchy-based image embeddings for semantic image retrieval. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 638\u2013647, 2019.", "[64] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.", "[79] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.", "[31] Christiane Fellbaum. WordNet: An Electronic Lexical Database. Bradford Books, 1998.", "[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.", "[54] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.", "[83] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 18\u201324 Jul 2021."]}, {"table": "<table><tbody><tr><td>Benchmarks\\rightarrow</td><td colspan=\"3\">CUB200-2011</td><td colspan=\"3\">CARS196</td><td colspan=\"3\">SOP({}^{*})</td></tr><tr><td>Approaches \\downarrow</td><td>R@1</td><td>NMI</td><td>mAP@1000</td><td>R@1</td><td>NMI</td><td>mAP@1000</td><td>R@1</td><td>NMI</td><td>mAP@1000</td></tr><tr><td>Multisimilarity</td><td>62.8 \\pm 0.2</td><td>67.8 \\pm 0.4</td><td>31.1 \\pm 0.3</td><td>81.6 \\pm 0.3</td><td>69.6 \\pm 0.5</td><td>31.7 \\pm 0.1</td><td>76.0 \\pm 0.1</td><td>89.4 \\pm 0.1</td><td>43.3 \\pm 0.1</td></tr><tr><td>+ELG</td><td>67.3 \\pm 0.2</td><td>69.6 \\pm 0.6</td><td>34.8 \\pm 0.2</td><td>85.3 \\pm 0.1</td><td>71.7 \\pm 0.2</td><td>32.7 \\pm 0.2</td><td>76.0 \\pm 0.2</td><td>89.5 \\pm 0.1</td><td>43.5 \\pm 0.1</td></tr><tr><td>+PLG Top-5</td><td>67.1 \\pm 0.4</td><td>69.6 \\pm 0.6</td><td>34.6 \\pm 0.5</td><td>85.4 \\pm 0.2</td><td>71.3 \\pm 0.1</td><td>32.8 \\pm 0.2</td><td>76.4 \\pm 0.1</td><td>89.6 \\pm 0.1</td><td>43.7 \\pm 0.1</td></tr><tr><td>Margin, \\beta=1.2</td><td>62.7 \\pm 0.6</td><td>68.0 \\pm 0.3</td><td>32.2 \\pm 0.3</td><td>79.4 \\pm 0.5</td><td>66.6 \\pm 0.7</td><td>32.8 \\pm 0.2</td><td>78.0 \\pm 0.3</td><td>90.3 \\pm 0.2</td><td>46.3 \\pm 0.2</td></tr><tr><td>+ELG</td><td>65.3 \\pm 0.5</td><td>68.5 \\pm 0.4</td><td>33.5 \\pm 0.3</td><td>83.2 \\pm 0.5</td><td>69.0 \\pm 0.6</td><td>33.4 \\pm 0.3</td><td>77.8 \\pm 0.1</td><td>90.2 \\pm 0.1</td><td>46.1 \\pm 0.1</td></tr><tr><td>+PLG Top-5</td><td>65.2 \\pm 0.5</td><td>68.5 \\pm 0.4</td><td>33.5 \\pm 0.3</td><td>83.4 \\pm 0.4</td><td>69.1 \\pm 0.4</td><td>33.7 \\pm 0.3</td><td>78.3 \\pm 0.2</td><td>90.3 \\pm 0.2</td><td>46.5 \\pm 0.1</td></tr><tr><td>Multisimilarity + S2SD</td><td>67.7 \\pm 0.3</td><td>71.5 \\pm 0.2</td><td>35.5 \\pm 0.2</td><td>86.5 \\pm 0.1</td><td>71.4 \\pm 0.4</td><td>35.1 \\pm 0.3</td><td>77.7 \\pm 0.2</td><td>89.9 \\pm 0.1</td><td>45.3 \\pm 0.3</td></tr><tr><td>+ELG</td><td>68.9 \\pm 0.4</td><td>72.5 \\pm 0.3</td><td>36.4 \\pm 0.5</td><td>88.2 \\pm 0.2</td><td>72.0 \\pm 0.1</td><td>36.0 \\pm 0.1</td><td>77.8 \\pm 0.1</td><td>90.0 \\pm 0.2</td><td>45.3 \\pm 0.2</td></tr><tr><td>+PLG Top-5</td><td>69.0 \\pm 0.4</td><td>72.4 \\pm 0.2</td><td>36.6 \\pm 0.3</td><td>88.4 \\pm 0.3</td><td>72.4 \\pm 0.2</td><td>36.2 \\pm 0.2</td><td>78.0 \\pm 0.1</td><td>90.0 \\pm 0.1</td><td>45.6 \\pm 0.1</td></tr></tbody></table>", "caption": "Table S1: Relative comparison. We follow protocols proposed in [91]<sup>8</sup><sup>8</sup>footnotemark: 8, with no learning rate scheduling, to ensure exact comparability. The results show significant improvements on CUB200 and CARS196 when language-guidance is applied (with expert- and pseudolabels). ({}^{*}) For SOP, only 12 superlabels are given for 11,318 training classes. Similarly, SOP only contains very few samples per class, making pseudolabel class estimates very noisy. This makes the benefits of language guidance limited.", "list_citation_info": ["[91] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8242\u20138252. PMLR, 13\u201318 Jul 2020."]}, {"table": "<table><tbody><tr><td>Benchmarks\\rightarrow</td><td colspan=\"2\">CUB200-2011</td><td colspan=\"2\">CARS196</td></tr><tr><td rowspan=\"2\">Models \\downarrow</td><td rowspan=\"2\">R@1</td><td>mAP</td><td rowspan=\"2\">R@1</td><td>mAP</td></tr><tr><td>@1000</td><td>@1000</td></tr><tr><td>Baseline</td><td>62.8 \\pm 0.2</td><td>31.1 \\pm 0.3</td><td>81.6 \\pm 0.3</td><td>31.7 \\pm 0.1</td></tr><tr><td>+ CLIP-L [83]</td><td>67.3 \\pm 0.2</td><td>34.8 \\pm 0.2</td><td>85.3 \\pm 0.1</td><td>32.7 \\pm 0.2</td></tr><tr><td colspan=\"5\">(a) Language Models</td></tr><tr><td>+ BERT [23]</td><td>66.9 \\pm 0.3</td><td>33.5 \\pm 0.2</td><td>84.9 \\pm 0.1</td><td>32.3 \\pm 0.1</td></tr><tr><td>+ DistBert [93]</td><td>66.7 \\pm 0.1</td><td>33.4 \\pm 0.2</td><td>85.4 \\pm 0.4</td><td>32.4 \\pm 0.1</td></tr><tr><td>+ Roberta-B [64]</td><td>67.0 \\pm 0.2</td><td>33.8 \\pm 0.2</td><td>84.9 \\pm 0.1</td><td>32.3 \\pm 0.3</td></tr><tr><td>+ Roberta-L [64]</td><td>67.3 \\pm 0.2</td><td>33.9 \\pm 0.3</td><td>85.1 \\pm 0.2</td><td>32.4 \\pm 0.2</td></tr><tr><td>+ DistRoberta [113]</td><td>66.0 \\pm 0.2</td><td>32.2 \\pm 0.2</td><td>85.0 \\pm 0.3</td><td>32.1 \\pm 0.2</td></tr><tr><td>+ Reformer [54]</td><td>66.7 \\pm 0.1</td><td>33.1 \\pm 0.1</td><td>85.5 \\pm 0.2</td><td>32.0 \\pm 0.2</td></tr><tr><td>+ MPNet [98]</td><td>66.2 \\pm 0.3</td><td>32.3 \\pm 0.2</td><td>85.4 \\pm 0.2</td><td>32.3 \\pm 0.3</td></tr><tr><td>+ GPT2 [84]</td><td>67.0 \\pm 0.3</td><td>33.7 \\pm 0.1</td><td>84.8 \\pm 0.4</td><td>32.4 \\pm 0.1</td></tr><tr><td>+ Top 3</td><td>67.5 \\pm 0.2</td><td>34.5 \\pm 0.3</td><td>85.6 \\pm 0.3</td><td>32.5 \\pm 0.3</td></tr></tbody></table>", "caption": "Table S2: Models vs. guidance quality. Performance improves regardless of the exact large pretrained language model. Strong improvements can even be achieved through large-scale pretrained word embeddings such as FastText [5] and GloVe [79]. However, using less transferable word hierarchies falls short in comparison.", "list_citation_info": ["[98] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 16857\u201316867. Curran Associates, Inc., 2020.", "[84] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019.", "[5] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.", "[93] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020.", "[64] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.", "[79] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.", "[113] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\u2019s transformers: State-of-the-art natural language processing, 2020.", "[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.", "[54] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.", "[83] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 18\u201324 Jul 2021."]}], "citation_info_to_title": {"[118] Hong Xuan, Abby Stylianou, and Robert Pless. Improved embeddings with easy positive triplet mining. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), March 2020.": "Improved embeddings with easy positive triplet mining", "[29] Ismail Elezi, Sebastiano Vascon, Alessandro Torcinovich, Marcello Pelillo, and Laura Leal-Taix\u00e9. The group loss for deep metric learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision \u2013 ECCV 2020, pages 277\u2013294, Cham, 2020. Springer International Publishing.": "The Group Loss for Deep Metric Learning", "[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.": "BERT: Pre-training of deep bidirectional transformers for language understanding", "[54] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.": "Reformer: The efficient transformer", "[93] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2020.": "Distilbert: A Distilled Version of BERT", "[91] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 8242\u20138252. PMLR, 13\u201318 Jul 2020.": "Revisiting training strategies and generalization performance in deep metric learning", "[84] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019.": "Language Models are Unsupervised Multitask Learners", "[61] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In International Conference on Learning Representations, 2020.": "On the Variance of the Adaptive Learning Rate and Beyond", "[110] Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss with general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "Multi-similarity loss with general pair weighting for deep metric learning", "[108] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.": "The Caltech-UCSD Birds-200-2011 Dataset", "[89] Karsten Roth, Timo Milbich, and Bjorn Ommer. Pads: Policy-adapted sampling for visual similarity learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.": "Pads: Policy-adapted sampling for visual similarity learning", "[31] Christiane Fellbaum. WordNet: An Electronic Lexical Database. Bradford Books, 1998.": "WordNet: An Electronic Lexical Database", "[92] Artsiom Sanakoyeu, Vadim Tschernezki, Uta Buchler, and Bjorn Ommer. Divide and conquer the embedding space for metric learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Divide and Conquer the Embedding Space for Metric Learning", "[113] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\u2019s transformers: State-of-the-art natural language processing, 2020.": "Huggingface\u2019s transformers: State-of-the-art natural language processing", "[121] Andrew Zhai and Hao-Yu Wu. Making classification competitive for deep metric learning. CoRR, abs/1811.12649, 2018.": "Making classification competitive for deep metric learning", "[88] Karsten Roth, Biagio Brattoli, and Bjorn Ommer. Mic: Mining interclass characteristics for improved metric learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 8000\u20138009, 2019.": "Mic: Mining interclass characteristics for improved metric learning", "[79] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.": "Glove: Global vectors for word representation", "[124] Wenzhao Zheng, Chengkun Wang, Jiwen Lu, and Jie Zhou. Deep compositional metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9320\u20139329, June 2021.": "Deep Compositional Metric Learning", "[90] Karsten Roth, Timo Milbich, Bjorn Ommer, Joseph Paul Cohen, and Marzyeh Ghassemi. Simultaneous similarity-based self-distillation for deep metric learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9095\u20139106. PMLR, 18\u201324 Jul 2021.": "Simultaneous similarity-based self-distillation for deep metric learning", "[73] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4004\u20134012, 2016.": "Deep Metric Learning via Lifted Structured Feature Embedding", "[115] Chao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling matters in deep embedding learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 2840\u20132848, 2017.": "Sampling Matters in Deep Embedding Learning", "[5] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.": "Enriching word vectors with subword information", "[95] Jenny Denise Seidenschwarz, Ismail Elezi, and Laura Leal-Taix\u00e9. Learning intra-batch connections for deep metric learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 9410\u20139421. PMLR, 18\u201324 Jul 2021.": "Learning intra-batch connections for deep metric learning", "[2] Bj\u00f6rn Barz and Joachim Denzler. Hierarchy-based image embeddings for semantic image retrieval. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 638\u2013647, 2019.": "Hierarchy-based image embeddings for semantic image retrieval", "[83] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763. PMLR, 18\u201324 Jul 2021.": "Learning transferable visual models from natural language supervision", "[56] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 554\u2013561, 2013.": "3D Object Representations for Fine-Grained Categorization", "[64] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.": "Roberta: A Robustly Optimized BERT Pretraining Approach", "[66] Timo Milbich, Karsten Roth, Homanga Bharadhwaj, Samarth Sinha, Yoshua Bengio, Bj\u00f6rn Ommer, and Joseph Paul Cohen. Diva: Diverse visual feature aggregation for deep metric learning. CoRR, abs/2004.13458, 2020.": "Diva: Diverse visual feature aggregation for deep metric learning", "[27] Ujjal Kr Dutta, Mehrtash Harandi, and Chellu Chandra Sekhar. Unsupervised deep metric learning via orthogonality based probabilistic loss. IEEE Transactions on Artificial Intelligence, 1(1):74\u201384, 2020.": "Unsupervised deep metric learning via orthogonality based probabilistic loss", "[50] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.": "Proxy Anchor Loss for Deep Metric Learning", "[98] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 16857\u201316867. Curran Associates, Inc., 2020.": "Mpnet: Masked and permuted pre-training for language understanding", "[125] Yuehua Zhu, Muli Yang, Cheng Deng, and Wei Liu. Fewer is more: A deep graph metric learning perspective using fewer proxies. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 17792\u201317803. Curran Associates, Inc., 2020.": "Fewer is more: A deep graph metric learning perspective using fewer proxies"}, "source_title_to_arxiv_id": {"Learning intra-batch connections for deep metric learning": "2102.07753"}}