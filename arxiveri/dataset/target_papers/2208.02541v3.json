{"title": "MVSFormer: Multi-View Stereo by Learning Robust Image Features and Temperature-based Depth", "abstract": "Feature representation learning is the key recipe for learning-based\nMulti-View Stereo (MVS). As the common feature extractor of learning-based MVS,\nvanilla Feature Pyramid Networks (FPNs) suffer from discouraged feature\nrepresentations for reflection and texture-less areas, which limits the\ngeneralization of MVS. Even FPNs worked with pre-trained Convolutional Neural\nNetworks (CNNs) fail to tackle these issues. On the other hand, Vision\nTransformers (ViTs) have achieved prominent success in many 2D vision tasks.\nThus we ask whether ViTs can facilitate feature learning in MVS? In this paper,\nwe propose a pre-trained ViT enhanced MVS network called MVSFormer, which can\nlearn more reliable feature representations benefited by informative priors\nfrom ViT. The finetuned MVSFormer with hierarchical ViTs of efficient attention\nmechanisms can achieve prominent improvement based on FPNs. Besides, the\nalternative MVSFormer with frozen ViT weights is further proposed. This largely\nalleviates the training cost with competitive performance strengthened by the\nattention map from the self-distillation pre-training. MVSFormer can be\ngeneralized to various input resolutions with efficient multi-scale training\nstrengthened by gradient accumulation. Moreover, we discuss the merits and\ndrawbacks of classification and regression-based MVS methods, and further\npropose to unify them with a temperature-based strategy. MVSFormer achieves\nstate-of-the-art performance on the DTU dataset. Particularly, MVSFormer ranks\nas Top-1 on both intermediate and advanced sets of the highly competitive\nTanks-and-Temples leaderboard.", "authors": ["Chenjie Cao", "Xinlin Ren", "Yanwei Fu"], "published_date": "2022_08_04", "pdf_url": "http://arxiv.org/pdf/2208.02541v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Methods</td><td>Acc.\\downarrow</td><td>Cop.\\downarrow</td><td>Ovl.\\downarrow</td></tr><tr><td>Gipuma[20]</td><td>0.283</td><td>0.873</td><td>0.578</td></tr><tr><td>COLMAP[43]</td><td>0.400</td><td>0.664</td><td>0.532</td></tr><tr><td>R-MVSNet[61]</td><td>0.385</td><td>0.459</td><td>0.422</td></tr><tr><td>AA-RMVSNet[52]</td><td>0.376</td><td>0.339</td><td>0.357</td></tr><tr><td>CasMVSNet[22]</td><td>0.325</td><td>0.385</td><td>0.355</td></tr><tr><td>CDS-MVSNet[21]</td><td>0.352</td><td>0.280</td><td>0.316</td></tr><tr><td>UniMVSNet[41]</td><td>0.352</td><td>0.278</td><td>0.315</td></tr><tr><td>TransMVSNet[15]</td><td>0.321</td><td>0.289</td><td>0.305</td></tr><tr><td>GBiNet*[40]</td><td>0.312</td><td>0.293</td><td>0.303</td></tr><tr><td>MVSFormer-P</td><td>0.327</td><td>0.265</td><td>0.296</td></tr><tr><td>MVSFormer-H</td><td>0.327</td><td>0.251</td><td>0.289</td></tr></tbody></table>", "caption": "Table 1: Quantitative point cloud results (mm) with Accuracy (Acc.), Completeness (Cop.), and Overall (Ovl.) on DTU [1] (lower is better). Best results are in bold, and second ones are underlined. * means that GBiNet [40] is re-tested with the same post-processing threshold to all scans for fair comparisons with other methods.", "list_citation_info": ["[43] Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision, pages 501\u2013518. Springer, 2016.", "[15] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnet: Global context-aware multi-view stereo network with transformers. arXiv preprint arXiv:2111.14600, 2021.", "[40] Zhenxing Mi, Di Chang, and Dan Xu. Generalized binary search network for highly-efficient multi-view stereo. arXiv preprint arXiv:2112.02338, 2021.", "[21] Khang Truong Giang, Soohwan Song, and Sungho Jo. Curvature-guided dynamic scale networks for multi-view stereo. arXiv preprint arXiv:2112.05999, 2021.", "[22] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.", "[52] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6187\u20136196, 2021.", "[20] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In The IEEE International Conference on Computer Vision (ICCV), June 2015.", "[61] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.", "[41] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi-view stereo: A unified representation and focal loss. arXiv preprint arXiv:2201.01501, 2022.", "[1] Henrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, pages 1\u201316, 2016."]}, {"table": "<table><thead><tr><th></th><th></th><th>FPN</th><th>ResNet34</th><th>ResNet50</th><th>MAE-base</th><th>DINO-small</th><th>Twins-small</th></tr></thead><tbody><tr><th rowspan=\"5\">Error(%)\\downarrow</th><th>2mm</th><td>19.53</td><td>20.65</td><td>20.55</td><td>21.85</td><td>26.66</td><td>19.65</td></tr><tr><th>4mm</th><td>11.85</td><td>12.43</td><td>12.32</td><td>12.42</td><td>14.42</td><td>11.53</td></tr><tr><th>8mm</th><td>8.25</td><td>8.55</td><td>8.45</td><td>8.17</td><td>8.87</td><td>7.92</td></tr><tr><th>14mm</th><td>6.48</td><td>6.75</td><td>6.60</td><td>6.33</td><td>6.60</td><td>6.24</td></tr><tr><th>mean</th><td>11.52</td><td>12.09</td><td>11.98</td><td>12.19</td><td>14.14</td><td>11.34</td></tr></tbody></table>", "caption": "Table 8: Depth metrics for DTU (256\\times 320) compared with FPN, pre-trained CNNs and VITs [25, 7, 11]. Note that FPN is trained from scratch without pre-training.", "list_citation_info": ["[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021."]}], "citation_info_to_title": {"[21] Khang Truong Giang, Soohwan Song, and Sungho Jo. Curvature-guided dynamic scale networks for multi-view stereo. arXiv preprint arXiv:2112.05999, 2021.": "Curvature-guided dynamic scale networks for multi-view stereo", "[15] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnet: Global context-aware multi-view stereo network with transformers. arXiv preprint arXiv:2111.14600, 2021.": "Transmvsnet: Global context-aware multi-view stereo network with transformers", "[61] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5525\u20135534, 2019.": "Recurrent MVSNet for High-Resolution Multi-View Stereo Depth Inference", "[52] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6187\u20136196, 2021.": "Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network", "[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.": "Masked Autoencoders are Scalable Vision Learners", "[43] Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision, pages 501\u2013518. Springer, 2016.": "Pixelwise view selection for unstructured multi-view stereo", "[22] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.": "Cascade cost volume for high-resolution multi-view stereo and stereo matching", "[1] Henrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, pages 1\u201316, 2016.": "Large-scale data for multiple-view stereopsis", "[20] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In The IEEE International Conference on Computer Vision (ICCV), June 2015.": "Massively Parallel Multiview Stereopsis by Surface Normal Diffusion", "[40] Zhenxing Mi, Di Chang, and Dan Xu. Generalized binary search network for highly-efficient multi-view stereo. arXiv preprint arXiv:2112.02338, 2021.": "Generalized Binary Search Network for Highly-Efficient Multi-View Stereo", "[41] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi-view stereo: A unified representation and focal loss. arXiv preprint arXiv:2201.01501, 2022.": "Rethinking depth estimation for multi-view stereo: A unified representation and focal loss"}, "source_title_to_arxiv_id": {"Curvature-guided dynamic scale networks for multi-view stereo": "2112.05999", "Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network": "2108.03824", "Masked Autoencoders are Scalable Vision Learners": "2111.06377"}}