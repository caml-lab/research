{"title": "DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation", "abstract": "This paper proposes a simple baseline framework for video-based 2D/3D human\npose estimation that can achieve 10 times efficiency improvement over existing\nworks without any performance degradation, named DeciWatch. Unlike current\nsolutions that estimate each frame in a video, DeciWatch introduces a simple\nyet effective sample-denoise-recover framework that only watches sparsely\nsampled frames, taking advantage of the continuity of human motions and the\nlightweight pose representation. Specifically, DeciWatch uniformly samples less\nthan 10% video frames for detailed estimation, denoises the estimated 2D/3D\nposes with an efficient Transformer architecture, and then accurately recovers\nthe rest of the frames using another Transformer-based network. Comprehensive\nexperimental results on three video-based human pose estimation and body mesh\nrecovery tasks with four datasets validate the efficiency and effectiveness of\nDeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.", "authors": ["Ailing Zeng", "Xuan Ju", "Lei Yang", "Ruiyuan Gao", "Xizhou Zhu", "Bo Dai", "Qiang Xu"], "published_date": "2022_03_16", "pdf_url": "http://arxiv.org/pdf/2203.08713v2", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"5\">Human3.6M</th><th colspan=\"5\">AIST++</th></tr><tr><th>Interval N</th><th>2</th><th>5</th><th>10</th><th>15</th><th>20</th><th>2</th><th>5</th><th>10</th><th>15</th><th>20</th></tr></thead><tbody><tr><th>Linear</th><td>2.21</td><td>6.55</td><td>10.81</td><td>24.15</td><td>35.20</td><td>7.21</td><td>21.31</td><td>27.72</td><td>73.69</td><td>99.04</td></tr><tr><th>Quadratic</th><td>1.26</td><td>4.31</td><td>10.05</td><td>17.22</td><td>22.85</td><td>2.04</td><td>8.33</td><td>23.59</td><td>43.13</td><td>61.16</td></tr><tr><th>Cubic-Spline</th><td>0.18</td><td>0.99</td><td>5.36</td><td>18.42</td><td>29.21</td><td>0.89</td><td>5.12</td><td>18.31</td><td>45.32</td><td>77.39</td></tr><tr><th>DeciWatch</th><td>0.25</td><td>1.33</td><td>2.89</td><td>6.21</td><td>10.59</td><td>0.83</td><td>4.03</td><td>11.25</td><td>20.12</td><td>41.25</td></tr></tbody></table>", "caption": "Table 1: Comparison of MPJPE on efficient pose labeling that labels one frame in every N frames on Human3.6M [22] and AIST++ [34] dataset.", "list_citation_info": ["[34] Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance generation with aist++. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 13401\u201313412 (October 2021)", "[22] Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence 36(7), 1325\u20131339 (2013)"]}, {"table": "<table><thead><tr><th colspan=\"5\">Sub-JHMDB dataset - 2D Pose Estimation</th></tr></thead><tbody><tr><th>Sampling Ratio</th><th>Evaluation Metric</th><td>PCK@0.2 \\uparrow</td><td>PCK@0.1 \\uparrow</td><td>PCK@0.05 \\uparrow</td></tr><tr><th></th><th>SimplePose</th><td>93.92%</td><td>81.25%</td><td>56.88%</td></tr><tr><th rowspan=\"-2\">20%</th><th>DeciWatch</th><td>99.11%</td><td>95.43%</td><td>82.66%</td></tr><tr><th></th><th>SimplePose</th><td>93.94%</td><td>81.61%</td><td>57.30%</td></tr><tr><th rowspan=\"-2\">10%</th><th>DeciWatch</th><td>98.75%</td><td>94.05%</td><td>79.44%</td></tr><tr><th></th><th>SimplePose</th><td>92.38%</td><td>82.79%</td><td>58.95%</td></tr><tr><th rowspan=\"-2\">5%</th><th>DeciWatch</th><td>97.50%</td><td>91.76%</td><td>73.02%</td></tr></tbody></table>", "caption": "Table 2: Comparison of DeciWatch and SimplePose[51] on PCK@0.2, PCK@0.1, and PCK@0.05. In future work, we recommend using PCK@0.05 as the main metrics for 2D pose estimation.", "list_citation_info": ["[51] Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and tracking. In: Proceedings of the European conference on computer vision (ECCV). pp. 466\u2013481 (2018)"]}, {"table": "<table><tbody><tr><th>Metrics/N</th><td>Ori.</td><td>1</td><td>3</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td></tr><tr><th colspan=\"20\">PARE [29] Backbone on 3DPW dataset</th></tr><tr><th>MPJPE</th><td>78.9</td><td>75.7</td><td>75.0</td><td>75.1</td><td>75.2</td><td>75.4</td><td>76.0</td><td>76.4</td><td>77.2</td><td>77.7</td><td>78.4</td><td>79.4</td><td>80.3</td><td>80.7</td><td>81.7</td><td>82.5</td><td>83.5</td><td>84.4</td><td>85.3</td></tr><tr><th>Accel</th><td>25.7</td><td>25.2</td><td>9.2</td><td>7.7</td><td>7.4</td><td>7.2</td><td>7.1</td><td>7.0</td><td>6.9</td><td>6.9</td><td>6.8</td><td>6.8</td><td>6.7</td><td>6.7</td><td>6.7</td><td>6.7</td><td>6.6</td><td>6.6</td><td>6.6</td></tr><tr><th colspan=\"20\">EFT [25] Backbone on 3DPW dataset</th></tr><tr><th>MPJPE</th><td>90.3</td><td>88.0</td><td>87.2</td><td>87.2</td><td>87.3</td><td>87.6</td><td>88.3</td><td>88.4</td><td>89.0</td><td>89.8</td><td>90.3</td><td>91.5</td><td>92.3</td><td>92.3</td><td>93.1</td><td>94.0</td><td>94.6</td><td>95.4</td><td>96.1</td></tr><tr><th>Accel</th><td>32.8</td><td>32.7</td><td>10.2</td><td>8.0</td><td>7.5</td><td>7.3</td><td>7.1</td><td>6.9</td><td>6.8</td><td>6.8</td><td>6.7</td><td>6.7</td><td>6.6</td><td>6.6</td><td>6.5</td><td>6.5</td><td>6.5</td><td>6.4</td><td>6.4</td></tr><tr><th colspan=\"20\">SPIN [30] Backbone on 3DPW dataset</th></tr><tr><th>MPJPE</th><td>96.9</td><td>93.8</td><td>92.9</td><td>92.2</td><td>92.7</td><td>92.6</td><td>93.2</td><td>93.4</td><td>93.3</td><td>94.2</td><td>95.0</td><td>95.3</td><td>96.6</td><td>96.7</td><td>97.4</td><td>97.6</td><td>98.8</td><td>99.2</td><td>100.2</td></tr><tr><th>Accel</th><td>34.6</td><td>33.5</td><td>10.5</td><td>8.2</td><td>7.7</td><td>7.5</td><td>7.3</td><td>7.2</td><td>7.1</td><td>7.0</td><td>6.9</td><td>6.9</td><td>6.9</td><td>6.9</td><td>6.8</td><td>6.9</td><td>6.8</td><td>6.8</td><td>6.8</td></tr><tr><th colspan=\"20\">FCN [39] Backbone on Human3.6M dataset</th></tr><tr><th>MPJPE</th><td>54.6</td><td>53.3</td><td>53.0</td><td>52.8</td><td>52.6</td><td>52.3</td><td>52.5</td><td>52.6</td><td>52.8</td><td>53.0</td><td>53.2</td><td>53.2</td><td>53.4</td><td>53.5</td><td>53.9</td><td>53.8</td><td>54.0</td><td>54.2</td><td>54.4</td></tr><tr><th>Accel</th><td>19.2</td><td>15.4</td><td>3.1</td><td>2.0</td><td>1.8</td><td>1.6</td><td>1.6</td><td>1.5</td><td>1.5</td><td>1.4</td><td>1.4</td><td>1.4</td><td>1.4</td><td>1.4</td><td>1.4</td><td>1.4</td><td>1.4</td><td>1.4</td><td>1.4</td></tr><tr><th colspan=\"20\">SPIN [30] Backbone on AIST++ dataset</th></tr><tr><th>MPJPE</th><td>107.7</td><td>67.2</td><td>66.6</td><td>67.6</td><td>68.4</td><td>69.7</td><td>71.2</td><td>71.6</td><td>71.3</td><td>76.1</td><td>77.1</td><td>79.0</td><td>80.2</td><td>82.3</td><td>84.3</td><td>85.2</td><td>87.0</td><td>88.9</td><td>90.8</td></tr><tr><th>Accel</th><td>33.8</td><td>7.6</td><td>7.6</td><td>6.6</td><td>6.3</td><td>6.1</td><td>6.0</td><td>5.9</td><td>5.7</td><td>5.7</td><td>5.6</td><td>5.6</td><td>5.5</td><td>5.5</td><td>5.5</td><td>5.5</td><td>5.4</td><td>5.3</td><td>5.3</td></tr></tbody></table>", "caption": "Table 4: Results of original (Ori.) estimators [29, 25, 30, 39] and DeciWatch under different sampling ratios. Ori. is the watch-every-frame pose estimator. Sampling interval N is set from 1 to 20. The best results are in bold.", "list_citation_info": ["[30] Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2252\u20132261 (2019)", "[29] Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127\u201311137 (2021)", "[25] Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In: 2021 International Conference on 3D Vision (3DV). pp. 42\u201352. IEEE (2021)", "[39] Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet effective baseline for 3d human pose estimation. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2640\u20132649 (2017)"]}, {"table": "<table><thead><tr><th>Metrics</th><th>No DenoiseNet</th><th>TCNs [43]</th><th>MLPs [58]</th><th>Ours</th></tr></thead><tbody><tr><th>MPJPE</th><td>79.8</td><td>80.5</td><td>79.5</td><td>77.2</td></tr></tbody></table>", "caption": "Table 5: Comparison results with different denoise network designs on 3DPW dataset with the state-of-the-art pose estimator Pare [29] (MPJPE is 78.9mm).", "list_citation_info": ["[43] Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.: 3d human pose estimation in video with temporal convolutions and semi-supervised training. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7753\u20137762 (2019)", "[29] Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127\u201311137 (2021)", "[58] Zeng, A., Yang, L., Ju, X., Li, J., Wang, J., Xu, Q.: Smoothnet: A plug-and-play network for refining human poses in videos. arXiv preprint arXiv:2112.13715 (2021)"]}, {"table": "<table><thead><tr><th>Metrics</th><th>Linear</th><th>TCNs [43]</th><th>TCNs w/MLPs</th><th>MLPs [58]</th><th>Ours</th></tr></thead><tbody><tr><th>MPJPE</th><td>79.8</td><td>172.3</td><td>99.5</td><td>78.0</td><td>77.2</td></tr></tbody></table>", "caption": "Table 6: Comparison results with different recovery network designs on 3DPW dataset with the state-of-the-art pose estimator PARE [29](MPJPE is 78.9mm).", "list_citation_info": ["[43] Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.: 3d human pose estimation in video with temporal convolutions and semi-supervised training. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7753\u20137762 (2019)", "[29] Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127\u201311137 (2021)", "[58] Zeng, A., Yang, L., Ju, X., Li, J., Wang, J., Xu, Q.: Smoothnet: A plug-and-play network for refining human poses in videos. arXiv preprint arXiv:2112.13715 (2021)"]}, {"table": "<table><thead><tr><th>\\lambda</th><th>1</th><th>2</th><th>5</th><th>10</th></tr></thead><tbody><tr><th>MPJPE</th><td>78.0</td><td>77.6</td><td>77.2</td><td>77.5</td></tr></tbody></table>", "caption": "Table 7: Comparison results of different loss weight \\lambda on 3DPW dataset with the state-of-the-art pose estimator Pare [29](MPJPE is 78.9mm).", "list_citation_info": ["[29] Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127\u201311137 (2021)"]}, {"table": "<table><thead><tr><th>C</th><th>12</th><th>32</th><th>64</th><th>128</th><th>256</th></tr></thead><tbody><tr><th>MPJPE</th><td>78.0</td><td>77.2</td><td>77.4</td><td>77.6</td><td>77.4</td></tr></tbody></table>", "caption": "Table 8: Comparison results of different embedding dimension C on 3DPW dataset with the state-of-the-art pose estimator Pare [29].", "list_citation_info": ["[29] Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127\u201311137 (2021)"]}, {"table": "<table><thead><tr><th>\\lambda</th><th>1</th><th>3</th><th>5</th><th>10</th></tr></thead><tbody><tr><th>MPJPE</th><td>79.3</td><td>77.5</td><td>77.2</td><td>77.6</td></tr></tbody></table>", "caption": "Table 9: Comparison results of different block number M on 3DPW dataset with the state-of-the-art pose estimator Pare [29].", "list_citation_info": ["[29] Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127\u201311137 (2021)"]}], "citation_info_to_title": {"[39] Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet effective baseline for 3d human pose estimation. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2640\u20132649 (2017)": "A simple yet effective baseline for 3d human pose estimation", "[22] Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence 36(7), 1325\u20131339 (2013)": "Human36m: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments", "[58] Zeng, A., Yang, L., Ju, X., Li, J., Wang, J., Xu, Q.: Smoothnet: A plug-and-play network for refining human poses in videos. arXiv preprint arXiv:2112.13715 (2021)": "Smoothnet: A Plug-and-Play Network for Refining Human Poses in Videos", "[30] Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2252\u20132261 (2019)": "Learning to reconstruct 3D human pose and shape via model-fitting in the loop", "[51] Xiao, B., Wu, H., Wei, Y.: Simple baselines for human pose estimation and tracking. In: Proceedings of the European conference on computer vision (ECCV). pp. 466\u2013481 (2018)": "Simple baselines for human pose estimation and tracking", "[43] Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.: 3d human pose estimation in video with temporal convolutions and semi-supervised training. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 7753\u20137762 (2019)": "3D Human Pose Estimation in Video with Temporal Convolutions and Semi-Supervised Training", "[34] Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance generation with aist++. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 13401\u201313412 (October 2021)": "Ai choreographer: Music conditioned 3d dance generation with aist++", "[29] Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127\u201311137 (2021)": "Pare: Part attention regressor for 3d human body estimation", "[25] Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In: 2021 International Conference on 3D Vision (3DV). pp. 42\u201352. IEEE (2021)": "Exemplar fine-tuning for 3D human model fitting towards in-the-wild 3D human pose estimation"}, "source_title_to_arxiv_id": {"Smoothnet: A Plug-and-Play Network for Refining Human Poses in Videos": "2112.13715", "Ai choreographer: Music conditioned 3d dance generation with aist++": "2101.08779"}}