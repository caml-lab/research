{"title": "DifferSketching: How Differently Do People Sketch 3D Objects?", "abstract": "Multiple sketch datasets have been proposed to understand how people draw 3D\nobjects. However, such datasets are often of small scale and cover a small set\nof objects or categories. In addition, these datasets contain freehand sketches\nmostly from expert users, making it difficult to compare the drawings by expert\nand novice users, while such comparisons are critical in informing more\neffective sketch-based interfaces for either user groups. These observations\nmotivate us to analyze how differently people with and without adequate drawing\nskills sketch 3D objects. We invited 70 novice users and 38 expert users to\nsketch 136 3D objects, which were presented as 362 images rendered from\nmultiple views. This leads to a new dataset of 3,620 freehand multi-view\nsketches, which are registered with their corresponding 3D objects under\ncertain views. Our dataset is an order of magnitude larger than the existing\ndatasets. We analyze the collected data at three levels, i.e., sketch-level,\nstroke-level, and pixel-level, under both spatial and temporal characteristics,\nand within and across groups of creators. We found that the drawings by\nprofessionals and novices show significant differences at stroke-level, both\nintrinsically and extrinsically. We demonstrate the usefulness of our dataset\nin two applications: (i) freehand-style sketch synthesis, and (ii) posing it as\na potential benchmark for sketch-based 3D reconstruction. Our dataset and code\nare available at https://chufengxiao.github.io/DifferSketching/.", "authors": ["Chufeng Xiao", "Wanchao Su", "Jing Liao", "Zhouhui Lian", "Yi-Zhe Song", "Hongbo Fu"], "published_date": "2022_09_19", "pdf_url": "http://arxiv.org/pdf/2209.08791v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Granularity</td><td>Dataset</td><td>User</td><td>Temporal</td><td>Register</td><td>Prompt</td><td>Repetition</td><td>Quantity</td><td>Category</td><td>View</td></tr><tr><td rowspan=\"3\">Symbolic</td><td>QuickDraw (Ha and Eck, 2017)</td><td>N/A</td><td>\u2713</td><td>\u2717</td><td>345</td><td>\\sim75k</td><td>\\sim25.9M</td><td>345</td><td>S</td></tr><tr><td>SlowSketch (Bhunia et al., 2020)</td><td>N/A</td><td>\u2713</td><td>\u2717</td><td>20</td><td>84-96</td><td>1,702</td><td>20</td><td>S</td></tr><tr><td>TU-Berlin (Eitz et al., 2012a)</td><td>N/A</td><td>\u2713</td><td>\u2717</td><td>250</td><td>\\sim80</td><td>\\sim20k</td><td>250</td><td>S</td></tr><tr><td rowspan=\"5\">Object-level</td><td>Sketchy (Sangkloy et al., 2016)</td><td>N/A</td><td>\u2713</td><td>\u2717</td><td>12,500</td><td>\\sim6</td><td>75,471</td><td>125</td><td>S</td></tr><tr><td>FG-SBIR (Yu et al., 2016)</td><td>N/A</td><td>\u2717</td><td>\u2717</td><td>1,432</td><td>1</td><td>1,432</td><td>2</td><td>S</td></tr><tr><td>FG-SBSR  (Qi et al., 2021)</td><td>N/A</td><td>\u2717</td><td>\u2717</td><td>4,680</td><td>1</td><td>4,680</td><td>2</td><td>M</td></tr><tr><td>ShapeNet-Sketch  (Zhang et al., 2021)</td><td>N/A</td><td>\u2717</td><td>\u2717</td><td>1,300</td><td>1</td><td>1,300</td><td>13</td><td>S</td></tr><tr><td>ProSketch-3DChair (Zhong et al., 2020b)</td><td>P</td><td>\u2717</td><td>\u2717</td><td>1,500</td><td>1</td><td>1,500</td><td>1</td><td>M</td></tr><tr><td rowspan=\"4\">Stroke-level</td><td>Princeton (Cole et al., 2008)</td><td>P</td><td>\u2717</td><td>\u2713</td><td>48</td><td>1-11</td><td>208</td><td>N/A</td><td>M</td></tr><tr><td>OpenSketch (Gryaditskaya et al., 2019)</td><td>P</td><td>\u2713</td><td>\u2713</td><td>24</td><td>12-30</td><td>357</td><td>N/A</td><td>M</td></tr><tr><td>SpeedTracer (Wang et al., 2021)</td><td>P</td><td>\u2713</td><td>\u2713</td><td>70</td><td>3-5</td><td>288</td><td>N/A</td><td>S</td></tr><tr><td>DifferSketching (Ours)</td><td>P+N</td><td>\u2713</td><td>\u2713</td><td>362</td><td>10</td><td>3,620</td><td>9</td><td>M</td></tr></tbody></table>", "caption": "Table 1. Comparisons of the closely related freehand sketch datasets from various perspectives.The \u201cUser\u201d column indicates the drawing skill levels of the participants involved in the data collection: \u201cN/A\u201d means no clear indication or specific requirement of the types of the users;\u201cP\u201d and \u201cN\u201d represent professional and novice users, respectively.The \u201cRepetition\u201d column reports the number of times for each prompt drawn by different users in each dataset.In the \u201cView\u201d column, \u201cS\u201d represents single views mostly used for data collection while \u201cM\u201d means the collection of multi-view sketches.\u201cN/A\u201d in the \u201cCategory\u201d column means that there is no clear categorical division in the corresponding dataset.Since OpenSketch (Gryaditskaya et al., 2019) collects the design drawings for the CAD models without clear semantic class information,we identify its category as \u201cN/A\u201d.For the data of SpeedTracer (Wang et al., 2021), we only consider its freehand sketches and exclude its drawings collected via tracing.", "list_citation_info": ["Bhunia et al. (2020) Ayan Kumar Bhunia, Ayan Das, Umar Riaz Muhammad, Yongxin Yang, Timothy M Hospedales, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe Song. 2020. Pixelor: A Competitive Sketching AI Agent. So you think you can sketch? ACM Transactions on Graphics (TOG) 39, 6 (2020), 1\u201315.", "Qi et al. (2021) Anran Qi, Yulia Gryaditskaya, Jifei Song, Yongxin Yang, Yonggang Qi, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song. 2021. Toward Fine-Grained Sketch-Based 3D Shape Retrieval. IEEE Transactions on Image Processing 30 (2021), 8595\u20138606.", "Eitz et al. (2012a) Mathias Eitz, James Hays, and Marc Alexa. 2012a. How do humans sketch objects? ACM Transactions on graphics (TOG) 31, 4 (2012), 1\u201310.", "Zhong et al. (2020b) Yue Zhong, Yonggang Qi, Yulia Gryaditskaya, Honggang Zhang, and Yi-Zhe Song. 2020b. Towards practical sketch-based 3d shape generation: The role of professional sketches. IEEE Transactions on Circuits and Systems for Video Technology 31, 9 (2020), 3518\u20133528.", "Cole et al. (2008) Forrester Cole, Aleksey Golovinskiy, Alex Limpaecher, Heather Stoddart Barros, Adam Finkelstein, Thomas Funkhouser, and Szymon Rusinkiewicz. 2008. Where do people draw lines? In ACM SIGGRAPH 2008 papers. 1\u201311.", "Gryaditskaya et al. (2019) Yulia Gryaditskaya, Mark Sypesteyn, Jan Willem Hoftijzer, Sylvia C Pont, Fr\u00e9do Durand, and Adrien Bousseau. 2019. OpenSketch: a richly-annotated dataset of product design sketches. ACM Trans. Graph. 38, 6 (2019), 232\u20131.", "Ha and Eck (2017) David Ha and Douglas Eck. 2017. A neural representation of sketch drawings. arXiv preprint arXiv:1704.03477 (2017).", "Yu et al. (2016) Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, and Chen-Change Loy. 2016. Sketch me that shoe. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 799\u2013807.", "Sangkloy et al. (2016) Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. 2016. The sketchy database: learning to retrieve badly drawn bunnies. ACM Transactions on Graphics (TOG) 35, 4 (2016), 1\u201312.", "Zhang et al. (2021) Song-Hai Zhang, Yuan-Chen Guo, and Qing-Wen Gu. 2021. Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6012\u20136021.", "Wang et al. (2021) Zeyu Wang, Sherry Qiu, Nicole Feng, Holly Rushmeier, Leonard McMillan, and Julie Dorsey. 2021. Tracing Versus Freehand for Evaluating Computer-Generated Drawings. ACM Trans. Graph. 40, 4 (Aug. 2021), 12 pages. https://doi.org/10.1145/3450626.3459819"]}], "citation_info_to_title": {"Zhong et al. (2020b) Yue Zhong, Yonggang Qi, Yulia Gryaditskaya, Honggang Zhang, and Yi-Zhe Song. 2020b. Towards practical sketch-based 3d shape generation: The role of professional sketches. IEEE Transactions on Circuits and Systems for Video Technology 31, 9 (2020), 3518\u20133528.": "Towards practical sketch-based 3d shape generation: The role of professional sketches", "Bhunia et al. (2020) Ayan Kumar Bhunia, Ayan Das, Umar Riaz Muhammad, Yongxin Yang, Timothy M Hospedales, Tao Xiang, Yulia Gryaditskaya, and Yi-Zhe Song. 2020. Pixelor: A Competitive Sketching AI Agent. So you think you can sketch? ACM Transactions on Graphics (TOG) 39, 6 (2020), 1\u201315.": "Pixelor: A Competitive Sketching AI Agent", "Gryaditskaya et al. (2019) Yulia Gryaditskaya, Mark Sypesteyn, Jan Willem Hoftijzer, Sylvia C Pont, Fr\u00e9do Durand, and Adrien Bousseau. 2019. OpenSketch: a richly-annotated dataset of product design sketches. ACM Trans. Graph. 38, 6 (2019), 232\u20131.": "OpenSketch: a richly-annotated dataset of product design sketches", "Ha and Eck (2017) David Ha and Douglas Eck. 2017. A neural representation of sketch drawings. arXiv preprint arXiv:1704.03477 (2017).": "A neural representation of sketch drawings", "Qi et al. (2021) Anran Qi, Yulia Gryaditskaya, Jifei Song, Yongxin Yang, Yonggang Qi, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song. 2021. Toward Fine-Grained Sketch-Based 3D Shape Retrieval. IEEE Transactions on Image Processing 30 (2021), 8595\u20138606.": "Toward Fine-Grained Sketch-Based 3D Shape Retrieval", "Cole et al. (2008) Forrester Cole, Aleksey Golovinskiy, Alex Limpaecher, Heather Stoddart Barros, Adam Finkelstein, Thomas Funkhouser, and Szymon Rusinkiewicz. 2008. Where do people draw lines? In ACM SIGGRAPH 2008 papers. 1\u201311.": "Where do people draw lines?", "Yu et al. (2016) Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, and Chen-Change Loy. 2016. Sketch me that shoe. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 799\u2013807.": "Sketch me that shoe", "Sangkloy et al. (2016) Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. 2016. The sketchy database: learning to retrieve badly drawn bunnies. ACM Transactions on Graphics (TOG) 35, 4 (2016), 1\u201312.": "The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies", "Wang et al. (2021) Zeyu Wang, Sherry Qiu, Nicole Feng, Holly Rushmeier, Leonard McMillan, and Julie Dorsey. 2021. Tracing Versus Freehand for Evaluating Computer-Generated Drawings. ACM Trans. Graph. 40, 4 (Aug. 2021), 12 pages. https://doi.org/10.1145/3450626.3459819": "Tracing Versus Freehand for Evaluating Computer-Generated Drawings", "Eitz et al. (2012a) Mathias Eitz, James Hays, and Marc Alexa. 2012a. How do humans sketch objects? ACM Transactions on graphics (TOG) 31, 4 (2012), 1\u201310.": "How do humans sketch objects?", "Zhang et al. (2021) Song-Hai Zhang, Yuan-Chen Guo, and Qing-Wen Gu. 2021. Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6012\u20136021.": "Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches"}, "source_title_to_arxiv_id": {"Sketch2Model: View-Aware 3D Modeling from Single Free-Hand Sketches": "2105.06663"}}