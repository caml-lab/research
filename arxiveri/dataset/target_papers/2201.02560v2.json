{"title": "A Novel Incremental Learning Driven Instance Segmentation Framework to Recognize Highly Cluttered Instances of the Contraband Items", "abstract": "Screening cluttered and occluded contraband items from baggage X-ray scans is\na cumbersome task even for the expert security staff. This paper presents a\nnovel strategy that extends a conventional encoder-decoder architecture to\nperform instance-aware segmentation and extract merged instances of contraband\nitems without using any additional sub-network or an object detector. The\nencoder-decoder network first performs conventional semantic segmentation and\nretrieves cluttered baggage items. The model then incrementally evolves during\ntraining to recognize individual instances using significantly reduced training\nbatches. To avoid catastrophic forgetting, a novel objective function minimizes\nthe network loss in each iteration by retaining the previously acquired\nknowledge while learning new class representations and resolving their complex\nstructural inter-dependencies through Bayesian inference. A thorough evaluation\nof our framework on two publicly available X-ray datasets shows that it\noutperforms state-of-the-art methods, especially within the challenging\ncluttered scenarios, while achieving an optimal trade-off between detection\naccuracy and efficiency.", "authors": ["Taimur Hassan", "Samet Akcay", "Mohammed Bennamoun", "Salman Khan", "Naoufel Werghi"], "published_date": "2022_01_07", "pdf_url": "http://arxiv.org/pdf/2201.02560v2", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th colspan=\"3\">IoU</th><th colspan=\"3\">DC</th></tr><tr><th>S</th><th>G</th><th>C</th><th>S</th><th>G</th><th>C</th></tr></thead><tbody><tr><td>CIE-Net</td><td>0.6883</td><td>0.7723</td><td>0.5861</td><td>0.8153</td><td>0.8715</td><td>0.7390</td></tr><tr><td>CIE-R-Net</td><td>0.6702</td><td>0.7852</td><td>0.5749</td><td>0.8025</td><td>0.8796</td><td>0.7300</td></tr><tr><td>PSPNet</td><td>0.6641</td><td>0.7694</td><td>0.5728</td><td>0.7981</td><td>0.8696</td><td>0.7283</td></tr><tr><td>SegNet</td><td>0.6559</td><td>0.7463</td><td>0.5640</td><td>0.7921</td><td>0.8547</td><td>0.7212</td></tr><tr><td>U-Net</td><td>0.6434</td><td>0.7384</td><td>0.5514</td><td>0.7830</td><td>0.8495</td><td>0.7108</td></tr><tr><td>FCN-8</td><td>0.5792</td><td>0.6431</td><td>0.4527</td><td>0.6973</td><td>0.7827</td><td>0.6232</td></tr><tr><td>FCN-32</td><td>0.5084</td><td>0.6246</td><td>0.3931</td><td>0.6740</td><td>0.7689</td><td>0.5643</td></tr></tbody></table>", "caption": "Table 1: Evaluation of the different segmentation models on the SIXray (S) [1], GDXray (G) [44] and Combined (C) dataset. Bold indicates the best performance.", "list_citation_info": ["[1] C. Miao, L. Xie, F. Wan, C. Su, H. Liu, J. Jiao, and Q. Ye, \u201cSIXray: A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images,\u201d in IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2119\u20132128, 2019.", "[44] D. Mery, V. Riffo, U. Zscherpel, G. Mondrag\u00f3n, I. Lillo, I. Zuccar, H. Lobel, and M. Carrasco, \u201cGDXray: The database of X-ray images for nondestructive testing,\u201d Journal of Nondestructive Evaluation, Volume 34, Issue: 4, 2015."]}, {"table": "<table><thead><tr><th>Loss Functions</th><th>GDXray [44]</th><th>SIXray [1]</th><th>Combined</th></tr></thead><tbody><tr><th>L_{mi}</th><td>0.7723</td><td>0.6883</td><td>0.5861</td></tr><tr><th>L_{sp} [56]</th><td>0.7504</td><td>0.6734</td><td>0.5480</td></tr><tr><th>L_{od} [54]</th><td>0.7349</td><td>0.6162</td><td>0.5018</td></tr><tr><th>L_{ds} [39]</th><td>0.7421</td><td>0.6395</td><td>0.5237</td></tr><tr><th>L_{cd} [35]</th><td>0.6052</td><td>0.4793</td><td>0.2746</td></tr></tbody></table>", "caption": "Table 3: Comparison of L_{mi} with state-of-the-art knowledge distillation loss functions in terms of IoU. To ensure fairness, we used CIE-Net with all the loss functions. ", "list_citation_info": ["[54] U. Michieli and P. Zanuttigh, \u201cIncremental Learning Techniques for Semantic Segmentation,\u201d IEEE International Conference on Computer Vision Workshops (ICCVW), 2019.", "[39] U. Michieli and P. Zanuttigh, \u201cKnowledge Distillation for Incremental Learning in Semantic Segmentation,\u201d arXiv:1911.03462, 2020.", "[35] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \u201ciCaRL: Incremental Classifier and Representation Learning,\u201d IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[56] F. Tung and G. Mori, \u201cSimilarity-Preserving Knowledge Distillation,\u201d IEEE International Conference on Computer Vision (ICCV), 2019.", "[44] D. Mery, V. Riffo, U. Zscherpel, G. Mondrag\u00f3n, I. Lillo, I. Zuccar, H. Lobel, and M. Carrasco, \u201cGDXray: The database of X-ray images for nondestructive testing,\u201d Journal of Nondestructive Evaluation, Volume 34, Issue: 4, 2015.", "[1] C. Miao, L. Xie, F. Wan, C. Su, H. Liu, J. Jiao, and Q. Ye, \u201cSIXray: A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images,\u201d in IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2119\u20132128, 2019."]}, {"table": "<table><thead><tr><th>Metric</th><th>Method</th><th>GDXray</th><th>SIXray</th><th>Combined</th></tr></thead><tbody><tr><td>IoU</td><td>Proposed</td><td>0.7723</td><td>0.6883</td><td>0.5861</td></tr><tr><td></td><td>MS RCNN [47]</td><td>0.7201</td><td>0.6484</td><td>0.5482</td></tr><tr><td></td><td>Mask RCNN [12]</td><td>0.7098</td><td>0.6381</td><td>0.5243</td></tr><tr><td></td><td>HTC [48]</td><td>0.7364</td><td>0.6559</td><td>0.5804</td></tr><tr><td></td><td>YOLACT [49]</td><td>0.7089</td><td>0.6110</td><td>0.4937</td></tr><tr><td></td><td>DSRL [29]</td><td>0.7421</td><td>0.6542</td><td>0.5709</td></tr><tr><td></td><td>MvRF-CNN [28]</td><td>0.6982</td><td>0.6016</td><td>0.4918</td></tr><tr><td></td><td>TST-L_{s} [7]</td><td>0.6851</td><td>0.5874</td><td>0.4285</td></tr><tr><td>DC</td><td>Proposed</td><td>0.8715</td><td>0.8153</td><td>0.7390</td></tr><tr><td></td><td>MS RCNN [47]</td><td>0.8372</td><td>0.7867</td><td>0.7081</td></tr><tr><td></td><td>Mask RCNN [12]</td><td>0.8302</td><td>0.7790</td><td>0.6879</td></tr><tr><td></td><td>HTC [48]</td><td>0.8481</td><td>0.7921</td><td>0.7344</td></tr><tr><td></td><td>YOLACT [49]</td><td>0.8296</td><td>0.7585</td><td>0.6610</td></tr><tr><td></td><td>DSRL [29]</td><td>0.8519</td><td>0.7909</td><td>0.7268</td></tr><tr><td></td><td>MvRF-CNN [28]</td><td>0.8222</td><td>0.7512</td><td>0.6593</td></tr><tr><td></td><td>TST-L_{s} [7]</td><td>0.8131</td><td>0.7400</td><td>0.5999</td></tr><tr><td>Recall</td><td>Proposed</td><td>0.8643</td><td>0.8057</td><td>0.7391</td></tr><tr><td></td><td>MS RCNN [47]</td><td>0.8238</td><td>0.7613</td><td>0.6846</td></tr><tr><td></td><td>Mask RCNN [12]</td><td>0.8183</td><td>0.7542</td><td>0.6653</td></tr><tr><td></td><td>HTC [48]</td><td>0.8392</td><td>0.7736</td><td>0.7284</td></tr><tr><td></td><td>YOLACT [49]</td><td>0.8195</td><td>0.7461</td><td>0.6548</td></tr><tr><td></td><td>DSRL [29]</td><td>0.8407</td><td>0.7705</td><td>0.7173</td></tr><tr><td></td><td>MvRF-CNN [28]</td><td>0.8196</td><td>0.7344</td><td>0.6419</td></tr><tr><td></td><td>TST-L_{s} [7]</td><td>0.8092</td><td>0.7269</td><td>0.5764</td></tr><tr><td>Precision</td><td>Proposed</td><td>0.8952</td><td>0.8348</td><td>0.7401</td></tr><tr><td></td><td>MS RCNN [47]</td><td>0.8564</td><td>0.8153</td><td>0.7269</td></tr><tr><td></td><td>Mask RCNN [12]</td><td>0.8439</td><td>0.8072</td><td>0.7154</td></tr><tr><td></td><td>HTC [48]</td><td>0.8607</td><td>0.8236</td><td>0.7318</td></tr><tr><td></td><td>YOLACT [49]</td><td>0.8353</td><td>0.7669</td><td>0.6703</td></tr><tr><td></td><td>DSRL [29]</td><td>0.8736</td><td>0.8125</td><td>0.7311</td></tr><tr><td></td><td>MvRF-CNN [28]</td><td>0.8245</td><td>0.7801</td><td>0.6786</td></tr><tr><td></td><td>TST-L_{s} [7]</td><td>0.8173</td><td>0.7614</td><td>0.6256</td></tr></tbody></table>", "caption": "Table 4:  Comparison of the proposed framework with state-of-the-art solutions for extracting baggage threats. Bold indicates the best performance, while the second-best scores are underlined.", "list_citation_info": ["[47] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, \u201cMask Scoring R-CNN,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6409\u20136418, 2019.", "[28] T. Akilan, Q. M. J. Wu, and W. Zhang, \u201cVideo foreground extraction using multi-view receptive field and encoder\u2013decoder dcnn for traffic and surveillance applications,\u201d IEEE Transactions on Vehicular Technology, vol. 68, no. 10, pp. 9478\u20139493, 2019.", "[29] L. Wang, D. Li, Y. Zhu, L. Tian, and Y. Shan, \u201cDual super-resolution learning for semantic segmentation,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3773\u20133782, 2020.", "[7] T. Hassan and N. Werghi, \u201cTrainable Structure Tensors for Autonomous Baggage Threat Detection Under Extreme Occlusion,\u201d Asian Conference on Computer Vision (ACCV), September 2020.", "[48] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, et al., \u201cHybrid Task Cascade for Instance Segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 4974\u20134983, 2019.", "[49] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYOLACT: Real-Time Instance Segmentation,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 9157\u20139166, 2019.", "[12] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \u201cMask R-CNN,\u201d in IEEE International Conference on Computer Vision (ICCV), pp. 2961\u20132969, 2017."]}, {"table": "<table><thead><tr><th>D</th><th>M</th><th>\\mu_{ap}^{m}</th><th>\\mu_{ap}^{m:50}</th><th>\\mu_{ap}^{m:75}</th><th>\\mu_{ap}^{b}</th><th>\\mu_{ap}^{b:50}</th><th>\\mu_{ap}^{m:75}</th></tr></thead><tbody><tr><td>G</td><td>PF</td><td>0.5068</td><td>0.7902</td><td>0.5006</td><td>0.6101</td><td>0.8556</td><td>0.6462</td></tr><tr><td></td><td>MSR</td><td>0.4584</td><td>0.7283</td><td>0.4986</td><td>0.5564</td><td>0.8091</td><td>0.6033</td></tr><tr><td></td><td>MR</td><td>0.4311</td><td>0.7194</td><td>0.4893</td><td>0.5282</td><td>0.7833</td><td>0.5842</td></tr><tr><td></td><td>HTC</td><td>0.4861</td><td>0.7706</td><td>0.4997</td><td>0.5971</td><td>0.8314</td><td>0.6324</td></tr><tr><td></td><td>YT</td><td>0.3629</td><td>0.6518</td><td>0.3794</td><td>0.4852</td><td>0.7478</td><td>0.5491</td></tr><tr><td></td><td>CST{}_{o}*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.9343</td><td>-</td></tr><tr><td></td><td>TST{}_{o}*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.9672</td><td>-</td></tr><tr><td></td><td>TST{}_{i}</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.8245</td><td>-</td></tr><tr><td></td><td>CST{}_{i}</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.8169</td><td>-</td></tr><tr><td></td><td>TSD*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.9162</td><td>-</td></tr><tr><td>S</td><td>PF</td><td>0.4795</td><td>0.6893</td><td>0.4872</td><td>0.5367</td><td>0.7653</td><td>0.5374</td></tr><tr><td></td><td>MSR</td><td>0.4017</td><td>0.6347</td><td>0.4063</td><td>0.4653</td><td>0.6756</td><td>0.4782</td></tr><tr><td></td><td>MR</td><td>0.3654</td><td>0.5973</td><td>0.3592</td><td>0.4182</td><td>0.6326</td><td>0.4067</td></tr><tr><td></td><td>HTC</td><td>0.4525</td><td>0.6629</td><td>0.4538</td><td>0.5082</td><td>0.7384</td><td>0.5021</td></tr><tr><td></td><td>YT</td><td>0.3355</td><td>0.5632</td><td>0.3190</td><td>0.3811</td><td>0.6237</td><td>0.3643</td></tr><tr><td></td><td>CST{}_{o}*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.9595</td><td>-</td></tr><tr><td></td><td>TST{}_{o}*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.9516</td><td>-</td></tr><tr><td></td><td>TST{}_{i}</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.7248</td><td>-</td></tr><tr><td></td><td>CST{}_{i}</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.7351</td><td>-</td></tr><tr><td></td><td>TSD*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.6457</td><td>-</td></tr><tr><td></td><td>CHR</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.5760</td><td>-</td></tr><tr><td>C</td><td>PF</td><td>0.4059</td><td>0.6249</td><td>0.4153</td><td>0.4862</td><td>0.7249</td><td>0.4983</td></tr><tr><td></td><td>MSR</td><td>0.3591</td><td>0.5986</td><td>0.3865</td><td>0.4023</td><td>0.6298</td><td>0.4572</td></tr><tr><td></td><td>MR</td><td>0.3129</td><td>0.5542</td><td>0.3301</td><td>0.3627</td><td>0.5983</td><td>0.3821</td></tr><tr><td></td><td>HTC</td><td>0.4023</td><td>0.6173</td><td>0.4102</td><td>0.4752</td><td>0.7203</td><td>0.4859</td></tr><tr><td></td><td>YT</td><td>0.3098</td><td>0.5286</td><td>0.3123</td><td>0.3561</td><td>0.5937</td><td>0.3696</td></tr><tr><td></td><td>TST{}_{i}</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.6718</td><td>-</td></tr><tr><td></td><td>CST{}_{i}</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.6526</td><td>-</td></tr></tbody></table><ul><li><p>Abbreviations: D: Dataset, G: GDXray [44], S: SIXray [1], C: Combined Dataset, M: Methods,PF: Proposed Framework, MSR: Mask Scoring R-CNN [47], MR: Mask R-CNN [12], and YT: YOLACT [49]. Moreover, \u2019*\u2019 indicates unfair comparison.</p></li></ul>", "caption": "Table 5:  Comparison of the proposed framework with state-of-the-art solutions for extracting contraband items. Bold indicates the best scores, while \u2018-\u2019 means that the metric is not computed.", "list_citation_info": ["[47] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, \u201cMask Scoring R-CNN,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6409\u20136418, 2019.", "[44] D. Mery, V. Riffo, U. Zscherpel, G. Mondrag\u00f3n, I. Lillo, I. Zuccar, H. Lobel, and M. Carrasco, \u201cGDXray: The database of X-ray images for nondestructive testing,\u201d Journal of Nondestructive Evaluation, Volume 34, Issue: 4, 2015.", "[49] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYOLACT: Real-Time Instance Segmentation,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 9157\u20139166, 2019.", "[1] C. Miao, L. Xie, F. Wan, C. Su, H. Liu, J. Jiao, and Q. Ye, \u201cSIXray: A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images,\u201d in IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2119\u20132128, 2019.", "[12] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \u201cMask R-CNN,\u201d in IEEE International Conference on Computer Vision (ICCV), pp. 2961\u20132969, 2017."]}, {"table": "<table><thead><tr><th>Method</th><th>Time Performance (sec)</th></tr></thead><tbody><tr><td>YOLOv3 [57]</td><td>0.023</td></tr><tr><td>CST [25]</td><td>0.023</td></tr><tr><td>RetinaNet [2]</td><td>0.033</td></tr><tr><td>YOLCAT [49]</td><td>0.036</td></tr><tr><td>CIE-Net (Proposed)</td><td>0.072</td></tr><tr><td>Mask R-CNN [12]</td><td>0.141</td></tr><tr><td>MS R-CNN [47]</td><td>0.156</td></tr><tr><td>HTC [48]</td><td>0.311</td></tr></tbody></table>", "caption": "Table 6:  Comparison of the run-time performance. The scores here represent the mean inference time of the two datasets. Bold indicates the best performance while the second-best performance is underlined. ", "list_citation_info": ["[47] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, \u201cMask Scoring R-CNN,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6409\u20136418, 2019.", "[25] T. Hassan, M. Bettayeb, S. Ak\u00e7ay, S. Khan, M. Bennamoun, and N. Werghi, \u201cDetecting Prohibited Items in X-ray Images: A Contour Proposal Learning Approach,\u201d IEEE International Conference on Image Processing (ICIP), pp. 2016-2020, 2020.", "[2] T. Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, \u201cFocal Loss for Dense Object Detection,\u201d IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.", "[48] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, et al., \u201cHybrid Task Cascade for Instance Segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 4974\u20134983, 2019.", "[49] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYOLACT: Real-Time Instance Segmentation,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 9157\u20139166, 2019.", "[12] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \u201cMask R-CNN,\u201d in IEEE International Conference on Computer Vision (ICCV), pp. 2961\u20132969, 2017.", "[57] J. Redmon and A. Farhadi, \u201cYOLOv3: An Incremental Improvement,\u201d arXiv, 2018."]}], "citation_info_to_title": {"[7] T. Hassan and N. Werghi, \u201cTrainable Structure Tensors for Autonomous Baggage Threat Detection Under Extreme Occlusion,\u201d Asian Conference on Computer Vision (ACCV), September 2020.": "Trainable Structure Tensors for Autonomous Baggage Threat Detection Under Extreme Occlusion", "[1] C. Miao, L. Xie, F. Wan, C. Su, H. Liu, J. Jiao, and Q. Ye, \u201cSIXray: A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images,\u201d in IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2119\u20132128, 2019.": "SIXray: A Large-scale Security Inspection X-ray Benchmark for Prohibited Item Discovery in Overlapping Images", "[28] T. Akilan, Q. M. J. Wu, and W. Zhang, \u201cVideo foreground extraction using multi-view receptive field and encoder\u2013decoder dcnn for traffic and surveillance applications,\u201d IEEE Transactions on Vehicular Technology, vol. 68, no. 10, pp. 9478\u20139493, 2019.": "Video foreground extraction using multi-view receptive field and encoder-decoder DCNN for traffic and surveillance applications", "[57] J. Redmon and A. Farhadi, \u201cYOLOv3: An Incremental Improvement,\u201d arXiv, 2018.": "YOLOv3: An Incremental Improvement", "[48] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Shi, W. Ouyang, et al., \u201cHybrid Task Cascade for Instance Segmentation,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, pp. 4974\u20134983, 2019.": "Hybrid Task Cascade for Instance Segmentation", "[39] U. Michieli and P. Zanuttigh, \u201cKnowledge Distillation for Incremental Learning in Semantic Segmentation,\u201d arXiv:1911.03462, 2020.": "Knowledge Distillation for Incremental Learning in Semantic Segmentation", "[2] T. Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, \u201cFocal Loss for Dense Object Detection,\u201d IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.": "Focal Loss for Dense Object Detection", "[35] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, \u201ciCaRL: Incremental Classifier and Representation Learning,\u201d IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2017.": "iCaRL: Incremental Classifier and Representation Learning", "[49] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, \u201cYOLACT: Real-Time Instance Segmentation,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 9157\u20139166, 2019.": "YOLACT: Real-Time Instance Segmentation", "[12] K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \u201cMask R-CNN,\u201d in IEEE International Conference on Computer Vision (ICCV), pp. 2961\u20132969, 2017.": "Mask R-CNN", "[25] T. Hassan, M. Bettayeb, S. Ak\u00e7ay, S. Khan, M. Bennamoun, and N. Werghi, \u201cDetecting Prohibited Items in X-ray Images: A Contour Proposal Learning Approach,\u201d IEEE International Conference on Image Processing (ICIP), pp. 2016-2020, 2020.": "Detecting Prohibited Items in X-ray Images: A Contour Proposal Learning Approach", "[44] D. Mery, V. Riffo, U. Zscherpel, G. Mondrag\u00f3n, I. Lillo, I. Zuccar, H. Lobel, and M. Carrasco, \u201cGDXray: The database of X-ray images for nondestructive testing,\u201d Journal of Nondestructive Evaluation, Volume 34, Issue: 4, 2015.": "GDXray: The database of X-ray images for nondestructive testing", "[54] U. Michieli and P. Zanuttigh, \u201cIncremental Learning Techniques for Semantic Segmentation,\u201d IEEE International Conference on Computer Vision Workshops (ICCVW), 2019.": "Incremental Learning Techniques for Semantic Segmentation", "[56] F. Tung and G. Mori, \u201cSimilarity-Preserving Knowledge Distillation,\u201d IEEE International Conference on Computer Vision (ICCV), 2019.": "Similarity-Preserving Knowledge Distillation", "[47] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, \u201cMask Scoring R-CNN,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6409\u20136418, 2019.": "Mask Scoring R-CNN", "[29] L. Wang, D. Li, Y. Zhu, L. Tian, and Y. Shan, \u201cDual super-resolution learning for semantic segmentation,\u201d in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3773\u20133782, 2020.": "Dual super-resolution learning for semantic segmentation"}, "source_title_to_arxiv_id": {"Trainable Structure Tensors for Autonomous Baggage Threat Detection Under Extreme Occlusion": "2009.13158"}}