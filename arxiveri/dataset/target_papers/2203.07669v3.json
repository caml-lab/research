{"title": "Progressive End-to-End Object Detection in Crowded Scenes", "abstract": "In this paper, we propose a new query-based detection framework for crowd\ndetection. Previous query-based detectors suffer from two drawbacks: first,\nmultiple predictions will be inferred for a single object, typically in crowded\nscenes; second, the performance saturates as the depth of the decoding stage\nincreases. Benefiting from the nature of the one-to-one label assignment rule,\nwe propose a progressive predicting method to address the above issues.\nSpecifically, we first select accepted queries prone to generate true positive\npredictions, then refine the rest noisy queries according to the previously\naccepted predictions. Experiments show that our method can significantly boost\nthe performance of query-based detectors in crowded scenes. Equipped with our\napproach, Sparse RCNN achieves 92.0\\% $\\text{AP}$, 41.4\\% $\\text{MR}^{-2}$ and\n83.2\\% $\\text{JI}$ on the challenging CrowdHuman \\cite{shao2018crowdhuman}\ndataset, outperforming the box-based method MIP \\cite{chu2020detection} that\nspecifies in handling crowded scenarios. Moreover, the proposed method, robust\nto crowdedness, can still obtain consistent improvements on moderately and\nslightly crowded datasets like CityPersons \\cite{zhang2017citypersons} and COCO\n\\cite{lin2014microsoft}. Code will be made publicly available at\nhttps://github.com/megvii-model/Iter-E2EDET.", "authors": ["Anlin Zheng", "Yuang Zhang", "Xiangyu Zhang", "Xiaojuan Qi", "Jian Sun"], "published_date": "2022_03_15", "pdf_url": "http://arxiv.org/pdf/2203.07669v3", "list_table_and_caption": [{"table": "<table><thead><tr><th>Dataset</th><th># objects/img</th><th># overlaps/img</th></tr></thead><tbody><tr><th>CrowdHuman [37]</th><td>22.64</td><td>2.40</td></tr><tr><th>CityPersons [49]</th><td>6.47</td><td>0.32</td></tr><tr><th>COCO{}^{*} [27]</th><td>9.34</td><td>0.015</td></tr></tbody></table>", "caption": "Table 1: Instance density of each dataset. The threshold for overlap statistics is \\mathrm{IoU}&gt;0.5. *Averaged by the number of classes.", "list_citation_info": ["[49] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. Citypersons: A diverse dataset for pedestrian detection. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4457\u20134465, 2017.", "[27] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740\u2013755, 2014.", "[37] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: A benchmark for detecting human in a crowd. arXiv preprint arXiv:1805.00123, 2018."]}, {"table": "<table><tbody><tr><th>Method</th><th>#Queries</th><td>AP</td><td>\\text{MR}^{-2}</td><td>JI</td></tr><tr><th>box-based</th><th></th><td></td><td></td><td></td></tr><tr><th>RetinaNet [26]</th><th>-</th><td>85.3</td><td>55.1</td><td>73.7</td></tr><tr><th>ATSS [50]</th><th>-</th><td>87.0</td><td>51.1</td><td>75.9</td></tr><tr><th>ATSS [50]+MIP [8]</th><th>-</th><td>88.7</td><td>51.6</td><td>77.0</td></tr><tr><th>FPN [25]+NMS</th><th>-</th><td>85.8</td><td>42.9</td><td>79.8</td></tr><tr><th>FPN [25]+soft NMS</th><th>-</th><td>88.2</td><td>42.9</td><td>79.8</td></tr><tr><th>FPN+MIP [8]</th><th>-</th><td>90.7</td><td>41.4</td><td>82.4</td></tr><tr><th>{\\text{FPN}}^{{\\dagger}}\\text{+NMS}</th><th>-</th><td>84.9</td><td>46.3</td><td>\u2013</td></tr><tr><th>{\\text{Adaptive NMS}}^{{\\dagger}} [28]</th><th>-</th><td>84.7</td><td>47.7</td><td>\u2013</td></tr><tr><th>{\\text{PBN}^{{\\dagger}}} [21]</th><th>-</th><td>89.3</td><td>43.4</td><td>\u2013</td></tr><tr><th>point-based</th><th></th><td></td><td></td><td></td></tr><tr><th>FCOS [42]</th><th>-</th><td>86.8</td><td>54.0</td><td>75.7</td></tr><tr><th>FCOS [42]+MIP [8]</th><th>-</th><td>87.3</td><td>51.2</td><td>77.3</td></tr><tr><th>POTO [46]</th><th>-</th><td>89.1</td><td>47.8</td><td>79.3</td></tr><tr><th>query-based</th><th></th><td></td><td></td><td></td></tr><tr><th>DETR [3]</th><th>100</th><td>75.9</td><td>73.2</td><td>74.4</td></tr><tr><th>PEDR [24]</th><th>1000</th><td>91.6</td><td>43.7</td><td>83.3</td></tr><tr><th>D-DETR [54]</th><th>1000</th><td>91.5</td><td>43.7</td><td>83.1</td></tr><tr><th>S-RCNN [39]</th><th>500</th><td>90.7</td><td>44.7</td><td>81.4</td></tr><tr><th>S-RCNN [39]</th><th>750</th><td>91.3</td><td>44.8</td><td>81.3</td></tr><tr><th>S-RCNN+Ours</th><th>500</th><td>92.0</td><td>41.4</td><td>83.2</td></tr><tr><th>S-RCNN+Ours</th><th>750</th><td>92.5</td><td>41.6</td><td>83.3</td></tr><tr><th>D-DETR+Ours</th><th>1000</th><td>92.1</td><td>41.5</td><td>84.0</td></tr></tbody></table>", "caption": "Table 2: Comparisons of different methods on CrowdHuman validation set, +MIP represents multiple instance prediction with set NMS as post-processing. {{\\dagger}} indicates the approach is implemented by PBM [21]. S-RCNN \u2013 Sparse RCNN [39]. D-DETR \u2013 deformable DETR [54].", "list_citation_info": ["[25] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 936\u2013944, 2017.", "[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213\u2013229, 2020.", "[24] Matthieu Lin, Chuming Li, Xingyuan Bu, Ming Sun, Chen Lin, Junjie Yan, Wanli Ouyang, and Zhidong Deng. Detr for crowd pedestrian detection. arXiv preprint arXiv:2012.06785, 2020.", "[39] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse r-cnn: End-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450, 2020.", "[50] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. arXiv preprint arXiv:1912.02424, 2019.", "[42] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: A simple and strong anchor-free object detector. 2021.", "[26] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318\u2013327, 2020.", "[46] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. arXiv preprint arXiv:2012.03544, 2020.", "[8] Xuangeng Chu, Anlin Zheng, Xiangyu Zhang, and Jian Sun. Detection in crowded scenes: One proposal, multiple predictions. pages 12214\u201312223, 2020.", "[21] Xin Huang, Zheng Ge, Zequn Jie, and Osamu Yoshie. Nms by representative region: Towards crowded pedestrian detection by proposal pairing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10750\u201310759, 2020.", "[28] Songtao Liu, Di Huang, and Yunhong Wang. Adaptive nms: Refining pedestrian detection in a crowd. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6459\u20136468, 2019.", "[54] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR 2021: The Ninth International Conference on Learning Representations, 2021."]}, {"table": "<table><thead><tr><th><p>Method</p></th><th><p>AP</p></th><th><p>\\text{AP}_{50}</p></th><th><p>\\text{AP}_{75}</p></th><th><p>\\text{AP}_{S}</p></th><th><p>\\text{AP}_{M}</p></th><th><p>\\text{AP}_{L}</p></th></tr></thead><tbody><tr><th><p>S-RCNN [39]</p></th><td><p>45.0</p></td><td><p>64.2</p></td><td><p>49.1</p></td><td><p>27.6</p></td><td><p>47.5</p></td><td><p>59.1</p></td></tr><tr><th><p>D-DETR [54]</p></th><td><p>45.8</p></td><td><p>64.5</p></td><td><p>49.4</p></td><td><p>28.2</p></td><td><p>49.0</p></td><td><p>61.7</p></td></tr><tr><th><p>S-RCNN+Ours</p></th><td><p>46.1</p></td><td><p>65.3</p></td><td><p>50.6</p></td><td><p>29.2</p></td><td><p>48.7</p></td><td><p>59.9</p></td></tr><tr><th><p>D-DETR+Ours</p></th><td>46.7</td><td><p>65.3</p></td><td><p>50.3</p></td><td><p>28.6</p></td><td><p>49.8</p></td><td><p>61.7</p></td></tr></tbody></table>", "caption": "Table 4: Performance comparisons of different methods on COCO 2017  [27] minival set.", "list_citation_info": ["[54] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR 2021: The Ninth International Conference on Learning Representations, 2021.", "[39] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse r-cnn: End-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450, 2020.", "[27] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740\u2013755, 2014."]}, {"table": "<table><thead><tr><th><p>Method</p></th><th><p>Dataset</p></th><th><p>#Queries</p></th><th><p>AP</p></th><th><p>\\text{MR}^{-2}</p></th><th><p>JI</p></th></tr></thead><tbody><tr><th><p>S-RCNN</p></th><th></th><th><p>500</p></th><td><p>93.1</p></td><td><p>39.9</p></td><td><p>85.1</p></td></tr><tr><th><p>S-RCNN+Ours</p></th><th><p>CHuman</p></th><th><p>500</p></th><td><p>93.8</p></td><td>37.4</td><td><p>86.5</p></td></tr><tr><th><p>D-DETR</p></th><th></th><th><p>1000</p></th><td><p>93.4</p></td><td><p>39.6</p></td><td><p>86.3</p></td></tr><tr><th><p>D-DETR+Ours</p></th><th></th><th><p>1000</p></th><td>94.1</td><td><p>37.7</p></td><td>87.1</td></tr><tr><th><p>S-RCNN</p></th><th></th><th><p>500</p></th><td><p>98.3</p></td><td><p>5.9</p></td><td><p>93.7</p></td></tr><tr><th><p>D-DETR</p></th><th><p>CPersons</p></th><th><p>500</p></th><td><p>96.4</p></td><td><p>8.4</p></td><td><p>92.0</p></td></tr><tr><th><p>S-RCNN+Ours</p></th><th></th><th><p>500</p></th><td>98.4</td><td>4.9</td><td>94.2</td></tr><tr><th><p>D-DETR+Ours</p></th><th></th><th><p>500</p></th><td><p>97.5</p></td><td><p>5.9</p></td><td><p>93.7</p></td></tr></tbody></table>", "caption": "Table 6: Experiment on CHuman(CrowdHuman) and CPersons(CityPersons) with Swin-L [29]. S-RCNN \u2013 Sparse RCNN [39], D-DETR \u2013 Deformable DETR [54]", "list_citation_info": ["[54] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR 2021: The Ninth International Conference on Learning Representations, 2021.", "[39] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse r-cnn: End-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450, 2020.", "[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021."]}], "citation_info_to_title": {"[46] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object detection with fully convolutional network. arXiv preprint arXiv:2012.03544, 2020.": "End-to-end object detection with fully convolutional network", "[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213\u2013229, 2020.": "End-to-end object detection with transformers", "[42] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: A simple and strong anchor-free object detector. 2021.": "FCOS: A simple and strong anchor-free object detector", "[8] Xuangeng Chu, Anlin Zheng, Xiangyu Zhang, and Jian Sun. Detection in crowded scenes: One proposal, multiple predictions. pages 12214\u201312223, 2020.": "Detection in crowded scenes: One proposal, multiple predictions", "[26] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318\u2013327, 2020.": "Focal loss for dense object detection", "[25] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 936\u2013944, 2017.": "Feature Pyramid Networks for Object Detection", "[29] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[28] Songtao Liu, Di Huang, and Yunhong Wang. Adaptive nms: Refining pedestrian detection in a crowd. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6459\u20136468, 2019.": "Adaptive NMS: Refining Pedestrian Detection in a Crowd", "[49] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. Citypersons: A diverse dataset for pedestrian detection. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4457\u20134465, 2017.": "Citypersons: A diverse dataset for pedestrian detection", "[21] Xin Huang, Zheng Ge, Zequn Jie, and Osamu Yoshie. Nms by representative region: Towards crowded pedestrian detection by proposal pairing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10750\u201310759, 2020.": "Towards crowded pedestrian detection by proposal pairing", "[24] Matthieu Lin, Chuming Li, Xingyuan Bu, Ming Sun, Chen Lin, Junjie Yan, Wanli Ouyang, and Zhidong Deng. Detr for crowd pedestrian detection. arXiv preprint arXiv:2012.06785, 2020.": "Detr for Crowd Pedestrian Detection", "[37] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: A benchmark for detecting human in a crowd. arXiv preprint arXiv:1805.00123, 2018.": "Crowdhuman: A benchmark for detecting human in a crowd", "[39] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse r-cnn: End-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450, 2020.": "Sparse R-CNN: End-to-End Object Detection with Learnable Proposals", "[50] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. arXiv preprint arXiv:1912.02424, 2019.": "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection", "[54] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR 2021: The Ninth International Conference on Learning Representations, 2021.": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "[27] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740\u2013755, 2014.": "Microsoft coco: Common objects in context"}, "source_title_to_arxiv_id": {"End-to-end object detection with fully convolutional network": "2012.03544", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Detr for Crowd Pedestrian Detection": "2012.06785"}}