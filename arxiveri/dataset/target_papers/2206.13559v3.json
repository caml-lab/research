{"title": "ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning", "abstract": "Capitalizing on large pre-trained models for various downstream tasks of\ninterest have recently emerged with promising performance. Due to the\never-growing model size, the standard full fine-tuning based task adaptation\nstrategy becomes prohibitively costly in terms of model training and storage.\nThis has led to a new research direction in parameter-efficient transfer\nlearning. However, existing attempts typically focus on downstream tasks from\nthe same modality (e.g., image understanding) of the pre-trained model. This\ncreates a limit because in some specific modalities, (e.g., video\nunderstanding) such a strong pre-trained model with sufficient knowledge is\nless or not available. In this work, we investigate such a novel cross-modality\ntransfer learning setting, namely parameter-efficient image-to-video transfer\nlearning. To solve this problem, we propose a new Spatio-Temporal Adapter\n(ST-Adapter) for parameter-efficient fine-tuning per video task. With a\nbuilt-in spatio-temporal reasoning capability in a compact design, ST-Adapter\nenables a pre-trained image model without temporal knowledge to reason about\ndynamic video content at a small (~8%) per-task parameter cost, requiring\napproximately 20 times fewer updated parameters compared to previous work.\nExtensive experiments on video action recognition tasks show that our\nST-Adapter can match or even outperform the strong full fine-tuning strategy\nand state-of-the-art video models, whilst enjoying the advantage of parameter\nefficiency. The code and model are available at\nhttps://github.com/linziyi96/st-adapter", "authors": ["Junting Pan", "Ziyi Lin", "Xiatian Zhu", "Jing Shao", "Hongsheng Li"], "published_date": "2022_06_27", "pdf_url": "http://arxiv.org/pdf/2206.13559v3", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td><p>Pretrain</p></td><td><p>#Frames</p></td><td><p>GFlops</p></td><td><p>Top-1</p></td><td><p>Top-5</p></td></tr><tr><th colspan=\"6\">Methods with full-finetuning</th></tr><tr><th>LGD(Qiu et al., 2019)</th><td><p>IN-1K</p></td><td><p>128\\timesN/A</p></td><td><p>N/A</p></td><td><p>79.4</p></td><td><p>94.4</p></td></tr><tr><th>SlowFast+NL(Feichtenhofer et al., 2019)</th><td><p>-</p></td><td><p>16\\times3\\times10</p></td><td><p>7020</p></td><td><p>79.8</p></td><td><p>93.9</p></td></tr><tr><th>ip-CSN(Tran et al., 2019)</th><td><p>Sports1M</p></td><td><p>32\\times3\\times10</p></td><td><p>3270</p></td><td><p>79.2</p></td><td><p>93.8</p></td></tr><tr><th>CorrNet(Wang et al., 2020a)</th><td><p>Sports1M</p></td><td><p>32\\times3\\times10</p></td><td><p>6720</p></td><td><p>81.0</p></td><td><p>-</p></td></tr><tr><th>X3D-XL(Feichtenhofer, 2020)</th><td><p>-</p></td><td><p>16\\times3\\times10</p></td><td><p>1452</p></td><td><p>79.1</p></td><td><p>93.9</p></td></tr><tr><th>MoViNet-A6(Kondratyuk et al., 2021)</th><td><p>-</p></td><td><p>120\\times1\\times1</p></td><td><p>386</p></td><td><p>81.5</p></td><td><p>95.3</p></td></tr><tr><th>ViT-B-VTN (Neimark et al., 2021)</th><td><p>IN21K</p></td><td><p>250\\times1\\times1</p></td><td><p>3992</p></td><td><p>78.6</p></td><td><p>93.7</p></td></tr><tr><th>TimeSformer-L(Bertasius et al., 2021)</th><td><p>IN21K</p></td><td><p>96\\times3\\times1</p></td><td><p>7140</p></td><td><p>80.7</p></td><td><p>94.7</p></td></tr><tr><th>STAM (Sharir et al., 2021)</th><td><p>IN21K</p></td><td><p>64\\times1\\times1</p></td><td><p>1040</p></td><td><p>79.2</p></td><td><p>-</p></td></tr><tr><th>X-ViT(Bulat et al., 2021)</th><td><p>IN21K</p></td><td><p>16\\times3\\times1</p></td><td><p>850</p></td><td><p>80.2</p></td><td><p>94.7</p></td></tr><tr><th>Mformer-HR(Patrick et al., 2021)</th><td><p>IN-21K</p></td><td><p>16\\times3\\times10</p></td><td><p>28764</p></td><td><p>81.1</p></td><td><p>95.2</p></td></tr><tr><th>MViT-B,32\\times3(Fan et al., 2021)</th><td><p>-</p></td><td><p>32\\times1\\times5</p></td><td><p>850</p></td><td><p>80.2</p></td><td><p>94.4</p></td></tr><tr><th>ViViT-L(Arnab et al., 2021)</th><td><p>JFT300M</p></td><td><p>16\\times3\\times4</p></td><td><p>17352</p></td><td><p>82.8</p></td><td><p>95.3</p></td></tr><tr><th>Swin-B(Liu et al., 2021c)</th><td><p>IN1K</p></td><td><p>32\\times3\\times4</p></td><td><p>3384</p></td><td><p>80.6</p></td><td><p>94.6</p></td></tr><tr><th>Swin-L(384)(Liu et al., 2021c)</th><td><p>IN21K</p></td><td><p>32\\times5\\times10</p></td><td><p>105350</p></td><td><p>84.9</p></td><td><p>96.7</p></td></tr><tr><th>UniFormer-B(Li et al., 2022)</th><td><p>IN1K</p></td><td><p>32\\times1\\times4</p></td><td><p>1036</p></td><td><p>82.9</p></td><td><p>95.4</p></td></tr><tr><th>VATT-Large(320)(Akbari et al., 2021)</th><td><p>HowTo100M</p></td><td><p>32\\times3\\times4</p></td><td><p>29800</p></td><td><p>82.1</p></td><td><p>95.5</p></td></tr><tr><th>TokenLearner(Ryoo et al., 2021)</th><td><p>JFT300M</p></td><td><p>64\\times3\\times4</p></td><td><p>48912</p></td><td><p>85.4</p></td><td><p>96.3</p></td></tr><tr><th>OMNIVORE(Swin-L)(Girdhar et al., 2022)</th><td>IN22K+SUN</td><td>32\\times3\\times4</td><td>7248</td><td>84.1</td><td>96.3</td></tr><tr><th>MTV-H(Yan et al., 2022)</th><td>WTS-280</td><td>32\\times3\\times4</td><td>73570</td><td>89.9</td><td>98.3</td></tr><tr><th>ViT-B w/o ST-Adapter</th><td>CLIP</td><td>8\\times3\\times1</td><td>419</td><td>81.0</td><td>95.5</td></tr><tr><th>ViT-L w/o ST-Adapter</th><td>CLIP</td><td>8\\times3\\times1</td><td>1941</td><td>85.8</td><td>97.2</td></tr><tr><th colspan=\"6\">Methods with frozen backbone</th></tr><tr><th>Our ViT-B w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>8\\times3\\times1</p></td><td><p>455</p></td><td><p>82.0</p></td><td><p>95.7</p></td></tr><tr><th>Our ViT-B w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>16\\times3\\times1</p></td><td><p>911</p></td><td><p>82.5</p></td><td><p>96.0</p></td></tr><tr><th>Our ViT-B w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>32\\times3\\times1</p></td><td><p>1821</p></td><td><p>82.7</p></td><td><p>96.2</p></td></tr><tr><th>Our ViT-L w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>8\\times3\\times1</p></td><td><p>2062</p></td><td><p>86.7</p></td><td><p>97.5</p></td></tr><tr><th>Our ViT-L w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>16\\times3\\times1</p></td><td><p>4124</p></td><td><p>86.9</p></td><td><p>97.6</p></td></tr><tr><th>Our ViT-L w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>32\\times3\\times1</p></td><td><p>8248</p></td><td><p>87.2</p></td><td><p>97.6</p></td></tr></tbody></table>", "caption": "Table 2: Results on Kinetics-400 validation set. \u201cFrames\u201d denotes the total number of frames used during inference which is: # frames per clip \\times # temporal clip \\times # spatial crop. \u201cGFlops\u201d means 10^{9} Flops.Our ViT w/ ST-Adapter achieves new state-of-the-art performances on K400 at similar GFlops.", "list_citation_info": ["Liu et al. [2021c] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021c.", "Akbari et al. [2021] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34, 2021.", "Tran et al. [2019] Du Tran, Heng Wang, L. Torresani, and Matt Feiszli. Video classification with channel-separated convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5551\u20135560, 2019.", "Sharir et al. [2021] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth? ArXiv, abs/2103.13915, 2021.", "Girdhar et al. [2022] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan Misra. Omnivore: A single model for many visual modalities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16102\u201316112, 2022.", "Bulat et al. [2021] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, Brais Martinez, and Georgios Tzimiropoulos. Space-time mixing attention for video transformer. Advances in Neural Information Processing Systems, 34, 2021.", "Yan et al. [2022] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview transformers for video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3333\u20133343, 2022.", "Patrick et al. [2021] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo\u00e3o F Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. Advances in Neural Information Processing Systems, 34, 2021.", "Bertasius et al. [2021] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095, 2021.", "Li et al. [2022] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.", "Ryoo et al. [2021] Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: Adaptive space-time tokenization for videos. Advances in Neural Information Processing Systems, 34, 2021.", "Neimark et al. [2021] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. ArXiv, abs/2102.00719, 2021.", "Qiu et al. [2019] Zhaofan Qiu, Ting Yao, C. Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal representation with local and global diffusion. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12048\u201312057, 2019.", "Kondratyuk et al. [2021] D. Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew A. Brown, and Boqing Gong. Movinets: Mobile video networks for efficient video recognition. ArXiv, abs/2103.11511, 2021.", "Feichtenhofer [2020] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.", "Arnab et al. [2021] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836\u20136846, 2021.", "Wang et al. [2020a] Heng Wang, Du Tran, L. Torresani, and Matt Feiszli. Video modeling with correlation networks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 349\u2013358, 2020a.", "Feichtenhofer et al. [2019] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6202\u20136211, 2019.", "Fan et al. [2021] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227, 2021."]}, {"table": "<table><tbody><tr><th>Model</th><td><p>Pretrain</p></td><td><p>#Frames</p></td><td><p>GFlops</p></td><td><p>Top-1</p></td><td><p>Top-5</p></td></tr><tr><th colspan=\"6\">Methods with full-finetuning</th></tr><tr><th>TSM(Lin et al., 2019)</th><td><p>IN1K</p></td><td><p>16\\times1\\times1</p></td><td><p>66</p></td><td><p>63.3</p></td><td><p>88.5</p></td></tr><tr><th>GST(Luo and Yuille, 2019)</th><td><p>IN1K</p></td><td><p>16\\times1\\times1</p></td><td><p>59</p></td><td><p>62.6</p></td><td><p>87.9</p></td></tr><tr><th>MSNet(Kwon et al., 2020)</th><td><p>IN1K</p></td><td><p>16\\times1\\times1</p></td><td><p>101</p></td><td><p>64.7</p></td><td><p>89.4</p></td></tr><tr><th>CT-Net(Li et al., 2021a)</th><td><p>IN1K</p></td><td><p>16\\times1\\times1</p></td><td><p>75</p></td><td><p>64.5</p></td><td><p>89.3</p></td></tr><tr><th>TDN(Wang et al., 2020b)</th><td><p>IN1K</p></td><td><p>16\\times1\\times1</p></td><td><p>72</p></td><td><p>65.3</p></td><td><p>89.5</p></td></tr><tr><th>TimeSformer-HR(Bertasius et al., 2021)</th><td><p>IN21K</p></td><td><p>16\\times3\\times1</p></td><td><p>5109</p></td><td><p>62.5</p></td><td><p>-</p></td></tr><tr><th>X-ViT(Bulat et al., 2021)</th><td><p>IN21K</p></td><td><p>32\\times3\\times1</p></td><td><p>1270</p></td><td><p>65.4</p></td><td><p>90.7</p></td></tr><tr><th>Mformer-L(Patrick et al., 2021)</th><td><p>IN21K+K400</p></td><td><p>32\\times3\\times1</p></td><td><p>3555</p></td><td><p>68.1</p></td><td><p>91.2</p></td></tr><tr><th>ViViT-L(Arnab et al., 2021)</th><td><p>IN21K+K400</p></td><td><p>16\\times3\\times4</p></td><td><p>11892</p></td><td><p>65.4</p></td><td><p>89.8</p></td></tr><tr><th>MViT-B-24,32\\times3(Fan et al., 2021)</th><td><p>K600</p></td><td><p>32\\times1\\times3</p></td><td><p>708</p></td><td><p>68.7</p></td><td><p>91.5</p></td></tr><tr><th>Swin-B(Liu et al., 2021c)</th><td><p>IN21K+K400</p></td><td><p>32\\times3\\times1</p></td><td><p>963</p></td><td><p>69.6</p></td><td><p>92.7</p></td></tr><tr><th>UniFormer-B(Li et al., 2022)</th><td><p>IN1K+K600</p></td><td><p>32\\times3\\times1</p></td><td><p>777</p></td><td><p>71.2</p></td><td><p>92.8</p></td></tr><tr><th>OMNIVORE (Swin-B)(Girdhar et al., 2022)</th><td>IN22K+K400+SUN</td><td>32\\times3\\times1</td><td>963</td><td>71.4</td><td>93.5</td></tr><tr><th>MTV-B(320p)(Yan et al., 2022)</th><td>IN21K+K400</td><td>32\\times3\\times4</td><td>11160</td><td>68.5</td><td>90.4</td></tr><tr><th>ViT-B w/o ST-Adapter</th><td>CLIP</td><td>8\\times3\\times1</td><td>419</td><td>44.0</td><td>77.0</td></tr><tr><th>ViT-L w/o ST-Adapter</th><td>CLIP</td><td>8\\times3\\times1</td><td>1941</td><td>48.7</td><td>77.5</td></tr><tr><th colspan=\"6\">Methods with frozen backbone</th></tr><tr><th>Our ViT-B w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>8\\times3\\times1</p></td><td><p>489</p></td><td><p>67.1</p></td><td><p>91.2</p></td></tr><tr><th>Our ViT-B w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>16\\times3\\times1</p></td><td><p>977</p></td><td><p>69.3</p></td><td><p>92.3</p></td></tr><tr><th>Our ViT-B w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>32\\times3\\times1</p></td><td><p>1955</p></td><td><p>69.5</p></td><td><p>92.6</p></td></tr><tr><th>Our ViT-L w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>8\\times3\\times1</p></td><td><p>2062</p></td><td><p>70.0</p></td><td><p>92.3</p></td></tr><tr><th>Our ViT-L w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>16\\times3\\times1</p></td><td><p>4124</p></td><td><p>71.9</p></td><td><p>93.4</p></td></tr><tr><th>Our ViT-L w/ ST-Adapter</th><td><p>CLIP</p></td><td><p>32\\times3\\times1</p></td><td><p>8248</p></td><td>72.3</td><td>93.9</td></tr></tbody></table>", "caption": "Table 3: Results on Something-Something-v2 validation set. \u201cFrames\u201d denotes the total number of frames used during inference which is: # frames per clip \\times # temporal clip \\times # spatial crop. \u201cGFlops\u201d means 10^{9} Flops. Our ViT w/ ST-Adapter outperforms most of the current methods by only fine-tuning a very small set of parameters. Here the ViT-B w/ ST-Adapter result is reported using 2 ST-Adapters per block.", "list_citation_info": ["Lin et al. [2019] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.", "Wang et al. [2020b] Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efficient action recognition. ArXiv, abs/2012.10071, 2020b.", "Liu et al. [2021c] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021c.", "Kwon et al. [2020] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion feature learning for video understanding. In ECCV, 2020.", "Bulat et al. [2021] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, Brais Martinez, and Georgios Tzimiropoulos. Space-time mixing attention for video transformer. Advances in Neural Information Processing Systems, 34, 2021.", "Girdhar et al. [2022] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan Misra. Omnivore: A single model for many visual modalities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16102\u201316112, 2022.", "Patrick et al. [2021] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo\u00e3o F Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. Advances in Neural Information Processing Systems, 34, 2021.", "Bertasius et al. [2021] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095, 2021.", "Yan et al. [2022] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview transformers for video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3333\u20133343, 2022.", "Li et al. [2021a] Kunchang Li, Xianhang Li, Yali Wang, Jun Wang, and Y. Qiao. Ct-net: Channel tensorization network for video classification. ArXiv, abs/2106.01603, 2021a.", "Fan et al. [2021] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227, 2021.", "Li et al. [2022] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.", "Luo and Yuille [2019] Chenxu Luo and Alan L. Yuille. Grouped spatial-temporal aggregation for efficient action recognition. 2019 IEEE International Conference on Computer Vision (ICCV), pages 5511\u20135520, 2019.", "Arnab et al. [2021] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836\u20136846, 2021."]}, {"table": "<table><tbody><tr><td>Model</td><td>Pre-train data</td><td>#Frames</td><td>Verb</td><td>Noun</td></tr><tr><td colspan=\"4\">Methods with full-finetuning</td><td></td></tr><tr><td>ViViT-L Arnab et al. (2021)</td><td>IN21K+K400</td><td>16 \\times 3 \\times 10</td><td>66.4</td><td>56.8</td></tr><tr><td>MFormer-B Patrick et al. (2021)</td><td>IN21K+K400</td><td>16 \\times 3 \\times 10</td><td>66.7</td><td>56.5</td></tr><tr><td>XViT(8x) Bulat et al. (2021)</td><td>IN21K+K400</td><td>8 \\times 3 \\times 1</td><td>66.7</td><td>53.3</td></tr><tr><td>ViT-B/16 w/o ST-Adapter</td><td>CLIP</td><td>8 \\times 3 \\times 1</td><td>54.8</td><td>50.4</td></tr><tr><td colspan=\"4\">Methods with frozen backbone</td><td></td></tr><tr><td>Our ViT-B/16 w/ ST-Adapter</td><td>CLIP</td><td>8 \\times 3 \\times 1</td><td>67.6</td><td>55.0</td></tr></tbody></table>", "caption": "Table 4: Results on Epic-Kitchens-100 validation set. \u201cFrames\u201d denotes the total number of frames used during inference which is: # frames per clip \\times # temporal clip \\times # spatial crop.", "list_citation_info": ["Bulat et al. [2021] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, Brais Martinez, and Georgios Tzimiropoulos. Space-time mixing attention for video transformer. Advances in Neural Information Processing Systems, 34, 2021.", "Patrick et al. [2021] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo\u00e3o F Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. Advances in Neural Information Processing Systems, 34, 2021.", "Arnab et al. [2021] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836\u20136846, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><td>Pre-train data</td><td>UCF101</td><td>HMDB51</td></tr><tr><th>STC Diba et al. (2018)</th><td>K400</td><td>95.8</td><td>72.6</td></tr><tr><th>ECO Zolfaghari et al. (2018)</th><td>K400</td><td>93.6</td><td>68.4</td></tr><tr><th>R(2+1)D-34 Tran et al. (2018a)</th><td>K400</td><td>96.8</td><td>74.5</td></tr><tr><th>I3D Carreira and Zisserman (2017)</th><td>ImageNet+K400</td><td>95.6</td><td>74.8</td></tr><tr><th>S3D Xie et al. (2018)</th><td>ImageNet+K400</td><td>96.8</td><td>75.9</td></tr><tr><th>FASTER32 Zhu et al. (2020)</th><td>K400</td><td>96.9</td><td>75.7</td></tr><tr><th>VideoPrompt Ju et al. (2021)</th><td>CLIP</td><td>93.6</td><td>66.4</td></tr><tr><th>SlowOnly-8x8-R101 Duan et al. (2020)</th><td>Kinetics+OmniSourceDuan et al. (2020)</td><td>97.3</td><td>79.0</td></tr><tr><th>ViT-B/16 w/ ST-Adapter (Ours)</th><td>CLIP+K400</td><td>96.4</td><td>77.7</td></tr><tr><th>ViT-L/14 w/ ST-Adapter (Ours)</th><td>CLIP+K400</td><td>98.1</td><td>81.7</td></tr><tr><th>ViT-L/14@336px w/ ST-Adapter (Ours)</th><td>CLIP+K400</td><td>98.3</td><td>82.8</td></tr></tbody></table>", "caption": "Table 12: Comparing the state-of-the-art video recognition methods on UCF101 and HMDB51.", "list_citation_info": ["Zolfaghari et al. [2018] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient convolutional network for online video understanding. In Proceedings of the European conference on computer vision (ECCV), pages 695\u2013712, 2018.", "Carreira and Zisserman [2017] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.", "Tran et al. [2018a] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450\u20136459, 2018a.", "Ju et al. [2021] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. arXiv preprint arXiv:2112.04478, 2021.", "Duan et al. [2020] Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, and Dahua Lin. Omni-sourced webly-supervised learning for video recognition. In European Conference on Computer Vision, pages 670\u2013688. Springer, 2020.", "Xie et al. [2018] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European conference on computer vision (ECCV), pages 305\u2013321, 2018.", "Zhu et al. [2020] Linchao Zhu, Du Tran, Laura Sevilla-Lara, Yi Yang, Matt Feiszli, and Heng Wang. Faster recurrent networks for efficient video classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13098\u201313105, 2020.", "Diba et al. [2018] Ali Diba, Mohsen Fayyaz, Vivek Sharma, M Mahdi Arzani, Rahman Yousefzadeh, Juergen Gall, and Luc Van Gool. Spatio-temporal channel correlation networks for action classification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 284\u2013299, 2018."]}], "citation_info_to_title": {"Akbari et al. [2021] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34, 2021.": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text", "Neimark et al. [2021] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. ArXiv, abs/2102.00719, 2021.": "Video Transformer Network", "Zhu et al. [2020] Linchao Zhu, Du Tran, Laura Sevilla-Lara, Yi Yang, Matt Feiszli, and Heng Wang. Faster recurrent networks for efficient video classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13098\u201313105, 2020.": "Faster recurrent networks for efficient video classification", "Girdhar et al. [2022] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan Misra. Omnivore: A single model for many visual modalities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16102\u201316112, 2022.": "Omnivore: A single model for many visual modalities", "Kwon et al. [2020] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion feature learning for video understanding. In ECCV, 2020.": "Motionsqueeze: Neural motion feature learning for video understanding", "Patrick et al. [2021] Mandela Patrick, Dylan Campbell, Yuki Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo\u00e3o F Henriques. Keeping your eye on the ball: Trajectory attention in video transformers. Advances in Neural Information Processing Systems, 34, 2021.": "Keeping your eye on the ball: Trajectory attention in video transformers", "Diba et al. [2018] Ali Diba, Mohsen Fayyaz, Vivek Sharma, M Mahdi Arzani, Rahman Yousefzadeh, Juergen Gall, and Luc Van Gool. Spatio-temporal channel correlation networks for action classification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 284\u2013299, 2018.": "Spatio-temporal channel correlation networks for action classification", "Wang et al. [2020a] Heng Wang, Du Tran, L. Torresani, and Matt Feiszli. Video modeling with correlation networks. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 349\u2013358, 2020a.": "Video modeling with correlation networks", "Wang et al. [2020b] Limin Wang, Zhan Tong, Bin Ji, and Gangshan Wu. Tdn: Temporal difference networks for efficient action recognition. ArXiv, abs/2012.10071, 2020b.": "TDN: Temporal Difference Networks for Efficient Action Recognition", "Lin et al. [2019] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.": "Tsm: Temporal shift module for efficient video understanding", "Feichtenhofer [2020] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.": "X3d: Expanding architectures for efficient video recognition", "Carreira and Zisserman [2017] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.": "Quo vadis, action recognition? a new model and the kinetics dataset", "Luo and Yuille [2019] Chenxu Luo and Alan L. Yuille. Grouped spatial-temporal aggregation for efficient action recognition. 2019 IEEE International Conference on Computer Vision (ICCV), pages 5511\u20135520, 2019.": "Grouped spatial-temporal aggregation for efficient action recognition", "Duan et al. [2020] Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, and Dahua Lin. Omni-sourced webly-supervised learning for video recognition. In European Conference on Computer Vision, pages 670\u2013688. Springer, 2020.": "Omni-sourced webly-supervised learning for video recognition", "Qiu et al. [2019] Zhaofan Qiu, Ting Yao, C. Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal representation with local and global diffusion. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12048\u201312057, 2019.": "Learning spatio-temporal representation with local and global diffusion", "Kondratyuk et al. [2021] D. Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew A. Brown, and Boqing Gong. Movinets: Mobile video networks for efficient video recognition. ArXiv, abs/2103.11511, 2021.": "Movinets: Mobile video networks for efficient video recognition", "Fan et al. [2021] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227, 2021.": "Multiscale Vision Transformers", "Arnab et al. [2021] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836\u20136846, 2021.": "Vivit: A video vision transformer", "Xie et al. [2018] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European conference on computer vision (ECCV), pages 305\u2013321, 2018.": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification", "Bulat et al. [2021] Adrian Bulat, Juan Manuel Perez Rua, Swathikiran Sudhakaran, Brais Martinez, and Georgios Tzimiropoulos. Space-time mixing attention for video transformer. Advances in Neural Information Processing Systems, 34, 2021.": "Space-time mixing attention for video transformer", "Feichtenhofer et al. [2019] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6202\u20136211, 2019.": "Slowfast networks for video recognition", "Bertasius et al. [2021] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095, 2021.": "Is space-time attention all you need for video understanding?", "Tran et al. [2018a] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450\u20136459, 2018a.": "A closer look at spatiotemporal convolutions for action recognition", "Tran et al. [2019] Du Tran, Heng Wang, L. Torresani, and Matt Feiszli. Video classification with channel-separated convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5551\u20135560, 2019.": "Video classification with channel-separated convolutional networks", "Zolfaghari et al. [2018] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient convolutional network for online video understanding. In Proceedings of the European conference on computer vision (ECCV), pages 695\u2013712, 2018.": "Eco: Efficient Convolutional Network for Online Video Understanding", "Liu et al. [2021c] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021c.": "Video Swin Transformer", "Li et al. [2022] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.": "Uniformer: Unifying Convolution and Self-Attention for Visual Recognition", "Li et al. [2021a] Kunchang Li, Xianhang Li, Yali Wang, Jun Wang, and Y. Qiao. Ct-net: Channel tensorization network for video classification. ArXiv, abs/2106.01603, 2021a.": "Ct-net: Channel tensorization network for video classification", "Ryoo et al. [2021] Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: Adaptive space-time tokenization for videos. Advances in Neural Information Processing Systems, 34, 2021.": "Tokenlearner: Adaptive Space-Time Tokenization for Videos", "Ju et al. [2021] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. arXiv preprint arXiv:2112.04478, 2021.": "Prompting visual-language models for efficient video understanding", "Yan et al. [2022] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview transformers for video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3333\u20133343, 2022.": "Multiview Transformers for Video Recognition", "Sharir et al. [2021] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth? ArXiv, abs/2103.13915, 2021.": "An image is worth 16x16 words, what is a video worth?"}, "source_title_to_arxiv_id": {"TDN: Temporal Difference Networks for Efficient Action Recognition": "2012.10071", "Video Swin Transformer": "2106.13230", "Multiview Transformers for Video Recognition": "2201.04288"}}