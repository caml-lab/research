{"title": "Exploiting More Information in Sparse Point Cloud for 3D Single Object Tracking", "abstract": "3D single object tracking is a key task in 3D computer vision. However, the\nsparsity of point clouds makes it difficult to compute the similarity and\nlocate the object, posing big challenges to the 3D tracker. Previous works\ntried to solve the problem and improved the tracking performance in some common\nscenarios, but they usually failed in some extreme sparse scenarios, such as\nfor tracking objects at long distances or partially occluded. To address the\nabove problems, in this letter, we propose a sparse-to-dense and\ntransformer-based framework for 3D single object tracking. First, we transform\nthe 3D sparse points into 3D pillars and then compress them into 2D BEV\nfeatures to have a dense representation. Then, we propose an attention-based\nencoder to achieve global similarity computation between template and search\nbranches, which could alleviate the influence of sparsity. Meanwhile, the\nencoder applies the attention on multi-scale features to compensate for the\nlack of information caused by the sparsity of point cloud and the single scale\nof features. Finally, we use set-prediction to track the object through a\ntwo-stage decoder which also utilizes attention. Extensive experiments show\nthat our method achieves very promising results on the KITTI and NuScenes\ndatasets.", "authors": ["Yubo Cui", "Jiayao Shan", "Zuoxu Gu", "Zhiheng Li", "Zheng Fang"], "published_date": "2022_10_02", "pdf_url": "http://arxiv.org/pdf/2210.00519v1", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Paradigm</th><th colspan=\"2\">Car-64159</th><th colspan=\"2\">Pedestrian-33227</th><th colspan=\"2\">Truck-13587</th><th colspan=\"2\">Trailer-3352</th><th colspan=\"2\">Bus-2953</th><th colspan=\"2\">Mean-117278</th></tr><tr><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th></tr><tr><th>M^{2}-Tracker [26]</th><th>Motion</th><th>55.85</th><th>65.09</th><th>32.10</th><th>60.92</th><th>57.36</th><th>59.54</th><th>57.61</th><th>58.26</th><th>51.39</th><th>51.44</th><th>49.23</th><th>62.73</th></tr></thead><tbody><tr><th>SC3D [7]</th><th rowspan=\"4\">Similarity</th><td>22.31</td><td>21.93</td><td>11.29</td><td>12.65</td><td>30.67</td><td>27.73</td><td>35.28</td><td>28.12</td><td>29.35</td><td>24.08</td><td>20.70</td><td>20.20</td></tr><tr><th>P2B [8]</th><td>38.81</td><td>43.18</td><td>28.39</td><td>52.24</td><td>42.95</td><td>41.59</td><td>48.96</td><td>40.05</td><td>32.95</td><td>27.41</td><td>36.48</td><td>45.08</td></tr><tr><th>BAT [9]</th><td>40.73</td><td>43.29</td><td>28.83</td><td>53.32</td><td>45.34</td><td>42.58</td><td>52.59</td><td>44.89</td><td>35.44</td><td>28.01</td><td>38.10</td><td>45.71</td></tr><tr><th>SMAT (Ours)</th><td>43.51</td><td>49.04</td><td>32.27</td><td>60.28</td><td>44.78</td><td>44.69</td><td>37.45</td><td>34.10</td><td>39.42</td><td>34.32</td><td>40.20</td><td>50.92</td></tr></tbody></table>", "caption": "TABLE I: Performance comparison on the NuScenes dataset. The best two results are highlighted in red, blue", "list_citation_info": ["[8] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d in CVPR, 2020, pp. 6329\u20136338.", "[9] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, \u201cBox-aware feature enhancement for single object tracking on point clouds,\u201d in ICCV, 2021, pp. 13\u2009199\u201313\u2009208.", "[7] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in CVPR, 2019, pp. 1359\u20131368.", "[26] C. Zheng, X. Yan, H. Zhang, B. Wang, S. Cheng, S. Cui, and Z. Li, \u201cBeyond 3d siamese tracking: A motion-centric paradigm for 3d single object tracking in point clouds,\u201d in CVPR, 2022, pp. 8111\u20138120."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Paradigm</th><th colspan=\"2\">Car-6424</th><th colspan=\"2\">Pedestrian-6088</th><th colspan=\"2\">Van-1248</th><th colspan=\"2\">Cyclist-308</th><th colspan=\"2\">Mean-14068</th></tr><tr><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th><th>Success</th><th>Precision</th></tr><tr><th>M^{2}-Tracker [26]</th><th>Motion</th><th>65.5</th><th>80.8</th><th>61.5</th><th>88.2</th><th>53.8</th><th>70.7</th><th>73.2</th><th>93.5</th><th>62.9</th><th>83.4</th></tr></thead><tbody><tr><th>SC3D [7]</th><th rowspan=\"9\">Similarity</th><td>41.3</td><td>57.9</td><td>18.2</td><td>37.8</td><td>40.4</td><td>47.0</td><td>41.5</td><td>70.4</td><td>31.2</td><td>48.5</td></tr><tr><th>SC3D-RPN [43]</th><td>36.3</td><td>51.0</td><td>17.9</td><td>37.8</td><td>-</td><td>-</td><td>43.2</td><td>81.2</td><td>-</td><td>-</td></tr><tr><th>P2B [8]</th><td>56.2</td><td>72.8</td><td>28.7</td><td>49.6</td><td>40.8</td><td>48.4</td><td>32.1</td><td>44.7</td><td>42.4</td><td>60.0</td></tr><tr><th>PTT [10]</th><td>67.8</td><td>81.8</td><td>44.9</td><td>72.0</td><td>43.6</td><td>52.5</td><td>37.2</td><td>47.3</td><td>55.1</td><td>74.2</td></tr><tr><th>BAT [9]</th><td>60.5</td><td>77.7</td><td>42.1</td><td>70.1</td><td>52.4</td><td>67.0</td><td>33.7</td><td>45.4</td><td>51.2</td><td>72.8</td></tr><tr><th>LTTR [12]</th><td>65.0</td><td>77.1</td><td>33.2</td><td>56.8</td><td>35.8</td><td>45.6</td><td>66.2</td><td>89.9</td><td>48.7</td><td>65.8</td></tr><tr><th>V2B [11]</th><td>70.5</td><td>81.3</td><td>48.3</td><td>73.5</td><td>50.1</td><td>58.0</td><td>40.8</td><td>49.7</td><td>58.4</td><td>75.2</td></tr><tr><th>PTTR [25]</th><td>65.2</td><td>77.4</td><td>50.9</td><td>81.6</td><td>52.5</td><td>61.8</td><td>65.1</td><td>90.5</td><td>57.9</td><td>78.1</td></tr><tr><th>SMAT (Ours)</th><td>71.9</td><td>82.4</td><td>52.1</td><td>81.5</td><td>41.4</td><td>53.2</td><td>61.2</td><td>87.3</td><td>60.4</td><td>79.5</td></tr></tbody></table>", "caption": "TABLE II: Performance comparison on the KITTI dataset. The best two results are highlighted in red, blue.", "list_citation_info": ["[7] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in CVPR, 2019, pp. 1359\u20131368.", "[11] L. Hui, L. Wang, M. Cheng, J. Xie, and J. Yang, \u201c3d siamese voxel-to-bev tracker for sparse point clouds,\u201d in NeurIPS, vol. 34, 2021, pp. 28\u2009714\u201328\u2009727.", "[25] C. Zhou, Z. Luo, Y. Luo, T. Liu, L. Pan, Z. Cai, H. Zhao, and S. Lu, \u201cPttr: Relational 3d point cloud object tracking with transformer,\u201d in CVPR, 2022, pp. 8531\u20138540.", "[8] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d in CVPR, 2020, pp. 6329\u20136338.", "[9] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, \u201cBox-aware feature enhancement for single object tracking on point clouds,\u201d in ICCV, 2021, pp. 13\u2009199\u201313\u2009208.", "[26] C. Zheng, X. Yan, H. Zhang, B. Wang, S. Cheng, S. Cui, and Z. Li, \u201cBeyond 3d siamese tracking: A motion-centric paradigm for 3d single object tracking in point clouds,\u201d in CVPR, 2022, pp. 8111\u20138120.", "[12] Y. Cui, Z. Fang, J. Shan, Z. Gu, and S. Zhou, \u201c3d object tracking with transformer,\u201d in 32nd BMVC, 2021, p. 317.", "[43] J. Zarzar, S. Giancola, and B. Ghanem, \u201cEfficient bird eye view proposals for 3d siamese tracking,\u201d ArXiv, vol. abs/1903.10168, 2019.", "[10] J. Shan, S. Zhou, Z. Fang, and Y. Cui, \u201cPtt: Point-track-transformer module for 3d single object tracking in point clouds,\u201d in IROS, 2021, pp. 1310\u20131316."]}, {"table": "<table><thead><tr><th>Method</th><th>Params</th><th>FLOPs</th><th>Success</th><th>Precision</th></tr></thead><tbody><tr><th>DDETR [35]</th><td>9.4M</td><td>31.7G</td><td>70.9</td><td>81.4</td></tr><tr><th>Ours</th><td>10.1M</td><td>28.2G</td><td>71.9</td><td>82.4</td></tr></tbody></table>", "caption": "TABLE VI: Comparison with the decoder of Deformable-DETR.", "list_citation_info": ["[35] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable detr: Deformable transformers for end-to-end object detection,\u201d in ICLR, 2021."]}, {"table": "<table><thead><tr><th></th><th rowspan=\"2\">Method</th><th colspan=\"4\">Source of template</th></tr><tr><th></th><th>F</th><th>P</th><th>F&amp;P</th><th>AP</th></tr></thead><tbody><tr><th rowspan=\"6\">Success</th><th>SC3D [7]</th><td>31.6</td><td>25.7</td><td>34.9</td><td>41.3</td></tr><tr><th>P2B [8]</th><td>46.7</td><td>53.1</td><td>56.2</td><td>51.4</td></tr><tr><th>BAT [9]</th><td>51.8</td><td>59.2</td><td>60.5</td><td>55.8</td></tr><tr><th>PTT [10]</th><td>62.9</td><td>64.9</td><td>67.8</td><td>59.8</td></tr><tr><th>V2B [11]</th><td>67.8</td><td>70.0</td><td>70.5</td><td>69.8</td></tr><tr><th>SMAT (Ours)</th><td>68.1</td><td>66.7</td><td>71.9</td><td>69.9</td></tr><tr><th rowspan=\"6\">Precision</th><th>SC3D [7]</th><td>44.4</td><td>35.1</td><td>49.8</td><td>57.9</td></tr><tr><th>P2B [8]</th><td>59.7</td><td>68.9</td><td>72.8</td><td>66.8</td></tr><tr><th>BAT [9]</th><td>65.5</td><td>75.6</td><td>77.7</td><td>71.4</td></tr><tr><th>PTT [10]</th><td>76.5</td><td>77.5</td><td>81.8</td><td>74.5</td></tr><tr><th>V2B [11]</th><td>79.3</td><td>81.3</td><td>81.3</td><td>81.2</td></tr><tr><th>SMAT (Ours)</th><td>77.5</td><td>76.1</td><td>82.4</td><td>79.8</td></tr></tbody></table>", "caption": "TABLE VII: Different template generations. \u201cF\", \u201cP\" and \u201cAP\" denotes the first ground-truth, the previous results and all previous results respectively. The default setting is \u201cF&amp;P\u201d. ", "list_citation_info": ["[7] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in CVPR, 2019, pp. 1359\u20131368.", "[11] L. Hui, L. Wang, M. Cheng, J. Xie, and J. Yang, \u201c3d siamese voxel-to-bev tracker for sparse point clouds,\u201d in NeurIPS, vol. 34, 2021, pp. 28\u2009714\u201328\u2009727.", "[8] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d in CVPR, 2020, pp. 6329\u20136338.", "[9] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, \u201cBox-aware feature enhancement for single object tracking on point clouds,\u201d in ICCV, 2021, pp. 13\u2009199\u201313\u2009208.", "[10] J. Shan, S. Zhou, Z. Fang, and Y. Cui, \u201cPtt: Point-track-transformer module for 3d single object tracking in point clouds,\u201d in IROS, 2021, pp. 1310\u20131316."]}], "citation_info_to_title": {"[7] S. Giancola, J. Zarzar, and B. Ghanem, \u201cLeveraging shape completion for 3d siamese tracking,\u201d in CVPR, 2019, pp. 1359\u20131368.": "Leveraging shape completion for 3d siamese tracking", "[35] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable detr: Deformable transformers for end-to-end object detection,\u201d in ICLR, 2021.": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "[9] C. Zheng, X. Yan, J. Gao, W. Zhao, W. Zhang, Z. Li, and S. Cui, \u201cBox-aware feature enhancement for single object tracking on point clouds,\u201d in ICCV, 2021, pp. 13\u2009199\u201313\u2009208.": "Box-aware feature enhancement for single object tracking on point clouds", "[26] C. Zheng, X. Yan, H. Zhang, B. Wang, S. Cheng, S. Cui, and Z. Li, \u201cBeyond 3d siamese tracking: A motion-centric paradigm for 3d single object tracking in point clouds,\u201d in CVPR, 2022, pp. 8111\u20138120.": "Beyond 3D Siamese Tracking: A Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds", "[11] L. Hui, L. Wang, M. Cheng, J. Xie, and J. Yang, \u201c3d siamese voxel-to-bev tracker for sparse point clouds,\u201d in NeurIPS, vol. 34, 2021, pp. 28\u2009714\u201328\u2009727.": "3D Siamese Voxel-to-BEV Tracker for Sparse Point Clouds", "[8] H. Qi, C. Feng, Z. Cao, F. Zhao, and Y. Xiao, \u201cP2b: Point-to-box network for 3d object tracking in point clouds,\u201d in CVPR, 2020, pp. 6329\u20136338.": "P2b: Point-to-box network for 3d object tracking in point clouds", "[43] J. Zarzar, S. Giancola, and B. Ghanem, \u201cEfficient bird eye view proposals for 3d siamese tracking,\u201d ArXiv, vol. abs/1903.10168, 2019.": "Efficient bird eye view proposals for 3D Siamese tracking", "[12] Y. Cui, Z. Fang, J. Shan, Z. Gu, and S. Zhou, \u201c3d object tracking with transformer,\u201d in 32nd BMVC, 2021, p. 317.": "3d object tracking with transformer", "[25] C. Zhou, Z. Luo, Y. Luo, T. Liu, L. Pan, Z. Cai, H. Zhao, and S. Lu, \u201cPttr: Relational 3d point cloud object tracking with transformer,\u201d in CVPR, 2022, pp. 8531\u20138540.": "Pttr: Relational 3d point cloud object tracking with transformer", "[10] J. Shan, S. Zhou, Z. Fang, and Y. Cui, \u201cPtt: Point-track-transformer module for 3d single object tracking in point clouds,\u201d in IROS, 2021, pp. 1310\u20131316.": "Ptt: Point-track-transformer module for 3d single object tracking in point clouds"}, "source_title_to_arxiv_id": {"3d object tracking with transformer": "2108.06455", "Ptt: Point-track-transformer module for 3d single object tracking in point clouds": "2108.06455"}}