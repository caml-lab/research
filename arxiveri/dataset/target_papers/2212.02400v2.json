{"title": "Location-Aware Self-Supervised Transformers for Semantic Segmentation", "abstract": "Pixel-level labels are particularly expensive to acquire. Hence, pretraining\nis a critical step to improve models on a task like semantic segmentation.\nHowever, prominent algorithms for pretraining neural networks use image-level\nobjectives, e.g. image classification, image-text alignment a la CLIP, or\nself-supervised contrastive learning. These objectives do not model spatial\ninformation, which might be sub-optimal when finetuning on downstream tasks\nwith spatial reasoning. In this work, we pretrain network with a location-aware\n(LOCA) self-supervised method which fosters the emergence of strong dense\nfeatures. Specifically, we use both a patch-level clustering scheme to mine\ndense pseudo-labels and a relative location prediction task to encourage\nlearning about object parts and their spatial arrangements. Our experiments\nshow that LOCA pretraining leads to representations that transfer competitively\nto challenging and diverse semantic segmentation datasets.", "authors": ["Mathilde Caron", "Neil Houlsby", "Cordelia Schmid"], "published_date": "2022_12_05", "pdf_url": "http://arxiv.org/pdf/2212.02400v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>Supervised</td><td>ADE20k</td><td>P.Context</td><td>Cityscapes</td></tr><tr><th colspan=\"3\">ImageNet-1k / ViT-Base</th><td></td><td></td></tr><tr><th>Random init.</th><td></td><td>21.1</td><td>19.6</td><td>51.4</td></tr><tr><th>DINO [11]</th><td></td><td>44.1</td><td>50.7</td><td>78.4</td></tr><tr><th>MoCo-v3 [15]</th><td></td><td>45.4</td><td>51.6</td><td>78.6</td></tr><tr><th>MAE [31]</th><td></td><td>45.5</td><td>51.7</td><td>79.7</td></tr><tr><th>DeiT [58, 56]</th><td>\u2713</td><td>47.1</td><td>\u2013</td><td>\u2013</td></tr><tr><th>DeiT-III [59]</th><td>\u2713</td><td>47.3</td><td>53.9</td><td>79.7</td></tr><tr><th>iBOT [74]</th><td></td><td>47.0</td><td>54.6</td><td>79.8</td></tr><tr><th>LOCA (Ours)</th><td></td><td>47.9</td><td>54.9</td><td>79.8</td></tr><tr><th colspan=\"3\">ImageNet-21k / ViT-Large</th><td></td><td></td></tr><tr><th>Augreg [55, 56]</th><td>\u2713</td><td>50.7</td><td>  56.5*</td><td>  80.7*</td></tr><tr><th>LOCA (Ours)</th><td></td><td>52.3</td><td>60.3</td><td>81.5</td></tr></tbody></table>", "caption": "Table 2: Comparison with previous results on semantic segmentation.We report mean IoU for semantic segmentation on the validation set of ADE20k [73], Cityscapes [19] and Pascal Context (\u201cP.Context\u201d) [44] for different self-supervised and supervised pretraining methods.We consider two settings:(i) pretraining on ImageNet-1k with ViT-Base and (ii) pretraining on ImageNet-21k with ViT-Large.Networks are then end-to-end finetuned on the semantic segmentation train set.We follow the experimental setup of Segmenter [56] for end-to-end finetuning of ViT with linear decoder.We report their number for the supervised checkpoints \u201cDeiT\u201d and \u201cAugreg\u201d.Results for the self-supervised methods and for DeiT-III are run by us from official released checkpoints.We report the average over 5 runs with single-scale mode (*: numbers with multi-scale evaluation).", "list_citation_info": ["[44] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014.", "[73] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.", "[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021.", "[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. preprint arXiv:2012.12877, 2020.", "[59] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.", "[74] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.", "[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "[56] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In ICCV, 2021.", "[55] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021.", "[19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.", "[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>1/32</th><th>1/16</th><th>1/8</th><th>1/4</th><th>1/2</th></tr></thead><tbody><tr><th>DINO [11]</th><td>18.4</td><td>24.5</td><td>29.5</td><td>35.2</td><td>39.5</td></tr><tr><th>MoCo-v3 [15]</th><td>17.7</td><td>25.2</td><td>30.8</td><td>36.5</td><td>40.7</td></tr><tr><th>MAE [31]</th><td>18.4</td><td>25.3</td><td>30.5</td><td>36.1</td><td>40.6</td></tr><tr><th>Supervised - DeiT-III [59]</th><td>20.9</td><td>27.1</td><td>32.7</td><td>38.3</td><td>42.0</td></tr><tr><th>iBOT [74]</th><td>20.9</td><td>28.0</td><td>33.4</td><td>38.7</td><td>42.6</td></tr><tr><th>LOCA (Ours)</th><td>22.2</td><td>30.0</td><td>34.4</td><td>39.1</td><td>42.8</td></tr></tbody></table>", "caption": "Table 3: Fewshot semantic segmentation on ADE20k.We report mean IoU for semantic segmentation on the validation set of ADE20k for different pretrained models.Only a fraction (either 1/32, 1/16, 1/8, 1/4 or 1/2) of ADE20k training images are used for finetuning.Results are averaged over 5 different splits.Architecture is ViT-B for all runs and pretraining data is ImageNet-1k.", "list_citation_info": ["[59] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.", "[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021.", "[74] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.", "[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"4\">Classification only (mAP)</td><td colspan=\"4\">Localization only (mIoU)</td><td colspan=\"4\">Both (mIoU)</td></tr><tr><th>Method</th><td>ADE20k</td><td>P. Cont.</td><td>P. VOC</td><td>Citysc.</td><td>ADE20k</td><td>P. Cont.</td><td>P. VOC</td><td>Citysc.</td><td>ADE20k</td><td>P. Cont.</td><td>P. VOC</td><td>Citysc.</td></tr><tr><th>Image-level pretrainings</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>DINO [11]</th><td>61.6</td><td>67.7</td><td>89.9</td><td>81.5</td><td>64.5</td><td>71.6</td><td>78.7</td><td>79.6</td><td>44.1</td><td>50.7</td><td>74.7</td><td>78.4</td></tr><tr><th>MoCo-v3 [15]</th><td>61.1</td><td>69.3</td><td>93.6</td><td>82.1</td><td>66.2</td><td>73.7</td><td>79.0</td><td>79.9</td><td>45.4</td><td>51.6</td><td>74.3</td><td>78.6</td></tr><tr><th>Supervised (DeiT-III [59])</th><td>64.8</td><td>71.5</td><td>94.6</td><td>84.0</td><td>66.5</td><td>73.6</td><td>80.1</td><td>80.7</td><td>47.3</td><td>53.9</td><td>76.3</td><td>79.7</td></tr><tr><th>Spatially-aware pretrainings</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>MAE [31]</th><td>59.0</td><td>67.6</td><td>92.8</td><td>84.3</td><td>67.0</td><td>74.3</td><td>79.9</td><td>81.1</td><td>45.5</td><td>51.7</td><td>75.4</td><td>79.7</td></tr><tr><th>LOCA (Ours)</th><td>62.2</td><td>69.9</td><td>93.7</td><td>83.6</td><td>67.9</td><td>75.4</td><td>80.5</td><td>81.4</td><td>47.9</td><td>54.9</td><td>76.6</td><td>79.8</td></tr></tbody></table>", "caption": "Table 4: Disentangling localization and classification on semantic segmentation.We report classification only (with a multi-label classification training) and localization only (with an oracle giving the class of the segmentation masks) evaluations on 4 popular semantic segmentation benchmarks: ADE20k [73], Pascal Context (\u201cP.Cont.\u201d) [44], Pascal VOC (\u201cP.VOC\u201d) [25] and Cityscapes (\u201cCity.\u201d) [19].Details about the classification/location only protocols are in Sec. 4.2.Best number is in bold and second best is underlined.We report performance for different methods pretrained on ImageNet-1k (with or without labels) with ViT-B.LOCA yields excellent locality and good semantic understanding but is still behind image-level pretraining methods on the pure semantic axis (classification only evaluation).", "list_citation_info": ["[44] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014.", "[73] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.", "[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021.", "[59] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.", "[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "[25] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010.", "[19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.", "[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>ADE20k</th></tr><tr><th colspan=\"2\">Supervised on ImageNet-1k</th></tr></thead><tbody><tr><th>Supervised used in MAE [31]</th><td>47.4</td></tr><tr><th>DeiT-III [59]</th><td>49.3</td></tr><tr><th colspan=\"2\">Self-supervised on ImageNet-1k without labels</th></tr><tr><th>DINO [11]</th><td>47.2</td></tr><tr><th>SimMIM [68]</th><td>47.6</td></tr><tr><th>FD-DINO [63]</th><td>47.7</td></tr><tr><th>MAE [31]</th><td>48.1</td></tr><tr><th>data2vec [5]</th><td>48.2</td></tr><tr><th>iBOT [74]</th><td>48.4</td></tr><tr><th>LOCA (Ours)</th><td>48.5</td></tr></tbody></table>", "caption": "Table 6: Comparison to previous results with UperNet.We report mean IoU for semantic segmentation on the validation set of ADE20k for different self-supervised and supervised representation learning methods.All models use the popular ViT-Base/16 architecture and are pre-trained on ImageNet-1k before being end-to-end finetuned on ADE20k.Results are reported in single-scale mode.", "list_citation_info": ["[5] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022.", "[59] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.", "[74] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.", "[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "[68] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In CVPR, 2022.", "[63] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.", "[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>AP</th><th>AP50</th><th>AP75</th></tr></thead><tbody><tr><th>DINO [11]</th><td>48.5</td><td>68.9</td><td>52.8</td></tr><tr><th>MoCo-v3 [15]</th><td>48.8</td><td>69.4</td><td>53.0</td></tr><tr><th>Supervised - DeiT-III [59]</th><td>49.0</td><td>69.5</td><td>53.4</td></tr><tr><th>LOCA (Ours)</th><td>49.6</td><td>70.0</td><td>54.1</td></tr><tr><th>iBOT [74]</th><td>49.9</td><td>70.6</td><td>54.3</td></tr><tr><th>MAE [31]</th><td>52.5</td><td>71.9</td><td>57.5</td></tr></tbody></table>", "caption": "Table 8: Object detection with ViTDet.We report Average Precision for object detection with CenterNet [75] and ViTDet adaptation protocol [38] on COCO dataset [39].We follow the standard training schedule of 100 epochs.Architecture is ViT-B for all runs and pretraining data is ImageNet-1k.", "list_citation_info": ["[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021.", "[59] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.", "[74] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.", "[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.", "[38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022.", "[75] Xingyi Zhou, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. Objects as points. arXiv preprint arXiv:1904.07850, 2019.", "[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.", "[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022."]}], "citation_info_to_title": {"[15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In ICCV, 2021.": "An empirical study of training self-supervised vision transformers", "[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.": "Emerging properties in self-supervised vision transformers", "[19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.": "The title of this publication is The Cityscapes Dataset for Semantic Urban Scene Understanding", "[44] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014.": "The title of this paper is The role of context for object detection and semantic segmentation in the wild", "[39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.": "Microsoft COCO: Common Objects in Context", "[5] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022.": "Data2vec: A general framework for self-supervised learning in speech, vision and language", "[25] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010.": "The Pascal Visual Object Classes (VOC) Challenge", "[68] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In CVPR, 2022.": "Simmim: A simple framework for masked image modeling", "[59] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.": "Deit iii: Revenge of the vit", "[74] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.": "ibot: Image bert pre-training with online tokenizer", "[73] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.": "Scene Parsing through ADE20K Dataset", "[63] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.": "Contrastive learning rivals masked image modeling in fine-tuning via feature distillation", "[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022.": "Masked Autoencoders are Scalable Vision Learners", "[55] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021.": "How to Train Your ViT? Data, Augmentation, and Regularization in Vision Transformers", "[38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022.": "Exploring plain vision transformer backbones for object detection", "[56] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In ICCV, 2021.": "Segmenter: Transformer for semantic segmentation", "[75] Xingyi Zhou, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. Objects as points. arXiv preprint arXiv:1904.07850, 2019.": "Objects as Points", "[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. preprint arXiv:2012.12877, 2020.": "Training data-efficient image transformers & distillation through attention"}, "source_title_to_arxiv_id": {"An empirical study of training self-supervised vision transformers": "2104.02057", "Emerging properties in self-supervised vision transformers": "2104.14294", "Simmim: A simple framework for masked image modeling": "2111.09886", "ibot: Image bert pre-training with online tokenizer": "2111.07832", "Contrastive learning rivals masked image modeling in fine-tuning via feature distillation": "2205.14141", "Masked Autoencoders are Scalable Vision Learners": "2111.06377"}}