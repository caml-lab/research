{"title": "Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework", "abstract": "Tumor infiltration of the recurrent laryngeal nerve (RLN) is a\ncontraindication for robotic thyroidectomy and can be difficult to detect via\nstandard laryngoscopy. Ultrasound (US) is a viable alternative for RLN\ndetection due to its safety and ability to provide real-time feedback. However,\nthe tininess of the RLN, with a diameter typically less than 3mm, poses\nsignificant challenges to the accurate localization of the RLN. In this work,\nwe propose a knowledge-driven framework for RLN localization, mimicking the\nstandard approach surgeons take to identify the RLN according to its\nsurrounding organs. We construct a prior anatomical model based on the inherent\nrelative spatial relationships between organs. Through Bayesian shape alignment\n(BSA), we obtain the candidate coordinates of the center of a region of\ninterest (ROI) that encloses the RLN. The ROI allows a decreased field of view\nfor determining the refined centroid of the RLN using a dual-path\nidentification network, based on multi-scale semantic information. Experimental\nresults indicate that the proposed method achieves superior hit rates and\nsubstantially smaller distance errors compared with state-of-the-art methods.", "authors": ["Haoran Dou", "Luyi Han", "Yushuang He", "Jun Xu", "Nishant Ravikumar", "Ritse Mann", "Alejandro F. Frangi", "Pew-Thian Yap", "Yunzhi Huang"], "published_date": "2022_06_30", "pdf_url": "http://arxiv.org/pdf/2206.15254v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th rowspan=\"2\">Methods</th><td colspan=\"2\">Left RLN</td><td colspan=\"2\">Right RLN</td></tr><tr><th></th><td>Distance (pix)</td><td>Hit Rate (\\%)</td><td>Distance (pix)</td><td>Hit Rate (\\%)</td></tr><tr><th rowspan=\"3\">Coord-based</th><th>ResNet-50 [4]</th><td>10.9 \\pm 9.7</td><td>77.5</td><td>12.3 \\pm 8.4</td><td>70.0</td></tr><tr><th>SwinT-C [8]</th><td>19.7 \\pm 11.8</td><td>42.3</td><td>14.4\\pm 9.6</td><td>59.4</td></tr><tr><th>ConvNeXt-C [9]</th><td>16.5 \\pm 8.2</td><td>47.3</td><td>14.0 \\pm 9.5</td><td>62.5</td></tr><tr><th rowspan=\"4\">Heatmap-based</th><th>U-Net [12]</th><td>29.3 \\pm 12.8</td><td>11.5</td><td>20.9 \\pm 10.5</td><td>31.9</td></tr><tr><th>DeepLab [2]</th><td>17.5 \\pm 8.0</td><td>41.2</td><td>11.9 \\pm 7.1</td><td>71.3</td></tr><tr><th>SwinT-H [8]</th><td>22.7 \\pm 13.6</td><td>30.8</td><td>20.2 \\pm 10.6</td><td>35.6</td></tr><tr><th>ConvNeXt-H [9]</th><td>12.7 \\pm 12.1</td><td>73.1</td><td>13.1 \\pm 8.7</td><td>64.4</td></tr><tr><th></th><th>Proposed</th><td>3.49\\pm7.53</td><td>95.6</td><td>4.55 \\pm7.61</td><td>92.5</td></tr></tbody></table>", "caption": "Table 1: Statistics of competing methods for the testing dataset.", "list_citation_info": ["[2] Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.: Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In: Computer Vision and Pattern Recognition (2020)", "[8] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv: Computer Vision and Pattern Recognition (2021)", "[12] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234\u2013241. Springer (2015)", "[4] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Computer Vision and Pattern Recognition (2016)", "[9] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. arXiv preprint arXiv:2201.03545 (2022)"]}], "citation_info_to_title": {"[9] Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. arXiv preprint arXiv:2201.03545 (2022)": "A convnet for the 2020s", "[8] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv: Computer Vision and Pattern Recognition (2021)": "Swin transformer: Hierarchical vision transformer using shifted windows", "[2] Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.: Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In: Computer Vision and Pattern Recognition (2020)": "Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation", "[4] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Computer Vision and Pattern Recognition (2016)": "Deep Residual Learning for Image Recognition", "[12] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234\u2013241. Springer (2015)": "U-net: Convolutional networks for biomedical image segmentation"}, "source_title_to_arxiv_id": {"A convnet for the 2020s": "2201.03545", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}