{"title": "Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification", "abstract": "Recent years have seen a growth in user-centric applications that require\neffective knowledge transfer across tasks in the low-data regime. An example is\npersonalization, where a pretrained system is adapted by learning on small\namounts of labeled data belonging to a specific user. This setting requires\nhigh accuracy under low computational complexity, therefore the Pareto frontier\nof accuracy vs. adaptation cost plays a crucial role. In this paper we push\nthis Pareto frontier in the few-shot image classification setting with a key\ncontribution: a new adaptive block called Contextual Squeeze-and-Excitation\n(CaSE) that adjusts a pretrained neural network on a new task to significantly\nimprove performance with a single forward pass of the user data (context). We\nuse meta-trained CaSE blocks to conditionally adapt the body of a network and a\nfine-tuning routine to adapt a linear head, defining a method called UpperCaSE.\nUpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on\nthe 26 datasets of VTAB+MD and on a challenging real-world personalization\nbenchmark (ORBIT), narrowing the gap with leading fine-tuning methods with the\nbenefit of orders of magnitude lower adaptation cost.", "authors": ["Massimiliano Patacchiola", "John Bronskill", "Aliaksandra Shysheya", "Katja Hofmann", "Sebastian Nowozin", "Richard E. Turner"], "published_date": "2022_06_20", "pdf_url": "http://arxiv.org/pdf/2206.09843v3", "list_table_and_caption": [{"table": "<p>CostClean Video Evaluation (CLE-VE)Clutter Video Evaluation (CLU-VE)MethodMACs\\downarrowframe acc.\\uparrowFTR\\downarrowvideo acc.\\uparrowframe acc.\\uparrowFTR\\downarrowvideo acc.\\uparrowProtoNet3.259.0\\pm2.211.5\\pm1.869.2\\pm3.047.0\\pm1.820.4\\pm1.752.8\\pm2.5CNAPs3.551.9\\pm2.520.8\\pm2.360.8\\pm3.241.6\\pm1.930.7\\pm2.143.0\\pm2.5MAML95.342.5\\pm2.737.3\\pm3.047.0\\pm3.224.3\\pm1.862.3\\pm2.325.7\\pm2.2FineTuner317.761.0\\pm2.211.5\\pm1.872.6\\pm2.948.4\\pm1.919.1\\pm1.754.1\\pm2.5UpperCaSE3.563.0\\pm2.28.8\\pm1.674.4\\pm2.848.1\\pm1.818.2\\pm1.754.5\\pm2.5</p>", "caption": "Table 3: ORBIT: UpperCaSE obtains the best average-score in most metrics, being within error bars with leading methods. Average accuracy and 95% confidence interval for frames, videos, and frames to recognition (FTR). Cost: average MACs over all tasks (Teras). Results and setup from Massiceti et al., (2021): meta-train on MetaDataset and test on ORBIT, image-size 84\\times 84, ResNet18 backbone, 85 test tasks (17 test users, 5 tasks per user). Best results (within error bars) in bold.", "list_citation_info": ["Massiceti et al., (2021) Massiceti, D., Zintgraf, L., Bronskill, J., Theodorou, L., Harris, M. T., Cutrell, E., Morrison, C., Hofmann, K., and Stumpf, S. (2021). Orbit: A real-world few-shot dataset for teachable object recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision."]}, {"table": "<br/><table><thead><tr><th>Model</th><th>SE</th><th>CaSE</th><th>SE</th><th>CaSE</th></tr><tr><th>Contextual pooling</th><th>No</th><th>Yes</th><th>No</th><th>Yes</th></tr><tr><th>Adaptation head</th><th>MD</th><th>MD</th><th>Linear</th><th>Linear</th></tr><tr><th>Image size</th><th>84</th><th>84</th><th>224</th><th>224</th></tr></thead><tbody><tr><th>MetaDataset (all)</th><td>67.8</td><td>69.6</td><td>74.6</td><td>76.2</td></tr><tr><th>VTAB (all)</th><td>43.6</td><td>45.3</td><td>56.6</td><td>58.2</td></tr><tr><th>VTAB (natural)</th><td>47.5</td><td>50.2</td><td>65.3</td><td>68.1</td></tr><tr><th>VTAB (specialized)</th><td>63.6</td><td>64.9</td><td>79.8</td><td>79.6</td></tr><tr><th>VTAB (structured)</th><td>30.6</td><td>31.8</td><td>38.6</td><td>40.1</td></tr></tbody></table>", "caption": "Table 4: Comparing CaSE against standard Squeeze-and-Excitation (SE) on VTAB+MD using different adaptation heads. MD: Mahalanobis distance head (Bronskill et al.,, 2021). Linear: linear head trained with UpperCaSE. All adaptive blocks use a reduction of 32. Best results in bold.", "list_citation_info": ["Bronskill et al., (2021) Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efficient meta-learning with large images. Advances in Neural Information Processing Systems."]}, {"table": "<br/><table><thead><tr><th>Adaptation type</th><th>None</th><th>FiLM</th><th>CaSE64</th><th>CaSE32</th><th>CaSE16</th></tr><tr><th>Adaptive Params (M)</th><th>n/a</th><th>0.02</th><th>0.01</th><th>0.01</th><th>0.01</th></tr><tr><th>Amortiz. Params (M)</th><th>n/a</th><th>1.7</th><th>0.4</th><th>0.8</th><th>1.6</th></tr></thead><tbody><tr><th>MetaDataset (all)</th><td>53.4</td><td>68.4</td><td>69.8</td><td>69.6</td><td>70.4</td></tr><tr><th>VTAB (all)</th><td>43.5</td><td>44.7</td><td>46.2</td><td>45.3</td><td>46.4</td></tr><tr><th>VTAB (natural)</th><td>45.4</td><td>49.5</td><td>52.1</td><td>50.2</td><td>52.6</td></tr><tr><th>VTAB (specialized)</th><td>69.4</td><td>63.8</td><td>66.3</td><td>64.9</td><td>65.5</td></tr><tr><th>VTAB (structured)</th><td>29.1</td><td>31.7</td><td>31.8</td><td>31.8</td><td>32.1</td></tr></tbody></table>", "caption": "Table 5: Comparing CaSE adaptive blocks (with reduction 64, 32, 16) on VTAB+MD against the FiLM generators used in Bronskill et al., (2021), and a baseline with no body adaptation. CaSE blocks are more efficient in terms of adaptive and amortization parameters while providing higher classification accuracy. All models have been trained and tested on 84\\times 84 images, using a Mahalanobis distance head. Best results in bold.", "list_citation_info": ["Bronskill et al., (2021) Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efficient meta-learning with large images. Advances in Neural Information Processing Systems."]}], "citation_info_to_title": {"Massiceti et al., (2021) Massiceti, D., Zintgraf, L., Bronskill, J., Theodorou, L., Harris, M. T., Cutrell, E., Morrison, C., Hofmann, K., and Stumpf, S. (2021). Orbit: A real-world few-shot dataset for teachable object recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision.": "Orbit: A real-world few-shot dataset for teachable object recognition", "Bronskill et al., (2021) Bronskill, J., Massiceti, D., Patacchiola, M., Hofmann, K., Nowozin, S., and Turner, R. (2021). Memory efficient meta-learning with large images. Advances in Neural Information Processing Systems.": "Memory Efficient Meta-Learning with Large Images"}, "source_title_to_arxiv_id": {"Orbit: A real-world few-shot dataset for teachable object recognition": "2104.03841"}}