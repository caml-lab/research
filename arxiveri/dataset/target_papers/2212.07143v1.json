{"title": "Reproducible scaling laws for contrastive language-image learning", "abstract": "Scaling up neural networks has led to remarkable performance across a wide\nrange of tasks. Moreover, performance often follows reliable scaling laws as a\nfunction of training set size, model size, and compute, which offers valuable\nguidance as large-scale experiments are becoming increasingly expensive.\nHowever, previous work on scaling laws has primarily used private data \\&\nmodels or focused on uni-modal language or vision learning. To address these\nlimitations, we investigate scaling laws for contrastive language-image\npre-training (CLIP) with the public LAION dataset and the open-source OpenCLIP\nrepository. Our large-scale experiments involve models trained on up to two\nbillion image-text pairs and identify power law scaling for multiple downstream\ntasks including zero-shot classification, retrieval, linear probing, and\nend-to-end fine-tuning. We find that the training distribution plays a key role\nin scaling laws as the OpenAI and OpenCLIP models exhibit different scaling\nbehavior despite identical model architectures and similar training recipes. We\nopen-source our evaluation workflow and all models, including the largest\npublic CLIP models, to ensure reproducibility and make scaling laws research\nmore accessible. Source code and instructions to reproduce this study will be\navailable at https://github.com/LAION-AI/scaling-laws-openclip", "authors": ["Mehdi Cherti", "Romain Beaumont", "Ross Wightman", "Mitchell Wortsman", "Gabriel Ilharco", "Cade Gordon", "Christoph Schuhmann", "Ludwig Schmidt", "Jenia Jitsev"], "published_date": "2022_12_14", "pdf_url": "http://arxiv.org/pdf/2212.07143v1", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th>Data</th><th>Arch.</th><th>ImageNet</th><th>VTAB+</th><th>COCO</th></tr></thead><tbody><tr><th>CLIP [55]</th><th>WIT-400M</th><th>L/14</th><td>75.5</td><td>55.8</td><td>61.1</td></tr><tr><th>Ours</th><th>LAION-2B</th><th>L/14</th><td>75.2</td><td>54.6</td><td>71.1</td></tr><tr><th>Ours</th><th>LAION-2B</th><th>H/14</th><td>78.0</td><td>56.4</td><td>73.4</td></tr></tbody></table>", "caption": "Table 1: We study the scaling behavior of large CLIP models using fully open-source training code and data.All models in our investigation will be made available and include the largest public CLIP models.This table shows zero-shot performance at 224 pixel resolution, displaying accuracy on ImageNet [15], average accuracy on 35 VTAB+ datasets [65, 85], and image retrieval recall at 5 on MS-COCO image retrieval [46].", "list_citation_info": ["[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.", "[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.", "[65] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022.", "[15] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 248\u2013255, June 2009."]}, {"table": "<table><thead><tr><th>Dataset</th><th># English Img-Txt Pairs</th></tr></thead><tbody><tr><td colspan=\"2\">Public Datasets</td></tr><tr><td>LAION-400M</td><td>407M</td></tr><tr><td>LAION-2B</td><td>2.3B</td></tr><tr><td colspan=\"2\">Private Datasets</td></tr><tr><td>CLIP WIT (OpenAI)</td><td>400M</td></tr><tr><td>ALIGN</td><td>1.8B</td></tr><tr><td>BASIC</td><td>6.6B</td></tr></tbody></table>", "caption": "Table 2: Open LAION datasets used for pre-training in this study. Adapted from [65]. LAION-2B is a subset of multi-lingual LAION-5B and is more than 20 times larger than other public English image-text datasets. The scale of LAION-2B is comparable to the largest private dataset used for language-vision model training.", "list_citation_info": ["[65] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Overlap%</th></tr></thead><tbody><tr><td>ImageNet</td><td>1.02</td></tr><tr><td>ImageNet-v2</td><td>1.35</td></tr><tr><td>ImageNet-R</td><td>3.80</td></tr><tr><td>ImageNet Sketch</td><td>5.15</td></tr><tr><td>ImageNet-A</td><td>0.40</td></tr><tr><td>ObjectNet</td><td>0.10</td></tr><tr><td>CIFAR-100</td><td>0.02</td></tr><tr><td>CIFAR-10</td><td>0.03</td></tr><tr><td>MS-COCO</td><td>1.12</td></tr><tr><td>Flickr30K</td><td>1.30</td></tr></tbody></table>", "caption": "Table 3: Ratio of images (%) on downstream datasets that were detected on LAION-400M, using pHash [82].", "list_citation_info": ["[82] Christoph Zauner. Implementation and benchmarking of perceptual image hash functions. 2010."]}, {"table": "<table><thead><tr><th></th><th></th><th></th><th></th><th colspan=\"3\">ImageNet</th><th colspan=\"3\">CIFAR100</th></tr><tr><th>Model</th><th>Samples Seen</th><th>Dataset</th><th>VTAB</th><th>10 shot</th><th>25 shot</th><th>Full</th><th>10 shot</th><th>25 shot</th><th>Full</th></tr></thead><tbody><tr><th>ViT-B/32</th><th>13B</th><th>CLIP-WIT</th><td>69.71</td><td>59.16</td><td>65.27</td><td>75.61</td><td>63.93</td><td>70.64</td><td>79.97</td></tr><tr><th>ViT-B/32</th><th>13B</th><th>LAION-400M</th><td>71.84</td><td>59.36</td><td>65.17</td><td>74.90</td><td>70.50</td><td>75.18</td><td>82.92</td></tr><tr><th>ViT-B/32</th><th>34B</th><th>LAION-2B</th><td>71.53</td><td>62.40</td><td>67.98</td><td>76.93</td><td>75.47</td><td>79.97</td><td>85.99</td></tr><tr><th>ViT-B/16</th><th>13B</th><th>CLIP-WIT</th><td>71.25</td><td>65.42</td><td>70.97</td><td>79.82</td><td>68.91</td><td>74.67</td><td>82.40</td></tr><tr><th>ViT-B/16</th><th>13B</th><th>LAION-400M</th><td>72.72</td><td>64.46</td><td>69.94</td><td>78.74</td><td>71.96</td><td>77.21</td><td>84.07</td></tr><tr><th>ViT-L/14</th><th>13B</th><th>CLIP-WIT</th><td>73.77</td><td>73.51</td><td>77.67</td><td>84.39</td><td>77.57</td><td>81.91</td><td>87.14</td></tr><tr><th>ViT-L/14</th><th>13B</th><th>LAION-400M</th><td>73.98</td><td>70.86</td><td>75.02</td><td>81.77</td><td>78.06</td><td>82.48</td><td>87.95</td></tr><tr><th>ViT-L/14</th><th>34B</th><th>LAION-2B</th><td>74.48</td><td>73.94</td><td>77.45</td><td>83.46</td><td>82.76</td><td>86.04</td><td>90.14</td></tr><tr><th>ViT-H/14</th><th>34B</th><th>LAION-2B</th><td>75.96</td><td>75.79</td><td>79.07</td><td>84.85</td><td>84.74</td><td>87.82</td><td>91.43</td></tr><tr><th>ViT-g/14</th><th>13B</th><th>LAION-2B</th><td>75.18</td><td>74.87</td><td>78.25</td><td>84.09</td><td>84.66</td><td>87.76</td><td>91.09</td></tr></tbody></table>", "caption": "Table 4: Scaling model and data size leads to lower error linear classifers on ImageNet [15], CIFAR100 [43], and the visual task adaptation benchmark (VTAB) [85].We train linear probes for models with at least 13B samples seen.We train probes by first caching the image features, thus no data augmentation is used.k shot denotes that k images per-class are used to train the linear probe.", "list_citation_info": ["[43] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.", "[85] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019.", "[15] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 248\u2013255, June 2009."]}, {"table": "<table><thead><tr><th></th><th></th><th></th><th colspan=\"9\">Top-1 Accuracy (%)</th></tr><tr><th>Arch.</th><th># samples</th><th>Dataset</th><th>ImageNet</th><th>Cars</th><th>DTD</th><th>EuroSAT</th><th>GTSRB</th><th>MNIST</th><th>RESISC45</th><th>SUN397</th><th>SVHN</th></tr></thead><tbody><tr><th>ViT-B/32</th><th>13B</th><th>CLIP-WIT</th><td>62.36</td><td>72.96</td><td>73.40</td><td>97.81</td><td>95.53</td><td>98.83</td><td>91.67</td><td>72.08</td><td>93.50</td></tr><tr><th>ViT-B/32</th><th>13B</th><th>LAION-400M</th><td>62.27</td><td>85.24</td><td>71.70</td><td>96.11</td><td>93.97</td><td>98.18</td><td>88.44</td><td>71.03</td><td>91.29</td></tr><tr><th>ViT-B/32</th><th>34B</th><th>LAION-2B</th><td>64.84</td><td>88.93</td><td>78.19</td><td>97.56</td><td>95.60</td><td>98.90</td><td>92.21</td><td>74.22</td><td>94.53</td></tr><tr><th>ViT-B/16</th><th>13B</th><th>CLIP-WIT</th><td>67.70</td><td>81.07</td><td>77.61</td><td>98.00</td><td>97.40</td><td>99.15</td><td>93.27</td><td>75.59</td><td>95.53</td></tr><tr><th>ViT-B/16</th><th>13B</th><th>LAION-400M</th><td>66.18</td><td>89.73</td><td>77.50</td><td>97.81</td><td>97.22</td><td>99.07</td><td>92.03</td><td>74.13</td><td>94.69</td></tr><tr><th>ViT-L/14</th><th>13B</th><th>CLIP-WIT</th><td>75.04</td><td>90.98</td><td>83.62</td><td>98.74</td><td>98.99</td><td>99.64</td><td>96.14</td><td>80.81</td><td>97.34</td></tr><tr><th>ViT-L/14</th><th>13B</th><th>LAION-400M</th><td>71.76</td><td>92.85</td><td>83.67</td><td>98.63</td><td>98.88</td><td>99.56</td><td>96.02</td><td>78.93</td><td>96.95</td></tr><tr><th>ViT-L/14</th><th>34B</th><th>LAION-2B</th><td>74.50</td><td>94.79</td><td>85.16</td><td>98.78</td><td>98.65</td><td>99.60</td><td>95.97</td><td>80.72</td><td>96.98</td></tr><tr><th>ViT-H/14</th><th>34B</th><th>LAION-2B</th><td>77.12</td><td>95.15</td><td>85.32</td><td>98.78</td><td>98.84</td><td>99.57</td><td>96.51</td><td>81.98</td><td>97.45</td></tr><tr><th>ViT-g/14</th><th>13B</th><th>LAION-2B</th><td>76.16</td><td>95.27</td><td>85.11</td><td>98.56</td><td>98.80</td><td>99.53</td><td>95.83</td><td>80.86</td><td>96.66</td></tr></tbody></table>", "caption": "Table 7: Accuracy after joint patching [31] for various models on downstream tasks from Section B.2.2. Patching by jointly fine-tuning on the eight tasks with the exception of ImageNet (used only as control), then interpolating the weights of the fine-tuned model with the weights of the zero-shot model. The mixing coefficient for the interpolation is chosen so it maximizes average accuracy on the eight downstream tasks while maintaining ImageNet accuracy within 1 percentage point of the corresponding zero-shot model.", "list_citation_info": ["[31] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. arXiv preprint arXiv:2208.05592, 2022."]}, {"table": "<table><tbody><tr><td></td><th></th><th></th><th></th><th></th><th></th><th></th><th colspan=\"6\">Top-1 Accuracy (%)</th></tr><tr><th>Model</th><th>Im Size</th><th>Dataset</th><th>Extra FT</th><th>Params (M)</th><th>GMAC</th><th>Acts (M)</th><th>IN</th><th>IN-ReaL</th><th>IN-V2</th><th>IN-A</th><th>IN-R</th><th>IN-Sketch</th></tr><tr><td>ViT-B/32</td><td>224</td><td>CLIP-WIT</td><td>None</td><td>88.2</td><td>4.4</td><td>5.0</td><td>81.93</td><td>87.17</td><td>70.70</td><td>22.57</td><td>55.90</td><td>45.04</td></tr><tr><td>ViT-B/32</td><td>224</td><td>LAION-2B</td><td>None</td><td>88.2</td><td>4.4</td><td>5.0</td><td>82.58</td><td>87.54</td><td>71.21</td><td>22.85</td><td>59.16</td><td>49.07</td></tr><tr><td>ViT-B/32</td><td>224</td><td>LAION-2B</td><td>IN-12k</td><td>88.2</td><td>4.4</td><td>5.0</td><td>83.30</td><td>87.81</td><td>72.50</td><td>30.57</td><td>57.06</td><td>45.74</td></tr><tr><td>ViT-B/32</td><td>384</td><td>CLIP-WIT</td><td>IN-12k</td><td>88.3</td><td>13.1</td><td>16.5</td><td>85.11</td><td>89.04</td><td>74.53</td><td>44.75</td><td>58.21</td><td>45.75</td></tr><tr><td>ViT-B/16</td><td>224</td><td>CLIP-WIT</td><td>None</td><td>86.6</td><td>17.6</td><td>23.9</td><td>85.28</td><td>89.16</td><td>75.57</td><td>47.23</td><td>66.02</td><td>50.94</td></tr><tr><td>ViT-B/32</td><td>384</td><td>LAION-2B</td><td>IN-12k</td><td>88.3</td><td>13.1</td><td>16.5</td><td>85.38</td><td>89.20</td><td>75.08</td><td>47.95</td><td>60.37</td><td>47.95</td></tr><tr><td>ViT-B/16</td><td>224</td><td>LAION-2B</td><td>None</td><td>86.6</td><td>17.6</td><td>23.9</td><td>85.47</td><td>89.43</td><td>75.13</td><td>41.57</td><td>68.75</td><td>55.40</td></tr><tr><td>ViT-B/16</td><td>384</td><td>CLIP-WIT</td><td>None</td><td>86.9</td><td>55.5</td><td>101.6</td><td>86.24</td><td>89.71</td><td>76.68</td><td>57.55</td><td>67.22</td><td>52.15</td></tr><tr><td>ViT-B/16</td><td>384</td><td>LAION-2B</td><td>None</td><td>86.9</td><td>55.5</td><td>101.6</td><td>86.53</td><td>90.04</td><td>77.55</td><td>56.96</td><td>69.94</td><td>55.85</td></tr><tr><td>ViT-B/16</td><td>384</td><td>LAION-2B</td><td>IN-12k</td><td>86.9</td><td>55.5</td><td>101.6</td><td>87.17</td><td>90.11</td><td>78.16</td><td>62.61</td><td>65.53</td><td>52.62</td></tr><tr><td>ViT-L/14</td><td>224</td><td>LAION-2B</td><td>None</td><td>304.2</td><td>81.1</td><td>88.8</td><td>87.30</td><td>90.10</td><td>78.42</td><td>59.89</td><td>81.70</td><td>64.81</td></tr><tr><td>ViT-H/14</td><td>224</td><td>LAION-2B</td><td>None</td><td>632.0</td><td>167.4</td><td>139.4</td><td>87.59</td><td>90.17</td><td>79.36</td><td>65.56</td><td>83.28</td><td>67.41</td></tr><tr><td>ViT-L/14</td><td>336</td><td>LAION-2B</td><td>None</td><td>304.5</td><td>191.1</td><td>270.2</td><td>87.78</td><td>90.30</td><td>79.07</td><td>69.03</td><td>82.60</td><td>64.79</td></tr><tr><td>ViT-L/14</td><td>224</td><td>CLIP-WIT</td><td>None</td><td>304.2</td><td>81.1</td><td>88.8</td><td>87.85</td><td>90.31</td><td>79.59</td><td>71.79</td><td>82.32</td><td>62.63</td></tr><tr><td>ViT-L/14</td><td>224</td><td>LAION-2B</td><td>IN-12k</td><td>304.2</td><td>81.1</td><td>88.8</td><td>87.89</td><td>90.30</td><td>78.51</td><td>67.01</td><td>78.26</td><td>62.06</td></tr><tr><td>ViT-L/14</td><td>336</td><td>LAION-2B</td><td>IN-12k</td><td>304.5</td><td>191.1</td><td>270.2</td><td>88.17</td><td>90.43</td><td>78.84</td><td>73.64</td><td>77.68</td><td>60.97</td></tr><tr><td>ViT-L/14</td><td>224</td><td>CLIP-WIT</td><td>IN-12k</td><td>304.2</td><td>81.1</td><td>88.8</td><td>88.17</td><td>90.37</td><td>79.38</td><td>72.33</td><td>78.68</td><td>61.40</td></tr><tr><td>ViT-H/14</td><td>224</td><td>LAION-2B</td><td>IN-12k</td><td>632.0</td><td>167.4</td><td>139.4</td><td>88.25</td><td>90.41</td><td>79.22</td><td>70.72</td><td>82.82</td><td>65.32</td></tr><tr><td>ViT-H/14</td><td>336</td><td>LAION-2B</td><td>IN-12k</td><td>632.5</td><td>391.0</td><td>407.5</td><td>88.50</td><td>90.49</td><td>79.55</td><td>75.68</td><td>82.26</td><td>64.62</td></tr></tbody></table>", "caption": "Table 8: Fine-tune results for ImageNet-1k and associated robustness test sets (ImageNet-ReaL [7], ImageNet-V2 [60], ImageNet-A [24], Imagenet-R [22], and ImageNet-Sketch [75]). Rows with the \u2019Extra FT\u2019 set to IN-12k were fine-tuned on a 12k class subset of ImageNet-22k before fine-tuning on ImageNet.", "list_citation_info": ["[24] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. Conference on Computer Vision and Pattern Recognition (CVPR), 2021. https://arxiv.org/abs/1907.07174.", "[60] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning (ICML), 2019. https://arxiv.org/abs/1902.10811.", "[22] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. International Conference on Computer Vision (ICCV), 2021. https://arxiv.org/abs/2006.16241.", "[75] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), 2019. https://arxiv.org/abs/1905.13549.", "[7] Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020."]}, {"table": "<table><tbody><tr><th>Model</th><th>Samples seen</th><td>LAION-80M</td><td>LAION-400M</td><td>LAION-2B</td></tr><tr><th>ViT-B/32</th><th>3B</th><td>38.05</td><td>41.53</td><td>43.66</td></tr><tr><th></th><th>13B</th><td>42.30</td><td>46.18</td><td>45.50</td></tr><tr><th></th><th>34B</th><td>42.10</td><td>46.41</td><td>50.69</td></tr><tr><th>ViT-B/16</th><th>3B</th><td>43.48</td><td>45.14</td><td>46.93</td></tr><tr><th></th><th>13B</th><td>44.42</td><td>48.39</td><td>48.72</td></tr><tr><th></th><th>34B</th><td>44.45</td><td>48.31</td><td>52.60</td></tr><tr><th>ViT-L/14</th><th>3B</th><td>45.69</td><td>50.50</td><td>51.64</td></tr><tr><th></th><th>13B</th><td>46.36</td><td>51.51</td><td>53.01</td></tr><tr><th></th><th>34B</th><td>45.70</td><td>52.83</td><td>54.63</td></tr><tr><th>ViT-H/14</th><th>34B</th><td>-</td><td>-</td><td>56.43</td></tr><tr><th>ViT-g/14</th><th>13B</th><td>-</td><td>56.54</td><td>-</td></tr></tbody></table>", "caption": "Table 15: Detailed results on VTAB+ [65] zero-shot classification, where we average over 35 tasks.", "list_citation_info": ["[65] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022."]}, {"table": "<table><tbody><tr><td>Dataset</td><td>Abbr.</td><td>Test size</td><td>#Classes</td></tr><tr><td>ImageNet</td><td>INet</td><td>50,000</td><td>1,000</td></tr><tr><td>ImageNet-v2</td><td>INet-v2</td><td>10,000</td><td>1,000</td></tr><tr><td>ImageNet-R</td><td>INet-R</td><td>30,000</td><td>200</td></tr><tr><td>ImageNet Sketch</td><td>INet-S</td><td>50,889</td><td>1,000</td></tr><tr><td>ObjectNet</td><td>ObjNet</td><td>18,574</td><td>113</td></tr><tr><td>ImageNet-A</td><td>INet-A</td><td>7,500</td><td>200</td></tr><tr><td>CIFAR-10</td><td>-</td><td>10,000</td><td>10</td></tr><tr><td>CIFAR-100</td><td>-</td><td>10,000</td><td>100</td></tr><tr><td>MNIST</td><td>-</td><td>10,000</td><td>10</td></tr><tr><td>Oxford Flowers 102</td><td>Flowers102</td><td>6,149</td><td>102</td></tr><tr><td>Stanford Cars</td><td>Cars</td><td>8,041</td><td>196</td></tr><tr><td>SVHN</td><td>-</td><td>26,032</td><td>10</td></tr><tr><td>Facial Emotion Recognition 2013</td><td>FER2013</td><td>7,178</td><td>7</td></tr><tr><td>RenderedSST2</td><td>-</td><td>1,821</td><td>2</td></tr><tr><td>Oxford-IIIT Pets</td><td>Pets</td><td>3,669</td><td>37</td></tr><tr><td>Caltech-101</td><td>-</td><td>6,085</td><td>102</td></tr><tr><td>Pascal VOC 2007 Classification</td><td>VOC2007-Cl</td><td>14,976</td><td>20</td></tr><tr><td>SUN397</td><td>-</td><td>108,754</td><td>397</td></tr><tr><td>FGVC Aircraft</td><td>-</td><td>3,333</td><td>100</td></tr><tr><td>Country211</td><td>-</td><td>21,100</td><td>211</td></tr><tr><td>Describable Textures</td><td>DTD</td><td>1,880</td><td>47</td></tr><tr><td>GTSRB</td><td>-</td><td>12,630</td><td>43</td></tr><tr><td>STL10</td><td>-</td><td>8,000</td><td>10</td></tr><tr><td>Diabetic Retinopathy</td><td>Retino</td><td>42,670</td><td>5</td></tr><tr><td>EuroSAT</td><td>-</td><td>5,400</td><td>10</td></tr><tr><td>RESISC45</td><td>-</td><td>6,300</td><td>45</td></tr><tr><td>PatchCamelyon</td><td>PCAM</td><td>32,768</td><td>2</td></tr><tr><td>CLEVR Counts</td><td>-</td><td>15,000</td><td>8</td></tr><tr><td>CLEVR Object Distance</td><td>CLEVR Dist</td><td>15,000</td><td>6</td></tr><tr><td>DSPRITES Orientation</td><td>DSPRITES Orient</td><td>73,728</td><td>40</td></tr><tr><td>DSPRITES Position</td><td>DSPRITES pos</td><td>73,728</td><td>32</td></tr><tr><td>SmallNORB Elevation</td><td>SmallNORB Elv</td><td>12,150</td><td>9</td></tr><tr><td>SmallNORB Azimuth</td><td>SmallNORB Azim</td><td>12,150</td><td>18</td></tr><tr><td>DMLAB</td><td>-</td><td>22,735</td><td>6</td></tr><tr><td>KITTI closest vehicle distance</td><td>KITTI Dist</td><td>711</td><td>4</td></tr><tr><td>MS-COCO</td><td>-</td><td>5,000</td><td>-</td></tr><tr><td>Flickr30K</td><td>-</td><td>1,000</td><td>-</td></tr></tbody></table>", "caption": "Table 25: Datasets used for evaluating downstream performance. Adapted from [65].", "list_citation_info": ["[65] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022."]}], "citation_info_to_title": {"[43] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.": "Learning multiple layers of features from tiny images", "[31] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. arXiv preprint arXiv:2208.05592, 2022.": "Patching open-vocabulary models by interpolating weights", "[75] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), 2019. https://arxiv.org/abs/1905.13549.": "Learning Robust Global Representations by Penalizing Local Predictive Power", "[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.": "Learning transferable visual models from natural language supervision", "[15] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 248\u2013255, June 2009.": "Imagenet: A large-scale hierarchical image database", "[85] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019.": "A large-scale study of representation learning with the visual task adaptation benchmark", "[60] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning (ICML), 2019. https://arxiv.org/abs/1902.10811.": "Do ImageNet classifiers generalize to ImageNet?", "[22] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. International Conference on Computer Vision (ICCV), 2021. https://arxiv.org/abs/2006.16241.": "The many faces of robustness: A critical analysis of out-of-distribution generalization", "[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.": "Microsoft COCO: Common Objects in Context", "[65] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022.": "LAION-5B: An open large-scale dataset for training next generation image-text models", "[7] Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.": "Are we done with imagenet?", "[82] Christoph Zauner. Implementation and benchmarking of perceptual image hash functions. 2010.": "Implementation and benchmarking of perceptual image hash functions", "[24] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. Conference on Computer Vision and Pattern Recognition (CVPR), 2021. https://arxiv.org/abs/1907.07174.": "Natural adversarial examples"}, "source_title_to_arxiv_id": {"Patching open-vocabulary models by interpolating weights": "2208.05592", "LAION-5B: An open large-scale dataset for training next generation image-text models": "2210.08402"}}