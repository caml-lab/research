{"title": "SALISA: Saliency-based Input Sampling for Efficient Video Object Detection", "abstract": "High-resolution images are widely adopted for high-performance object\ndetection in videos. However, processing high-resolution inputs comes with high\ncomputation costs, and naive down-sampling of the input to reduce the\ncomputation costs quickly degrades the detection performance. In this paper, we\npropose SALISA, a novel non-uniform SALiency-based Input SAmpling technique for\nvideo object detection that allows for heavy down-sampling of unimportant\nbackground regions while preserving the fine-grained details of a\nhigh-resolution image. The resulting image is spatially smaller, leading to\nreduced computational costs while enabling a performance comparable to a\nhigh-resolution input. To achieve this, we propose a differentiable resampling\nmodule based on a thin plate spline spatial transformer network (TPS-STN). This\nmodule is regularized by a novel loss to provide an explicit supervision signal\nto learn to \"magnify\" salient regions. We report state-of-the-art results in\nthe low compute regime on the ImageNet-VID and UA-DETRAC video object detection\ndatasets. We demonstrate that on both datasets, the mAP of an EfficientDet-D1\n(EfficientDet-D2) gets on par with EfficientDet-D2 (EfficientDet-D3) at a much\nlower computational cost. We also show that SALISA significantly improves the\ndetection of small objects. In particular, SALISA with an EfficientDet-D1\ndetector improves the detection of small objects by $77\\%$, and remarkably also\noutperforms EfficientDetD3 baseline.", "authors": ["Babak Ehteshami Bejnordi", "Amirhossein Habibian", "Fatih Porikli", "Amir Ghodrati"], "published_date": "2022_04_05", "pdf_url": "http://arxiv.org/pdf/2204.02397v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><td>Backbone</td><td>mAP (%)</td><td>FLOPs (G)</td></tr><tr><th>DFF [43]</th><td>ResNet-50</td><td>52.6</td><td>75.3</td></tr><tr><th>SpotNet [27]</th><td>CenterNet [41]</td><td>62.8</td><td>972.0</td></tr><tr><th>EfficientDet [35]</th><td>EfficientNet-B2</td><td>59.4</td><td>5.9</td></tr><tr><th>EfficientDet [35]</th><td>EfficientNet-B3</td><td>60.3</td><td>13.4</td></tr><tr><th>SALISA(Ours)</th><td>EfficientNet-B2</td><td>61.2</td><td>5.9</td></tr><tr><th>SALISA(Ours)</th><td>EfficientNet-B3</td><td>62.4</td><td>13.4</td></tr><tr><th>EfficientDet [35]</th><td>EfficientNet-B0</td><td>51.3</td><td>1.36</td></tr><tr><th>EfficientDet [35]</th><td>EfficientNet-B1</td><td>56.9</td><td>3.20</td></tr><tr><th>SALISA(Ours)</th><td>EfficientNet-B0</td><td>54.2</td><td>1.39</td></tr><tr><th>SALISA(Ours)</th><td>EfficientNet-B1</td><td>59.1</td><td>3.23</td></tr></tbody></table>", "caption": "Table 1: Comparison with state of the art on UA-DETRAC.", "list_citation_info": ["[35] Tan, M., Pang, R., Le, Q.V.: Efficientdet: Scalable and efficient object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10781\u201310790 (2020)", "[27] Perreault, H., Bilodeau, G.A., Saunier, N., H\u00e9ritier, M.: Spotnet: Self-attention multi-task network for object detection. CRV (2020)", "[43] Zhu, X., Xiong, Y., Dai, J., Yuan, L., Wei, Y.: Deep feature flow for video recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2349\u20132358 (2017)", "[41] Zhou, X., Wang, D., Kr\u00e4henb\u00fchl, P.: Objects as points. arXiv preprint arXiv:1904.07850 (2019)"]}, {"table": "<table><tbody><tr><th>Method</th><th>Backbone</th><td>mAP (%)</td><td>FLOPs (G)</td></tr><tr><th>DFF (R-FCN) [43]</th><th>ResNet-101</th><td>72.5</td><td>34.9</td></tr><tr><th>PatchNet (R-FCN) [25]</th><th>ResNet-101</th><td>73.1</td><td>34.2</td></tr><tr><th>TSM [16]</th><th>ResNext101 [39]</th><td>76.3</td><td>169</td></tr><tr><th>SkipConv [10]</th><th>EfficientNet-B2</th><td>72.3</td><td>9.2</td></tr><tr><th>SkipConv [10]</th><th>EfficientNet-B3</th><td>75.2</td><td>22.4</td></tr><tr><th>EfficientDet [35]</th><th>EfficientNet-B2</th><td>72.5</td><td>7.2</td></tr><tr><th>EfficientDet [35]</th><th>EfficientNet-B3</th><td>74.5</td><td>14.4</td></tr><tr><th>SALISA(Ours)</th><th>EfficientNet-B2</th><td>74.5</td><td>7.2</td></tr><tr><th>SALISA(Ours)</th><th>EfficientNet-B3</th><td>75.4</td><td>14.4</td></tr><tr><th>Mobile-SSD</th><th>MobileNet-V2</th><td>54.7</td><td>2.0</td></tr><tr><th>PatchWork [5]</th><th>MobileNet-V2</th><td>57.4</td><td>0.97</td></tr><tr><th>PatchNet (EfficientDet) [25]</th><th>EfficientNet-B0</th><td>58.9</td><td>0.73</td></tr><tr><th>Mobile-DFF [43]</th><th>MobileNet</th><td>62.8</td><td>0.71</td></tr><tr><th>TAFM (SSDLite) [18]</th><th>MobileNet-V2</th><td>64.1</td><td>1.18</td></tr><tr><th>SkipConv [10]</th><th>EfficientNet-B0</th><td>66.2</td><td>0.98</td></tr><tr><th>SkipConv [10]</th><th>EfficientNet-B1</th><td>70.5</td><td>2.90</td></tr><tr><th>EfficientDet [35]</th><th>EfficientNet-B0</th><td>66.6</td><td>1.48</td></tr><tr><th>EfficientDet [35]</th><th>EfficientNet-B1</th><td>69.7</td><td>3.35</td></tr><tr><th>SALISA(Ours)</th><th>EfficientNet-B0{}^{*}</th><td>67.4</td><td>0.86</td></tr><tr><th>SALISA(Ours)</th><th>EfficientNet-B0</th><td>68.7</td><td>1.50</td></tr><tr><th>SALISA(Ours)</th><th>EfficientNet-B1</th><td>71.8</td><td>3.38</td></tr></tbody></table>", "caption": "Table 2: Comparison with state of the art on ImageNet-VID. {}^{*} indicates that the model has been applied every three frames.", "list_citation_info": ["[35] Tan, M., Pang, R., Le, Q.V.: Efficientdet: Scalable and efficient object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10781\u201310790 (2020)", "[10] Habibian, A., Abati, D., Cohen, T.S., Bejnordi, B.E.: Skip-convolutions for efficient video processing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2695\u20132704 (2021)", "[25] Mao, H., Zhu, S., Han, S., Dally, W.J.: Patchnet\u2013short-range template matching for efficient video processing. arXiv preprint arXiv:2103.07371 (2021)", "[16] Lin, J., Gan, C., Han, S.: Tsm: Temporal shift module for efficient video understanding. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7083\u20137093 (2019)", "[18] Liu, M., Zhu, M.: Mobile video object detection with temporally-aware feature maps. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5686\u20135695 (2018)", "[43] Zhu, X., Xiong, Y., Dai, J., Yuan, L., Wei, Y.: Deep feature flow for video recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2349\u20132358 (2017)", "[39] Xie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1492\u20131500 (2017)", "[5] Chai, Y.: Patchwork: A patch-wise attention network for efficient object detection and segmentation in video streams. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3415\u20133424 (2019)"]}, {"table": "<table><thead><tr><th>Method</th><th>mAP</th><th>FLOPs</th></tr></thead><tbody><tr><th>D1 + D0</th><td>52.2</td><td>2.9 G</td></tr><tr><th>D1 + Copy</th><td>30.6</td><td>0.8 G</td></tr><tr><th>D1 + SiamFC [2]</th><td>51.6</td><td>3.1 G</td></tr><tr><th>D1 + SALISA (D0)</th><td>56.0</td><td>2.9 G</td></tr></tbody></table>", "caption": "Table 7: Comparison of SALISA with several tracking baselines.", "list_citation_info": ["[2] Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.: Fully-convolutional siamese networks for object tracking. In: European conference on computer vision. pp. 850\u2013865. Springer (2016)"]}], "citation_info_to_title": {"[10] Habibian, A., Abati, D., Cohen, T.S., Bejnordi, B.E.: Skip-convolutions for efficient video processing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2695\u20132704 (2021)": "Skip-convolutions for efficient video processing", "[39] Xie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., He, K.: Aggregated residual transformations for deep neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1492\u20131500 (2017)": "Aggregated residual transformations for deep neural networks", "[43] Zhu, X., Xiong, Y., Dai, J., Yuan, L., Wei, Y.: Deep feature flow for video recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2349\u20132358 (2017)": "Deep feature flow for video recognition", "[5] Chai, Y.: Patchwork: A patch-wise attention network for efficient object detection and segmentation in video streams. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3415\u20133424 (2019)": "Patchwork: A patch-wise attention network for efficient object detection and segmentation in video streams", "[18] Liu, M., Zhu, M.: Mobile video object detection with temporally-aware feature maps. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5686\u20135695 (2018)": "Mobile video object detection with temporally-aware feature maps", "[2] Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.: Fully-convolutional siamese networks for object tracking. In: European conference on computer vision. pp. 850\u2013865. Springer (2016)": "Fully-convolutional siamese networks for object tracking", "[16] Lin, J., Gan, C., Han, S.: Tsm: Temporal shift module for efficient video understanding. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7083\u20137093 (2019)": "Tsm: Temporal shift module for efficient video understanding", "[41] Zhou, X., Wang, D., Kr\u00e4henb\u00fchl, P.: Objects as points. arXiv preprint arXiv:1904.07850 (2019)": "Objects as points", "[35] Tan, M., Pang, R., Le, Q.V.: Efficientdet: Scalable and efficient object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10781\u201310790 (2020)": "Efficientdet: Scalable and efficient object detection", "[25] Mao, H., Zhu, S., Han, S., Dally, W.J.: Patchnet\u2013short-range template matching for efficient video processing. arXiv preprint arXiv:2103.07371 (2021)": "PatchNet \u2013 Short-Range Template Matching for Efficient Video Processing", "[27] Perreault, H., Bilodeau, G.A., Saunier, N., H\u00e9ritier, M.: Spotnet: Self-attention multi-task network for object detection. CRV (2020)": "Spotnet: Self-attention multi-task network for object detection"}, "source_title_to_arxiv_id": {"Skip-convolutions for efficient video processing": "2104.11487"}}