{"title": "Representative Image Feature Extraction via Contrastive Learning Pretraining for Chest X-ray Report Generation", "abstract": "Medical report generation is a challenging task since it is time-consuming\nand requires expertise from experienced radiologists. The goal of medical\nreport generation is to accurately capture and describe the image findings.\nPrevious works pretrain their visual encoding neural networks with large\ndatasets in different domains, which cannot learn general visual representation\nin the specific medical domain. In this work, we propose a medical report\ngeneration framework that uses a contrastive learning approach to pretrain the\nvisual encoder and requires no additional meta information. In addition, we\nadopt lung segmentation as an augmentation method in the contrastive learning\nframework. This segmentation guides the network to focus on encoding the visual\nfeature within the lung region. Experimental results show that the proposed\nframework improves the performance and the quality of the generated medical\nreports both quantitatively and qualitatively.", "authors": ["Yu-Jen Chen", "Wei-Hsiang Shen", "Hao-Wei Chung", "Ching-Hao Chiu", "Da-Cheng Juan", "Tsung-Ying Ho", "Chi-Tung Cheng", "Meng-Lin Li", "Tsung-Yi Ho"], "published_date": "2022_09_04", "pdf_url": "http://arxiv.org/pdf/2209.01604v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Dataset</th><th>Method</th><td>B-1</td><td>B-2</td><td>B-3</td><td>B-4</td><td>R-L</td></tr><tr><th rowspan=\"8\">IU X-ray</th><th>Att2in{}^{\\dagger} [Rennie et al.(2017)Rennie, Marcheret, Mroueh, Ross, andGoel]</th><td>0.399</td><td>0.249</td><td>0.172</td><td>0.126</td><td>0.321</td></tr><tr><th>AdaAtt{}^{\\dagger} [Lu et al.(2017)Lu, Xiong, Parikh, and Socher]</th><td>0.436</td><td>0.288</td><td>0.203</td><td>0.150</td><td>0.354</td></tr><tr><th>HRGR-Agent{}^{\\dagger} [Li et al.(2018)Li, Liang, Hu, and Xing]</th><td>0.438</td><td>0.298</td><td>0.208</td><td>0.151</td><td>0.322</td></tr><tr><th>CoAtt{}^{\\dagger} [Jing et al.(2017)Jing, Xie, and Xing]</th><td>0.455</td><td>0.288</td><td>0.205</td><td>0.154</td><td>0.369</td></tr><tr><th>Transformer [Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,Kaiser, and Polosukhin]</th><td>0.420</td><td>0.256</td><td>0.175</td><td>0.127</td><td>0.326</td></tr><tr><th>Ours (SimCLR pretraining + Transformer)</th><td>0.456</td><td>0.289</td><td>0.202</td><td>0.150</td><td>0.362</td></tr><tr><th>R2Gen (Res50) [Chen et al.(2020c)Chen, Song, Chang, andWan]</th><td>0.455</td><td>0.293</td><td>0.210</td><td>0.160</td><td>0.360</td></tr><tr><th>Ours (MoCo pretraining + R2Gen)</th><td>0.466</td><td>0.303</td><td>0.218</td><td>0.165</td><td>0.361</td></tr><tr><th rowspan=\"5\">MIMIC-CXR</th><th>AdaAt{}^{\\dagger} [Lu et al.(2017)Lu, Xiong, Parikh, and Socher]</th><td>0.299</td><td>0.185</td><td>0.124</td><td>0.088</td><td>0.266</td></tr><tr><th>Att2in{}^{\\dagger} [Rennie et al.(2017)Rennie, Marcheret, Mroueh, Ross, andGoel]</th><td>0.325</td><td>0.203</td><td>0.136</td><td>0.096</td><td>0.276</td></tr><tr><th>Transformer [Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,Kaiser, and Polosukhin]</th><td>0.314</td><td>0.192</td><td>0.127</td><td>0.090</td><td>0.265</td></tr><tr><th>Ours (SimCLR pretraining + Transformer)</th><td>0.343</td><td>0.208</td><td>0.137</td><td>0.095</td><td>0.260</td></tr><tr><th>R2Gen (Res50) [Chen et al.(2020c)Chen, Song, Chang, andWan]</th><td>0.354</td><td>0.236</td><td>0.164</td><td>0.118</td><td>0.305</td></tr><tr><th></th><th>Ours (MoCo pretraining + R2Gen)</th><td>0.359</td><td>0.239</td><td>0.167</td><td>0.121</td><td>0.304</td></tr></tbody></table>", "caption": "Table 4: Performance comparison with previous studies on the IU X-Ray dataset and MIMIC-CXR dataset. B-n and R-L are short for BLEU-n and ROUGE-L, respectively. {}^{\\dagger} indicates the results are quoted from the published paper. Note that we reproduce R2Gen [Chen et al.(2020c)Chen, Song, Chang, andWan] in both datasets with ResNet-50 architecture followed by their recommended setting.", "list_citation_info": ["[Rennie et al.(2017)Rennie, Marcheret, Mroueh, Ross, and Goel] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7008\u20137024, 2017.", "[Chen et al.(2020c)Chen, Song, Chang, and Wan] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating radiology reports via memory-driven transformer. arXiv preprint arXiv:2010.16056, 2020c.", "[Li et al.(2018)Li, Liang, Hu, and Xing] Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. Hybrid retrieval-generation reinforced agent for medical image report generation. arXiv preprint arXiv:1805.08298, 2018.", "[Lu et al.(2017)Lu, Xiong, Parikh, and Socher] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 375\u2013383, 2017.", "[Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.", "[Jing et al.(2017)Jing, Xie, and Xing] Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195, 2017."]}], "citation_info_to_title": {"[Lu et al.(2017)Lu, Xiong, Parikh, and Socher] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive attention via a visual sentinel for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 375\u2013383, 2017.": "Knowing when to look: Adaptive attention via a visual sentinel for image captioning", "[Rennie et al.(2017)Rennie, Marcheret, Mroueh, Ross, and Goel] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7008\u20137024, 2017.": "Self-critical sequence training for image captioning", "[Jing et al.(2017)Jing, Xie, and Xing] Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195, 2017.": "On the automatic generation of medical imaging reports", "[Chen et al.(2020c)Chen, Song, Chang, and Wan] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating radiology reports via memory-driven transformer. arXiv preprint arXiv:2010.16056, 2020c.": "Generating Radiology Reports via Memory-Driven Transformer", "[Li et al.(2018)Li, Liang, Hu, and Xing] Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. Hybrid retrieval-generation reinforced agent for medical image report generation. arXiv preprint arXiv:1805.08298, 2018.": "Hybrid retrieval-generation reinforced agent for medical image report generation", "[Vaswani et al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.": "Attention is all you need"}, "source_title_to_arxiv_id": {"On the automatic generation of medical imaging reports": "1711.08195"}}