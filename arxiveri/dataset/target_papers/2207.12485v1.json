{"title": "3D Shape Sequence of Human Comparison and Classification using Current and Varifolds", "abstract": "In this paper we address the task of the comparison and the classification of\n3D shape sequences of human. The non-linear dynamics of the human motion and\nthe changing of the surface parametrization over the time make this task very\nchallenging. To tackle this issue, we propose to embed the 3D shape sequences\nin an infinite dimensional space, the space of varifolds, endowed with an inner\nproduct that comes from a given positive definite kernel. More specifically,\nour approach involves two steps: 1) the surfaces are represented as varifolds,\nthis representation induces metrics equivariant to rigid motions and invariant\nto parametrization; 2) the sequences of 3D shapes are represented by Gram\nmatrices derived from their infinite dimensional Hankel matrices. The problem\nof comparison of two 3D sequences of human is formulated as a comparison of two\nGram-Hankel matrices. Extensive experiments on CVSSP3D and Dyna datasets show\nthat our method is competitive with state-of-the-art in 3D human sequence\nmotion retrieval. Code for the experiments is available at\nhttps://github.com/CRISTAL-3DSAM/HumanComparisonVarifolds.", "authors": ["Emery Pierson", "Mohamed Daoudi", "Sylvain Arguillere"], "published_date": "2022_07_25", "pdf_url": "http://arxiv.org/pdf/2207.12485v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Representation</td><td rowspan=\"2\">\\Gamma inv.</td><td rowspan=\"2\">SO(3)</td><td colspan=\"3\">Artificial dataset</td><td colspan=\"3\">Real dataset</td><td colspan=\"3\">Dyna dataset</td></tr><tr><td>NN</td><td>FT</td><td>ST</td><td>NN</td><td>FT</td><td>ST</td><td>NN</td><td>FT</td><td>ST</td></tr><tr><td>Shape Dist. [27][40]</td><td>\u2713</td><td>\u2713</td><td>92.1</td><td>88.9</td><td>97.2</td><td>77.5</td><td>51.6</td><td>65.5</td><td>/</td><td>/</td><td>/</td></tr><tr><td>Spin Images [20][40]</td><td>\u2713</td><td>\u2713</td><td>100</td><td>87.1</td><td>94.1</td><td>77.5</td><td>51.6</td><td>65.5</td><td>/</td><td>/</td><td>/</td></tr><tr><td>3D harmonics [40]</td><td>\\approx</td><td>\\approx</td><td>100</td><td>98.3</td><td>99.9</td><td>92.5</td><td>72.7</td><td>86.1</td><td>/</td><td>/</td><td>/</td></tr><tr><td>Breadths spectrum [30]</td><td>\u2713</td><td>\u2713</td><td>100</td><td>99.8</td><td>100</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td></tr><tr><td>Shape invariant [30]</td><td>\u2713</td><td>\u2713</td><td>82.1</td><td>56.8</td><td>68.5</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td></tr><tr><td>Q-Breadths spectrum [30]</td><td>\\approx</td><td>\u2713</td><td>/</td><td>/</td><td>/</td><td>80.0</td><td>44.8</td><td>59.5</td><td>/</td><td>/</td><td>/</td></tr><tr><td>Q-shape invariant [30]</td><td>\\approx</td><td>\u2713</td><td>/</td><td>/</td><td>/</td><td>82.5</td><td>51.3</td><td>68.8</td><td>/</td><td>/</td><td>/</td></tr><tr><td>Areas [30]</td><td>\u2713</td><td>\u2717</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>37.2</td><td>24.5</td><td>35.8</td></tr><tr><td>Breadths [30]</td><td>\u2713</td><td>\u2717</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>50.7</td><td>36.2</td><td>50.5</td></tr><tr><td>Areas &amp; Breadths [30]</td><td>\u2713</td><td>\u2717</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>50.7</td><td>37.2</td><td>51.7</td></tr><tr><td>GDVAE [6]</td><td>\u2713</td><td>\u2713</td><td>100</td><td>97.6</td><td>98.8</td><td>38.7</td><td>31.6</td><td>51.6</td><td>18.7</td><td>19.6</td><td>32.2</td></tr><tr><td>Zhou et al. [43]</td><td>\u2717</td><td>\u2717</td><td>100</td><td>99.6</td><td>99.6</td><td>/</td><td>/</td><td>/</td><td>50.0</td><td>40.4</td><td>57.0</td></tr><tr><td>LIMP [14]</td><td>\u2713</td><td>\u2717</td><td>100</td><td>99.98</td><td>99.98</td><td>/</td><td>/</td><td>/</td><td>29.1</td><td>20.7</td><td>33.9</td></tr><tr><td>SMPL pose vector [25]</td><td>\\approx</td><td>\u2713</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>/</td><td>58.2</td><td>45.7</td><td>63.2</td></tr><tr><td>Current</td><td>\u2713</td><td>\u2713</td><td>100</td><td>100</td><td>100</td><td>92.5</td><td>66.0</td><td>78.5</td><td>59.0</td><td>34.1</td><td>50.4</td></tr><tr><td>Absolute varifolds</td><td>\u2713</td><td>\u2713</td><td>100</td><td>100</td><td>100</td><td>95.0</td><td>66.6</td><td>80.7</td><td>60.4</td><td>40.0</td><td>55.9</td></tr><tr><td>Oriented varifolds</td><td>\u2713</td><td>\u2713</td><td>100</td><td>100</td><td>100</td><td>93.8</td><td>65.4</td><td>78.2</td><td>60.4</td><td>40.8</td><td>55.9</td></tr></tbody></table>", "caption": "Table 1: Full comparison of motion retrieval approaches. First two columns correspond to group invariance (\\Gamma: reparameterization group, SO(3): rotation group), telling whether or not the required invariance is fullfilled (\u2713: fully invariant, \\mathbf{\\approx} : approximately invariant (normalization, supplementary information, \u2026), \u2717: no invariance). Remaining columns correspond to retrieval scores, where the \u2019/\u2019 symbol means that there is no result for the method on the given dataset for various reasons, such as unavailable implementation or the method not being adapted for the dataset (for example, in line 470, [41] is based on a given mesh with vertex correspondences and cannot be applied to CVSSP3D real dataset). The results are displayed for CVSSP3D artificial and real datasets, and Dyna datasets. Our method is competitive or better than the approach consisting of combing DTW with any descriptor, while showing all required invariances. ", "list_citation_info": ["[27] Osada, R., Funkhouser, T.A., Chazelle, B., Dobkin, D.P.: Shape distributions. ACM Trans. Graph. 21(4), 807\u2013832 (2002)", "[20] Johnson, A.E., Hebert, M.: Using spin images for efficient object recognition in cluttered 3D scenes. IEEE Trans. Pattern Anal. Mach. Intell. 21(5), 433\u2013449 (1999)", "[6] Aumentado-Armstrong, T., Tsogkas, S., Jepson, A., Dickinson, S.: Geometric disentanglement for generative latent shape models. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 8180\u20138189 (2019)", "[14] Cosmo, L., Norelli, A., Halimi, O., Kimmel, R., Rodol\u00e0, E.: Limp: Learning latent shape representations with metric preservation priors. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16. pp. 19\u201335. Springer (2020)", "[30] Pierson, E., Paiva, J.C.\u00c1., Daoudi, M.: Projection-based classification of surfaces for 3d human mesh sequence retrieval. Computers & Graphics (2021)", "[43] Zhou, K., Bhatnagar, B.L., Pons-Moll, G.: Unsupervised shape and pose disentanglement for 3D meshes. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision \u2013 ECCV 2020. pp. 341\u2013357. Cham (2020)", "[40] Veinidis, C., Danelakis, A., Pratikakis, I., Theoharis, T.: Effective descriptors for human action retrieval from 3d mesh sequences. International Journal of Image and Graphics 19(03), 1950018 (2019)", "[25] Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia) 34(6), 248:1\u2013248:16 (Oct 2015)"]}], "citation_info_to_title": {"[14] Cosmo, L., Norelli, A., Halimi, O., Kimmel, R., Rodol\u00e0, E.: Limp: Learning latent shape representations with metric preservation priors. In: Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16. pp. 19\u201335. Springer (2020)": "Limp: Learning Latent Shape Representations with Metric Preservation Priors", "[6] Aumentado-Armstrong, T., Tsogkas, S., Jepson, A., Dickinson, S.: Geometric disentanglement for generative latent shape models. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV). pp. 8180\u20138189 (2019)": "Geometric disentanglement for generative latent shape models", "[30] Pierson, E., Paiva, J.C.\u00c1., Daoudi, M.: Projection-based classification of surfaces for 3d human mesh sequence retrieval. Computers & Graphics (2021)": "Projection-based classification of surfaces for 3D human mesh sequence retrieval", "[20] Johnson, A.E., Hebert, M.: Using spin images for efficient object recognition in cluttered 3D scenes. IEEE Trans. Pattern Anal. Mach. Intell. 21(5), 433\u2013449 (1999)": "Using spin images for efficient object recognition in cluttered 3D scenes", "[40] Veinidis, C., Danelakis, A., Pratikakis, I., Theoharis, T.: Effective descriptors for human action retrieval from 3d mesh sequences. International Journal of Image and Graphics 19(03), 1950018 (2019)": "Effective descriptors for human action retrieval from 3D mesh sequences", "[25] Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia) 34(6), 248:1\u2013248:16 (Oct 2015)": "SMPL: A skinned multi-person linear model", "[27] Osada, R., Funkhouser, T.A., Chazelle, B., Dobkin, D.P.: Shape distributions. ACM Trans. Graph. 21(4), 807\u2013832 (2002)": "Shape distributions", "[43] Zhou, K., Bhatnagar, B.L., Pons-Moll, G.: Unsupervised shape and pose disentanglement for 3D meshes. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision \u2013 ECCV 2020. pp. 341\u2013357. Cham (2020)": "Unsupervised shape and pose disentanglement for 3D meshes"}, "source_title_to_arxiv_id": {"Projection-based classification of surfaces for 3D human mesh sequence retrieval": "2111.13985"}}