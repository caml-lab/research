{"title": "Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition", "abstract": "We address the problem of data augmentation for video action recognition.\nStandard augmentation strategies in video are hand-designed and sample the\nspace of possible augmented data points either at random, without knowing which\naugmented points will be better, or through heuristics. We propose to learn\nwhat makes a good video for action recognition and select only high-quality\nsamples for augmentation. In particular, we choose video compositing of a\nforeground and a background video as the data augmentation process, which\nresults in diverse and realistic new samples. We learn which pairs of videos to\naugment without having to actually composite them. This reduces the space of\npossible augmentations, which has two advantages: it saves computational cost\nand increases the accuracy of the final trained classifier, as the augmented\npairs are of higher quality than average. We present experimental results on\nthe entire spectrum of training settings: few-shot, semi-supervised and fully\nsupervised. We observe consistent improvements across all of them over prior\nwork and baselines on Kinetics, UCF101, HMDB51, and achieve a new\nstate-of-the-art on settings with limited data. We see improvements of up to\n8.6% in the semi-supervised setting.", "authors": ["Shreyank N Gowda", "Marcus Rohrbach", "Frank Keller", "Laura Sevilla-Lara"], "published_date": "2022_06_09", "pdf_url": "http://arxiv.org/pdf/2206.04790v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td></td><td colspan=\"4\">Kinetics 100</td><td colspan=\"4\">UCF101</td><td colspan=\"3\">HMDB51</td></tr><tr><th>Method</th><td>Conference</td><td>50%</td><td>20%</td><td>10%</td><td>5%</td><td>50%</td><td>20%</td><td>10%</td><td>5%</td><td>60%</td><td>50%</td><td>40%</td></tr><tr><th>CutMix [37]</th><td>ICCV19</td><td>53.7</td><td>46.1</td><td>43.2</td><td>39.9</td><td>46.1</td><td>36.5</td><td>34.6</td><td>25.8</td><td>33.9</td><td>30.8</td><td>27.8</td></tr><tr><th>MixUp [41]</th><td>ICLR18</td><td>53.4</td><td>45.5</td><td>43.0</td><td>39.6</td><td>45.8</td><td>36.1</td><td>34.2</td><td>25.5</td><td>33.7</td><td>31.0</td><td>27.5</td></tr><tr><th>CutOut [10]</th><td>Arxiv17</td><td>52.8</td><td>45.1</td><td>42.3</td><td>38.8</td><td>45.2</td><td>35.6</td><td>33.9</td><td>24.6</td><td>33.0</td><td>30.5</td><td>27.1</td></tr><tr><th>ST-VideoMix [38]</th><td>Arxiv21</td><td>55.3</td><td>46.6</td><td>43.9</td><td>40.4</td><td>46.4</td><td>36.4</td><td>35.2</td><td>25.9</td><td>34.8</td><td>31.3</td><td>28.7</td></tr><tr><th>PseudoLabel [24]</th><td>ICMLW13</td><td>59.0</td><td>48.0</td><td>38.9</td><td>27.9</td><td>47.5</td><td>37.0</td><td>24.7</td><td>17.6</td><td>33.5</td><td>32.4</td><td>27.3</td></tr><tr><th>MeanTeacher [34]</th><td>Neurips17</td><td>59.3</td><td>47.1</td><td>36.4</td><td>27.8</td><td>45.8</td><td>36.3</td><td>25.6</td><td>17.5</td><td>32.2</td><td>30.4</td><td>27.2</td></tr><tr><th>S4L [39]</th><td>ICCV19</td><td>54.6</td><td>51.1</td><td>43.3</td><td>33.0</td><td>47.9</td><td>37.7</td><td>29.1</td><td>22.7</td><td>35.6</td><td>31.0</td><td>29.8</td></tr><tr><th>VideoSSL [18]</th><td>WACV21</td><td>65.0</td><td>57.7</td><td>52.6</td><td>47.6</td><td>54.3</td><td>48.7</td><td>42.0</td><td>32.4</td><td>37.0</td><td>36.2</td><td>32.7</td></tr><tr><th>ActorCut [43]</th><td>Arxiv21</td><td>68.7</td><td>61.2</td><td>56.8</td><td>52.7</td><td>59.9</td><td>51.7</td><td>40.2</td><td>27.0</td><td>38.9</td><td>38.2</td><td>32.9</td></tr><tr><th>ActorCut+ID [43]</th><td>Arxiv21</td><td>72.2</td><td>68.7</td><td>63.9</td><td>59.1</td><td>64.7</td><td>57.4</td><td>53.0</td><td>45.1</td><td>40.8</td><td>39.5</td><td>35.7</td></tr><tr><th>TCL [30]</th><td>ICCV21</td><td>70.4</td><td>64.7</td><td>61.1</td><td>58.2</td><td>62.1</td><td>55.4</td><td>52.1</td><td>42.8</td><td>41.2</td><td>40.4</td><td>34.8</td></tr><tr><th>L2A</th><td></td><td>75.9</td><td>72.1</td><td>67.5</td><td>63.7</td><td>72.1</td><td>60.3</td><td>56.1</td><td>48.0</td><td>44.5</td><td>43.2</td><td>37.9</td></tr><tr><th>L2A +Pre-training</th><td></td><td>-</td><td>-</td><td>-</td><td>-</td><td>73.3</td><td>64.8</td><td>60.1</td><td>50.9</td><td>47.1</td><td>46.3</td><td>42.1</td></tr></tbody></table>", "caption": "Table 3: Results on the semi-supervised setting. Results for TCL and ActorCut are obtained by us running the author\u2019s code. All methods are run with a 3D ResNet-18 backbone for fair comparison. L2A +Pre-training refers to pre-training the selector and fixing it.", "list_citation_info": ["[18] Jing, L., Parag, T., Wu, Z., Tian, Y., Wang, H.: Videossl: Semi-supervised learning for video classification. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 1110\u20131119 (January 2021)", "[10] DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552 (2017)", "[30] Singh, A., Chakraborty, O., Varshney, A., Panda, R., Feris, R., Saenko, K., Das, A.: Semi-supervised action recognition with temporal contrastive learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10389\u201310399 (2021)", "[37] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: International Conference on Computer Vision (ICCV) (2019)", "[39] Zhai, X., Oliver, A., Kolesnikov, A., Beyer, L.: S4l: Self-supervised semi-supervised learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1476\u20131485 (2019)", "[41] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. In: International Conference on Learning Representations (2018)", "[24] Lee, D.H., et al.: Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In: Workshop on challenges in representation learning, ICML. vol. 3, p. 896 (2013)", "[38] Yun, S., Oh, S.J., Heo, B., Han, D., Kim, J.: Videomix: Rethinking data augmentation for video classification. arXiv preprint arXiv:2012.03457 (2020)", "[34] Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780 (2017)", "[43] Zou, Y., Choi, J., Wang, Q., Huang, J.: Learning representational invariances for data-efficient action recognition. CoRR abs/2103.16565 (2021), https://arxiv.org/abs/2103.16565"]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"5\">UCF101</th><th colspan=\"5\">HMDB51</th></tr><tr><th>Method</th><th>Split</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><th>C3D-PN [31]</th><td>S</td><td>57.1</td><td>66.4</td><td>71.7</td><td>75.5</td><td>78.2</td><td>38.1</td><td>47.5</td><td>50.3</td><td>55.6</td><td>57.4</td></tr><tr><th>C3D-PN + L2A</th><td>S</td><td>60.8</td><td>68.9</td><td>73.3</td><td>76.6</td><td>79.1</td><td>39.8</td><td>48.9</td><td>51.5</td><td>57.3</td><td>58.2</td></tr><tr><th>ARN [40]</th><td>S</td><td>66.3</td><td>73.1</td><td>77.9</td><td>80.4</td><td>83.1</td><td>45.5</td><td>50.1</td><td>54.2</td><td>58.7</td><td>60.6</td></tr><tr><th>ARN + L2A</th><td>S</td><td>67.7</td><td>74.2</td><td>79.6</td><td>81.1</td><td>84.4</td><td>47.3</td><td>51.7</td><td>55.5</td><td>60.1</td><td>61.8</td></tr><tr><th>TRX [29]</th><td>S</td><td>77.5</td><td>88.8</td><td>92.8</td><td>94.7</td><td>96.1</td><td>50.5</td><td>62.7</td><td>66.9</td><td>73.5</td><td>75.6</td></tr><tr><th>TRX + L2A</th><td>S</td><td>79.2</td><td>89.2</td><td>93.2</td><td>95.0</td><td>96.3</td><td>51.9</td><td>63.8</td><td>68.2</td><td>74.4</td><td>77.0</td></tr><tr><th>C3D-PN [31]</th><td>T</td><td>50.9</td><td>61.9</td><td>67.5</td><td>72.9</td><td>75.4</td><td>28.8</td><td>38.5</td><td>43.4</td><td>46.7</td><td>49.1</td></tr><tr><th>C3D-PN + L2A</th><td>T</td><td>52.5</td><td>63.8</td><td>70.1</td><td>75.2</td><td>78.2</td><td>29.9</td><td>40.1</td><td>44.5</td><td>47.7</td><td>50.8</td></tr><tr><th>ARN [40]</th><td>T</td><td>61.2</td><td>70.7</td><td>75.2</td><td>78.8</td><td>80.2</td><td>31.9</td><td>42.3</td><td>46.5</td><td>49.8</td><td>53.2</td></tr><tr><th>ARN + L2A</th><td>T</td><td>63.9</td><td>73.1</td><td>77.4</td><td>80.4</td><td>81.3</td><td>33.6</td><td>43.7</td><td>48.0</td><td>51.1</td><td>53.8</td></tr><tr><th>TRX [29]</th><td>T</td><td>75.2</td><td>88.1</td><td>91.5</td><td>93.1</td><td>93.5</td><td>33.5</td><td>46.7</td><td>49.8</td><td>57.9</td><td>61.5</td></tr><tr><th>TRX + L2A</th><td>T</td><td>76.8</td><td>88.9</td><td>92.7</td><td>93.8</td><td>94.1</td><td>35.0</td><td>48.1</td><td>51.1</td><td>59.2</td><td>62.1</td></tr></tbody></table>", "caption": "Table 4: Results on UCF101 for the Few-Shot Learning setting, with different splits. Accuracies are reported for 5-way, 1, 2, 3, 4, 5-shot classification. S corresponds to the split used in [40, 29] and T is the TruZe split [12], which avoids overlapping classes with Kinetics.", "list_citation_info": ["[31] Snell, J., Swersky, K., Zemel, R.S.: Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175 (2017)", "[29] Perrett, T., Masullo, A., Burghardt, T., Mirmehdi, M., Damen, D.: Temporal-relational crosstransformers for few-shot action recognition. arXiv preprint arXiv:2101.06184 (2021)", "[40] Zhang, H., Zhang, L., Qi, X., Li, H., Torr, P.H., Koniusz, P.: Few-shot action recognition with permutation-invariant attention. In: Proceedings of the European Conference on Computer Vision (ECCV). Springer (2020)", "[12] Gowda, S.N., Sevilla-Lara, L., Kim, K., Keller, F., Rohrbach, M.: A new split for evaluating true zero-shot action recognition. In: 43rd DAGM German Conference on Pattern Recognition (2021)"]}, {"table": "<table><tbody><tr><td>Augmentation</td><td>Dataset</td><td>Pretrained</td><td>Top-1</td></tr><tr><td>Standard</td><td>UCF101</td><td>No Pretraining</td><td>55.7</td></tr><tr><td>ActorCut [43]</td><td>UCF101</td><td>No Pretraining</td><td>68.3</td></tr><tr><td>L2A</td><td>UCF101</td><td>No Pretraining</td><td>73.1</td></tr><tr><td>Standard</td><td>HMDB51</td><td>No Pretraining</td><td>40.8</td></tr><tr><td>ActorCut [43]</td><td>HMDB51</td><td>No Pretraining</td><td>44.5</td></tr><tr><td>L2A</td><td>HMDB51</td><td>NoPretraining</td><td>46.4</td></tr><tr><td>Standard</td><td>UCF101</td><td>Sports1M</td><td>93.6</td></tr><tr><td>L2A</td><td>UCF101</td><td>Sports1M</td><td>95.3</td></tr><tr><td>Standard</td><td>HMDB51</td><td>Sports1M</td><td>66.6</td></tr><tr><td>L2A</td><td>HMDB51</td><td>Sports1M</td><td>68.4</td></tr><tr><td>Standard</td><td>Kinetics</td><td>Sports1M</td><td>75.4</td></tr><tr><td>L2A</td><td>Kinetics</td><td>Sports1M</td><td>76.3</td></tr></tbody></table>", "caption": "Table 5: Augmenting standard datasets improves classification even with a model pre-trained on the largest existing dataset (Sports1M). ", "list_citation_info": ["[43] Zou, Y., Choi, J., Wang, Q., Huang, J.: Learning representational invariances for data-efficient action recognition. CoRR abs/2103.16565 (2021), https://arxiv.org/abs/2103.16565"]}, {"table": "<table><tbody><tr><td></td><th colspan=\"2\">50%</th><th colspan=\"2\">20%</th><th colspan=\"2\">10%</th><th colspan=\"2\">5%</th></tr><tr><th>Method</th><th>Acc</th><th>SS</th><th>Acc</th><th>SS</th><th>Acc</th><th>SS</th><th>Acc</th><th>SS</th></tr><tr><td>Random</td><td>61.9</td><td>430K</td><td>56.2</td><td>99K</td><td>51.8</td><td>44K</td><td>42.3</td><td>9.7K</td></tr><tr><td>Discriminator</td><td>62.8</td><td>430K</td><td>57.3</td><td>99K</td><td>52.2</td><td>44K</td><td>41.1</td><td>9.7K</td></tr><tr><td>SMART [11]</td><td>68.9</td><td>430K</td><td>58.9</td><td>99K</td><td>57.8</td><td>44K</td><td>46.5</td><td>9.7K</td></tr><tr><td>Proposed</td><td>72.1</td><td>39K</td><td>60.3</td><td>12K</td><td>56.1</td><td>5.2K</td><td>48.0</td><td>1.2K</td></tr></tbody></table>", "caption": "Table 6: Comparison of approaches for the use of Selector. All results are reported on UCF101. \u2019Acc\u2019 corresponds to accuracy and \u2019SS\u2019 corresponds to the number of mixed videos that the Selector looks at. All results are on different percentage of labeled data in UCF101.", "list_citation_info": ["[11] Gowda, S.N., Rohrbach, M., Sevilla-Lara, L.: Smart frame selection for action recognition. Proceedings of the AAAI Conference on Artificial Intelligence 35(2), 1451\u20131459 (May 2021), https://ojs.aaai.org/index.php/AAAI/article/view/16235"]}, {"table": "<table><tbody><tr><td>Method</td><td>Class Matching</td><td>1-shot</td><td>3-shot</td><td>5-shot</td></tr><tr><td>C3D-PN</td><td>Random</td><td>28.1</td><td>42.9</td><td>47.7</td></tr><tr><td>C3D-PN</td><td>Semantic</td><td>29.9</td><td>44.5</td><td>50.8</td></tr><tr><td>TRX</td><td>Random</td><td>33.5</td><td>49.9</td><td>60.3</td></tr><tr><td>TRX</td><td>Semantic</td><td>35.0</td><td>51.1</td><td>62.1</td></tr></tbody></table>", "caption": "Table 8: Results on FSL using the proposed Semantic Matching vs random matching using the TruZe [12] split.", "list_citation_info": ["[12] Gowda, S.N., Sevilla-Lara, L., Kim, K., Keller, F., Rohrbach, M.: A new split for evaluating true zero-shot action recognition. In: 43rd DAGM German Conference on Pattern Recognition (2021)"]}], "citation_info_to_title": {"[37] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: International Conference on Computer Vision (ICCV) (2019)": "Cutmix: Regularization strategy to train strong classifiers with localizable features", "[39] Zhai, X., Oliver, A., Kolesnikov, A., Beyer, L.: S4l: Self-supervised semi-supervised learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1476\u20131485 (2019)": "S4l: Self-supervised semi-supervised learning", "[41] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. In: International Conference on Learning Representations (2018)": "mixup: Beyond Empirical Risk Minimization", "[29] Perrett, T., Masullo, A., Burghardt, T., Mirmehdi, M., Damen, D.: Temporal-relational crosstransformers for few-shot action recognition. arXiv preprint arXiv:2101.06184 (2021)": "Temporal-Relational Crosstransformers for Few-Shot Action Recognition", "[11] Gowda, S.N., Rohrbach, M., Sevilla-Lara, L.: Smart frame selection for action recognition. Proceedings of the AAAI Conference on Artificial Intelligence 35(2), 1451\u20131459 (May 2021), https://ojs.aaai.org/index.php/AAAI/article/view/16235": "Smart frame selection for action recognition", "[18] Jing, L., Parag, T., Wu, Z., Tian, Y., Wang, H.: Videossl: Semi-supervised learning for video classification. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). pp. 1110\u20131119 (January 2021)": "Videossl: Semi-supervised learning for video classification", "[31] Snell, J., Swersky, K., Zemel, R.S.: Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175 (2017)": "Prototypical networks for few-shot learning", "[40] Zhang, H., Zhang, L., Qi, X., Li, H., Torr, P.H., Koniusz, P.: Few-shot action recognition with permutation-invariant attention. In: Proceedings of the European Conference on Computer Vision (ECCV). Springer (2020)": "Few-shot action recognition with permutation-invariant attention", "[38] Yun, S., Oh, S.J., Heo, B., Han, D., Kim, J.: Videomix: Rethinking data augmentation for video classification. arXiv preprint arXiv:2012.03457 (2020)": "Videomix: Rethinking data augmentation for video classification", "[12] Gowda, S.N., Sevilla-Lara, L., Kim, K., Keller, F., Rohrbach, M.: A new split for evaluating true zero-shot action recognition. In: 43rd DAGM German Conference on Pattern Recognition (2021)": "A new split for evaluating true zero-shot action recognition", "[30] Singh, A., Chakraborty, O., Varshney, A., Panda, R., Feris, R., Saenko, K., Das, A.: Semi-supervised action recognition with temporal contrastive learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10389\u201310399 (2021)": "Semi-supervised action recognition with temporal contrastive learning", "[24] Lee, D.H., et al.: Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In: Workshop on challenges in representation learning, ICML. vol. 3, p. 896 (2013)": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks", "[34] Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780 (2017)": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results", "[43] Zou, Y., Choi, J., Wang, Q., Huang, J.: Learning representational invariances for data-efficient action recognition. CoRR abs/2103.16565 (2021), https://arxiv.org/abs/2103.16565": "Learning representational invariances for data-efficient action recognition", "[10] DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552 (2017)": "Improved regularization of convolutional neural networks with cutout"}, "source_title_to_arxiv_id": {"Temporal-Relational Crosstransformers for Few-Shot Action Recognition": "2101.06184", "A new split for evaluating true zero-shot action recognition": "2107.13029"}}