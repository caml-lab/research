{"title": "R(Det)^2: Randomized Decision Routing for Object Detection", "abstract": "In the paradigm of object detection, the decision head is an important part,\nwhich affects detection performance significantly. Yet how to design a\nhigh-performance decision head remains to be an open issue. In this paper, we\npropose a novel approach to combine decision trees and deep neural networks in\nan end-to-end learning manner for object detection. First, we disentangle the\ndecision choices and prediction values by plugging soft decision trees into\nneural networks. To facilitate effective learning, we propose randomized\ndecision routing with node selective and associative losses, which can boost\nthe feature representative learning and network decision simultaneously.\nSecond, we develop the decision head for object detection with narrow branches\nto generate the routing probabilities and masks, for the purpose of obtaining\ndivergent decisions from different nodes. We name this approach as the\nrandomized decision routing for object detection, abbreviated as R(Det)$^2$.\nExperiments on MS-COCO dataset demonstrate that R(Det)$^2$ is effective to\nimprove the detection performance. Equipped with existing detectors, it\nachieves $1.4\\sim 3.6$\\% AP improvement.", "authors": ["Ya-Li Li", "Shengjin Wang"], "published_date": "2022_04_02", "pdf_url": "http://arxiv.org/pdf/2204.00794v1", "list_table_and_caption": [{"table": "<table><thead><tr><th><p>L^{cls}</p></th><th><p>L^{bbox}</p></th><th><p>AP</p></th><th><p>AP_{50}</p></th><th><p>AP_{75}</p></th><th><p>AP_{S}</p></th><th><p>AP_{M}</p></th><th><p>AP_{L}</p></th></tr><tr><th><p>Baseline</p></th><th></th><th><p>37.4</p></th><th><p>58.1</p></th><th><p>40.4</p></th><th><p>21.2</p></th><th><p>41.0</p></th><th><p>48.1</p></th></tr></thead><tbody><tr><td><p>CE</p></td><td>S-L1</td><td><p>40.4</p></td><td><p>61.2</p></td><td><p>44.1</p></td><td><p>23.8</p></td><td><p>43.7</p></td><td><p>53.0</p></td></tr><tr><td><p>Focal</p></td><td>S-L1</td><td><p>40.5</p></td><td><p>61.2</p></td><td><p>44.4</p></td><td><p>24.2</p></td><td><p>43.6</p></td><td><p>52.6</p></td></tr><tr><td><p>CE</p></td><td><p>IoU</p></td><td><p>40.9</p></td><td><p>61.2</p></td><td><p>44.5</p></td><td><p>23.9</p></td><td><p>44.2</p></td><td><p>53.7</p></td></tr><tr><td><p>Focal</p></td><td><p>IoU</p></td><td><p>41.0</p></td><td><p>61.1</p></td><td><p>44.5</p></td><td><p>24.3</p></td><td><p>44.3</p></td><td><p>53.7</p></td></tr></tbody></table>", "caption": "Table 2: Comparison with different loss functions. The baseline model is Faster R-CNN with ResNet-50 as the backbone. CE indicates the cross-entropy loss. Focal indicates the original focal loss [22]. S-L1 indicates the Smooth-L{}_{1} loss. IoU indicates the loss computed by the negative-log of intersection-over-union [45].", "list_citation_info": ["[45] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and Thomas Huang. Unitbox: An advanced object detection network. Proceedings of the 24th ACM international conference on Multimedia, pages 516\u2013520, 2016.", "[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. ICCV, pages 2980\u20132988, 2017."]}, {"table": "<table><thead><tr><th><p>Detector</p></th><th><p>AP</p></th><th><p>AP_{50}</p></th><th><p>AP_{75}</p></th><th><p>AP_{S}</p></th><th><p>AP_{M}</p></th><th><p>AP_{L}</p></th></tr></thead><tbody><tr><td><p>Libra R-CNN [30]</p></td><td><p>38.3</p></td><td><p>59.5</p></td><td><p>41.9</p></td><td><p>22.1</p></td><td><p>42.0</p></td><td><p>48.5</p></td></tr><tr><td><p>+R(Det){}^{2}</p></td><td>41.4(+3.1)</td><td>61.4(+1.9)</td><td>45.5(+3.6)</td><td>24.7(+2.5)</td><td>45.0(+3.0)</td><td>53.7(+5.2)</td></tr><tr><td><p>Cascade R-CNN [1]</p></td><td><p>40.3</p></td><td><p>58.6</p></td><td><p>44.0</p></td><td><p>22.5</p></td><td><p>43.8</p></td><td><p>52.9</p></td></tr><tr><td><p>+R(Det){}^{2}</p></td><td>42.5(+2.2)</td><td>61.0(+2.4)</td><td>45.8(+1.8)</td><td>24.6(+2.1)</td><td>45.5(+1.7)</td><td>57.0(+4.1)</td></tr><tr><td><p>Dynamic R-CNN [46]</p></td><td><p>38.9</p></td><td><p>57.6</p></td><td><p>42.7</p></td><td><p>22.1</p></td><td><p>41.9</p></td><td><p>51.7</p></td></tr><tr><td><p>+R(Det){}^{2}</p></td><td>41.0(+2.1)</td><td>59.7(+2.1)</td><td>44.8(+2.1)</td><td>23.3(+1.2)</td><td>44.2(+2.3)</td><td>54.8(+3.1)</td></tr><tr><td><p>DoubleHead R-CNN [43]</p></td><td><p>40.1</p></td><td><p>59.4</p></td><td><p>43.5</p></td><td><p>22.9</p></td><td><p>43.6</p></td><td><p>52.9</p></td></tr><tr><td><p>+R(Det){}^{2}</p></td><td>41.5(+1.4)</td><td>60.8(+1.4)</td><td>44.5(+1.0)</td><td>24.2(+1.3)</td><td>45.0(+1.4)</td><td>53.9(+1.0)</td></tr><tr><td><p>RetinaNet [22]</p></td><td><p>36.5</p></td><td><p>55.4</p></td><td><p>39.1</p></td><td><p>20.4</p></td><td><p>40.3</p></td><td><p>48.1</p></td></tr><tr><td><p>+R(Det){}^{2}</p></td><td>38.3(+1.8)</td><td>57.4(+2.0)</td><td>40.8(+1.7)</td><td>22.6(+2.2)</td><td>42.0(+1.7)</td><td>50.5(+2.4)</td></tr></tbody></table>", "caption": "Table 4: Generalization with different detectors. R(Det){}^{2} shows AP improvement on various detectors. ", "list_citation_info": ["[43] Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan Wang, Hongzhi Li, and Yun Fu. Rethinking classification and localization for object detection. CVPR, pages 10186\u201310195, 2020.", "[46] Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. Dynamic r-cnn: Towards high quality object detection via dynamic training. ECCV, pages 260\u2013275, 2020.", "[30] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards balanced learning for object detection. CVPR, pages 821\u2013830, 2019.", "[1] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. CVPR, pages 6154\u20136162, 2018.", "[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. ICCV, pages 2980\u20132988, 2017."]}, {"table": "<table><tbody><tr><th><p>Methods</p></th><th><p>Backbone</p></th><th><p>ME</p></th><th><p>TTA</p></th><td><p>AP</p></td><td><p>AP_{50}</p></td><td><p>AP_{75}</p></td><td><p>AP_{S}</p></td><td><p>AP_{M}</p></td><td><p>AP_{L}</p></td></tr><tr><th><p>Retina-Net [22]</p></th><th><p>ResNeXt-101</p></th><th><p>18e</p></th><th></th><td><p>40.8</p></td><td><p>61.1</p></td><td><p>44.1</p></td><td><p>24.1</p></td><td><p>44.2</p></td><td><p>51.2</p></td></tr><tr><th><p>FCOS [40]</p></th><th><p>ResNeXt-101</p></th><th><p>24e</p></th><th></th><td><p>43.2</p></td><td><p>62.8</p></td><td><p>46.6</p></td><td><p>26.5</p></td><td><p>46.2</p></td><td><p>53.3</p></td></tr><tr><th><p>ATSS [47]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th></th><td><p>47.7</p></td><td><p>66.5</p></td><td><p>51.9</p></td><td><p>29.7</p></td><td><p>50.8</p></td><td><p>59.4</p></td></tr><tr><th><p>OTA [14]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th></th><td><p>49.2</p></td><td><p>67.6</p></td><td><p>53.5</p></td><td><p>30.0</p></td><td><p>52.5</p></td><td><p>62.3</p></td></tr><tr><th><p>IQDet [28]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th></th><td><p>49.0</p></td><td><p>67.5</p></td><td><p>53.1</p></td><td><p>30.0</p></td><td><p>52.3</p></td><td><p>62.0</p></td></tr><tr><th><p>Faster R-CNN [33]</p></th><th><p>ResNet-101</p></th><th><p>12e</p></th><th></th><td><p>36.7</p></td><td><p>54.8</p></td><td><p>39.8</p></td><td><p>19.2</p></td><td><p>40.9</p></td><td><p>51.6</p></td></tr><tr><th><p>Libra R-CNN [30]</p></th><th><p>ResNeXt-101</p></th><th><p>12e</p></th><th></th><td><p>43.0</p></td><td><p>64.0</p></td><td><p>47.0</p></td><td><p>25.3</p></td><td><p>45.6</p></td><td><p>54.6</p></td></tr><tr><th><p>Cascade R-CNN [1]</p></th><th><p>ResNet-101</p></th><th><p>18e</p></th><th></th><td><p>42.8</p></td><td><p>62.1</p></td><td><p>46.3</p></td><td><p>23.7</p></td><td><p>45.5</p></td><td><p>55.2</p></td></tr><tr><th><p>TSP-RCNN [39]</p></th><th><p>ResNet-101-DCN</p></th><th><p>96e</p></th><th></th><td><p>47.4</p></td><td><p>66.7</p></td><td><p>51.9</p></td><td><p>29.0</p></td><td><p>49.7</p></td><td><p>59.1</p></td></tr><tr><th><p>Sparse R-CNN [38]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>36e</p></th><th></th><td><p>48.9</p></td><td><p>68.3</p></td><td><p>53.4</p></td><td><p>29.9</p></td><td><p>50.9</p></td><td><p>62.4</p></td></tr><tr><th><p>Deformable DETR [51]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>50e</p></th><th></th><td><p>50.1</p></td><td><p>69.7</p></td><td><p>54.6</p></td><td><p>30.6</p></td><td><p>52.8</p></td><td><p>64.7</p></td></tr><tr><th><p>Ours - R(Det){}^{2}</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>12e</p></th><th></th><td>50.0</td><td>69.2</td><td>54.3</td><td>30.9</td><td>53.0</td><td>63.9</td></tr><tr><th><p>Ours - R(Det){}^{2}</p></th><th><p>Swin-L [26]</p></th><th><p>12e</p></th><th></th><td>55.1</td><td>74.1</td><td>60.4</td><td>36.0</td><td>58.6</td><td>70.0</td></tr><tr><th><p>Centernet [11]</p></th><th><p>Hourglass-104</p></th><th><p>100e</p></th><th><p>\u2713</p></th><td><p>47.0</p></td><td><p>64.5</p></td><td><p>50.7</p></td><td><p>28.9</p></td><td><p>49.9</p></td><td><p>58.9</p></td></tr><tr><th><p>ATSS [47]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th><p>\u2713</p></th><td><p>50.7</p></td><td><p>68.9</p></td><td><p>56.3</p></td><td><p>33.2</p></td><td><p>52.9</p></td><td><p>62.4</p></td></tr><tr><th><p>IQDet [28]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th><p>\u2713</p></th><td><p>51.6</p></td><td><p>68.7</p></td><td><p>57.0</p></td><td><p>34.5</p></td><td><p>53.6</p></td><td><p>64.5</p></td></tr><tr><th><p>OTA [14]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th><p>\\checkmark</p></th><td><p>51.5</p></td><td><p>68.6</p></td><td><p>57.1</p></td><td><p>34.1</p></td><td><p>53.7</p></td><td><p>64.1</p></td></tr><tr><th><p>Dynamic R-CNN [46]</p></th><th><p>ResNet-101-DCN</p></th><th><p>36e</p></th><th><p>\u2713</p></th><td><p>50.1</p></td><td><p>68.3</p></td><td><p>55.6</p></td><td><p>32.8</p></td><td><p>53.0</p></td><td><p>61.2</p></td></tr><tr><th><p>TSD [37]</p></th><th><p>SENet154-DCN</p></th><th><p>36e</p></th><th><p>\u2713</p></th><td><p>51.2</p></td><td><p>71.9</p></td><td><p>56.0</p></td><td><p>33.8</p></td><td><p>54.8</p></td><td><p>64.2</p></td></tr><tr><th><p>Sparse R-CNN [38]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>36e</p></th><th><p>\u2713</p></th><td><p>51.5</p></td><td><p>71.1</p></td><td><p>57.1</p></td><td><p>34.2</p></td><td><p>53.4</p></td><td><p>64.1</p></td></tr><tr><th><p>RepPoints v2 [5]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th><p>\u2713</p></th><td><p>52.1</p></td><td><p>70.1</p></td><td><p>57.5</p></td><td><p>34.5</p></td><td><p>54.6</p></td><td><p>63.6</p></td></tr><tr><th><p>Deformable DETR [51]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>50e</p></th><th><p>\u2713</p></th><td><p>52.3</p></td><td><p>71.9</p></td><td><p>58.1</p></td><td><p>34.4</p></td><td><p>54.4</p></td><td><p>65.6</p></td></tr><tr><th><p>RelationNet++ [6]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th><p>\u2713</p></th><td><p>52.7</p></td><td><p>70.4</p></td><td><p>58.3</p></td><td><p>35.8</p></td><td><p>55.3</p></td><td><p>64.7</p></td></tr><tr><th><p>DyHead [8]</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th><p>\u2713</p></th><td><p>54.0</p></td><td><p>72.1</p></td><td><p>59.3</p></td><td><p>37.1</p></td><td><p>57.2</p></td><td><p>66.3</p></td></tr><tr><th><p>Ours - R(Det){}^{2}</p></th><th><p>ResNeXt-101-DCN</p></th><th><p>24e</p></th><th><p>\u2713</p></th><td>54.1</td><td>72.4</td><td>59.4</td><td>35.5</td><td>57.0</td><td>67.3</td></tr><tr><th><p>Ours - R(Det){}^{2}</p></th><th><p>Swin-L [26]</p></th><th><p>12e</p></th><th><p>\u2713</p></th><td>57.4</td><td>76.1</td><td>63.0</td><td>39.4</td><td>60.5</td><td>71.5</td></tr></tbody></table>", "caption": "Table 6: Comparison of R(Det){}^{2} with the state-of-the-art object detection methods on COCO test-dev dataset. DCN indicates that using the deformable convolution to enhance the feature representations of backbone. TTA indicates test-time augmentation such as multi-scale testing and horizontal image flipping. ME indicates more epochs of training. ", "list_citation_info": ["[14] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun. Ota: Optimal transport assignment for object detection. CVPR, pages 303\u2013312, 2021.", "[38] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse r-cnn: End-to-end object detection with learnable proposals. CVPR, pages 14454\u201314463, 2021.", "[37] Guanglu Song, Yu Liu, and Xiaogang Wang. Revisiting the sibling head in object detector. CVPR, pages 11563\u201311572, 2020.", "[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ICCV, pages 10012\u201310022, 2021.", "[8] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. CVPR, pages 7373\u20137382, 2021.", "[11] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. ICCV, pages 6569\u20136578, 2019.", "[39] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set prediction for object detection. ICCV, pages 3611\u20133620, 2021.", "[6] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Bridging visual representations for object detection via transformer decoder. NIPS, pages 13564\u201313574, 2020.", "[46] Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. Dynamic r-cnn: Towards high quality object detection via dynamic training. ECCV, pages 260\u2013275, 2020.", "[51] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. ICLR, 2021.", "[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Trans. on Pat. Anal. and Mach. Intell., 39(6):1137\u20131149, 2017.", "[30] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards balanced learning for object detection. CVPR, pages 821\u2013830, 2019.", "[47] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. CVPR, pages 9759\u20139768, 2020.", "[40] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. ICCV, pages 9627\u20139636, 2019.", "[1] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. CVPR, pages 6154\u20136162, 2018.", "[5] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, and Han Hu. Reppoints v2: Verification meets regression for object detection. NIPS, pages 5621\u20135631, 2020.", "[28] Yuchen Ma, Songtao Liu, Zeming Li, and Jian Sun. Iqdet: Instance-wise quality distribution sampling for object detection. CVPR, pages 1717\u20131725, 2021.", "[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. ICCV, pages 2980\u20132988, 2017."]}], "citation_info_to_title": {"[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Trans. on Pat. Anal. and Mach. Intell., 39(6):1137\u20131149, 2017.": "Faster r-cnn: Towards real-time object detection with region proposal networks", "[14] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun. Ota: Optimal transport assignment for object detection. CVPR, pages 303\u2013312, 2021.": "Ota: Optimal transport assignment for object detection", "[46] Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. Dynamic r-cnn: Towards high quality object detection via dynamic training. ECCV, pages 260\u2013275, 2020.": "Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training", "[6] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Bridging visual representations for object detection via transformer decoder. NIPS, pages 13564\u201313574, 2020.": "Relationnet++: Bridging Visual Representations for Object Detection via Transformer Decoder", "[47] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. CVPR, pages 9759\u20139768, 2020.": "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection", "[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. ICCV, pages 10012\u201310022, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[38] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, and Ping Luo. Sparse r-cnn: End-to-end object detection with learnable proposals. CVPR, pages 14454\u201314463, 2021.": "Sparse R-CNN: End-to-End Object Detection with Learnable Proposals", "[45] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and Thomas Huang. Unitbox: An advanced object detection network. Proceedings of the 24th ACM international conference on Multimedia, pages 516\u2013520, 2016.": "Unitbox: An advanced object detection network", "[8] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. CVPR, pages 7373\u20137382, 2021.": "Dynamic head: Unifying object detection heads with attentions", "[5] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, and Han Hu. Reppoints v2: Verification meets regression for object detection. NIPS, pages 5621\u20135631, 2020.": "Reppoints v2: Verification meets regression for object detection", "[11] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. ICCV, pages 6569\u20136578, 2019.": "Centernet: Keypoint triplets for object detection", "[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. ICCV, pages 2980\u20132988, 2017.": "Focal loss for dense object detection", "[40] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. ICCV, pages 9627\u20139636, 2019.": "FCOS: Fully Convolutional One-Stage Object Detection", "[51] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. ICLR, 2021.": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "[1] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. CVPR, pages 6154\u20136162, 2018.": "Cascade r-cnn: Delving into high quality object detection", "[28] Yuchen Ma, Songtao Liu, Zeming Li, and Jian Sun. Iqdet: Instance-wise quality distribution sampling for object detection. CVPR, pages 1717\u20131725, 2021.": "Iqdet: Instance-wise quality distribution sampling for object detection", "[39] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set prediction for object detection. ICCV, pages 3611\u20133620, 2021.": "Rethinking transformer-based set prediction for object detection", "[30] Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards balanced learning for object detection. CVPR, pages 821\u2013830, 2019.": "Libra r-cnn: Towards Balanced Learning for Object Detection", "[43] Yue Wu, Yinpeng Chen, Lu Yuan, Zicheng Liu, Lijuan Wang, Hongzhi Li, and Yun Fu. Rethinking classification and localization for object detection. CVPR, pages 10186\u201310195, 2020.": "Rethinking classification and localization for object detection", "[37] Guanglu Song, Yu Liu, and Xiaogang Wang. Revisiting the sibling head in object detector. CVPR, pages 11563\u201311572, 2020.": "Revisiting the sibling head in object detector"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Dynamic head: Unifying object detection heads with attentions": "2106.08322", "Rethinking transformer-based set prediction for object detection": "2011.10881"}}