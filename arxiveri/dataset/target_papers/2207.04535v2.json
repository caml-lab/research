{"title": "Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion", "abstract": "Attention-based models such as transformers have shown outstanding\nperformance on dense prediction tasks, such as semantic segmentation, owing to\ntheir capability of capturing long-range dependency in an image. However, the\nbenefit of transformers for monocular depth prediction has seldom been explored\nso far. This paper benchmarks various transformer-based models for the depth\nestimation task on an indoor NYUV2 dataset and an outdoor KITTI dataset. We\npropose a novel attention-based architecture, Depthformer for monocular depth\nestimation that uses multi-head self-attention to produce the multiscale\nfeature maps, which are effectively combined by our proposed decoder network.\nWe also propose a Transbins module that divides the depth range into bins whose\ncenter value is estimated adaptively per image. The final depth estimated is a\nlinear combination of bin centers for each pixel. Transbins module takes\nadvantage of the global receptive field using the transformer module in the\nencoding stage. Experimental results on NYUV2 and KITTI depth estimation\nbenchmark demonstrate that our proposed method improves the state-of-the-art by\n3.3%, and 3.3% respectively in terms of Root Mean Squared Error (RMSE). Code is\navailable at https://github.com/ashutosh1807/Depthformer.git.", "authors": ["Ashutosh Agarwal", "Chetan Arora"], "published_date": "2022_07_10", "pdf_url": "http://arxiv.org/pdf/2207.04535v2", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>RMSE\\downarrow</th><th>Rel\\downarrow</th><th>\\delta_{1}\\uparrow</th><th>\\delta_{2}\\uparrow</th><th>\\delta_{3}\\uparrow</th></tr></thead><tbody><tr><th>Eigen et al. [1]</th><td>0.641</td><td>0.158</td><td>0.769</td><td>0.95</td><td>0.988</td></tr><tr><th>DORN [2]</th><td>0.509</td><td>0.115</td><td>0.828</td><td>0.965</td><td>0.992</td></tr><tr><th>Chen et al. [5]</th><td>0.514</td><td>0.111</td><td>0.878</td><td>0.977</td><td>0.994</td></tr><tr><th>VNL [23]</th><td>0.416</td><td>0.108</td><td>0.875</td><td>0.976</td><td>0.994</td></tr><tr><th>BTS [3]</th><td>0.392</td><td>0.110</td><td>0.885</td><td>0.978</td><td>0.994</td></tr><tr><th>DAV [4]</th><td>0.412</td><td>0.108</td><td>0.882</td><td>0.980</td><td>0.996</td></tr><tr><th>DPT-Hybrid [18]</th><td>0.357</td><td>0.110</td><td>0.904</td><td>0.988</td><td>0.998</td></tr><tr><th>Adabins [19]</th><td>0.364</td><td>0.103</td><td>0.903</td><td>0.984</td><td>0.997</td></tr><tr><th>Depthformer (ours)</th><td>0.345</td><td>0.100</td><td>0.911</td><td>0.988</td><td>0.997</td></tr></tbody></table>", "caption": "Table 1: Results on NYUV2 Dataset. The best results are in bold and second best are underlined. Our method outperforms the previous SoTA methodsin most of the metrics. ", "list_citation_info": ["[3] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh, \u201cFrom Big To small: Multi-scale Local Planar Guidance For Monocular Depth Estimation,\u201d .", "[5] Xiaotian Chen, Xuejin Chen, and Zheng-Jun Zha, \u201cStructure-Aware Residual Pyramid Network For Monocular Depth Estimation,\u201d in IJCAI-19.", "[19] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka, \u201cAdaBins: Depth Estimation Using Adaptive Bins,\u201d in CVPR 2021.", "[18] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun, \u201cVision Transformers For Dense Prediction,\u201d in ICCV 2021.", "[2] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao, \u201cDeep Ordinal Regression Network For Monocular Depth Estimation,\u201d in CVPR 2018.", "[23] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan, \u201cEnforcing Geometric Constraints Of Virtual Normal For Depth Prediction,\u201d in ICCV 2019.", "[4] Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, and Janne Heikkil\u00e4, \u201cGuiding Monocular Depth Estimation Using Depth-Attention Volume,\u201d in ECCV 2020.", "[1] David Eigen, Christian Puhrsch, and Rob Fergus, \u201cDepthmap Prediction From A Single Image Using A Multi-scale Deep network,\u201d in NIPS 2014."]}, {"table": "<table><thead><tr><th>Method</th><th>RMSE\\downarrow</th><th>Rel\\downarrow</th><th>\\delta_{1}\\uparrow</th><th>\\delta_{2}\\uparrow</th><th>\\delta_{3}\\uparrow</th></tr></thead><tbody><tr><th>Eigen et al. [1]</th><td>6.307</td><td>0.203</td><td>0.702</td><td>0.898</td><td>0.967</td></tr><tr><th>Goddard et al. [6]</th><td>4.935</td><td>0.114</td><td>0.861</td><td>0.949</td><td>0.976</td></tr><tr><th>Gan et al. [9]</th><td>3.933</td><td>0.098</td><td>0.890</td><td>0.964</td><td>0.985</td></tr><tr><th>DORN [2]</th><td>2.727</td><td>0.072</td><td>0.932</td><td>0.984</td><td>0.994</td></tr><tr><th>Yin et al. [23]</th><td>3.258</td><td>0.072</td><td>0.938</td><td>0.990</td><td>0.998</td></tr><tr><th>BTS [3]</th><td>2.756</td><td>0.059</td><td>0.956</td><td>0.993</td><td>0.998</td></tr><tr><th>DPT-Hybrid [18]</th><td>2.573</td><td>0.062</td><td>0.959</td><td>0.995</td><td>0.999</td></tr><tr><th>Adabins [19]</th><td>2.360</td><td>0.058</td><td>0.964</td><td>0.995</td><td>0.999</td></tr><tr><th>Depthformer (ours)</th><td>2.285</td><td>0.058</td><td>0.967</td><td>0.996</td><td>0.999</td></tr></tbody></table>", "caption": "Table 2: Results on KITTI Dataset. The best results are in bold and second best are underlined. ", "list_citation_info": ["[3] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh, \u201cFrom Big To small: Multi-scale Local Planar Guidance For Monocular Depth Estimation,\u201d .", "[19] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka, \u201cAdaBins: Depth Estimation Using Adaptive Bins,\u201d in CVPR 2021.", "[18] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun, \u201cVision Transformers For Dense Prediction,\u201d in ICCV 2021.", "[2] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao, \u201cDeep Ordinal Regression Network For Monocular Depth Estimation,\u201d in CVPR 2018.", "[9] Yukang Gan, Xiangyu Xu, Wenxiu Sun, and Liang Lin, \u201cMonocular Depth Estimation With Affinity, Vertical Pooling, And Label Enhancement,\u201d in ECCV 2018, Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, Eds.", "[6] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic, \u201cUnsupervised Video Summarization With Adversarial LSTM Networks,\u201d in CVPR 2017.", "[23] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan, \u201cEnforcing Geometric Constraints Of Virtual Normal For Depth Prediction,\u201d in ICCV 2019.", "[1] David Eigen, Christian Puhrsch, and Rob Fergus, \u201cDepthmap Prediction From A Single Image Using A Multi-scale Deep network,\u201d in NIPS 2014."]}, {"table": "<table><thead><tr><th>Method</th><th>Reference</th><th>Param. (M)</th><th>RMSE\\downarrow</th><th>REL\\downarrow</th></tr></thead><tbody><tr><th>ResNet-50 [27]</th><td>CVPR\u201916</td><td>23.5</td><td>0.510</td><td>0.152</td></tr><tr><th>PVTv1 [12]</th><td>ICCV\u201921</td><td>23.9</td><td>0.508</td><td>0.166</td></tr><tr><th>Swin-T [15]</th><td>ICCV\u201921</td><td>27.5</td><td>0.456</td><td>0.142</td></tr><tr><th>Twins-SVT-S [16]</th><td>NeurIPS\u201921</td><td>23.5</td><td>0.443</td><td>0.141</td></tr><tr><th>MiT-B2 [11]</th><td>NeurIPS\u201921</td><td>24.2</td><td>0.394</td><td>0.118</td></tr><tr><th>MPVIT [17]</th><td>CVPR\u201922</td><td>22.6</td><td>0.403</td><td>0.120</td></tr></tbody></table>", "caption": "Table 3: Performance of different state of the art multiscale-vision transformers for monocular depth estimation on the benchmark NYUV2 dataset.", "list_citation_info": ["[17] Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju Hwang, \u201cMPViT: Multi-Path Vision Transformer For Dense Prediction,\u201d in CVPR 2022.", "[12] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao, \u201cPyramid Vision Transformer: A Versatile Backbone For Dense Prediction Without Convolutions,\u201d in ICCV 2021.", "[16] Xiangxiang Chu, \u201cTwins: Revisiting The Design Of Spatial Attention In Vision Transformers ,\u201d in NIPS 2021.", "[11] Enze Xie, \u201cSegFormer: Simple and Efficient Design For Semantic Segmentation With Transformers,\u201d in NIPS 2021.", "[15] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo, \u201cSwin Transformer: Hierarchical Vision Transformer Using Shifted Windows,\u201d in ICCV 2021.", "[27] Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, and Trevor Darrell, \u201cDeep Compositional Captioning: Describing Novel Object Categories Without Paired Training Data,\u201d in CVPR 2016."]}, {"table": "<table><thead><tr><th>Method</th><th>RMSE\\downarrow</th><th>REL\\downarrow</th></tr></thead><tbody><tr><th>Decoder Xie et al. ([11])</th><td>0.375</td><td>0.114</td></tr><tr><th>Decoder(ours) + GAP</th><td>0.350</td><td>0.105</td></tr><tr><th>Decoder(ours) + Adabins [19]</th><td>0.394</td><td>0.115</td></tr><tr><th>Decoder(ours) + Transbins</th><td>0.345</td><td>0.100</td></tr></tbody></table>", "caption": "Table 4: Ablation study on NYUV2 dataset for different decoder designs in the proposed Depthformer model.", "list_citation_info": ["[11] Enze Xie, \u201cSegFormer: Simple and Efficient Design For Semantic Segmentation With Transformers,\u201d in NIPS 2021.", "[19] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka, \u201cAdaBins: Depth Estimation Using Adaptive Bins,\u201d in CVPR 2021."]}], "citation_info_to_title": {"[18] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun, \u201cVision Transformers For Dense Prediction,\u201d in ICCV 2021.": "Vision Transformers For Dense Prediction", "[1] David Eigen, Christian Puhrsch, and Rob Fergus, \u201cDepthmap Prediction From A Single Image Using A Multi-scale Deep network,\u201d in NIPS 2014.": "Depthmap Prediction From A Single Image Using A Multi-scale Deep network", "[17] Youngwan Lee, Jonghee Kim, Jeffrey Willette, and Sung Ju Hwang, \u201cMPViT: Multi-Path Vision Transformer For Dense Prediction,\u201d in CVPR 2022.": "MPViT: Multi-Path Vision Transformer For Dense Prediction", "[5] Xiaotian Chen, Xuejin Chen, and Zheng-Jun Zha, \u201cStructure-Aware Residual Pyramid Network For Monocular Depth Estimation,\u201d in IJCAI-19.": "Structure-Aware Residual Pyramid Network For Monocular Depth Estimation", "[9] Yukang Gan, Xiangyu Xu, Wenxiu Sun, and Liang Lin, \u201cMonocular Depth Estimation With Affinity, Vertical Pooling, And Label Enhancement,\u201d in ECCV 2018, Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, Eds.": "Monocular Depth Estimation With Affinity, Vertical Pooling, And Label Enhancement", "[4] Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Esa Rahtu, and Janne Heikkil\u00e4, \u201cGuiding Monocular Depth Estimation Using Depth-Attention Volume,\u201d in ECCV 2020.": "Guiding Monocular Depth Estimation Using Depth-Attention Volume", "[19] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka, \u201cAdaBins: Depth Estimation Using Adaptive Bins,\u201d in CVPR 2021.": "AdaBins: Depth Estimation Using Adaptive Bins", "[23] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan, \u201cEnforcing Geometric Constraints Of Virtual Normal For Depth Prediction,\u201d in ICCV 2019.": "Enforcing Geometric Constraints Of Virtual Normal For Depth Prediction", "[2] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao, \u201cDeep Ordinal Regression Network For Monocular Depth Estimation,\u201d in CVPR 2018.": "Deep Ordinal Regression Network For Monocular Depth Estimation", "[12] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao, \u201cPyramid Vision Transformer: A Versatile Backbone For Dense Prediction Without Convolutions,\u201d in ICCV 2021.": "Pyramid Vision Transformer: A Versatile Backbone For Dense Prediction Without Convolutions", "[16] Xiangxiang Chu, \u201cTwins: Revisiting The Design Of Spatial Attention In Vision Transformers ,\u201d in NIPS 2021.": "Twins: Revisiting The Design Of Spatial Attention In Vision Transformers", "[11] Enze Xie, \u201cSegFormer: Simple and Efficient Design For Semantic Segmentation With Transformers,\u201d in NIPS 2021.": "SegFormer: Simple and Efficient Design For Semantic Segmentation With Transformers", "[15] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo, \u201cSwin Transformer: Hierarchical Vision Transformer Using Shifted Windows,\u201d in ICCV 2021.": "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows", "[27] Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, and Trevor Darrell, \u201cDeep Compositional Captioning: Describing Novel Object Categories Without Paired Training Data,\u201d in CVPR 2016.": "Deep Compositional Captioning: Describing Novel Object Categories Without Paired Training Data", "[3] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh, \u201cFrom Big To small: Multi-scale Local Planar Guidance For Monocular Depth Estimation,\u201d .": "From Big To small: Multi-scale Local Planar Guidance For Monocular Depth Estimation", "[6] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic, \u201cUnsupervised Video Summarization With Adversarial LSTM Networks,\u201d in CVPR 2017.": "Unsupervised Video Summarization With Adversarial LSTM Networks"}, "source_title_to_arxiv_id": {"Vision Transformers For Dense Prediction": "2102.12122", "AdaBins: Depth Estimation Using Adaptive Bins": "2011.14141", "Pyramid Vision Transformer: A Versatile Backbone For Dense Prediction Without Convolutions": "2102.12122", "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows": "2103.14030"}}