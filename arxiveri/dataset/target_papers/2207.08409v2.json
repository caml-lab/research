{"title": "TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers", "abstract": "CutMix is a popular augmentation technique commonly used for training modern\nconvolutional and transformer vision networks. It was originally designed to\nencourage Convolution Neural Networks (CNNs) to focus more on an image's global\ncontext instead of local information, which greatly improves the performance of\nCNNs. However, we found it to have limited benefits for transformer-based\narchitectures that naturally have a global receptive field. In this paper, we\npropose a novel data augmentation technique TokenMix to improve the performance\nof vision transformers. TokenMix mixes two images at token level via\npartitioning the mixing region into multiple separated parts. Besides, we show\nthat the mixed learning target in CutMix, a linear combination of a pair of the\nground truth labels, might be inaccurate and sometimes counter-intuitive. To\nobtain a more suitable target, we propose to assign the target score according\nto the content-based neural activation maps of the two images from a\npre-trained teacher model, which does not need to have high performance. With\nplenty of experiments on various vision transformer architectures, we show that\nour proposed TokenMix helps vision transformers focus on the foreground area to\ninfer the classes and enhances their robustness to occlusion, with consistent\nperformance gains. Notably, we improve DeiT-T/S/B with +1% ImageNet top-1\naccuracy. Besides, TokenMix enjoys longer training, which achieves 81.2% top-1\naccuracy on ImageNet with DeiT-S trained for 400 epochs. Code is available at\nhttps://github.com/Sense-X/TokenMix.", "authors": ["Jihao Liu", "Boxiao Liu", "Hang Zhou", "Hongsheng Li", "Yu Liu"], "published_date": "2022_07_18", "pdf_url": "http://arxiv.org/pdf/2207.08409v2", "list_table_and_caption": [{"table": "<table><tr><td>Model</td><td> #FLOPs (G) </td><td> #Params (M) </td><td>CutMix</td><td>TokenMix</td></tr><tr><td>DeiT-T [29]</td><td>1.3</td><td>5.7</td><td>72.2</td><td>73.2 (+1.0)</td></tr><tr><td>PVT-T [34]</td><td>1.9</td><td>13.2</td><td>75.1</td><td>75.6 (+0.5)</td></tr><tr><td>CaiT-XXS-24 [30]</td><td>2.5</td><td>9.5</td><td>77.6</td><td>78.0 (+0.4)</td></tr><tr><td>DeiT-S [29]</td><td>4.6</td><td>22.1</td><td>79.8</td><td>80.8 (+1.0)</td></tr><tr><td>Swin-T [21]</td><td>4.5</td><td>29</td><td>81.2</td><td>81.6 (+0.4)</td></tr><tr><td>DeiT-B [29]</td><td>17.6</td><td>86.6</td><td>81.8</td><td>82.9 (+1.1)</td></tr></table>", "caption": "Table 1: ImageNet classification performances based on various transformer-based architectures. TokenMix consistently improves DeiT for \\sim 1% top-1 accuracy with nearly no extra training overhead.", "list_citation_info": ["[30] Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., J\u00e9gou, H.: Going deeper with image transformers. arXiv preprint arXiv:2103.17239 (2021)", "[34] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122 (2021)", "[21] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)", "[29] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning. pp. 10347\u201310357. PMLR (2021)"]}, {"table": "<table><tr><td>Mixup [38]</td><td>CutMix [36]</td><td>TokenMix</td><td>Top-1 Acc.</td></tr><tr><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>75.8</td></tr><tr><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>78.7</td></tr><tr><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>80.0</td></tr><tr><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>81.5</td></tr><tr><td>\u2713</td><td>\u2713</td><td>\u2717</td><td>81.8</td></tr><tr><td>\u2717</td><td>\u2713</td><td>\u2713</td><td>82.0</td></tr><tr><td>\u2713</td><td>\u2717</td><td>\u2713</td><td>82.9</td></tr></table>", "caption": "Table 5: Performances of using a single or randomly sampled one of the multiple mixing methods for training DeiT-B.", "list_citation_info": ["[38] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017)", "[36] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6023\u20136032 (2019)"]}, {"table": "<table><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>AdamW [22]</td></tr><tr><td>learning rate</td><td>0.001</td></tr><tr><td>weight decay</td><td>0.05</td></tr><tr><td>batch size</td><td>1024</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td></tr><tr><td>warmup epochs</td><td>5</td></tr><tr><td>training epochs</td><td>300</td></tr><tr><td>augmentation</td><td>RandAug(9, 0.5) [9]</td></tr><tr><td>LabelSmooth [27]</td><td>0.1</td></tr><tr><td>DropPath [15]</td><td>0.1</td></tr><tr><td>Mixup <sup>\\ast</sup> [38]</td><td>0.8</td></tr><tr><td>CutMix <sup>\\ast</sup> [36]</td><td>1.0</td></tr><tr><td>TokenMix <sup>\\ast</sup></td><td>0.5</td></tr></table>", "caption": "Table 12: Training settings on ImageNet-1K. \\ast optional config.", "list_citation_info": ["[22] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)", "[36] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6023\u20136032 (2019)", "[15] Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth. In: European conference on computer vision. pp. 646\u2013661. Springer (2016)", "[38] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017)", "[27] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818\u20132826 (2016)", "[9] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 702\u2013703 (2020)"]}, {"table": "<table><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>Adam [19]</td></tr><tr><td>learning rate</td><td>0.001</td></tr><tr><td>weight decay</td><td>0.05</td></tr><tr><td>batch size</td><td>16</td></tr><tr><td>learning rate schedule</td><td>linear</td></tr><tr><td>warmup steps</td><td>1500</td></tr><tr><td>training steps</td><td>160K</td></tr><tr><td>input resolution</td><td>512 \\times 512</td></tr></table>", "caption": "Table 13: Training settings on ADE20K.", "list_citation_info": ["[19] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)"]}], "citation_info_to_title": {"[34] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122 (2021)": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "[22] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)": "Decoupled weight decay regularization", "[15] Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth. In: European conference on computer vision. pp. 646\u2013661. Springer (2016)": "Deep Networks with Stochastic Depth", "[21] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)": "Swin transformer: Hierarchical vision transformer using shifted windows", "[27] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818\u20132826 (2016)": "Rethinking the inception architecture for computer vision", "[30] Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., J\u00e9gou, H.: Going deeper with image transformers. arXiv preprint arXiv:2103.17239 (2021)": "Going deeper with image transformers", "[36] Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 6023\u20136032 (2019)": "Cutmix: Regularization strategy to train strong classifiers with localizable features", "[9] Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 702\u2013703 (2020)": "Randaugment: Practical automated data augmentation with a reduced search space", "[38] Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017)": "mixup: Beyond Empirical Risk Minimization", "[19] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)": "Adam: A method for stochastic optimization", "[29] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning. pp. 10347\u201310357. PMLR (2021)": "Training data-efficient image transformers & distillation through attention"}, "source_title_to_arxiv_id": {"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions": "2102.12122", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}