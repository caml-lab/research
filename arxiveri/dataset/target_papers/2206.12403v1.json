{"title": "ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings", "abstract": "We present a scalable approach for learning open-world object-goal navigation\n(ObjectNav) -- the task of asking a virtual robot (agent) to find any instance\nof an object in an unexplored environment (e.g., \"find a sink\"). Our approach\nis entirely zero-shot -- i.e., it does not require ObjectNav rewards or\ndemonstrations of any kind. Instead, we train on the image-goal navigation\n(ImageNav) task, in which agents find the location where a picture (i.e., goal\nimage) was captured. Specifically, we encode goal images into a multimodal,\nsemantic embedding space to enable training semantic-goal navigation\n(SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D).\nAfter training, SemanticNav agents can be instructed to find objects described\nin free-form natural language (e.g., \"sink\", \"bathroom sink\", etc.) by\nprojecting language goals into the same multimodal, semantic embedding space.\nAs a result, our approach enables open-world ObjectNav. We extensively evaluate\nour agents on three ObjectNav datasets (Gibson, HM3D, and MP3D) and observe\nabsolute improvements in success of 4.2% - 20.0% over existing zero-shot\nmethods. For reference, these gains are similar or better than the 5%\nimprovement in success between the Habitat 2020 and 2021 ObjectNav challenge\nwinners. In an open-world setting, we discover that our agents can generalize\nto compound instructions with a room explicitly mentioned (e.g., \"Find a\nkitchen sink\") and when the target room can be inferred (e.g., \"Find a sink and\na stove\").", "authors": ["Arjun Majumdar", "Gunjan Aggarwal", "Bhavika Devnani", "Judy Hoffman", "Dhruv Batra"], "published_date": "2022_06_24", "pdf_url": "http://arxiv.org/pdf/2206.12403v1", "list_table_and_caption": [{"table": "<table><tr><td></td><td></td><td></td><td colspan=\"2\"> ImageNav(Gibson) </td><td colspan=\"2\"> ObjectNav(Gibson) </td></tr><tr><td>Method</td><td> VisualEncoder </td><td> TrainingDataset </td><td>SPL</td><td>SR</td><td>SPL</td><td>SR</td></tr><tr><td>ZER Al-Halah et al. (2022)</td><td>ResNet-9</td><td>Gibson</td><td>21.6%</td><td>29.2%</td><td>-</td><td>11.3%</td></tr><tr><td>Ours</td><td>ResNet-9</td><td>Gibson</td><td>22.8%</td><td>33.3%</td><td>7.4%</td><td>15.3%</td></tr></table>", "caption": "Table 2: Comparison with ZER Al-Halah et al. (2022) using a ResNet-9 and the Gibson dataset with our approach. Learning SemanticNav (Ours) outperforms learning ImageNav then language grounding (ZER Al-Halah et al. (2022)).", "list_citation_info": ["Al-Halah et al. [2022] Ziad Al-Halah, Santhosh K Ramakrishnan, and Kristen Grauman. Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation. arXiv preprint arXiv:2202.02440, 2022."]}, {"table": "<table><tr><td>Method</td><td>Open-World</td><td>Zero-Shot</td></tr><tr><td> Fully-Supervised Methods Ramrakhya et al. (2022); Chaplot et al. (2020); Ye et al. (2021); Maksymets et al. (2021); Yadav et al. (2022a) </td><td>\u2717</td><td>\u2717</td></tr><tr><td>EmbCLIP Khandelwal et al. (2021)</td><td>\u2713</td><td>\u2717</td></tr><tr><td>ZER Al-Halah et al. (2022)</td><td>\u2717</td><td>\u2713</td></tr><tr><td>CoW Gadre et al. (2022)</td><td>\u2713</td><td>\u2713</td></tr><tr><td>ZSON (ours)</td><td>\u2713</td><td>\u2713</td></tr></table>", "caption": "Table 4: Comparison of ObjectNav methods. Open-world methods are not limited to a closed set of object categories. Zero-shot methods do not use ObjectNav annotations for training.", "list_citation_info": ["Gadre et al. [2022] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song. CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration. arXiv preprint arXiv:2203.10421, 2022.", "Khandelwal et al. [2021] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but Effective: CLIP Embeddings for Embodied AI. arXiv preprint arXiv:2111.09888, 2021.", "Ramrakhya et al. [2022] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In CVPR, 2022.", "Al-Halah et al. [2022] Ziad Al-Halah, Santhosh K Ramakrishnan, and Kristen Grauman. Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation. arXiv preprint arXiv:2202.02440, 2022."]}], "citation_info_to_title": {"Ramrakhya et al. [2022] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In CVPR, 2022.": "Habitat-web: Learning embodied object-search strategies from human demonstrations at scale", "Al-Halah et al. [2022] Ziad Al-Halah, Santhosh K Ramakrishnan, and Kristen Grauman. Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation. arXiv preprint arXiv:2202.02440, 2022.": "Zero Experience Required: Plug & Play Modular Transfer Learning for Semantic Visual Navigation", "Khandelwal et al. [2021] Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but Effective: CLIP Embeddings for Embodied AI. arXiv preprint arXiv:2111.09888, 2021.": "Simple but Effective: CLIP Embeddings for Embodied AI", "Gadre et al. [2022] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song. CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration. arXiv preprint arXiv:2203.10421, 2022.": "CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration"}, "source_title_to_arxiv_id": {"Habitat-web: Learning embodied object-search strategies from human demonstrations at scale": "2204.03514"}}