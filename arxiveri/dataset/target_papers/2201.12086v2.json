{"title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "abstract": "Vision-Language Pre-training (VLP) has advanced the performance for many\nvision-language tasks. However, most existing pre-trained models only excel in\neither understanding-based tasks or generation-based tasks. Furthermore,\nperformance improvement has been largely achieved by scaling up the dataset\nwith noisy image-text pairs collected from the web, which is a suboptimal\nsource of supervision. In this paper, we propose BLIP, a new VLP framework\nwhich transfers flexibly to both vision-language understanding and generation\ntasks. BLIP effectively utilizes the noisy web data by bootstrapping the\ncaptions, where a captioner generates synthetic captions and a filter removes\nthe noisy ones. We achieve state-of-the-art results on a wide range of\nvision-language tasks, such as image-text retrieval (+2.7% in average\nrecall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).\nBLIP also demonstrates strong generalization ability when directly transferred\nto video-language tasks in a zero-shot manner. Code, models, and datasets are\nreleased at https://github.com/salesforce/BLIP.", "authors": ["Junnan Li", "Dongxu Li", "Caiming Xiong", "Steven Hoi"], "published_date": "2022_01_28", "pdf_url": "http://arxiv.org/pdf/2201.12086v2", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Method</td><td>Pre-train</td><td colspan=\"6\">COCO (5K test set)</td><td colspan=\"6\">Flickr30K (1K test set)</td></tr><tr><td># Images</td><td colspan=\"3\">TR</td><td colspan=\"3\">IR</td><td colspan=\"3\">TR</td><td colspan=\"3\">IR</td></tr><tr><td></td><td></td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><td>UNITER (Chen et al., 2020)</td><td>4M</td><td>65.7</td><td>88.6</td><td>93.8</td><td>52.9</td><td>79.9</td><td>88.0</td><td>87.3</td><td>98.0</td><td>99.2</td><td>75.6</td><td>94.1</td><td>96.8</td></tr><tr><td>VILLA (Gan et al., 2020)</td><td>4M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>87.9</td><td>97.5</td><td>98.8</td><td>76.3</td><td>94.2</td><td>96.8</td></tr><tr><td>OSCAR (Li et al., 2020)</td><td>4M</td><td>70.0</td><td>91.1</td><td>95.5</td><td>54.0</td><td>80.8</td><td>88.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>UNIMO (Li et al., 2021b)</td><td>5.7M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>89.4</td><td>98.9</td><td>99.8</td><td>78.0</td><td>94.2</td><td>97.1</td></tr><tr><td>ALIGN (Jia et al., 2021)</td><td>1.8B</td><td>77.0</td><td>93.5</td><td>96.9</td><td>59.9</td><td>83.3</td><td>89.8</td><td>95.3</td><td>99.8</td><td>100.0</td><td>84.9</td><td>97.4</td><td>98.6</td></tr><tr><td>ALBEF (Li et al., 2021a)</td><td>14M</td><td>77.6</td><td>94.3</td><td>97.2</td><td>60.7</td><td>84.3</td><td>90.5</td><td>95.9</td><td>99.8</td><td>100.0</td><td>85.6</td><td>97.5</td><td>98.9</td></tr><tr><td>BLIP</td><td>14M</td><td>80.6</td><td>95.2</td><td>97.6</td><td>63.1</td><td>85.3</td><td>91.1</td><td>96.6</td><td>99.8</td><td>100.0</td><td>87.2</td><td>97.5</td><td>98.8</td></tr><tr><td>BLIP</td><td>129M</td><td>81.9</td><td>95.4</td><td>97.8</td><td>64.3</td><td>85.7</td><td>91.5</td><td>97.3</td><td>99.9</td><td>100.0</td><td>87.3</td><td>97.6</td><td>98.9</td></tr><tr><td>BLIP{}_{\\text{CapFilt-L}}</td><td>129M</td><td>81.2</td><td>95.7</td><td>97.9</td><td>64.1</td><td>85.8</td><td>91.6</td><td>97.2</td><td>99.9</td><td>100.0</td><td>87.5</td><td>97.7</td><td>98.9</td></tr><tr><td>BLIP{}_{\\text{ViT-L}}</td><td>129M</td><td>82.4</td><td>95.4</td><td>97.9</td><td>65.1</td><td>86.3</td><td>91.8</td><td>97.4</td><td>99.8</td><td>99.9</td><td>87.6</td><td>97.7</td><td>99.0</td></tr></table>", "caption": "Table 5: Comparison with state-of-the-art image-text retrieval methods,finetuned on COCO and Flickr30K datasets.BLIP{}_{\\text{CapFilt-L}} pre-trains a model with ViT-B backbone using a dataset bootstrapped by captioner and filter with ViT-L.", "list_citation_info": ["Jia et al. (2021) Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.", "Li et al. (2021a) Li, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong, C., and Hoi, S. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS, 2021a.", "Li et al. (2020) Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, pp. 121\u2013137, 2020.", "Gan et al. (2020) Gan, Z., Chen, Y., Li, L., Zhu, C., Cheng, Y., and Liu, J. Large-scale adversarial training for vision-and-language representation learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), NeurIPS, 2020.", "Chen et al. (2020) Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. UNITER: universal image-text representation learning. In ECCV, volume 12375, pp. 104\u2013120, 2020.", "Li et al. (2021b) Li, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu, H., and Wang, H. UNIMO: towards unified-modal understanding and generation via cross-modal contrastive learning. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), ACL, pp. 2592\u20132607, 2021b."]}, {"table": "<table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">Pre-train#Images </td><td colspan=\"8\">NoCaps validation</td><td colspan=\"2\">COCO Caption</td></tr><tr><td colspan=\"2\">in-domain</td><td colspan=\"2\">near-domain</td><td colspan=\"2\">out-domain</td><td colspan=\"2\">overall</td><td colspan=\"2\">Karpathy test</td></tr><tr><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>B@4</td><td>C</td></tr><tr><td>Enc-Dec (Changpinyo et al., 2021)</td><td>15M</td><td>92.6</td><td>12.5</td><td>88.3</td><td>12.1</td><td>94.5</td><td>11.9</td><td>90.2</td><td>12.1</td><td>-</td><td>110.9</td></tr><tr><td>VinVL\u2020 (Zhang et al., 2021)</td><td>5.7M</td><td>103.1</td><td>14.2</td><td>96.1</td><td>13.8</td><td>88.3</td><td>12.1</td><td>95.5</td><td>13.5</td><td>38.2</td><td>129.3</td></tr><tr><td>LEMON{}_{\\mathrm{base}}\u2020 (Hu et al., 2021)</td><td>12M</td><td>104.5</td><td>14.6</td><td>100.7</td><td>14.0</td><td>96.7</td><td>12.4</td><td>100.4</td><td>13.8</td><td>-</td><td>-</td></tr><tr><td>LEMON{}_{\\mathrm{base}}\u2020 (Hu et al., 2021)</td><td>200M</td><td>107.7</td><td>14.7</td><td>106.2</td><td>14.3</td><td>107.9</td><td>13.1</td><td>106.8</td><td>14.1</td><td>40.3</td><td>133.3</td></tr><tr><td>BLIP</td><td>14M</td><td>111.3</td><td>15.1</td><td>104.5</td><td>14.4</td><td>102.4</td><td>13.7</td><td>105.1</td><td>14.4</td><td>38.6</td><td>129.7</td></tr><tr><td>BLIP</td><td>129M</td><td>109.1</td><td>14.8</td><td>105.8</td><td>14.4</td><td>105.7</td><td>13.7</td><td>106.3</td><td>14.3</td><td>39.4</td><td>131.4</td></tr><tr><td>BLIP{}_{\\text{CapFilt-L}}</td><td>129M</td><td>111.8</td><td>14.9</td><td>108.6</td><td>14.8</td><td>111.5</td><td>14.2</td><td>109.6</td><td>14.7</td><td>39.7</td><td>133.3</td></tr><tr><td>LEMON{}_{\\mathrm{large}}\u2020 (Hu et al., 2021)</td><td>200M</td><td>116.9</td><td>15.8</td><td>113.3</td><td>15.1</td><td>111.3</td><td>14.0</td><td>113.4</td><td>15.0</td><td>40.6</td><td>135.7</td></tr><tr><td>SimVLM{}_{\\mathrm{huge}} (Wang et al., 2021)</td><td>1.8B</td><td>113.7</td><td>-</td><td>110.9</td><td>-</td><td>115.2</td><td>-</td><td>112.2</td><td>-</td><td>40.6</td><td>143.3</td></tr><tr><td>BLIP{}_{\\text{ViT-L}}</td><td>129M</td><td>114.9</td><td>15.2</td><td>112.1</td><td>14.9</td><td>115.3</td><td>14.4</td><td>113.2</td><td>14.8</td><td>40.4</td><td>136.7</td></tr></table>", "caption": "Table 7: Comparison with state-of-the-art image captioning methods on NoCaps and COCO Caption.All methods optimize the cross-entropy loss during finetuning. C: CIDEr, S: SPICE, B@4: BLEU@4. BLIP{}_{\\text{CapFilt-L}} is pre-trained on a dataset bootstrapped by captioner and filter with ViT-L.VinVL\u2020 and LEMON\u2020 require an object detector pre-trained on 2.5M images with human-annotated bounding boxes and high resolution (800\\times1333) input images.SimVLM{}_{\\mathrm{huge}} uses 13\\times more training data and a larger vision backbone than ViT-L.", "list_citation_info": ["Changpinyo et al. (2021) Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.", "Zhang et al. (2021) Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., and Gao, J. Vinvl: Making visual representations matter in vision-language models. arXiv preprint arXiv:2101.00529, 2021.", "Wang et al. (2021) Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.", "Hu et al. (2021) Hu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu, Y., and Wang, L. Scaling up vision-language pre-training for image captioning, 2021."]}, {"table": "<table><tr><td>Method</td><td>MRR\\uparrow</td><td>R@1\\uparrow</td><td>R@5\\uparrow</td><td>R@10\\uparrow</td><td>MR\\downarrow</td></tr><tr><td>VD-BERT</td><td>67.44</td><td>54.02</td><td>83.96</td><td>92.33</td><td>3.53</td></tr><tr><td>VD-ViLBERT\u2020</td><td>69.10</td><td>55.88</td><td>85.50</td><td>93.29</td><td>3.25</td></tr><tr><td>BLIP</td><td>69.41</td><td>56.44</td><td>85.90</td><td>93.30</td><td>3.20</td></tr></table>", "caption": "Table 9: Comparison with state-of-the-art methods on VisDial v1.0 validation set. VD-ViLBERT\u2020 (Murahari et al., 2020) pre-trains ViLBERT (Lu et al., 2019) with additional VQA data.", "list_citation_info": ["Murahari et al. (2020) Murahari, V., Batra, D., Parikh, D., and Das, A. Large-scale pretraining for visual dialog: A simple state-of-the-art baseline. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.), ECCV, pp. 336\u2013352, 2020.", "Lu et al. (2019) Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R. (eds.), NeurIPS, pp. 13\u201323, 2019."]}, {"table": "<table><tr><td>Method</td><td>R1\\uparrow</td><td>R5\\uparrow</td><td>R10\\uparrow</td><td>MdR\\downarrow</td></tr><tr><td colspan=\"3\">zero-shot</td><td></td><td></td></tr><tr><td>ActBERT (Zhu &amp; Yang, 2020)</td><td>8.6</td><td>23.4</td><td>33.1</td><td>36</td></tr><tr><td>SupportSet (Patrick et al., 2021)</td><td>8.7</td><td>23.0</td><td>31.1</td><td>31</td></tr><tr><td>MIL-NCE (Miech et al., 2020)</td><td>9.9</td><td>24.0</td><td>32.4</td><td>29.5</td></tr><tr><td>VideoCLIP (Xu et al., 2021)</td><td>10.4</td><td>22.2</td><td>30.0</td><td>-</td></tr><tr><td>FiT (Bain et al., 2021)</td><td>18.7</td><td>39.5</td><td>51.6</td><td>10</td></tr><tr><td>BLIP</td><td>43.3</td><td>65.6</td><td>74.7</td><td>2</td></tr><tr><td colspan=\"3\">finetuning</td><td></td><td></td></tr><tr><td>ClipBERT (Lei et al., 2021)</td><td>22.0</td><td>46.8</td><td>59.9</td><td>6</td></tr><tr><td>VideoCLIP (Xu et al., 2021)</td><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr></table>", "caption": "Table 10: Comparisons with state-of-the-art methods for text-to-video retrieval on the 1k test split of the MSRVTT dataset.", "list_citation_info": ["Bain et al. (2021) Bain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.", "Miech et al. (2020) Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J., and Zisserman, A. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, pp. 9879\u20139889, 2020.", "Patrick et al. (2021) Patrick, M., Huang, P.-Y., Asano, Y., Metze, F., Hauptmann, A. G., Henriques, J. F., and Vedaldi, A. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.", "Xu et al. (2021) Xu, H., Ghosh, G., Huang, P.-Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., and Feichtenhofer, C. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In EMNLP, pp. 6787\u20136800, 2021.", "Zhu & Yang (2020) Zhu, L. and Yang, Y. Actbert: Learning global-local video-text representations. In CVPR, pp. 8746\u20138755, 2020.", "Lei et al. (2021) Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T. L., Bansal, M., and Liu, J. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pp. 7331\u20137341, 2021."]}, {"table": "<table><tr><td>Method</td><td>MSRVTT-QA</td><td>MSVD-QA</td></tr><tr><td colspan=\"3\">zero-shot</td></tr><tr><td>VQA-T (Yang et al., 2021)</td><td>2.9</td><td>7.5</td></tr><tr><td>BLIP</td><td>19.2</td><td>35.2</td></tr><tr><td colspan=\"3\">finetuning</td></tr><tr><td>HME (Fan et al., 2019)</td><td>33.0</td><td>33.7</td></tr><tr><td>HCRN (Le et al., 2020)</td><td>35.6</td><td>36.1</td></tr><tr><td>VQA-T (Yang et al., 2021)</td><td>41.5</td><td>46.3</td></tr></table>", "caption": "Table 11: Comparisons with state-of-the-art methods for video question answering. We report top-1 test accuracy on two datasets.", "list_citation_info": ["Fan et al. (2019) Fan, C., Zhang, X., Zhang, S., Wang, W., Zhang, C., and Huang, H. Heterogeneous memory enhanced multimodal attention model for video question answering. In CVPR, pp. 1999\u20132007, 2019.", "Le et al. (2020) Le, T. M., Le, V., Venkatesh, S., and Tran, T. Hierarchical conditional relation networks for video question answering. In CVPR, pp. 9972\u20139981, 2020.", "Yang et al. (2021) Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. Just ask: Learning to answer questions from millions of narrated videos. In ICCV, pp. 1686\u20131697, 2021."]}], "citation_info_to_title": {"Le et al. (2020) Le, T. M., Le, V., Venkatesh, S., and Tran, T. Hierarchical conditional relation networks for video question answering. In CVPR, pp. 9972\u20139981, 2020.": "Hierarchical conditional relation networks for video question answering", "Murahari et al. (2020) Murahari, V., Batra, D., Parikh, D., and Das, A. Large-scale pretraining for visual dialog: A simple state-of-the-art baseline. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.), ECCV, pp. 336\u2013352, 2020.": "Large-scale pretraining for visual dialog: A simple state-of-the-art baseline", "Li et al. (2020) Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, pp. 121\u2013137, 2020.": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "Lei et al. (2021) Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T. L., Bansal, M., and Liu, J. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pp. 7331\u20137341, 2021.": "Less is more: Clipbert for video-and-language learning via sparse sampling", "Gan et al. (2020) Gan, Z., Chen, Y., Li, L., Zhu, C., Cheng, Y., and Liu, J. Large-scale adversarial training for vision-and-language representation learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), NeurIPS, 2020.": "Large-scale adversarial training for vision-and-language representation learning", "Jia et al. (2021) Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint arXiv:2102.05918, 2021.": "Scaling up visual and vision-language representation learning with noisy text supervision", "Chen et al. (2020) Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z., Cheng, Y., and Liu, J. UNITER: universal image-text representation learning. In ECCV, volume 12375, pp. 104\u2013120, 2020.": "UNITER: Universal Image-Text Representation Learning", "Xu et al. (2021) Xu, H., Ghosh, G., Huang, P.-Y., Okhonko, D., Aghajanyan, A., Metze, F., Zettlemoyer, L., and Feichtenhofer, C. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In EMNLP, pp. 6787\u20136800, 2021.": "Videoclip: Contrastive pre-training for zero-shot video-text understanding", "Wang et al. (2021) Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao, Y. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.": "Simvlm: Simple visual language model pretraining with weak supervision", "Fan et al. (2019) Fan, C., Zhang, X., Zhang, S., Wang, W., Zhang, C., and Huang, H. Heterogeneous memory enhanced multimodal attention model for video question answering. In CVPR, pp. 1999\u20132007, 2019.": "Heterogeneous memory enhanced multimodal attention model for video question answering", "Hu et al. (2021) Hu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu, Y., and Wang, L. Scaling up vision-language pre-training for image captioning, 2021.": "Scaling up vision-language pre-training for image captioning", "Miech et al. (2020) Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J., and Zisserman, A. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, pp. 9879\u20139889, 2020.": "End-to-end learning of visual representations from uncurated instructional videos", "Changpinyo et al. (2021) Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts", "Zhu & Yang (2020) Zhu, L. and Yang, Y. Actbert: Learning global-local video-text representations. In CVPR, pp. 8746\u20138755, 2020.": "Actbert: Learning global-local video-text representations", "Li et al. (2021a) Li, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong, C., and Hoi, S. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS, 2021a.": "Align before fuse: Vision and language representation learning with momentum distillation", "Lu et al. (2019) Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R. (eds.), NeurIPS, pp. 13\u201323, 2019.": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks", "Li et al. (2021b) Li, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu, H., and Wang, H. UNIMO: towards unified-modal understanding and generation via cross-modal contrastive learning. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), ACL, pp. 2592\u20132607, 2021b.": "UNIMO: towards unified-modal understanding and generation via cross-modal contrastive learning", "Patrick et al. (2021) Patrick, M., Huang, P.-Y., Asano, Y., Metze, F., Hauptmann, A. G., Henriques, J. F., and Vedaldi, A. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.": "Support-set bottlenecks for video-text representation learning", "Bain et al. (2021) Bain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "Yang et al. (2021) Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. Just ask: Learning to answer questions from millions of narrated videos. In ICCV, pp. 1686\u20131697, 2021.": "Just ask: Learning to answer questions from millions of narrated videos", "Zhang et al. (2021) Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., and Gao, J. Vinvl: Making visual representations matter in vision-language models. arXiv preprint arXiv:2101.00529, 2021.": "Vinvl: Making visual representations matter in vision-language models"}, "source_title_to_arxiv_id": {"Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts": "2102.08981", "Align before fuse: Vision and language representation learning with momentum distillation": "2107.07651", "Frozen in time: A joint video and image encoder for end-to-end retrieval": "2104.00650"}}