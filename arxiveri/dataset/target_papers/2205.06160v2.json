{"title": "Localized Vision-Language Matching for Open-vocabulary Object Detection", "abstract": "In this work, we propose an open-vocabulary object detection method that,\nbased on image-caption pairs, learns to detect novel object classes along with\na given set of known classes. It is a two-stage training approach that first\nuses a location-guided image-caption matching technique to learn class labels\nfor both novel and known classes in a weakly-supervised manner and second\nspecializes the model for the object detection task using known class\nannotations. We show that a simple language model fits better than a large\ncontextualized language model for detecting novel objects. Moreover, we\nintroduce a consistency-regularization technique to better exploit\nimage-caption pair information. Our method compares favorably to existing\nopen-vocabulary detection approaches while being data-efficient. Source code is\navailable at https://github.com/lmb-freiburg/locov .", "authors": ["Maria A. Bravo", "Sudhanshu Mittal", "Thomas Brox"], "published_date": "2022_05_12", "pdf_url": "http://arxiv.org/pdf/2205.06160v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><th>Img-Cap</th><td colspan=\"4\">Constrained</td><td colspan=\"6\">Generalized</td></tr><tr><th>Data</th><td colspan=\"2\">Novel (17)</td><td colspan=\"2\">Known (48)</td><td colspan=\"2\">Novel (17)</td><td colspan=\"2\">Known (48)</td><td colspan=\"2\">All (65)</td></tr><tr><th>Size</th><td>AP</td><td>AP<sub>50</sub></td><td>AP</td><td>AP<sub>50</sub></td><td>AP</td><td>AP<sub>50</sub></td><td>AP</td><td>AP<sub>50</sub></td><td>AP</td><td>AP<sub>50</sub></td></tr><tr><th>Faster R-CNN</th><th rowspan=\"6\">-</th><td></td><td>-</td><td>-</td><td>54.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SB [2]</th><td>-</td><td>0.70</td><td>-</td><td>29.7</td><td>-</td><td>0.31</td><td>-</td><td>29.2</td><td>-</td><td>24.9</td></tr><tr><th>LAB [2]</th><td>-</td><td>0.27</td><td>-</td><td>21.1</td><td>-</td><td>0.22</td><td>-</td><td>20.8</td><td>-</td><td>18.0</td></tr><tr><th>DSES [2]</th><td>-</td><td>0.54</td><td>-</td><td>27.2</td><td>-</td><td>0.27</td><td>-</td><td>26.7</td><td>-</td><td>22.1</td></tr><tr><th>DELO [46]</th><td>-</td><td>7.6</td><td>-</td><td>14.0</td><td>-</td><td>3.41</td><td>-</td><td>13.8</td><td>-</td><td>13.0</td></tr><tr><th>PL [31]</th><td>-</td><td>10.0</td><td>-</td><td>36.8</td><td>-</td><td>4.12</td><td>-</td><td>35.9</td><td>-</td><td>27.9</td></tr><tr><th>STT-ZSD (Ours)</th><th></th><td>0.21</td><td>0.31</td><td>33.2</td><td>53.4</td><td>0.03</td><td>0.05</td><td>33.0</td><td>53.1</td><td>24.4</td><td>39.2</td></tr><tr><th>OVR{}^{*\\mathsection c} [41]</th><th rowspan=\"2\">0.6M</th><td>14.6</td><td>27.5</td><td>26.9</td><td>46.8</td><td>-</td><td>22.8</td><td>-</td><td>46.0</td><td>22.8</td><td>39.9</td></tr><tr><th>LocOv {}^{*\\mathsection c} (Ours)</th><td>17.2</td><td>30.1</td><td>33.5</td><td>53.4</td><td>16.6</td><td>28.6</td><td>31.9</td><td>51.3</td><td>28.1</td><td>45.7</td></tr><tr><th>XP-Mask{}^{\\ddagger\\mathsection\\star c} [16]</th><th>5.7M</th><td>-</td><td>29.9</td><td>-</td><td>46.8</td><td>-</td><td>27.0</td><td>-</td><td>46.3</td><td>-</td><td>41.2</td></tr><tr><th>CLIP (cropped reg){}^{\\dagger} [13]</th><th>400M</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>26.3</td><td>-</td><td>28.3</td><td>-</td><td>27.8</td></tr><tr><th>RegionCLIP{}^{\\dagger\\mathsection c} [44]</th><th>400.6M</th><td>-</td><td>30.8</td><td>-</td><td>55.2</td><td>-</td><td>26.8</td><td>-</td><td>54.8</td><td>-</td><td>47.5</td></tr><tr><th>ViLD{}^{\\dagger c} [13]</th><th>400M</th><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>27.6</td><td>-</td><td>59.5</td><td>-</td><td>51.3</td></tr></tbody></table>", "caption": "Table 1: Comparing mAP and AP<sub>50</sub> state-of-the-art methods. LocOv outperforms all other methods for Novel objects in the generalized setup while using only 0.6M of image-caption pairs. Training dataset: {}^{*}ImageNet1k, {}^{\\mathsection}COCO captions, {}^{\\dagger}CLIP400M, {}^{\\ddagger}Conceptual Captions, {}^{\\star}Open Images, and {}^{c}COCO", "list_citation_info": ["[13] Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision and language knowledge distillation. In: International Conference on Learning Representations (2022), https://openreview.net/forum?id=lL3lnMbR4WU", "[44] Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L.H., Zhou, L., Dai, X., Yuan, L., Li, Y., et al.: Regionclip: Region-based language-image pretraining. arXiv preprint arXiv:2112.09106 (2021)", "[2] Bansal, A., Sikka, K., Sharma, G., Chellappa, R., Divakaran, A.: Zero-shot object detection. In: Proceedings of the European Conference on Computer Vision (ECCV) (2018)", "[46] Zhu, P., Wang, H., Saligrama, V.: Don\u2019t even look once: Synthesizing features for zero-shot detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)", "[16] Huynh, D., Kuen, J., Lin, Z., Gu, J., Elhamifar, E.: Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling. arXiv preprint arXiv:2111.12698 (2021)", "[41] Zareian, A., Rosa, K.D., Hu, D.H., Chang, S.F.: Open-vocabulary object detection using captions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)", "[31] Rahman, S., Khan, S., Barnes, N.: Improved visual-semantic alignment for zero-shot object detection. Proceedings of the AAAI Conference on Artificial Intelligence (2020)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">\u2004Method</th><th colspan=\"3\">Novel (297)</th><th colspan=\"3\">Known (1020)</th><th colspan=\"3\">Generalized (2060)</th></tr><tr><th>AP</th><th>AP<sub>50</sub></th><th>AP<sub>75</sub></th><th>AP</th><th>AP<sub>50</sub></th><th>AP<sub>75</sub></th><th>AP</th><th>AP<sub>50</sub></th><th>AP<sub>75</sub></th></tr></thead><tbody><tr><th>\u2004STT-ZSD (Ours)</th><td>0.14</td><td>0.28</td><td>0.15</td><td>1.33</td><td>2.56</td><td>1.16</td><td>0.95</td><td>1.84</td><td>0.82</td></tr><tr><th>\u2004OVR [41]</th><td>0.59</td><td>1.27</td><td>0.45</td><td>0.92</td><td>2.08</td><td>0.72</td><td>0.70</td><td>1.57</td><td>0.54</td></tr><tr><th>\u2004LocOv (Ours)</th><td>0.67</td><td>1.42</td><td>0.59</td><td>1.21</td><td>2.31</td><td>1.11</td><td>0.91</td><td>1.77</td><td>0.81</td></tr></tbody></table>", "caption": "Table 5: Comparing open-vocabulary object detection results on the VAW test set.", "list_citation_info": ["[41] Zareian, A., Rosa, K.D., Hu, D.H., Chang, S.F.: Open-vocabulary object detection using captions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)"]}], "citation_info_to_title": {"[44] Zhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li, L.H., Zhou, L., Dai, X., Yuan, L., Li, Y., et al.: Regionclip: Region-based language-image pretraining. arXiv preprint arXiv:2112.09106 (2021)": "Regionclip: Region-based language-image pretraining", "[31] Rahman, S., Khan, S., Barnes, N.: Improved visual-semantic alignment for zero-shot object detection. Proceedings of the AAAI Conference on Artificial Intelligence (2020)": "Improved visual-semantic alignment for zero-shot object detection", "[41] Zareian, A., Rosa, K.D., Hu, D.H., Chang, S.F.: Open-vocabulary object detection using captions. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)": "Open-vocabulary object detection using captions", "[2] Bansal, A., Sikka, K., Sharma, G., Chellappa, R., Divakaran, A.: Zero-shot object detection. In: Proceedings of the European Conference on Computer Vision (ECCV) (2018)": "Zero-shot object detection", "[46] Zhu, P., Wang, H., Saligrama, V.: Don\u2019t even look once: Synthesizing features for zero-shot detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)": "Dont even look once: Synthesizing features for zero-shot detection", "[13] Gu, X., Lin, T.Y., Kuo, W., Cui, Y.: Open-vocabulary object detection via vision and language knowledge distillation. In: International Conference on Learning Representations (2022), https://openreview.net/forum?id=lL3lnMbR4WU": "Open-vocabulary object detection via vision and language knowledge distillation", "[16] Huynh, D., Kuen, J., Lin, Z., Gu, J., Elhamifar, E.: Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling. arXiv preprint arXiv:2111.12698 (2021)": "Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling"}, "source_title_to_arxiv_id": {"Zero-shot object detection": "1803.06049"}}