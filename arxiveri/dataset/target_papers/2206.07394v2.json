{"title": "Efficient Adaptive Ensembling for Image Classification", "abstract": "In recent times, except for sporadic cases, the trend in Computer Vision is\nto achieve minor improvements over considerable increases in complexity.\n  To reverse this tendency, we propose a novel method to boost image\nclassification performances without an increase in complexity.\n  To this end, we revisited ensembling, a powerful approach, not often\nadequately used due to its nature of increased complexity and training time,\nmaking it viable by specific design choices. First, we trained end-to-end two\nEfficientNet-b0 models (known to be the architecture with the best overall\naccuracy/complexity trade-off in image classification) on disjoint subsets of\ndata (i.e. bagging). Then, we made an efficient adaptive ensemble by performing\nfine-tuning of a trainable combination layer. In this way, we were able to\noutperform the state-of-the-art by an average of 0.5\\% on the accuracy with\nrestrained complexity both in terms of number of parameters (by 5-60 times),\nand FLoating point Operations Per Second (by 10-100 times) on several major\nbenchmark datasets, fully embracing the green AI.", "authors": ["Antonio Bruno", "Davide Moroni", "Massimo Martinelli"], "published_date": "2022_06_15", "pdf_url": "http://arxiv.org/pdf/2206.07394v2", "list_table_and_caption": [{"table": "<table><thead><tr><th>Model</th><th>Year</th><th>Accuracy</th><th>Parameters</th><th>FLOPs</th></tr></thead><tbody><tr><th>AlexNet alexnet </th><th>2012</th><td>63.3%</td><td>\\approx 60M</td><td>\\approx 0.7G</td></tr><tr><th>InceptionV3 inceptionv3 </th><th>2015</th><td>78.8%</td><td>\\approx 24M</td><td>\\approx 6G</td></tr><tr><th>ResNeXt-101 64x4 resnetx </th><th>2016</th><td>80.9%</td><td>\\approx 84M</td><td>\\approx 16G</td></tr><tr><th>EfficientNet-b0 efficientnet </th><th>2019</th><td>77.1%</td><td>\\approx 5.3M</td><td>\\approx 0.4G</td></tr><tr><th>EfficientNet-b7 efficientnet </th><th>2019</th><td>84.3%</td><td>\\approx 67M</td><td>\\approx 37G</td></tr><tr><th>Swin-L swin </th><th>2021</th><td>87.3%</td><td>\\approx 197M</td><td>\\approx 103G</td></tr><tr><th>NFNet-F4+ nfnet </th><th>2021</th><td>89.2%</td><td>\\approx 527M</td><td>\\approx 215G</td></tr><tr><th>ViT-G/14 vit </th><th>2021</th><td>90.45%</td><td>\\approx 1843M</td><td>\\approx 965G</td></tr><tr><th>CoAtNet-7 coatnet </th><th>2021</th><td>90.88%</td><td>\\approx 2440M</td><td>\\approx 2586G</td></tr></tbody></table>", "caption": "Table 1: Evolution of the state-of-the-art on the ImageNet classification task: as can be seen, complexity in models having accuracy &gt; 80% (both in the number of parameters and FLOPs) grows exponentially in spite of a minimal improvement. The same trend can be noticed on other computer vision tasks. N.B. only some architectures providing relevant improvements are shown in this table.", "list_citation_info": ["(3) S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, K. He, Aggregated residual transformations for deep neural networks, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5987\u20135995. doi:10.1109/CVPR.2017.634.", "(1) A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, Communications of the ACM 60 (2012) 84 \u2013 90.", "(4) M. Tan, Q. Le, EfficientNet: Rethinking model scaling for convolutional neural networks, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning, Vol. 97 of Proceedings of Machine Learning Research, PMLR, 2019, pp. 6105\u20136114.", "(8) Z. Dai, H. Liu, Q. V. Le, M. Tan, Coatnet: Marrying convolution and attention for all data sizes, CoRR abs/2106.04803 (2021). arXiv:2106.04803. URL https://arxiv.org/abs/2106.04803", "(7) X. Zhai, A. Kolesnikov, N. Houlsby, L. Beyer, Scaling vision transformers, ArXiv abs/2106.04560 (2021).", "(6) A. Brock, S. De, S. L. Smith, K. Simonyan, High-performance large-scale image recognition without normalization, CoRR abs/2102.06171 (2021). arXiv:2102.06171. URL https://arxiv.org/abs/2102.06171", "(5) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, CoRR abs/2103.14030 (2021). arXiv:2103.14030. URL https://arxiv.org/abs/2103.14030", "(2) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the inception architecture for computer vision, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2818\u20132826. doi:10.1109/CVPR.2016.308."]}, {"table": "<table><thead><tr><th>Dataset</th><th>SOTA accuracy</th><th>Our accuracy</th><th>Improvement</th></tr></thead><tbody><tr><th>CIFAR-10 vith14 </th><td>99.500%</td><td>99.612%</td><td>0.112%</td></tr><tr><th>CIFAR-100 SAM </th><td>96.080%</td><td>96.808%</td><td>0.728%</td></tr><tr><th>Cars tresentlv2 </th><td>96.320%</td><td>96.868%</td><td>0.548%</td></tr><tr><th>Food-101 SAM </th><td>96.180%</td><td>96.879%</td><td>0.699%</td></tr><tr><th>Flower102 cvt24 </th><td>99.720%</td><td>99.847%</td><td>0.127%</td></tr><tr><th>CINIC-10 NATm3 </th><td>94.300%</td><td>95.064%</td><td>0.764%</td></tr><tr><th>Pets SAM </th><td>97.100%</td><td>98.220%</td><td>1.120%</td></tr></tbody></table>", "caption": "Table 4: Classification test accuracy comparison between SOTA and our work on datasets used during experiments.", "list_citation_info": ["(26) T. Ridnik, E. Ben-Baruch, A. Noy, L. Zelnik-Manor, Imagenet-21k pretraining for the masses (2021). arXiv:2104.10972.", "(24) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in: ICLR 2021: The Ninth International Conference on Learning Representations, 2021.", "(25) P. Foret, A. Kleiner, H. Mobahi, B. Neyshabur, Sharpness-aware minimization for efficiently improving generalization, in: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.", "(27) H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, L. Zhang, Cvt: Introducing convolutions to vision transformers (2021). arXiv:2103.15808.", "(28) Z. Lu, G. Sreekumar, E. Goodman, W. Banzhaf, K. Deb, V. N. Boddeti, Neural architecture transfer, IEEE Transactions on Pattern Analysis and Machine Intelligence 43 (9) (2021) 2971\u20132989. doi:10.1109/tpami.2021.3052758."]}, {"table": "<table><thead><tr><th>Dataset</th><th>SOTA parameters</th><th>Our parameters</th><th>SOTA FLOPs</th><th>Our FLOPs</th></tr></thead><tbody><tr><th>CIFAR-10 vith14 </th><td>\\approx 632M</td><td>\\approx 11M (100K)</td><td>\\approx 916G{}^{\\dagger}</td><td>\\approx 0.9G</td></tr><tr><th>CIFAR-100 SAM </th><td>\\approx 480M</td><td>\\approx 11M (100K)</td><td>\\approx 299G{}^{*}</td><td>\\approx 0.9G</td></tr><tr><th>Cars tresentlv2 </th><td>\\approx 54.7M</td><td>\\approx 11M (100K)</td><td>\\approx 10G</td><td>\\approx 0.9G</td></tr><tr><th>Food-101 SAM </th><td>\\approx 480M</td><td>\\approx 11M (100K)</td><td>\\approx 299G{}^{*}</td><td>\\approx 0.9G</td></tr><tr><th>Flower102 cvt24 </th><td>\\approx 277M</td><td>\\approx 11M (100K)</td><td>\\approx 60G</td><td>\\approx 0.9G</td></tr><tr><th>CINIC-10 NATm3 </th><td>\\approx 8.1M</td><td>\\approx 11M (100K)</td><td>\\approx 1G</td><td>\\approx 0.9G</td></tr><tr><th>Pets SAM </th><td>\\approx 480M</td><td>\\approx 11M (100K)</td><td>\\approx 299G{}^{*}</td><td>\\approx 0.9G</td></tr></tbody></table><ul><li>1.<p>\\dagger Estimation based on similar architecture with similar number of parameters.</p></li><li>2.<p>* Estimation based on the same architecture but scaling FLOPs w.r.t. the number of parameters ratio.</p></li></ul>", "caption": "Table 5: Complexity, both number of parameters and FLOPs, comparison between SOTA and our work on datasets used during experiments.", "list_citation_info": ["(26) T. Ridnik, E. Ben-Baruch, A. Noy, L. Zelnik-Manor, Imagenet-21k pretraining for the masses (2021). arXiv:2104.10972.", "(24) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in: ICLR 2021: The Ninth International Conference on Learning Representations, 2021.", "(25) P. Foret, A. Kleiner, H. Mobahi, B. Neyshabur, Sharpness-aware minimization for efficiently improving generalization, in: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.", "(27) H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, L. Zhang, Cvt: Introducing convolutions to vision transformers (2021). arXiv:2103.15808.", "(28) Z. Lu, G. Sreekumar, E. Goodman, W. Banzhaf, K. Deb, V. N. Boddeti, Neural architecture transfer, IEEE Transactions on Pattern Analysis and Machine Intelligence 43 (9) (2021) 2971\u20132989. doi:10.1109/tpami.2021.3052758."]}], "citation_info_to_title": {"(3) S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, K. He, Aggregated residual transformations for deep neural networks, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5987\u20135995. doi:10.1109/CVPR.2017.634.": "Aggregated residual transformations for deep neural networks", "(26) T. Ridnik, E. Ben-Baruch, A. Noy, L. Zelnik-Manor, Imagenet-21k pretraining for the masses (2021). arXiv:2104.10972.": "Imagenet-21k pretraining for the masses", "(1) A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, Communications of the ACM 60 (2012) 84 \u2013 90.": "Imagenet classification with deep convolutional neural networks", "(5) Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, CoRR abs/2103.14030 (2021). arXiv:2103.14030. URL https://arxiv.org/abs/2103.14030": "Swin transformer: Hierarchical vision transformer using shifted windows", "(24) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in: ICLR 2021: The Ninth International Conference on Learning Representations, 2021.": "An image is worth 16x16 words: Transformers for image recognition at scale", "(7) X. Zhai, A. Kolesnikov, N. Houlsby, L. Beyer, Scaling vision transformers, ArXiv abs/2106.04560 (2021).": "Scaling Vision Transformers", "(25) P. Foret, A. Kleiner, H. Mobahi, B. Neyshabur, Sharpness-aware minimization for efficiently improving generalization, in: 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021.": "Sharpness-aware minimization for efficiently improving generalization", "(8) Z. Dai, H. Liu, Q. V. Le, M. Tan, Coatnet: Marrying convolution and attention for all data sizes, CoRR abs/2106.04803 (2021). arXiv:2106.04803. URL https://arxiv.org/abs/2106.04803": "Coatnet: Marrying convolution and attention for all data sizes", "(2) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the inception architecture for computer vision, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2818\u20132826. doi:10.1109/CVPR.2016.308.": "Rethinking the inception architecture for computer vision", "(4) M. Tan, Q. Le, EfficientNet: Rethinking model scaling for convolutional neural networks, in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning, Vol. 97 of Proceedings of Machine Learning Research, PMLR, 2019, pp. 6105\u20136114.": "EfficientNet: Rethinking model scaling for convolutional neural networks", "(27) H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, L. Zhang, Cvt: Introducing convolutions to vision transformers (2021). arXiv:2103.15808.": "Cvt: Introducing convolutions to vision transformers", "(6) A. Brock, S. De, S. L. Smith, K. Simonyan, High-performance large-scale image recognition without normalization, CoRR abs/2102.06171 (2021). arXiv:2102.06171. URL https://arxiv.org/abs/2102.06171": "High-performance large-scale image recognition without normalization", "(28) Z. Lu, G. Sreekumar, E. Goodman, W. Banzhaf, K. Deb, V. N. Boddeti, Neural architecture transfer, IEEE Transactions on Pattern Analysis and Machine Intelligence 43 (9) (2021) 2971\u20132989. doi:10.1109/tpami.2021.3052758.": "Neural architecture transfer"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Cvt: Introducing convolutions to vision transformers": "2103.15808"}}