{"title": "Extensible Proxy for Efficient NAS", "abstract": "Neural Architecture Search (NAS) has become a de facto approach in the recent\ntrend of AutoML to design deep neural networks (DNNs). Efficient or\nnear-zero-cost NAS proxies are further proposed to address the demanding\ncomputational issues of NAS, where each candidate architecture network only\nrequires one iteration of backpropagation. The values obtained from the proxies\nare considered the predictions of architecture performance on downstream tasks.\nHowever, two significant drawbacks hinder the extended usage of Efficient NAS\nproxies. (1) Efficient proxies are not adaptive to various search spaces. (2)\nEfficient proxies are not extensible to multi-modality downstream tasks. Based\non the observations, we design a Extensible proxy (Eproxy) that utilizes\nself-supervised, few-shot training (i.e., 10 iterations of backpropagation)\nwhich yields near-zero costs. The key component that makes Eproxy efficient is\nan untrainable convolution layer termed barrier layer that add the\nnon-linearities to the optimization spaces so that the Eproxy can discriminate\nthe performance of architectures in the early stage. Furthermore, to make\nEproxy adaptive to different downstream tasks/search spaces, we propose a\nDiscrete Proxy Search (DPS) to find the optimized training settings for Eproxy\nwith only handful of benchmarked architectures on the target tasks. Our\nextensive experiments confirm the effectiveness of both Eproxy and Eproxy+DPS.\nCode is available at https://github.com/leeyeehoo/GenNAS-Zero.", "authors": ["Yuhong Li", "Jiajie Li", "Cong Han", "Pan Li", "Jinjun Xiong", "Deming Chen"], "published_date": "2022_10_17", "pdf_url": "http://arxiv.org/pdf/2210.09459v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">Test Err. (%)</td><td>Params</td><td>FLOPS</td><td>Search Cost</td><td>Searched</td><td>Searched</td></tr><tr><td>top-1</td><td>top-5</td><td>(M)</td><td>(M)</td><td>(GPU days)</td><td>Method</td><td>dataset</td></tr><tr><td>NASNet-A Zoph et al. (2018)</td><td>26.0</td><td>8.4</td><td>5.3</td><td>564</td><td> 2000</td><td>RL</td><td>CIFAR-10</td></tr><tr><td>AmoebaNet-C Real et al. (2019)</td><td>24.3</td><td>7.6</td><td>6.4</td><td>570</td><td>3150</td><td>evolution</td><td>CIFAR-10</td></tr><tr><td>PNAS Liu et al. (2018a)</td><td>25.8</td><td>8.1</td><td>5.1</td><td>588</td><td>225</td><td>SMBO</td><td>CIFAR-10</td></tr><tr><td>DARTS(2nd order) Liu et al. (2018b)</td><td>26.7</td><td>8.7</td><td>4.7</td><td>574</td><td>4.0</td><td>gradient-based</td><td>CIFAR-10</td></tr><tr><td>SNAS Xie et al. (2018)</td><td>27.3</td><td>9.2</td><td>4.3</td><td>522</td><td>1.5</td><td>gradient-based</td><td>CIFAR-10</td></tr><tr><td>GDAS Dong and Yang (2019)</td><td>26.0</td><td>8.5</td><td>5.3</td><td>581</td><td>0.21</td><td>gradient-based</td><td>CIFAR-10</td></tr><tr><td>P-DARTS Chen et al. (2019)</td><td>24.4</td><td>7.4</td><td>4.9</td><td>557</td><td>0.3</td><td>gradient-based</td><td>CIFAR-10</td></tr><tr><td>P-DARTS</td><td>24.7</td><td>7.5</td><td>5.1</td><td>577</td><td>0.3</td><td>gradient-based</td><td>CIFAR-100</td></tr><tr><td>PC-DARTS Xu et al. (2019a)</td><td>25.1</td><td>7.8</td><td>5.3</td><td>586</td><td>0.1</td><td>gradient-based</td><td>CIFAR-10</td></tr><tr><td>TE-NAS Chen et al. (2021b)</td><td>26.2</td><td>8.3</td><td>6.3</td><td>-</td><td>0.05</td><td>training-free</td><td>CIFAR-10</td></tr><tr><td>PC-DARTS</td><td>24.2</td><td>7.3</td><td>5.3</td><td>597</td><td>3.8</td><td>gradient-based</td><td>ImageNet</td></tr><tr><td>ProxylessNAS Cai et al. (2018)</td><td>24.9</td><td>7.5</td><td>7.1</td><td>465</td><td>8.3</td><td>gradient-based</td><td>ImageNet</td></tr><tr><td>TE-NAS Chen et al. (2021b)</td><td>24.5</td><td>7.5</td><td>5.4</td><td>599</td><td>0.17</td><td>training-free</td><td>ImageNet</td></tr><tr><td>Eproxy</td><td>25.7</td><td>8.1</td><td>4.9</td><td>542</td><td>0.02</td><td>evolution+proxy</td><td>CIFAR-10</td></tr><tr><td>Eproxy+DPS{}_{\\textnormal{T}}</td><td>24.4</td><td>7.3</td><td>5.3</td><td>578</td><td>0.06</td><td>evolution+proxy</td><td>CIFAR-10</td></tr></tbody></table>", "caption": "Table 8: Comparison with state-of-the-art NAS methods on ImageNet. {}_{\\textnormal{T}} stands for DPS is conducted in NDS search space and directly transferred to the target. Note Eproxy+DPS achieves the best results among NAS methods on CIFAR-10.", "list_citation_info": ["Cai et al. (2018) Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018.", "Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 4780\u20134789, 2019.", "Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697\u20138710, 2018.", "Dong and Yang (2019) Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1761\u20131770, 2019.", "Chen et al. (2021b) Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. arXiv preprint arXiv:2102.11535, 2021b.", "Xie et al. (2018) Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. arXiv preprint arXiv:1812.09926, 2018.", "Liu et al. (2018a) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceedings of the European conference on computer vision (ECCV), pages 19\u201334, 2018a.", "Liu et al. (2018b) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018b.", "Xu et al. (2019a) Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. Pc-darts: Partial channel connections for memory-efficient architecture search. arXiv preprint arXiv:1907.05737, 2019a.", "Chen et al. (2019) Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1294\u20131303, 2019."]}], "citation_info_to_title": {"Liu et al. (2018b) Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018b.": "Darts: Differentiable architecture search", "Zoph et al. (2018) Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697\u20138710, 2018.": "Learning Transferable Architectures for Scalable Image Recognition", "Xie et al. (2018) Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. Snas: stochastic neural architecture search. arXiv preprint arXiv:1812.09926, 2018.": "Snas: Stochastic Neural Architecture Search", "Chen et al. (2021b) Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective. arXiv preprint arXiv:2102.11535, 2021b.": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective", "Dong and Yang (2019) Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1761\u20131770, 2019.": "Searching for a robust neural architecture in four gpu hours", "Liu et al. (2018a) Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceedings of the European conference on computer vision (ECCV), pages 19\u201334, 2018a.": "Progressive neural architecture search", "Real et al. (2019) Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 4780\u20134789, 2019.": "Regularized Evolution for Image Classifier Architecture Search", "Xu et al. (2019a) Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. Pc-darts: Partial channel connections for memory-efficient architecture search. arXiv preprint arXiv:1907.05737, 2019a.": "Pc-darts: Partial channel connections for memory-efficient architecture search", "Chen et al. (2019) Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1294\u20131303, 2019.": "Progressive differentiable architecture search: Bridging the depth gap between search and evaluation", "Cai et al. (2018) Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018.": "Proxylessnas: Direct Neural Architecture Search on Target Task and Hardware"}, "source_title_to_arxiv_id": {"Proxylessnas: Direct Neural Architecture Search on Target Task and Hardware": "1812.00332"}}