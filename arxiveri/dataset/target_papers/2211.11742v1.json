{"title": "SceneComposer: Any-Level Semantic Image Synthesis", "abstract": "We propose a new framework for conditional image synthesis from semantic\nlayouts of any precision levels, ranging from pure text to a 2D semantic canvas\nwith precise shapes. More specifically, the input layout consists of one or\nmore semantic regions with free-form text descriptions and adjustable precision\nlevels, which can be set based on the desired controllability. The framework\nnaturally reduces to text-to-image (T2I) at the lowest level with no shape\ninformation, and it becomes segmentation-to-image (S2I) at the highest level.\nBy supporting the levels in-between, our framework is flexible in assisting\nusers of different drawing expertise and at different stages of their creative\nworkflow. We introduce several novel techniques to address the challenges\ncoming with this new setup, including a pipeline for collecting training data;\na precision-encoded mask pyramid and a text feature map representation to\njointly encode precision level, semantics, and composition information; and a\nmulti-scale guided diffusion model to synthesize images. To evaluate the\nproposed method, we collect a test dataset containing user-drawn layouts with\ndiverse scenes and styles. Experimental results show that the proposed method\ncan generate high-quality images following the layout at given precision, and\ncompares favorably against existing methods. Project page\n\\url{https://zengxianyu.github.io/scenec/}", "authors": ["Yu Zeng", "Zhe Lin", "Jianming Zhang", "Qing Liu", "John Collomosse", "Jason Kuen", "Vishal M. Patel"], "published_date": "2022_11_21", "pdf_url": "http://arxiv.org/pdf/2211.11742v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Setting</td><td>Open-domain layout</td><td>Shape control</td><td>Sparse layout</td><td>Coarse shape</td><td>Level control</td></tr><tr><td>T2I</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2717</td><td>\u2717</td></tr><tr><td>S2I</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2717</td></tr><tr><td>ST2I</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2717</td><td>\u2717</td></tr><tr><td>Box2I</td><td>\u2717</td><td>\u2717</td><td>\u2713</td><td>\u2717</td><td>\u2717</td></tr><tr><td>Ours</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td>\u2713</td></tr></tbody></table>", "caption": "Table 1: Difference from related conditional image synthesis works. T2I: text to image, S2I: segmentation to image, ST2I: Scene-based text to image [13], Box2I: bounding box layout to image [53]. ", "list_citation_info": ["[53] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In he AAAI Conference on Artificial Intelligence, 2021.", "[13] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>FID \\downarrow</th><th>zero-shot FID\\downarrow</th></tr></thead><tbody><tr><td>SSA-GAN [31]</td><td>19.37</td><td>-</td></tr><tr><td>VQ-Diffusion [15]</td><td>13.86</td><td>-</td></tr><tr><td>DF-GAN [55]</td><td>19.32</td><td>-</td></tr><tr><td>GLIDE [37]</td><td>-</td><td>12.89</td></tr><tr><td>SD [45, 1]</td><td>-</td><td>9.89</td></tr><tr><td>DALLE-2 [41]</td><td>-</td><td>10.87</td></tr><tr><td>Ours</td><td>8.55</td><td>9.47</td></tr></tbody></table>", "caption": "Table 3: Quantitative comparison with T2I methods on COCO. ", "list_citation_info": ["[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.", "[55] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, pages 16515\u201316525, 2022.", "[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.", "[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.", "[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.", "[31] Wentong Liao, Kai Hu, Michael Ying Yang, and Bodo Rosenhahn. Text to image generation with semantic-spatial aware gan. In IEEE Conference on Computer Vision and Pattern Recognition, pages 18187\u201318196, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>FID \\downarrow</th><th>zero-shot FID \\downarrow</th><th>mIOU\\uparrow</th><th>zero-shot mIOU\\uparrow</th></tr></thead><tbody><tr><td>SPADE [38]</td><td>50.91</td><td>-</td><td>17.49</td><td>-</td></tr><tr><td>SAFM [35]</td><td>62.28</td><td>-</td><td>12.28</td><td>-</td></tr><tr><td>CLADE [54]</td><td>55.30</td><td>-</td><td>17.21</td><td>-</td></tr><tr><td>Ours</td><td>17.20</td><td>20.17</td><td>23.01</td><td>17.15</td></tr></tbody></table>", "caption": "Table 4: Quantitative comparison with S2I methods on COCO-stuff. ", "list_citation_info": ["[54] Zhentao Tan, Dongdong Chen, Qi Chu, Menglei Chai, Jing Liao, Mingming He, Lu Yuan, Gang Hua, and Nenghai Yu. Efficient semantic image synthesis via class-adaptive normalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[35] Zhengyao Lv, Xiaoming Li, Zhenxing Niu, Bing Cao, and Wangmeng Zuo. Semantic-shape adaptive feature modulation for semantic image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, pages 11214\u201311223, 2022.", "[38] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In IEEE Conference on Computer Vision and Pattern Recognition, 2019."]}, {"table": "<table><tbody><tr><td></td><th>Method</th><th>FID \\downarrow</th><th>zero-shot FID\\downarrow</th></tr><tr><td rowspan=\"4\">64\\times 64</td><td>LostGAN-V1 [51]</td><td>34.31</td><td>-</td></tr><tr><td>OC-GAN [53]</td><td>33.1</td><td>-</td></tr><tr><td>Layout2Im [66]</td><td>44.19</td><td>-</td></tr><tr><td>Ours</td><td>30.27</td><td>38.25</td></tr><tr><td rowspan=\"4\">256\\times 256</td><td>LostGAN-V2 [52]</td><td>42.55</td><td>-</td></tr><tr><td>OC-GAN [53]</td><td>41.65</td><td>-</td></tr><tr><td>LDM [45]</td><td>40.91</td><td>-</td></tr><tr><td>Ours</td><td>35.69</td><td>41.74</td></tr></tbody></table>", "caption": "Table A: Quantitative comparison with bounding box based layout-to-image synthesis models on COCO-Stuff. ", "list_citation_info": ["[66] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8584\u20138593, 2019.", "[53] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In he AAAI Conference on Artificial Intelligence, 2021.", "[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.", "[52] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5070\u20135087, 2021.", "[51] Wei Sun and Tianfu Wu. Image synthesis from reconfigurable layout and style. In International Conference on Computer Vision, pages 10531\u201310540, 2019."]}], "citation_info_to_title": {"[38] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.": "Semantic Image Synthesis with Spatially-Adaptive Normalization", "[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022.": "High-Resolution Image Synthesis with Latent Diffusion Models", "[55] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, pages 16515\u201316525, 2022.": "Df-gan: A simple and effective baseline for text-to-image synthesis", "[66] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8584\u20138593, 2019.": "Image Generation from Layout", "[51] Wei Sun and Tianfu Wu. Image synthesis from reconfigurable layout and style. In International Conference on Computer Vision, pages 10531\u201310540, 2019.": "Image Synthesis from Reconfigurable Layout and Style", "[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.": "Hierarchical text-conditional image generation with clip latents", "[52] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5070\u20135087, 2021.": "Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis", "[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, pages 10696\u201310706, 2022.": "Vector Quantized Diffusion Model for Text-to-Image Synthesis", "[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models", "[53] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In he AAAI Conference on Artificial Intelligence, 2021.": "Object-centric image generation from layouts", "[35] Zhengyao Lv, Xiaoming Li, Zhenxing Niu, Bing Cao, and Wangmeng Zuo. Semantic-shape adaptive feature modulation for semantic image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, pages 11214\u201311223, 2022.": "Semantic-Shape Adaptive Feature Modulation for Semantic Image Synthesis", "[54] Zhentao Tan, Dongdong Chen, Qi Chu, Menglei Chai, Jing Liao, Mingming He, Lu Yuan, Gang Hua, and Nenghai Yu. Efficient semantic image synthesis via class-adaptive normalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.": "Efficient Semantic Image Synthesis via Class-Adaptive Normalization", "[13] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022.": "Make-a-scene: Scene-based text-to-image generation with human priors", "[31] Wentong Liao, Kai Hu, Michael Ying Yang, and Bodo Rosenhahn. Text to image generation with semantic-spatial aware gan. In IEEE Conference on Computer Vision and Pattern Recognition, pages 18187\u201318196, 2022.": "Text to Image Generation with Semantic-Spatial Aware GAN"}, "source_title_to_arxiv_id": {"Hierarchical text-conditional image generation with clip latents": "2204.06125", "Semantic-Shape Adaptive Feature Modulation for Semantic Image Synthesis": "2203.16898"}}