{"title": "Contrastive Video-Language Learning with Fine-grained Frame Sampling", "abstract": "Despite recent progress in video and language representation learning, the\nweak or sparse correspondence between the two modalities remains a bottleneck\nin the area. Most video-language models are trained via pair-level loss to\npredict whether a pair of video and text is aligned. However, even in paired\nvideo-text segments, only a subset of the frames are semantically relevant to\nthe corresponding text, with the remainder representing noise; where the ratio\nof noisy frames is higher for longer videos. We propose FineCo (Fine-grained\nContrastive Loss for Frame Sampling), an approach to better learn video and\nlanguage representations with a fine-grained contrastive objective operating on\nvideo frames. It helps distil a video by selecting the frames that are\nsemantically equivalent to the text, improving cross-modal correspondence.\nBuilding on the well established VideoCLIP model as a starting point, FineCo\nachieves state-of-the-art performance on YouCookII, a text-video retrieval\nbenchmark with long videos. FineCo also achieves competitive results on\ntext-video retrieval (MSR-VTT), and video question answering datasets (MSR-VTT\nQA and MSR-VTT MC) with shorter videos.", "authors": ["Zixu Wang", "Yujie Zhong", "Yishu Miao", "Lin Ma", "Lucia Specia"], "published_date": "2022_10_10", "pdf_url": "http://arxiv.org/pdf/2210.05039v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>YouCookII</th><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><th>HowTo100M (Miech et al., 2019)</th><td>8.2</td><td>24.5</td><td>35.3</td><td>24.0</td></tr><tr><th>MIL-NCE (Miech et al., 2020)</th><td>15.1</td><td>38.0</td><td>51.2</td><td>10.0</td></tr><tr><th>COOT (Ging et al., 2020)</th><td>16.7</td><td>40.2</td><td>52.3</td><td>9.0</td></tr><tr><th>UniVL (Luo et al., 2020)</th><td>28.9</td><td>57.6</td><td>70.0</td><td>4.0</td></tr><tr><th>VideoCLIP (Xu et al., 2021)</th><td>32.2</td><td>62.6</td><td>75.0</td><td>3.0</td></tr><tr><th>Ours w/o DS</th><td>35.7</td><td>65.9</td><td>77.5</td><td>3.0</td></tr><tr><th>Ours w DS</th><td>37.6</td><td>66.6</td><td>78.2</td><td>3.0</td></tr></tbody></table>", "caption": "Table 1: YouCookII Retrieval Results. DS denotes Dual Softmax.", "list_citation_info": ["Luo et al. (2020) Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. 2020. Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353.", "Ging et al. (2020) Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, and Thomas Brox. 2020. Coot: Cooperative hierarchical transformer for video-text representation learning. In Advances on Neural Information Processing Systems (NeurIPS).", "Miech et al. (2020) Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. End-to-End Learning of Visual Representations from Uncurated Instructional Videos. In CVPR.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV.", "Xu et al. (2021) Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."]}, {"table": "<table><tbody><tr><th>MSR-VTT 1k</th><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><th>JSFusion (Yu et al., 2018)</th><td>10.2</td><td>31.2</td><td>43.2</td><td>13.0</td></tr><tr><th>HowTo100M (Miech et al., 2019)</th><td>14.9</td><td>40.2</td><td>52.8</td><td>9.0</td></tr><tr><th>ClipBERT (Lei et al., 2021)</th><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><th>Support-set (Patrick et al., 2021)</th><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><th>FiT (Bain et al., 2021)</th><td>32.5</td><td>61.5</td><td>71.2</td><td>3.0</td></tr><tr><th>VideoCLIP (Xu et al., 2021)</th><td>30.9</td><td>55.4</td><td>66.8</td><td>4.0</td></tr><tr><th>Ours</th><td>32.6</td><td>62.1</td><td>71.4</td><td>3.0</td></tr></tbody></table>", "caption": "Table 2: MSR-VTT Results - 1k", "list_citation_info": ["Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision.", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV.", "Yu et al. (2018) Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV).", "Patrick et al. (2021) Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations.", "Xu et al. (2021) Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."]}, {"table": "<table><tbody><tr><th>MSVD</th><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><th>VSE (Kiros et al., 2014)</th><td>12.3</td><td>30.1</td><td>42.3</td><td>14.0</td></tr><tr><th>VSE ++ (Faghri et al., 2018)</th><td>15.4</td><td>39.6</td><td>53.0</td><td>9.0</td></tr><tr><th>CE (Liu et al., 2019)</th><td>19.8</td><td>49.0</td><td>63.8</td><td>6.0</td></tr><tr><th>Support-set (Patrick et al., 2021)</th><td>28.4</td><td>60.0</td><td>72.9</td><td>4.0</td></tr><tr><th>FiT (Bain et al., 2021)</th><td>33.7</td><td>64.7</td><td>76.3</td><td>3.0</td></tr><tr><th>VideoCLIP (Xu et al., 2021)</th><td>26.4</td><td>52.2</td><td>63.3</td><td>5.0</td></tr><tr><th>Ours</th><td>27.2</td><td>54.0</td><td>64.0</td><td>5.0</td></tr></tbody></table>", "caption": "Table 3: MSVD Results", "list_citation_info": ["Faghri et al. (2018) Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. Vse++: Improving visual-semantic embeddings with hard negatives.", "Liu et al. (2019) Y. Liu, S. Albanie, A. Nagrani, and A. Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. In arXiv preprint arxiv:1907.13487.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision.", "Kiros et al. (2014) Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. ArXiv, abs/1411.2539.", "Patrick et al. (2021) Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations.", "Xu et al. (2021) Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."]}, {"table": "<table><tbody><tr><th>DiDeMo</th><td>R@1</td><td>R@5</td><td>R@10</td><td>MedR</td></tr><tr><th>S2VT (Venugopalan et al., 2015)</th><td>11.9</td><td>33.6</td><td>-</td><td>13.0</td></tr><tr><th>FSE (Zhang et al., 2018)</th><td>13.9</td><td>36.0</td><td>-</td><td>11.0</td></tr><tr><th>CE (Liu et al., 2019)</th><td>16.1</td><td>41.1</td><td>-</td><td>8.3</td></tr><tr><th>ClipBERT (Lei et al., 2021)</th><td>20.4</td><td>44.5</td><td>56.7</td><td>7.0</td></tr><tr><th>FiT (Bain et al., 2021)</th><td>31.0</td><td>59.8</td><td>72.4</td><td>3.0</td></tr><tr><th>VideoCLIP (Xu et al., 2021)</th><td>16.6</td><td>46.9</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>19.5</td><td>48.8</td><td>55.9</td><td>7.0</td></tr></tbody></table>", "caption": "Table 4: DiDeMo Results", "list_citation_info": ["Liu et al. (2019) Y. Liu, S. Albanie, A. Nagrani, and A. Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. In arXiv preprint arxiv:1907.13487.", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision.", "Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.", "Venugopalan et al. (2015) Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2015. Translating videos to natural language using deep recurrent neural networks. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1494\u20131504, Denver, Colorado. Association for Computational Linguistics.", "Zhang et al. (2018) Bowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-modal and hierarchical modeling of video and text. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, pages 385\u2013401.", "Xu et al. (2021) Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."]}, {"table": "<table><thead><tr><th>MSR-VTT QA</th><th>Accuracy</th></tr></thead><tbody><tr><th>AMU (Xu et al., 2017)</th><td>32.5</td></tr><tr><th>HME (Fan et al., 2019)</th><td>33.0</td></tr><tr><th>HCRN (Le et al., 2020)</th><td>35.6</td></tr><tr><th>ClipBERT (Lei et al., 2021)</th><td>37.4</td></tr><tr><th>VideoCLIP (Xu et al., 2021)</th><td>35.9</td></tr><tr><th>Ours</th><td>37.4</td></tr></tbody></table>", "caption": "Table 5: MSR-VTT QA Results", "list_citation_info": ["Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.", "Xu et al. (2017) Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia.", "Le et al. (2020) Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical conditional relation networks for video question answering. arXiv preprint arXiv:2002.10698.", "Fan et al. (2019) Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. 2019. Heterogeneous memory enhanced multimodal attention model for video question answering. In CVPR.", "Xu et al. (2021) Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."]}, {"table": "<table><thead><tr><th>MSR-VTT MC</th><th>Accuracy</th></tr></thead><tbody><tr><th>MLB (Kim et al., 2016)</th><td>76.1</td></tr><tr><th>JSFusion (Yu et al., 2018)</th><td>83.4</td></tr><tr><th>ActBERT (Zhu and Yang, 2020)</th><td>85.7</td></tr><tr><th>ClipBERT (Lei et al., 2021)</th><td>88.2</td></tr><tr><th>VideoCLIP (Xu et al., 2021)</th><td>92.1</td></tr><tr><th>Ours</th><td>92.7</td></tr></tbody></table>", "caption": "Table 8: MSR-VTT MC Results", "list_citation_info": ["Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.", "Kim et al. (2016) Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. 2016. Hadamard product for low-rank bilinear pooling. arXiv preprint arXiv:1610.04325.", "Zhu and Yang (2020) Linchao Zhu and Yi Yang. 2020. Actbert: Learning global-local video-text representations. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).", "Yu et al. (2018) Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV).", "Xu et al. (2021) Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics."]}], "citation_info_to_title": {"Lei et al. (2021) Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. 2021. Less is more: Clipbert for video-and-language learningvia sparse sampling. In CVPR.": "Less is more: Clipbert for video-and-language learning via sparse sampling", "Venugopalan et al. (2015) Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2015. Translating videos to natural language using deep recurrent neural networks. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1494\u20131504, Denver, Colorado. Association for Computational Linguistics.": "Translating videos to natural language using deep recurrent neural networks", "Kim et al. (2016) Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. 2016. Hadamard product for low-rank bilinear pooling. arXiv preprint arXiv:1610.04325.": "Hadamard product for low-rank bilinear pooling", "Luo et al. (2020) Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. 2020. Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353.": "Univl: A unified video and language pre-training model for multimodal understanding and generation", "Bain et al. (2021) Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision.": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "Zhu and Yang (2020) Linchao Zhu and Yi Yang. 2020. Actbert: Learning global-local video-text representations. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).": "Actbert: Learning Global-Local Video-Text Representations", "Ging et al. (2020) Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, and Thomas Brox. 2020. Coot: Cooperative hierarchical transformer for video-text representation learning. In Advances on Neural Information Processing Systems (NeurIPS).": "Coot: Cooperative Hierarchical Transformer for Video-Text Representation Learning", "Yu et al. (2018) Youngjae Yu, Jongseok Kim, and Gunhee Kim. 2018. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV).": "A joint sequence fusion model for video question answering and retrieval", "Le et al. (2020) Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical conditional relation networks for video question answering. arXiv preprint arXiv:2002.10698.": "Hierarchical conditional relation networks for video question answering", "Kiros et al. (2014) Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. Unifying visual-semantic embeddings with multimodal neural language models. ArXiv, abs/1411.2539.": "Unifying visual-semantic embeddings with multimodal neural language models", "Miech et al. (2019) Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV.": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips", "Faghri et al. (2018) Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. 2018. Vse++: Improving visual-semantic embeddings with hard negatives.": "Vse++: Improving visual-semantic embeddings with hard negatives", "Fan et al. (2019) Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. 2019. Heterogeneous memory enhanced multimodal attention model for video question answering. In CVPR.": "Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering", "Xu et al. (2021) Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787\u20136800, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.": "VideoCLIP: Contrastive pre-training for zero-shot video-text understanding", "Liu et al. (2019) Y. Liu, S. Albanie, A. Nagrani, and A. Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. In arXiv preprint arxiv:1907.13487.": "Use what you have: Video retrieval using representations from collaborative experts", "Xu et al. (2017) Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia.": "Video Question Answering via Gradually Refined Attention over Appearance and Motion", "Miech et al. (2020) Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. 2020. End-to-End Learning of Visual Representations from Uncurated Instructional Videos. In CVPR.": "End-to-End Learning of Visual Representations from Uncurated Instructional Videos", "Zhang et al. (2018) Bowen Zhang, Hexiang Hu, and Fei Sha. 2018. Cross-modal and hierarchical modeling of video and text. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, pages 385\u2013401.": "Cross-modal and hierarchical modeling of video and text", "Patrick et al. (2021) Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for video-text representation learning. In International Conference on Learning Representations.": "Support-set bottlenecks for video-text representation learning"}, "source_title_to_arxiv_id": {"Frozen in time: A joint video and image encoder for end-to-end retrieval": "2104.00650", "Coot: Cooperative Hierarchical Transformer for Video-Text Representation Learning": "2011.00597"}}