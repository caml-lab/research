{"title": "LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery", "abstract": "Creating high-quality articulated 3D models of animals is challenging either\nvia manual creation or using 3D scanning tools. Therefore, techniques to\nreconstruct articulated 3D objects from 2D images are crucial and highly\nuseful. In this work, we propose a practical problem setting to estimate 3D\npose and shape of animals given only a few (10-30) in-the-wild images of a\nparticular animal species (say, horse). Contrary to existing works that rely on\npre-defined template shapes, we do not assume any form of 2D or 3D ground-truth\nannotations, nor do we leverage any multi-view or temporal information.\nMoreover, each input image ensemble can contain animal instances with varying\nposes, backgrounds, illuminations, and textures. Our key insight is that 3D\nparts have much simpler shape compared to the overall animal and that they are\nrobust w.r.t. animal pose articulations. Following these insights, we propose\nLASSIE, a novel optimization framework which discovers 3D parts in a\nself-supervised manner with minimal user intervention. A key driving force\nbehind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory\ndeep features. Experiments on Pascal-Part and self-collected in-the-wild animal\ndatasets demonstrate considerably better 3D reconstructions as well as both 2D\nand 3D part discovery compared to prior arts. Project page:\nchhankyao.github.io/lassie/", "authors": ["Chun-Han Yao", "Wei-Chih Hung", "Yuanzhen Li", "Michael Rubinstein", "Ming-Hsuan Yang", "Varun Jampani"], "published_date": "2022_07_07", "pdf_url": "http://arxiv.org/pdf/2207.03434v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><td colspan=\"3\">Pascal-Part dataset</td><td colspan=\"6\">Our dataset</td></tr><tr><th>Method</th><td>Horse</td><td>Cow</td><td>Sheep</td><td>Zebra</td><td>Tiger</td><td>Giraffe</td><td>Elephant</td><td>Kangaroo</td><td>Penguin</td></tr><tr><th>3D Safari [40]</th><td>71.8/ 57.1</td><td>63.4/ 50.3</td><td>62.6/ 50.5</td><td>80.8/ 62.1</td><td>63.4/ 50.3</td><td>57.6/ 32.5</td><td>55.4/ 29.9</td><td>35.5/ 20.7</td><td>49.3/ 28.9</td></tr><tr><th>A-CSM [18]</th><td>69.3/ 55.3</td><td>68.8/ 60.5</td><td>67.4/ 54.7</td><td>78.5/ 60.3</td><td>69.1/ 55.7</td><td>71.2/ 52.2</td><td>67.3/ 39.5</td><td>42.1/ 26.9</td><td>53.7/ 33.0</td></tr><tr><th>LASSIE w/o \\mathcal{L}_{sem}</th><td>60.4/ 45.6</td><td>58.9/ 43.1</td><td>55.3/ 42.3</td><td>62.7/ 47.5</td><td>53.6/ 40.0</td><td>54.7/ 28.9</td><td>52.0/ 25.6</td><td>33.8/ 19.8</td><td>50.6/ 25.5</td></tr><tr><th>LASSIE w/o \\mathcal{F}^{p}</th><td>71.1/ 56.3</td><td>69.0/ 59.5</td><td>68.9/ 52.2</td><td>77.0/ 60.2</td><td>71.5/ 59.2</td><td>79.3/ 57.8</td><td>66.6/ 37.1</td><td>44.3/ 30.1</td><td>63.3/ 38.2</td></tr><tr><th>LASSIE</th><td>73.0/ 58.0</td><td>71.3/ 62.4</td><td>70.8/ 55.5</td><td>79.9/ 63.3</td><td>73.3/ 62.4</td><td>80.8/ 60.5</td><td>68.7/ 40.3</td><td>47.0/ 31.5</td><td>65.5/ 40.6</td></tr><tr><th>LASSIE-heuristic</th><td>72.1/ 57.0</td><td>69.5/ 61.1</td><td>69.7/ 55.3</td><td>78.9/ 61.9</td><td>71.9/ 60.0</td><td>80.4/ 60.1</td><td>66.5/ 38.7</td><td>45.6/ 30.9</td><td>60.8/ 37.6</td></tr></tbody></table>", "caption": "Table 1: Keypoint transfer evaluations. We evaluate on all the source-target image pairs and report the percentage of correct keypoints under two different thresholds (PCK@0.1/ PCK@0.05).", "list_citation_info": ["[40] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and Michael J Black. Three-d safari: Learning to estimate zebra pose, shape, and texture from images\" in the wild\". In ICCV, pages 5359\u20135368, 2019.", "[18] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shubham Tulsiani. Articulation-aware canonical surface mapping. In CVPR, pages 452\u2013461, 2020."]}, {"table": "<table><tbody><tr><th></th><td colspan=\"3\">Overall IOU</td><td colspan=\"3\">Part IOU</td><td colspan=\"3\">Part transfer (PCP)</td></tr><tr><th>Method</th><td>Horse</td><td>Cow</td><td>Sheep</td><td>Horse</td><td>Cow</td><td>Sheep</td><td>Horse</td><td>Cow</td><td>Sheep</td></tr><tr><th>SCOPS [11]</th><td>62.9</td><td>67.7</td><td>63.2</td><td>23.0</td><td>19.1</td><td>26.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DINO clustering [1]</th><td>81.3</td><td>85.1</td><td>83.9</td><td>26.3</td><td>21.8</td><td>30.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>3D Safari [40]</th><td>72.2</td><td>71.3</td><td>70.8</td><td>-</td><td>-</td><td>-</td><td>71.7</td><td>69.0</td><td>69.3</td></tr><tr><th>A-CSM [18]</th><td>72.5</td><td>73.4</td><td>71.9</td><td>-</td><td>-</td><td>-</td><td>73.8</td><td>71.1</td><td>72.5</td></tr><tr><th>LASSIE w/o \\mathcal{L}_{sem}</th><td>65.3</td><td>67.5</td><td>60.0</td><td>29.7</td><td>23.2</td><td>31.5</td><td>62.0</td><td>60.3</td><td>59.7</td></tr><tr><th>LASSIE w/o \\mathcal{F}^{p}</th><td>81.0</td><td>86.5</td><td>85.0</td><td>37.1</td><td>33.7</td><td>41.9</td><td>76.1</td><td>75.6</td><td>72.4</td></tr><tr><th>LASSIE</th><td>81.9</td><td>87.1</td><td>85.5</td><td>38.2</td><td>35.1</td><td>43.7</td><td>78.5</td><td>77.0</td><td>74.3</td></tr></tbody></table>", "caption": "Table 2: Quantitative evaluations on the Pascal-part images. We report the overall foreground IOU, part mask IOU, and percentage of correct pixels (PCP) under dense part segmentation transfer between all pairs of source-target images.", "list_citation_info": ["[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2021.", "[11] Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz. Scops: Self-supervised co-part segmentation. In CVPR, pages 869\u2013878, 2019.", "[40] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and Michael J Black. Three-d safari: Learning to estimate zebra pose, shape, and texture from images\" in the wild\". In ICCV, pages 5359\u20135368, 2019.", "[18] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shubham Tulsiani. Articulation-aware canonical surface mapping. In CVPR, pages 452\u2013461, 2020."]}], "citation_info_to_title": {"[18] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shubham Tulsiani. Articulation-aware canonical surface mapping. In CVPR, pages 452\u2013461, 2020.": "Articulation-aware Canonical Surface Mapping", "[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2021.": "Deep Vit Features as Dense Visual Descriptors", "[40] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and Michael J Black. Three-d safari: Learning to estimate zebra pose, shape, and texture from images\" in the wild\". In ICCV, pages 5359\u20135368, 2019.": "Three-d safari: Learning to estimate zebra pose, shape, and texture from images in the wild", "[11] Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz. Scops: Self-supervised co-part segmentation. In CVPR, pages 869\u2013878, 2019.": "Scops: Self-supervised co-part segmentation"}, "source_title_to_arxiv_id": {"Deep Vit Features as Dense Visual Descriptors": "2112.05814"}}