{"title": "Learning Where to Learn in Cross-View Self-Supervised Learning", "abstract": "Self-supervised learning (SSL) has made enormous progress and largely\nnarrowed the gap with the supervised ones, where the representation learning is\nmainly guided by a projection into an embedding space. During the projection,\ncurrent methods simply adopt uniform aggregation of pixels for embedding;\nhowever, this risks involving object-irrelevant nuisances and spatial\nmisalignment for different augmentations. In this paper, we present a new\napproach, Learning Where to Learn (LEWEL), to adaptively aggregate spatial\ninformation of features, so that the projected embeddings could be exactly\naligned and thus guide the feature learning better. Concretely, we reinterpret\nthe projection head in SSL as a per-pixel projection and predict a set of\nspatial alignment maps from the original features by this weight-sharing\nprojection head. A spectrum of aligned embeddings is thus obtained by\naggregating the features with spatial weighting according to these alignment\nmaps. As a result of this adaptive alignment, we observe substantial\nimprovements on both image-level prediction and dense prediction at the same\ntime: LEWEL improves MoCov2 by 1.6%/1.3%/0.5%/0.4% points, improves BYOL by\n1.3%/1.3%/0.7%/0.6% points, on ImageNet linear/semi-supervised classification,\nPascal VOC semantic segmentation, and object detection, respectively.", "authors": ["Lang Huang", "Shan You", "Mingkai Zheng", "Fei Wang", "Chen Qian", "Toshihiko Yamasaki"], "published_date": "2022_03_28", "pdf_url": "http://arxiv.org/pdf/2203.14898v1", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">100 Epochs</th><th colspan=\"2\">200 Epochs</th><th>400 Epochs</th></tr><tr><th>Acc@1</th><th>Acc@5</th><th>Acc@1</th><th>Acc@5</th><th>Acc@1</th></tr></thead><tbody><tr><th>InstDisc [42]</th><td>-</td><td>-</td><td>56.5</td><td>-</td><td>-</td></tr><tr><th>PCL [23]</th><td>-</td><td>-</td><td>67.6</td><td>-</td><td>-</td></tr><tr><th>SimCLR [7]</th><td>64.6</td><td>-</td><td>66.6</td><td>-</td><td>-</td></tr><tr><th>SimCLR [7]{}^{\\dagger}</th><td>66.5</td><td>-</td><td>68.3</td><td>-</td><td>70.4</td></tr><tr><th>BYOL [14]{}^{\\dagger}</th><td>66.5</td><td>-</td><td>70.6</td><td>-</td><td>73.2</td></tr><tr><th>SwAV [4]{}^{\\dagger}</th><td>66.5</td><td>-</td><td>69.1</td><td>-</td><td>70.7</td></tr><tr><th>SimSiam [9]{}^{\\dagger}</th><td>68.1</td><td>-</td><td>70.0</td><td>-</td><td>70.8</td></tr><tr><th>MoCov2 [15]{}^{*}</th><td>64.5</td><td>86.1</td><td>67.5</td><td>88.1</td><td>-</td></tr><tr><th>BYOL [14]{}^{*}</th><td>70.6</td><td>89.9</td><td>71.9</td><td>90.4</td><td>-</td></tr><tr><th>LEWEL{}_{M}</th><td>66.1</td><td>87.2</td><td>68.4</td><td>88.6</td><td>-</td></tr><tr><th>LEWEL{}_{B}</th><td>71.9</td><td>90.5</td><td>72.8</td><td>91.0</td><td>73.8</td></tr></tbody></table>", "caption": "Table 1: Comparison on IN-1K linear classification with the ResNet-50 models pre-trained on the IN-1K dataset. {\\dagger}: results cited from [9]. *: our reproduction.", "list_citation_info": ["[9] Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. arXiv preprint arXiv:2011.10566, 2020.", "[42] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3733\u20133742, 2018.", "[14] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33, 2020.", "[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33, 2020.", "[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.", "[23] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2020.", "[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Epochs</th><th colspan=\"2\">1% Labels</th><th colspan=\"2\">10% Labels</th></tr><tr><th>Acc@1</th><th>Acc@5</th><th>Acc@1</th><th>Acc@5</th></tr></thead><tbody><tr><th>PCL [23]</th><th>200</th><td>-</td><td>75.3</td><td>-</td><td>86.5</td></tr><tr><th>MoCov2 [15]{}^{*}</th><th>200</th><td>43.8</td><td>72.3</td><td>61.9</td><td>84.6</td></tr><tr><th>BYOL [14]{}^{*}</th><th>200</th><td>54.8</td><td>78.8</td><td>68.0</td><td>88.5</td></tr><tr><th>LEWEL{}_{M}</th><th>200</th><td>45.1</td><td>71.1</td><td>62.5</td><td>84.9</td></tr><tr><th>LEWEL{}_{B}</th><th>200</th><td>56.1</td><td>79.9</td><td>68.7</td><td>88.9</td></tr><tr><th>SimCLR [7]</th><th>1000</th><td>48.3</td><td>75.5</td><td>65.6</td><td>87.8</td></tr><tr><th>SwAV [4]</th><th>800</th><td>53.9</td><td>78.5</td><td>70.2</td><td>89.9</td></tr><tr><th>BYOL [14]</th><th>800</th><td>53.2</td><td>78.4</td><td>68.8</td><td>89.0</td></tr><tr><th>BarlowTw. [46]</th><th>1000</th><td>55.0</td><td>79.2</td><td>69.7</td><td>89.3</td></tr><tr><th>LEWEL{}_{B}</th><th>400</th><td>59.8</td><td>83.2</td><td>70.4</td><td>90.1</td></tr></tbody></table>", "caption": "Table 2: Comparison on IN-1K semi-supervised classification with the ResNet-50 models pre-trained on the IN-1K dataset. *: our reproductions.", "list_citation_info": ["[46] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.", "[14] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33, 2020.", "[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33, 2020.", "[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.", "[23] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2020.", "[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Epochs</th><td colspan=\"3\">VOC 07+12 Det.</td><td>12 Seg.</td></tr><tr><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>mIoU</td></tr><tr><th>Supervised{}^{\\dagger}</th><th>90</th><td>53.5</td><td>81.3</td><td>58.8</td><td>67.7</td></tr><tr><th>MoCov2 [15]{}^{*}</th><th>100</th><td>56.1</td><td>81.5</td><td>62.4</td><td>66.3</td></tr><tr><th>BYOL [14]{}^{*}</th><th>100</th><td>55.5</td><td>81.9</td><td>61.2</td><td>66.9</td></tr><tr><th>LEWEL{}_{M}</th><th>100</th><td>56.5</td><td>82.1</td><td>63.0</td><td>66.8</td></tr><tr><th>LEWEL{}_{B}</th><th>100</th><td>56.1</td><td>82.1</td><td>62.3</td><td>67.6</td></tr><tr><th>SimCLR [7]{}^{\\dagger}</th><th>200</th><td>55.5</td><td>81.8</td><td>61.4</td><td>-</td></tr><tr><th>SwAV [3]{}^{\\dagger}</th><th>200</th><td>55.4</td><td>81.5</td><td>61.4</td><td>-</td></tr><tr><th>BYOL [14]{}^{\\dagger}</th><th>200</th><td>55.3</td><td>81.4</td><td>61.1</td><td>-</td></tr><tr><th>SimSiam [9]{}^{\\dagger}</th><th>200</th><td>56.4</td><td>82.0</td><td>62.8</td><td>-</td></tr><tr><th>MoCov2 [15]{}^{*}</th><th>200</th><td>57.0</td><td>82.2</td><td>63.4</td><td>66.7</td></tr><tr><th>BYOL [14]{}^{*}</th><th>200</th><td>55.8</td><td>81.6</td><td>61.6</td><td>67.2</td></tr><tr><th>LEWEL{}_{M}</th><th>200</th><td>57.3</td><td>82.3</td><td>63.6</td><td>67.2</td></tr><tr><th>LEWEL{}_{B}</th><th>200</th><td>56.5</td><td>82.6</td><td>63.7</td><td>67.8</td></tr></tbody></table>", "caption": "Table 3: Transfer learning to Pascal-VOC Object Detection and Semantic Segmentation with models pre-trained on IN-1K datasets. All entries are based on the Faster R-CNN [34] architecture with the ResNet-50 C4 backbone [41]. {\\dagger}: results cited from [9]. *: our reproductions.", "list_citation_info": ["[9] Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. arXiv preprint arXiv:2011.10566, 2020.", "[41] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.", "[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In European Conference on Computer Vision, pages 132\u2013149, 2018.", "[14] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33, 2020.", "[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91\u201399, 2015.", "[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.", "[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td colspan=\"3\">Object Det.</td><td colspan=\"3\">Instance Seg.</td></tr><tr><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td></tr><tr><th colspan=\"7\">ResNet50-C4:</th></tr><tr><th>Supervised{}^{\\dagger}</th><td>38.2</td><td>58.2</td><td>41.2</td><td>33.3</td><td>54.7</td><td>35.2</td></tr><tr><th>SimCLR [7]{}^{\\dagger}</th><td>37.9</td><td>57.7</td><td>40.9</td><td>33.3</td><td>54.6</td><td>35.3</td></tr><tr><th>SwAV [3]{}^{\\dagger}</th><td>37.6</td><td>57.6</td><td>40.3</td><td>33.1</td><td>54.2</td><td>35.1</td></tr><tr><th>BYOL [14]{}^{\\dagger}</th><td>37.9</td><td>57.8</td><td>40.9</td><td>33.2</td><td>54.3</td><td>35.0</td></tr><tr><th>SimSiam [9]{}^{\\dagger}</th><td>37.9</td><td>57.5</td><td>40.9</td><td>33.2</td><td>54.2</td><td>35.2</td></tr><tr><th>MoCov2 [15]{}^{*}</th><td>38.8</td><td>58.0</td><td>42.0</td><td>34.0</td><td>55.2</td><td>36.3</td></tr><tr><th>BYOL [14]{}^{*}</th><td>38.1</td><td>58.4</td><td>40.9</td><td>33.3</td><td>55.0</td><td>35.3</td></tr><tr><th>LEWEL{}_{M}</th><td>38.9</td><td>58.6</td><td>42.0</td><td>34.1</td><td>55.3</td><td>36.3</td></tr><tr><th>LEWEL{}_{B}</th><td>38.5</td><td>58.9</td><td>41.2</td><td>33.7</td><td>55.5</td><td>35.5</td></tr><tr><th colspan=\"7\">ResNet50-FPN:</th></tr><tr><th>DenseCL [40]</th><td>40.3</td><td>59.9</td><td>44.3</td><td>36.4</td><td>57.0</td><td>39.2</td></tr><tr><th>ReSim [43]</th><td>39.8</td><td>60.2</td><td>43.5</td><td>36.0</td><td>57.1</td><td>38.6</td></tr><tr><th>LEWEL{}_{M}</th><td>40.0</td><td>59.8</td><td>43.7</td><td>36.1</td><td>57.0</td><td>38.7</td></tr><tr><th>LEWEL{}_{B}</th><td>41.3</td><td>61.2</td><td>45.4</td><td>37.4</td><td>58.3</td><td>40.3</td></tr><tr><th>PixelPro [44] (400 ep)</th><td>41.4</td><td>61.6</td><td>45.4</td><td>37.4</td><td>-</td><td>-</td></tr><tr><th>LEWEL{}_{B} (400 ep)</th><td>41.9</td><td>62.4</td><td>46.0</td><td>37.9</td><td>59.3</td><td>40.7</td></tr></tbody></table>", "caption": "Table 4: Transfer learning to MS-COCO Object Detection and Instance Segmentation with models pre-trained for 200 epochs on IN-1K dataset. All entries are based on the Mask R-CNN [16] architecture. {\\dagger}: results from [9]. *: our reproduction.", "list_citation_info": ["[9] Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. arXiv preprint arXiv:2011.10566, 2020.", "[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In European Conference on Computer Vision, pages 132\u2013149, 2018.", "[14] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33, 2020.", "[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.", "[43] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, and Trevor Darrell. Region similarity representation learning. arXiv preprint arXiv:2103.12902, 2021.", "[40] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.", "[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.", "[44] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693, 2021.", "[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>Extra p_{\\theta}</th><th>d</th><th>IN-100 Acc.</th><th>VOC mIoU</th></tr></thead><tbody><tr><td rowspan=\"4\">MoCov2 [15]</td><td>\\times</td><td>128</td><td>79.5</td><td>61.6</td></tr><tr><td>\\times</td><td>256</td><td>79.8</td><td>60.6</td></tr><tr><td>\\times</td><td>512</td><td>80.2</td><td>61.5</td></tr><tr><td>\\checkmark</td><td>128</td><td>79.9</td><td>62.2</td></tr><tr><td>LEWEL{}_{M}</td><td>\\times</td><td>128</td><td>81.8</td><td>63.0</td></tr><tr><td>LEWEL{}_{M} w/ rand. \\mathbf{W}</td><td>\\checkmark</td><td>128</td><td>79.8</td><td>61.9</td></tr><tr><td>LEWEL{}_{M}</td><td>\\checkmark</td><td>128</td><td>82.1</td><td>63.4</td></tr></tbody></table>", "caption": "Table 6: Comparison with MoCo with large projection heads on IN-100 linear classification and VOC semantic segmentation.", "list_citation_info": ["[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">Epoch</th><td colspan=\"2\">IN-1K</td><td colspan=\"6\">MS-COCO (1\\times Schedule)</td><td colspan=\"6\">MS-COCO (2\\times Schedule)</td></tr><tr><td>Acc@1</td><td>Acc@5</td><td>AP{}^{\\mathrm{b}}</td><td>AP{}^{\\mathrm{b}}_{50}</td><td>AP{}^{\\mathrm{b}}_{75}</td><td>AP{}^{\\mathrm{m}}</td><td>AP{}^{\\mathrm{m}}_{50}</td><td>AP{}^{\\mathrm{m}}_{75}</td><td>AP{}^{\\mathrm{b}}</td><td>AP{}^{\\mathrm{b}}_{50}</td><td>AP{}^{\\mathrm{b}}_{75}</td><td>AP{}^{\\mathrm{m}}</td><td>AP{}^{\\mathrm{m}}_{50}</td><td>AP{}^{\\mathrm{m}}_{75}</td></tr><tr><th>DenseCL [40]</th><th>200</th><td>63.6</td><td>85.8</td><td>40.3</td><td>59.9</td><td>44.3</td><td>36.4</td><td>57.0</td><td>39.2</td><td>41.2</td><td>61.9</td><td>45.1</td><td>37.3</td><td>58.9</td><td>40.1</td></tr><tr><th>ReSim [43]</th><th>200</th><td>66.1</td><td>-</td><td>39.8</td><td>60.2</td><td>43.5</td><td>36.0</td><td>57.1</td><td>38.6</td><td>41.4</td><td>61.9</td><td>45.4</td><td>37.5</td><td>59.1</td><td>40.3</td></tr><tr><th>LEWEL{}_{M}</th><th>200</th><td>68.1</td><td>88.6</td><td>40.0</td><td>59.8</td><td>43.7</td><td>36.1</td><td>57.0</td><td>38.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>LEWEL{}_{B}</th><th>200</th><td>72.8</td><td>91.0</td><td>41.3</td><td>61.2</td><td>45.4</td><td>37.4</td><td>58.3</td><td>40.3</td><td>42.2</td><td>62.3</td><td>46.1</td><td>38.2</td><td>59.6</td><td>41.1</td></tr><tr><th>PixelPro [44]</th><th>400</th><td>60.2</td><td>83.0</td><td>41.4</td><td>61.6</td><td>45.4</td><td>37.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>LEWEL{}_{B}</th><th>400</th><td>73.8</td><td>91.7</td><td>41.9</td><td>62.4</td><td>46.0</td><td>37.9</td><td>59.3</td><td>40.7</td><td>43.4</td><td>63.5</td><td>47.7</td><td>39.1</td><td>60.7</td><td>42.4</td></tr></tbody></table>", "caption": "Table 8: Comparison with hand-crafted spatial alignment methods. The experiments on MS-COCO is based on Mask R-CNN architecture with the ResNet-50 FPN backbone and the 1\\times/2\\times schedule [41], following [40, 44, 43].", "list_citation_info": ["[43] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, and Trevor Darrell. Region similarity representation learning. arXiv preprint arXiv:2103.12902, 2021.", "[41] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.", "[40] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.", "[44] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693, 2021."]}], "citation_info_to_title": {"[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33, 2020.": "Unsupervised learning of visual features by contrasting cluster assignments", "[9] Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. arXiv preprint arXiv:2011.10566, 2020.": "Exploring Simple Siamese Representation Learning", "[46] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised learning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.": "Barlow twins: Self-supervised learning via redundancy reduction", "[14] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33, 2020.": "Bootstrap your own latent: A new approach to self-supervised learning", "[40] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.": "Dense contrastive learning for self-supervised visual pre-training", "[16] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.": "Mask R-CNN", "[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In European Conference on Computer Vision, pages 132\u2013149, 2018.": "Deep clustering for unsupervised learning of visual features", "[42] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3733\u20133742, 2018.": "Unsupervised feature learning via non-parametric instance discrimination", "[34] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91\u201399, 2015.": "Faster r-cnn: Towards real-time object detection with region proposal networks", "[44] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693, 2021.": "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning", "[23] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive learning of unsupervised representations. In International Conference on Learning Representations, 2020.": "Prototypical contrastive learning of unsupervised representations", "[41] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.": "Detectron2", "[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.": "A simple framework for contrastive learning of visual representations", "[43] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, and Trevor Darrell. Region similarity representation learning. arXiv preprint arXiv:2103.12902, 2021.": "Region Similarity Representation Learning", "[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.": "Momentum contrast for unsupervised visual representation learning"}, "source_title_to_arxiv_id": {"Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning": "2011.10043"}}