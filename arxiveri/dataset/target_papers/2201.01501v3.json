{"title": "Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation", "abstract": "Depth estimation is solved as a regression or classification problem in\nexisting learning-based multi-view stereo methods. Although these two\nrepresentations have recently demonstrated their excellent performance, they\nstill have apparent shortcomings, e.g., regression methods tend to overfit due\nto the indirect learning cost volume, and classification methods cannot\ndirectly infer the exact depth due to its discrete prediction. In this paper,\nwe propose a novel representation, termed Unification, to unify the advantages\nof regression and classification. It can directly constrain the cost volume\nlike classification methods, but also realize the sub-pixel depth prediction\nlike regression methods. To excavate the potential of unification, we design a\nnew loss function named Unified Focal Loss, which is more uniform and\nreasonable to combat the challenge of sample imbalance. Combining these two\nunburdened modules, we present a coarse-to-fine framework, that we call\nUniMVSNet. The results of ranking first on both DTU and Tanks and Temples\nbenchmarks verify that our model not only performs the best but also has the\nbest generalization ability.", "authors": ["Rui Peng", "Rongjie Wang", "Zhenyu Wang", "Yawen Lai", "Ronggang Wang"], "published_date": "2022_01_05", "pdf_url": "http://arxiv.org/pdf/2201.01501v3", "list_table_and_caption": [{"table": "<table><thead><tr><th>Method</th><th>ACC.(mm)</th><th>Comp.(mm)</th><th>Overall(mm)</th></tr></thead><tbody><tr><th>Furu [7]</th><td>0.613</td><td>0.941</td><td>0.777</td></tr><tr><th>Gipuma [8]</th><td>0.283</td><td>0.873</td><td>0.578</td></tr><tr><th>COLMAP [24, 25]</th><td>0.400</td><td>0.664</td><td>0.532</td></tr><tr><th>SurfaceNet [11]</th><td>0.450</td><td>1.040</td><td>0.745</td></tr><tr><th>MVSNet [35]</th><td>0.396</td><td>0.527</td><td>0.462</td></tr><tr><th>P-MVSNet [20]</th><td>0.406</td><td>0.434</td><td>0.420</td></tr><tr><th>R-MVSNet [36]</th><td>0.383</td><td>0.452</td><td>0.417</td></tr><tr><th>Point-MVSNet [4]</th><td>0.342</td><td>0.411</td><td>0.376</td></tr><tr><th>AA-RMVSNet [30]</th><td>0.376</td><td>0.339</td><td>0.357</td></tr><tr><th>CasMVSNet [9]</th><td>0.325</td><td>0.385</td><td>0.355</td></tr><tr><th>CVP-MVSNet [34]</th><td>0.296</td><td>0.406</td><td>0.351</td></tr><tr><th>UCS-Net [5]</th><td>0.338</td><td>0.349</td><td>0.344</td></tr><tr><th>UniMVSNet (ours)</th><td>0.352</td><td>0.278</td><td>0.315</td></tr></tbody></table>", "caption": "Table 1: Quantitative results on DTU evaluation set. Best results ineach category are in bold. Our model ranks first in terms of Completenessand Overall metrics.", "list_citation_info": ["[5] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In CVPR, pages 2524\u20132534, 2020.", "[4] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In ICCV, pages 1538\u20131547, 2019.", "[36] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In CVPR, pages 5525\u20135534, 2019.", "[20] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In ICCV, pages 10452\u201310461, 2019.", "[9] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, pages 2495\u20132504, 2020.", "[8] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In ICCV, pages 873\u2013881, 2015.", "[35] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In ECCV, pages 767\u2013783, 2018.", "[34] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In CVPR, pages 4877\u20134886, 2020.", "[24] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, pages 4104\u20134113, 2016.", "[7] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE TPAMI, 32(8):1362\u20131376, 2009.", "[11] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d neural network for multiview stereopsis. In ICCV, pages 2307\u20132315, 2017.", "[30] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. In ICCV, 2021."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"9\">Intermediate</th><th colspan=\"7\">Advanced</th></tr><tr><th>Mean</th><th>Fam.</th><th>Fra.</th><th>Hor.</th><th>Lig.</th><th>M60</th><th>Pan.</th><th>Pla.</th><th>Tra.</th><th>Mean</th><th>Aud.</th><th>Bal.</th><th>Cou.</th><th>Mus.</th><th>Pal.</th><th>Tem.</th></tr></thead><tbody><tr><th>Point-MVSNet [4]</th><td>48.27</td><td>61.79</td><td>41.15</td><td>34.20</td><td>50.79</td><td>51.97</td><td>50.85</td><td>52.38</td><td>43.06</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PatchmatchNet [29]</th><td>53.15</td><td>66.99</td><td>52.64</td><td>43.24</td><td>54.87</td><td>52.87</td><td>49.54</td><td>54.21</td><td>50.81</td><td>32.31</td><td>23.69</td><td>37.73</td><td>30.04</td><td>41.80</td><td>28.31</td><td>32.29</td></tr><tr><th>UCS-Net [5]</th><td>54.83</td><td>76.09</td><td>53.16</td><td>43.03</td><td>54.00</td><td>55.60</td><td>51.49</td><td>57.38</td><td>47.89</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CVP-MVSNet [34]</th><td>54.03</td><td>76.50</td><td>47.74</td><td>36.34</td><td>55.12</td><td>57.28</td><td>54.28</td><td>57.43</td><td>47.54</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>P-MVSNet [20]</th><td>55.62</td><td>70.04</td><td>44.64</td><td>40.22</td><td>65.20</td><td>55.08</td><td>55.17</td><td>60.37</td><td>54.29</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>CasMVSNet [9]</th><td>56.84</td><td>76.37</td><td>58.45</td><td>46.26</td><td>55.81</td><td>56.11</td><td>54.06</td><td>58.18</td><td>49.51</td><td>31.12</td><td>19.81</td><td>38.46</td><td>29.10</td><td>43.87</td><td>27.36</td><td>28.11</td></tr><tr><th>ACMP [32]</th><td>58.41</td><td>70.30</td><td>54.06</td><td>54.11</td><td>61.65</td><td>54.16</td><td>57.60</td><td>58.12</td><td>57.25</td><td>37.44</td><td>30.12</td><td>34.68</td><td>44.58</td><td>50.64</td><td>27.20</td><td>37.43</td></tr><tr><th>D^{2}HC-RMVSNet [33]</th><td>59.20</td><td>74.69</td><td>56.04</td><td>49.42</td><td>60.08</td><td>59.81</td><td>59.61</td><td>60.04</td><td>53.92</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>VisMVSNet [41]</th><td>60.03</td><td>77.40</td><td>60.23</td><td>47.07</td><td>63.44</td><td>62.21</td><td>57.28</td><td>60.54</td><td>52.07</td><td>33.78</td><td>20.79</td><td>38.77</td><td>32.45</td><td>44.20</td><td>28.73</td><td>37.70</td></tr><tr><th>AA-RMVSNet [30]</th><td>61.51</td><td>77.77</td><td>59.53</td><td>51.53</td><td>64.02</td><td>64.05</td><td>59.47</td><td>60.85</td><td>55.50</td><td>33.53</td><td>20.96</td><td>40.15</td><td>32.05</td><td>46.01</td><td>29.28</td><td>32.71</td></tr><tr><th>EPP-MVSNet [21]</th><td>61.68</td><td>77.86</td><td>60.54</td><td>52.96</td><td>62.33</td><td>61.69</td><td>60.34</td><td>62.44</td><td>55.30</td><td>35.72</td><td>21.28</td><td>39.74</td><td>35.34</td><td>49.21</td><td>30.00</td><td>38.75</td></tr><tr><th>UniMVSNet (ours)</th><td>64.36</td><td>81.20</td><td>66.43</td><td>53.11</td><td>63.46</td><td>66.09</td><td>64.84</td><td>62.23</td><td>57.53</td><td>38.96</td><td>28.33</td><td>44.36</td><td>39.74</td><td>52.89</td><td>33.80</td><td>34.63</td></tr></tbody></table>", "caption": "Table 2: Quantitative results of F-score on Tanks and Temples benchmark.Best results in each category are in bold. \u201cMean\u201d refers to the mean F-score ofall scenes (higher is better).Our model outperforms all previous MVS methods with a significant margin on bothIntermediate and Advanced set.", "list_citation_info": ["[5] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In CVPR, pages 2524\u20132534, 2020.", "[4] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In ICCV, pages 1538\u20131547, 2019.", "[32] Qingshan Xu and Wenbing Tao. Planar prior assisted patchmatch multi-view stereo. In AAAI, volume 34, pages 12516\u201312523, 2020.", "[33] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. In ECCV, pages 674\u2013689, 2020.", "[20] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In ICCV, pages 10452\u201310461, 2019.", "[9] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, pages 2495\u20132504, 2020.", "[41] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. In BMVC, 2020.", "[21] Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. In ICCV, pages 5732\u20135740, 2021.", "[30] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. In ICCV, 2021.", "[29] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In CVPR, pages 14194\u201314203, 2021.", "[34] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In CVPR, pages 4877\u20134886, 2020."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Representation</td><td colspan=\"5\">Loss Function</td><td colspan=\"2\">Aggregation</td><td rowspan=\"2\">FGT</td><td rowspan=\"2\">Input</td><td rowspan=\"2\">ACC.(mm)</td><td rowspan=\"2\">Comp.(mm)</td><td rowspan=\"2\">Overall.(mm)</td></tr><tr><td>Reg</td><td>Cla</td><td>Uni</td><td>L1</td><td>CE</td><td>BCE</td><td>GFL</td><td>UFL</td><td>Adaptive</td><td>Variance</td></tr><tr><td>Baseline (Reg)</td><td>\u2713</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td></td><td></td><td>\u2713</td><td></td><td>3</td><td>0.369</td><td>0.317</td><td>0.343</td></tr><tr><td>Baseline (Reg)</td><td>\u2713</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td></td><td></td><td>\u2713</td><td></td><td>5</td><td>0.368</td><td>0.312</td><td>0.340</td></tr><tr><td>Baseline (Cla)</td><td></td><td>\u2713</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td></td><td>\u2713</td><td></td><td>5</td><td>0.425</td><td>0.285</td><td>0.355</td></tr><tr><td>Baseline (Uni)</td><td></td><td></td><td>\u2713</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td>\u2713</td><td></td><td>5</td><td>0.372</td><td>0.282</td><td>0.327</td></tr><tr><td>Baseline (Uni) + GFL</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td>\u2713</td><td></td><td></td><td>\u2713</td><td></td><td>5</td><td>0.361</td><td>0.289</td><td>0.325</td></tr><tr><td>Baseline (Uni) + UFL</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td></td><td>\u2713</td><td></td><td>\u2713</td><td></td><td>5</td><td>0.353</td><td>0.287</td><td>0.320</td></tr><tr><td>Baseline (Uni) + UFL + AA</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td></td><td>\u2713</td><td>\u2713</td><td></td><td></td><td>5</td><td>0.355</td><td>0.279</td><td>0.317</td></tr><tr><td>Baseline (Uni) + UFL + AA + FGT</td><td></td><td></td><td>\u2713</td><td></td><td></td><td></td><td></td><td>\u2713</td><td>\u2713</td><td></td><td>\u2713</td><td>5</td><td>0.352</td><td>0.278</td><td>0.315</td></tr></tbody></table>", "caption": "Table 3: Ablation results on DTU evaluation set. \u201cAA\u201dand \u201cFGT\u201d refer to adaptive aggregation and finerground-truth respectively.\u201cBaseline (Reg)\u201d is the original CasMVSNet [9]. We set the confidencethreshold and the consistent views to 0.3 and 3 for all models. ", "list_citation_info": ["[9] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, pages 2495\u20132504, 2020."]}], "citation_info_to_title": {"[11] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d neural network for multiview stereopsis. In ICCV, pages 2307\u20132315, 2017.": "SurfaceNet: An End-to-End 3D Neural Network for Multiview Stereopsis", "[30] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. In ICCV, 2021.": "Aa-rmvsnet: Adaptive Aggregation Recurrent Multi-View Stereo Network", "[41] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. In BMVC, 2020.": "Visibility-aware Multi-View Stereo Network", "[35] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In ECCV, pages 767\u2013783, 2018.": "Mvsnet: Depth inference for unstructured multi-view stereo", "[32] Qingshan Xu and Wenbing Tao. Planar prior assisted patchmatch multi-view stereo. In AAAI, volume 34, pages 12516\u201312523, 2020.": "Planar Prior Assisted PatchMatch Multi-View Stereo", "[36] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In CVPR, pages 5525\u20135534, 2019.": "Recurrent MVSNet for High-Resolution Multi-View Stereo Depth Inference", "[33] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. In ECCV, pages 674\u2013689, 2020.": "Dense hybrid recurrent multi-view stereo net with dynamic consistency checking", "[21] Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. In ICCV, pages 5732\u20135740, 2021.": "Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo", "[24] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, pages 4104\u20134113, 2016.": "Structure-from-motion revisited", "[4] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In ICCV, pages 1538\u20131547, 2019.": "Point-based multi-view stereo network", "[20] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In ICCV, pages 10452\u201310461, 2019.": "P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo", "[8] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In ICCV, pages 873\u2013881, 2015.": "Massively Parallel Multiview Stereopsis by Surface Normal Diffusion", "[29] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In CVPR, pages 14194\u201314203, 2021.": "Patchmatchnet: Learned Multi-View Patchmatch Stereo", "[7] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE TPAMI, 32(8):1362\u20131376, 2009.": "Accurate, dense, and robust multiview stereopsis", "[9] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, pages 2495\u20132504, 2020.": "Cascade cost volume for high-resolution multi-view stereo and stereo matching", "[34] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In CVPR, pages 4877\u20134886, 2020.": "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo", "[5] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In CVPR, pages 2524\u20132534, 2020.": "Deep stereo using adaptive thin volume representation with uncertainty awareness"}, "source_title_to_arxiv_id": {"Aa-rmvsnet: Adaptive Aggregation Recurrent Multi-View Stereo Network": "2108.03824"}}