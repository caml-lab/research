{"title": "MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment", "abstract": "No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual\nquality of images in accordance with human subjective perception.\nUnfortunately, existing NR-IQA methods are far from meeting the needs of\npredicting accurate quality scores on GAN-based distortion images. To this end,\nwe propose Multi-dimension Attention Network for no-reference Image Quality\nAssessment (MANIQA) to improve the performance on GAN-based distortion. We\nfirstly extract features via ViT, then to strengthen global and local\ninteractions, we propose the Transposed Attention Block (TAB) and the Scale\nSwin Transformer Block (SSTB). These two modules apply attention mechanisms\nacross the channel and spatial dimension, respectively. In this\nmulti-dimensional manner, the modules cooperatively increase the interaction\namong different regions of images globally and locally. Finally, a dual branch\nstructure for patch-weighted quality prediction is applied to predict the final\nscore depending on the weight of each patch's score. Experimental results\ndemonstrate that MANIQA outperforms state-of-the-art methods on four standard\ndatasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our\nmethod ranked first place in the final testing phase of the NTIRE 2022\nPerceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and\nmodels are available at https://github.com/IIGROUP/MANIQA.", "authors": ["Sidi Yang", "Tianhe Wu", "Shuwei Shi", "Shanshan Lao", "Yuan Gong", "Mingdeng Cao", "Jiahao Wang", "Yujiu Yang"], "published_date": "2022_04_19", "pdf_url": "http://arxiv.org/pdf/2204.08958v2", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"2\">LIVE</th><th colspan=\"2\">CSIQ</th><th colspan=\"2\">TID2013</th><th colspan=\"2\">KADID-10K</th></tr><tr><th></th><th>PLCC</th><th>SROCC</th><th>PLCC</th><th>SROCC</th><th>PLCC</th><th>SROCC</th><th>PLCC</th><th>SROCC</th></tr></thead><tbody><tr><th>DIIVINE[34]</th><td>0.908</td><td>0.892</td><td>0.776</td><td>0.804</td><td>0.567</td><td>0.643</td><td>0.435</td><td>0.413</td></tr><tr><th>BRISQUE[28]</th><td>0.944</td><td>0.929</td><td>0.748</td><td>0.812</td><td>0.571</td><td>0.626</td><td>0.567</td><td>0.528</td></tr><tr><th>ILNIQE[55]</th><td>0.906</td><td>0.902</td><td>0.865</td><td>0.822</td><td>0.648</td><td>0.521</td><td>0.558</td><td>0.528</td></tr><tr><th>BIECON[16]</th><td>0.961</td><td>0.958</td><td>0.823</td><td>0.815</td><td>0.762</td><td>0.717</td><td>0.648</td><td>0.623</td></tr><tr><th>MEON[26]</th><td>0.955</td><td>0.951</td><td>0.864</td><td>0.852</td><td>0.824</td><td>0.808</td><td>0.691</td><td>0.604</td></tr><tr><th>WaDIQaM[3]</th><td>0.955</td><td>0.960</td><td>0.844</td><td>0.852</td><td>0.855</td><td>0.835</td><td>0.752</td><td>0.739</td></tr><tr><th>DBCNN[59]</th><td>0.971</td><td>0.968</td><td>0.959</td><td>0.946</td><td>0.865</td><td>0.816</td><td>0.856</td><td>0.851</td></tr><tr><th>TIQA[52]</th><td>0.965</td><td>0.949</td><td>0.838</td><td>0.825</td><td>0.858</td><td>0.846</td><td>0.855</td><td>0.850</td></tr><tr><th>MetaIQA[61]</th><td>0.959</td><td>0.960</td><td>0.908</td><td>0.899</td><td>0.868</td><td>0.856</td><td>0.775</td><td>0.762</td></tr><tr><th>P2P-BM[51]</th><td>0.958</td><td>0.959</td><td>0.902</td><td>0.899</td><td>0.856</td><td>0.862</td><td>0.849</td><td>0.840</td></tr><tr><th>HyperIQA[37]</th><td>0.966</td><td>0.962</td><td>0.942</td><td>0.923</td><td>0.858</td><td>0.840</td><td>0.845</td><td>0.852</td></tr><tr><th>TReS[9]</th><td>0.968</td><td>0.969</td><td>0.942</td><td>0.922</td><td>0.883</td><td>0.863</td><td>0.858</td><td>0.915</td></tr><tr><th>MANIQA(Ours)</th><td>0.983</td><td>0.982</td><td>0.968</td><td>0.961</td><td>0.943</td><td>0.937</td><td>0.946</td><td>0.944</td></tr></tbody></table>", "caption": "Table 1: Comparison of MANIQA v.s. state-of-the-art NR-IQA algorithms on three standard datasets. Bold entries in black and blue are the best and second-best performances, respectively. Some data borrowed from [9].", "list_citation_info": ["[55] Lin Zhang, Lei Zhang, and Alan C Bovik. A feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 2015.", "[59] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou Wang. Blind image quality assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology, 2018.", "[16] Jongyoo Kim and Sanghoon Lee. Fully deep blind image quality predictor. IEEE Journal of Selected Topics in Signal Processing, 2016.", "[26] Kede Ma, Wentao Liu, Kai Zhang, Zhengfang Duanmu, Zhou Wang, and Wangmeng Zuo. End-to-end blind image quality assessment using deep neural networks. IEEE Transactions on Image Processing, 2017.", "[3] Sebastian Bosse, Dominique Maniry, Klaus-Robert M\u00fcller, Thomas Wiegand, and Wojciech Samek. Deep neural networks for no-reference and full-reference image quality assessment. IEEE Transactions on Image Processing, 2017.", "[52] Junyong You and Jari Korhonen. Transformer for image quality assessment. In Proc. of ICIP, 2021.", "[37] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by a self-adaptive hyper network. In Proc. of CVPR, 2020.", "[9] S Alireza Golestaneh, Saba Dadsetan, and Kris M Kitani. No-reference image quality assessment via transformers, relative ranking, and self-consistency. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022.", "[61] Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi. Metaiqa: Deep meta-learning for no-reference image quality assessment. In Proc. of CVPR, 2020.", "[51] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, and Alan Bovik. From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In Proc. of CVPR, 2020.", "[28] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on Image Processing, 2012.", "[34] Michele A Saad, Alan C Bovik, and Christophe Charrier. Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE Transactions on Image Processing, 2012."]}, {"table": "<table><thead><tr><th rowspan=\"2\">IQA Name</th><th colspan=\"2\">Validation</th><th colspan=\"2\">Test</th></tr><tr><th>SROCC</th><th>PLCC</th><th>SROCC</th><th>PLCC</th></tr></thead><tbody><tr><th>PSNR</th><td>0.250</td><td>0.284</td><td>0.269</td><td>0.303</td></tr><tr><th>SSIM[43]</th><td>0.332</td><td>0.386</td><td>0.377</td><td>0.407</td></tr><tr><th>LPIPS-Alex[58]</th><td>0.581</td><td>0.616</td><td>0.584</td><td>0.592</td></tr><tr><th>FSIM[56]</th><td>0.473</td><td>0.575</td><td>0.528</td><td>0.610</td></tr><tr><th>NIQE[29]</th><td>0.005</td><td>0.115</td><td>0.03</td><td>0.112</td></tr><tr><th>MA[24]</th><td>0.129</td><td>0.131</td><td>0.173</td><td>0.224</td></tr><tr><th>PI[2]</th><td>0.079</td><td>0.133</td><td>0.123</td><td>0.153</td></tr><tr><th>Brisque[27]</th><td>0.015</td><td>0.059</td><td>0.087</td><td>0.097</td></tr><tr><th>MANIQA-S</th><td>0.686</td><td>0.707</td><td>0.667</td><td>0.702</td></tr><tr><th>MANIQA-E</th><td>-</td><td>-</td><td>0.704</td><td>0.740</td></tr></tbody></table>", "caption": "Table 3: Performance of different methods on the NTIRE2022 IQA Challenge validation and testing datasets. MANIQA-S and MANIQA-E indicate the single model and ensemble model of MANIQA. Best performances are indicated with bold. Some data borrowed from [12].", "list_citation_info": ["[2] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proc. of CVPR, 2018.", "[56] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. Fsim: A feature similarity index for image quality assessment. IEEE Transactions on Image Processing, 2011.", "[27] Anish Mittal, Anush K Moorthy, and Alan C Bovik. Blind/referenceless image spatial quality evaluator. In Proc. of ASILOMAR, 2011.", "[12] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S. Ren, Radu Timofte, et al. NTIRE 2022 challenge on perceptual image quality assessment. In Proc. of CVPR, 2022.", "[43] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 2004.", "[24] Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and Ming-Hsuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 2017.", "[29] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a \u201ccompletely blind\u201d image quality analyzer. IEEE Signal Processing Letters, 2012.", "[58] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. of CVPR, 2018."]}, {"table": "<table><tbody><tr><th colspan=\"2\">Train on</th><td colspan=\"4\">PIPAL</td></tr><tr><th colspan=\"2\" rowspan=\"2\">Test on</th><td colspan=\"2\">LIVE</td><td colspan=\"2\">TID2013</td></tr><tr><td>PLCC</td><td>SROCC</td><td>PLCC</td><td>SROCC</td></tr><tr><th rowspan=\"3\">FR</th><th>PSNR</th><td>0.873</td><td>0.865</td><td>0.687</td><td>0.677</td></tr><tr><th>WaDIQaM[3]</th><td>0.883</td><td>0.837</td><td>0.698</td><td>0.741</td></tr><tr><th>RADN[36]</th><td>0.905</td><td>0.878</td><td>0.747</td><td>0.796</td></tr><tr><th rowspan=\"2\">NR</th><th>TReS[9]</th><td>0.643</td><td>0.663</td><td>0.516</td><td>0.563</td></tr><tr><th>MANIQA(ours)</th><td>0.835</td><td>0.855</td><td>0.704</td><td>0.619</td></tr></tbody></table>", "caption": "Table 4: Evaluations on cross datasets. FR refers full-reference IQA methods. NR refers no-reference methods. We train each model on PIPAL and test on LIVE and TID2013.", "list_citation_info": ["[36] Shuwei Shi, Qingyan Bai, Mingdeng Cao, Weihao Xia, Jiahao Wang, Yifan Chen, and Yujiu Yang. Region-adaptive deformable network for image quality assessment. In Proc. of CVPR, 2021.", "[3] Sebastian Bosse, Dominique Maniry, Klaus-Robert M\u00fcller, Thomas Wiegand, and Wojciech Samek. Deep neural networks for no-reference and full-reference image quality assessment. IEEE Transactions on Image Processing, 2017.", "[9] S Alireza Golestaneh, Saba Dadsetan, and Kris M Kitani. No-reference image quality assessment via transformers, relative ranking, and self-consistency. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022."]}], "citation_info_to_title": {"[16] Jongyoo Kim and Sanghoon Lee. Fully deep blind image quality predictor. IEEE Journal of Selected Topics in Signal Processing, 2016.": "Fully Deep Blind Image Quality Predictor", "[55] Lin Zhang, Lei Zhang, and Alan C Bovik. A feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 2015.": "A feature-enriched completely blind image quality evaluator", "[37] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by a self-adaptive hyper network. In Proc. of CVPR, 2020.": "Blindly assess image quality in the wild guided by a self-adaptive hyper network", "[43] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 2004.": "Image Quality Assessment: From Error Visibility to Structural Similarity", "[26] Kede Ma, Wentao Liu, Kai Zhang, Zhengfang Duanmu, Zhou Wang, and Wangmeng Zuo. End-to-end blind image quality assessment using deep neural networks. IEEE Transactions on Image Processing, 2017.": "End-to-end blind image quality assessment using deep neural networks", "[24] Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and Ming-Hsuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 2017.": "Learning a no-reference quality metric for single-image super-resolution", "[61] Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi. Metaiqa: Deep meta-learning for no-reference image quality assessment. In Proc. of CVPR, 2020.": "Metaiqa: Deep Meta-Learning for No-Reference Image Quality Assessment", "[56] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. Fsim: A feature similarity index for image quality assessment. IEEE Transactions on Image Processing, 2011.": "Fsim: A feature similarity index for image quality assessment", "[28] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on Image Processing, 2012.": "No-reference image quality assessment in the spatial domain", "[51] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan, Deepti Ghadiyaram, and Alan Bovik. From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In Proc. of CVPR, 2020.": "From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality", "[2] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proc. of CVPR, 2018.": "The title in the form of {title} is: \n[2] The perception-distortion tradeoff", "[34] Michele A Saad, Alan C Bovik, and Christophe Charrier. Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE Transactions on Image Processing, 2012.": "Blind Image Quality Assessment: A Natural Scene Statistics Approach in the DCT Domain", "[3] Sebastian Bosse, Dominique Maniry, Klaus-Robert M\u00fcller, Thomas Wiegand, and Wojciech Samek. Deep neural networks for no-reference and full-reference image quality assessment. IEEE Transactions on Image Processing, 2017.": "Deep Neural Networks for Image Quality Assessment", "[36] Shuwei Shi, Qingyan Bai, Mingdeng Cao, Weihao Xia, Jiahao Wang, Yifan Chen, and Yujiu Yang. Region-adaptive deformable network for image quality assessment. In Proc. of CVPR, 2021.": "Region-adaptive deformable network for image quality assessment", "[9] S Alireza Golestaneh, Saba Dadsetan, and Kris M Kitani. No-reference image quality assessment via transformers, relative ranking, and self-consistency. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2022.": "No-reference image quality assessment via transformers, relative ranking, and self-consistency", "[29] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a \u201ccompletely blind\u201d image quality analyzer. IEEE Signal Processing Letters, 2012.": "Making a completely blind image quality analyzer", "[59] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou Wang. Blind image quality assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology, 2018.": "Blind Image Quality Assessment Using a Deep Bilinear Convolutional Neural Network", "[27] Anish Mittal, Anush K Moorthy, and Alan C Bovik. Blind/referenceless image spatial quality evaluator. In Proc. of ASILOMAR, 2011.": "Blind/Referenceless Image Spatial Quality Evaluator", "[58] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. of CVPR, 2018.": "The unreasonable effectiveness of deep features as a perceptual metric", "[12] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy S. Ren, Radu Timofte, et al. NTIRE 2022 challenge on perceptual image quality assessment. In Proc. of CVPR, 2022.": "NTIRE 2022 challenge on perceptual image quality assessment", "[52] Junyong You and Jari Korhonen. Transformer for image quality assessment. In Proc. of ICIP, 2021.": "Transformer for Image Quality Assessment"}, "source_title_to_arxiv_id": {"Learning a no-reference quality metric for single-image super-resolution": "1612.05890"}}