{"title": "Parameter-efficient Model Adaptation for Vision Transformers", "abstract": "In computer vision, it has achieved great transfer learning performance via\nadapting large-scale pretrained vision models (e.g., vision transformers) to\ndownstream tasks. Common approaches for model adaptation either update all\nmodel parameters or leverage linear probes. In this paper, we aim to study\nparameter-efficient model adaptation strategies for vision transformers on the\nimage classification task. We formulate efficient model adaptation as a\nsubspace training problem and perform a comprehensive benchmarking over\ndifferent efficient adaptation methods. We conduct an empirical study on each\nefficient model adaptation method focusing on its performance alongside\nparameter cost. Furthermore, we propose a parameter-efficient model adaptation\nframework, which first selects submodules by measuring local intrinsic\ndimensions and then projects them into subspace for further decomposition via a\nnovel Kronecker Adaptation (KAdaptation) method. We analyze and compare our\nmethod with a diverse set of baseline model adaptation methods (including\nstate-of-the-art methods for pretrained language models). Our method performs\nthe best in terms of the tradeoff between accuracy and parameter efficiency\nacross 20 image classification datasets under the few-shot setting and 7 image\nclassification datasets under the full-shot setting.", "authors": ["Xuehai He", "Chunyuan Li", "Pengchuan Zhang", "Jianwei Yang", "Xin Eric Wang"], "published_date": "2022_03_29", "pdf_url": "http://arxiv.org/pdf/2203.16329v2", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td># of parameters</td><td>Complexity</td></tr><tr><td>Adapter-tuning Houlsby et al. (2019)</td><td>4Lkd</td><td>\\mathcal{O}\\left(kd\\right)</td></tr><tr><td>LoRA Hu et al. (2021)</td><td>2Lrd_{model}</td><td>\\mathcal{O}\\left(rd_{model}\\right)</td></tr><tr><td>Compacter Mahabadi et al. (2021)</td><td>4L(\\frac{k}{n}+\\frac{d}{n})+n^{3}</td><td>\\mathcal{O}\\left(\\frac{k+d}{n}\\right)</td></tr><tr><td>Ours</td><td>2L(\\frac{d_{model}}{n}+\\frac{r}{n})+n^{3}</td><td>\\mathcal{O}\\left(\\frac{r+d_{model}}{n}\\right)</td></tr></table>", "caption": "Table 1: Parameter count in Adapter-tuning, LoRA, Compacter, and Kronecker Adaptation. L is the number of layers in Transformer. k is the size of the input dimension to the Adapter layer. d is the bottleneck dimension in the Adapter layer. d_{model} is the Transformer hidden size. r denotes the rank in the low-rank decomposition step. n is the number of Kronecker products. n is a hyperparameter and usually very small, e.g., the number is 4 in our experiments.", "list_citation_info": ["Mahabadi et al. [2021] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. arXiv:2106.04647 [cs], June 2021.", "Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs], October 2021.", "Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], June 2019."]}, {"table": "<table><tr><td rowspan=\"2\">Method</td><td colspan=\"5\">CLIP</td><td colspan=\"5\">Supervised ViT</td></tr><tr><td>CIFAR-10</td><td>CIFAR-100</td><td>SUN-397</td><td>Average</td><td>#params</td><td>CIFAR-10</td><td>CIFAR-100</td><td>SUN-397</td><td>Average</td><td>#params</td></tr><tr><td colspan=\"11\">Commonly-used fine-tuning methods for vision models</td></tr><tr><td>Full-model Fine-tuning</td><td>97.7</td><td>85.4</td><td>73.8</td><td>85.6</td><td>151,364,010</td><td>99.0</td><td>92.4</td><td>75.0</td><td>88.8</td><td>86,697,617</td></tr><tr><td>Linear-probing</td><td>94.8</td><td>80.1</td><td>72.4</td><td>82.4</td><td>86,697</td><td>96.3</td><td>87.7</td><td>70.1</td><td>84.7</td><td>86,697</td></tr><tr><td colspan=\"11\">SOTA methods for NLP models</td></tr><tr><td>BitFit Zaken et al. (2021)</td><td>92.1</td><td>76.0</td><td>70.8</td><td>79.6</td><td>233,873</td><td>92.3</td><td>81.0</td><td>71.8</td><td>81.7</td><td>349,701</td></tr><tr><td>Adapter-tuning Houlsby et al. (2019)</td><td>94.7</td><td>81.4</td><td>77.1</td><td>84.4</td><td>190,545</td><td>98.4</td><td>90.6</td><td>74.2</td><td>87.7</td><td>1,813,929</td></tr><tr><td>AdapterDrop R\u00fcckl\u00e9 et al. (2021)</td><td>93.3</td><td>78.3</td><td>71.4</td><td>81.0</td><td>95,351</td><td>96.8</td><td>88.4</td><td>72.3</td><td>85.8</td><td>230,633</td></tr><tr><td>LoRA Hu et al. (2021)</td><td>95.1</td><td>78.1</td><td>80.8</td><td>84.7</td><td>185,001</td><td>98.7</td><td>90.6</td><td>73.6</td><td>87.6</td><td>277,417</td></tr><tr><td colspan=\"11\">Baseline methods developed in this work</td></tr><tr><td>Transformer-probing</td><td>95.6</td><td>80.1</td><td>74.3</td><td>83.3</td><td>3,236,521</td><td>96.5</td><td>86.9</td><td>76.7</td><td>86.7</td><td>3,236,521</td></tr><tr><td>LoRA-Fix</td><td>92.5</td><td>77.1</td><td>60.0</td><td>76.5</td><td>135,849</td><td>96.2</td><td>88.3</td><td>72.0</td><td>85.5</td><td>203,689</td></tr><tr><td>LayerNorm Tuning</td><td>82.5</td><td>76.6</td><td>66.7</td><td>75.2</td><td>89,769</td><td>92.2</td><td>71.7</td><td>72.0</td><td>78.6</td><td>131,497</td></tr><tr><td>Attention Tuning</td><td>96.8</td><td>81.8</td><td>73.1</td><td>83.9</td><td>41,042,601</td><td>93.9</td><td>85.7</td><td>73.8</td><td>84.4</td><td>284,783,77</td></tr><tr><td>LePE Tuning</td><td>95.1</td><td>78.9</td><td>68.0</td><td>80.7</td><td>148,137</td><td>93.7</td><td>90.8</td><td>73.2</td><td>85.8</td><td>207,801</td></tr><tr><td>RPB Tuning</td><td>94.7</td><td>77.1</td><td>68.4</td><td>80.1</td><td>102,921</td><td>96.7</td><td>87.0</td><td>72.4</td><td>85.3</td><td>201,704</td></tr><tr><td>Kronecker Adaptation (Ours)</td><td>95.9</td><td>84.8</td><td>74.0</td><td>84.9</td><td>107,727</td><td>97.9</td><td>91.2</td><td>75.1</td><td>88.1</td><td>144,396</td></tr></table>", "caption": "Table 2: Experimental result comparison on CIFAR-10 Krizhevsky and Hinton (2009), CIFAR-100 Krizhevsky and Hinton (2009), SUN-397 Xiao et al. (2016) datasets in terms of accuracy (%) and number of fine-tuning parameters (#params).Two Vision Transformers are evaluated, CLIP Radford et al. (2021) and Supervised ViT Dosovitskiy et al. (2020).We compare the proposed Kronecker Adaptation method with three baseline categories: (1) commonly-used fine-tuning methods for vision models (Full-model Fine-tuning and Linear-probing); (2) the re-implementation of SOTA methods for NLP models; (3) various baseline methods introduced in this work. Our method achieves the best tradeoff between accuracy and parameter efficiency: it obtains the best average accuracy among all efficient fine-tuning methods, which is also comparable to Full-model Fine-tuning, while updating only 0.07% of the model parameters in CLIP and 0.17% in Supervised ViT.", "list_citation_info": ["R\u00fcckl\u00e9 et al. [2021] Andreas R\u00fcckl\u00e9, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. AdapterDrop: On the Efficiency of Adapters in Transformers. arXiv:2010.11918 [cs], October 2021.", "Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.", "Xiao et al. [2016] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision, 119(1):3\u201322, 2016.", "Zaken et al. [2021] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. arXiv:2106.10199 [cs], June 2021.", "Krizhevsky and Hinton [2009] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009.", "Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs], October 2021.", "Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], June 2019.", "Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021."]}, {"table": "<table><tr><td>Method</td><td> Average Accuracy</td></tr><tr><td>Kronecker Adaptation</td><td>88.1</td></tr><tr><td>Kronecker Adaptation to MLP modules</td><td>86.6</td></tr><tr><td>Adapters on attention layer</td><td>54.1</td></tr><tr><td>Standard Adapter-tuning</td><td>87.7</td></tr></table>", "caption": "Table 3: Kronecker Adaptation and Adapter-tuning ablation experiments with Supervised ViT on CIFAR-10 Krizhevsky and Hinton (2009), CIFAR-100 Krizhevsky and Hinton (2009), and SUN-397 Xiao et al. (2016). We report the average accuracy (%) across the three datasets.", "list_citation_info": ["Krizhevsky and Hinton [2009] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009.", "Xiao et al. [2016] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision, 119(1):3\u201322, 2016."]}], "citation_info_to_title": {"Zaken et al. [2021] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. arXiv:2106.10199 [cs], June 2021.": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models", "Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. arXiv:1902.00751 [cs, stat], June 2019.": "Parameter-Efficient Transfer Learning for NLP", "Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.": "Learning transferable visual models from natural language supervision", "Xiao et al. [2016] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision, 119(1):3\u201322, 2016.": "Sun database: Exploring a large collection of scene categories", "Dosovitskiy et al. [2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.": "An image is worth 16x16 words: Transformers for image recognition at scale", "Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs], October 2021.": "LoRA: Low-Rank Adaptation of Large Language Models", "Mahabadi et al. [2021] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. arXiv:2106.04647 [cs], June 2021.": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers", "R\u00fcckl\u00e9 et al. [2021] Andreas R\u00fcckl\u00e9, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. AdapterDrop: On the Efficiency of Adapters in Transformers. arXiv:2010.11918 [cs], October 2021.": "AdapterDrop: On the Efficiency of Adapters in Transformers", "Krizhevsky and Hinton [2009] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report, 2009.": "Learning Multiple Layers of Features from Tiny Images"}, "source_title_to_arxiv_id": {"BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models": "2106.10199"}}