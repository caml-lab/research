{"title": "Panoramic Vision Transformer for Saliency Detection in 360\u00b0 Videos", "abstract": "360$^\\circ$ video saliency detection is one of the challenging benchmarks for\n360$^\\circ$ video understanding since non-negligible distortion and\ndiscontinuity occur in the projection of any format of 360$^\\circ$ videos, and\ncapture-worthy viewpoint in the omnidirectional sphere is ambiguous by nature.\nWe present a new framework named Panoramic Vision Transformer (PAVER). We\ndesign the encoder using Vision Transformer with deformable convolution, which\nenables us not only to plug pretrained models from normal videos into our\narchitecture without additional modules or finetuning but also to perform\ngeometric approximation only once, unlike previous deep CNN-based approaches.\nThanks to its powerful encoder, PAVER can learn the saliency from three simple\nrelative relations among local patch features, outperforming state-of-the-art\nmodels for the Wild360 benchmark by large margins without supervision or\nauxiliary information like class activation. We demonstrate the utility of our\nsaliency prediction model with the omnidirectional video quality assessment\ntask in VQA-ODV, where we consistently improve performance without any form of\nsupervision, including head movement.", "authors": ["Heeseung Yun", "Sehun Lee", "Gunhee Kim"], "published_date": "2022_09_19", "pdf_url": "http://arxiv.org/pdf/2209.08956v1", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"4\">PSNR</th><th colspan=\"4\">WS-PSNR [40]</th><th colspan=\"4\">S-PSNR [53]</th></tr><tr><th>Weight</th><th>PCC</th><th>SRCC</th><th>RMSE</th><th>MAE</th><th>PCC</th><th>SRCC</th><th>RMSE</th><th>MAE</th><th>PCC</th><th>SRCC</th><th>RMSE</th><th>MAE</th></tr></thead><tbody><tr><th>None</th><td>0.650</td><td>0.664</td><td>9.004</td><td>7.027</td><td>0.672</td><td>0.684</td><td>8.771</td><td>6.909</td><td>0.693</td><td>0.698</td><td>8.541</td><td>6.681</td></tr><tr><th>Random</th><td>0.650</td><td>0.663</td><td>9.004</td><td>7.027</td><td>0.672</td><td>0.684</td><td>8.771</td><td>6.909</td><td>0.693</td><td>0.698</td><td>8.540</td><td>6.680</td></tr><tr><th>Reverse</th><td>0.646</td><td>0.654</td><td>9.041</td><td>6.883</td><td>0.646</td><td>0.660</td><td>9.044</td><td>7.033</td><td>0.683</td><td>0.693</td><td>8.652</td><td>6.711</td></tr><tr><th>DINO [8]</th><td>0.657</td><td>0.677</td><td>8.934</td><td>7.105</td><td>0.674</td><td>0.690</td><td>8.747</td><td>7.033</td><td>0.699</td><td>0.710</td><td>8.468</td><td>6.665</td></tr><tr><th>PAVER</th><td>0.657</td><td>0.664</td><td>8.931</td><td>7.133</td><td>0.692</td><td>0.704</td><td>8.551</td><td>6.794</td><td>0.702</td><td>0.707</td><td>8.438</td><td>6.659</td></tr><tr><th>HM(Supervised)</th><td>0.733</td><td>0.726</td><td>8.054</td><td>6.479</td><td>0.731</td><td>0.722</td><td>8.086</td><td>6.565</td><td>0.736</td><td>0.741</td><td>8.022</td><td>6.305</td></tr></tbody></table>", "caption": "Table 3: Results of omnidirectional video quality assessment on VQA-ODV [30]. The better is higher PCC and SRCC or lower RMSE and MAE.", "list_citation_info": ["[53] Yu, M., Lakshman, H., Girod, B.: A framework to evaluate omnidirectional video coding schemes. In: ISMAR (2015)", "[30] Li, C., Xu, M., Du, X., Wang, Z.: Bridge the gap between VQA and human behavior on omnidirectional video: A large-scale dataset and a deep learning model. In: ACMMM (2018)", "[8] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging Properties in Self-Supervised Vision Transformers. In: ICCV (2021)", "[40] Sun, Y., Lu, A., Yu, L.: Weighted-to-spherically-uniform quality evaluation for omnidirectional video. SPL (2017)"]}], "citation_info_to_title": {"[8] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging Properties in Self-Supervised Vision Transformers. In: ICCV (2021)": "Emerging Properties in Self-Supervised Vision Transformers", "[30] Li, C., Xu, M., Du, X., Wang, Z.: Bridge the gap between VQA and human behavior on omnidirectional video: A large-scale dataset and a deep learning model. In: ACMMM (2018)": "Bridge the gap between VQA and human behavior on omnidirectional video: A large-scale dataset and a deep learning model", "[40] Sun, Y., Lu, A., Yu, L.: Weighted-to-spherically-uniform quality evaluation for omnidirectional video. SPL (2017)": "Weighted-to-spherically-uniform quality evaluation for omnidirectional video", "[53] Yu, M., Lakshman, H., Girod, B.: A framework to evaluate omnidirectional video coding schemes. In: ISMAR (2015)": "A framework to evaluate omnidirectional video coding schemes"}, "source_title_to_arxiv_id": {"Emerging Properties in Self-Supervised Vision Transformers": "2104.14294"}}