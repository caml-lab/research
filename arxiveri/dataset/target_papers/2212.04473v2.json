{"title": "Diffusion Guided Domain Adaptation of Image Generators", "abstract": "Can a text-to-image diffusion model be used as a training objective for\nadapting a GAN generator to another domain? In this paper, we show that the\nclassifier-free guidance can be leveraged as a critic and enable generators to\ndistill knowledge from large-scale text-to-image diffusion models. Generators\ncan be efficiently shifted into new domains indicated by text prompts without\naccess to groundtruth samples from target domains. We demonstrate the\neffectiveness and controllability of our method through extensive experiments.\nAlthough not trained to minimize CLIP loss, our model achieves equally high\nCLIP scores and significantly lower FID than prior work on short prompts, and\noutperforms the baseline qualitatively and quantitatively on long and\ncomplicated prompts. To our best knowledge, the proposed method is the first\nattempt at incorporating large-scale pre-trained diffusion models and\ndistillation sampling for text-driven image generator domain adaptation and\ngives a quality previously beyond possible. Moreover, we extend our work to\n3D-aware style-based generators and DreamBooth guidance.", "authors": ["Kunpeng Song", "Ligong Han", "Bingchen Liu", "Dimitris Metaxas", "Ahmed Elgammal"], "published_date": "2022_12_08", "pdf_url": "http://arxiv.org/pdf/2212.04473v2", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"2\">Cat</th><th colspan=\"2\">Dog</th></tr><tr><th></th><th>Ours</th><th>NADA</th><th>Ours</th><th>NADA</th></tr></thead><tbody><tr><th>Dog/Cat</th><td>150.76</td><td>206.93</td><td>124.72</td><td>139.35</td></tr><tr><th>Fox</th><td>51.51</td><td>90.40</td><td>61.10</td><td>129.58</td></tr><tr><th>Lion</th><td>30.34</td><td>153.82</td><td>52.52</td><td>173.81</td></tr><tr><th>Tiger</th><td>19.29</td><td>115.46</td><td>31.15</td><td>223.33</td></tr><tr><th>Wolf</th><td>45.33</td><td>139.66</td><td>71.29</td><td>160.00</td></tr></tbody></table>", "caption": "Table 1: FID scores of Cat/Dog-to-Animals. Ground-truth images are extracted from the AFHQ dataset [2]. Our models achieve significantly better FIDs than the baseline.", "list_citation_info": ["[2] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. CoRR, abs/1912.01865, 2019."]}], "citation_info_to_title": {"[2] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. CoRR, abs/1912.01865, 2019.": "StarGAN v2: Diverse Image Synthesis for Multiple Domains"}, "source_title_to_arxiv_id": {"StarGAN v2: Diverse Image Synthesis for Multiple Domains": "1912.01865"}}