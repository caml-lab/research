{"title": "Video Instance Segmentation via Multi-scale Spatio-temporal Split Attention Transformer", "abstract": "State-of-the-art transformer-based video instance segmentation (VIS)\napproaches typically utilize either single-scale spatio-temporal features or\nper-frame multi-scale features during the attention computations. We argue that\nsuch an attention computation ignores the multi-scale spatio-temporal feature\nrelationships that are crucial to tackle target appearance deformations in\nvideos. To address this issue, we propose a transformer-based VIS framework,\nnamed MS-STS VIS, that comprises a novel multi-scale spatio-temporal split\n(MS-STS) attention module in the encoder. The proposed MS-STS module\neffectively captures spatio-temporal feature relationships at multiple scales\nacross frames in a video. We further introduce an attention block in the\ndecoder to enhance the temporal consistency of the detected instances in\ndifferent frames of a video. Moreover, an auxiliary discriminator is introduced\nduring training to ensure better foreground-background separability within the\nmulti-scale spatio-temporal feature space. We conduct extensive experiments on\ntwo benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves\nstate-of-the-art performance on both benchmarks. When using the ResNet50\nbackbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best\nreported results in literature by 2.7 % and by 4.8 % at higher overlap\nthreshold of AP_75, while being comparable in model size and speed on\nYoutube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS\nachieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models\nare available at https://github.com/OmkarThawakar/MSSTS-VIS.", "authors": ["Omkar Thawakar", "Sanath Narayan", "Jiale Cao", "Hisham Cholakkal", "Rao Muhammad Anwer", "Muhammad Haris Khan", "Salman Khan", "Michael Felsberg", "Fahad Shahbaz Khan"], "published_date": "2022_03_24", "pdf_url": "http://arxiv.org/pdf/2203.13253v1", "list_table_and_caption": [{"table": "<p>MethodVenueBackboneTypeAPAP{}_{\\mathtt{50}}AP{}_{\\mathtt{75}}AR{}_{\\mathtt{1}}AR{}_{\\mathtt{10}}IoUTracker+ [26]ICCV 2019ResNet-50-23.639.225.526.230.9OSMN [27]CVPR 2018ResNet-50Two-Stage27.545.129.128.633.1DeepSORT [23]ICIP 2017ResNet-50Two-stage26.142.926.127.831.3FEELVOS [21]CVPR 2019ResNet-50Two-stage26.942.029.729.933.4SeqTracker [26]ICCV 2019ResNet-50-27.545.728.729.732.5MaskTrack R-CNN [26]ICCV 2019ResNet-50Two-stage30.351.132.631.035.5MaskProp [2]CVPR 2020ResNet-50-40.0-42.9--SipMask-VIS [3]ECCV 2020ResNet-50One-stage32.553.033.333.538.9SipMask-VIS [3]ECCV 2020ResNet-50One-stage33.754.135.835.440.1STEm-Seg [1]ECCV 2020ResNet-50-30.650.733.531.637.1Johnander et al.[10]GCPR 2021ResNet-50-35.3----CompFeat [5]AAAI 2021ResNet-50-35.356.038.633.140.3CrossVIS[28]ICCV 2021ResNet-50One-stage36.356.838.935.640.7PCAN [11]NeurIPS 2021ResNet-50One-stage36.154.939.436.341.6VisTR [22]CVPR 2021ResNet-50Transformer35.656.837.035.240.2SeqFormer [24]Arxiv 2021ResNet-50Transformer47.469.851.845.554.8MS-STS VIS (Ours)ResNet-50Transformer50.173.256.646.157.7MaskTrack R-CNN [26]ICCV 2019ResNet-101Two-stage31.953.732.332.537.7MaskProp [2]CVPR 2020ResNet-101-42.5-45.6--STEm-Seg [1]ECCV, 2020ResNet-101-34.655.837.934.441.6CrossVIS [28]ICCV 2021ResNet-101One-stage36.657.339.736.042.0PCAN [11]NeurIPS 2021ResNet-101One-stage37.657.241.337.243.9VisTR [22]CVPR 2021ResNet-101Transformer38.661.342.337.644.2SeqFormer [24]Arxiv 2021ResNet-101Transformer49.071.155.746.856.9MS-STS VIS (Ours)ResNet-101Transformer51.173.259.048.358.7SeqFormer [24]Arxiv 2021Swin-LTransformer59.382.166.651.764.4MS-STS VIS (Ours)Swin-LTransformer61.085.268.654.766.4</p>", "caption": "Table 1: State-of-the-art comparison on YouTube-VIS 2019 \\mathtt{val} set. Our MR-STS VIS consistently outperforms the state-of-the-art results reported in literature. When using the ResNet-50 backbone, MS-STS VIS achieves overall mask AP score of 50.1% with an absolute gain of 2.7% over the best existing SeqFormer, while being comparable in terms of model size and speed (SeqFormer: 11 FPS vs. Ours: 10 FPS). Similarly, when using the ResNet-101 backbone, our MS-STS VIS achieves overall mask AP of 51.1%. Further, MS-STS VIS achieves the best accuracy reported on this dataset with a mask AP of 61.0% and outperforms SeqFormer with an absolute gain of 1.7%, using the same Swin-L backbone.", "list_citation_info": ["[23] Wojke, N., Bewley, A., Paulus, D.: Simple online and realtime tracking with a deep association metric. In: ICIP (2017)", "[27] Yang, L., Wang, Y., Xiong, X., Yang, J., Katsaggelos, A.K.: Efficient video object segmentation via network modulation. In: CVPR (2018)", "[3] Cao, J., Anwer, R.M., Cholakkal, H., Khan, F.S., Pang, Y., Shao, L.: Sipmask: Spatial information preservation for fast image and video instance segmentation. In: ECCV (2020)", "[28] Yang, S., Fang, Y., Wang, X., Li, Y., Fang, C., Shan, Y., Feng, B., Liu, W.: Crossover learning for fast online video instance segmentation. In: ICCV (2021)", "[5] Fu, Y., Yang, L., Liu, D., Huang, T.S., Shi, H.: Compfeat: Comprehensive feature aggregation for video instance segmentation. AAAI (2021)", "[24] Wu, J., Jiang, Y., Zhang, W., Bai, X., Bai, S.: Seqformer: a frustratingly simple model for video instance segmentation. In: arXiv preprint arXiv:2112.08275 (2021)", "[26] Yang, L., Fan, Y., Xu, N.: Video instance segmentation. In: ICCV (2019)", "[22] Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end video instance segmentation with transformers. In: CVPR (2021)", "[11] Ke, L., Li, X., Danelljan, M., Tai, Y.W., Tang, C.K., Yu, F.: Prototypical cross-attention networks for multiple object tracking and segmentation. In: NeurIPS (2021)", "[21] Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.: Feelvos: Fast end-to-end embedding learning for video object segmentation. In: CVPR (2019)", "[1] Athar, A., Mahadevan, S., Osep, A., Leal-Taix\u00e9, L., Leibe, B.: Stem-seg: Spatio-temporal embeddings for instance segmentation in videos. In: ECCV (2020)", "[10] Johnander, J., Brissman, E., Danelljan, M., Felsberg, M.: Learning video instance segmentation with recurrent graph neural networks. In: GCPI (2021)", "[2] Bertasius, G., Torresani, L.: Classifying, segmenting, and tracking object instances in video with mask propagation. In: CVPR (2020)"]}, {"table": "<p>MethodAPAP{}_{\\mathtt{50}}AP{}_{\\mathtt{75}}AR{}_{\\mathtt{1}}AR{}_{\\mathtt{10}}MaskTrack R-CNN[25]28.648.929.626.533.8SipMask-VIS[3]31.752.534.030.837.8VisTR[22]31.851.734.529.736.9CrossVIS[28]34.254.437.930.438.2IFC[8]36.657.939.9--SeqFormer[24]40.562.443.736.148.1MS-STS VIS (Ours)42.263.746.541.751.1</p>", "caption": "Table 2: State-of-the-art comparison on YouTube-VIS 2021 \\mathtt{val} set. All results are reported using the same ResNet-50 backbone. Our MS-STS VIS achieves state-of-the-art results with an overall mask AP of 42.2% and an absolute gain of 2.8% over the best existing SeqFormer at a higher overlap threshold of AP{}_{\\mathtt{75}}. ", "list_citation_info": ["[3] Cao, J., Anwer, R.M., Cholakkal, H., Khan, F.S., Pang, Y., Shao, L.: Sipmask: Spatial information preservation for fast image and video instance segmentation. In: ECCV (2020)", "[25] Xu, N., Yang, L., Yang, J., Yue, D., Fan, Y., Liang, Y., Huang, T.S.: Youtube-vis dataset 2021 version. https://youtube-vos.org/dataset/vis (2021)", "[28] Yang, S., Fang, Y., Wang, X., Li, Y., Fang, C., Shan, Y., Feng, B., Liu, W.: Crossover learning for fast online video instance segmentation. In: ICCV (2021)", "[24] Wu, J., Jiang, Y., Zhang, W., Bai, X., Bai, S.: Seqformer: a frustratingly simple model for video instance segmentation. In: arXiv preprint arXiv:2112.08275 (2021)", "[8] Hwang, S., Heo, M., Oh, S.W., Kim, S.J.: Video instance segmentation using inter-frame communication transformers. In: NeurIPS (2021)", "[22] Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end video instance segmentation with transformers. In: CVPR (2021)"]}], "citation_info_to_title": {"[22] Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end video instance segmentation with transformers. In: CVPR (2021)": "End-to-end video instance segmentation with transformers", "[28] Yang, S., Fang, Y., Wang, X., Li, Y., Fang, C., Shan, Y., Feng, B., Liu, W.: Crossover learning for fast online video instance segmentation. In: ICCV (2021)": "Crossover learning for fast online video instance segmentation", "[3] Cao, J., Anwer, R.M., Cholakkal, H., Khan, F.S., Pang, Y., Shao, L.: Sipmask: Spatial information preservation for fast image and video instance segmentation. In: ECCV (2020)": "Sipmask: Spatial Information Preservation for Fast Image and Video Instance Segmentation", "[26] Yang, L., Fan, Y., Xu, N.: Video instance segmentation. In: ICCV (2019)": "Video Instance Segmentation", "[27] Yang, L., Wang, Y., Xiong, X., Yang, J., Katsaggelos, A.K.: Efficient video object segmentation via network modulation. In: CVPR (2018)": "Efficient video object segmentation via network modulation", "[24] Wu, J., Jiang, Y., Zhang, W., Bai, X., Bai, S.: Seqformer: a frustratingly simple model for video instance segmentation. In: arXiv preprint arXiv:2112.08275 (2021)": "Seqformer: A Frustratingly Simple Model for Video Instance Segmentation", "[8] Hwang, S., Heo, M., Oh, S.W., Kim, S.J.: Video instance segmentation using inter-frame communication transformers. In: NeurIPS (2021)": "Video Instance Segmentation Using Inter-Frame Communication Transformers", "[23] Wojke, N., Bewley, A., Paulus, D.: Simple online and realtime tracking with a deep association metric. In: ICIP (2017)": "Simple online and realtime tracking with a deep association metric", "[25] Xu, N., Yang, L., Yang, J., Yue, D., Fan, Y., Liang, Y., Huang, T.S.: Youtube-vis dataset 2021 version. https://youtube-vos.org/dataset/vis (2021)": "Youtube-vis dataset 2021 version", "[10] Johnander, J., Brissman, E., Danelljan, M., Felsberg, M.: Learning video instance segmentation with recurrent graph neural networks. In: GCPI (2021)": "Learning Video Instance Segmentation with Recurrent Graph Neural Networks", "[1] Athar, A., Mahadevan, S., Osep, A., Leal-Taix\u00e9, L., Leibe, B.: Stem-seg: Spatio-temporal embeddings for instance segmentation in videos. In: ECCV (2020)": "Stem-seg: Spatio-temporal embeddings for instance segmentation in videos", "[5] Fu, Y., Yang, L., Liu, D., Huang, T.S., Shi, H.: Compfeat: Comprehensive feature aggregation for video instance segmentation. AAAI (2021)": "Compfeat: Comprehensive feature aggregation for video instance segmentation", "[11] Ke, L., Li, X., Danelljan, M., Tai, Y.W., Tang, C.K., Yu, F.: Prototypical cross-attention networks for multiple object tracking and segmentation. In: NeurIPS (2021)": "Prototypical cross-attention networks for multiple object tracking and segmentation", "[21] Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.: Feelvos: Fast end-to-end embedding learning for video object segmentation. In: CVPR (2019)": "Feelvos: Fast End-to-End Embedding Learning for Video Object Segmentation", "[2] Bertasius, G., Torresani, L.: Classifying, segmenting, and tracking object instances in video with mask propagation. In: CVPR (2020)": "Classifying, segmenting, and tracking object instances in video with mask propagation"}, "source_title_to_arxiv_id": {"End-to-end video instance segmentation with transformers": "2011.14503"}}