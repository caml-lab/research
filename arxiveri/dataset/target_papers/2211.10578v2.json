{"title": "ABINet++: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Spotting", "abstract": "Scene text spotting is of great importance to the computer vision community\ndue to its wide variety of applications. Recent methods attempt to introduce\nlinguistic knowledge for challenging recognition rather than pure visual\nclassification. However, how to effectively model the linguistic rules in\nend-to-end deep networks remains a research challenge. In this paper, we argue\nthat the limited capacity of language models comes from 1) implicit language\nmodeling; 2) unidirectional feature representation; and 3) language model with\nnoise input. Correspondingly, we propose an autonomous, bidirectional and\niterative ABINet++ for scene text spotting. Firstly, the autonomous suggests\nenforcing explicitly language modeling by decoupling the recognizer into vision\nmodel and language model and blocking gradient flow between both models.\nSecondly, a novel bidirectional cloze network (BCN) as the language model is\nproposed based on bidirectional feature representation. Thirdly, we propose an\nexecution manner of iterative correction for the language model which can\neffectively alleviate the impact of noise input. Finally, to polish ABINet++ in\nlong text recognition, we propose to aggregate horizontal features by embedding\nTransformer units inside a U-Net, and design a position and content attention\nmodule which integrates character order and content to attend to character\nfeatures precisely. ABINet++ achieves state-of-the-art performance on both\nscene text recognition and scene text spotting benchmarks, which consistently\ndemonstrates the superiority of our method in various environments especially\non low-quality images. Besides, extensive experiments including in English and\nChinese also prove that, a text spotter that incorporates our language modeling\nmethod can significantly improve its performance both in accuracy and speed\ncompared with commonly used attention-based recognizers.", "authors": ["Shancheng Fang", "Zhendong Mao", "Hongtao Xie", "Yuxin Wang", "Chenggang Yan", "Yongdong Zhang"], "published_date": "2022_11_19", "pdf_url": "http://arxiv.org/pdf/2211.10578v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">PVM</td><td rowspan=\"2\">PLM{}_{in}</td><td rowspan=\"2\">PLM{}_{out}</td><td rowspan=\"2\">AGF</td><td>IC13</td><td>SVT</td><td>IIIT</td><td rowspan=\"2\">Avg</td></tr><tr><td>IC15</td><td>SVTP</td><td>CUTE</td></tr><tr><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td>96.7</td><td>93.4</td><td>95.7</td><td rowspan=\"2\">91.7</td></tr><tr><td>84.5</td><td>86.8</td><td>86.8</td></tr><tr><td rowspan=\"2\">\u2713</td><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td>97.0</td><td>93.0</td><td>96.3</td><td rowspan=\"2\">92.3</td></tr><tr><td>85.0</td><td>88.5</td><td>89.2</td></tr><tr><td rowspan=\"2\">-</td><td rowspan=\"2\">\u2713</td><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td>97.1</td><td>93.8</td><td>95.5</td><td rowspan=\"2\">91.6</td></tr><tr><td>83.6</td><td>88.1</td><td>86.8</td></tr><tr><td rowspan=\"2\">\u2713</td><td rowspan=\"2\">\u2713</td><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td>97.2</td><td>93.5</td><td>96.3</td><td rowspan=\"2\">92.3</td></tr><tr><td>84.9</td><td>89.0</td><td>88.5</td></tr><tr><td rowspan=\"2\">\u2713</td><td rowspan=\"2\">-</td><td rowspan=\"2\">\u2713</td><td rowspan=\"2\">-</td><td>97.0</td><td>93.7</td><td>96.5</td><td rowspan=\"2\">92.5</td></tr><tr><td>85.3</td><td>88.5</td><td>89.6</td></tr><tr><td rowspan=\"2\">\u2713</td><td rowspan=\"2\">-</td><td rowspan=\"2\">-</td><td rowspan=\"2\">\u2713</td><td>96.7</td><td>92.6</td><td>95.7</td><td rowspan=\"2\">91.4</td></tr><tr><td>83.3</td><td>86.5</td><td>88.5</td></tr></tbody></table>", "caption": "TABLE III: Ablation study of the autonomous strategy. \u201cPVM\u201d is pre-training the vision model on MJ and ST. \u201cPLM{}_{in}\u201d is pre-training the language model using the text on MJ and ST. \u201cPLM{}_{out}\u201d is pre-training the language model on WikiText-103 [80]. \u201cAGF\u201d means allowing gradient flow between the vision model and the language model.", "list_citation_info": ["[80] S. Merity, C. Xiong, J. Bradbury, and R. Socher, \u201cPointer sentinel mixture models,\u201d arXiv preprint arXiv:1609.07843, 2016."]}, {"table": "<table><tbody><tr><th rowspan=\"3\">Methods</th><th rowspan=\"3\">Year</th><td rowspan=\"3\">LabeledDatasets</td><td rowspan=\"3\">UnlabeledDatasets</td><td colspan=\"4\">Regular Text</td><td colspan=\"4\">Irregular Text</td><td rowspan=\"3\">Time(ms)</td></tr><tr><td>IC13</td><td>IC13</td><td>SVT</td><td>IIIT</td><td>IC15</td><td>IC15</td><td>SVTP</td><td>CUTE</td></tr><tr><td>857</td><td>1015</td><td>647</td><td>3000</td><td>1811</td><td>2077</td><td>645</td><td>288</td></tr><tr><th>SEED Qiao et al. [11]</th><th>2020</th><td>MJ+ST</td><td>-</td><td>-</td><td>92.8</td><td>89.6</td><td>93.8</td><td>80.0</td><td>-</td><td>81.4</td><td>83.6</td><td>53.3</td></tr><tr><th>Textscanner Wan et al. [35]</th><th>2020</th><td>MJ+ST</td><td>-</td><td>-</td><td>92.9</td><td>90.1</td><td>93.9</td><td>79.4</td><td>-</td><td>84.3</td><td>83.3</td><td>-</td></tr><tr><th>DAN Wang et al. [20]</th><th>2020</th><td>MJ+ST</td><td>-</td><td>-</td><td>93.9</td><td>89.2</td><td>94.3</td><td>-</td><td>74.5</td><td>80.0</td><td>84.4</td><td>-</td></tr><tr><th>RobustScanner Yue et al. [46]</th><th>2020</th><td>MJ+ST</td><td>-</td><td>-</td><td>94.8</td><td>88.1</td><td>95.3</td><td>-</td><td>77.1</td><td>79.5</td><td>90.3</td><td>36.0</td></tr><tr><th>SRN Yu et al. [21]</th><th>2020</th><td>MJ+ST</td><td>-</td><td>95.5</td><td>-</td><td>91.5</td><td>94.8</td><td>82.7</td><td>-</td><td>85.1</td><td>87.8</td><td>46.2</td></tr><tr><th>PIMNet Qiao et al. [49]</th><th>2021</th><td>MJ+ST</td><td>-</td><td>95.2</td><td>93.4</td><td>91.2</td><td>95.2</td><td>83.5</td><td>81.0</td><td>84.3</td><td>84.4</td><td>19.1</td></tr><tr><th>Bhunia et al. [48]</th><th>2021</th><td>MJ+ST</td><td>-</td><td>-</td><td>95.5</td><td>92.2</td><td>95.2</td><td>-</td><td>84.0</td><td>85.7</td><td>89.7</td><td>-</td></tr><tr><th>VisionLan Wang et al. [81]</th><th>2021</th><td>MJ+ST</td><td>-</td><td>95.7</td><td>-</td><td>91.7</td><td>95.8</td><td>83.7</td><td>-</td><td>86.0</td><td>88.5</td><td>11.5</td></tr><tr><th>SRN{}^{*} (SV)</th><th></th><td>MJ+ST</td><td>-</td><td>96.3</td><td>-</td><td>90.9</td><td>95.0</td><td>82.6</td><td>-</td><td>86.4</td><td>87.5</td><td>24.2</td></tr><tr><th>ABINet++ (SV)</th><th></th><td>MJ+ST</td><td>-</td><td>96.8</td><td>-</td><td>93.2</td><td>95.4</td><td>84.0</td><td>-</td><td>87.0</td><td>88.9</td><td>31.6</td></tr><tr><th>SRN{}^{*} (LV)</th><th></th><td>MJ+ST</td><td>-</td><td>96.8</td><td>-</td><td>92.3</td><td>96.3</td><td>84.2</td><td>-</td><td>87.9</td><td>88.2</td><td>26.9</td></tr><tr><th>ABINet++ (LV)</th><th></th><td>MJ+ST</td><td>-</td><td>97.4</td><td>95.7</td><td>93.5</td><td>96.2</td><td>86.0</td><td>85.1</td><td>89.3</td><td>89.2</td><td>33.9</td></tr><tr><th>ABINet++{}^{\\dagger} (LV)</th><th></th><td>MJ+ST</td><td>Uber-Text</td><td>97.3</td><td>-</td><td>94.9</td><td>96.8</td><td>87.4</td><td>-</td><td>90.1</td><td>93.4</td><td>-</td></tr><tr><th>ABINet++{}^{\\ddagger} (LV)</th><th></th><td>MJ+ST</td><td>Uber-Text</td><td>97.7</td><td>-</td><td>95.5</td><td>97.2</td><td>86.9</td><td>-</td><td>89.9</td><td>94.1</td><td>-</td></tr><tr><th>GTC Hu et al. [33]</th><th>2020</th><td>MJ+ST+Real</td><td>-</td><td>-</td><td>94.4</td><td>92.9</td><td>95.8</td><td>-</td><td>79.5</td><td>85.7</td><td>92.2</td><td>-</td></tr><tr><th>Textscanner Wan et al. [35]</th><th>2020</th><td>MJ+ST+Real</td><td>-</td><td>-</td><td>94.9</td><td>92.7</td><td>95.7</td><td>83.5</td><td>-</td><td>84.8</td><td>91.6</td><td>-</td></tr><tr><th>PIMNet Qiao et al. [49]</th><th>2021</th><td>MJ+ST+Real</td><td>-</td><td>96.6</td><td>95.4</td><td>94.7</td><td>96.7</td><td>88.7</td><td>85.9</td><td>88.2</td><td>92.7</td><td>-</td></tr><tr><th>ABINet++ (LV)</th><th></th><td>MJ+ST+Real</td><td>-</td><td>98.1</td><td>97.1</td><td>96.1</td><td>97.1</td><td>89.2</td><td>86.0</td><td>92.2</td><td>94.4</td><td>-</td></tr></tbody></table>", "caption": "TABLE VII: Accuracy comparison with other methods for scene text recognition. \u201cSV\u201d and \u201cLV\u201d denote small and large vision models. {}^{*} indicates reproduced version. {}^{\\dagger} and {}^{\\ddagger} are trained with self-training and ensemble self-training, respectively.", "list_citation_info": ["[49] Z. Qiao, Y. Zhou, J. Wei, W. Wang, Y. Zhang, N. Jiang, H. Wang, and W. Wang, \u201cPIMNet: a parallel, iterative and mimicking network for scene text recognition,\u201d in Proc. the 29th ACM Int. Conf. on Multimedia, 2021, pp. 2046\u20132055.", "[81] Y. Wang, H. Xie, S. Fang, J. Wang, S. Zhu, and Y. Zhang, \u201cFrom two to one: A new scene text recognizer with visual language modeling network,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2021, pp. 14\u2009194\u201314\u2009203.", "[11] Z. Qiao, Y. Zhou, D. Yang, Y. Zhou, and W. Wang, \u201cSEED: Semantics enhanced encoder-decoder framework for scene text recognition,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020, pp. 13\u2009528\u201313\u2009537.", "[20] T. Wang, Y. Zhu, L. Jin, C. Luo, X. Chen, Y. Wu, Q. Wang, and M. Cai, \u201cDecoupled attention network for text recognition,\u201d in AAAI, 2020, pp. 12\u2009216\u201312\u2009224.", "[46] X. Yue, Z. Kuang, C. Lin, H. Sun, and W. Zhang, \u201cRobustScanner: Dynamically enhancing positional clues for robust text recognition,\u201d in Proc. of the Proc. Eur. Conf. Comp. Vis., 2020.", "[48] A. K. Bhunia, A. Sain, A. Kumar, S. Ghose, P. N. Chowdhury, and Y.-Z. Song, \u201cJoint visual semantic reasoning: Multi-stage decoder for text recognition,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2021, pp. 14\u2009940\u201314\u2009949.", "[33] W. Hu, X. Cai, J. Hou, S. Yi, and Z. Lin, \u201cGTC: Guided training of CTC towards efficient and accurate scene text recognition.\u201d in AAAI, 2020, pp. 11\u2009005\u201311\u2009012.", "[21] D. Yu, X. Li, C. Zhang, T. Liu, J. Han, J. Liu, and E. Ding, \u201cTowards accurate scene text recognition with semantic reasoning networks,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020, pp. 12\u2009113\u201312\u2009122.", "[35] Z. Wan, M. He, H. Chen, X. Bai, and C. Yao, \u201cTextScanner: Reading characters in order for robust scene text recognition,\u201d in AAAI, 2020."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Year</td><td colspan=\"2\">Total-Text</td><td colspan=\"2\">SCUT-CTW1500</td><td colspan=\"4\">ICDAR 2015 End-to-End</td><td>ReCTS</td><td rowspan=\"2\">FPS</td></tr><tr><td>None</td><td>Full</td><td>None</td><td>Full</td><td>S</td><td>W</td><td>G</td><td>None</td><td>1-NED</td></tr><tr><td>FOTS Liu et al. [53]</td><td>2018</td><td>-</td><td>-</td><td>21.1</td><td>39.7</td><td>83.6</td><td>79.1</td><td>65.3</td><td>-</td><td>50.8</td><td>-</td></tr><tr><td>Qin et al. [55]</td><td>2019</td><td>67.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>4.8</td></tr><tr><td>CharNet Xing et al. {}^{*} [62]</td><td>2019</td><td>-</td><td>-</td><td>-</td><td>-</td><td>80.1</td><td>74.5</td><td>62.2</td><td>-</td><td>-</td><td>1.2</td></tr><tr><td>TextDragon Feng et al. [58]</td><td>2019</td><td>48.8</td><td>74.8</td><td>39.7</td><td>72.4</td><td>82.5</td><td>78.3</td><td>65.2</td><td>-</td><td>-</td><td>2.6</td></tr><tr><td>Mask TextSpotter\u201919 Liao et al. [34]</td><td>2019</td><td>65.3</td><td>77.4</td><td>-</td><td>-</td><td>83.0</td><td>77.7</td><td>73.5</td><td>-</td><td>67.8</td><td>2.0</td></tr><tr><td>ABCNet Liu et al. [26]</td><td>2020</td><td>64.2</td><td>75.7</td><td>45.2</td><td>74.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>17.9</td></tr><tr><td>Boundary Wang et al. [59]</td><td>2020</td><td>-</td><td>-</td><td>-</td><td>-</td><td>79.7</td><td>75.2</td><td>64.1</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Craft Baek et al. {}^{*} [61]</td><td>2020</td><td>78.7</td><td>-</td><td>-</td><td>-</td><td>83.1</td><td>82.1</td><td>74.9</td><td>-</td><td>-</td><td>5.4</td></tr><tr><td>Mask TextSpotter v3 Liao et al. [12]</td><td>2020</td><td>71.2</td><td>78.4</td><td>-</td><td>-</td><td>83.3</td><td>78.1</td><td>74.2</td><td>-</td><td>-</td><td>2.5</td></tr><tr><td>Feng et al. [91]</td><td>2021</td><td>55.8</td><td>79.2</td><td>42.2</td><td>74.9</td><td>87.3</td><td>83.1</td><td>69.5</td><td>-</td><td>-</td><td>7.2</td></tr><tr><td>PGNet Wang et al. [64]</td><td>2021</td><td>63.1</td><td>-</td><td>-</td><td>-</td><td>83.3</td><td>78.3</td><td>63.5</td><td>-</td><td>-</td><td>35.5</td></tr><tr><td>MANGO Qiao et al. [63]</td><td>2021</td><td>72.9</td><td>83.6</td><td>58.9</td><td>78.7</td><td>85.4</td><td>80.1</td><td>73.9</td><td>-</td><td>-</td><td>4.3</td></tr><tr><td>PAN++ Wang et al. [56]</td><td>2021</td><td>68.6</td><td>78.6</td><td>-</td><td>-</td><td>82.7</td><td>78.2</td><td>69.2</td><td>68.0</td><td>-</td><td>21.1</td></tr><tr><td>ABCNet v2 Liu et al. [6]</td><td>2021</td><td>70.4</td><td>78.1</td><td>57.5</td><td>77.2</td><td>82.7</td><td>78.5</td><td>73.0</td><td>-</td><td>62.7</td><td>10</td></tr><tr><td>ABINet++</td><td></td><td>77.6</td><td>84.5</td><td>60.2</td><td>80.3</td><td>84.1</td><td>80.4</td><td>75.4</td><td>73.3</td><td>76.5</td><td>10.6</td></tr><tr><td>ABINet++ MS</td><td></td><td>79.4</td><td>85.4</td><td>61.5</td><td>81.2</td><td>86.1</td><td>81.9</td><td>77.8</td><td>74.7</td><td>77.4</td><td>-</td></tr></tbody></table>", "caption": "TABLE XI: Performance comparison with other methods for end-to-end scene text spotting. \u201cNone\u201d indicates lexicon-free. \u201cFull\u201d represents the lexicon that includes all the words in the test set. \u201cS\u201d, \u201cW\u201d, and \u201cG\u201d mean recognition with \u201cStrong\u201d, \u201cWeak\u201d, and \u201cGeneric\u201d lexicon, respectively. {}^{*} denotes requiring character-level annotations. \u201cMS\u201d is multi-scale result.", "list_citation_info": ["[26] Y. Liu, H. Chen, C. Shen, T. He, L. Jin, and L. Wang, \u201cABCNet: Real-time scene text spotting with adaptive bezier-curve network,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020, pp. 9809\u20139818.", "[55] S. Qin, A. Bissacco, M. Raptis, Y. Fujii, and Y. Xiao, \u201cTowards unconstrained end-to-end text spotting,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2019, pp. 4704\u20134714.", "[91] W. Feng, F. Yin, X.-Y. Zhang, W. He, and C.-L. Liu, \u201cResidual dual scale scene text spotting by fusing bottom-up and top-down processing,\u201d Int. J. of Com. Vis., vol. 129, no. 3, pp. 619\u2013637, 2021.", "[61] Y. Baek, S. Shin, J. Baek, S. Park, J. Lee, D. Nam, and H. Lee, \u201cCharacter region attention for text spotting,\u201d in Proc. Eur. Conf. Comp. Vis. Springer, 2020, pp. 504\u2013521.", "[34] M. Liao, P. Lyu, M. He, C. Yao, W. Wu, and X. Bai, \u201cMask TextSpotter: An end-to-end trainable neural network for spotting text with arbitrary shapes,\u201d IEEE Trans. Pattern Anal. Mach. Intell., 2019.", "[59] H. Wang, P. Lu, H. Zhang, M. Yang, X. Bai, Y. Xu, M. He, Y. Wang, and W. Liu, \u201cAll you need is boundary: Toward arbitrary-shaped text spotting,\u201d in Proc. AAAI Conf. Artificial Intell., vol. 34, no. 07, 2020, pp. 12\u2009160\u201312\u2009167.", "[12] M. Liao, G. Pang, J. Huang, T. Hassner, and X. Bai, \u201cMask TextSpotter v3: Segmentation proposal network for robust scene text spotting,\u201d in Proc. of the Proc. Eur. Conf. Comp. Vis. Springer, 2020, pp. 706\u2013722.", "[62] L. Xing, Z. Tian, W. Huang, and M. R. Scott, \u201cConvolutional character networks,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2019, pp. 9126\u20139136.", "[64] P. Wang, C. Zhang, F. Qi, S. Liu, X. Zhang, P. Lyu, J. Han, J. Liu, E. Ding, and G. Shi, \u201cPGNet: Real-time arbitrarily-shaped text spotting with point gathering network,\u201d in Proc. AAAI Conf. Artificial Intell., vol. 35, no. 4, 2021, pp. 2782\u20132790.", "[56] W. Wang, E. Xie, X. Li, X. Liu, D. Liang, Y. Zhibo, T. Lu, and C. Shen, \u201cPAN++: Towards efficient and accurate end-to-end spotting of arbitrarily-shaped text,\u201d IEEE Trans. Pattern Anal. Mach. Intell., 2021.", "[6] Y. Liu, C. Shen, L. Jin, T. He, P. Chen, C. Liu, and H. Chen, \u201cABCNet v2: Adaptive bezier-curve network for real-time end-to-end text spotting,\u201d IEEE Trans. Pattern Anal. Mach. Intell., pp. 1\u20131, 2021.", "[63] L. Qiao, Y. Chen, Z. Cheng, Y. Xu, Y. Niu, S. Pu, and F. Wu, \u201cMANGO: A mask attention guided one-stage scene text spotter,\u201d in Proc. AAAI Conf. Artificial Intell., vol. 35, no. 3, 2021, pp. 2467\u20132476.", "[58] W. Feng, W. He, F. Yin, X.-Y. Zhang, and C.-L. Liu, \u201cTextDragon: An end-to-end framework for arbitrary shaped text spotting,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2019, pp. 9076\u20139085.", "[53] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao, and J. Yan, \u201cFOTS: Fast oriented text spotting with a unified network,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018, pp. 5676\u20135685."]}], "citation_info_to_title": {"[20] T. Wang, Y. Zhu, L. Jin, C. Luo, X. Chen, Y. Wu, Q. Wang, and M. Cai, \u201cDecoupled attention network for text recognition,\u201d in AAAI, 2020, pp. 12\u2009216\u201312\u2009224.": "Decoupled attention network for text recognition", "[81] Y. Wang, H. Xie, S. Fang, J. Wang, S. Zhu, and Y. Zhang, \u201cFrom two to one: A new scene text recognizer with visual language modeling network,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2021, pp. 14\u2009194\u201314\u2009203.": "From two to one: A new scene text recognizer with visual language modeling network", "[61] Y. Baek, S. Shin, J. Baek, S. Park, J. Lee, D. Nam, and H. Lee, \u201cCharacter region attention for text spotting,\u201d in Proc. Eur. Conf. Comp. Vis. Springer, 2020, pp. 504\u2013521.": "Character Region Attention for Text Spotting", "[11] Z. Qiao, Y. Zhou, D. Yang, Y. Zhou, and W. Wang, \u201cSEED: Semantics enhanced encoder-decoder framework for scene text recognition,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020, pp. 13\u2009528\u201313\u2009537.": "SEED: Semantics enhanced encoder-decoder framework for scene text recognition", "[6] Y. Liu, C. Shen, L. Jin, T. He, P. Chen, C. Liu, and H. Chen, \u201cABCNet v2: Adaptive bezier-curve network for real-time end-to-end text spotting,\u201d IEEE Trans. Pattern Anal. Mach. Intell., pp. 1\u20131, 2021.": "ABCNet v2: Adaptive bezier-curve network for real-time end-to-end text spotting", "[33] W. Hu, X. Cai, J. Hou, S. Yi, and Z. Lin, \u201cGTC: Guided training of CTC towards efficient and accurate scene text recognition.\u201d in AAAI, 2020, pp. 11\u2009005\u201311\u2009012.": "GTC: Guided training of CTC towards efficient and accurate scene text recognition", "[35] Z. Wan, M. He, H. Chen, X. Bai, and C. Yao, \u201cTextScanner: Reading characters in order for robust scene text recognition,\u201d in AAAI, 2020.": "TextScanner: Reading characters in order for robust scene text recognition", "[55] S. Qin, A. Bissacco, M. Raptis, Y. Fujii, and Y. Xiao, \u201cTowards unconstrained end-to-end text spotting,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2019, pp. 4704\u20134714.": "Towards unconstrained end-to-end text spotting", "[58] W. Feng, W. He, F. Yin, X.-Y. Zhang, and C.-L. Liu, \u201cTextDragon: An end-to-end framework for arbitrary shaped text spotting,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2019, pp. 9076\u20139085.": "TextDragon: An end-to-end framework for arbitrary shaped text spotting", "[12] M. Liao, G. Pang, J. Huang, T. Hassner, and X. Bai, \u201cMask TextSpotter v3: Segmentation proposal network for robust scene text spotting,\u201d in Proc. of the Proc. Eur. Conf. Comp. Vis. Springer, 2020, pp. 706\u2013722.": "Mask TextSpotter v3: Segmentation proposal network for robust scene text spotting", "[48] A. K. Bhunia, A. Sain, A. Kumar, S. Ghose, P. N. Chowdhury, and Y.-Z. Song, \u201cJoint visual semantic reasoning: Multi-stage decoder for text recognition,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2021, pp. 14\u2009940\u201314\u2009949.": "Joint visual semantic reasoning: Multi-stage decoder for text recognition", "[80] S. Merity, C. Xiong, J. Bradbury, and R. Socher, \u201cPointer sentinel mixture models,\u201d arXiv preprint arXiv:1609.07843, 2016.": "Pointer Sentinel Mixture Models", "[26] Y. Liu, H. Chen, C. Shen, T. He, L. Jin, and L. Wang, \u201cABCNet: Real-time scene text spotting with adaptive bezier-curve network,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020, pp. 9809\u20139818.": "ABCNet: Real-time scene text spotting with adaptive bezier-curve network", "[64] P. Wang, C. Zhang, F. Qi, S. Liu, X. Zhang, P. Lyu, J. Han, J. Liu, E. Ding, and G. Shi, \u201cPGNet: Real-time arbitrarily-shaped text spotting with point gathering network,\u201d in Proc. AAAI Conf. Artificial Intell., vol. 35, no. 4, 2021, pp. 2782\u20132790.": "PGNet: Real-time arbitrarily-shaped text spotting with point gathering network", "[21] D. Yu, X. Li, C. Zhang, T. Liu, J. Han, J. Liu, and E. Ding, \u201cTowards accurate scene text recognition with semantic reasoning networks,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020, pp. 12\u2009113\u201312\u2009122.": "Towards accurate scene text recognition with semantic reasoning networks", "[56] W. Wang, E. Xie, X. Li, X. Liu, D. Liang, Y. Zhibo, T. Lu, and C. Shen, \u201cPAN++: Towards efficient and accurate end-to-end spotting of arbitrarily-shaped text,\u201d IEEE Trans. Pattern Anal. Mach. Intell., 2021.": "PAN++: Towards efficient and accurate end-to-end spotting of arbitrarily-shaped text", "[91] W. Feng, F. Yin, X.-Y. Zhang, W. He, and C.-L. Liu, \u201cResidual dual scale scene text spotting by fusing bottom-up and top-down processing,\u201d Int. J. of Com. Vis., vol. 129, no. 3, pp. 619\u2013637, 2021.": "Residual dual scale scene text spotting by fusing bottom-up and top-down processing", "[63] L. Qiao, Y. Chen, Z. Cheng, Y. Xu, Y. Niu, S. Pu, and F. Wu, \u201cMANGO: A mask attention guided one-stage scene text spotter,\u201d in Proc. AAAI Conf. Artificial Intell., vol. 35, no. 3, 2021, pp. 2467\u20132476.": "MANGO: A mask attention guided one-stage scene text spotter", "[49] Z. Qiao, Y. Zhou, J. Wei, W. Wang, Y. Zhang, N. Jiang, H. Wang, and W. Wang, \u201cPIMNet: a parallel, iterative and mimicking network for scene text recognition,\u201d in Proc. the 29th ACM Int. Conf. on Multimedia, 2021, pp. 2046\u20132055.": "PIMNet: a parallel, iterative and mimicking network for scene text recognition", "[53] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao, and J. Yan, \u201cFOTS: Fast oriented text spotting with a unified network,\u201d in Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018, pp. 5676\u20135685.": "FOTS: Fast oriented text spotting with a unified network", "[62] L. Xing, Z. Tian, W. Huang, and M. R. Scott, \u201cConvolutional character networks,\u201d in Proc. IEEE Int. Conf. Comp. Vis., 2019, pp. 9126\u20139136.": "Convolutional character networks", "[34] M. Liao, P. Lyu, M. He, C. Yao, W. Wu, and X. Bai, \u201cMask TextSpotter: An end-to-end trainable neural network for spotting text with arbitrary shapes,\u201d IEEE Trans. Pattern Anal. Mach. Intell., 2019.": "Mask TextSpotter: An end-to-end trainable neural network for spotting text with arbitrary shapes", "[46] X. Yue, Z. Kuang, C. Lin, H. Sun, and W. Zhang, \u201cRobustScanner: Dynamically enhancing positional clues for robust text recognition,\u201d in Proc. of the Proc. Eur. Conf. Comp. Vis., 2020.": "RobustScanner: Dynamically enhancing positional clues for robust text recognition", "[59] H. Wang, P. Lu, H. Zhang, M. Yang, X. Bai, Y. Xu, M. He, Y. Wang, and W. Liu, \u201cAll you need is boundary: Toward arbitrary-shaped text spotting,\u201d in Proc. AAAI Conf. Artificial Intell., vol. 34, no. 07, 2020, pp. 12\u2009160\u201312\u2009167.": "All you need is boundary: Toward arbitrary-shaped text spotting"}, "source_title_to_arxiv_id": {"Joint visual semantic reasoning: Multi-stage decoder for text recognition": "2107.12090"}}