{"title": "Multi-scale Network with Attentional Multi-resolution Fusion for Point Cloud Semantic Segmentation", "abstract": "In this paper, we present a comprehensive point cloud semantic segmentation\nnetwork that aggregates both local and global multi-scale information. First,\nwe propose an Angle Correlation Point Convolution (ACPConv) module to\neffectively learn the local shapes of points. Second, based upon ACPConv, we\nintroduce a local multi-scale split (MSS) block that hierarchically connects\nfeatures within one single block and gradually enlarges the receptive field\nwhich is beneficial for exploiting the local context. Third, inspired by HRNet\nwhich has excellent performance on 2D image vision tasks, we build an HRNet\ncustomized for point cloud to learn global multi-scale context. Lastly, we\nintroduce a point-wise attention fusion approach that fuses multi-resolution\npredictions and further improves point cloud semantic segmentation performance.\nOur experimental results and ablations on several benchmark datasets show that\nour proposed method is effective and able to achieve state-of-the-art\nperformances compared to existing methods.", "authors": ["Yuyan Li", "Ye Duan"], "published_date": "2022_06_27", "pdf_url": "http://arxiv.org/pdf/2206.13628v1", "list_table_and_caption": [{"table": "<table><thead><tr><th><p>Method</p></th><th><p>mIoU</p></th><th><p>OA</p></th><th><p>ceil.</p></th><th><p>floor</p></th><th><p>wall</p></th><th><p>beam</p></th><th><p>col.</p></th><th><p>wind.</p></th><th><p>door</p></th><th><p>chair</p></th><th><p>table</p></th><th><p>book.</p></th><th><p>sofa</p></th><th><p>board</p></th><th><p>clut.</p></th></tr></thead><tbody><tr><th><p>PointNet [6]</p></th><th><p>41.1</p></th><th><p>49.0</p></th><td><p>88.8</p></td><td><p>97.3</p></td><td><p>69.8</p></td><td><p>0.1</p></td><td><p>3.9</p></td><td><p>46.3</p></td><td><p>10.8</p></td><td><p>52.6</p></td><td><p>58.9</p></td><td><p>40.3</p></td><td><p>5.9</p></td><td><p>26.4</p></td><td><p>33.2</p></td></tr><tr><th><p>KPConv (rigid) [10]</p></th><th><p>65.4</p></th><th><p>-</p></th><td><p>92.6</p></td><td><p>97.3</p></td><td><p>81.4</p></td><td><p>0.0</p></td><td><p>16.5</p></td><td><p>54.5</p></td><td><p>69.5</p></td><td><p>90.1</p></td><td><p>80.2</p></td><td><p>74.6</p></td><td><p>66.4</p></td><td><p>63.7</p></td><td><p>58.1</p></td></tr><tr><th><p>KPConv (deform) [10]</p></th><th><p>67.1</p></th><th><p>-</p></th><td><p>92.8</p></td><td><p>97.3</p></td><td><p>82.4</p></td><td><p>0.0</p></td><td><p>23.9</p></td><td><p>58.0</p></td><td><p>69.0</p></td><td><p>91.0</p></td><td><p>81.5</p></td><td><p>75.3</p></td><td><p>75.4</p></td><td><p>66.7</p></td><td><p>58.9</p></td></tr><tr><th><p>SPH-GCN [21]</p></th><th><p>59.5</p></th><th><p>87.7</p></th><td><p>93.3</p></td><td><p>97.1</p></td><td><p>81.1</p></td><td><p>0.0</p></td><td><p>33.2</p></td><td><p>45.8</p></td><td><p>43.8</p></td><td><p>79.7</p></td><td><p>86.9</p></td><td><p>33.2</p></td><td><p>71.5</p></td><td><p>54.1</p></td><td><p>53.7</p></td></tr><tr><th><p>SPNet [11]</p></th><th><p>69.9</p></th><th><p>90.3</p></th><td><p>94.5</p></td><td><p>98.3</p></td><td><p>84.0</p></td><td><p>0.0</p></td><td><p>24.0</p></td><td><p>59.7</p></td><td><p>79.8</p></td><td><p>89.6</p></td><td><p>81.0</p></td><td><p>75.2</p></td><td><p>82.4</p></td><td><p>80.4</p></td><td><p>60.4</p></td></tr><tr><th>Ours</th><th>70.7</th><th>91.0</th><td>94.7</td><td><p>98.2</p></td><td>86.2</td><td><p>0.0</p></td><td>45.8</td><td>61.4</td><td><p>71.1</p></td><td><p>82.5</p></td><td>90.3</td><td><p>73.4</p></td><td><p>76.1</p></td><td><p>77.8</p></td><td>61.2</td></tr></tbody></table>", "caption": "TABLE I: Semantic segmentation quantitative comparisons on S3DIS [33], tested on Area 5. We reported mean intersection over union (mIou) (%) and overall accuracy (%) scores as well as mIoU for individual classes.", "list_citation_info": ["[6] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652\u2013660.", "[11] Y. Li, C. Fan, X. Wang, and Y. Duan, \u201cSpnet: Multi-shell kernel convolution for point cloud semantic segmentation,\u201d in International Symposium on Visual Computing. Springer, 2021, pp. 366\u2013378.", "[33] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017.", "[21] H. Lei, N. Akhtar, and A. Mian, \u201cSpherical kernel for efficient graph convolution on 3d point clouds,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.", "[10] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas, \u201cKpconv: Flexible and deformable convolution for point clouds,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6411\u20136420."]}, {"table": "<table><thead><tr><th><p>Method</p></th><th><p>mIoU</p></th><th><p>floor</p></th><th><p>wall</p></th><th><p>chair</p></th><th><p>sofa</p></th><th><p>table</p></th><th><p>door</p></th><th><p>cab</p></th><th><p>bed</p></th><th><p>desk</p></th><th><p>toil</p></th><th><p>sink</p></th><th><p>wind</p></th><th><p>pic</p></th><th><p>bkshf</p></th><th><p>curt</p></th><th><p>show</p></th><th><p>cntr</p></th><th><p>fridg</p></th><th><p>bath</p></th><th><p>other</p></th></tr></thead><tbody><tr><th><p>PointNet++ [7]</p></th><th><p>33.9</p></th><td><p>67.7</p></td><td><p>52.3</p></td><td><p>36.0</p></td><td><p>34.6</p></td><td><p>23.2</p></td><td><p>26.1</p></td><td><p>25.6</p></td><td><p>47.8</p></td><td><p>27.8</p></td><td><p>54.8</p></td><td><p>36.4</p></td><td><p>25.2</p></td><td><p>11.7</p></td><td><p>45.8</p></td><td><p>24.7</p></td><td><p>14.5</p></td><td><p>25.0</p></td><td><p>21.2</p></td><td><p>58.4</p></td><td><p>18.3</p></td></tr><tr><th><p>RandLa-Net [8]</p></th><th><p>64.5</p></th><td><p>94.5</p></td><td><p>79.2</p></td><td><p>82.9</p></td><td><p>73.8</p></td><td><p>59.9</p></td><td><p>52.3</p></td><td><p>57.7</p></td><td><p>73.1</p></td><td><p>47.7</p></td><td><p>82.7</p></td><td><p>61.8</p></td><td><p>62.1</p></td><td><p>26.9</p></td><td><p>69.9</p></td><td><p>73.6</p></td><td><p>74.9</p></td><td><p>44.6</p></td><td><p>48.4</p></td><td><p>77.8</p></td><td><p>45.4</p></td></tr><tr><th><p>SPH-GCN [21]</p></th><th><p>61.0</p></th><td><p>93.5</p></td><td><p>77.3</p></td><td><p>79.2</p></td><td><p>70.5</p></td><td><p>54.9</p></td><td><p>50.7</p></td><td><p>53.2</p></td><td><p>77.2</p></td><td><p>57.0</p></td><td><p>85.9</p></td><td><p>60.2</p></td><td><p>53.4</p></td><td><p>4.6</p></td><td><p>48.9</p></td><td><p>64.3</p></td><td><p>70.2</p></td><td><p>40.4</p></td><td><p>51.0</p></td><td><p>85.8</p></td><td><p>41.4</p></td></tr><tr><th><p>KPConv (rigid) [10]</p></th><th><p>68.4</p></th><td><p>93.5</p></td><td><p>81.9</p></td><td><p>81.4</p></td><td><p>78.5</p></td><td><p>61.4</p></td><td><p>59.4</p></td><td><p>64.7</p></td><td><p>75.8</p></td><td><p>60.5</p></td><td><p>88.2</p></td><td><p>69.0</p></td><td><p>63.2</p></td><td><p>18.1</p></td><td><p>78.4</p></td><td><p>77.2</p></td><td><p>80.5</p></td><td><p>47.3</p></td><td><p>58.7</p></td><td><p>84.7</p></td><td><p>45.0</p></td></tr><tr><th><p>MinkowskiNet [18]</p></th><th><p>73.6</p></th><td><p>95.1</p></td><td><p>85.2</p></td><td><p>84.0</p></td><td><p>77.2</p></td><td><p>68.3</p></td><td><p>64.3</p></td><td><p>70.9</p></td><td><p>81.8</p></td><td><p>66.0</p></td><td><p>87.4</p></td><td><p>67.5</p></td><td><p>72.7</p></td><td><p>28.6</p></td><td><p>83.2</p></td><td><p>85.3</p></td><td><p>89.3</p></td><td><p>52.1</p></td><td><p>73.1</p></td><td><p>85.9</p></td><td><p>54.4</p></td></tr><tr><th><p>VMVF [17]</p></th><th><p>74.6</p></th><td><p>94.8</p></td><td><p>86.6</p></td><td><p>86.5</p></td><td><p>79.6</p></td><td><p>70.4</p></td><td><p>66.4</p></td><td><p>70.2</p></td><td><p>81.9</p></td><td><p>69.9</p></td><td><p>93.5</p></td><td><p>76.4</p></td><td><p>72.8</p></td><td><p>33.0</p></td><td><p>84.8</p></td><td><p>89.9</p></td><td><p>85.1</p></td><td><p>39.7</p></td><td><p>74.6</p></td><td><p>77.1</p></td><td><p>58.8</p></td></tr><tr><th>Ours</th><th>69.6</th><td><p>94.1</p></td><td><p>84.0</p></td><td><p>84.1</p></td><td>80.6</td><td><p>60.2</p></td><td><p>59.1</p></td><td><p>67.5</p></td><td><p>75.5</p></td><td><p>56.6</p></td><td><p>88.6</p></td><td><p>68.7</p></td><td><p>66.5</p></td><td><p>31.0</p></td><td><p>81.4</p></td><td><p>81.3</p></td><td><p>74.7</p></td><td><p>49.8</p></td><td><p>57.0</p></td><td>86.4</td><td><p>45.0</p></td></tr></tbody></table>", "caption": "TABLE II: Semantic segmentation quantitative comparisons on ScanNet online test set [34]. We reported mIoU (%) scores.", "list_citation_info": ["[17] A. Kundu, X. Yin, A. Fathi, D. Ross, B. Brewington, T. Funkhouser, and C. Pantofaru, \u201cVirtual multi-view fusion for 3d semantic segmentation,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 518\u2013535.", "[34] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5828\u20135839.", "[18] C. Choy, J. Gwak, and S. Savarese, \u201c4d spatio-temporal convnets: Minkowski convolutional neural networks,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3075\u20133084.", "[8] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham, \u201cRandla-net: Efficient semantic segmentation of large-scale point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009108\u201311\u2009117.", "[7] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d in Advances in neural information processing systems, 2017, pp. 5099\u20135108.", "[21] H. Lei, N. Akhtar, and A. Mian, \u201cSpherical kernel for efficient graph convolution on 3d point clouds,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.", "[10] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas, \u201cKpconv: Flexible and deformable convolution for point clouds,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6411\u20136420."]}, {"table": "<table><thead><tr><th><p>Method</p></th><th><p>S3DIS 6-fold (mIoU)</p></th><th><p>NPM3D (mIoU)</p></th></tr></thead><tbody><tr><td><p>KPConv [10]</p></td><td><p>69.6</p></td><td>82.0</td></tr><tr><td><p>ShellNet [13]</p></td><td><p>66.8</p></td><td><p>-</p></td></tr><tr><td><p>SPH-GCN [21]</p></td><td><p>68.9</p></td><td><p>-</p></td></tr><tr><td><p>RandLa-Net [8]</p></td><td><p>70.0</p></td><td><p>78.5</p></td></tr><tr><td>Ours</td><td>73.5</td><td><p>80.1</p></td></tr></tbody></table>", "caption": "TABLE III: Segmentation quantitative comparisons on S3DIS [33], 6-fold and NPM3D [35]. We reported mIou (%) Scores.", "list_citation_info": ["[13] Z. Zhang, B.-S. Hua, and S.-K. Yeung, \u201cShellnet: Efficient point cloud convolutional neural networks using concentric shells statistics,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 1607\u20131616.", "[8] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham, \u201cRandla-net: Efficient semantic segmentation of large-scale point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009108\u201311\u2009117.", "[35] X. Roynard, J.-E. Deschaud, and F. Goulette, \u201cParis-lille-3d: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification,\u201d The International Journal of Robotics Research, vol. 37, no. 6, pp. 545\u2013557, 2018.", "[33] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017.", "[21] H. Lei, N. Akhtar, and A. Mian, \u201cSpherical kernel for efficient graph convolution on 3d point clouds,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.", "[10] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas, \u201cKpconv: Flexible and deformable convolution for point clouds,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6411\u20136420."]}, {"table": "<table><thead><tr><th></th><th><p>Method</p></th><th><p>mIoU</p></th><th><p>Gain</p></th><th><p>#params</p></th></tr></thead><tbody><tr><td><p>(1)</p></td><td><p>Unet + ACPConv + ResNet block</p></td><td><p>68.2</p></td><td><p>-</p></td><td><p>14.1M</p></td></tr><tr><td><p>(2)</p></td><td><p>Unet + ACPConv + MSS block</p></td><td><p>69.1</p></td><td><p>+0.9</p></td><td><p>10.9M</p></td></tr><tr><td><p>(3)</p></td><td><p>HRNet + ACPConv + ResNet block</p></td><td><p>69.6</p></td><td><p>+1.3</p></td><td><p>19.5M</p></td></tr><tr><td><p>(4)</p></td><td><p>HRNet + ACPConv + MSS block</p></td><td><p>70.3</p></td><td><p>+2.1</p></td><td><p>16.6M</p></td></tr><tr><td><p>(5)</p></td><td><p>HRNet + ACPConv + MSS block + Attentional Fusion (full pipeline)</p></td><td>70.7</td><td><p>+2.5</p></td><td><p>17.0M</p></td></tr></tbody></table>", "caption": "TABLE IV: Abaltion study of individual components of our method, evaluated on S3DIS [33] Area 5. ", "list_citation_info": ["[33] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017."]}, {"table": "<table><thead><tr><th></th><th></th><th><p>mIoU</p></th><th><p>ceil.</p></th><th><p>floor</p></th><th><p>wall</p></th><th><p>beam</p></th><th><p>col.</p></th><th><p>wind.</p></th><th><p>door</p></th><th><p>chair</p></th><th><p>table</p></th><th><p>book.</p></th><th><p>sofa</p></th><th><p>board</p></th><th><p>clut.</p></th></tr></thead><tbody><tr><td><p>(1)</p></td><td><p>Scale 1, downsample rate = 0.04m</p></td><td><p>68.2</p></td><td><p>92.9</p></td><td><p>98.1</p></td><td><p>83.7</p></td><td><p>0.0</p></td><td><p>27.7</p></td><td><p>71.7</p></td><td><p>57.4</p></td><td><p>81.7</p></td><td><p>90.9</p></td><td><p>73.4</p></td><td>76.8</td><td><p>72.5</p></td><td><p>60.1</p></td></tr><tr><td><p>(2)</p></td><td><p>Scale 2, downsample rate = 0.08m</p></td><td><p>67.6</p></td><td><p>94.7</p></td><td><p>98.3</p></td><td>85.8</td><td><p>0.0</p></td><td><p>40.2</p></td><td>75.7</td><td>58.8</td><td><p>82.3</p></td><td><p>89.0</p></td><td><p>48.6</p></td><td><p>74.0</p></td><td><p>71.3</p></td><td><p>60.2</p></td></tr><tr><td><p>(3)</p></td><td><p>Fusion of scale 1 and scale 2, average</p></td><td><p>69.6</p></td><td>95.4</td><td><p>98.1</p></td><td><p>83.5</p></td><td><p>0.0</p></td><td><p>37.7</p></td><td><p>74.5</p></td><td><p>57.3</p></td><td><p>83.1</p></td><td><p>91.1</p></td><td><p>74.7</p></td><td><p>76.2</p></td><td><p>70.8</p></td><td>62.8</td></tr><tr><td><p>(4)</p></td><td><p>Fusion of scale 1 and scale 2, attention</p></td><td>70.0</td><td><p>94.9</p></td><td>98.3</td><td><p>83.8</p></td><td><p>0.0</p></td><td>41.4</td><td><p>75.0</p></td><td><p>54.6</p></td><td>83.4</td><td>91.6</td><td>74.8</td><td><p>76.4</p></td><td>75.0</td><td><p>61.3</p></td></tr></tbody></table>", "caption": "TABLE V: Ablation Study on multi-resolution fusion, evaluted on S3DIS [33] Area 5.", "list_citation_info": ["[33] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017."]}], "citation_info_to_title": {"[35] X. Roynard, J.-E. Deschaud, and F. Goulette, \u201cParis-lille-3d: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification,\u201d The International Journal of Robotics Research, vol. 37, no. 6, pp. 545\u2013557, 2018.": "Paris-lille-3d: A large and high-quality ground-truth urban point cloud dataset for automatic segmentation and classification", "[6] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, \u201cPointnet: Deep learning on point sets for 3d classification and segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652\u2013660.": "Pointnet: Deep learning on point sets for 3d classification and segmentation", "[11] Y. Li, C. Fan, X. Wang, and Y. Duan, \u201cSpnet: Multi-shell kernel convolution for point cloud semantic segmentation,\u201d in International Symposium on Visual Computing. Springer, 2021, pp. 366\u2013378.": "Spnet: Multi-shell kernel convolution for point cloud semantic segmentation", "[8] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, and A. Markham, \u201cRandla-net: Efficient semantic segmentation of large-scale point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 11\u2009108\u201311\u2009117.": "Randla-net: Efficient Semantic Segmentation of Large-Scale Point Clouds", "[10] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas, \u201cKpconv: Flexible and deformable convolution for point clouds,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 6411\u20136420.": "Kpconv: Flexible and deformable convolution for point clouds", "[18] C. Choy, J. Gwak, and S. Savarese, \u201c4d spatio-temporal convnets: Minkowski convolutional neural networks,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3075\u20133084.": "4d spatio-temporal convnets: Minkowski convolutional neural networks", "[17] A. Kundu, X. Yin, A. Fathi, D. Ross, B. Brewington, T. Funkhouser, and C. Pantofaru, \u201cVirtual multi-view fusion for 3d semantic segmentation,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 518\u2013535.": "Virtual multi-view fusion for 3D semantic segmentation", "[33] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese, \u201cJoint 2d-3d-semantic data for indoor scene understanding,\u201d arXiv preprint arXiv:1702.01105, 2017.": "Joint 2d-3d-semantic data for indoor scene understanding", "[7] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \u201cPointnet++: Deep hierarchical feature learning on point sets in a metric space,\u201d in Advances in neural information processing systems, 2017, pp. 5099\u20135108.": "Pointnet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space", "[13] Z. Zhang, B.-S. Hua, and S.-K. Yeung, \u201cShellnet: Efficient point cloud convolutional neural networks using concentric shells statistics,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 1607\u20131616.": "Shellnet: Efficient Point Cloud Convolutional Neural Networks Using Concentric Shells Statistics", "[34] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5828\u20135839.": "Scannet: Richly-annotated 3d reconstructions of indoor scenes", "[21] H. Lei, N. Akhtar, and A. Mian, \u201cSpherical kernel for efficient graph convolution on 3d point clouds,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.": "Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds"}, "source_title_to_arxiv_id": {"Spnet: Multi-shell kernel convolution for point cloud semantic segmentation": "2109.11610"}}