{"title": "VidConv: A modernized 2D ConvNet for Efficient Video Recognition", "abstract": "Since being introduced in 2020, Vision Transformers (ViT) has been steadily\nbreaking the record for many vision tasks and are often described as\n``all-you-need\" to replace ConvNet. Despite that, ViTs are generally\ncomputational, memory-consuming, and unfriendly for embedded devices. In\naddition, recent research shows that standard ConvNet if redesigned and trained\nappropriately can compete favorably with ViT in terms of accuracy and\nscalability. In this paper, we adopt the modernized structure of ConvNet to\ndesign a new backbone for action recognition. Particularly, our main target is\nto serve for industrial product deployment, such as FPGA boards in which only\nstandard operations are supported. Therefore, our network simply consists of 2D\nconvolutions, without using any 3D convolution, long-range attention plugin, or\nTransformer blocks. While being trained with much fewer epochs (5x-10x), our\nbackbone surpasses the methods using (2+1)D and 3D convolution, and achieve\ncomparable results with ViT on two benchmark datasets.", "authors": ["Chuong H. Nguyen", "Su Huynh", "Vinh Nguyen", "Ngoc Nguyen"], "published_date": "2022_07_08", "pdf_url": "http://arxiv.org/pdf/2207.03782v1", "list_table_and_caption": [{"table": "<p>MethodPretrainTrainingEpochsTop-1Top-5ViewsFrames/viewFLOPs/viewLatency/viewParamsTSN-R50 [38]IN-1K10070.689.21\\times102510331.024.3TANet-R50 [28]IN-1K10076.392.61\\times3843.113.825.6I3D-R50 [3]IN-1K10072.6890.7810\\times33243.525.728.0SlowFast-8x8 [14]-1967792.610\\times33265.838.134.5SlowFast-16x8[14]-25678.993.510\\times332213N/AN/AX3D-M [13]-3007692.310\\times3166.212.13.8X3D-XL [13]-30079.193.910\\times31648.4N/A11.0TSM-R50 [23]IN-1K10074.7-10\\times31665.922.924.3TimeSformer [1]IN-1K157893.71\\times38100.950.5121.4TimeSformer-HR [1]IN-1K1579.794.41\\times316807.2403.2121.4TimeSformer-L [1]IN-21K1580.794.71\\times3961210.8515.4121.4Swin-T [26]IN-1K3078.893.64\\times3328857.728.2Swin-S [26]IN-1K3080.694.54\\times33216693.549.8Swin-B [26]IN-1K3080.694.64\\times332282127.988.1MViT-B 16\\times4 [11]-20078.493.55\\times11670.528.736.6MViT-B 32\\times4 [11]-20080.294.45\\times13217086.636.6Uniformer-S [21]IN-1K11080.894.74\\times116167??21.4Uniformer-B [21]IN-1K11082.095.14\\times116389??49.4ViC-T [Ours]IN-1K2475.492.24\\times1940.911.444.7ViC-S [Ours]IN-21K2478.293.54\\times1979.022.466.4ViC-S [Ours]IN-21K2479.594.04\\times116140.478.9ViC-B [Ours]IN-21K2480.294.44\\times19139.233.8107.4ViC-B [Ours]IN-21K2480.594.54\\times39139.233.8107.4</p><p>MethodPretrainTrainingEpochsOfficialReportEvaluate onCVDF-K600ViewsFrames/viewFLOPs/viewParamsTop-1Top 5Top-1Top-5SlowFast 16\u00d78 +NL [14]-19681.895.1--10\\times31623459.9X3D-M [13]-25678.894.5--10\\times3166.23.8X3D-XL [13]-51281.995.5--10\\times3-48.411TimeSformer [1]IN-21K1579.194.484.6961\\times38196121.4TimeSformer-HR [1]IN-21K1581.895.88897.21\\times3161703121.4TimeSformer-L [1]IN-21K1582.295.687.9971\\times3962308121.4Swin-B [26]IN-21K308496.587.297.34\\times33228288.1MViT-B, 16x4 [11]-20082.195.7--5\\times11670.536.8MViT-B, 32x3 [11]-20083.496.3--5\\times13217036.8MViT-B-24, 32x3 [11]-20083.896.3--5\\times13223652.9UniFormer-BIN-1K2008496.483.393.44x11638949.4UniFormer-BIN-1K20083.896.784.193.64x132103649.4ViC-B [Ours]IN-21K24--86.196.24\\times19139.2107.4</p>", "caption": "Table 2: Comparison to state-of-the-art on Kinetics-400. ", "list_citation_info": ["[38] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, pages 20\u201336. Springer, 2016.", "[13] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.", "[14] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211, 2019.", "[28] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13708\u201313718, 2021.", "[3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.", "[26] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021.", "[23] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.", "[11] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6824\u20136835, 2021.", "[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In Proceedings of the International Conference on Machine Learning (ICML), July 2021.", "[21] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatiotemporal representation learning, 2022."]}, {"table": "<p>MethodPretrainTrainingEpochsTop-1Top 5ViewsFrames/viewFLOPs/viewParamsSlowFast R101 8\\times8 [14]K-4002263.187.61\\times36410653.3TSM-R50 [23]K-4005063.388.22\\times3166242.9STM-R50 [17]IN-1K5064.289.810\\times31666.524.0MSNet-R50 [19]IN-1K4064.789.41\\times1166724.6TANet-R50 [28]IN-1K5064.689.52\\times31666-TEA-R50 [22]IN-21K5065.189.910\\times31670-TimeSformer [1]IN-21K1559.5-1\\times38101121.4TimeSformer-HR [1]IN-21K1562.2-1\\times316807.2121.4TimeSformer-L [1]IN-21K1562.4-1\\times3641210.9121.4Swin-B [26]K-4006069.692.71\\times33228288.1MViT-B 16\\times4 [11]K-40010064.789.21\\times31670.536.6MViT-B 32\\times3 [11]K-40010067.190.81\\times33217036.6Uniformer-S [21]K-40020067.788.51\\times31612521.4Uniformer-B [21]K-40020070.492.81\\times31629049.4ViC-T [Ours]K-4002061.386.91\\times3940.944.2ViC-S [Ours]K-4002064.488.81\\times3979.065.9ViC-S [Ours]K-4002065.189.61\\times316140.478.4</p>", "caption": "Table 4: Comparison to state-of-the-art on Something-Something V2. ", "list_citation_info": ["[17] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2000\u20132009, 2019.", "[14] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211, 2019.", "[28] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13708\u201313718, 2021.", "[19] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion feature learning for video understanding. In European Conference on Computer Vision, pages 345\u2013362. Springer, 2020.", "[26] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021.", "[23] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.", "[22] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 909\u2013918, 2020.", "[11] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6824\u20136835, 2021.", "[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In Proceedings of the International Conference on Machine Learning (ICML), July 2021.", "[21] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatiotemporal representation learning, 2022."]}, {"table": "<p>MethodKinetics-400SS-v2Top-1Top-5Top-1Top-5StNet - R50 [16]69.85---StNet - R101 [16]71.4---SIFAR - SwinB - 7 [12]79.694.456.783.3SIFAR - SwinB - 12 [12]8094.560.186.8ViC-S - 3\\times378.293.564.488.8ViC-S - 4\\times479.594.065.189.6</p>", "caption": "Table 9: Compare to other close works on Kinetics-400 and Something-Something V2 dataset", "list_citation_info": ["[12] Quanfu Fan, Rameswar Panda, et al. An image classifier can suffice for video understanding. arXiv preprint arXiv:2106.14104, 2021.", "[16] Dongliang He, Zhichao Zhou, Chuang Gan, Fu Li, Xiao Liu, Yandong Li, Limin Wang, and Shilei Wen. Stnet: Local and global spatial-temporal modeling for action recognition. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 8401\u20138408, 2019."]}], "citation_info_to_title": {"[38] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In European conference on computer vision, pages 20\u201336. Springer, 2016.": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition", "[23] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7083\u20137093, 2019.": "Tsm: Temporal shift module for efficient video understanding", "[12] Quanfu Fan, Rameswar Panda, et al. An image classifier can suffice for video understanding. arXiv preprint arXiv:2106.14104, 2021.": "An image classifier can suffice for video understanding", "[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In Proceedings of the International Conference on Machine Learning (ICML), July 2021.": "Is space-time attention all you need for video understanding?", "[19] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion feature learning for video understanding. In European Conference on Computer Vision, pages 345\u2013362. Springer, 2020.": "Motionsqueeze: Neural motion feature learning for video understanding", "[13] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020.": "X3d: Expanding architectures for efficient video recognition", "[28] Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, and Tong Lu. Tam: Temporal adaptive module for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13708\u201313718, 2021.": "Tam: Temporal Adaptive Module for Video Recognition", "[21] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatiotemporal representation learning, 2022.": "Uniformer: Unified Transformer for Efficient Spatiotemporal Representation Learning", "[17] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion encoding for action recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2000\u20132009, 2019.": "STM: Spatiotemporal and Motion Encoding for Action Recognition", "[3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.": "Quo vadis, action recognition? a new model and the kinetics dataset", "[22] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and aggregation for action recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 909\u2013918, 2020.": "Tea: Temporal excitation and aggregation for action recognition", "[14] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211, 2019.": "Slowfast networks for video recognition", "[11] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6824\u20136835, 2021.": "Multiscale Vision Transformers", "[16] Dongliang He, Zhichao Zhou, Chuang Gan, Fu Li, Xiao Liu, Yandong Li, Limin Wang, and Shilei Wen. Stnet: Local and global spatial-temporal modeling for action recognition. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 8401\u20138408, 2019.": "STNet: Local and Global Spatial-Temporal Modeling for Action Recognition", "[26] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. arXiv preprint arXiv:2106.13230, 2021.": "Video Swin Transformer"}, "source_title_to_arxiv_id": {"Video Swin Transformer": "2106.13230"}}