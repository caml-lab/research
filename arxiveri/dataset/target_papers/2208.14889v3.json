{"title": "LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data", "abstract": "Existing techniques for image-to-image translation commonly have suffered\nfrom two critical problems: heavy reliance on per-sample domain annotation\nand/or inability of handling multiple attributes per image. Recent\ntruly-unsupervised methods adopt clustering approaches to easily provide\nper-sample one-hot domain labels. However, they cannot account for the\nreal-world setting: one sample may have multiple attributes. In addition, the\nsemantics of the clusters are not easily coupled to the human understanding. To\novercome these, we present a LANguage-driven Image-to-image Translation model,\ndubbed LANIT. We leverage easy-to-obtain candidate attributes given in texts\nfor a dataset: the similarity between images and attributes indicates\nper-sample domain labels. This formulation naturally enables multi-hot label so\nthat users can specify the target domain with a set of attributes in language.\nTo account for the case that the initial prompts are inaccurate, we also\npresent prompt learning. We further present domain regularization loss that\nenforces translated images be mapped to the corresponding domain. Experiments\non several standard benchmarks demonstrate that LANIT achieves comparable or\nsuperior performance to existing models.", "authors": ["Jihye Park", "Sunwoo Kim", "Soohyun Kim", "Jaejun Yoo", "Seokju Cho", "Youngjung Uh", "Seungryong Kim"], "published_date": "2022_08_31", "pdf_url": "http://arxiv.org/pdf/2208.14889v3", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Methods</td><td></td><td colspan=\"2\">CelebA-HQ</td></tr><tr><td></td><td>mFID</td><td>D&amp;C</td></tr><tr><td></td><td>StarGAN2 (Choi et al. 2020) (sup.)</td><td> 32.16</td><td>1.221 / 0.446</td></tr><tr><td></td><td>TUNIT (Baek et al. 2021) (unsup.)</td><td> 61.29</td><td>0.244 / 0.130</td></tr><tr><td></td><td>Kim et al. (Kim et al. 2022a) (unsup.)</td><td> 41.33</td><td>0.602 / 0.241</td></tr><tr><td>(I)</td><td>LANIT (Top-1)</td><td>49.65</td><td>0.561 / 0.320</td></tr><tr><td>(II)</td><td>LANIT (Top-3)</td><td>41.68</td><td>0.681 / 0.338</td></tr><tr><td>(III)</td><td>(II) + \\varnothing domain</td><td>33.39</td><td>0.963 / 0.366</td></tr><tr><td>(IV)</td><td>(III) + prompt learning</td><td>32.71</td><td>1.181 / 0.425</td></tr></table>", "caption": "Table 1: Quantitative comparison on CelebA-HQ.", "list_citation_info": ["Baek et al. (2021) Baek, K.; Choi, Y.; Uh, Y.; Yoo, J.; and Shim, H. 2021. Rethinking the truly unsupervised image-to-image translation. In ICCV, 14154\u201314163.", "Kim et al. (2022a) Kim, K.; Park, S.; Jeon, E.; Kim, T.; and Kim, D. 2022a. A Style-aware Discriminator for Controllable Image Translation. In CVPR.", "Choi et al. (2020) Choi, Y.; Uh, Y.; Yoo, J.; and Ha, J.-W. 2020. StarGAN v2: Diverse Image Synthesis for Multiple Domains. In CVPR."]}, {"table": "<table><tr><td>Datasets</td><td>Template</td><td>N</td><td>Domain Descriptions</td></tr><tr><td rowspan=\"3\"> CelebA-HQ (Liu et al. 2015)FFHQ (Karras, Laine, and Aila 2019) </td><td rowspan=\"3\">A face of</td><td>4</td><td>\u2018blond hair\u2019, \u2018black hair\u2019, \u2018smiling\u2019, \u2018eyeglasses\u2019</td></tr><tr><td>7</td><td> \u2018blond hair\u2019, \u2018wavy hair\u2019, \u2018black hair\u2019 , \u2018smiling\u2019, \u2018eyeglasses\u2019,\u2018goatee\u2019, \u2018bangs\u2019 </td></tr><tr><td>10</td><td> \u2018blond hair\u2019, \u2018bald\u2019, \u2018wavy hair\u2019,\u2018black hair\u2019 , \u2018smiling\u2019,\u2018straight hair\u2019, \u2018eyeglasses\u2019, \u2018goatee\u2019, \u2018bangs\u2019, \u2018arched eyebrows\u2019 </td></tr><tr><td rowspan=\"3\">Animal Faces-10 (Liu et al. 2019)</td><td rowspan=\"3\"> A photo ofanimal face with </td><td>4</td><td>\u2018beagle\u2019, \u2018golden retriever\u2019, \u2018tabby cat\u2019, \u2018tiger\u2019</td></tr><tr><td>7</td><td> \u2018beagle\u2019, \u2018dandie dinmont terrier\u2019, \u2018golden retriever\u2019, \u2018white fox\u2019,\u2018tabby cat\u2019, \u2018snow leopard\u2019, \u2018tiger\u2019 </td></tr><tr><td>10</td><td> \u2018appenzeller sennenhund\u2019, \u2018beagle\u2019, \u2018dandie dinmont terrier\u2019, \u2018golden retriever\u2019,\u2018malinois\u2019, \u2018white fox\u2019, \u2018tabby cat\u2019, \u2018snow leopard\u2019, \u2018lion\u2019, \u2018tiger\u2019 </td></tr><tr><td rowspan=\"3\">Food-10 (Bossard, Guillaumin, and Gool 2014)</td><td rowspan=\"3\">A photo of food with</td><td>4</td><td>\u2018baby back ribs\u2019, \u2018beignets\u2019, \u2018dumplings\u2019, \u2018edamame\u2019</td></tr><tr><td>7</td><td> \u2018baby back ribs\u2019, \u2018beef carpaccio\u2019, \u2018beignets\u2019, \u2018clam chowder\u2019,\u2018dumplings\u2019, \u2018edamame\u2019, \u2018strawberry shortcake\u2019 </td></tr><tr><td>10</td><td> \u2018baby back ribs\u2019, \u2018beef carpaccio\u2019, \u2018beignets\u2019, \u2018bibimbap\u2019, \u2018caesar salad\u2019,\u2018clam chowder\u2019, \u2018dumplings\u2019, \u2018edamame\u2019, \u2018spaghetti bolognese\u2019, \u2018strawberry shortcake\u2019 </td></tr><tr><td>LHQ (Skorokhodov, Sotnikov, and Elhoseiny 2021)</td><td>A photo of scene</td><td>10</td><td> \u2018with mountain\u2019, \u2018with field\u2019, \u2018with lake\u2019 ,\u2018with ocean\u2019, \u2018with waterfall\u2019, \u2018in summer\u2019,\u2018in winter\u2019, \u2018on sunny day\u2019, \u2018on cloudy day\u2019, \u2018at sunset\u2019 </td></tr><tr><td>MetFace (Karras et al. 2020)</td><td>A portrait with</td><td>10</td><td> \u2018oil painting\u2019, \u2018grayscale\u2019, \u2018black hair\u2019, \u2018wavy hair\u2019, \u2018male\u2019,\u2018mustache\u2019, \u2018smiling\u2019, \u2018gray hair\u2019, \u2018blonde hair\u2019, \u2018sculpture\u2019 </td></tr><tr><td>Anime (Chao 2019)</td><td> A photo ofanime with </td><td>10</td><td> \u2018brown hair\u2019, \u2018red hair\u2019, \u2018black hair\u2019, \u2018purple hair\u2019, \u2018blond hair\u2019,\u2018blue hair\u2019, \u2018pink hair\u2019, \u2018silver hair\u2019, \u2018green hair\u2019, \u2018white hair\u2019 </td></tr><tr><td>LSUN-Car (Yu et al. 2015)</td><td>A car painted with</td><td>10</td><td> \u2018red color\u2019, \u2018orange color\u2019, \u2018gray color\u2019, \u2018blue color\u2019, \u2018yellow color\u2019,\u2018white color\u2019, \u2018black color\u2019, \u2018silver color\u2019, \u2018green color\u2019, \u2018pink color\u2019 </td></tr><tr><td>LSUN-Church (Yu et al. 2015)</td><td>A church</td><td>7</td><td> \u2018at night\u2019, \u2018with sunset\u2019, \u2018in winter\u2019, \u2018on cloudy day\u2019,\u2018on sunny day\u2019, \u2018with trees\u2019, \u2018with a river\u2019 </td></tr></table>", "caption": "Table 5: Examples of Domain Descriptions for Each Dataset.", "list_citation_info": ["Karras et al. (2020) Karras, T.; Aittala, M.; Hellsten, J.; Laine, S.; Lehtinen, J.; and Aila, T. 2020. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33: 12104\u201312114.", "Skorokhodov, Sotnikov, and Elhoseiny (2021) Skorokhodov, I.; Sotnikov, G.; and Elhoseiny, M. 2021. Aligning latent and image spaces to connect the unconnectable. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 14144\u201314153.", "Karras, Laine, and Aila (2019) Karras, T.; Laine, S.; and Aila, T. 2019. A style-based generator architecture for generative adversarial networks. In CVPR, 4401\u20134410.", "Chao (2019) Chao, B. 2019. Anime Face Dataset: a collection of high-quality anime faces.", "Liu et al. (2019) Liu, M.-Y.; Huang, X.; Mallya, A.; Karras, T.; Aila, T.; Lehtinen, J.; and Kautz., J. 2019. Few-shot Unsueprvised Image-to-Image Translation. In arxiv.", "Bossard, Guillaumin, and Gool (2014) Bossard, L.; Guillaumin, M.; and Gool, L. V. 2014. Food-101\u2013mining discriminative components with random forests. In ECCV, 446\u2013461. Springer.", "Yu et al. (2015) Yu, F.; Seff, A.; Zhang, Y.; Song, S.; Funkhouser, T.; and Xiao, J. 2015. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365.", "Liu et al. (2015) Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep learning face attributes in the wild. In ICCV, 3730\u20133738."]}, {"table": "<table><tr><td rowspan=\"2\">Methods</td><td></td><td colspan=\"2\">CelebA-HQ</td></tr><tr><td></td><td>mFID</td><td>D&amp;C</td></tr><tr><td></td><td>Smoothing (Liu et al. 2021c)(supervised)</td><td> 35.93</td><td>1.253 / 0.431</td></tr><tr><td></td><td>StarGAN2 (Choi et al. 2020) (supervised)</td><td> 32.16</td><td>1.221 / 0.446</td></tr><tr><td></td><td>TUNIT (Baek et al. 2021) (unsupervised)</td><td> 61.29</td><td>0.244 / 0.13</td></tr><tr><td></td><td>Kim et al. (Kim et al. 2022a) (unsupervised)</td><td> 41.33</td><td>0.602 / 0.241</td></tr><tr><td>(I)</td><td>LANIT (Top-1)</td><td>49.65</td><td>0.561 / 0.320</td></tr><tr><td>(II)</td><td>LANIT (Top-3)</td><td>41.68</td><td>0.681 / 0.338</td></tr><tr><td>(III)</td><td>(II) + \\varnothing domain</td><td>33.39</td><td>0.963 / 0.366</td></tr><tr><td>(IV)</td><td>(III) + prompt learning</td><td>32.71</td><td>1.181 / 0.425</td></tr></table>", "caption": "Table 7: Quantitative comparison on CelebA-HQ (Liu et al. 2015).", "list_citation_info": ["Choi et al. (2020) Choi, Y.; Uh, Y.; Yoo, J.; and Ha, J.-W. 2020. StarGAN v2: Diverse Image Synthesis for Multiple Domains. In CVPR.", "Kim et al. (2022a) Kim, K.; Park, S.; Jeon, E.; Kim, T.; and Kim, D. 2022a. A Style-aware Discriminator for Controllable Image Translation. In CVPR.", "Baek et al. (2021) Baek, K.; Choi, Y.; Uh, Y.; Yoo, J.; and Shim, H. 2021. Rethinking the truly unsupervised image-to-image translation. In ICCV, 14154\u201314163.", "Liu et al. (2021c) Liu, Y.; Sangineto, E.; Chen, Y.; Bao, L.; Zhang, H.; Sebe, N.; Lepri, B.; Wang, W.; and De Nadai, M. 2021c. Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. In CVPR, 10785\u201310794.", "Liu et al. (2015) Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep learning face attributes in the wild. In ICCV, 3730\u20133738."]}], "citation_info_to_title": {"Chao (2019) Chao, B. 2019. Anime Face Dataset: a collection of high-quality anime faces.": "Anime Face Dataset: a collection of high-quality anime faces", "Karras et al. (2020) Karras, T.; Aittala, M.; Hellsten, J.; Laine, S.; Lehtinen, J.; and Aila, T. 2020. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33: 12104\u201312114.": "Training generative adversarial networks with limited data", "Karras, Laine, and Aila (2019) Karras, T.; Laine, S.; and Aila, T. 2019. A style-based generator architecture for generative adversarial networks. In CVPR, 4401\u20134410.": "A style-based generator architecture for generative adversarial networks", "Skorokhodov, Sotnikov, and Elhoseiny (2021) Skorokhodov, I.; Sotnikov, G.; and Elhoseiny, M. 2021. Aligning latent and image spaces to connect the unconnectable. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 14144\u201314153.": "Aligning latent and image spaces to connect the unconnectable", "Yu et al. (2015) Yu, F.; Seff, A.; Zhang, Y.; Song, S.; Funkhouser, T.; and Xiao, J. 2015. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365.": "LSUN: Construction of a Large-Scale Image Dataset Using Deep Learning with Humans in the Loop", "Liu et al. (2019) Liu, M.-Y.; Huang, X.; Mallya, A.; Karras, T.; Aila, T.; Lehtinen, J.; and Kautz., J. 2019. Few-shot Unsueprvised Image-to-Image Translation. In arxiv.": "Few-shot Unsupervised Image-to-Image Translation", "Bossard, Guillaumin, and Gool (2014) Bossard, L.; Guillaumin, M.; and Gool, L. V. 2014. Food-101\u2013mining discriminative components with random forests. In ECCV, 446\u2013461. Springer.": "Food-101\u2013mining discriminative components with random forests", "Choi et al. (2020) Choi, Y.; Uh, Y.; Yoo, J.; and Ha, J.-W. 2020. StarGAN v2: Diverse Image Synthesis for Multiple Domains. In CVPR.": "StarGAN v2: Diverse Image Synthesis for Multiple Domains", "Liu et al. (2021c) Liu, Y.; Sangineto, E.; Chen, Y.; Bao, L.; Zhang, H.; Sebe, N.; Lepri, B.; Wang, W.; and De Nadai, M. 2021c. Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation. In CVPR, 10785\u201310794.": "Smoothing the Disentangled Latent Style Space for Unsupervised Image-to-Image Translation", "Baek et al. (2021) Baek, K.; Choi, Y.; Uh, Y.; Yoo, J.; and Shim, H. 2021. Rethinking the truly unsupervised image-to-image translation. In ICCV, 14154\u201314163.": "Rethinking the truly unsupervised image-to-image translation", "Liu et al. (2015) Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep learning face attributes in the wild. In ICCV, 3730\u20133738.": "Deep Learning Face Attributes in the Wild", "Kim et al. (2022a) Kim, K.; Park, S.; Jeon, E.; Kim, T.; and Kim, D. 2022a. A Style-aware Discriminator for Controllable Image Translation. In CVPR.": "A Style-aware Discriminator for Controllable Image Translation"}, "source_title_to_arxiv_id": {"Aligning latent and image spaces to connect the unconnectable": "2104.06954", "StarGAN v2: Diverse Image Synthesis for Multiple Domains": "1912.01865"}}