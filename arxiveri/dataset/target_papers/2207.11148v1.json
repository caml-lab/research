{"title": "InfiniteNature-Zero: Learning Perpetual View Generation of Natural Scenes from Single Images", "abstract": "We present a method for learning to generate unbounded flythrough videos of\nnatural scenes starting from a single view, where this capability is learned\nfrom a collection of single photographs, without requiring camera poses or even\nmultiple views of each scene. To achieve this, we propose a novel\nself-supervised view generation training paradigm, where we sample and\nrendering virtual camera trajectories, including cyclic ones, allowing our\nmodel to learn stable view generation from a collection of single views. At\ntest time, despite never seeing a video during training, our approach can take\na single image and generate long camera trajectories comprised of hundreds of\nnew views with realistic and diverse content. We compare our approach with\nrecent state-of-the-art supervised view generation methods that require posed\nmulti-view videos and demonstrate superior performance and synthesis quality.", "authors": ["Zhengqi Li", "Qianqian Wang", "Noah Snavely", "Angjoo Kanazawa"], "published_date": "2022_07_22", "pdf_url": "http://arxiv.org/pdf/2207.11148v1", "list_table_and_caption": [{"table": "<table><tr><td></td><td></td><td colspan=\"3\">View Synthesis</td><td colspan=\"4\">View Generation</td></tr><tr><td>Method</td><td>MV?</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>LPIPS\\downarrow</td><td>FID\\downarrow</td><td>\\text{FID}_{\\text{sw}}\\downarrow</td><td>KID\\downarrow</td><td>Style\\downarrow</td></tr><tr><td>GFVS [62]</td><td>Yes</td><td>11.3/11.9</td><td>0.68/0.69</td><td>0.33/0.34</td><td>109</td><td>117</td><td>0.87</td><td>14.6</td></tr><tr><td>PixelSynth [61]</td><td>Yes</td><td>20.0/19.7</td><td>0.73/0.70</td><td>0.19/0.20</td><td>111</td><td>119</td><td>1.12</td><td>10.54</td></tr><tr><td>SLAMP [1]</td><td>Yes</td><td>-</td><td>-</td><td>-</td><td>114</td><td>138</td><td>1.91</td><td>15.2</td></tr><tr><td>DIGAN [92]</td><td>Yes</td><td>-</td><td>-</td><td>-</td><td>53.4</td><td>57.6</td><td>0.43</td><td>5.85</td></tr><tr><td>Liu et al. [43]</td><td>Yes</td><td>23.0/21.1</td><td>0.83/0.74</td><td>0.14/0.18</td><td>32.4</td><td>37.2</td><td>0.22</td><td>9.37</td></tr><tr><td>Ours</td><td>No</td><td>23.5/21.1</td><td>0.81/0.71</td><td>\\textbf{0.10}/\\textbf{0.15}</td><td>\\mathbf{19.3}</td><td>\\mathbf{25.1}</td><td>\\mathbf{0.11}</td><td>\\mathbf{5.63}</td></tr></table>", "caption": "Table 1: Quantitative comparisons on the ACID test set. \u201cMV?\u201d indicates whether a method requires (posed) multi-view data for training. We report view synthesis results with two different types of ground truth (shown as X/Y): sequences rendered with 3D Photos [71] (left), and real sequences (right). KID and Style are scaled by 10 and 10^{5} respectively.See Sec. 4.4 for descriptions of baselines. ", "list_citation_info": ["[71] Shih, M.L., Su, S.Y., Kopf, J., Huang, J.B.: 3D photography using context-aware layered depth inpainting. In: Proc. Computer Vision and Pattern Recognition (CVPR) (2020)", "[43] Liu, A., Tucker, R., Jampani, V., Makadia, A., Snavely, N., Kanazawa, A.: Infinite nature: Perpetual view generation of natural scenes from a single image. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14458\u201314467 (2021)", "[92] Yu, S., Tack, J., Mo, S., Kim, H., Kim, J., Ha, J.W., Shin, J.: Generating videos with dynamics-aware implicit generative adversarial networks. In: Proc. Int. Conf. on Learning Representations (ICLR) (2022)", "[1] Akan, A.K., Erdem, E., Erdem, A., Guney, F.: Slamp: Stochastic latent appearance and motion prediction. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14728\u201314737 (October 2021)", "[62] Rombach, R., Esser, P., Ommer, B.: Geometry-free view synthesis: Transformers and no 3d priors. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14356\u201314366 (2021)", "[61] Rockwell, C., Fouhey, D.F., Johnson, J.: Pixelsynth: Generating a 3d-consistent experience from a single image. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14104\u201314113 (2021)"]}], "citation_info_to_title": {"[43] Liu, A., Tucker, R., Jampani, V., Makadia, A., Snavely, N., Kanazawa, A.: Infinite nature: Perpetual view generation of natural scenes from a single image. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14458\u201314467 (2021)": "Infinite nature: Perpetual view generation of natural scenes from a single image", "[61] Rockwell, C., Fouhey, D.F., Johnson, J.: Pixelsynth: Generating a 3d-consistent experience from a single image. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14104\u201314113 (2021)": "Pixelsynth: Generating a 3d-consistent experience from a single image", "[92] Yu, S., Tack, J., Mo, S., Kim, H., Kim, J., Ha, J.W., Shin, J.: Generating videos with dynamics-aware implicit generative adversarial networks. In: Proc. Int. Conf. on Learning Representations (ICLR) (2022)": "Generating videos with dynamics-aware implicit generative adversarial networks", "[71] Shih, M.L., Su, S.Y., Kopf, J., Huang, J.B.: 3D photography using context-aware layered depth inpainting. In: Proc. Computer Vision and Pattern Recognition (CVPR) (2020)": "3D photography using context-aware layered depth inpainting", "[62] Rombach, R., Esser, P., Ommer, B.: Geometry-free view synthesis: Transformers and no 3d priors. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14356\u201314366 (2021)": "Geometry-free view synthesis: Transformers and no 3d priors", "[1] Akan, A.K., Erdem, E., Erdem, A., Guney, F.: Slamp: Stochastic latent appearance and motion prediction. In: Proc. Int. Conf. on Computer Vision (ICCV). pp. 14728\u201314737 (October 2021)": "Slamp: Stochastic Latent Appearance and Motion Prediction"}, "source_title_to_arxiv_id": {"Slamp: Stochastic Latent Appearance and Motion Prediction": "2108.02760"}}