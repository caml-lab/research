{"title": "Objects as Spatio-Temporal 2.5D points", "abstract": "Determining accurate bird's eye view (BEV) positions of objects and tracks in\na scene is vital for various perception tasks including object interactions\nmapping, scenario extraction etc., however, the level of supervision required\nto accomplish that is extremely challenging to procure. We propose a\nlight-weight, weakly supervised method to estimate 3D position of objects by\njointly learning to regress the 2D object detections and scene's depth\nprediction in a single feed-forward pass of a network. Our proposed method\nextends a center-point based single-shot object detector, and introduces a\nnovel object representation where each object is modeled as a BEV point\nspatio-temporally, without the need of any 3D or BEV annotations for training\nand LiDAR data at query time. The approach leverages readily available 2D\nobject supervision along with LiDAR point clouds (used only during training) to\njointly train a single network, that learns to predict 2D object detection\nalongside the whole scene's depth, to spatio-temporally model object tracks as\npoints in BEV. The proposed method is computationally over $\\sim$10x efficient\ncompared to recent SOTA approaches while achieving comparable accuracies on\nKITTI tracking benchmark.", "authors": ["Paridhi Singh", "Gaurav Singh", "Arun Kumar"], "published_date": "2022_12_06", "pdf_url": "http://arxiv.org/pdf/2212.02755v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><th>Time(ms)\u2193 / fps\u2191</th><th>MOTA \u2191</th><th>MOTP \u2191</th><th>MT \u2191</th><th>ML \u2193</th><th>IDSW \u2193</th><th>FRAG \u2193</th></tr><tr><td>Centertrack*</td><td>30.47/ 33</td><td>81.63</td><td>82.96</td><td>85.25</td><td>2.87</td><td>44</td><td>157</td></tr><tr><td>Ours (DLA34)*</td><td>31.70/ 31.5</td><td>83.54</td><td>84.67</td><td>79.86</td><td>3.59</td><td>27</td><td>138</td></tr><tr><td>Ours (MobileNetV2)*</td><td>18.20/ 55</td><td>84.02</td><td>84.09</td><td>76.61</td><td>2.51</td><td>99</td><td>208</td></tr><tr><td>Ours (ResNet18)*</td><td>28.84/ 34.6</td><td>84.55</td><td>83.32</td><td>76.04</td><td>4.6</td><td>75</td><td>189</td></tr></tbody></table>", "caption": "Table 1: KITTI test set evaluation results (comparison made on the class Car). * represents evaluation results of  [47] reproduced by us on KITTI validation split of [47].", "list_citation_info": ["[47] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In European Conference on Computer Vision, pages 474\u2013490. Springer, 2020."]}, {"table": "<table><tbody><tr><th>\\theta</th><td colspan=\"3\">Supervision</td><td colspan=\"4\">Error Metric</td><td colspan=\"3\">Accuracy Metric</td></tr><tr><th></th><td>Depth</td><td>Pose</td><td>Unsupervised</td><td>Abs Rel</td><td>Sq Rel</td><td>RMSE</td><td>RMSE log</td><td>\\delta&lt;1.25</td><td>\\delta&lt;1.25^{2}</td><td>\\delta&lt;1.25^{3}</td></tr><tr><th>DORN  [7] (50m cap)</th><td>\u2713</td><td></td><td></td><td>0.071</td><td>0.268</td><td>2.271</td><td>0.116</td><td>0.936</td><td>0.985</td><td>0.995</td></tr><tr><th>BTS  [19]</th><td>\u2713</td><td></td><td></td><td>0.056</td><td>0.169</td><td>1.925</td><td>0.087</td><td>0.964</td><td>0.994</td><td>0.999</td></tr><tr><th>DPT-Hybrid[27]</th><td>\u2713</td><td></td><td></td><td>0.062</td><td>-</td><td>2.573</td><td>0.092</td><td>0.959</td><td>0.995</td><td>0.999</td></tr><tr><th>Adabins  [1]</th><td>\u2713</td><td></td><td></td><td>0.058</td><td>0.190</td><td>2.360</td><td>0.088</td><td>0.964</td><td>0.995</td><td>0.999</td></tr><tr><th>AdaBins*  [1]</th><td>\u2713</td><td></td><td></td><td>0.066</td><td>0.203</td><td>2.31</td><td>0.126</td><td>0.946</td><td>0.978</td><td>0.99</td></tr><tr><th>Ours (DLA34){}^{*}</th><td>\u2713</td><td></td><td></td><td>0.102</td><td>0.750</td><td>4.137</td><td>0.169</td><td>0.898</td><td>0.967</td><td>0.986</td></tr><tr><th>Ours (MobileNetV2){}^{*}</th><td>\u2713</td><td></td><td></td><td>0.0835</td><td>0.4716</td><td>3.6386</td><td>0.1508</td><td>0.9220</td><td>0.9736</td><td>0.9895</td></tr><tr><th>Ours (ResNet18){}^{*}</th><td>\u2713</td><td></td><td></td><td>0.1128</td><td>0.7137</td><td>4.7172</td><td>0.2022</td><td>0.8532</td><td>0.9427</td><td>0.9807</td></tr><tr><th>PackNet-SfM  [11] (640 x 192 res.)</th><td></td><td>\u2713</td><td></td><td>0.078</td><td>0.420</td><td>3.485</td><td>0.121</td><td>0.931</td><td>0.986</td><td>0.996</td></tr><tr><th>Zhou et al. [46](w/o exp. mask)</th><td></td><td></td><td>\u2713</td><td>0.221</td><td>2.226</td><td>7.527</td><td>0.294</td><td>0.676</td><td>0.885</td><td>0.954</td></tr><tr><th>Zhou et al. [46]</th><td></td><td></td><td>\u2713</td><td>0.208</td><td>1.768</td><td>6.856</td><td>0.283</td><td>0.678</td><td>0.885</td><td>0.957</td></tr><tr><th>Kuznietsov et al. [16]</th><td>\u2713</td><td>\u2713(stereo)</td><td></td><td>0.113</td><td>0.741</td><td>4.621</td><td>0.189</td><td>0.875</td><td>0.964</td><td>0.988</td></tr></tbody></table>", "caption": "Table 2: Comparison of Monocular depth prediction results on KITTI dataset [8]. * Networks (with our implementation) trained on KITTI tracking dataset (as our method needs 2D object annotations), using only 4009 images (see Section 5 for explanation), as opposed to other entries in this table that are trained using 85K images of KITTI and evaluated on our KITTI tracking split as opposed to the Eigen split [5] used by other methods.", "list_citation_info": ["[16] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-supervised deep learning for monocular depth map prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6647\u20136655, 2017.", "[8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013.", "[19] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.", "[27] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12179\u201312188, 2021.", "[7] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.", "[46] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1851\u20131858, 2017.", "[11] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2485\u20132494, 2020.", "[1] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4009\u20134018, 2021.", "[5] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. arXiv preprint arXiv:1406.2283, 2014."]}], "citation_info_to_title": {"[5] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. arXiv preprint arXiv:1406.2283, 2014.": "Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network", "[8] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231\u20131237, 2013.": "Vision meets robotics: The kitti dataset", "[27] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12179\u201312188, 2021.": "Vision transformers for dense prediction", "[7] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.": "Deep ordinal regression network for monocular depth estimation", "[1] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4009\u20134018, 2021.": "Adabins: Depth estimation using adaptive bins", "[46] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1851\u20131858, 2017.": "Unsupervised learning of depth and ego-motion from video", "[19] Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale local planar guidance for monocular depth estimation. arXiv preprint arXiv:1907.10326, 2019.": "From big to small: Multi-scale local planar guidance for monocular depth estimation", "[11] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2485\u20132494, 2020.": "3D Packing for Self-Supervised Monocular Depth Estimation", "[16] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-supervised deep learning for monocular depth map prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6647\u20136655, 2017.": "Semi-supervised deep learning for monocular depth map prediction", "[47] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In European Conference on Computer Vision, pages 474\u2013490. Springer, 2020.": "Tracking Objects as Points"}, "source_title_to_arxiv_id": {"Vision transformers for dense prediction": "2102.12122", "Adabins: Depth estimation using adaptive bins": "2011.14141"}}