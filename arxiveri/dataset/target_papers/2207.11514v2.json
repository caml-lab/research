{"title": "Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models", "abstract": "We study open-world 3D scene understanding, a family of tasks that require\nagents to reason about their 3D environment with an open-set vocabulary and\nout-of-domain visual inputs - a critical skill for robots to operate in the\nunstructured 3D world. Towards this end, we propose Semantic Abstraction\n(SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D\nspatial capabilities, while maintaining their zero-shot robustness. We achieve\nthis abstraction using relevancy maps extracted from CLIP, and learn 3D spatial\nand geometric reasoning skills on top of those abstractions in a\nsemantic-agnostic manner. We demonstrate the usefulness of SemAbs on two\nopen-world 3D scene understanding tasks: 1) completing partially observed\nobjects and 2) localizing hidden objects from language descriptions.\nExperiments show that SemAbs can generalize to novel vocabulary,\nmaterials/lighting, classes, and domains (i.e., real-world scans) from training\non limited 3D synthetic data. Code and data is available at\nhttps://semantic-abstraction.cs.columbia.edu/", "authors": ["Huy Ha", "Shuran Song"], "published_date": "2022_07_23", "pdf_url": "http://arxiv.org/pdf/2207.11514v2", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Approach</th><th colspan=\"4\">Novel</th></tr><tr><th>Room</th><th>Visual</th><th>Vocab</th><th>Class</th></tr></thead><tbody><tr><th>SemAware</th><td>37.6</td><td>37.4</td><td>13.0</td><td>2.0</td></tr><tr><th>SemAbs+[18]</th><td>40.2</td><td>39.8</td><td>6.6</td><td>4.5</td></tr><tr><th>Ours</th><td>61.4</td><td>58.0</td><td>24.3</td><td>32.3</td></tr></tbody></table>", "caption": "Table 1:  Open Vocabulary Semantic Scene Completion", "list_citation_info": ["Chefer et al. [2021] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 397\u2013406, 2021."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Approach</th><th colspan=\"4\">Novel</th></tr><tr><th>Room</th><th>Visual</th><th>Vocab</th><th>Class</th></tr></thead><tbody><tr><th>SemAware</th><td>19.0</td><td>18.6</td><td>18.3</td><td>12.2</td></tr><tr><th>SemAbs+[18]</th><td>8.5</td><td>8.5</td><td>10.8</td><td>11.7</td></tr><tr><th>ClipSpatial</th><td>18.1</td><td>15.5</td><td>20.2</td><td>27.1</td></tr><tr><th>Ours</th><td>28.7</td><td>25.9</td><td>31.1</td><td>34.0</td></tr></tbody></table>", "caption": "Table 2: Visually Obscured Object Localization", "list_citation_info": ["Chefer et al. [2021] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 397\u2013406, 2021."]}], "citation_info_to_title": {"Chefer et al. [2021] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 397\u2013406, 2021.": "Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers"}, "source_title_to_arxiv_id": {"Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers": "2103.15679"}}