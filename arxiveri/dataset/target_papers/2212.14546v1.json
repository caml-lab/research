{"title": "HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training", "abstract": "Video-language pre-training has advanced the performance of various\ndownstream video-language tasks. However, most previous methods directly\ninherit or adapt typical image-language pre-training paradigms to\nvideo-language pre-training, thus not fully exploiting the unique\ncharacteristic of video, i.e., temporal. In this paper, we propose a\nHierarchical Temporal-Aware video-language pre-training framework, HiTeA, with\ntwo novel pre-training tasks for modeling cross-modal alignment between moments\nand texts as well as the temporal relations of video-text pairs. Specifically,\nwe propose a cross-modal moment exploration task to explore moments in videos,\nwhich results in detailed video moment representation. Besides, the inherent\ntemporal relations are captured by aligning video-text pairs as a whole in\ndifferent time resolutions with multi-modal temporal relation exploration task.\nFurthermore, we introduce the shuffling test to evaluate the temporal reliance\nof datasets and video-language pre-training models. We achieve state-of-the-art\nresults on 15 well-established video-language understanding and generation\ntasks, especially on temporal-oriented datasets (e.g., SSv2-Template and\nSSv2-Label) with 8.6% and 11.1% improvement respectively. HiTeA also\ndemonstrates strong generalization ability when directly transferred to\ndownstream tasks in a zero-shot manner. Models and demo will be available on\nModelScope.", "authors": ["Qinghao Ye", "Guohai Xu", "Ming Yan", "Haiyang Xu", "Qi Qian", "Ji Zhang", "Fei Huang"], "published_date": "2022_12_30", "pdf_url": "http://arxiv.org/pdf/2212.14546v1", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th></th><th colspan=\"3\">MSRVTT</th><th colspan=\"3\">DiDeMo</th><th colspan=\"3\">LSMDC</th><th colspan=\"3\">ActivityNet Caption</th></tr><tr><th>Method</th><th># PT Data</th><th>R@1</th><th>R@5</th><th>R@10</th><th>R@1</th><th>R@5</th><th>R@10</th><th>R@1</th><th>R@5</th><th>R@10</th><th>R@1</th><th>R@5</th><th>R@10</th></tr></thead><tbody><tr><th>ClipBERT [26]</th><th>0.2M</th><td>22.0</td><td>46.8</td><td>59.9</td><td>20.4</td><td>48.0</td><td>60.8</td><td>-</td><td>-</td><td>-</td><td>21.3</td><td>49.0</td><td>63.5</td></tr><tr><th>Frozen [2]</th><th>5M</th><td>31.0</td><td>59.5</td><td>70.5</td><td>31.0</td><td>59.8</td><td>72.4</td><td>15.0</td><td>30.8</td><td>39.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>ALPRO [28]</th><th>5M</th><td>33.9</td><td>60.7</td><td>73.2</td><td>35.9</td><td>67.5</td><td>78.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BridgeFormer [12]</th><th>5M</th><td>37.6</td><td>64.8</td><td>75.1</td><td>37.0</td><td>62.2</td><td>73.9</td><td>17.9</td><td>35.4</td><td>44.5</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Singularity [25]</th><th>5M</th><td>36.8</td><td>65.9</td><td>75.5</td><td>47.4</td><td>75.2</td><td>84.0</td><td>-</td><td>-</td><td>-</td><td>43.0</td><td>70.6</td><td>81.3</td></tr><tr><th>LAVENDER [32]</th><th>5M</th><td>37.8</td><td>63.8</td><td>75.0</td><td>47.4</td><td>74.7</td><td>82.4</td><td>22.2</td><td>43.8</td><td>53.5</td><td>-</td><td>-</td><td>-</td></tr><tr><th colspan=\"5\">Models pre-trained on more data</th><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>VIOLET [11]</th><th>183M</th><td>34.5</td><td>63.0</td><td>73.4</td><td>32.6</td><td>62.8</td><td>74.7</td><td>16.1</td><td>36.6</td><td>41.2</td><td>-</td><td>-</td><td>-</td></tr><tr><th>All-in-one [58]</th><th>138M</th><td>37.9</td><td>68.1</td><td>77.1</td><td>32.7</td><td>61.4</td><td>73.5</td><td>-</td><td>-</td><td>-</td><td>22.4</td><td>53.7</td><td>67.7</td></tr><tr><th>Clip4Clip [40]</th><th>400M</th><td>42.1</td><td>71.9</td><td>81.4</td><td>43.4</td><td>70.2</td><td>80.6</td><td>21.6</td><td>41.8</td><td>49.8</td><td>40.5</td><td>72.4</td><td>-</td></tr><tr><th>X-CLIP [41]</th><th>400M</th><td>46.1</td><td>73.0</td><td>83.1</td><td>45.2</td><td>74.0</td><td>-</td><td>23.3</td><td>43.0</td><td>-</td><td>44.3</td><td>74.1</td><td>-</td></tr><tr><th>HiTeA</th><th>5M</th><td>44.4</td><td>69.3</td><td>78.9</td><td>51.8</td><td>79.1</td><td>85.3</td><td>27.1</td><td>46.2</td><td>54.5</td><td>45.1</td><td>73.5</td><td>84.2</td></tr><tr><th>HiTeA</th><th>17M</th><td>46.8</td><td>71.2</td><td>81.9</td><td>56.5</td><td>81.7</td><td>89.7</td><td>28.7</td><td>50.3</td><td>59.0</td><td>49.7</td><td>77.1</td><td>86.7</td></tr></tbody></table>", "caption": "Table 1: Performance comparison on text-to-video retrieval. All results are reported on R@1/R@5/R@10. We gray out methods that use significantly more pre-training data for fair comparison. # PT Data is the number of video-text pairs for pre-training.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[32] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160, 2022.", "[28] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4953\u20134963, 2022.", "[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.", "[11] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.", "[58] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.", "[40] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293\u2013304, 2022.", "[41] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval. arXiv preprint arXiv:2207.07285, 2022.", "[12] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"3\">TGIF</th><th colspan=\"2\">MSRVTT</th><th colspan=\"2\">LSMDC</th><th>MSVD</th><th>ActivityNet</th></tr><tr><th>Method</th><th>#PT Data</th><th>Action</th><th>Transition</th><th>Frame</th><th>MC</th><th>QA</th><th>MC</th><th>FiB</th><th>QA</th><th>QA</th></tr></thead><tbody><tr><td>ClipBERT [26]</td><td>0.2M</td><td>82.8</td><td>87.8</td><td>60.3</td><td>88.2</td><td>37.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ALPRO [28]</td><td>5M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>42.1</td><td>-</td><td>-</td><td>46.3</td><td>-</td></tr><tr><td>Singularity [25]</td><td>5M</td><td>-</td><td>-</td><td>-</td><td>92.0</td><td>42.7</td><td>-</td><td>-</td><td>-</td><td>41.8</td></tr><tr><td>LAVENDER [32]</td><td>5M</td><td>96.6</td><td>99.1</td><td>72.2</td><td>96.6</td><td>44.2</td><td>86.0</td><td>56.9</td><td>55.4</td><td>-</td></tr><tr><td>Clover [15]</td><td>5M</td><td>94.9</td><td>98.0</td><td>71.4</td><td>95.0</td><td>43.9</td><td>83.2</td><td>54.1</td><td>51.9</td><td>-</td></tr><tr><th colspan=\"5\">Models pre-trained on more data</th><th></th><th></th><th></th><th></th><th></th><th></th></tr><tr><th>VIOLET [11]</th><th>183M</th><th>92.5</th><th>95.7</th><th>68.9</th><th>91.9</th><th>43.9</th><th>82.8</th><th>53.7</th><th>47.9</th><th>38.9</th></tr><tr><td>JustAsk [65]</td><td>69M</td><td>-</td><td>-</td><td>-</td><td>-</td><td>41.5</td><td>-</td><td>-</td><td>46.3</td><td>38.9</td></tr><tr><td>MERLOT [72]</td><td>180M</td><td>94.0</td><td>96.2</td><td>69.5</td><td>90.9</td><td>43.1</td><td>81.7</td><td>52.9</td><td>-</td><td>41.4</td></tr><tr><td>All-in-one [58]</td><td>283M</td><td>95.5</td><td>94.7</td><td>66.3</td><td>92.3</td><td>46.8</td><td>84.4</td><td>-</td><td>48.3</td><td>-</td></tr><tr><td>HiTeA</td><td>5M</td><td>96.8</td><td>98.8</td><td>72.5</td><td>97.2</td><td>45.4</td><td>85.8</td><td>54.6</td><td>55.6</td><td>45.1</td></tr><tr><td>HiTeA</td><td>17M</td><td>97.2</td><td>98.8</td><td>73.2</td><td>97.4</td><td>45.9</td><td>85.3</td><td>54.5</td><td>55.3</td><td>46.4</td></tr></tbody></table>", "caption": "Table 2: Performance comparison on video question answering. Accuracy is reported for evaluation. We gray out methods that use significantly more pre-training data for fair comparison.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[32] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160, 2022.", "[15] Jingjia Huang, Yinan Li, Jiashi Feng, Xiaoshuai Sun, and Rongrong Ji. Clover: Towards a unified video-language alignment and fusion model. arXiv preprint arXiv:2207.07885, 2022.", "[28] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4953\u20134963, 2022.", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.", "[11] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.", "[58] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.", "[72] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34:23634\u201323651, 2021.", "[65] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><th># PT Data</th><td>MSRVTT</td><td>MSVD</td></tr><tr><th>UniVL [39]</th><th>180M</th><td>49.9</td><td>-</td></tr><tr><th>SwinBERT [37]</th><th>-</th><td>53.8</td><td>120.6</td></tr><tr><th>MV-GPT [51]</th><th>53M</th><td>60.0</td><td>-</td></tr><tr><th>CLIP4Caption [55]</th><th>400M</th><td>57.7</td><td>-</td></tr><tr><th>LAVENDER [32]</th><th>5M</th><td>58.0</td><td>142.9</td></tr><tr><th>HiTeA</th><th>5M</th><td>62.5</td><td>145.1</td></tr><tr><th>HiTeA</th><th>17M</th><td>65.1</td><td>146.9</td></tr></tbody></table>", "caption": "Table 3: Performance comparison on video captioning. CIDEr [57] is reported for evaluation.", "list_citation_info": ["[51] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining for multimodal video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17959\u201317968, 2022.", "[55] Mingkang Tang, Zhanyu Wang, Zhenhua Liu, Fengyun Rao, Dian Li, and Xiu Li. Clip4caption: Clip for video caption. In Proceedings of the 29th ACM International Conference on Multimedia, pages 4858\u20134862, 2021.", "[32] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160, 2022.", "[37] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert: End-to-end transformers with sparse attention for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17949\u201317958, 2022.", "[39] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020.", "[57] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"3\">SSv2-Label</th><th colspan=\"3\">SSv2-Template</th></tr><tr><th>Method</th><th># PT Data</th><th>R@1</th><th>R@5</th><th>R@10</th><th>R@1</th><th>R@5</th><th>R@10</th></tr></thead><tbody><tr><th>Frozen [2]</th><th>5M</th><td>-</td><td>-</td><td>-</td><td>52.9</td><td>94.8</td><td>99.4</td></tr><tr><th>Clip4Clip [40]</th><th>400M</th><td>43.1</td><td>71.4</td><td>80.7</td><td>77.0</td><td>96.6</td><td>98.3</td></tr><tr><th>Singularity [25]</th><th>5M</th><td>44.1</td><td>73.5</td><td>82.2</td><td>77.0</td><td>98.9</td><td>99.4</td></tr><tr><th>HiTeA</th><th>5M</th><td>55.2</td><td>81.4</td><td>89.1</td><td>85.6</td><td>100.0</td><td>100.0</td></tr></tbody></table>", "caption": "Table 5: Comparison of existing methods on Something-to-Something (SSv2) text-to-video retrieval.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[40] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293\u2013304, 2022.", "[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><th># PT Data</th><td>Acc@C</td><td>Acc@T</td><td>Acc@D</td><td>Acc.</td></tr><tr><th>Full Set</th><th></th><td></td><td></td><td></td><td></td></tr><tr><th>Human</th><th>-</th><td>87.6</td><td>88.6</td><td>90.4</td><td>88.4</td></tr><tr><th>HCRN [23]</th><th>-</th><td>45.9</td><td>49.3</td><td>53.7</td><td>48.2</td></tr><tr><th>HGA [18]</th><th>-</th><td>46.3</td><td>50.7</td><td>59.3</td><td>49.7</td></tr><tr><th>VGT [61]</th><th>0.18M</th><td>53.4</td><td>56.4</td><td>69.5</td><td>56.9</td></tr><tr><th>HGA* [18]</th><th>400M</th><td>46.8</td><td>52.1</td><td>59.3</td><td>50.4</td></tr><tr><th>ATP [4]</th><th>400M</th><td>51.3</td><td>50.2</td><td>66.8</td><td>54.3</td></tr><tr><th>HiTeA</th><th>5M</th><td>62.4</td><td>58.3</td><td>75.6</td><td>63.1</td></tr><tr><th>Hard Split</th><th></th><td></td><td></td><td></td><td></td></tr><tr><th>ATP [4]</th><th>400M</th><td>38.4</td><td>36.5</td><td>/</td><td>/</td></tr><tr><th>HGA [18]</th><th>-</th><td>43.3</td><td>45.3</td><td>/</td><td>/</td></tr><tr><th>HiTeA</th><th>5M</th><td>47.8</td><td>48.6</td><td>/</td><td>/</td></tr></tbody></table>", "caption": "Table 6: Comparison of existing methods on NExT-QA [60]. We report accuracy on the Causal (C), Temporal (T), Descriptive (D) splits and overall accuracy on validation set. * stands for using CLIP as the initialization of visual encoder.", "list_citation_info": ["[4] Shyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the\" video\" in video-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2917\u20132927, 2022.", "[18] Pin Jiang and Yahong Han. Reasoning with heterogeneous graph alignment for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11109\u201311116, 2020.", "[23] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for multimodal video question answering. International Journal of Computer Vision, 129(11):3027\u20133050, 2021.", "[60] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9777\u20139786, 2021.", "[61] Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. Video graph transformer for video question answering. arXiv preprint arXiv:2207.05342, 2022."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Original \\uparrow</th><th>Shuffled \\downarrow</th><th>Gap \\uparrow</th></tr></thead><tbody><tr><th>MSRVTT [64]</th><td>64.2</td><td>63.3</td><td>0.9</td></tr><tr><th>DiDeMo [1]</th><td>72.1</td><td>70.2</td><td>1.9</td></tr><tr><th>LSMDC [49]</th><td>42.6</td><td>41.7</td><td>0.9</td></tr><tr><th>ActivityNet Caption [21]</th><td>67.6</td><td>66.8</td><td>0.8</td></tr><tr><th>SSv2 Template [25]</th><td>95.2</td><td>72.4</td><td>22.8</td></tr><tr><th>SSv2 Label [25]</th><td>76.7</td><td>73.5</td><td>3.2</td></tr></tbody></table>", "caption": "Table 7: Dependency on temporal information for text-to-video retrieval datasets with temporal shuffling test. The average recall of Recall@1, Recall@5, and Recall@10 are reported. We evaluate the performance drop when shuffling the input during inference. \u201cOriginal\u201d and \u201cShuffled\u201d denote the original and shuffled input videos, respectively, and \u201cGap\u201d is the difference between the Original and Shuffled metric. The larger \"Gap\" indicates the dataset relies on temporal information, and the model utilizes more temporal information to solve the task.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[49] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3202\u20133212, 2015.", "[21] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706\u2013715, 2017.", "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 5803\u20135812, 2017.", "[64] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Original \\uparrow</th><th>Shuffled \\downarrow</th><th>Gap \\uparrow</th></tr></thead><tbody><tr><th>MSRVTT-QA [62]</th><td>45.4</td><td>45.2</td><td>0.2</td></tr><tr><th>MSVD-QA [62]</th><td>55.6</td><td>55.5</td><td>0.1</td></tr><tr><th>TGIF-FrameQA [16]</th><td>72.5</td><td>72.1</td><td>0.4</td></tr><tr><th>ActivityNet-QA [70]</th><td>45.1</td><td>45.0</td><td>0.1</td></tr><tr><th>NExT-QA (Hard) [60]</th><td>47.1</td><td>45.6</td><td>0.5</td></tr></tbody></table>", "caption": "Table 8: Dependency on temporal information for video question answering datasets by temporal shuffling test. We report the accuarcy for each dataset. For NExT-QA dataset, we evaluate with the hard split of the validation set [4].", "list_citation_info": ["[4] Shyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the\" video\" in video-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2917\u20132927, 2022.", "[70] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9127\u20139134, 2019.", "[62] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.", "[16] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766, 2017.", "[60] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9777\u20139786, 2021."]}, {"table": "<table><thead><tr><th></th><th></th><th colspan=\"3\">MSRVTT</th><th colspan=\"3\">DiDeMo</th><th colspan=\"3\">LSMDC</th></tr><tr><th>Method</th><th># PT Data</th><th>R@1</th><th>R@5</th><th>R@10</th><th>R@1</th><th>R@5</th><th>R@10</th><th>R@1</th><th>R@5</th><th>R@10</th></tr></thead><tbody><tr><th>Frozen [2]</th><th>5M</th><td>18.7</td><td>39.5</td><td>51.6</td><td>21.1</td><td>46.0</td><td>56.2</td><td>9.3</td><td>22.0</td><td>30.1</td></tr><tr><th>ALPRO [28]</th><th>5M</th><td>24.1</td><td>44.7</td><td>55.4</td><td>23.8</td><td>47.3</td><td>57.9</td><td>-</td><td>-</td><td>-</td></tr><tr><th>BridgeFormer [12]</th><th>5M</th><td>26.0</td><td>46.4</td><td>56.4</td><td>25.6</td><td>50.6</td><td>61.1</td><td>12.2</td><td>25.9</td><td>32.2</td></tr><tr><th>Singularity [25]</th><th>5M</th><td>28.4</td><td>50.2</td><td>59.5</td><td>36.9</td><td>61.6</td><td>69.3</td><td>-</td><td>-</td><td>-</td></tr><tr><th colspan=\"5\">Models pre-trained on more data</th><th></th><th></th><th></th><th></th><th></th><th></th></tr><tr><th>VideoCLIP [63]</th><th>138M</th><th>10.4</th><th>22.2</th><th>30.0</th><th>16.6</th><th>46.9</th><th>-</th><th>-</th><th>-</th><th>-</th></tr><tr><th>VIOLET [11]</th><th>183M</th><td>25.9</td><td>49.5</td><td>59.7</td><td>23.5</td><td>49.8</td><td>59.8</td><td>-</td><td>-</td><td>-</td></tr><tr><th>Clip4Clip [40]</th><th>400M</th><td>31.2</td><td>53.7</td><td>64.2</td><td>-</td><td>-</td><td>-</td><td>11.3</td><td>22.7</td><td>29.2</td></tr><tr><th>HiTeA</th><th>5M</th><td>29.9</td><td>54.2</td><td>62.9</td><td>36.1</td><td>60.1</td><td>70.3</td><td>15.5</td><td>31.1</td><td>39.8</td></tr><tr><th>HiTeA</th><th>17M</th><td>34.4</td><td>60.0</td><td>69.9</td><td>43.2</td><td>69.3</td><td>79.0</td><td>18.3</td><td>36.7</td><td>44.2</td></tr></tbody></table>", "caption": "Table 9: Zero-shot evaluation on text-to-video retrieval. All results are reported on R@1/R@5/R@10. We gray out methods that use significantly more pre-training data for fair comparison.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[28] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4953\u20134963, 2022.", "[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.", "[11] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.", "[63] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021.", "[40] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293\u2013304, 2022.", "[12] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022."]}, {"table": "<table><tbody><tr><th>Method</th><th># PT Data</th><td>MSRVTT-QA</td><td>MSVD-QA</td></tr><tr><th>Just Ask [65]</th><th>69M</th><td>2.9</td><td>7.5</td></tr><tr><th>LAVENDER [32]</th><th>5M</th><td>4.5</td><td>11.6</td></tr><tr><th>MERLOT Reserve [71]</th><th>1B</th><td>5.8</td><td>-</td></tr><tr><th>FrozenBiLM [66]</th><th>10M</th><td>6.4</td><td>11.7</td></tr><tr><th>HiTeA</th><th>5M</th><td>8.6</td><td>18.2</td></tr><tr><th>BLIP [29]</th><th>129M</th><td>19.2</td><td>35.2</td></tr><tr><th>mPLUG [27]</th><th>400M</th><td>21.1</td><td>37.2</td></tr><tr><th>HiTeA</th><th>5M</th><td>21.7</td><td>37.4</td></tr></tbody></table>", "caption": "Table 10: Zero-shot evaluation on video question answering. Accuracy is reported. We gray out those methods additionally supervised pre-training on VQA v2 [13] dataset.", "list_citation_info": ["[32] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160, 2022.", "[66] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. arXiv preprint arXiv:2206.08155, 2022.", "[27] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. arXiv preprint arXiv:2205.12005, 2022.", "[65] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697, 2021.", "[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.", "[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022.", "[71] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16375\u201316387, 2022."]}, {"table": "<table><thead><tr><th rowspan=\"3\">Method</th><th rowspan=\"3\">#PT Data</th><th colspan=\"6\">COCO (5K test)</th></tr><tr><th colspan=\"3\">TR</th><th colspan=\"3\">IR</th></tr><tr><th>R1</th><th>R5</th><th>R10</th><th>R1</th><th>R5</th><th>R10</th></tr></thead><tbody><tr><th>ViLT [19]</th><th>4M</th><td>61.5</td><td>86.3</td><td>92.7</td><td>42.7</td><td>72.9</td><td>83.1</td></tr><tr><th>UNITER [8]</th><th>4M</th><td>65.7</td><td>88.6</td><td>93.8</td><td>52.9</td><td>79.9</td><td>88.0</td></tr><tr><th>OSCAR [35]</th><th>4M</th><td>70.0</td><td>91.1</td><td>95.5</td><td>54.0</td><td>80.8</td><td>88.5</td></tr><tr><th>ALBEF [30]</th><th>4M</th><td>73.1</td><td>91.4</td><td>96.0</td><td>56.8</td><td>81.5</td><td>89.2</td></tr><tr><th>BLIP [29]</th><th>14M</th><td>80.6</td><td>95.2</td><td>97.6</td><td>63.1</td><td>85.3</td><td>91.1</td></tr><tr><th>ALIGN [17]</th><th>1.2B</th><td>77.0</td><td>93.5</td><td>96.9</td><td>59.9</td><td>83.3</td><td>89.8</td></tr><tr><th>Singularity [25]</th><th>5M</th><td>71.9</td><td>90.8</td><td>95.4</td><td>54.6</td><td>80.0</td><td>87.8</td></tr><tr><th>HiTeA</th><th>5M</th><td>72.4</td><td>90.9</td><td>95.4</td><td>55.6</td><td>80.6</td><td>87.8</td></tr></tbody></table>", "caption": "Table 11: Comparison to existing methods on image-text retrieval on COCO dataset. We show results for both text retrieval (image-to-text retrieval, TR) and image retrieval (IR).", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[19] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.", "[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer, 2020.", "[35] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.", "[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.", "[30] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34:9694\u20139705, 2021.", "[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022."]}, {"table": "<table><thead><tr><th>Method</th><th>#PT Data</th><th>test-dev</th><th>test-std</th></tr></thead><tbody><tr><th>ClipBERT [26]</th><th>0.2M</th><td>69.08</td><td>69.43</td></tr><tr><th>ViLT [19]</th><th>4M</th><td>70.94</td><td>-</td></tr><tr><th>VL-BART [9]</th><th>0.2M</th><td>-</td><td>71.30</td></tr><tr><th>LXMERT [54]</th><th>4M</th><td>72.42</td><td>72.54</td></tr><tr><th>UNITER [8]</th><th>4M</th><td>72.70</td><td>72.91</td></tr><tr><th>UNIMO [34]</th><th>4M</th><td>73.79</td><td>74.02</td></tr><tr><th>OSCAR [35]</th><th>4M</th><td>73.16</td><td>73.44</td></tr><tr><th>ALBEF [30]</th><th>4M</th><td>74.54</td><td>74.70</td></tr><tr><th>BLIP [29]</th><th>14M</th><td>77.54</td><td>77.62</td></tr><tr><th>Singularity [25]</th><th>5M</th><td>70.30</td><td>70.53</td></tr><tr><th>HiTeA</th><th>5M</th><td>74.06</td><td>74.28</td></tr></tbody></table>", "caption": "Table 12: Comparison to existing methods on VQA.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[19] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.", "[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer, 2020.", "[54] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.", "[9] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In International Conference on Machine Learning, pages 1931\u20131942. PMLR, 2021.", "[34] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409, 2020.", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.", "[35] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.", "[30] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34:9694\u20139705, 2021.", "[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">MSRVTT [64]</th><th colspan=\"3\">SSv2-Template [25]</th><th colspan=\"3\">SSv2-Label [25]</th></tr><tr><th>Method</th><th>Original \\uparrow</th><th>Shuffled \\downarrow</th><th>Gap \\uparrow</th><th>Original \\uparrow</th><th>Shuffled \\downarrow</th><th>Gap \\uparrow</th><th>Original \\uparrow</th><th>Shuffled \\downarrow</th><th>Gap \\uparrow</th></tr></thead><tbody><tr><th>\\mathcal{L}_{\\text{base}}</th><td>61.7</td><td>60.8</td><td>0.9</td><td>93.5</td><td>75.1</td><td>18.4</td><td>74.6</td><td>71.9</td><td>2.7</td></tr><tr><th>\\mathcal{L}_{\\text{base}} + \\mathcal{L}_{\\text{CME}}</th><td>63.7</td><td>62.9</td><td>0.8</td><td>94.4</td><td>72.6</td><td>21.8</td><td>74.8</td><td>71.8</td><td>3.0</td></tr><tr><th>\\mathcal{L}_{\\text{base}} + \\mathcal{L}_{\\text{MTRE}}</th><td>63.0</td><td>62.6</td><td>0.4</td><td>94.1</td><td>73.0</td><td>21.1</td><td>75.8</td><td>72.2</td><td>3.6</td></tr><tr><th>\\mathcal{L}_{\\text{base}} + \\mathcal{L}_{\\text{CME}} + \\mathcal{L}_{\\text{MTRE}}</th><td>64.2</td><td>63.3</td><td>0.9</td><td>95.2</td><td>72.4</td><td>22.8</td><td>76.7</td><td>73.5</td><td>3.2</td></tr></tbody></table>", "caption": "Table 13: Evaluation of proposed methods for temporal dependency with temporal shuffling test. We evaluate the performance drop when shuffling the input during inference. \u201cOriginal\u201d and \u201cShuffled\u201d denote the original and shuffled input videos, respectively, and \u201cGap\u201d is the difference between the Original and Shuffled metric. The larger \"Gap\" indicates the dataset relies on temporal information, and the model utilizes more temporal information to solve the task.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[64] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016."]}, {"table": "<table><thead><tr><th>Method</th><th>MSRVTT</th><th>DiDeMo</th><th>SSv2-Template</th></tr></thead><tbody><tr><th>TimeSformer (\\mathcal{L}_{\\text{base}})</th><td>57.30</td><td>62.38</td><td>92.91</td></tr><tr><th>+ \\mathcal{L}_{\\text{MTRE}}</th><td>59.23</td><td>63.18</td><td>93.68</td></tr><tr><th>+ \\mathcal{L}_{\\text{CME}}</th><td>59.03</td><td>63.78</td><td>93.30</td></tr><tr><th>+ \\mathcal{L}_{\\text{CME}} + \\mathcal{L}_{\\text{MTRE}}</th><td>59.93</td><td>65.34</td><td>94.25</td></tr></tbody></table>", "caption": "Table 14: Effectiveness of the proposed methods on different video backbone. We use TimeSformer [3] pre-trained on ImageNet-21K [48] to verify the generalization ability of our proposed method. For text-to-video retrieval, the Mean Recall of Recall@1, Recall@5, and Recall@10 is reported. For video question answering task, we report the Top-1 accuracy.", "list_citation_info": ["[48] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.", "[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, page 4, 2021."]}, {"table": "<table><thead><tr><th>Method</th><th># of Parameter</th></tr></thead><tbody><tr><th>ClipBERT [26]</th><td>137M</td></tr><tr><th>Frozen [2]</th><td>232M</td></tr><tr><th>BridgeFormer [12]</th><td>152M</td></tr><tr><th>All-in-one [58]</th><td>110M</td></tr><tr><th>VIOLET [11]</th><td>198M</td></tr><tr><th>ALPRO [28]</th><td>231M</td></tr><tr><th>Singularity [25]</th><td>209M</td></tr><tr><th>LAVENDER [11]</th><td>198M</td></tr><tr><th>HiTeA</th><td>297M</td></tr></tbody></table>", "caption": "Table 15: Comparison to other models in the number of parameters.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[28] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4953\u20134963, 2022.", "[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.", "[11] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.", "[58] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.", "[12] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022."]}, {"table": "<table><thead><tr><th>Dataset</th><th>Optimizer</th><th>Learning Rate</th><th>Weight Decay</th><th>LR Schedule</th><th>Batch Size \\times # GPUs</th><th>Epochs</th></tr></thead><tbody><tr><td>MSRVTT-Ret [64]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>10</td></tr><tr><td>DiDeMo [1]</td><td>AdamW</td><td>1e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>20</td></tr><tr><td>LSMDC [49]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>10</td></tr><tr><td>Activity Caption [21]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>20</td></tr><tr><td>SSv2-Template [25]</td><td>AdamW</td><td>5e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>20</td></tr><tr><td>SSv2-Label [25]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>20</td></tr><tr><td>MSRVTT-QA [62]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>8</td></tr><tr><td>MSVD-QA [62]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>8</td></tr><tr><td>TGIF-FrameQA [16]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>8</td></tr><tr><td>LSMDC-FIB [42]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>8</td></tr><tr><td>ActivityNet-QA [70]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>8</td></tr><tr><td>TGIF-Action [16]</td><td>AdamW</td><td>3e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>56</td></tr><tr><td>TGIF-Transition [16]</td><td>AdamW</td><td>3e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>30</td></tr><tr><td>LSMDC-MC [56]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>10</td></tr><tr><td>NExT-QA [60]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>16\\times 8</td><td>10</td></tr><tr><td>MSRVTT-Caption [64]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>10</td></tr><tr><td>MSVD-Caption [5]</td><td>AdamW</td><td>2e-5</td><td>0.02</td><td>Cosine Decay</td><td>24\\times 8</td><td>10</td></tr></tbody></table>", "caption": "Table 16: End-to-end fine-tuning configurations for video-language downstream tasks.", "list_citation_info": ["[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.", "[49] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3202\u20133212, 2015.", "[21] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706\u2013715, 2017.", "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 5803\u20135812, 2017.", "[5] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190\u2013200, 2011.", "[56] Atousa Torabi, Niket Tandon, and Leonid Sigal. Learning language-visual embedding for movie understanding with natural-language. arXiv preprint arXiv:1609.08124, 2016.", "[70] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9127\u20139134, 2019.", "[62] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.", "[64] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.", "[16] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766, 2017.", "[60] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9777\u20139786, 2021.", "[42] Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6884\u20136893, 2017."]}], "citation_info_to_title": {"[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 5803\u20135812, 2017.": "Localizing moments in video with natural language", "[66] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. arXiv preprint arXiv:2206.08155, 2022.": "Zero-shot video question answering via frozen bidirectional language models", "[12] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022.": "Bridging video-text retrieval with multiple choice questions", "[51] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining for multimodal video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17959\u201317968, 2022.": "End-to-end generative pretraining for multimodal video captioning", "[4] Shyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the\" video\" in video-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2917\u20132927, 2022.": "Revisiting the video in video-language understanding", "[71] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16375\u201316387, 2022.": "Merlot Reserve: Neural Script Knowledge Through Vision and Language and Sound", "[32] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160, 2022.": "Lavender: Unifying video-language understanding as masked language modeling", "[40] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293\u2013304, 2022.": "Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning", "[39] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020.": "Univl: A unified video and language pre-training model for multimodal understanding and generation", "[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021.": "Less is more: Clipbert for video-and-language learning via sparse sampling", "[5] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190\u2013200, 2011.": "Collecting highly parallel data for paraphrase evaluation", "[28] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven CH Hoi. Align and prompt: Video-and-language pre-training with entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4953\u20134963, 2022.": "Align and prompt: Video-and-language pre-training with entity prompts", "[15] Jingjia Huang, Yinan Li, Jiashi Feng, Xiaoshuai Sun, and Rongrong Ji. Clover: Towards a unified video-language alignment and fusion model. arXiv preprint arXiv:2207.07885, 2022.": "Clover: Towards a unified video-language alignment and fusion model", "[48] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. arXiv preprint arXiv:2104.10972, 2021.": "Imagenet-21k pretraining for the masses", "[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021.": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "[27] Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et al. mplug: Effective and efficient vision-language learning by cross-modal skip-connections. arXiv preprint arXiv:2205.12005, 2022.": "mplug: Effective and efficient vision-language learning by cross-modal skip-connections", "[58] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.": "All in one: Exploring unified video-language pre-training", "[49] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3202\u20133212, 2015.": "A dataset for movie description", "[65] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697, 2021.": "Just ask: Learning to answer questions from millions of narrated videos", "[70] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9127\u20139134, 2019.": "Activitynet-qa: A dataset for understanding complex web videos via question answering", "[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.": "Scaling up visual and vision-language representation learning with noisy text supervision", "[13] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "[63] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021.": "Videoclip: Contrastive pre-training for zero-shot video-text understanding", "[54] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.": "Lxmert: Learning cross-modality encoder representations from transformers", "[30] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34:9694\u20139705, 2021.": "Align before fuse: Vision and language representation learning with momentum distillation", "[34] Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning. arXiv preprint arXiv:2012.15409, 2020.": "Unimo: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning", "[37] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. Swinbert: End-to-end transformers with sparse attention for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17949\u201317958, 2022.": "Swinbert: End-to-end transformers with sparse attention for video captioning", "[11] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021.": "Violet: End-to-end video-language transformers with masked visual-token modeling", "[41] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval. arXiv preprint arXiv:2207.07285, 2022.": "X-clip: End-to-end multi-grained contrastive learning for video-text retrieval", "[56] Atousa Torabi, Niket Tandon, and Leonid Sigal. Learning language-visual embedding for movie understanding with natural-language. arXiv preprint arXiv:1609.08124, 2016.": "Learning Language-Visual Embedding for Movie Understanding with Natural-Language", "[60] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9777\u20139786, 2021.": "Next-qa: Next phase of question-answering to explaining temporal actions", "[55] Mingkang Tang, Zhanyu Wang, Zhenhua Liu, Fengyun Rao, Dian Li, and Xiu Li. Clip4caption: Clip for video caption. In Proceedings of the 29th ACM International Conference on Multimedia, pages 4858\u20134862, 2021.": "Clip4caption: Clip for video caption", "[16] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758\u20132766, 2017.": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering", "[18] Pin Jiang and Yahong Han. Reasoning with heterogeneous graph alignment for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11109\u201311116, 2020.": "Reasoning with heterogeneous graph alignment for video question answering", "[61] Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan. Video graph transformer for video question answering. arXiv preprint arXiv:2207.05342, 2022.": "Video Graph Transformer for Video Question Answering", "[29] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022.": "Blip: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "[21] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706\u2013715, 2017.": "Dense-captioning events in videos", "[23] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for multimodal video question answering. International Journal of Computer Vision, 129(11):3027\u20133050, 2021.": "Hierarchical Conditional Relation Networks for Multimodal Video Question Answering", "[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer, 2020.": "Uniter: Universal image-text representation learning", "[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, page 4, 2021.": "Is space-time attention all you need for video understanding?", "[72] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34:23634\u201323651, 2021.": "Merlot: Multimodal neural script knowledge models", "[42] Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6884\u20136893, 2017.": "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering", "[35] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer, 2020.": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "[19] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021.": "Vilt: Vision-and-language transformer without convolution or region supervision", "[9] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In International Conference on Machine Learning, pages 1931\u20131942. PMLR, 2021.": "Unifying vision-and-language tasks via text generation", "[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022.": "Revealing single frame bias for video-and-language learning", "[57] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.": "Cider: Consensus-based image description evaluation", "[64] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language", "[62] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.": "Video question answering via gradually refined attention over appearance and motion"}, "source_title_to_arxiv_id": {"Merlot Reserve: Neural Script Knowledge Through Vision and Language and Sound": "2201.02639", "Align and prompt: Video-and-language pre-training with entity prompts": "2112.09583", "Frozen in time: A joint video and image encoder for end-to-end retrieval": "2104.00650", "mplug: Effective and efficient vision-language learning by cross-modal skip-connections": "2205.12005", "Align before fuse: Vision and language representation learning with momentum distillation": "2107.07651", "Violet: End-to-end video-language transformers with masked visual-token modeling": "2111.12681", "Clip4caption: Clip for video caption": "2110.05204", "Blip: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation": "2201.12086"}}