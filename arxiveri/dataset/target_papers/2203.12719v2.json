{"title": "What to Hide from Your Students: Attention-Guided Masked Image Modeling", "abstract": "Transformers and masked language modeling are quickly being adopted and\nexplored in computer vision as vision transformers and masked image modeling\n(MIM). In this work, we argue that image token masking differs from token\nmasking in text, due to the amount and correlation of tokens in an image. In\nparticular, to generate a challenging pretext task for MIM, we advocate a shift\nfrom random masking to informed masking. We develop and exhibit this idea in\nthe context of distillation-based MIM, where a teacher transformer encoder\ngenerates an attention map, which we use to guide masking for the student. We\nthus introduce a novel masking strategy, called attention-guided masking\n(AttMask), and we demonstrate its effectiveness over random masking for dense\ndistillation-based MIM as well as plain distillation-based self-supervised\nlearning on classification tokens. We confirm that AttMask accelerates the\nlearning process and improves the performance on a variety of downstream tasks.\nWe provide the implementation code at https://github.com/gkakogeorgiou/attmask.", "authors": ["Ioannis Kakogeorgiou", "Spyros Gidaris", "Bill Psomas", "Yannis Avrithis", "Andrei Bursuc", "Konstantinos Karantzalos", "Nikos Komodakis"], "published_date": "2022_03_23", "pdf_url": "http://arxiv.org/pdf/2203.12719v2", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">iBOT Masking</th><th rowspan=\"2\">Ratio (%)</th><th colspan=\"2\">ImageNet-1k</th><th>CIFAR10</th><th>CIFAR100</th></tr><tr><th>k-NN</th><th>Linear</th><th colspan=\"2\">Fine-tuning</th></tr></thead><tbody><tr><th>Random Block-Wise{}^{\\dagger}</th><td>10-50</td><td>46.7</td><td>56.4</td><td>98.0</td><td>86.0</td></tr><tr><th>Random{}^{\\ddagger}</th><td>75</td><td>47.3</td><td>55.5</td><td>97.7</td><td>85.5</td></tr><tr><th>Random</th><td>10-50</td><td>47.8</td><td>56.7</td><td>98.0</td><td>86.1</td></tr><tr><th>AttMask-Low (ours)</th><td>10-50</td><td>44.0</td><td>53.4</td><td>97.6</td><td>84.6</td></tr><tr><th>AttMask-Hint (ours)</th><td>10-50</td><td>49.5</td><td>57.5</td><td>98.1</td><td>86.6</td></tr><tr><th>AttMask-High (ours)</th><td>10-50</td><td>49.7</td><td>57.9</td><td>98.2</td><td>86.6</td></tr></tbody></table>", "caption": "Table 1: Different masking strategies for iBOT [78] pre-training on 20% of ImageNet.Top-1 accuracy for k-NN, linear probing on ImageNet validation set; fine-tuning on CIFAR10/100. \\dagger: default iBOT masking strategy from BEiT [2]. \\ddagger: aggressive random masking strategy from MAE [24].", "list_citation_info": ["[2] Bao, H., Dong, L., Piao, S., Wei, F.: BEit: BERT pre-training of image transformers. In: International Conference on Learning Representations (2022)", "[78] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: ibot: Image bert pre-training with online tokenizer. In: International Conference on Learning Representations (2022)", "[24] He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 16000\u201316009 (2022)"]}, {"table": "<p>% ImageNet-1k51020100Random Block-Wise{}^{\\dagger}15.731.946.771.5AttMask-High (ours)17.533.849.772.5 </p><svg><g><g><clippath><path></path></clippath><g><g><path></path></g><g></g></g><clippath><path></path></clippath><g><g><path></path></g><g></g></g><clippath><path></path></clippath><g><g><path></path></g><g></g></g><clippath><path></path></clippath><g><g><path></path></g><g></g></g><path></path><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>0</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>10</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>20</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>30</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>40</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>50</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>60</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>70</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>80</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>90</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>100</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>0</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>5</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>10</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>15</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>20</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>25</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>30</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>35</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>40</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>45</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>50</text></g></g></g></g><clippath><path></path></clippath><g><g><path></path></g><g></g><g><path></path></g><g></g><g><path></path></g><g></g><g><path></path></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g></g></g><g></g><g><foreignobject></foreignobject><g><g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>42% fewer</text></g></g></g></g></g><g><g><text>epochs</text></g></g></g></g><g></g><path></path><g><path></path></g><g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><text>epoch</text></g></g></g></g><g><foreignobject></foreignobject><g><g><g><text>pt</text></g></g><g><g><foreignobject>k-NN</foreignobject></g></g></g></g><g><path></path></g><g><foreignobject></foreignobject><g><g><g><path></path></g></g><g><g><path></path></g></g></g></g></g></g></svg>", "caption": "Table 2: Top-1 k-NN accuracy on ImageNet-1k validation for iBOT pre-training on different percentage (%) of ImageNet-1k. \\dagger: default iBOT masking strategy from BEiT [2].", "list_citation_info": ["[2] Bao, H., Dong, L., Piao, S., Wei, F.: BEit: BERT pre-training of image transformers. In: International Conference on Learning Representations (2022)"]}, {"table": "<table><tbody><tr><th>No Masking{}^{\\dagger}</th><td>Random</td><td>AttMask-Low</td><td>AttMask-Hint</td><td>AttMask-High</td></tr><tr><th>43.0</th><td>43.4</td><td>42.7</td><td>43.6</td><td>43.5</td></tr></tbody></table>", "caption": "Table 3: Top-1 k-NN accuracy on ImageNet-1k validation for DINO [8] pre-training on 20% of the ImageNet-1k training set using mask ratio of 10-50%. \\dagger: default DINO.", "list_citation_info": ["[8] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9650\u20139660 (2021)"]}, {"table": "<table><tbody><tr><th></th><td></td><td><img/></td><td><img/></td><td><img/></td><td><img/></td><td><img/></td><td><img/></td><td><img/></td><td><img/></td></tr><tr><th>iBOT Masking</th><td>Ratio (%)</td><td>OF</td><td>MS</td><td>MR</td><td>MN</td><td>NF</td><td>OBB</td><td>OBT</td><td>IN-9</td></tr><tr><th>Random Block-wise{}^{\\dagger}</th><td>10-50</td><td>72.4</td><td>74.3</td><td>59.4</td><td>56.8</td><td>36.3</td><td>14.4</td><td>15.0</td><td>89.1</td></tr><tr><th>Random{}^{\\ddagger}</th><td>75</td><td>73.1</td><td>73.8</td><td>58.8</td><td>55.9</td><td>35.6</td><td>13.7</td><td>14.5</td><td>87.9</td></tr><tr><th>Random</th><td>10-50</td><td>72.8</td><td>75.3</td><td>60.4</td><td>57.5</td><td>34.9</td><td>10.3</td><td>14.4</td><td>89.3</td></tr><tr><th>AttMask-Low (ours)</th><td>10-50</td><td>66.0</td><td>71.1</td><td>55.2</td><td>52.2</td><td>32.4</td><td>12.5</td><td>14.0</td><td>86.6</td></tr><tr><th>AttMask-Hint (ours)</th><td>10-50</td><td>74.4</td><td>75.9</td><td>61.7</td><td>58.3</td><td>39.6</td><td>16.7</td><td>15.7</td><td>89.6</td></tr><tr><th>AttMask-High (ours)</th><td>10-50</td><td>75.2</td><td>76.2</td><td>62.3</td><td>59.4</td><td>40.6</td><td>15.2</td><td>15.3</td><td>89.8</td></tr></tbody></table>", "caption": "Table 4: Background robustness: Linear probing of iBOT model on IN-9 [69] and its variations, when pre-trained on 20% ImageNet-1k under different masking strategies. \\dagger: default iBOT masking strategy from BEiT [2]. \\ddagger: aggressive random masking.", "list_citation_info": ["[2] Bao, H., Dong, L., Piao, S., Wei, F.: BEit: BERT pre-training of image transformers. In: International Conference on Learning Representations (2022)", "[69] Xiao, K., Engstrom, L., Ilyas, A., Madry, A.: Noise or signal: The role of image backgrounds in object recognition. In: International Conference on Learning Representations (2021)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">(a) Full</th><th colspan=\"4\">(b) Few Examples</th></tr><tr><th>k-NN</th><th>Linear</th><th>\\nu=1</th><th>5</th><th>10</th><th>20</th></tr></thead><tbody><tr><td>DINO [8]</td><td>70.9</td><td>74.6</td><td></td><td></td><td></td><td></td></tr><tr><td>MST [38]</td><td>72.1</td><td>75.0</td><td></td><td></td><td></td><td></td></tr><tr><td>iBOT [78]</td><td>71.5</td><td>74.4</td><td>32.9</td><td>47.6</td><td>52.5</td><td>56.4</td></tr><tr><td>iBOT+AttMask-High</td><td>72.5</td><td>75.7</td><td>37.1</td><td>51.3</td><td>55.7</td><td>59.1</td></tr><tr><td>iBOT+AttMask-Hint</td><td>72.8</td><td>76.1</td><td>37.6</td><td>52.2</td><td>56.4</td><td>59.6</td></tr></tbody></table>", "caption": "Table 5: Top-1 accuracy on ImageNet validation set. (a) k-NN and linear probing using the full ImageNet training set; (b) k-NN using only \\nu\\in\\{1,5,10,20\\} examples per class. Pre-training on 100% ImageNet-1k for 100 epochs.", "list_citation_info": ["[78] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: ibot: Image bert pre-training with online tokenizer. In: International Conference on Learning Representations (2022)", "[8] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9650\u20139660 (2021)", "[38] Li, Z., Chen, Z., Yang, F., Li, W., Zhu, Y., Zhao, C., Deng, R., Wu, L., Zhao, R., Tang, M., et al.: Mst: Masked self-supervised transformer for visual representation. Advances in Neural Information Processing Systems 34 (2021)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th>CIFAR10</th><th>CIFAR100</th><th>Flowers</th><th colspan=\"2\">COCO</th><th>ADE20K</th></tr><tr><th colspan=\"3\">Accuracy</th><th>AP{}^{b}</th><th>AP{}^{m}</th><th>mIoU</th></tr></thead><tbody><tr><td>iBOT</td><td>98.8</td><td>89.5</td><td>96.8</td><td>48.2</td><td>41.8</td><td>44.9</td></tr><tr><td>iBOT+AttMask</td><td>98.8</td><td>90.1</td><td>97.7</td><td>48.8</td><td>42.0</td><td>45.3</td></tr></tbody></table>", "caption": "Table 6: Fine-tuning for image classification on CIFAR10 [34], CIFAR100 [34] and Oxford Flowers [47]; Object detection(AP{}^{b}, %) and instance segmentation (AP{}^{m}, %) on COCO [39]; and semantic segmentation on ADE20K [77] (mIoU, %). Models pre-trained on 100% ImageNet-1k training set for 100 epochs.", "list_citation_info": ["[39] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European Conference on Computer Vision. pp. 740\u2013755. Springer (2014)", "[47] Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number of classes. In: Indian Conference on Computer Vision, Graphics and Image Processing (Dec 2008)", "[34] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)", "[77] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 633\u2013641 (2017)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">(a) \\mathcal{R}Oxford</th><th colspan=\"2\">(b) \\mathcal{R}Paris</th><th colspan=\"3\">(c) DAVIS 2017</th></tr><tr><th>Medium</th><th>Hard</th><th>Medium</th><th>Hard</th><th>(\\mathcal{J}\\&amp;\\mathcal{F})_{m}</th><th>\\mathcal{J}_{m}</th><th>\\mathcal{F}_{m}</th></tr></thead><tbody><tr><td>iBOT</td><td>31.0</td><td>11.7</td><td>56.2</td><td>28.9</td><td>60.5</td><td>59.5</td><td>61.4</td></tr><tr><td>iBOT+AttMask</td><td>33.5</td><td>12.1</td><td>59.0</td><td>31.5</td><td>62.1</td><td>60.6</td><td>63.5</td></tr></tbody></table>", "caption": "Table 7: Image retrieval (mAP, %) on (a) \\mathcal{R}Oxford and (b) \\mathcal{R}Paris [53] and video object segmentation (mean region similarity \\mathcal{J}_{m} and contour-based accuracy \\mathcal{F}_{m}, %) on (c) DAVIS 2017 [52], without fine-tuning. Models pre-trained on 100% ImageNet-1k training set for 100 epochs.", "list_citation_info": ["[52] Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016)", "[53] Radenovi\u0107, F., Iscen, A., Tolias, G., Avrithis, Y., Chum, O.: Revisiting oxford and paris: Large-scale image retrieval benchmarking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5706\u20135715 (2018)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">CUB200</th><th colspan=\"3\">CARS196</th><th colspan=\"3\">SOP</th><th colspan=\"3\">In-Shop</th></tr><tr><th>R@1</th><th>2</th><th>4</th><th>R@1</th><th>2</th><th>4</th><th>R@1</th><th>10</th><th>100</th><th>R@1</th><th>10</th><th>20</th></tr></thead><tbody><tr><th>iBOT</th><td>51.4</td><td>63.8</td><td>75.0</td><td>35.6</td><td>46.0</td><td>56.3</td><td>57.4</td><td>72.2</td><td>84.0</td><td>39.1</td><td>61.9</td><td>68.2</td></tr><tr><th>iBOT+AttMask</th><td>57.2</td><td>69.4</td><td>80.3</td><td>39.8</td><td>50.4</td><td>61.4</td><td>59.0</td><td>73.9</td><td>85.4</td><td>40.7</td><td>63.7</td><td>70.3</td></tr></tbody></table>", "caption": "Table 8: Fine-grained classification (R@k: Recall@k, %) [45] without fine-tuning. Models pre-trained on 100% ImageNet-1k training set for 100 epochs.", "list_citation_info": ["[45] Musgrave, K., Belongie, S., Lim, S.N.: A metric learning reality check. In: European Conference on Computer Vision (2020)"]}, {"table": "<table><tbody><tr><td></td><td>iBOT</td><td>iBOT+AttMask</td></tr><tr><td>Places205</td><td>55.9</td><td>56.7</td></tr></tbody></table>", "caption": "Table A12: Scene classification measuring accuracy (%) using linear probing on Places205 [76]. Models pre-trained on 100% ImageNet-1k training set for 100 epochs.", "list_citation_info": ["[76] Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems (2014)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"2\">(a) Full</th><th colspan=\"4\">(b) Few Examples</th></tr><tr><th>k-NN</th><th>Linear</th><th>\\nu=1</th><th>5</th><th>10</th><th>20</th></tr></thead><tbody><tr><td>SimCLR [9]</td><td>-</td><td>69.0</td><td></td><td></td><td></td><td></td></tr><tr><td>BYOL [23]</td><td>66.6</td><td>71.4</td><td></td><td></td><td></td><td></td></tr><tr><td>MoBY [71]</td><td>-</td><td>72.8</td><td></td><td></td><td></td><td></td></tr><tr><td>DINO [8]</td><td>72.8</td><td>76.1</td><td></td><td></td><td></td><td></td></tr><tr><td>MST [38]</td><td>75.0</td><td>76.9</td><td></td><td></td><td></td><td></td></tr><tr><td>iBOT [78]</td><td>74.6</td><td>77.4</td><td>38.9</td><td>54.1</td><td>58.5</td><td>61.9</td></tr><tr><td>iBOT+AttMask (Ours)</td><td>75.0</td><td>77.5</td><td>40.4</td><td>55.5</td><td>59.9</td><td>63.1</td></tr></tbody></table>", "caption": "Table A13: Top-1 accuracy on ImageNet validation set. (a) k-NN and linear probing using the full ImageNet training set; (b) k-NN using only \\nu\\in\\{1,5,10,20\\} examples per class. Pre-training on 100% ImageNet-1k for 300 epochs.", "list_citation_info": ["[9] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International Conference on Machine Learning. pp. 1597\u20131607. PMLR (2020)", "[71] Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Self-supervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)", "[38] Li, Z., Chen, Z., Yang, F., Li, W., Zhu, Y., Zhao, C., Deng, R., Wu, L., Zhao, R., Tang, M., et al.: Mst: Masked self-supervised transformer for visual representation. Advances in Neural Information Processing Systems 34 (2021)", "[8] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9650\u20139660 (2021)", "[78] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: ibot: Image bert pre-training with online tokenizer. In: International Conference on Learning Representations (2022)", "[23] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems 33, 21271\u201321284 (2020)"]}, {"table": "<table><thead><tr><th>Mask Ratio r (%)</th><th>10-30</th><th>10-50</th><th>10-70</th><th>30</th></tr></thead><tbody><tr><th>Random Block-Wise</th><td>46.5</td><td>46.7{}^{\\dagger}</td><td>47.1</td><td>46.9</td></tr><tr><th>Random</th><td>47.6</td><td>47.8</td><td>47.8</td><td>48.2</td></tr><tr><th>AttMask-High</th><td>49.5</td><td>49.7</td><td>48.5</td><td>49.1</td></tr></tbody></table>", "caption": "Table A16: AttMask-High vs. random masking strategies: k-NN top-1 accuracy on ImageNet-1k validation for iBOT pre-training on 20% of ImageNet-1k for different mask ratio r. \\dagger: default iBOT masking strategy from BEiT [2].", "list_citation_info": ["[2] Bao, H., Dong, L., Piao, S., Wei, F.: BEit: BERT pre-training of image transformers. In: International Conference on Learning Representations (2022)"]}], "citation_info_to_title": {"[38] Li, Z., Chen, Z., Yang, F., Li, W., Zhu, Y., Zhao, C., Deng, R., Wu, L., Zhao, R., Tang, M., et al.: Mst: Masked self-supervised transformer for visual representation. Advances in Neural Information Processing Systems 34 (2021)": "Mst: Masked self-supervised transformer for visual representation", "[24] He, K., Chen, X., Xie, S., Li, Y., Doll\u00e1r, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 16000\u201316009 (2022)": "Masked autoencoders are scalable vision learners", "[78] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: ibot: Image bert pre-training with online tokenizer. In: International Conference on Learning Representations (2022)": "ibot: Image bert pre-training with online tokenizer", "[71] Xie, Z., Lin, Y., Yao, Z., Zhang, Z., Dai, Q., Cao, Y., Hu, H.: Self-supervised learning with swin transformers. arXiv preprint arXiv:2105.04553 (2021)": "Self-supervised learning with swin transformers", "[23] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems 33, 21271\u201321284 (2020)": "Bootstrap your own latent-a new approach to self-supervised learning", "[2] Bao, H., Dong, L., Piao, S., Wei, F.: BEit: BERT pre-training of image transformers. In: International Conference on Learning Representations (2022)": "BEit: BERT pre-training of image transformers", "[39] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European Conference on Computer Vision. pp. 740\u2013755. Springer (2014)": "Microsoft COCO: Common Objects in Context", "[52] Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-Hornung, A.: A benchmark dataset and evaluation methodology for video object segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016)": "A benchmark dataset and evaluation methodology for video object segmentation", "[45] Musgrave, K., Belongie, S., Lim, S.N.: A metric learning reality check. In: European Conference on Computer Vision (2020)": "A metric learning reality check", "[9] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International Conference on Machine Learning. pp. 1597\u20131607. PMLR (2020)": "A simple framework for contrastive learning of visual representations", "[47] Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number of classes. In: Indian Conference on Computer Vision, Graphics and Image Processing (Dec 2008)": "Automated flower classification over a large number of classes", "[53] Radenovi\u0107, F., Iscen, A., Tolias, G., Avrithis, Y., Chum, O.: Revisiting oxford and paris: Large-scale image retrieval benchmarking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5706\u20135715 (2018)": "Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking", "[76] Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features for scene recognition using places database. In: Advances in Neural Information Processing Systems (2014)": "Learning Deep Features for Scene Recognition Using Places Database", "[77] Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 633\u2013641 (2017)": "Scene parsing through ade20k dataset", "[8] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9650\u20139660 (2021)": "Emerging properties in self-supervised vision transformers", "[34] Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)": "Learning multiple layers of features from tiny images", "[69] Xiao, K., Engstrom, L., Ilyas, A., Madry, A.: Noise or signal: The role of image backgrounds in object recognition. In: International Conference on Learning Representations (2021)": "Noise or Signal: The Role of Image Backgrounds in Object Recognition"}, "source_title_to_arxiv_id": {"Mst: Masked self-supervised transformer for visual representation": "2106.05656", "Masked autoencoders are scalable vision learners": "2111.06377", "ibot: Image bert pre-training with online tokenizer": "2111.07832", "Emerging properties in self-supervised vision transformers": "2104.14294"}}