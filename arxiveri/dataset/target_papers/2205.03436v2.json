{"title": "EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers", "abstract": "Self-attention based models such as vision transformers (ViTs) have emerged\nas a very competitive architecture alternative to convolutional neural networks\n(CNNs) in computer vision. Despite increasingly stronger variants with\never-higher recognition accuracies, due to the quadratic complexity of\nself-attention, existing ViTs are typically demanding in computation and model\nsize. Although several successful design choices (e.g., the convolutions and\nhierarchical multi-stage structure) of prior CNNs have been reintroduced into\nrecent ViTs, they are still not sufficient to meet the limited resource\nrequirements of mobile devices. This motivates a very recent attempt to develop\nlight ViTs based on the state-of-the-art MobileNet-v2, but still leaves a\nperformance gap behind. In this work, pushing further along this under-studied\ndirection we introduce EdgeViTs, a new family of light-weight ViTs that, for\nthe first time, enable attention-based vision models to compete with the best\nlight-weight CNNs in the tradeoff between accuracy and on-device efficiency.\nThis is realized by introducing a highly cost-effective local-global-local\n(LGL) information exchange bottleneck based on optimal integration of\nself-attention and convolutions. For device-dedicated evaluation, rather than\nrelying on inaccurate proxies like the number of FLOPs or parameters, we adopt\na practical approach of focusing directly on on-device latency and, for the\nfirst time, energy efficiency. Specifically, we show that our models are\nPareto-optimal when both accuracy-latency and accuracy-energy trade-offs are\nconsidered, achieving strict dominance over other ViTs in almost all cases and\ncompeting with the most efficient CNNs. Code is available at\nhttps://github.com/saic-fi/edgevit.", "authors": ["Junting Pan", "Adrian Bulat", "Fuwen Tan", "Xiatian Zhu", "Lukasz Dudziak", "Hongsheng Li", "Georgios Tzimiropoulos", "Brais Martinez"], "published_date": "2022_05_06", "pdf_url": "http://arxiv.org/pdf/2205.03436v2", "list_table_and_caption": [{"table": "<table><thead><tr><th>Model</th><th><p>#Params</p></th><th><p>FLOPs</p></th><th><p>CPU(ms)</p></th><th><p>Acc Top-1 (%)</p></th></tr></thead><tbody><tr><th>MobileNet-v2[43]</th><td><p>3.4M</p></td><td><p>0.3G</p></td><td><p>33.3\\pm5.3</p></td><td><p>72.0</p></td></tr><tr><th>MobileNet-v3 0.75[22]</th><td>4.0M</td><td>0.16G</td><td>23.0\\pm3.7</td><td>73.3</td></tr><tr><th>EfficientNet-B0[48]</th><td>5.3M</td><td>0.4G</td><td>52.1\\pm7.4</td><td>77.1</td></tr><tr><th>MobileViT-XXS [37]</th><td><p>1.3M</p></td><td><p>0.4G</p></td><td><p>69.5\\pm5.1</p></td><td><p>69.0</p></td></tr><tr><th>PVT-v2-B0[56]</th><td><p>3.4M</p></td><td><p>0.6G</p></td><td><p>26.0\\pm6.9</p></td><td><p>70.5</p></td></tr><tr><th>Uniformer-Tiny*[26]</th><td><p>3.9M</p></td><td><p>0.6G</p></td><td><p>40.5\\pm3.1</p></td><td><p>74.1</p></td></tr><tr><th>Twins-SVT-Tiny*[7]</th><td><p>4.1M</p></td><td><p>0.6G</p></td><td><p>36.9\\pm2.3</p></td><td><p>71.2</p></td></tr><tr><th>EdgeViT-XXS</th><td>4.1M</td><td>0.6G</td><td>32.8\\pm2.7</td><td>74.4</td></tr><tr><th>T2T-ViT-7 [63]</th><td><p>4.3M</p></td><td><p>1.1G</p></td><td><p>48.8\\pm6.5</p></td><td><p>71.7</p></td></tr><tr><th>MobileViT-XS [37]</th><td><p>2.4M</p></td><td><p>1.1G</p></td><td><p>150.1\\pm6.1</p></td><td><p>74.7</p></td></tr><tr><th>DeiT-Tiny [52]</th><td><p>5.7M</p></td><td><p>1.3G</p></td><td><p>46.2\\pm13.6</p></td><td><p>72.2</p></td></tr><tr><th>TNT-Tiny [16]</th><td><p>6.1M</p></td><td><p>1.4G</p></td><td><p>86.4\\pm6.0</p></td><td><p>73.9</p></td></tr><tr><th>EdgeViT-XS</th><td>6.7M</td><td>1.1G</td><td>54.1\\pm2.2</td><td>77.5</td></tr><tr><th>T2T-ViT-12 [63]</th><td><p>6.9M</p></td><td><p>1.9G</p></td><td><p>69.9\\pm5.6</p></td><td><p>76.5</p></td></tr><tr><th>PVT-v2-B1[56]</th><td><p>14M</p></td><td><p>2.1G</p></td><td><p>75.4\\pm2.3</p></td><td><p>78.7</p></td></tr><tr><th>MobileViT-S [37]</th><td><p>5.6M</p></td><td><p>2.0G</p></td><td><p>221.3\\pm9.3</p></td><td><p>78.3</p></td></tr><tr><th>LeViT-384\\dagger [15]</th><td>39.1M</td><td>2.4G</td><td>71.3\\pm2.3</td><td>79.5</td></tr><tr><th>EdgeViT-S</th><td>11.1M</td><td>1.9G</td><td>85.3\\pm3.9</td><td>81.0</td></tr></tbody></table>", "caption": "Table 2: Results on ImageNet-1K validation set. All models are tested on input scale of 224\\times 224, except for MobileViTs [37] that are tested with 256\\times 256 according to their original implementation. * indicates down-scaled architectures beyond original definitions by authors to fit the mobile compute budget. LeViT-384\\dagger denotes the LeViT model retrained under the same setting as our EdgeViT. ", "list_citation_info": ["[48] Tan, M., Le, Q.: EfficientNet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning (2019)", "[43] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV2: Inverted residuals and linear bottlenecks. In: IEEE Conference on Computer Vision and Pattern Recognition (2018)", "[63] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token ViT: Training vision transformers from scratch on ImageNet. In: IEEE International Conference on Computer Vision (2021)", "[16] Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer. Advances on Neural Information Processing Systems (2021)", "[15] Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J\u00e9gou, H., Douze, M.: LeViT: a vision transformer in convnet\u2019s clothing for faster inference. In: IEEE International Conference on Computer Vision (2021)", "[26] Li, K., Wang, Y., Peng, G., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatial-temporal representation learning. In: International Conference on Learning Representations (2022)", "[56] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: PVTv2: Improved baselines with pyramid vision transformer. Computational Visual Media (2022)", "[22] Howard, A., Pang, R., Adam, H., Le, Q.V., Sandler, M., Chen, B., Wang, W., Chen, L., Tan, M., Chu, G., Vasudevan, V., Zhu, Y.: Searching for MobileNetV3. In: IEEE International Conference on Computer Vision (2019)", "[7] Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. Advances on Neural Information Processing Systems (2021)", "[52] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning (2021)", "[37] Mehta, S., Rastegari, M.: MobileViT: light-weight, general-purpose, and mobile-friendly vision transformer. International Conference on Learning Representations (2022)"]}, {"table": "<table><thead><tr><th>Model</th><th><p>Top-1 (%)</p></th><th><p>CPU (ms)</p></th><th><p>Energy (mJ)</p></th><th><p>Power<br/>(W)</p></th><th><p>Efficiency (%/msW)</p></th></tr></thead><tbody><tr><th>MobileNet-v2[43]</th><td><p>72.0</p></td><td><p>33.3</p></td><td><p>85.7\\pm7.4</p></td><td><p>3.31\\pm0.26</p></td><td><p>0.841</p></td></tr><tr><th>MobileNet-v3 0.75[22]</th><td>73.3</td><td>23.0</td><td>63.0\\pm9.6</td><td>3.46\\pm0.4</td><td>1.164</td></tr><tr><th>EfficientNet-B0[48]</th><td>77.1</td><td>52.1</td><td>159.0\\pm26.2</td><td>3.62\\pm0.45</td><td>0.485</td></tr><tr><th>PVT-v2-B0[56]</th><td><p>70.5</p></td><td><p>26.0</p></td><td><p>91.7\\pm19.7</p></td><td><p>3.94\\pm0.68</p></td><td><p>0.769</p></td></tr><tr><th>PVT-v2-B1[56]</th><td>78.7</td><td><p>75.4</p></td><td><p>309.0\\pm65.8</p></td><td><p>4.63\\pm0.71</p></td><td><p>0.255</p></td></tr><tr><th>Twins-SVT-Tiny*[7]</th><td><p>71.2</p></td><td><p>36.9</p></td><td><p>114.5\\pm17.3</p></td><td><p>3.71\\pm0.24</p></td><td><p>0.622</p></td></tr><tr><th>DeiT-Tiny [52]</th><td><p>72.2</p></td><td><p>46.2</p></td><td><p>187.2\\pm7.6</p></td><td><p>4.77\\pm0.21</p></td><td><p>0.386</p></td></tr><tr><th>Uniformer-Tiny*[26]</th><td><p>74.1</p></td><td><p>40.5</p></td><td><p>134.7\\pm27.3</p></td><td><p>4.1\\pm0.71</p></td><td><p>0.55</p></td></tr><tr><th>T2T-ViT-12 [63]</th><td><p>76.5</p></td><td><p>69.9</p></td><td><p>266.2\\pm42.6</p></td><td><p>4.37\\pm0.36</p></td><td><p>0.287</p></td></tr><tr><th>TNT-Tiny [16]</th><td><p>73.9</p></td><td><p>86.4</p></td><td><p>308.7\\pm70.5</p></td><td><p>3.94\\pm0.63</p></td><td><p>0.239</p></td></tr><tr><th>LeViT-384\\dagger [15]</th><td>79.5</td><td>71.3\\pm2.2</td><td>455.2\\pm125.8</td><td>6.18\\pm0.74</td><td>0.173</td></tr><tr><th>MobileViT-XXS [37]</th><td><p>69.0</p></td><td><p>69.5</p></td><td><p>175.3\\pm28.7</p></td><td><p>2.77\\pm0.24</p></td><td><p>0.394</p></td></tr><tr><th>MobileViT-XS [37]</th><td><p>74.7</p></td><td><p>150.1</p></td><td><p>251.5\\pm81.1</p></td><td><p>2.63\\pm0.61</p></td><td><p>0.297</p></td></tr><tr><th>MobileViT-S [37]</th><td><p>78.3</p></td><td><p>221.3</p></td><td><p>503.6\\pm117.0</p></td><td><p>2.76\\pm0.21</p></td><td><p>0.155</p></td></tr><tr><th>EdgeViT-XXS</th><td>74.4</td><td>32.8</td><td>127.4\\pm27.3</td><td>4.27\\pm0.67</td><td>0.584</td></tr><tr><th>EdgeViT-XS</th><td>77.5</td><td>54.1</td><td>234.6\\pm44.0</td><td>4.77\\pm0.84</td><td>0.33</td></tr><tr><th>EdgeViT-S</th><td>81.0</td><td>85.3</td><td>386.7\\pm43.5</td><td>4.8\\pm0.26</td><td>0.209</td></tr></tbody></table>", "caption": "Table 3: On-device energy evaluation on ImageNet-1K. All relevant metrics are reported as mean values per forward pass across 50 executions.For facilitating comparison, we define an energy-aware efficiency metric as the average gain in top-1 accuracy from each 1W run for 1ms (equivalent to consuming 1mJ of energy).(Pareto-optimal models are highlighted in bold in the last column).", "list_citation_info": ["[48] Tan, M., Le, Q.: EfficientNet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning (2019)", "[43] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV2: Inverted residuals and linear bottlenecks. In: IEEE Conference on Computer Vision and Pattern Recognition (2018)", "[63] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token ViT: Training vision transformers from scratch on ImageNet. In: IEEE International Conference on Computer Vision (2021)", "[16] Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer. Advances on Neural Information Processing Systems (2021)", "[15] Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J\u00e9gou, H., Douze, M.: LeViT: a vision transformer in convnet\u2019s clothing for faster inference. In: IEEE International Conference on Computer Vision (2021)", "[26] Li, K., Wang, Y., Peng, G., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatial-temporal representation learning. In: International Conference on Learning Representations (2022)", "[56] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: PVTv2: Improved baselines with pyramid vision transformer. Computational Visual Media (2022)", "[22] Howard, A., Pang, R., Adam, H., Le, Q.V., Sandler, M., Chen, B., Wang, W., Chen, L., Tan, M., Chu, G., Vasudevan, V., Zhu, Y.: Searching for MobileNetV3. In: IEEE International Conference on Computer Vision (2019)", "[7] Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. Advances on Neural Information Processing Systems (2021)", "[52] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning (2021)", "[37] Mehta, S., Rastegari, M.: MobileViT: light-weight, general-purpose, and mobile-friendly vision transformer. International Conference on Learning Representations (2022)"]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Backbone</td><td colspan=\"7\">RetinaNet 1\\times</td><td colspan=\"7\">Mask R-CNN 1\\times</td></tr><tr><td>#Par.</td><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td><td>#Par.</td><td>AP{}^{\\rm b}</td><td>AP{}_{50}^{\\rm b}</td><td>AP{}_{75}^{\\rm b}</td><td>AP{}^{\\rm m}</td><td>AP{}_{50}^{\\rm m}</td><td>AP{}_{75}^{\\rm m}</td></tr><tr><td>PVTv2-B0 [56]</td><td>13.0</td><td>37.2</td><td>57.2</td><td>39.5</td><td>23.1</td><td>40.4</td><td>49.7</td><td>23.5</td><td>38.2</td><td>60.5</td><td>40.7</td><td>36.2</td><td>57.8</td><td>38.6</td></tr><tr><td>EdgeViT-XXS</td><td>13.1</td><td>38.7</td><td>59.0</td><td>41.0</td><td>22.4</td><td>42.0</td><td>51.6</td><td>23.8</td><td>39.9</td><td>62.0</td><td>43.1</td><td>36.9</td><td>59.0</td><td>39.4</td></tr><tr><td>EdgeViT-XS</td><td>16.3</td><td>40.6</td><td>61.3</td><td>43.3</td><td>25.2</td><td>43.9</td><td>54.6</td><td>26.5</td><td>41.4</td><td>63.7</td><td>45.0</td><td>38.3</td><td>60.9</td><td>41.3</td></tr><tr><td>ResNet18 [20]</td><td>21.3</td><td>31.8</td><td>49.6</td><td>33.6</td><td>16.3</td><td>34.3</td><td>43.2</td><td>31.2</td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td></tr><tr><td>PVTv1-Tiny [55]</td><td>23.0</td><td>36.7</td><td>56.9</td><td>38.9</td><td>22.6</td><td>38.8</td><td>50.0</td><td>32.9</td><td>36.7</td><td>59.2</td><td>39.3</td><td>35.1</td><td>56.7</td><td>37.3</td></tr><tr><td>PVTv2-B1 [56]</td><td>23.8</td><td>41.2</td><td>61.9</td><td>43.9</td><td>25.4</td><td>44.5</td><td>54.3</td><td>33.7</td><td>41.8</td><td>64.3</td><td>45.9</td><td>38.8</td><td>61.2</td><td>41.6</td></tr><tr><td>EdgeViT-S</td><td>22.6</td><td>43.4</td><td>64.9</td><td>46.5</td><td>26.9</td><td>47.5</td><td>58.1</td><td>32.8</td><td>44.8</td><td>67.4</td><td>48.9</td><td>41.0</td><td>64.2</td><td>43.8</td></tr></tbody></table>", "caption": "Table 5: Comparison to other visual backbones using RetinaNet and Mask-RCNN on COCO val2017 object detection and instance segmentation. \u201c#Par.\u201d refers to number of parameters in million. AP{}^{\\rm b} and AP{}^{\\rm m} indicate bounding box AP and mask AP.", "list_citation_info": ["[55] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: IEEE International Conference on Computer Vision (2021)", "[20] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (2016)", "[56] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: PVTv2: Improved baselines with pyramid vision transformer. Computational Visual Media (2022)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Backbone</th><th colspan=\"3\">Semantic FPN</th></tr><tr><th>#Param (M)</th><th>GFLOPs</th><th>mIoU (%)</th></tr></thead><tbody><tr><th>PVTv2-B0[56]</th><td>7.6</td><td>25.0</td><td>37.2</td></tr><tr><th>EdgeViT-XXS</th><td>7.9</td><td>24.4</td><td>39.7</td></tr><tr><th>EdgeViT-XS</th><td>10.6</td><td>27.7</td><td>41.4</td></tr><tr><th>ResNet18 [20]</th><td>15.5</td><td>32.2</td><td>32.9</td></tr><tr><th>PVTv1-Tiny [55]</th><td>17.0</td><td>33.2</td><td>35.7</td></tr><tr><th>PVTv2-B1[56]</th><td>17.8</td><td>34.2</td><td>42.5</td></tr><tr><th>EdgeViT-S</th><td>16.9</td><td>32.1</td><td>45.9</td></tr></tbody></table>", "caption": "Table 6: Semantic segmentation results on the validation set of ADE20K.Segmentation model: Semantic FPN [25].GFLOPs: Calculated at 512\\times 512 input size.", "list_citation_info": ["[55] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: IEEE International Conference on Computer Vision (2021)", "[20] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (2016)", "[56] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: PVTv2: Improved baselines with pyramid vision transformer. Computational Visual Media (2022)", "[25] Kirillov, A., Girshick, R., He, K., Doll\u00e1r, P.: Panoptic feature pyramid networks. In: IEEE Conference on Computer Vision and Pattern Recognition (2019)"]}], "citation_info_to_title": {"[22] Howard, A., Pang, R., Adam, H., Le, Q.V., Sandler, M., Chen, B., Wang, W., Chen, L., Tan, M., Chu, G., Vasudevan, V., Zhu, Y.: Searching for MobileNetV3. In: IEEE International Conference on Computer Vision (2019)": "Searching for MobileNetV3", "[52] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning (2021)": "Training data-efficient image transformers & distillation through attention", "[63] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token ViT: Training vision transformers from scratch on ImageNet. In: IEEE International Conference on Computer Vision (2021)": "Tokens-to-token ViT: Training vision transformers from scratch on ImageNet", "[26] Li, K., Wang, Y., Peng, G., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatial-temporal representation learning. In: International Conference on Learning Representations (2022)": "Uniformer: Unified Transformer for Efficient Spatial-Temporal Representation Learning", "[56] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: PVTv2: Improved baselines with pyramid vision transformer. Computational Visual Media (2022)": "PVTv2: Improved Baselines with Pyramid Vision Transformer", "[43] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV2: Inverted residuals and linear bottlenecks. In: IEEE Conference on Computer Vision and Pattern Recognition (2018)": "MobileNetV2: Inverted residuals and linear bottlenecks", "[37] Mehta, S., Rastegari, M.: MobileViT: light-weight, general-purpose, and mobile-friendly vision transformer. International Conference on Learning Representations (2022)": "MobileViT: light-weight, general-purpose, and mobile-friendly vision transformer", "[55] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: IEEE International Conference on Computer Vision (2021)": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions", "[16] Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer. Advances on Neural Information Processing Systems (2021)": "Transformer in transformer", "[20] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (2016)": "Deep Residual Learning for Image Recognition", "[25] Kirillov, A., Girshick, R., He, K., Doll\u00e1r, P.: Panoptic feature pyramid networks. In: IEEE Conference on Computer Vision and Pattern Recognition (2019)": "Panoptic feature pyramid networks", "[15] Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J\u00e9gou, H., Douze, M.: LeViT: a vision transformer in convnet\u2019s clothing for faster inference. In: IEEE International Conference on Computer Vision (2021)": "LeViT: a vision transformer in convnets clothing for faster inference", "[48] Tan, M., Le, Q.: EfficientNet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning (2019)": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "[7] Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. Advances on Neural Information Processing Systems (2021)": "Twins: Revisiting the design of spatial attention in vision transformers"}, "source_title_to_arxiv_id": {"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions": "2102.12122"}}