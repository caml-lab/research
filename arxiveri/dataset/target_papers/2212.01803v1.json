{"title": "Controllable Image Captioning via Prompting", "abstract": "Despite the remarkable progress of image captioning, existing captioners\ntypically lack the controllable capability to generate desired image captions,\ne.g., describing the image in a rough or detailed manner, in a factual or\nemotional view, etc. In this paper, we show that a unified model is qualified\nto perform well in diverse domains and freely switch among multiple styles.\nSuch a controllable capability is achieved by embedding the prompt learning\ninto the image captioning framework. To be specific, we design a set of prompts\nto fine-tune the pre-trained image captioner. These prompts allow the model to\nabsorb stylized data from different domains for joint training, without\nperformance degradation in each domain. Furthermore, we optimize the prompts\nwith learnable vectors in the continuous word embedding space, avoiding the\nheuristic prompt engineering and meanwhile exhibiting superior performance. In\nthe inference stage, our model is able to generate desired stylized captions by\nchoosing the corresponding prompts. Extensive experiments verify the\ncontrollable capability of the proposed method. Notably, we achieve outstanding\nperformance on two diverse image captioning benchmarks including COCO Karpathy\nsplit and TextCaps using a unified model.", "authors": ["Ning Wang", "Jiahao Xie", "Jihao Wu", "Mingbo Jia", "Linlin Li"], "published_date": "2022_12_04", "pdf_url": "http://arxiv.org/pdf/2212.01803v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Configuration</td><td colspan=\"2\">COCO Test</td><td colspan=\"2\">TextCaps Val</td></tr><tr><td>B@4</td><td>C</td><td>B@4</td><td>C</td></tr><tr><td>w/o Fine-tuning (Frozen Model)</td><td></td><td></td><td></td><td></td></tr><tr><td>\u2460 w/o Prompt</td><td>33.9</td><td>106.6</td><td>18.6</td><td>48.6</td></tr><tr><td>\u2461 Manual Prompt (i.e., a picture of)</td><td>23.7</td><td>83.8</td><td>14.5</td><td>38.4</td></tr><tr><td>\u2462 Learned Prompt (N=16)</td><td>38.3</td><td>125.1</td><td>20.7</td><td>56.7</td></tr><tr><td>Multi-dataset Individual Training</td><td></td><td></td><td></td><td></td></tr><tr><td>\u2463 w/o Prompt</td><td>39.1</td><td>132.4</td><td>30.4</td><td>113.4</td></tr><tr><td>\u2464 Manual Prompt (i.e., a picture of)</td><td>39.4</td><td>132.6</td><td>30.1</td><td>111.2</td></tr><tr><td>\u2465 Learned Prompt (N=16)</td><td>40.5</td><td>133.5</td><td>31.2</td><td>115.9</td></tr><tr><td>Multi-dataset Joint Training</td><td></td><td></td><td></td><td></td></tr><tr><td>\u2466 w/o Prompt</td><td>39.2</td><td>131.9</td><td>30.1</td><td>111.6</td></tr><tr><td>\u2467 Shared Manual Prompt (i.e., a picture of)</td><td>39.3</td><td>132.2</td><td>30.0</td><td>110.4</td></tr><tr><td>\u2468 Multi-prompt (Manual Prompts in Table 1)</td><td>39.6</td><td>132.8</td><td>30.7</td><td>113.5</td></tr><tr><td>\u2469 Multi-prompt (Auto Learning, N=16)</td><td>40.5</td><td>133.7</td><td>31.3</td><td>116.7</td></tr></tbody></table>", "caption": "Table 2: Ablation comparisons on the COCO Karpathy test split (Lin et al. 2014) and TextCaps validation set (Sidorov et al. 2020), where B@4 and C denote BLEU@4 and CIDEr scores, respectively.", "list_citation_info": ["Sidorov et al. (2020) Sidorov, O.; Hu, R.; Rohrbach, M.; and Singh, A. 2020. Textcaps: a dataset for image captioning with reading comprehension. In ECCV.", "Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV."]}, {"table": "<table><tbody><tr><th rowspan=\"2\"></th><th colspan=\"2\">N=1</th><th colspan=\"2\">N=4</th><th colspan=\"2\">N=8</th><th colspan=\"2\">N=16</th><th colspan=\"2\">N=24</th></tr><tr><td>B@4</td><td>C</td><td>B@4</td><td>C</td><td>B@4</td><td>C</td><td>B@4</td><td>C</td><td>B@4</td><td>C</td></tr><tr><th>TextCaps Val</th><td>30.7</td><td>114.5</td><td>30.8</td><td>115.4</td><td>31.1</td><td>115.4</td><td>31.3</td><td>116.7</td><td>31.5</td><td>115.9</td></tr></tbody></table>", "caption": "Table 3: Ablation of the prompt embedding length N on the TextCaps validation set (Sidorov et al. 2020).", "list_citation_info": ["Sidorov et al. (2020) Sidorov, O.; Hu, R.; Rohrbach, M.; and Singh, A. 2020. Textcaps: a dataset for image captioning with reading comprehension. In ECCV."]}, {"table": "<table><thead><tr><th>Prompt Style</th><th>B@4</th><th>M</th><th>C</th><th>S</th></tr></thead><tbody><tr><td>Short-length Prompt</td><td>39.9</td><td>30.2</td><td>132.3</td><td>23.0</td></tr><tr><td>Medium-length Prompt</td><td>35.1</td><td>30.9</td><td>122.9</td><td>23.9</td></tr><tr><td>High-length Prompt</td><td>26.9</td><td>30.7</td><td>71.6</td><td>25.0</td></tr><tr><td>Positive Prompt</td><td>27.0</td><td>25.8</td><td>97.6</td><td>20.7</td></tr><tr><td>Negative Prompt</td><td>37.0</td><td>29.3</td><td>121.5</td><td>22.9</td></tr><tr><td>TextCap-style Prompt</td><td>22.1</td><td>25.9</td><td>66.0</td><td>20.5</td></tr><tr><td>COCO-style Prompt</td><td>40.5</td><td>30.9</td><td>133.7</td><td>23.8</td></tr></tbody></table>", "caption": "Table 4: Performance comparisons of different prompts on the COCO Karpathy test split (Lin et al. 2014), where B@4, M, C, S denote BLEU@4, METEOR, CIDEr, and SPICE.", "list_citation_info": ["Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV."]}, {"table": "<table><tbody><tr><th rowspan=\"3\">Method</th><th rowspan=\"3\">Pre-training Data</th><td colspan=\"4\">COCO Caption</td><td colspan=\"8\">NoCaps Validation</td></tr><tr><td colspan=\"4\">Karpathy Test</td><td colspan=\"2\">In-domain</td><td colspan=\"2\">Near-domain</td><td colspan=\"2\">Out-domain</td><td colspan=\"2\">Overall</td></tr><tr><td>B@4</td><td>M</td><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td><td>C</td><td>S</td></tr><tr><th>BUTD (Anderson et al. 2018)</th><th>N/A</th><td>36.2</td><td>27.0</td><td>113.5</td><td>20.3</td><td>80.0</td><td>12.0</td><td>73.6</td><td>11.3</td><td>66.4</td><td>9.7</td><td>73.1</td><td>11.1</td></tr><tr><th>AoANet (Huang et al. 2019)</th><th>N/A</th><td>37.2</td><td>28.4</td><td>119.8</td><td>21.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>X-LAN (Pan et al. 2020)</th><th>N/A</th><td>38.2</td><td>28.8</td><td>122.0</td><td>21.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>\\text{Oscar}_{\\text{base}} (Li et al. 2020)</th><th>7M</th><td>36.5</td><td>30.3</td><td>123.7</td><td>23.1</td><td>83.4</td><td>12.0</td><td>81.6</td><td>12.0</td><td>77.6</td><td>10.6</td><td>81.1</td><td>11.7</td></tr><tr><th>ViTCAP (Fang et al. 2021)</th><th>10M</th><td>36.3</td><td>29.3</td><td>125.2</td><td>22.6</td><td>98.7</td><td>13.3</td><td>92.3</td><td>13.3</td><td>95.4</td><td>12.7</td><td>93.8</td><td>13.0</td></tr><tr><th>\\text{VinVL}_{\\text{base}} (Zhang et al. 2021)</th><th>9M</th><td>38.2</td><td>30.3</td><td>129.3</td><td>23.6</td><td>103.1</td><td>14.2</td><td>96.1</td><td>13.8</td><td>88.3</td><td>12.1</td><td>95.5</td><td>13.5</td></tr><tr><th>\\text{LEMON}_{\\text{base}} (Hu et al. 2021)</th><th>200M</th><td>40.3</td><td>30.2</td><td>133.3</td><td>23.3</td><td>107.7</td><td>14.7</td><td>106.2</td><td>14.3</td><td>107.9</td><td>13.1</td><td>106.8</td><td>14.1</td></tr><tr><th>\\text{BLIP}_{\\text{base}} (Li et al. 2022)</th><th>129M</th><td>39.7</td><td>-</td><td>133.3</td><td>23.3</td><td>111.8</td><td>14.9</td><td>108.6</td><td>14.8</td><td>111.5</td><td>14.2</td><td>109.6</td><td>14.7</td></tr><tr><th>\\text{SimVLM}_{\\text{base}} (Wang et al. 2021b)</th><th>1.8B</th><td>39.0</td><td>32.9</td><td>134.8</td><td>24.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>94.8</td><td>13.1</td></tr><tr><th>ConCap (Ours)</th><th>129M</th><td>40.5</td><td>30.9</td><td>133.7</td><td>23.8</td><td>113.4</td><td>14.9</td><td>108.4</td><td>14.6</td><td>113.2</td><td>14.4</td><td>110.2</td><td>14.8</td></tr></tbody></table>", "caption": "Table 5: Performance comparisons on the COCO Karpathy test split (Lin et al. 2014) and NoCaps validation split (Agrawal et al. 2019), where B@4, M, C, S denote BLEU@4, METEOR, CIDEr, and SPICE scores. For a fair comparison, all the methods only adopt the standard cross-entropy without CIDEr optimization.", "list_citation_info": ["Pan et al. (2020) Pan, Y.; Yao, T.; Li, Y.; and Mei, T. 2020. X-linear attention networks for image captioning. In CVPR.", "Huang et al. (2019) Huang, L.; Wang, W.; Chen, J.; and Wei, X.-Y. 2019. Attention on attention for image captioning. In ICCV.", "Fang et al. (2021) Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2021. Injecting semantic concepts into end-to-end image captioning. arXiv preprint arXiv:2112.05230.", "Wang et al. (2021b) Wang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021b. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.", "Li et al. (2020) Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV.", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.", "Li et al. (2022) Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv preprint arXiv:2201.12086.", "Hu et al. (2021) Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2021. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233.", "Zhang et al. (2021) Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021. Vinvl: Revisiting visual representations in vision-language models. In CVPR.", "Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV.", "Agrawal et al. (2019) Agrawal, H.; Desai, K.; Wang, Y.; Chen, X.; Jain, R.; Johnson, M.; Batra, D.; Parikh, D.; Lee, S.; and Anderson, P. 2019. Nocaps: Novel object captioning at scale. In ICCV."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th>OCR</th><th colspan=\"2\">Validation</th><th colspan=\"2\">Test</th></tr><tr><th>Input</th><th>B@4</th><th>C</th><th>B@4</th><th>C</th></tr></thead><tbody><tr><th>BUTD (Anderson et al. 2018)</th><th>\u2717</th><td>20.1</td><td>41.9</td><td>14.9</td><td>33.8</td></tr><tr><th>AoANet (Huang et al. 2019)</th><th>\u2717</th><td>20.4</td><td>42.7</td><td>15.9</td><td>34.6</td></tr><tr><th>M4C (Hu et al. 2020)</th><th>\u2713</th><td>23.3</td><td>89.6</td><td>18.9</td><td>81.0</td></tr><tr><th>MMA-SR (Wang, Tang, and Luo 2020)</th><th>\u2713</th><td>24.6</td><td>98.0</td><td>19.8</td><td>88.0</td></tr><tr><th>CNMT (Wang et al. 2021a)</th><th>\u2713</th><td>24.8</td><td>101.7</td><td>20.0</td><td>93.0</td></tr><tr><th>TAP (Yang et al. 2021)</th><th>\u2713</th><td>25.8</td><td>109.2</td><td>21.9</td><td>103.2</td></tr><tr><th>ConCap (Ours)</th><th>\u2717</th><td>31.3</td><td>116.7</td><td>27.4</td><td>105.6</td></tr></tbody></table>", "caption": "Table 6: Comparsion results on the TextCaps validation set and test set (Sidorov et al. 2020), where B@4 and C denote BLEU@4 and CIDEr scores, respectively.", "list_citation_info": ["Huang et al. (2019) Huang, L.; Wang, W.; Chen, J.; and Wei, X.-Y. 2019. Attention on attention for image captioning. In ICCV.", "Hu et al. (2020) Hu, R.; Singh, A.; Darrell, T.; and Rohrbach, M. 2020. Iterative answer prediction with pointer-augmented multimodal transformers for textvqa. In CVPR.", "Yang et al. (2021) Yang, Z.; Lu, Y.; Wang, J.; Yin, X.; Florencio, D.; Wang, L.; Zhang, C.; Zhang, L.; and Luo, J. 2021. Tap: Text-aware pre-training for text-vqa and text-caption. In CVPR.", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.", "Wang, Tang, and Luo (2020) Wang, J.; Tang, J.; and Luo, J. 2020. Multimodal attention with image text spatial relationship for ocr-based image captioning. In ACM MM.", "Sidorov et al. (2020) Sidorov, O.; Hu, R.; Rohrbach, M.; and Singh, A. 2020. Textcaps: a dataset for image captioning with reading comprehension. In ECCV.", "Wang et al. (2021a) Wang, Z.; Bao, R.; Wu, Q.; and Liu, S. 2021a. Confidence-aware non-repetitive multimodal transformers for textcaps. In AAAI."]}, {"table": "<table><thead><tr><th rowspan=\"2\">  Method</th><th colspan=\"4\">Validation Set</th><th colspan=\"4\">Test Set</th></tr><tr><th>B@4</th><th>M</th><th>C</th><th>S</th><th>B@4</th><th>M</th><th>C</th><th>S</th></tr></thead><tbody><tr><th>  BUTD (Anderson et al. 2018)</th><td>20.1</td><td>17.8</td><td>41.9</td><td>11.7</td><td>14.9</td><td>15.2</td><td>33.8</td><td>8.8</td></tr><tr><th>  AoANet (Huang et al. 2019)</th><td>20.4</td><td>18.9</td><td>42.7</td><td>13.2</td><td>15.9</td><td>16.6</td><td>34.6</td><td>10.5</td></tr><tr><th>  M4C (Hu et al. 2020)</th><td>23.3</td><td>22.0</td><td>89.6</td><td>15.6</td><td>18.9</td><td>19.8</td><td>81.0</td><td>12.8</td></tr><tr><th>  M4C (GT OCR) (Hu et al. 2020)</th><td>26.0</td><td>23.2</td><td>104.3</td><td>16.2</td><td>21.3</td><td>21.1</td><td>97.2</td><td>13.5</td></tr><tr><th>  MMA-SR (Wang, Tang, and Luo 2020)</th><td>24.6</td><td>23.0</td><td>98.0</td><td>16.2</td><td>19.8</td><td>20.6</td><td>88.0</td><td>13.2</td></tr><tr><th>  CNMT (Wang et al. 2021a)</th><td>24.8</td><td>23.0</td><td>101.7</td><td>16.3</td><td>20.0</td><td>20.8</td><td>93.0</td><td>13.4</td></tr><tr><th>  TAP (Yang et al. 2021)</th><td>25.8</td><td>23.8</td><td>109.2</td><td>17.1</td><td>21.9</td><td>21.8</td><td>103.2</td><td>14.6</td></tr><tr><th>  ConCap (Ours)</th><td>31.3</td><td>26.0</td><td>116.7</td><td>19.6</td><td>27.4</td><td>23.9</td><td>105.6</td><td>17.3</td></tr></tbody></table>", "caption": "Table 10: Comparison results on the TextCaps (Sidorov et al. 2020) validation set and test set.", "list_citation_info": ["Huang et al. (2019) Huang, L.; Wang, W.; Chen, J.; and Wei, X.-Y. 2019. Attention on attention for image captioning. In ICCV.", "Hu et al. (2020) Hu, R.; Singh, A.; Darrell, T.; and Rohrbach, M. 2020. Iterative answer prediction with pointer-augmented multimodal transformers for textvqa. In CVPR.", "Yang et al. (2021) Yang, Z.; Lu, Y.; Wang, J.; Yin, X.; Florencio, D.; Wang, L.; Zhang, C.; Zhang, L.; and Luo, J. 2021. Tap: Text-aware pre-training for text-vqa and text-caption. In CVPR.", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.", "Wang, Tang, and Luo (2020) Wang, J.; Tang, J.; and Luo, J. 2020. Multimodal attention with image text spatial relationship for ocr-based image captioning. In ACM MM.", "Sidorov et al. (2020) Sidorov, O.; Hu, R.; Rohrbach, M.; and Singh, A. 2020. Textcaps: a dataset for image captioning with reading comprehension. In ECCV.", "Wang et al. (2021a) Wang, Z.; Bao, R.; Wu, Q.; and Liu, S. 2021a. Confidence-aware non-repetitive multimodal transformers for textcaps. In AAAI."]}, {"table": "<table><thead><tr><th>  Manual Prompt</th><th>B@4</th><th>M</th><th>C</th><th>S</th></tr></thead><tbody><tr><td>  a picture of</td><td>23.7</td><td>21.2</td><td>83.8</td><td>15.8</td></tr><tr><td>  a photo of</td><td>25.3</td><td>22.0</td><td>88.3</td><td>16.4</td></tr><tr><td>  a picture contains</td><td>27.3</td><td>22.8</td><td>92.5</td><td>18.0</td></tr><tr><td>  a picture with</td><td>22.3</td><td>20.2</td><td>79.2</td><td>15.0</td></tr><tr><td>  a picture that shows</td><td>24.5</td><td>22.3</td><td>88.2</td><td>16.4</td></tr><tr><td>  a beautiful picture that shows</td><td>11.7</td><td>15.9</td><td>58.5</td><td>11.4</td></tr><tr><td>  a terrible picture that shows</td><td>17.8</td><td>19.0</td><td>71.9</td><td>14.1</td></tr><tr><td>  a normal picture that shows</td><td>19.7</td><td>19.7</td><td>76.1</td><td>14.7</td></tr></tbody></table>", "caption": "Table 11: Performance of different manual prompts on the COCO Karpathy test split (Lin et al. 2014).", "list_citation_info": ["Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV."]}], "citation_info_to_title": {"Yang et al. (2021) Yang, Z.; Lu, Y.; Wang, J.; Yin, X.; Florencio, D.; Wang, L.; Zhang, C.; Zhang, L.; and Luo, J. 2021. Tap: Text-aware pre-training for text-vqa and text-caption. In CVPR.": "Tap: Text-aware pre-training for text-vqa and text-caption", "Li et al. (2020) Li, X.; Yin, X.; Li, C.; Zhang, P.; Hu, X.; Zhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F.; et al. 2020. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV.": "Oscar: Object-semantics aligned pre-training for vision-language tasks", "Pan et al. (2020) Pan, Y.; Yao, T.; Li, Y.; and Mei, T. 2020. X-linear attention networks for image captioning. In CVPR.": "X-linear attention networks for image captioning", "Agrawal et al. (2019) Agrawal, H.; Desai, K.; Wang, Y.; Chen, X.; Jain, R.; Johnson, M.; Batra, D.; Parikh, D.; Lee, S.; and Anderson, P. 2019. Nocaps: Novel object captioning at scale. In ICCV.": "Nocaps: Novel Object Captioning at Scale", "Hu et al. (2020) Hu, R.; Singh, A.; Darrell, T.; and Rohrbach, M. 2020. Iterative answer prediction with pointer-augmented multimodal transformers for textvqa. In CVPR.": "Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA", "Zhang et al. (2021) Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J. 2021. Vinvl: Revisiting visual representations in vision-language models. In CVPR.": "Vinvl: Revisiting visual representations in vision-language models", "Wang et al. (2021b) Wang, Z.; Yu, J.; Yu, A. W.; Dai, Z.; Tsvetkov, Y.; and Cao, Y. 2021b. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.": "Simvlm: Simple Visual Language Model Pretraining with Weak Supervision", "Wang et al. (2021a) Wang, Z.; Bao, R.; Wu, Q.; and Liu, S. 2021a. Confidence-aware non-repetitive multimodal transformers for textcaps. In AAAI.": "Confidence-aware non-repetitive multimodal transformers for textcaps", "Anderson et al. (2018) Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould, S.; and Zhang, L. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.": "Bottom-up and top-down attention for image captioning and visual question answering", "Lin et al. (2014) Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Doll\u00e1r, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In ECCV.": "Microsoft COCO: Common Objects in Context", "Hu et al. (2021) Hu, X.; Gan, Z.; Wang, J.; Yang, Z.; Liu, Z.; Lu, Y.; and Wang, L. 2021. Scaling up vision-language pre-training for image captioning. arXiv preprint arXiv:2111.12233.": "Scaling up vision-language pre-training for image captioning", "Wang, Tang, and Luo (2020) Wang, J.; Tang, J.; and Luo, J. 2020. Multimodal attention with image text spatial relationship for ocr-based image captioning. In ACM MM.": "Multimodal Attention with Image Text Spatial Relationship for OCR-based Image Captioning", "Li et al. (2022) Li, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv preprint arXiv:2201.12086.": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "Sidorov et al. (2020) Sidorov, O.; Hu, R.; Rohrbach, M.; and Singh, A. 2020. Textcaps: a dataset for image captioning with reading comprehension. In ECCV.": "Textcaps: A Dataset for Image Captioning with Reading Comprehension", "Huang et al. (2019) Huang, L.; Wang, W.; Chen, J.; and Wei, X.-Y. 2019. Attention on attention for image captioning. In ICCV.": "Attention on attention for image captioning", "Fang et al. (2021) Fang, Z.; Wang, J.; Hu, X.; Liang, L.; Gan, Z.; Wang, L.; Yang, Y.; and Liu, Z. 2021. Injecting semantic concepts into end-to-end image captioning. arXiv preprint arXiv:2112.05230.": "Injecting Semantic Concepts into End-to-End Image Captioning"}, "source_title_to_arxiv_id": {"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation": "2201.12086", "Injecting Semantic Concepts into End-to-End Image Captioning": "2112.05230"}}