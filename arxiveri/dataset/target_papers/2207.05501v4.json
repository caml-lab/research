{"title": "Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios", "abstract": "Due to the complex attention mechanisms and model design, most existing\nvision Transformers (ViTs) can not perform as efficiently as convolutional\nneural networks (CNNs) in realistic industrial deployment scenarios, e.g.\nTensorRT and CoreML. This poses a distinct challenge: Can a visual neural\nnetwork be designed to infer as fast as CNNs and perform as powerful as ViTs?\nRecent works have tried to design CNN-Transformer hybrid architectures to\naddress this issue, yet the overall performance of these works is far away from\nsatisfactory. To end these, we propose a next generation vision Transformer for\nefficient deployment in realistic industrial scenarios, namely Next-ViT, which\ndominates both CNNs and ViTs from the perspective of latency/accuracy\ntrade-off. In this work, the Next Convolution Block (NCB) and Next Transformer\nBlock (NTB) are respectively developed to capture local and global information\nwith deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is\ndesigned to stack NCB and NTB in an efficient hybrid paradigm, which boosts\nperformance in various downstream tasks. Extensive experiments show that\nNext-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer\nhybrid architectures with respect to the latency/accuracy trade-off across\nvarious vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from\n40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K\nsegmentation under similar latency. Meanwhile, it achieves comparable\nperformance with CSWin, while the inference speed is accelerated by 3.6x. On\nCoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on\nCOCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under\nsimilar latency. Our code and models are made public at:\nhttps://github.com/bytedance/Next-ViT", "authors": ["Jiashi Li", "Xin Xia", "Wei Li", "Huixia Li", "Xing Wang", "Xuefeng Xiao", "Rui Wang", "Min Zheng", "Xin Pan"], "published_date": "2022_07_12", "pdf_url": "http://arxiv.org/pdf/2207.05501v4", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td>Image</td><td>Param</td><td>FLOPs</td><td colspan=\"2\">Latency(ms)</td><td>Top-1</td></tr><tr><td>Size</td><td>(M)</td><td>(G)</td><td>TensorRT</td><td>CoreML</td><td>(%)</td></tr><tr><td>ResNet101[10]</td><td>224</td><td>44.6</td><td>7.9</td><td>7.8</td><td>4.0</td><td>80.8</td></tr><tr><td>ResNeXt101-32x4d[40]</td><td>224</td><td>44.2</td><td>8.0</td><td>8.0</td><td>4.0</td><td>78.8</td></tr><tr><td>RegNetY-8G[28]</td><td>224</td><td>44.2</td><td>8.0</td><td>11.4</td><td>4.1</td><td>81.7</td></tr><tr><td>ResNeSt50[45]</td><td>224</td><td>27.5</td><td>5.4</td><td>102.7</td><td>36.6</td><td>81.1</td></tr><tr><td>EfficientNet-B3[32]</td><td>300</td><td>12.0</td><td>1.8</td><td>12.5</td><td>5.8</td><td>81.5</td></tr><tr><td>MobileViTv2-1.0[26]</td><td>256</td><td>4.9</td><td>4.9</td><td>-</td><td>2.9</td><td>78.1</td></tr><tr><td>MobileViTv2-2.0[26]</td><td>256</td><td>18.5</td><td>7.5</td><td>-</td><td>6.7</td><td>81.2</td></tr><tr><td>ConvNeXt-T[22]</td><td>224</td><td>29.0</td><td>4.5</td><td>19.0</td><td>83.8</td><td>82.1</td></tr><tr><td>DeiT-T[33]</td><td>224</td><td>5.9</td><td>1.2</td><td>6.7</td><td>4.5</td><td>72.2</td></tr><tr><td>DeiT-S[33]</td><td>224</td><td>22.0</td><td>4.6</td><td>11.4</td><td>9.0</td><td>79.8</td></tr><tr><td>Swin-T[21]</td><td>224</td><td>29.0</td><td>4.5</td><td>-</td><td>-</td><td>81.3</td></tr><tr><td>PVTv2-B2[35]</td><td>224</td><td>25.4</td><td>4.0</td><td>34.5</td><td>96.7</td><td>82.0</td></tr><tr><td>Twins-SVT-S[3]</td><td>224</td><td>24.0</td><td>2.9</td><td>17.3</td><td>-</td><td>81.7</td></tr><tr><td>PoolFormer-S24[43]</td><td>224</td><td>21.1</td><td>3.4</td><td>14.4</td><td>6.2</td><td>80.3</td></tr><tr><td>PoolFormer-S36[43]</td><td>224</td><td>31.2</td><td>5.0</td><td>21.8</td><td>6.7</td><td>81.4</td></tr><tr><td>CMT-\\text{T}^{*}[8]</td><td>160</td><td>9.5</td><td>0.6</td><td>11.8</td><td>4.6</td><td>79.1</td></tr><tr><td>CMT-\\text{XS}^{*}[8]</td><td>192</td><td>15.2</td><td>1.5</td><td>21.9</td><td>8.3</td><td>81.8</td></tr><tr><td>CoaT Tiny[41]</td><td>224</td><td>5.5</td><td>4.4</td><td>52.9</td><td>55.4</td><td>78.3</td></tr><tr><td>CvT-13[37]</td><td>224</td><td>20.1</td><td>4.5</td><td>18.1</td><td>62.6</td><td>81.6</td></tr><tr><td>Next-ViT-S</td><td>224</td><td>31.7</td><td>5.8</td><td>7.7</td><td>3.5</td><td>82.5</td></tr><tr><td>Next-ViT-S</td><td>384</td><td>31.7</td><td>17.3</td><td>21.6</td><td>8.9</td><td>83.6</td></tr><tr><td>Next-ViT-\\text{S}^{\\dagger}</td><td>224</td><td>31.7</td><td>5.8</td><td>7.7</td><td>3.5</td><td>84.8</td></tr><tr><td>Next-ViT-\\text{S}^{\\dagger}</td><td>384</td><td>31.7</td><td>17.3</td><td>21.6</td><td>8.9</td><td>85.8</td></tr><tr><td>ResNet152[10]</td><td>224</td><td>60.2</td><td>4.0</td><td>11.3</td><td>5.0</td><td>81.7</td></tr><tr><td>ResNeXt101-64x4d[40]</td><td>224</td><td>83.5</td><td>15.6</td><td>13.6</td><td>6.8</td><td>79.6</td></tr><tr><td>ResNeSt101[45]</td><td>224</td><td>48.0</td><td>10.2</td><td>149.8</td><td>45.4</td><td>83.0</td></tr><tr><td>ConvNeXt-S[22]</td><td>224</td><td>50.0</td><td>8.7</td><td>28.1</td><td>159.5</td><td>83.1</td></tr><tr><td>Swin-S[21]</td><td>224</td><td>50.0</td><td>8.7</td><td>-</td><td>-</td><td>83.0</td></tr><tr><td>PVTv2-B3[35]</td><td>224</td><td>45.2</td><td>6.9</td><td>55.8</td><td>107.7</td><td>83.2</td></tr><tr><td>Twins-SVT-B[3]</td><td>224</td><td>56.0</td><td>8.6</td><td>32.0</td><td>-</td><td>83.2</td></tr><tr><td>PoolFormer-M36[43]</td><td>224</td><td>56.1</td><td>8.8</td><td>28.2</td><td>-</td><td>82.1</td></tr><tr><td>CSWin-T[6]</td><td>224</td><td>23.0</td><td>4.3</td><td>29.5</td><td>-</td><td>82.7</td></tr><tr><td>CoaT Mini[41]</td><td>224</td><td>10.0</td><td>6.8</td><td>68.0</td><td>60.8</td><td>81.0</td></tr><tr><td>CvT-21[37]</td><td>224</td><td>32.0</td><td>7.1</td><td>28.0</td><td>91.4</td><td>82.1</td></tr><tr><td>UniFormer-S[17]</td><td>224</td><td>22.1</td><td>3.6</td><td>14.4</td><td>4.6</td><td>82.9</td></tr><tr><td>TRT-ViT-C[38]</td><td>224</td><td>67.3</td><td>5.9</td><td>9.2</td><td>5.0</td><td>82.7</td></tr><tr><td>Next-ViT-B</td><td>224</td><td>44.8</td><td>8.3</td><td>10.5</td><td>4.5</td><td>83.2</td></tr><tr><td>Next-ViT-B</td><td>384</td><td>44.8</td><td>24.6</td><td>29.6</td><td>12.4</td><td>84.3</td></tr><tr><td>Next-ViT-\\text{B}^{\\dagger}</td><td>224</td><td>44.8</td><td>8.3</td><td>10.5</td><td>4.5</td><td>85.1</td></tr><tr><td>Next-ViT-\\text{B}^{\\dagger}</td><td>384</td><td>44.8</td><td>24.6</td><td>29.6</td><td>12.4</td><td>86.1</td></tr><tr><td>RegNetY-16G [28]</td><td>224</td><td>84.0</td><td>16.0</td><td>18.0</td><td>7.4</td><td>82.9</td></tr><tr><td>EfficientNet-B5[32]</td><td>456</td><td>30.0</td><td>9.9</td><td>64.4</td><td>23.2</td><td>83.7</td></tr><tr><td>ConvNeXt-B [22]</td><td>224</td><td>88.0</td><td>15.4</td><td>37.3</td><td>247.6</td><td>83.9</td></tr><tr><td>DeiT-B [33]</td><td>224</td><td>87.0</td><td>17.5</td><td>31.0</td><td>18.2</td><td>81.8</td></tr><tr><td>Swin-B [21]</td><td>224</td><td>88.0</td><td>15.4</td><td>-</td><td>-</td><td>83.3</td></tr><tr><td>PVTv2-B4[35]</td><td>224</td><td>62.6</td><td>10.1</td><td>70.8</td><td>139.8</td><td>83.6</td></tr><tr><td>Twins-SVT-L [3]</td><td>224</td><td>99.2</td><td>15.1</td><td>44.1</td><td>-</td><td>83.7</td></tr><tr><td>PoolFormer-M48[43]</td><td>224</td><td>73.2</td><td>11.6</td><td>38.2</td><td>-</td><td>82.5</td></tr><tr><td>CSWin-S[6]</td><td>224</td><td>35.0</td><td>6.9</td><td>45.0</td><td>-</td><td>83.6</td></tr><tr><td>CMT-\\text{S}^{*}[8]</td><td>224</td><td>25.1</td><td>4.0</td><td>52.0</td><td>14.6</td><td>83.5</td></tr><tr><td>CoaT Small[41]</td><td>224</td><td>22.0</td><td>12.6</td><td>82.7</td><td>122.4</td><td>82.1</td></tr><tr><td>UniFormer-B[17]</td><td>224</td><td>50.2</td><td>8.3</td><td>31.0</td><td>9.0</td><td>83.9</td></tr><tr><td>TRT-ViT-D[38]</td><td>224</td><td>103.0</td><td>9.7</td><td>15.1</td><td>8.3</td><td>83.4</td></tr><tr><td>EfficientFormer-L7[19]</td><td>224</td><td>82.0</td><td>7.9</td><td>17.4</td><td>6.9</td><td>83.3</td></tr><tr><td>Next-ViT-L</td><td>224</td><td>57.8</td><td>10.8</td><td>13.0</td><td>5.5</td><td>83.6</td></tr><tr><td>Next-ViT-L</td><td>384</td><td>57.8</td><td>32.0</td><td>36.0</td><td>15.2</td><td>84.7</td></tr><tr><td>Next-ViT-\\text{L}^{\\dagger}</td><td>224</td><td>57.8</td><td>10.8</td><td>13.0</td><td>5.5</td><td>85.4</td></tr><tr><td>Next-ViT-\\text{L}^{\\dagger}</td><td>384</td><td>57.8</td><td>32.0</td><td>36.0</td><td>15.2</td><td>86.4</td></tr></tbody></table>", "caption": "Table 4: Comparison of different state-of-the-art methods on ImageNet-1K classification. HardSwish is not well supported by CoreML, * denotes we replace it with GELU for fair comparison. \\dagger denotes we use large-scale dataset follow SSLD[4].", "list_citation_info": ["[32] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105\u20136114, 2019.", "[6] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357, 2021.", "[19] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191, 2022.", "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.", "[45] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2736\u20132746, 2022.", "[35] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021.", "[17] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.", "[28] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428\u201310436, 2020.", "[43] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10819\u201310829, 2022.", "[3] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 2021.", "[40] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1492\u20131500, 2017.", "[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[26] Sachin Mehta and Mohammad Rastegari. Separable self-attention for mobile vision transformers. arXiv preprint arXiv:2206.02680, 2022.", "[8] Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Cmt: Convolutional neural networks meet vision transformers. arXiv preprint arXiv:2107.06263, 2021.", "[38] Xin Xia, Jiashi Li, Jie Wu, Xing Wang, Mingkai Wang, Xuefeng Xiao, Min Zheng, and Rui Wang. Trt-vit: Tensorrt-oriented vision transformer. arXiv preprint arXiv:2205.09579, 2022.", "[37] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22\u201331, 2021.", "[4] Cheng Cui, Ruoyu Guo, Yuning Du, Dongliang He, Fu Li, Zewu Wu, Qiwen Liu, Shilei Wen, Jizhou Huang, Xiaoguang Hu, et al. Beyond self-supervision: A simple yet effective network distillation alternative to improve backbones. arXiv preprint arXiv:2103.05959, 2021.", "[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[41] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9981\u20139990, 2021."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Backbone</td><td colspan=\"2\">Latency(ms)</td><td colspan=\"3\">Semantic FPN 80k</td><td colspan=\"3\">UperNet 160k</td></tr><tr><td>TensorRT</td><td>CoreML</td><td>Param(M)</td><td>FLOPs(G)</td><td>mIoU(%)</td><td>Param(M)</td><td>FLOPs(G)</td><td>mIoU/MS mIoU(%)</td></tr><tr><td>ResNet101[10]</td><td>32.8</td><td>13.2</td><td>48.0</td><td>260</td><td>38.8</td><td>96</td><td>1029</td><td>-/44.9</td></tr><tr><td>ResNeXt101-32x4d[40]</td><td>37.0</td><td>15.3</td><td>47.1</td><td>-</td><td>39.7</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>ResNeSt50[45]</td><td>522.3</td><td>372.3</td><td>30.4</td><td>204</td><td>39.7</td><td>68.4</td><td>973</td><td>42.1/-</td></tr><tr><td>ConvNeXt-T[22]</td><td>78.5</td><td>349.4</td><td>-</td><td>-</td><td>-</td><td>60.0</td><td>939</td><td>-/46.7</td></tr><tr><td>Swin-T[21]</td><td>-</td><td>-</td><td>31.9</td><td>182</td><td>41.5</td><td>59.9</td><td>945</td><td>44.5/45.8</td></tr><tr><td>PVTv2-B2[35]</td><td>167.6</td><td>101.0</td><td>29.1</td><td>-</td><td>45.2</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>Twins-SVT-S[3]</td><td>127.2</td><td>-</td><td>28.3</td><td>144</td><td>43.2</td><td>54.4</td><td>901</td><td>46.2/47.1</td></tr><tr><td>PoolFormer-S12[43]</td><td>30.3</td><td>18.2</td><td>15.7</td><td>-</td><td>37.2</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>PoolFormer-S24[43]</td><td>59.1</td><td>23.0</td><td>23.2</td><td>-</td><td>40.3</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>TRT-ViT-\\text{C}^{*}[38]</td><td>40.6</td><td>20.7</td><td>70.6</td><td>213</td><td>46.2</td><td>105.0</td><td>978</td><td>47.6/48.9</td></tr><tr><td>EfficientFormer-L3[19]</td><td>35.9</td><td>10.6</td><td>-</td><td>-</td><td>43.5</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>Next-ViT-S</td><td>38.2</td><td>18.1</td><td>36.3</td><td>208</td><td>46.5</td><td>66.3</td><td>968</td><td>48.1/49.0</td></tr><tr><td>ResNeXt101-64x4d[40]</td><td>65.7</td><td>25.6</td><td>86.4</td><td>-</td><td>40.2</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>ResNeSt101[45]</td><td>798.9</td><td>443.6</td><td>51.2</td><td>305</td><td>42.4</td><td>89.2</td><td>1074</td><td>44.2/-</td></tr><tr><td>ConvNeXt-S[22]</td><td>131.5</td><td>658.0</td><td>-</td><td>-</td><td>-</td><td>82.0</td><td>1027</td><td>-/49.6</td></tr><tr><td>Swin-S[21]</td><td>-</td><td>-</td><td>53.2</td><td>274</td><td>45.2</td><td>81.3</td><td>1038</td><td>47.6/49.5</td></tr><tr><td>PVTv2-B3[35]</td><td>230.9</td><td>114.5</td><td>49.0</td><td>-</td><td>47.3</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>Twins-SVT-B[3]</td><td>231.8</td><td>-</td><td>60.4</td><td>261</td><td>45.3</td><td>88.5</td><td>1020</td><td>47.7/48.9</td></tr><tr><td>PoolFormer-S36[43]</td><td>87.7</td><td>27.7</td><td>34.6</td><td>-</td><td>42.0</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>PoolFormer-M36[43]</td><td>127.8</td><td>32.0</td><td>59.8</td><td>-</td><td>42.4</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>CSWin-T[6]</td><td>182.3</td><td>-</td><td>26.1</td><td>202</td><td>48.2</td><td>59.9</td><td>959</td><td>49.3/50.4</td></tr><tr><td>UniFormer-\\text{S}^{*}[17]</td><td>90.7</td><td>33.5</td><td>25.0</td><td>247</td><td>46.6</td><td>52.0</td><td>1088</td><td>47.6/48.5</td></tr><tr><td>TRT-ViT-\\text{D}^{*}[38]</td><td>58.1</td><td>29.4</td><td>105.9</td><td>296</td><td>46.7</td><td>143.7</td><td>1065</td><td>48.8/49.8</td></tr><tr><td>EfficientFormer-L7[19]</td><td>84.0</td><td>23.0</td><td>-</td><td>-</td><td>45.1</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>Next-ViT-B</td><td>51.6</td><td>24.4</td><td>49.3</td><td>260</td><td>48.6</td><td>79.3</td><td>1020</td><td>50.4/51.1</td></tr><tr><td>ConvNeXt-B[22]</td><td>181.3</td><td>1074.4</td><td>-</td><td>-</td><td>-</td><td>122.0</td><td>1170</td><td>-/49.9</td></tr><tr><td>Swin-B[21]</td><td>-</td><td>-</td><td>91.2</td><td>422</td><td>46.0</td><td>121.0</td><td>1188</td><td>48.1/49.7</td></tr><tr><td>PVTv2-B4[35]</td><td>326.4</td><td>147.6</td><td>66.3</td><td>-</td><td>47.9</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>Twins-SVT-L[3]</td><td>409.7</td><td>-</td><td>103.7</td><td>404</td><td>46.7</td><td>133.0</td><td>1164</td><td>48.8/49.7</td></tr><tr><td>PoolFormer-M48[43]</td><td>168.7</td><td>39.5</td><td>77.1</td><td>-</td><td>42.7</td><td>-</td><td>-</td><td>-/-</td></tr><tr><td>CSWin-S[6]</td><td>298.2</td><td>-</td><td>38.5</td><td>271</td><td>49.2</td><td>64.4</td><td>1027</td><td>50.4/51.5</td></tr><tr><td>UniFormer-\\text{B}^{*}[17]</td><td>195.1</td><td>70.6</td><td>54.0</td><td>471</td><td>48.0</td><td>80.0</td><td>1227</td><td>50.0/50.8</td></tr><tr><td>Next-ViT-L</td><td>65.3</td><td>30.1</td><td>62.4</td><td>331</td><td>49.1</td><td>92.4</td><td>1072</td><td>50.1/50.8</td></tr><tr><td>Swin-\\text{B}^{\\dagger}[21]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>121.0</td><td>1841</td><td>50.0/51.7</td></tr><tr><td>CSWin-\\text{B}^{\\dagger}[21]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>109.2</td><td>1941</td><td>51.8/52.6</td></tr><tr><td>Next-ViT-\\text{S}^{\\dagger}</td><td>38.2</td><td>18.1</td><td>36.3</td><td>208</td><td>48.8</td><td>66.3</td><td>968</td><td>49.8/50.8</td></tr><tr><td>Next-ViT-\\text{B}^{\\dagger}</td><td>51.6</td><td>24.4</td><td>49.3</td><td>260</td><td>50.2</td><td>79.3</td><td>1020</td><td>51.8/52.8</td></tr><tr><td>Next-ViT-\\text{L}^{\\dagger}</td><td>65.3</td><td>30.1</td><td>62.4</td><td>331</td><td>50.5</td><td>92.4</td><td>1072</td><td>51.5/52.0</td></tr></tbody></table>", "caption": "Table 5: Comparison of different backbones on ADE20K semantic segmentation task. FLOPs are measured with the input size of 512\\times2048. \\dagger denotes training Semantic FPN-80K for 80k iteration with a total batch size of 32, which is 2\\times training data iteration compared to regular setting. \\dagger indicates that the model is pre-trained on large-scale dataset.", "list_citation_info": ["[45] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2736\u20132746, 2022.", "[40] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1492\u20131500, 2017.", "[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[35] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021.", "[6] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "[17] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.", "[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[19] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191, 2022.", "[43] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10819\u201310829, 2022.", "[3] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 2021.", "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.", "[38] Xin Xia, Jiashi Li, Jie Wu, Xing Wang, Mingkai Wang, Xuefeng Xiao, Min Zheng, and Rui Wang. Trt-vit: Tensorrt-oriented vision transformer. arXiv preprint arXiv:2205.09579, 2022."]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Backbone</td><td>Param</td><td>FLOPs</td><td colspan=\"2\">Latency(ms)</td><td colspan=\"6\">Mask R-CNN 1\\times</td><td colspan=\"6\">Mask R-CNN 3\\times + MS</td></tr><tr><td>(M)</td><td>(G)</td><td>TensorRT</td><td>CoreML</td><td>AP{}^{b}</td><td>AP{}_{50}^{b}</td><td>AP{}_{75}^{b}</td><td>AP{}^{m}</td><td>AP{}_{50}^{m}</td><td>AP{}_{75}^{m}</td><td>AP{}^{b}</td><td>AP{}_{50}^{b}</td><td>AP{}_{75}^{b}</td><td>AP{}^{m}</td><td>AP{}_{50}^{m}</td><td>AP{}_{75}^{m}</td></tr><tr><td>ResNet101[10]</td><td>63.2</td><td>336</td><td>32.8</td><td>13.2</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td><td>42.8</td><td>63.2</td><td>47.1</td><td>38.5</td><td>60.1</td><td>41.3</td></tr><tr><td>ResNeXt101-32x4d[40]</td><td>62.8</td><td>-</td><td>37.0</td><td>15.3</td><td>41.9</td><td>62.5</td><td>45.9</td><td>37.5</td><td>59.4</td><td>40.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ResNeSt50[45]</td><td>47.4</td><td>400</td><td>522.3</td><td>372.3</td><td>42.6</td><td>-</td><td>-</td><td>38.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ConvNext-T[22]</td><td>-</td><td>262</td><td>78.5</td><td>349.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>46.2</td><td>67.9</td><td>50.8</td><td>41.7</td><td>65.0</td><td>44.9</td></tr><tr><td>Swin-T[21]</td><td>47.8</td><td>264</td><td>-</td><td>-</td><td>42.2</td><td>64.4</td><td>46.2</td><td>39.1</td><td>64.6</td><td>42.0</td><td>46.0</td><td>68.2</td><td>50.2</td><td>41.6</td><td>65.1</td><td>44.8</td></tr><tr><td>PVTv2-B2[35]</td><td>45.0</td><td>-</td><td>167.6</td><td>101.0</td><td>45.3</td><td>67.1</td><td>49.6</td><td>41.2</td><td>64.2</td><td>44.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Twins-SVT-S[3]</td><td>44.0</td><td>228</td><td>127.2</td><td>-</td><td>43.4</td><td>66.0</td><td>47.3</td><td>40.3</td><td>63.2</td><td>43.4</td><td>46.8</td><td>69.2</td><td>51.2</td><td>42.6</td><td>66.3</td><td>45.8</td></tr><tr><td>PoolFormer-S12[43]</td><td>31.6</td><td>-</td><td>30.3</td><td>18.2</td><td>37.3</td><td>59.0</td><td>40.1</td><td>34.6</td><td>55.8</td><td>36.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PoolFormer-S24[43]</td><td>41.0</td><td>-</td><td>59.1</td><td>22.9</td><td>40.1</td><td>62.2</td><td>43.4</td><td>37.0</td><td>59.1</td><td>39.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>TRT-ViT-C[38]</td><td>86.3</td><td>294</td><td>40.6</td><td>20.8</td><td>44.7</td><td>66.9</td><td>48.8</td><td>40.8</td><td>63.9</td><td>44.0</td><td>47.3</td><td>68.8.</td><td>51.9</td><td>42.7</td><td>65.9</td><td>46.0</td></tr><tr><td>EfficientFormer-L3[19]</td><td>-</td><td>-</td><td>35.9</td><td>10.6</td><td>41.4</td><td>63.9</td><td>44.7</td><td>38.1</td><td>61.0</td><td>40.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Next-ViT-S</td><td>51.8</td><td>290</td><td>38.2</td><td>18.1</td><td>45.9</td><td>68.3</td><td>50.7</td><td>41.8</td><td>65.1</td><td>45.1</td><td>48.0</td><td>69.7</td><td>52.8</td><td>43.2</td><td>66.8</td><td>46.7</td></tr><tr><td>ResNeXt101-64x4d[40]</td><td>101.9</td><td>-</td><td>65.7</td><td>25.6</td><td>42.8</td><td>63.8</td><td>47.3</td><td>38.4</td><td>60.6</td><td>41.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>ResNeSt101[45]</td><td>68.1</td><td>499</td><td>798.9</td><td>443.6</td><td>45.2</td><td>-</td><td>-</td><td>40.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Swin-S[21]</td><td>69.1</td><td>354</td><td>-</td><td>-</td><td>44.8</td><td>66.6</td><td>48.9</td><td>40.9</td><td>63.4</td><td>44.2</td><td>48.5</td><td>70.2</td><td>53.5</td><td>43.3</td><td>67.3</td><td>46.6</td></tr><tr><td>PVTv2-B3[35]</td><td>64.9</td><td>-</td><td>230.9</td><td>114.5</td><td>47.0</td><td>68.1</td><td>51.7</td><td>42.5</td><td>65.7</td><td>45.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Twins-SVT-B[3]</td><td>76.3</td><td>340</td><td>231.8</td><td>-</td><td>45.2</td><td>67.6</td><td>49.3</td><td>41.5</td><td>64.5</td><td>44.8</td><td>48.0</td><td>69.5</td><td>52.7</td><td>43.0</td><td>66.8</td><td>46.6</td></tr><tr><td>CSWin-T[6]</td><td>42.0</td><td>279</td><td>182.3</td><td>-</td><td>46.7</td><td>68.6</td><td>51.3</td><td>42.2</td><td>65.6</td><td>45.4</td><td>49.0</td><td>70.7</td><td>53.7</td><td>43.6</td><td>67.9</td><td>46.6</td></tr><tr><td>PoolFormer-S36[43]</td><td>50.5</td><td>-</td><td>87.7</td><td>27.7</td><td>41.0</td><td>63.1</td><td>44.8</td><td>37.7</td><td>60.1</td><td>40.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CMT-S[6]</td><td>30.2</td><td>-</td><td>200.5</td><td>73.4</td><td>44.6</td><td>66.8</td><td>48.9</td><td>40.7</td><td>63.9</td><td>43.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CoaT Mini[41]</td><td>41.6</td><td>-</td><td>509.6</td><td>476.9</td><td>45.1</td><td>-</td><td>-</td><td>40.6</td><td>-</td><td>-</td><td>46.5</td><td>-</td><td>-</td><td>41.8</td><td>-</td><td>-</td></tr><tr><td>UniFormer-S{}_{h14}[17]</td><td>41</td><td>269</td><td>164.0</td><td>-</td><td>45.6</td><td>68.1</td><td>49.7</td><td>41.6</td><td>64.8</td><td>45.0</td><td>48.2</td><td>70.4</td><td>52.5</td><td>43.4</td><td>67.1</td><td>47.0</td></tr><tr><td>TRT-ViT-D[38]</td><td>121.5</td><td>375</td><td>58.1</td><td>29.5</td><td>45.3</td><td>67.9</td><td>49.6</td><td>41.6</td><td>64.7</td><td>44.8</td><td>48.1</td><td>69.3</td><td>52.7</td><td>43.4</td><td>66.7</td><td>46.8</td></tr><tr><td>EfficientFormer-L7[19]</td><td>-</td><td>-</td><td>84.0</td><td>23.0</td><td>42.6</td><td>65.1</td><td>46.1</td><td>39.0</td><td>62.2</td><td>41.7</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Next-ViT-B</td><td>64.9</td><td>340</td><td>51.6</td><td>24.4</td><td>47.2</td><td>69.6</td><td>51.6</td><td>42.8</td><td>66.5</td><td>45.9</td><td>49.5</td><td>71.1</td><td>54.2</td><td>44.4</td><td>68.3</td><td>48.0</td></tr><tr><td>Swin-B[21]</td><td>107</td><td>496</td><td>-</td><td>-</td><td>46.9</td><td>-</td><td>-</td><td>42.3</td><td>-</td><td>-</td><td>48.5</td><td>69.8</td><td>53.2</td><td>43.4</td><td>66.8</td><td>46.9</td></tr><tr><td>PVTv2-B4[35]</td><td>82.2</td><td>-</td><td>326.4</td><td>147.6</td><td>47.5</td><td>68.7</td><td>52.0</td><td>42.7</td><td>66.1</td><td>46.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Twins-SVT-L[3]</td><td>111</td><td>474</td><td>409.7</td><td>-</td><td>45.7</td><td>-</td><td>-</td><td>41.6</td><td>-</td><td>-</td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CSWin-S[6]</td><td>54.0</td><td>342</td><td>298.2</td><td>-</td><td>47.9</td><td>70.1</td><td>52.6</td><td>43.2</td><td>67.1</td><td>46.2</td><td>50.0</td><td>71.3</td><td>54.7</td><td>44.5</td><td>68.4</td><td>47.7</td></tr><tr><td>CoaT Small[41]</td><td>54.0</td><td>-</td><td>601.7</td><td>612.9</td><td>46.5</td><td>-</td><td>-</td><td>41.8</td><td>-</td><td>-</td><td>49.0</td><td>-</td><td>-</td><td>43.7</td><td>-</td><td>-</td></tr><tr><td>UniFormer-B{}_{h14}[17]</td><td>69</td><td>399</td><td>367.7</td><td>-</td><td>47.4</td><td>69.7</td><td>52.1</td><td>43.1</td><td>66.0</td><td>46.5</td><td>50.3</td><td>72.7</td><td>55.3</td><td>44.8</td><td>69.0</td><td>48.3</td></tr><tr><td>Next-ViT-L</td><td>77.9</td><td>391</td><td>65.3</td><td>30.1</td><td>48.0</td><td>69.8</td><td>52.6</td><td>43.2</td><td>67.0</td><td>46.8</td><td>50.2</td><td>71.6</td><td>54.9</td><td>44.8</td><td>68.7</td><td>48.2</td></tr></tbody></table>", "caption": "Table 6: Comparison of different backbones on Mask R-CNN-based object detection and instance segmentation tasks. FLOPs are measured with the inpus size of 800\\times 1280. The superscript b and m denote the box detection and mask instance segmentation.", "list_citation_info": ["[45] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2736\u20132746, 2022.", "[40] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1492\u20131500, 2017.", "[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[35] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021.", "[6] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.", "[17] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.", "[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.", "[19] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191, 2022.", "[43] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10819\u201310829, 2022.", "[41] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9981\u20139990, 2021.", "[3] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 2021.", "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.", "[38] Xin Xia, Jiashi Li, Jie Wu, Xing Wang, Mingkai Wang, Xuefeng Xiao, Min Zheng, and Rui Wang. Trt-vit: Tensorrt-oriented vision transformer. arXiv preprint arXiv:2205.09579, 2022."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Block type</th><th>Latency(ms)</th><th>Cls</th><th>Det</th><th>Seg</th></tr><tr><th>TensorRT</th><th>Acc(%)</th><th>AP{}^{b}</th><th>mIoU(%)</th></tr></thead><tbody><tr><td>BottleNeck Block[10]</td><td>7.8</td><td>81.9</td><td>44.5</td><td>45.5</td></tr><tr><td>ConvNeXt Block[22]</td><td>7.8</td><td>79.6</td><td>41.4</td><td>43.7</td></tr><tr><td>LSA Block[3]</td><td>8.4</td><td>78.2</td><td>38.7</td><td>40.1</td></tr><tr><td>PoolFormer Block[43]</td><td>7.6</td><td>80.9</td><td>42.3</td><td>44.0</td></tr><tr><td>Local MHRA Block[17]</td><td>7.8</td><td>80.5</td><td>42.4</td><td>43.4</td></tr><tr><td>NCB (ours)</td><td>7.7</td><td>82.5</td><td>45.9</td><td>46.5</td></tr></tbody></table>", "caption": "Table 7: Comparison of different convolution blocks.", "list_citation_info": ["[17] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.", "[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[43] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10819\u201310829, 2022.", "[3] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 2021.", "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016."]}], "citation_info_to_title": {"[6] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652, 2021.": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows", "[41] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9981\u20139990, 2021.": "Co-scale conv-attentional image transformers", "[4] Cheng Cui, Ruoyu Guo, Yuning Du, Dongliang He, Fu Li, Zewu Wu, Qiwen Liu, Shilei Wen, Jizhou Huang, Xiaoguang Hu, et al. Beyond self-supervision: A simple yet effective network distillation alternative to improve backbones. arXiv preprint arXiv:2103.05959, 2021.": "Beyond self-supervision: A simple yet effective network distillation alternative to improve backbones", "[38] Xin Xia, Jiashi Li, Jie Wu, Xing Wang, Mingkai Wang, Xuefeng Xiao, Min Zheng, and Rui Wang. Trt-vit: Tensorrt-oriented vision transformer. arXiv preprint arXiv:2205.09579, 2022.": "Trt-vit: Tensorrt-oriented vision transformer", "[37] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22\u201331, 2021.": "Cvt: Introducing convolutions to vision transformers", "[28] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428\u201310436, 2020.": "Designing network design spaces", "[26] Sachin Mehta and Mohammad Rastegari. Separable self-attention for mobile vision transformers. arXiv preprint arXiv:2206.02680, 2022.": "Separable self-attention for mobile vision transformers", "[45] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2736\u20132746, 2022.": "Resnest: Split-attention networks", "[19] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191, 2022.": "Efficientformer: Vision transformers at mobilenet speed", "[22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.": "A convnet for the 2020s", "[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357, 2021.": "Training Data-Efficient Image Transformers & Distillation Through Attention", "[35] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint arXiv:2106.13797, 2021.": "Pvtv2: Improved baselines with pyramid vision transformer", "[3] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint arXiv:2104.13840, 2021.": "Twins: Revisiting the design of spatial attention in vision transformers", "[40] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1492\u20131500, 2017.": "Aggregated residual transformations for deep neural networks", "[43] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10819\u201310829, 2022.": "Metaformer is actually what you need for vision", "[17] Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition. arXiv preprint arXiv:2201.09450, 2022.": "Uniformer: Unifying Convolution and Self-Attention for Visual Recognition", "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.": "Deep residual learning for image recognition", "[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[32] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105\u20136114, 2019.": "Efficientnet: Rethinking model scaling for convolutional neural networks", "[8] Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Cmt: Convolutional neural networks meet vision transformers. arXiv preprint arXiv:2107.06263, 2021.": "Cmt: Convolutional neural networks meet vision transformers"}, "source_title_to_arxiv_id": {"Cvt: Introducing convolutions to vision transformers": "2103.15808", "A convnet for the 2020s": "2201.03545", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}