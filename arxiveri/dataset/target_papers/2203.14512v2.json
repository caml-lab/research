{"title": "Expressive Talking Head Video Encoding in StyleGAN2 Latent-Space", "abstract": "While the recent advances in research on video reenactment have yielded\npromising results, the approaches fall short in capturing the fine, detailed,\nand expressive facial features (e.g., lip-pressing, mouth puckering, mouth\ngaping, and wrinkles) which are crucial in generating realistic animated face\nvideos. To this end, we propose an end-to-end expressive face video encoding\napproach that facilitates data-efficient high-quality video re-synthesis by\noptimizing low-dimensional edits of a single Identity-latent. The approach\nbuilds on StyleGAN2 image inversion and multi-stage non-linear latent-space\nediting to generate videos that are nearly comparable to input videos. While\nexisting StyleGAN latent-based editing techniques focus on simply generating\nplausible edits of static images, we automate the latent-space editing to\ncapture the fine expressive facial deformations in a sequence of frames using\nan encoding that resides in the Style-latent-space (StyleSpace) of StyleGAN2.\nThe encoding thus obtained could be super-imposed on a single Identity-latent\nto facilitate re-enactment of face videos at $1024^2$. The proposed framework\neconomically captures face identity, head-pose, and complex expressive facial\nmotions at fine levels, and thereby bypasses training, person modeling,\ndependence on landmarks/ keypoints, and low-resolution synthesis which tend to\nhamper most re-enactment approaches. The approach is designed with maximum data\nefficiency, where a single $W+$ latent and 35 parameters per frame enable\nhigh-fidelity video rendering. This pipeline can also be used for puppeteering\n(i.e., motion transfer).", "authors": ["Trevine Oorloff", "Yaser Yacoob"], "published_date": "2022_03_28", "pdf_url": "http://arxiv.org/pdf/2203.14512v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><th>res.</th><td>L1 \\downarrow</td><td>LPIPS\\downarrow</td><td>\\mathcal{L}_{ID}\\downarrow</td><td>PSNR\\uparrow</td><td>SSIM\\uparrow</td><td>FID\\downarrow</td><td>FVD\\downarrow</td><td>\\text{FVD}_{\\scaleto{M}{3pt}}\\downarrow</td><td>\\rho_{\\scaleto{AU}{3pt}}\\uparrow</td><td>\\rho_{\\scaleto{GZ}{3pt}}\\uparrow</td><td>\\rho_{\\scaleto{pose}{3pt}}\\uparrow</td></tr><tr><th>Wang et al. [39]*</th><th>512^{2}</th><td>1.95</td><td>0.024</td><td>0.088</td><td>33.85</td><td>0.966</td><td>11.00</td><td>61.2</td><td>9.19</td><td>0.884</td><td>0.972</td><td>0.992</td></tr><tr><th>StyleVid.GAN [15]*</th><th>1024^{2}</th><td>4.04</td><td>0.109</td><td>0.104</td><td>28.76</td><td>0.926</td><td>28.77</td><td>223.3</td><td>24.07</td><td>0.739</td><td>0.884</td><td>0.979</td></tr><tr><th>Ours*</th><th>1024^{2}</th><td>1.97</td><td>0.029</td><td>0.067</td><td>34.03</td><td>0.961</td><td>13.65</td><td>85.7</td><td>14.66</td><td>0.895</td><td>0.956</td><td>0.988</td></tr><tr><th>FOMM [32]</th><th>256^{2}</th><td>3.07</td><td>0.036</td><td>0.174</td><td>31.03</td><td>0.932</td><td>28.73</td><td>140.3</td><td>38.94</td><td>0.723</td><td>0.596</td><td>0.418</td></tr><tr><th>fs-vid2vid [38]</th><th>512^{2}</th><td>5.75</td><td>0.093</td><td>0.158</td><td>25.21</td><td>0.900</td><td>42.36</td><td>359.6</td><td>74.40</td><td>0.542</td><td>0.678</td><td>0.327</td></tr><tr><th>Ours</th><th>1024^{2}</th><td>1.99</td><td>0.030</td><td>0.096</td><td>34.27</td><td>0.961</td><td>15.66</td><td>86.0</td><td>21.88</td><td>0.768</td><td>0.838</td><td>0.841</td></tr><tr><th>Ours (ReStyle)</th><th>1024^{2}</th><td>2.01</td><td>0.030</td><td>0.101</td><td>34.16</td><td>0.959</td><td>17.36</td><td>98.7</td><td>24.52</td><td>0.764</td><td>0.829</td><td>0.839</td></tr><tr><th>Ours \u2013 PTI<sub>ss</sub></th><th>1024^{2}</th><td>2.95</td><td>0.052</td><td>0.124</td><td>31.01</td><td>0.956</td><td>23.86</td><td>140.3</td><td>31.27</td><td>0.719</td><td>0.807</td><td>0.814</td></tr></tbody></table>", "caption": "Table 2: Quantitative comparison of video re-synthesis against baselines. The first section (*) consists of metrics computed for 6 videos received upon requests made to the authors. The second section comprises metrics evaluated against the dataset of 150 videos. The final section consists of the ablations performed, where \u201cOurs (ReStyle)\" refers to the e4e encoder replaced with ReStyle<sub>e4e</sub> and \u201cOurs \u2013 PTI<sub>ss</sub>\" refers to the stage prior to generator fine-tuning. We outperform the SOTA [15] at 1024^{2} on all metrics while using only 0.38% of latent space parameters used by them ", "list_citation_info": ["[39] Wang, T.C., Mallya, A., Liu, M.Y.: One-shot free-view neural talking-head synthesis for video conferencing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10039\u201310049 (2021)", "[38] Wang, T.C., Liu, M.Y., Tao, A., Liu, G., Catanzaro, B., Kautz, J.: Few-shot video-to-video synthesis. In: NeurIPS (2019)", "[32] Siarohin, A., Lathuili\u00e8re, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. Advances in Neural Information Processing Systems 32, 7137\u20137147 (2019)", "[15] Fox, G., Tewari, A., Elgharib, M., Theobalt, C.: Stylevideogan: A temporal generative model using a pretrained stylegan. arXiv preprint arXiv:2107.07224v1 (2021)"]}, {"table": "<table><thead><tr><th>Method</th><th>res.</th><th>\\mathcal{L}_{ID}\\downarrow</th><th>FID\\downarrow</th><th>FVD\\downarrow</th><th>\\text{FVD}_{\\scaleto{M}{3pt}}\\downarrow</th><th>\\rho_{\\scaleto{AU+GZ}{3pt}}\\uparrow</th></tr></thead><tbody><tr><th>FOMM [32]</th><th>256^{2}</th><td>0.153</td><td>77.00</td><td>396.78</td><td>103.04</td><td>0.501</td></tr><tr><th>fs-vid2vid [38]</th><th>512^{2}</th><td>0.202</td><td>73.57</td><td>445.05</td><td>112.65</td><td>0.640</td></tr><tr><th>Ours</th><th>1024^{2}</th><td>0.094</td><td>63.49</td><td>405.52</td><td>82.30</td><td>0.708</td></tr></tbody></table>", "caption": "Table 3: Quantitative comparison of puppeteering against baselines evaluated across 50 puppet-puppeteer pairs. Our approach achieves the best performance across all metrics except FVD while rendering high-resolution re-enactment videos", "list_citation_info": ["[32] Siarohin, A., Lathuili\u00e8re, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. Advances in Neural Information Processing Systems 32, 7137\u20137147 (2019)", "[38] Wang, T.C., Liu, M.Y., Tao, A., Liu, G., Catanzaro, B., Kautz, J.: Few-shot video-to-video synthesis. In: NeurIPS (2019)"]}], "citation_info_to_title": {"[38] Wang, T.C., Liu, M.Y., Tao, A., Liu, G., Catanzaro, B., Kautz, J.: Few-shot video-to-video synthesis. In: NeurIPS (2019)": "Few-shot video-to-video synthesis", "[32] Siarohin, A., Lathuili\u00e8re, S., Tulyakov, S., Ricci, E., Sebe, N.: First order motion model for image animation. Advances in Neural Information Processing Systems 32, 7137\u20137147 (2019)": "First order motion model for image animation", "[15] Fox, G., Tewari, A., Elgharib, M., Theobalt, C.: Stylevideogan: A temporal generative model using a pretrained stylegan. arXiv preprint arXiv:2107.07224v1 (2021)": "Stylevideogan: A Temporal Generative Model Using a Pretrained StyleGAN", "[39] Wang, T.C., Mallya, A., Liu, M.Y.: One-shot free-view neural talking-head synthesis for video conferencing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10039\u201310049 (2021)": "One-shot free-view neural talking-head synthesis for video conferencing"}, "source_title_to_arxiv_id": {"First order motion model for image animation": "2003.00196"}}