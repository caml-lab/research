{"title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions", "abstract": "Compared to the great progress of large-scale vision transformers (ViTs) in\nrecent years, large-scale models based on convolutional neural networks (CNNs)\nare still in an early state. This work presents a new large-scale CNN-based\nfoundation model, termed InternImage, which can obtain the gain from increasing\nparameters and training data like ViTs. Different from the recent CNNs that\nfocus on large dense kernels, InternImage takes deformable convolution as the\ncore operator, so that our model not only has the large effective receptive\nfield required for downstream tasks such as detection and segmentation, but\nalso has the adaptive spatial aggregation conditioned by input and task\ninformation. As a result, the proposed InternImage reduces the strict inductive\nbias of traditional CNNs and makes it possible to learn stronger and more\nrobust patterns with large-scale parameters from massive data like ViTs. The\neffectiveness of our model is proven on challenging benchmarks including\nImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved\na new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming\ncurrent leading CNNs and ViTs. The code will be released at\nhttps://github.com/OpenGVLab/InternImage.", "authors": ["Wenhai Wang", "Jifeng Dai", "Zhe Chen", "Zhenhang Huang", "Zhiqi Li", "Xizhou Zhu", "Xiaowei Hu", "Tong Lu", "Lewei Lu", "Hongsheng Li", "Xiaogang Wang", "Yu Qiao"], "published_date": "2022_11_10", "pdf_url": "http://arxiv.org/pdf/2211.05778v3", "list_table_and_caption": [{"table": "<table><tr><td>method</td><td>type</td><td>scale</td><td>#params</td><td>#FLOPs</td><td>acc (%)</td></tr><tr><td>DeiT-S [58]</td><td>T</td><td>224^{2}</td><td>22G</td><td>5G</td><td>79.9</td></tr><tr><td>PVT-S [10]</td><td>T</td><td>224^{2}</td><td>25M</td><td>4G</td><td>79.8</td></tr><tr><td>Swin-T [2]</td><td>T</td><td>224^{2}</td><td>29M</td><td>5G</td><td>81.3</td></tr><tr><td>CoAtNet-0 [20]</td><td>T</td><td>224^{2}</td><td>25M</td><td>4G</td><td>81.6</td></tr><tr><td>CSwin-T [12]</td><td>T</td><td>224^{2}</td><td>23M</td><td>4G</td><td>82.7</td></tr><tr><td>PVTv2-B2 [11]</td><td>T</td><td>224^{2}</td><td>25M</td><td>4G</td><td>82.0</td></tr><tr><td>DeiT III-S [65]</td><td>T</td><td>224^{2}</td><td>22M</td><td>5G</td><td>81.4</td></tr><tr><td>SwinV2-T/8 [16]</td><td>T</td><td>256^{2}</td><td>28M</td><td>6G</td><td>81.8</td></tr><tr><td>Focal-T [66]</td><td>T</td><td>224^{2}</td><td>29M</td><td>5G</td><td>82.2</td></tr><tr><td>ConvNeXt-T [21]</td><td>C</td><td>224^{2}</td><td>29M</td><td>5G</td><td>82.1</td></tr><tr><td>SLaK-T [29]</td><td>C</td><td>224^{2}</td><td>30M</td><td>5G</td><td>82.5</td></tr><tr><td>HorNet-T [44]</td><td>C</td><td>224^{2}</td><td>23M</td><td>4G</td><td>83.0</td></tr><tr><td>InternImage-T (ours)</td><td>C</td><td>224^{2}</td><td>30M</td><td>5G</td><td>83.5</td></tr><tr><td>PVT-L [10]</td><td>T</td><td>224^{2}</td><td>61M</td><td>10G</td><td>81.7</td></tr><tr><td>Swin-S [2]</td><td>T</td><td>224^{2}</td><td>50M</td><td>9G</td><td>83.0</td></tr><tr><td>CoAtNet-1 [20]</td><td>T</td><td>224^{2}</td><td>42M</td><td>8G</td><td>83.3</td></tr><tr><td>PVTv2-B4 [11]</td><td>T</td><td>224^{2}</td><td>63M</td><td>10G</td><td>83.6</td></tr><tr><td>SwinV2-S/8 [16]</td><td>T</td><td>256^{2}</td><td>50M</td><td>12G</td><td>83.7</td></tr><tr><td>ConvNeXt-S [21]</td><td>C</td><td>224^{2}</td><td>50M</td><td>9G</td><td>83.1</td></tr><tr><td>SLaK-S [29]</td><td>C</td><td>224^{2}</td><td>55M</td><td>10G</td><td>83.8</td></tr><tr><td>HorNet-S [44]</td><td>C</td><td>224^{2}</td><td>50M</td><td>9G</td><td>84.0</td></tr><tr><td>InternImage-S (ours)</td><td>C</td><td>224^{2}</td><td>50M</td><td>8G</td><td>84.2</td></tr><tr><td>DeiT-B [58]</td><td>T</td><td>224^{2}</td><td>87M</td><td>18G</td><td>83.1</td></tr><tr><td>Swin-B [2]</td><td>T</td><td>224^{2}</td><td>88M</td><td>15G</td><td>83.5</td></tr><tr><td>CoAtNet-2 [20]</td><td>T</td><td>224^{2}</td><td>75M</td><td>16G</td><td>84.1</td></tr><tr><td>PVTv2-B5 [11]</td><td>T</td><td>224^{2}</td><td>82M</td><td>12G</td><td>83.8</td></tr><tr><td>DeiT III-B [65]</td><td>T</td><td>224^{2}</td><td>87M</td><td>18G</td><td>83.8</td></tr><tr><td>SwinV2-B/8 [16]</td><td>T</td><td>256^{2}</td><td>88M</td><td>20G</td><td>84.2</td></tr><tr><td>RepLKNet-31B [22]</td><td>C</td><td>224^{2}</td><td>79M</td><td>15G</td><td>83.5</td></tr><tr><td>ConvNeXt-B [21]</td><td>C</td><td>224^{2}</td><td>88M</td><td>15G</td><td>83.8</td></tr><tr><td>SLaK-B [29]</td><td>C</td><td>224^{2}</td><td>95M</td><td>17G</td><td>84.0</td></tr><tr><td>HorNet-B [44]</td><td>C</td><td>224^{2}</td><td>88M</td><td>16G</td><td>84.3</td></tr><tr><td>InternImage-B (ours)</td><td>C</td><td>224^{2}</td><td>97M</td><td>16G</td><td>84.9</td></tr><tr><td>Swin-L{}^{\\ddagger} [2]</td><td>T</td><td>384^{2}</td><td>197M</td><td>104G</td><td>87.3</td></tr><tr><td>CoAtNet-3{}^{\\ddagger} [20]</td><td>T</td><td>384^{2}</td><td>168M</td><td>107G</td><td>87.6</td></tr><tr><td>CoAtNet-4{}^{\\ddagger} [20]</td><td>T</td><td>384^{2}</td><td>275M</td><td>190G</td><td>87.9</td></tr><tr><td>DeiT III-L{}^{\\ddagger} [65]</td><td>T</td><td>384^{2}</td><td>304M</td><td>191G</td><td>87.7</td></tr><tr><td>SwinV2-L/24{}^{\\ddagger} [16]</td><td>T</td><td>384^{2}</td><td>197M</td><td>115G</td><td>87.6</td></tr><tr><td>RepLKNet-31L{}^{\\ddagger} [22]</td><td>C</td><td>384^{2}</td><td>172M</td><td>96G</td><td>86.6</td></tr><tr><td>HorNet-L{}^{\\ddagger} [44]</td><td>C</td><td>384^{2}</td><td>202M</td><td>102G</td><td>87.7</td></tr><tr><td>ConvNeXt-L{}^{\\ddagger} [21]</td><td>C</td><td>384^{2}</td><td>198M</td><td>101G</td><td>87.5</td></tr><tr><td>ConvNeXt-XL{}^{\\ddagger} [21]</td><td>C</td><td>384^{2}</td><td>350M</td><td>179G</td><td>87.8</td></tr><tr><td>InternImage-L{}^{\\ddagger} (ours)</td><td>C</td><td>384^{2}</td><td>223M</td><td>108G</td><td>87.7</td></tr><tr><td>InternImage-XL{}^{\\ddagger} (ours)</td><td>C</td><td>384^{2}</td><td>335M</td><td>163G</td><td>88.0</td></tr><tr><td>ViT-G/14{}^{\\#} [30]</td><td>T</td><td>518^{2}</td><td>1.84B</td><td>5160G</td><td>90.5</td></tr><tr><td>CoAtNet-6{}^{\\#} [20]</td><td>T</td><td>512^{2}</td><td>1.47B</td><td>1521G</td><td>90.5</td></tr><tr><td>CoAtNet-7{}^{\\#} [20]</td><td>T</td><td>512^{2}</td><td>2.44B</td><td>2586G</td><td>90.9</td></tr><tr><td>Florence-CoSwin-H{}^{\\#} [59]</td><td>T</td><td>-</td><td>893M</td><td>-</td><td>90.0</td></tr><tr><td>SwinV2-G{}^{\\#} [16]</td><td>T</td><td>640^{2}</td><td>3.00B</td><td>-</td><td>90.2</td></tr><tr><td>RepLKNet-XL{}^{\\#} [22]</td><td>C</td><td>384^{2}</td><td>335M</td><td>129G</td><td>87.8</td></tr><tr><td>BiT-L-ResNet152x4{}^{\\#} [64]</td><td>C</td><td>480^{2}</td><td>928M</td><td>-</td><td>87.5</td></tr><tr><td>InternImage-H{}^{\\#} (ours)</td><td>C</td><td>224^{2}</td><td>1.08B</td><td>188G</td><td>88.5</td></tr><tr><td>InternImage-H{}^{\\#} (ours)</td><td>C</td><td>640^{2}</td><td>1.08B</td><td>1478G</td><td>89.2</td></tr></table>", "caption": "Table 2: Image classification performance on the ImageNet validation set. \u201ctype\u201d refers to model type, where \u201cT\u201d and \u201cC\u201d denote transformer and CNN, respectively. \u201cscale\u201d is the input scale. \u201cacc\u201d is the top-1 accuracy.\u201c{}^{\\ddagger}\u201d indicates the model is pre-trained on ImageNet-22K [31]. \u201c{}^{\\#}\u201d indicates pretraining on extra large-scale private dataset such as JFT-300M [67], FLD-900M [59], or the joint public dataset in this work.", "list_citation_info": ["[20] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965\u20133977, 2021.", "[11] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. CVMJ, pages 1\u201310, 2022.", "[21] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[29] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022.", "[31] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pages 248\u2013255, 2009.", "[59] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.", "[16] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. Adv. Neural Inform. Process. Syst., pages 12009\u201312019, 2022.", "[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pages 10012\u201310022, 2021.", "[64] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Eur. Conf. Comput. Vis., pages 491\u2013507. Springer, 2020.", "[67] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10687\u201310698, 2020.", "[66] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.", "[22] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11963\u201311975, 2022.", "[30] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12104\u201312113, 2022.", "[65] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.", "[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning., pages 10347\u201310357, 2021.", "[44] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. arXiv preprint arXiv:2207.14284, 2022.", "[10] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Int. Conf. Comput. Vis., pages 568\u2013578, 2021.", "[12] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. IEEE Conf. Comput. Vis. Pattern Recog., pages 12124\u201312134, 2022."]}, {"table": "<table><tr><td rowspan=\"2\">method</td><td rowspan=\"2\">#params</td><td rowspan=\"2\">#FLOPs</td><td colspan=\"6\">Mask R-CNN 1\\times schedule</td><td colspan=\"6\">Mask R-CNN 3\\times+MS schedule</td></tr><tr><td>\\rm AP^{b}</td><td>\\rm AP^{b}_{50}</td><td>\\rm AP^{b}_{75}</td><td>\\rm AP^{m}</td><td>\\rm AP^{m}_{50}</td><td>\\rm AP^{m}_{75}</td><td>\\rm AP^{b}</td><td>\\rm AP^{b}_{50}</td><td>\\rm AP^{b}_{75}</td><td>\\rm AP^{m}</td><td>\\rm AP^{m}_{50}</td><td>\\rm AP^{m}_{75}</td></tr><tr><td>Swin-T [2]</td><td>48M</td><td>267G</td><td>42.7</td><td>65.2</td><td>46.8</td><td>39.3</td><td>62.2</td><td>42.2</td><td>46.0</td><td>68.1</td><td>50.3</td><td>41.6</td><td>65.1</td><td>44.9</td></tr><tr><td>ConvNeXt-T [21]</td><td>48M</td><td>262G</td><td>44.2</td><td>66.6</td><td>48.3</td><td>40.1</td><td>63.3</td><td>42.8</td><td>46.2</td><td>67.9</td><td>50.8</td><td>41.7</td><td>65.0</td><td>44.9</td></tr><tr><td>PVTv2-B2 [11]</td><td>45M</td><td>309G</td><td>45.3</td><td>67.1</td><td>49.6</td><td>41.2</td><td>64.2</td><td>44.4</td><td>47.8</td><td>69.7</td><td>52.6</td><td>43.1</td><td>66.8</td><td>46.7</td></tr><tr><td>ViT-S [9, 68]</td><td>48M</td><td>353G</td><td>44.7</td><td>65.8</td><td>48.3</td><td>39.9</td><td>62.5</td><td>42.8</td><td>48.2</td><td>69.7</td><td>52.5</td><td>42.8</td><td>66.4</td><td>45.9</td></tr><tr><td>InternImage-T (ours)</td><td>49M</td><td>270G</td><td>47.2</td><td>69.0</td><td>52.1</td><td>42.5</td><td>66.1</td><td>45.8</td><td>49.1</td><td>70.3</td><td>54.0</td><td>43.7</td><td>67.3</td><td>47.1</td></tr><tr><td>Swin-S [2]</td><td>69M</td><td>354G</td><td>44.8</td><td>66.6</td><td>48.9</td><td>40.9</td><td>63.4</td><td>44.2</td><td>48.2</td><td>69.8</td><td>52.8</td><td>43.2</td><td>67.0</td><td>46.1</td></tr><tr><td>ConvNeXt-S [21]</td><td>70M</td><td>348G</td><td>45.4</td><td>67.9</td><td>50.0</td><td>41.8</td><td>65.2</td><td>45.1</td><td>47.9</td><td>70.0</td><td>52.7</td><td>42.9</td><td>66.9</td><td>46.2</td></tr><tr><td>PVTv2-B3 [11]</td><td>65M</td><td>397G</td><td>47.0</td><td>68.1</td><td>51.7</td><td>42.5</td><td>65.7</td><td>45.7</td><td>48.4</td><td>69.8</td><td>53.3</td><td>43.2</td><td>66.9</td><td>46.7</td></tr><tr><td>InternImage-S (ours)</td><td>69M</td><td>340G</td><td>47.8</td><td>69.9</td><td>52.8</td><td>43.3</td><td>67.1</td><td>46.7</td><td>49.7</td><td>71.1</td><td>54.5</td><td>44.4</td><td>68.5</td><td>47.8</td></tr><tr><td>Swin-B [2]</td><td>107M</td><td>496G</td><td>46.9</td><td>-</td><td>-</td><td>42.3</td><td>-</td><td>-</td><td>48.6</td><td>70.0</td><td>53.4</td><td>43.3</td><td>67.1</td><td>46.7</td></tr><tr><td>ConvNeXt-B [21]</td><td>108M</td><td>486G</td><td>47.0</td><td>69.4</td><td>51.7</td><td>42.7</td><td>66.3</td><td>46.0</td><td>48.5</td><td>70.1</td><td>53.3</td><td>43.5</td><td>67.1</td><td>46.7</td></tr><tr><td>PVTv2-B5 [11]</td><td>102M</td><td>557G</td><td>47.4</td><td>68.6</td><td>51.9</td><td>42.5</td><td>65.7</td><td>46.0</td><td>48.4</td><td>69.2</td><td>52.9</td><td>42.9</td><td>66.6</td><td>46.2</td></tr><tr><td>ViT-B [9, 68]</td><td>120M</td><td>781G</td><td>47.0</td><td>68.2</td><td>51.4</td><td>41.8</td><td>65.1</td><td>44.9</td><td>49.6</td><td>70.6</td><td>54.0</td><td>43.6</td><td>67.7</td><td>46.9</td></tr><tr><td>InternImage-B (ours)</td><td>115M</td><td>501G</td><td>48.8</td><td>71.0</td><td>53.9</td><td>44.0</td><td>67.8</td><td>47.5</td><td>50.3</td><td>71.4</td><td>55.3</td><td>44.8</td><td>68.7</td><td>48.0</td></tr><tr><td>method</td><td>#param</td><td>#FLOPs</td><td colspan=\"6\">Cascade Mask R-CNN 1\\times schedule</td><td colspan=\"6\">Cascade Mask R-CNN 3\\times+MS schedule</td></tr><tr><td>Swin-L{}^{\\ddagger} [2]</td><td>253M</td><td>1382G</td><td>51.8</td><td>71.0</td><td>56.2</td><td>44.9</td><td>68.4</td><td>48.9</td><td>53.9</td><td>72.4</td><td>58.8</td><td>46.7</td><td>70.1</td><td>50.8</td></tr><tr><td>ConvNeXt-L{}^{\\ddagger} [21]</td><td>255M</td><td>1354G</td><td>53.5</td><td>72.8</td><td>58.3</td><td>46.4</td><td>70.2</td><td>50.2</td><td>54.8</td><td>73.8</td><td>59.8</td><td>47.6</td><td>71.3</td><td>51.7</td></tr><tr><td>RepLKNet-31L{}^{\\ddagger} [22]</td><td>229M</td><td>1321G</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>53.9</td><td>72.5</td><td>58.6</td><td>46.5</td><td>70.0</td><td>50.6</td></tr><tr><td>HorNet-L{}^{\\ddagger} [44]</td><td>259M</td><td>1358G</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>56.0</td><td>-</td><td>-</td><td>48.6</td><td>-</td><td>-</td></tr><tr><td>InternImage-L{}^{\\ddagger} (ours)</td><td>277M</td><td>1399G</td><td>54.9</td><td>73.8</td><td>59.6</td><td>47.7</td><td>71.3</td><td>52.4</td><td>56.0</td><td>74.7</td><td>61.3</td><td>48.4</td><td>72.2</td><td>53.0</td></tr><tr><td>ConvNeXt-XL{}^{\\ddagger} [21]</td><td>407M</td><td>1898G</td><td>53.6</td><td>72.9</td><td>58.5</td><td>46.5</td><td>70.3</td><td>50.5</td><td>55.2</td><td>74.2</td><td>59.9</td><td>47.7</td><td>71.6</td><td>52.2</td></tr><tr><td>InternImage-XL{}^{\\ddagger} (ours)</td><td>387M</td><td>1782G</td><td>55.3</td><td>74.5</td><td>60.2</td><td>48.0</td><td>72.0</td><td>52.4</td><td>56.2</td><td>74.9</td><td>61.7</td><td>48.8</td><td>72.6</td><td>53.8</td></tr></table>", "caption": "Table 3: Object detection and instance segmentation performance on COCO val2017.The FLOPs are measured with 1280\\times800 inputs.AP{}^{\\text{b}} and AP{}^{\\text{m}} represent box AP and mask AP, respectively.\u201cMS\u201d means multi-scale training. ", "list_citation_info": ["[21] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[11] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. CVMJ, pages 1\u201310, 2022.", "[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020.", "[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pages 10012\u201310022, 2021.", "[22] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11963\u201311975, 2022.", "[44] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. arXiv preprint arXiv:2207.14284, 2022."]}, {"table": "<table><tr><td rowspan=\"2\">method</td><td rowspan=\"2\">detector</td><td rowspan=\"2\">#params</td><td colspan=\"2\">\\rm AP^{\\text{b}}</td></tr><tr><td>val2017</td><td>test-dev</td></tr><tr><td>Swin-L{}^{\\ddagger} [2]</td><td>HTC++ [2]</td><td>284M</td><td>58.0</td><td>58.7</td></tr><tr><td>Swin-L [2]</td><td>DyHead [72]</td><td>213M</td><td>56.2</td><td>58.4</td></tr><tr><td>ViT-L{}^{\\ddagger} [9]</td><td>ViT-Adapter [68]</td><td>401M</td><td>60.5</td><td>60.9</td></tr><tr><td>Swin-L{}^{\\ddagger} [2]</td><td>Soft-Teacher [73]</td><td>284M</td><td>60.7</td><td>61.3</td></tr><tr><td>Swin-L{}^{\\ddagger} [2]</td><td>DINO [74]</td><td>218M</td><td>63.2</td><td>63.3</td></tr><tr><td>FocalNet-H{}^{\\ddagger} [75]</td><td>DINO [74]</td><td>746M</td><td>64.2</td><td>64.3</td></tr><tr><td>ViT-Huge [76]</td><td>Group-DETRv2 [76]</td><td>629M</td><td>-</td><td>64.5</td></tr><tr><td>Florence-CoSwin-H{}^{\\#} [59]</td><td>DyHead [72]</td><td>637M</td><td>62.0</td><td>62.4</td></tr><tr><td>SwinV2-G{}^{\\#}[16]</td><td>HTC++ [2]</td><td>3.00B</td><td>62.5</td><td>63.1</td></tr><tr><td>BEiT-3{}^{\\#} [17]</td><td>ViTDet [77]</td><td>1.90B</td><td>-</td><td>63.7</td></tr><tr><td>FD-SwinV2-G{}^{\\#} [26]</td><td>HTC++ [2]</td><td>3.00B</td><td>-</td><td>64.2</td></tr><tr><td>InternImage-XL{}^{\\ddagger} (ours)</td><td>DINO [74]</td><td>602M</td><td>64.2</td><td>64.3</td></tr><tr><td>InternImage-H{}^{\\#} (ours)</td><td>DINO [74]</td><td>2.18B</td><td>65.0</td><td>65.4</td></tr></table>", "caption": "Table 4: Comparison of the state-of-the-art detectors on COCO val2017 and test-dev.", "list_citation_info": ["[17] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.", "[26] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.", "[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020.", "[59] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.", "[16] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. Adv. Neural Inform. Process. Syst., pages 12009\u201312019, 2022.", "[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pages 10012\u201310022, 2021.", "[76] Qiang Chen, Xiaokang Chen, Gang Zeng, and Jingdong Wang. Group detr: Fast training convergence with decoupled one-to-many label assignment. arXiv preprint arXiv:2207.13085, 2022.", "[77] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022.", "[73] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher. In Int. Conf. Comput. Vis., pages 3060\u20133069, 2021.", "[68] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.", "[75] Jianwei Yang, Chunyuan Li, and Jianfeng Gao. Focal modulation networks. arXiv preprint arXiv:2203.11926, 2022.", "[72] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7373\u20137382, 2021.", "[74] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022."]}, {"table": "<table><tr><td rowspan=\"2\">method</td><td>crop</td><td rowspan=\"2\">#params</td><td rowspan=\"2\">#FLOPs</td><td>mIoU</td><td>mIoU</td></tr><tr><td>size</td><td>(SS)</td><td>(MS)</td></tr><tr><td>Swin-T [2]</td><td>512{}^{2}</td><td>60M</td><td>945G</td><td>44.5</td><td>45.8</td></tr><tr><td>ConvNeXt-T [21]</td><td>512{}^{2}</td><td>60M</td><td>939G</td><td>46.0</td><td>46.7</td></tr><tr><td>SLaK-T [29]</td><td>512{}^{2}</td><td>65M</td><td>936G</td><td>47.6</td><td>-</td></tr><tr><td>InternImage-T (ours)</td><td>512{}^{2}</td><td>59M</td><td>944G</td><td>47.9</td><td>48.1</td></tr><tr><td>Swin-S [2]</td><td>512{}^{2}</td><td>81M</td><td>1038G</td><td>47.6</td><td>49.5</td></tr><tr><td>ConvNeXt-S [21]</td><td>512{}^{2}</td><td>82M</td><td>1027G</td><td>48.7</td><td>49.6</td></tr><tr><td>SLaK-S [29]</td><td>512{}^{2}</td><td>91M</td><td>1028G</td><td>49.4</td><td>-</td></tr><tr><td>InternImage-S (ours)</td><td>512{}^{2}</td><td>80M</td><td>1017G</td><td>50.1</td><td>50.9</td></tr><tr><td>Swin-B [2]</td><td>512{}^{2}</td><td>121M</td><td>1188G</td><td>48.1</td><td>49.7</td></tr><tr><td>ConvNeXt-B [21]</td><td>512{}^{2}</td><td>122M</td><td>1170G</td><td>49.1</td><td>49.9</td></tr><tr><td>RepLKNet-31B [22]</td><td>512{}^{2}</td><td>112M</td><td>1170G</td><td>49.9</td><td>50.6</td></tr><tr><td>SLaK-B [29]</td><td>512{}^{2}</td><td>135M</td><td>1172G</td><td>50.2</td><td>-</td></tr><tr><td>InternImage-B (ours)</td><td>512{}^{2}</td><td>128M</td><td>1185G</td><td>50.8</td><td>51.3</td></tr><tr><td>Swin-L{}^{\\ddagger} [2]</td><td>640{}^{2}</td><td>234M</td><td>2468G</td><td>52.1</td><td>53.5</td></tr><tr><td>RepLKNet-31L{}^{\\ddagger} [22]</td><td>640{}^{2}</td><td>207M</td><td>2404G</td><td>52.4</td><td>52.7</td></tr><tr><td>ConvNeXt-L{}^{\\ddagger} [21]</td><td>640{}^{2}</td><td>235M</td><td>2458G</td><td>53.2</td><td>53.7</td></tr><tr><td>ConvNeXt-XL{}^{\\ddagger} [21]</td><td>640{}^{2}</td><td>391M</td><td>3335G</td><td>53.6</td><td>54.0</td></tr><tr><td>InternImage-L{}^{\\ddagger} (ours)</td><td>640{}^{2}</td><td>256M</td><td>2526G</td><td>53.9</td><td>54.1</td></tr><tr><td>InternImage-XL{}^{\\ddagger} (ours)</td><td>640{}^{2}</td><td>368M</td><td>3142G</td><td>55.0</td><td>55.3</td></tr><tr><td>SwinV2-G{}^{\\#} [16]</td><td>896{}^{2}</td><td>3.00B</td><td>-</td><td>-</td><td>59.9</td></tr><tr><td>InternImage-H{}^{\\#} (ours)</td><td>896{}^{2}</td><td>1.12B</td><td>3566G</td><td>59.9</td><td>60.3</td></tr><tr><td>BEiT-3{}^{\\#} [17]</td><td>896{}^{2}</td><td>1.90B</td><td>-</td><td>-</td><td>62.8</td></tr><tr><td>FD-SwinV2-G{}^{\\#} [26]</td><td>896{}^{2}</td><td>3000</td><td>-</td><td>-</td><td>61.3</td></tr><tr><td>InternImage-H{}^{\\#} (ours) +</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mask2Former [80]</td><td rowspan=\"-2\">896{}^{2}</td><td rowspan=\"-2\">1.31B</td><td rowspan=\"-2\">4635G</td><td rowspan=\"-2\">62.5</td><td rowspan=\"-2\">62.9</td></tr></table>", "caption": "Table 5: Semantic segmentation performance on the ADE20K validation set.The FLOPs are measured with 512\\times2048, 640\\times2560, or 896\\times896 inputs according to the crop size.\u201cSS\u201d and \u201cMS\u201d means single-scale and multi-scale testing, respectively.", "list_citation_info": ["[17] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.", "[21] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.", "[26] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.", "[80] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. arXiv preprint arXiv:2112.01527, 2021.", "[29] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022.", "[16] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. Adv. Neural Inform. Process. Syst., pages 12009\u201312019, 2022.", "[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pages 10012\u201310022, 2021.", "[22] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11963\u201311975, 2022."]}], "citation_info_to_title": {"[65] Hugo Touvron, Matthieu Cord, and Herv\u00e9 J\u00e9gou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022.": "Deit iii: Revenge of the vit", "[29] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, and Zhangyang Wang. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022.": "More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity", "[68] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.": "Vision Transformer Adapter for Dense Predictions", "[17] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks", "[20] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965\u20133977, 2021.": "Coatnet: Marrying convolution and attention for all data sizes", "[80] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. arXiv preprint arXiv:2112.01527, 2021.": "Masked-attention mask transformer for universal image segmentation", "[58] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning., pages 10347\u201310357, 2021.": "Training Data-Efficient Image Transformers & Distillation Through Attention", "[21] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.": "A convnet for the 2020s", "[30] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12104\u201312113, 2022.": "Scaling Vision Transformers", "[59] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.": "Florence: A new foundation model for computer vision", "[67] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10687\u201310698, 2020.": "Self-training with Noisy Student Improves ImageNet Classification", "[12] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. IEEE Conf. Comput. Vis. Pattern Recog., pages 12124\u201312134, 2022.": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows", "[77] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022.": "Exploring plain vision transformer backbones for object detection", "[26] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.": "Contrastive learning rivals masked image modeling in fine-tuning via feature distillation", "[76] Qiang Chen, Xiaokang Chen, Gang Zeng, and Jingdong Wang. Group detr: Fast training convergence with decoupled one-to-many label assignment. arXiv preprint arXiv:2207.13085, 2022.": "Group detr: Fast training convergence with decoupled one-to-many label assignment", "[74] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection", "[44] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Nam Lim, and Jiwen Lu. Hornet: Efficient high-order spatial interactions with recursive gated convolutions. arXiv preprint arXiv:2207.14284, 2022.": "Hornet: Efficient high-order spatial interactions with recursive gated convolutions", "[72] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head: Unifying object detection heads with attentions. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7373\u20137382, 2021.": "Dynamic Head: Unifying Object Detection Heads with Attentions", "[2] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis., pages 10012\u201310022, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[11] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. CVMJ, pages 1\u201310, 2022.": "Pvtv2: Improved baselines with pyramid vision transformer", "[16] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. Adv. Neural Inform. Process. Syst., pages 12009\u201312019, 2022.": "Swin Transformer V2: Scaling up Capacity and Resolution", "[64] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In Eur. Conf. Comput. Vis., pages 491\u2013507. Springer, 2020.": "Big transfer (bit): General visual representation learning", "[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2020.": "An image is worth 16x16 words: Transformers for image recognition at scale", "[31] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., pages 248\u2013255, 2009.": "Imagenet: A large-scale hierarchical image database", "[10] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Int. Conf. Comput. Vis., pages 568\u2013578, 2021.": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions", "[73] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end semi-supervised object detection with soft teacher. In Int. Conf. Comput. Vis., pages 3060\u20133069, 2021.": "End-to-end semi-supervised object detection with soft teacher", "[75] Jianwei Yang, Chunyuan Li, and Jianfeng Gao. Focal modulation networks. arXiv preprint arXiv:2203.11926, 2022.": "Focal Modulation Networks", "[66] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.": "Focal self-attention for local-global interactions in vision transformers", "[22] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11963\u201311975, 2022.": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns"}, "source_title_to_arxiv_id": {"A convnet for the 2020s": "2201.03545", "Contrastive learning rivals masked image modeling in fine-tuning via feature distillation": "2205.14141", "Dynamic Head: Unifying Object Detection Heads with Attentions": "2106.08322", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Swin Transformer V2: Scaling up Capacity and Resolution": "2111.09883", "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions": "2102.12122", "End-to-end semi-supervised object detection with soft teacher": "2106.09018"}}