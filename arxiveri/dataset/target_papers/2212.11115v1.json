{"title": "What Makes for Good Tokenizers in Vision Transformer?", "abstract": "The architecture of transformers, which recently witness booming applications\nin vision tasks, has pivoted against the widespread convolutional paradigm.\nRelying on the tokenization process that splits inputs into multiple tokens,\ntransformers are capable of extracting their pairwise relationships using\nself-attention. While being the stemming building block of transformers, what\nmakes for a good tokenizer has not been well understood in computer vision. In\nthis work, we investigate this uncharted problem from an information trade-off\nperspective. In addition to unifying and understanding existing structural\nmodifications, our derivation leads to better design strategies for vision\ntokenizers. The proposed Modulation across Tokens (MoTo) incorporates\ninter-token modeling capability through normalization. Furthermore, a\nregularization objective TokenProp is embraced in the standard training regime.\nThrough extensive experiments on various transformer architectures, we observe\nboth improved performance and intriguing properties of these two plug-and-play\ndesigns with negligible computational overhead. These observations further\nindicate the importance of the commonly-omitted designs of tokenizers in vision\ntransformer.", "authors": ["Shengju Qian", "Yi Zhu", "Wenbo Li", "Mu Li", "Jiaya Jia"], "published_date": "2022_12_21", "pdf_url": "http://arxiv.org/pdf/2212.11115v1", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\"> Architecture</td><td colspan=\"4\">Tokenization</td><td rowspan=\"2\">Params</td><td rowspan=\"2\">GFLOPs</td><td colspan=\"2\">Classification</td><td rowspan=\"2\">Segmentation</td></tr><tr><td>Intra</td><td>Local</td><td>Inter</td><td>Frozen</td><td>Linear [18]</td><td>Supervised</td></tr><tr><td>DeiT-S</td><td>-</td><td>-</td><td>-</td><td>-</td><td>22.1</td><td>4.6</td><td>68.1</td><td>79.8</td><td>44.0</td></tr><tr><td>DeiT-B</td><td>-</td><td>-</td><td>-</td><td>-</td><td>86.6</td><td>17.6</td><td>69.6</td><td>81.8</td><td>45.2</td></tr><tr><td>DeiT-S</td><td></td><td></td><td></td><td>\u2713</td><td>22.1</td><td>4.6</td><td>69.0</td><td>79.4</td><td>42.9</td></tr><tr><td>DeiT-S</td><td>\u2713</td><td></td><td></td><td></td><td>22.3</td><td>4.7</td><td>68.4</td><td>80.5</td><td>44.3</td></tr><tr><td>DeiT-S</td><td>\u2713</td><td>\u2713</td><td></td><td></td><td>22.3</td><td>5.1</td><td>68.5</td><td>80.9</td><td>44.6</td></tr><tr><td>DeiT-S</td><td>\u2713</td><td>\u2713</td><td>\u2713</td><td></td><td>25.7</td><td>6.9</td><td>68.7</td><td>82.0</td><td>45.0</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "TABLE I: Performance on various vision tasks with different tokenizers. \u201cIntra\u201d, \u201cLocal\u201d, and \u201cInter\u201d respectively refer to applying the intra-token, locality, and inter-token refinement strategies. \u201cFrozen\u201d refers to the frozen randomly-initialized tokenization in [18]. ", "list_citation_info": ["[18] X. Chen, S. Xie, and K. He, \u201cAn empirical study of training self-supervised visual transformers,\u201d in ICCV, 2021."]}, {"table": "<table><tr><td rowspan=\"2\"> Model</td><td rowspan=\"2\">Loss Type</td><td colspan=\"2\">Top-1 Accuracy</td></tr><tr><td>Val</td><td>\\Delta</td></tr><tr><td>Baseline</td><td>-</td><td>79.8</td><td>-</td></tr><tr><td>-</td><td>L_{1}</td><td>80.2</td><td>+0.4</td></tr><tr><td>-</td><td>L_{2}</td><td>80.7</td><td>+0.9</td></tr><tr><td>-</td><td>Perceptual Loss [83]</td><td>80.4</td><td>+0.6</td></tr><tr><td>-</td><td>Contextual Loss [84]</td><td>80.8</td><td>+1.0</td></tr><tr><td> </td><td></td><td></td><td></td></tr></table>", "caption": "TABLE VII: Ablations about \\mathcal{L}_{rec}. The baseline architecture uses DeiT-S. Each row denotes result trained with different \\mathcal{L}_{rec}.", "list_citation_info": ["[83] J. Johnson, A. Alahi, and L. Fei-Fei, \u201cPerceptual losses for real-time style transfer and super-resolution,\u201d in ECCV, 2016.", "[84] R. Mechrez, I. Talmi, and L. Zelnik-Manor, \u201cThe contextual loss for image transformation with non-aligned data,\u201d arXiv preprint arXiv:1803.02077, 2018."]}, {"table": "<table><tr><td rowspan=\"2\"> Model Variants</td><td rowspan=\"2\">Optimizer</td><td colspan=\"2\">Top-1 Accuracy</td></tr><tr><td>Val</td><td>\\Delta</td></tr><tr><td>DeiT-S</td><td>AdamW</td><td>79.8</td><td>-</td></tr><tr><td>DeiT-S</td><td>SGD</td><td>76.7</td><td>\\downarrow3.1</td></tr><tr><td>DeiT-S w Frozen [18]</td><td>AdamW</td><td>79.4</td><td>-</td></tr><tr><td>DeiT-S w Frozen [18]</td><td>SGD</td><td>76.0</td><td>\\downarrow3.4</td></tr><tr><td>DeiT{}_{C}-S* [11]</td><td>SGD</td><td>78.2</td><td>\\downarrow1.6</td></tr><tr><td>DeiT-S w TokenProp</td><td>SGD</td><td>78.9</td><td>\\downarrow0.9</td></tr><tr><td>DeiT{}_{C}-S* w TokenProp</td><td>SGD</td><td>79.6</td><td>\\downarrow0.2</td></tr><tr><td> </td><td></td><td></td><td></td></tr></table>", "caption": "TABLE XI: Analysis about the compatibility with optimizers. The sign of \\downarrow shows how much the accuracy is lower than the baseline trained using AdamW. We re-implement DeiT with [11] and [18], as denoted by DeiT{}_{C}* and w Frozen. We highlight top-2 results in bold font.", "list_citation_info": ["[18] X. Chen, S. Xie, and K. He, \u201cAn empirical study of training self-supervised visual transformers,\u201d in ICCV, 2021.", "[11] T. Xiao, M. Singh, E. Mintun, T. Darrell, P. Doll\u00e1r, and R. Girshick, \u201cEarly convolutions help transformers see better,\u201d in NeurIPS, 2021."]}, {"table": "<table><tr><td rowspan=\"2\"> Training Dataset</td><td rowspan=\"2\">Percentage (%)</td><td colspan=\"3\">Val Top-1 Accuracy</td></tr><tr><td>Baseline</td><td>w Ours</td><td>\\Delta</td></tr><tr><td rowspan=\"4\"> ImageNet-1k</td><td>50</td><td>73.7</td><td>75.1</td><td>+1.4%</td></tr><tr><td>40</td><td>71.6</td><td>73.2</td><td>+1.6%</td></tr><tr><td>20</td><td>61.2</td><td>63.9</td><td>+2.7%</td></tr><tr><td>10</td><td>43.5</td><td>46.5</td><td>+2.9%</td></tr><tr><td> </td><td></td><td></td><td></td><td></td></tr></table>", "caption": "TABLE XIII: Performance when the model is trained under the resource-limited setting. The baseline structure uses Swin-T [28] architecture. Each row represents the results trained under a certain portion (percentage) of the original ImageNet-1k training data, for both the baseline and our refined counterpart. Top-1 Accuracy denotes the validation accuracy on original ImageNet validation set.", "list_citation_info": ["[28] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in ICCV, 2021."]}, {"table": "<table><tr><td rowspan=\"2\"> Method</td><td rowspan=\"2\">Backbone</td><td colspan=\"2\">Module</td><td rowspan=\"2\">Pre-trained</td><td rowspan=\"2\">Crop Size</td><td rowspan=\"2\">LR Schedule</td><td rowspan=\"2\">mIoU</td></tr><tr><td>MoTo</td><td>TokenProp</td></tr><tr><td>OCRNet [87]</td><td>HRNet-w48</td><td></td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>150K</td><td>45.7</td></tr><tr><td rowspan=\"5\">UperNet</td><td>DeiT-S</td><td></td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>44.0</td></tr><tr><td>DeiT-S w Frozen [18]</td><td></td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>42.9</td></tr><tr><td>DeiT-S</td><td>\\checkmark</td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>44.5</td></tr><tr><td>DeiT-S</td><td></td><td>\\checkmark</td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>44.3</td></tr><tr><td>DeiT-S</td><td>\\checkmark</td><td>\\checkmark</td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>44.7</td></tr><tr><td rowspan=\"4\">UperNet</td><td>Swin-T</td><td></td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>45.8</td></tr><tr><td>Swin-T</td><td>\\checkmark</td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>46.3</td></tr><tr><td>Swin-T</td><td></td><td>\\checkmark</td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>46.4</td></tr><tr><td>Swin-T</td><td>\\checkmark</td><td>\\checkmark</td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>46.7</td></tr><tr><td rowspan=\"4\">UperNet</td><td>Swin-S</td><td></td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>49.1</td></tr><tr><td>Swin-S</td><td>\\checkmark</td><td></td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>49.4</td></tr><tr><td>Swin-S</td><td></td><td>\\checkmark</td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>49.4</td></tr><tr><td>Swin-S</td><td>\\checkmark</td><td>\\checkmark</td><td>ImageNet-1k</td><td>512 \\times 512</td><td>160K</td><td>49.6</td></tr><tr><td> </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table>", "caption": "TABLE XIV: Downstream performance of semantic segmentation on ADE20K dataset. We modify the tokenizer of DeiT-S and Swin-T with our proposed modules and denote it in bold font. The reported mIoU exploits multi-scale and flip testing.", "list_citation_info": ["[87] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in ECCV, 2020.", "[18] X. Chen, S. Xie, and K. He, \u201cAn empirical study of training self-supervised visual transformers,\u201d in ICCV, 2021."]}], "citation_info_to_title": {"[18] X. Chen, S. Xie, and K. He, \u201cAn empirical study of training self-supervised visual transformers,\u201d in ICCV, 2021.": "An empirical study of training self-supervised visual transformers", "[84] R. Mechrez, I. Talmi, and L. Zelnik-Manor, \u201cThe contextual loss for image transformation with non-aligned data,\u201d arXiv preprint arXiv:1803.02077, 2018.": "The contextual loss for image transformation with non-aligned data", "[11] T. Xiao, M. Singh, E. Mintun, T. Darrell, P. Doll\u00e1r, and R. Girshick, \u201cEarly convolutions help transformers see better,\u201d in NeurIPS, 2021.": "Early convolutions help transformers see better", "[87] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in ECCV, 2020.": "Object-contextual representations for semantic segmentation", "[83] J. Johnson, A. Alahi, and L. Fei-Fei, \u201cPerceptual losses for real-time style transfer and super-resolution,\u201d in ECCV, 2016.": "Perceptual losses for real-time style transfer and super-resolution", "[28] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in ICCV, 2021.": "Swin transformer: Hierarchical vision transformer using shifted windows"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}