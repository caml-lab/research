{"title": "MAP-Gen: An Automated 3D-Box Annotation Flow with Multimodal Attention Point Generator", "abstract": "Manually annotating 3D point clouds is laborious and costly, limiting the\ntraining data preparation for deep learning in real-world object detection.\nWhile a few previous studies tried to automatically generate 3D bounding boxes\nfrom weak labels such as 2D boxes, the quality is sub-optimal compared to human\nannotators. This work proposes a novel autolabeler, called multimodal attention\npoint generator (MAP-Gen), that generates high-quality 3D labels from weak 2D\nboxes. It leverages dense image information to tackle the sparsity issue of 3D\npoint clouds, thus improving label quality. For each 2D pixel, MAP-Gen predicts\nits corresponding 3D coordinates by referencing context points based on their\n2D semantic or geometric relationships. The generated 3D points densify the\noriginal sparse point clouds, followed by an encoder to regress 3D bounding\nboxes. Using MAP-Gen, object detection networks that are weakly supervised by\n2D boxes can achieve 94~99% performance of those fully supervised by 3D\nannotations. It is hopeful this newly proposed MAP-Gen autolabeling flow can\nshed new light on utilizing multimodal information for enriching sparse point\nclouds.", "authors": ["Chang Liu", "Xiaoyan Qian", "Xiaojuan Qi", "Edmund Y. Lam", "Siew-Chong Tan", "Ngai Wong"], "published_date": "2022_03_29", "pdf_url": "http://arxiv.org/pdf/2203.15700v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td rowspan=\"2\">Full Supervision</td><td colspan=\"3\">\\text{AP}_{3D}(IoU=0.7)</td><td colspan=\"3\">\\text{AP}_{BEV}(IoU=0.7)</td></tr><tr><td>Easy</td><td>Moderate</td><td>Hard</td><td>Easy</td><td>Moderate</td><td>Hard</td></tr><tr><th>MV3D[24]</th><td>\u2713</td><td>74.97</td><td>63.63</td><td>54.00</td><td>86.62</td><td>78.93</td><td>69.80</td></tr><tr><th>F-PointNet[14]</th><td>\u2713</td><td>82.19</td><td>69.79</td><td>60.59</td><td>91.17</td><td>84.67</td><td>74.77</td></tr><tr><th>AVOD[25]</th><td>\u2713</td><td>83.07</td><td>71.76</td><td>65.73</td><td>90.99</td><td>84.82</td><td>79.62</td></tr><tr><th>SECOND[42]</th><td>\u2713</td><td>83.34</td><td>72.55</td><td>65.82</td><td>89.39</td><td>83.77</td><td>78.59</td></tr><tr><th>PointPillars[20]</th><td>\u2713</td><td>82.58</td><td>74.31</td><td>68.99</td><td>90.07</td><td>86.56</td><td>82.81</td></tr><tr><th>PointRCNN[43]</th><td>\u2713</td><td>86.96</td><td>75.64</td><td>70.70</td><td>92.13</td><td>87.39</td><td>82.72</td></tr><tr><th>Part-A{}^{2}[44]</th><td>\u2713</td><td>87.81</td><td>78.49</td><td>73.51</td><td>91.70</td><td>87.79</td><td>84.61</td></tr><tr><th>PV-RCNN[30]</th><td>\u2713</td><td>90.25</td><td>81.43</td><td>76.82</td><td>94.98</td><td>90.65</td><td>86.14</td></tr><tr><th>FGR[13]</th><td>(2D box)</td><td>80.26</td><td>68.47</td><td>61.57</td><td>90.64</td><td>82.67</td><td>75.46</td></tr><tr><th>WS3D[7]</th><td>(BEV Centroid)</td><td>80.15</td><td>69.64</td><td>63.71</td><td>90.11</td><td>84.02</td><td>76.97</td></tr><tr><th>WS3D(2021)[18]</th><td>(BEV Centroid)</td><td>80.99</td><td>70.59</td><td>64.23</td><td>90.96</td><td>84.93</td><td>77.96</td></tr><tr><th>MAP-Gen (Ours)</th><td>(2D box)</td><td>81.51</td><td>74.14</td><td>67.55</td><td>90.61</td><td>85.91</td><td>80.58</td></tr></tbody></table>", "caption": "TABLE I: Results on KITTI test set. We train the proposed MAP-Gen with 500 frames data from the KITTI training set. Then the MAP-Gen generates pseudo labels for all the frames in the training set to train another PointRCNN model.", "list_citation_info": ["[42] Y. Yan, Y. Mao, and B. Li, \u201cSecond: Sparsely embedded convolutional detection,\u201d Sensors, vol. 18, no. 10, p. 3337, 2018.", "[43] S. Shi, X. Wang, and H. Li, \u201cPointrcnn: 3d object proposal generation and detection from point cloud,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 770\u2013779.", "[44] S. Shi, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cFrom points to parts: 3d object detection from point cloud with part-aware and part-aggregation network,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.", "[7] Q. Meng, W. Wang, T. Zhou, J. Shen, L. Van Gool, and D. Dai, \u201cWeakly supervised 3d object detection from lidar point cloud,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 515\u2013531.", "[25] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, \u201cJoint 3d proposal generation and object detection from view aggregation,\u201d in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 1\u20138.", "[30] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cPv-rcnn: Point-voxel feature set abstraction for 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10\u2009529\u201310\u2009538.", "[13] Y. Wei, S. Su, J. Lu, and J. Zhou, \u201cFgr: Frustum-aware geometric reasoning for weakly supervised 3d vehicle detection,\u201d arXiv preprint arXiv:2105.07647, 2021.", "[20] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointpillars: Fast encoders for object detection from point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12\u2009697\u201312\u2009705.", "[14] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, \u201cFrustum pointnets for 3d object detection from rgb-d data,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 918\u2013927.", "[24] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, \u201cMulti-view 3d object detection network for autonomous driving,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1907\u20131915.", "[18] Q. Meng, W. Wang, T. Zhou, J. Shen, Y. Jia, and L. Van Gool, \u201cTowards a weakly supervised framework for 3d point cloud object detection and annotation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Method</th><td rowspan=\"2\">Full Supervision</td><td colspan=\"3\">\\text{AP}_{3D}(IoU=0.7)</td></tr><tr><td>Easy</td><td>Moderate</td><td>Hard</td></tr><tr><th>PointRCNN[43]</th><td>\u2713</td><td>88.88</td><td>78.63</td><td>77.38</td></tr><tr><th>WS3D[7]</th><td>(BEV Cent.)</td><td>84.04</td><td>75.10</td><td>73.29</td></tr><tr><th>WS3D(2021)[18]</th><td>(BEV Cent.)</td><td>85.04</td><td>75.94</td><td>74.38</td></tr><tr><th>FGR[13]</th><td>(2D box)</td><td>86.68</td><td>73.55</td><td>67.91</td></tr><tr><th>MAP-Gen (Ours)</th><td>(2D box)</td><td>87.87</td><td>77.98</td><td>76.18</td></tr></tbody></table>", "caption": "TABLE II: KITTI val set results versus the fully supervised PointRCNN and other weakly supervised baselines.", "list_citation_info": ["[18] Q. Meng, W. Wang, T. Zhou, J. Shen, Y. Jia, and L. Van Gool, \u201cTowards a weakly supervised framework for 3d point cloud object detection and annotation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[13] Y. Wei, S. Su, J. Lu, and J. Zhou, \u201cFgr: Frustum-aware geometric reasoning for weakly supervised 3d vehicle detection,\u201d arXiv preprint arXiv:2105.07647, 2021.", "[43] S. Shi, X. Wang, and H. Li, \u201cPointrcnn: 3d object proposal generation and detection from point cloud,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 770\u2013779.", "[7] Q. Meng, W. Wang, T. Zhou, J. Shen, L. Van Gool, and D. Dai, \u201cWeakly supervised 3d object detection from lidar point cloud,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 515\u2013531."]}], "citation_info_to_title": {"[44] S. Shi, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cFrom points to parts: 3d object detection from point cloud with part-aware and part-aggregation network,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.": "From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network", "[18] Q. Meng, W. Wang, T. Zhou, J. Shen, Y. Jia, and L. Van Gool, \u201cTowards a weakly supervised framework for 3d point cloud object detection and annotation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.": "Towards a weakly supervised framework for 3d point cloud object detection and annotation", "[30] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, \u201cPv-rcnn: Point-voxel feature set abstraction for 3d object detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 10\u2009529\u201310\u2009538.": "Pv-rcnn: Point-voxel feature set abstraction for 3d object detection", "[24] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, \u201cMulti-view 3d object detection network for autonomous driving,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1907\u20131915.": "Multi-view 3d object detection network for autonomous driving", "[43] S. Shi, X. Wang, and H. Li, \u201cPointrcnn: 3d object proposal generation and detection from point cloud,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 770\u2013779.": "Pointrcnn: 3d object proposal generation and detection from point cloud", "[20] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, \u201cPointpillars: Fast encoders for object detection from point clouds,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 12\u2009697\u201312\u2009705.": "Pointpillars: Fast encoders for object detection from point clouds", "[25] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, \u201cJoint 3d proposal generation and object detection from view aggregation,\u201d in 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 1\u20138.": "Joint 3d proposal generation and object detection from view aggregation", "[14] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, \u201cFrustum pointnets for 3d object detection from rgb-d data,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 918\u2013927.": "Frustum PointNets for 3D Object Detection from RGB-D Data", "[42] Y. Yan, Y. Mao, and B. Li, \u201cSecond: Sparsely embedded convolutional detection,\u201d Sensors, vol. 18, no. 10, p. 3337, 2018.": "Second: Sparsely embedded convolutional detection", "[7] Q. Meng, W. Wang, T. Zhou, J. Shen, L. Van Gool, and D. Dai, \u201cWeakly supervised 3d object detection from lidar point cloud,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 515\u2013531.": "Weakly supervised 3d object detection from lidar point cloud", "[13] Y. Wei, S. Su, J. Lu, and J. Zhou, \u201cFgr: Frustum-aware geometric reasoning for weakly supervised 3d vehicle detection,\u201d arXiv preprint arXiv:2105.07647, 2021.": "Fgr: Frustum-aware geometric reasoning for weakly supervised 3d vehicle detection"}, "source_title_to_arxiv_id": {"Joint 3d proposal generation and object detection from view aggregation": "1712.02294"}}