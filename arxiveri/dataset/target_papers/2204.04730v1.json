{"title": "Deep Non-rigid Structure-from-Motion: A Sequence-to-Sequence Translation Perspective", "abstract": "Directly regressing the non-rigid shape and camera pose from the individual\n2D frame is ill-suited to the Non-Rigid Structure-from-Motion (NRSfM) problem.\nThis frame-by-frame 3D reconstruction pipeline overlooks the inherent\nspatial-temporal nature of NRSfM, i.e., reconstructing the whole 3D sequence\nfrom the input 2D sequence. In this paper, we propose to model deep NRSfM from\na sequence-to-sequence translation perspective, where the input 2D frame\nsequence is taken as a whole to reconstruct the deforming 3D non-rigid shape\nsequence. First, we apply a shape-motion predictor to estimate the initial\nnon-rigid shape and camera motion from a single frame. Then we propose a\ncontext modeling module to model camera motions and complex non-rigid shapes.\nTo tackle the difficulty in enforcing the global structure constraint within\nthe deep framework, we propose to impose the union-of-subspace structure by\nreplacing the self-expressiveness layer with multi-head attention and delayed\nregularizers, which enables end-to-end batch-wise training. Experimental\nresults across different datasets such as Human3.6M, CMU Mocap and InterHand\nprove the superiority of our framework. The code will be made publicly\navailable", "authors": ["Hui Deng", "Tong Zhang", "Yuchao Dai", "Jiawei Shi", "Yiran Zhong", "Hongdong Li"], "published_date": "2022_04_10", "pdf_url": "http://arxiv.org/pdf/2204.04730v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th>Methods</th><td>S07</td><td>S20</td><td>S23</td><td>S33</td><td>S32</td><td>S38</td><td>S39</td><td>S43</td><td>S93</td><td>Mean</td></tr><tr><th rowspan=\"5\">All</th><th>CSF[19]</th><td>1.231</td><td>1.164</td><td>1.238</td><td>1.156</td><td>1.165</td><td>1.188</td><td>1.172</td><td>1.267</td><td>1.117</td><td>1.189</td></tr><tr><th>URN[13]</th><td>1.504</td><td>1.770</td><td>1.329</td><td>1.205</td><td>1.305</td><td>1.303</td><td>1.550</td><td>1.434</td><td>1.601</td><td>1.445</td></tr><tr><th>CNS[12]</th><td>0.310</td><td>0.217</td><td>0.184</td><td>0.177</td><td>0.249</td><td>0.223</td><td>0.312</td><td>0.266</td><td>0.245</td><td>0.243</td></tr><tr><th>C3DPO[28]</th><td>0.226</td><td>0.235</td><td>0.342</td><td>0.357</td><td>0.354</td><td>0.391</td><td>0.189</td><td>0.351</td><td>0.246</td><td>0.299</td></tr><tr><th>Ours</th><td><p>0.072</p></td><td><p>0.122</p></td><td><p>0.137</p></td><td><p>0.158</p></td><td><p>0.142</p></td><td><p>0.093</p></td><td><p>0.090</p></td><td><p>0.108</p></td><td><p>0.129</p></td><td><p>0.117</p></td></tr><tr><th rowspan=\"4\">Unseen</th><th>DNRSFM</th><td>0.097</td><td>0.219</td><td>0.264</td><td>0.219</td><td>0.209</td><td>0.137</td><td>0.127</td><td>0.223</td><td>0.164</td><td>0.184</td></tr><tr><th>PR-RRN</th><td><p>0.061</p></td><td>0.167</td><td>0.249</td><td>0.254</td><td>0.265</td><td>0.108</td><td><p>0.028</p></td><td><p>0.080</p></td><td>0.242</td><td>0.162</td></tr><tr><th>C3DPO</th><td>0.286</td><td>0.361</td><td>0.413</td><td>0.421</td><td>0.401</td><td>0.263</td><td>0.330</td><td>0.491</td><td>0.325</td><td>0.366</td></tr><tr><th><p>Ours</p></th><td>0.081</td><td><p>0.139</p></td><td><p>0.196</p></td><td><p>0.191</p></td><td><p>0.195</p></td><td><p>0.097</p></td><td>0.089</td><td>0.139</td><td><p>0.151</p></td><td><p>0.142</p></td></tr></tbody></table>", "caption": "Table 1: Results on the long sequences of the CMU motion capture dataset. We follow the comparison in [41]. Ours result surpasses the state-of-the-art on both All and Unseen dataset which is not available during training. DNRSFM and PR-RRN train a model for each subject separately for testing while we train only one model for different subjects. Notice that many methods have significant gaps in performance on the unseen set versus the training set, while our method achieves consistent performance on both datasets", "list_citation_info": ["[28] Novotny, D., Ravi, N., Graham, B., Neverova, N., Vedaldi, A.: C3dpo: Canonical 3d pose networks for non-rigid structure from motion. In: Int. Conf. Comput. Vis. (ICCV). pp. 7688\u20137697 (2019)", "[19] Gotardo, P.F., Martinez, A.M.: Computing smooth time trajectories for camera and deformable shape in structure from motion with occlusion. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 33(10), 2051\u20132065 (2011)", "[13] Cha, G., Lee, M., Oh, S.: Unsupervised 3d reconstruction networks. In: Int. Conf. Comput. Vis. (ICCV). pp. 3849\u20133858 (2019)", "[12] Cha, G., Lee, M., Cho, J., Oh, S.: Reconstruct as far as you can: Consensus of non-rigid reconstruction from feasible regions. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 43(2), 623\u2013637 (2019)", "[41] Zeng, H., Dai, Y., Yu, X., Wang, X., Yang, Y.: Pr-rrn: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion. In: Int. Conf. Comput. Vis. (ICCV). pp. 5600\u20135609 (2021)"]}, {"table": "<table><tbody><tr><th>Methods</th><td>Overall</td><td>Unseen</td></tr><tr><th>C3dpo[28]</th><td>0.371</td><td>0.364</td></tr><tr><th>DNRSFM[23]</th><td>0.201</td><td>0.219</td></tr><tr><th>Ours</th><td><p>0.191</p></td><td><p>0.203</p></td></tr></tbody></table>", "caption": "Table 2: We report the average Normalized Error (NE) as [38] of various methods after training on a mixed-subject(subject 7, 20, 34 and 93) dataset of CMU", "list_citation_info": ["[23] Kong, C., Lucey, S.: Deep non-rigid structure from motion with missing data. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 43(12), 4365\u20134377 (2021)", "[38] Wang, C., Lucey, S.: Paul: Procrustean autoencoder for unsupervised lifting. In: IEEE Conf. Comput. Vis. Pattern Recog. (CVPR). pp. 434\u2013443 (2021)", "[28] Novotny, D., Ravi, N., Graham, B., Neverova, N., Vedaldi, A.: C3dpo: Canonical 3d pose networks for non-rigid structure from motion. In: Int. Conf. Comput. Vis. (ICCV). pp. 7688\u20137697 (2019)"]}, {"table": "<table><tbody><tr><th></th><td colspan=\"2\">GT-H36M</td><td colspan=\"2\">HR-H36M</td><td colspan=\"2\">I26M</td></tr><tr><th>Methods</th><td>MPJPE</td><td>Stress</td><td>MPJPE</td><td>Stress</td><td>MPJPE</td><td>Stress</td></tr><tr><th>PRN[30]</th><td>86.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>PAUL[38]</th><td>88.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>C3dpo[28]</th><td>95.6</td><td>41.5</td><td>110.8</td><td>56.3</td><td>9.8</td><td>6.2</td></tr><tr><th>DNRSfM[23]</th><td>109.9</td><td>35.9</td><td>121.4</td><td>72.4</td><td>13.8</td><td>8.5</td></tr><tr><th>Ours</th><td><p>79.8</p></td><td><p>33.8</p></td><td><p>98.1</p></td><td><p>49.6</p></td><td><p>8.9</p></td><td><p>6.1</p></td></tr></tbody></table>", "caption": "Table 3: Experimental results on the Human3.6M datasets with ground truth 2D keypoint(Marked as GT-H36M) and HRNet detected 2D keypoints(Marked as HR-H36M) and InterHand2.6M(Marked as I26M). We report the mean per joint position error (MPJPE) over the set of test actions. Due to the code of PAUL and PRN are unavailable at the time of testing, we mark the result as \u201d-\u201d ", "list_citation_info": ["[23] Kong, C., Lucey, S.: Deep non-rigid structure from motion with missing data. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 43(12), 4365\u20134377 (2021)", "[38] Wang, C., Lucey, S.: Paul: Procrustean autoencoder for unsupervised lifting. In: IEEE Conf. Comput. Vis. Pattern Recog. (CVPR). pp. 434\u2013443 (2021)", "[28] Novotny, D., Ravi, N., Graham, B., Neverova, N., Vedaldi, A.: C3dpo: Canonical 3d pose networks for non-rigid structure from motion. In: Int. Conf. Comput. Vis. (ICCV). pp. 7688\u20137697 (2019)", "[30] Park, S., Lee, M., Kwak, N.: Procrustean regression networks: Learning 3d structure of non-rigid objects from 2d annotations. In: Eur. Conf. Comput. Vis. (ECCV). pp. 1\u201318 (2020)"]}], "citation_info_to_title": {"[28] Novotny, D., Ravi, N., Graham, B., Neverova, N., Vedaldi, A.: C3dpo: Canonical 3d pose networks for non-rigid structure from motion. In: Int. Conf. Comput. Vis. (ICCV). pp. 7688\u20137697 (2019)": "C3dpo: Canonical 3d pose networks for non-rigid structure from motion", "[13] Cha, G., Lee, M., Oh, S.: Unsupervised 3d reconstruction networks. In: Int. Conf. Comput. Vis. (ICCV). pp. 3849\u20133858 (2019)": "Unsupervised 3D Reconstruction Networks", "[30] Park, S., Lee, M., Kwak, N.: Procrustean regression networks: Learning 3d structure of non-rigid objects from 2d annotations. In: Eur. Conf. Comput. Vis. (ECCV). pp. 1\u201318 (2020)": "Procrustean Regression Networks: Learning 3D Structure of Non-Rigid Objects from 2D Annotations", "[41] Zeng, H., Dai, Y., Yu, X., Wang, X., Yang, Y.: Pr-rrn: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion. In: Int. Conf. Comput. Vis. (ICCV). pp. 5600\u20135609 (2021)": "Pr-rrn: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion", "[19] Gotardo, P.F., Martinez, A.M.: Computing smooth time trajectories for camera and deformable shape in structure from motion with occlusion. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 33(10), 2051\u20132065 (2011)": "Computing smooth time trajectories for camera and deformable shape in structure from motion with occlusion", "[23] Kong, C., Lucey, S.: Deep non-rigid structure from motion with missing data. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 43(12), 4365\u20134377 (2021)": "Deep non-rigid structure from motion with missing data", "[12] Cha, G., Lee, M., Cho, J., Oh, S.: Reconstruct as far as you can: Consensus of non-rigid reconstruction from feasible regions. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 43(2), 623\u2013637 (2019)": "Reconstruct as far as you can: Consensus of non-rigid reconstruction from feasible regions", "[38] Wang, C., Lucey, S.: Paul: Procrustean autoencoder for unsupervised lifting. In: IEEE Conf. Comput. Vis. Pattern Recog. (CVPR). pp. 434\u2013443 (2021)": "Paul: Procrustean autoencoder for unsupervised lifting"}, "source_title_to_arxiv_id": {"Pr-rrn: Pairwise-regularized residual-recursive networks for non-rigid structure-from-motion": "2108.07506"}}