{"title": "ReAct: Temporal Action Detection with Relational Queries", "abstract": "This work aims at advancing temporal action detection (TAD) using an\nencoder-decoder framework with action queries, similar to DETR, which has shown\ngreat success in object detection. However, the framework suffers from several\nproblems if directly applied to TAD: the insufficient exploration of\ninter-query relation in the decoder, the inadequate classification training due\nto a limited number of training samples, and the unreliable classification\nscores at inference. To this end, we first propose a relational attention\nmechanism in the decoder, which guides the attention among queries based on\ntheir relations. Moreover, we propose two losses to facilitate and stabilize\nthe training of action classification. Lastly, we propose to predict the\nlocalization quality of each action query at inference in order to distinguish\nhigh-quality queries. The proposed method, named ReAct, achieves the\nstate-of-the-art performance on THUMOS14, with much lower computational costs\nthan previous methods. Besides, extensive ablation studies are conducted to\nverify the effectiveness of each proposed component. The code is available at\nhttps://github.com/sssste/React.", "authors": ["Dingfeng Shi", "Yujie Zhong", "Qiong Cao", "Jing Zhang", "Lin Ma", "Jia Li", "Dacheng Tao"], "published_date": "2022_07_14", "pdf_url": "http://arxiv.org/pdf/2207.07097v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Type</th><th>Method</th><td>0.3</td><td>0.4</td><td>0.5</td><td>0.6</td><td>0.7</td><td>Avg.</td><td>FLOPs</td></tr><tr><th rowspan=\"9\">Two-stage</th><th>BSN[21]</th><td>53.5</td><td>45.0</td><td>36.9</td><td>28.4</td><td>20.0</td><td>36.8</td><td>3.4</td></tr><tr><th>BMN[19]</th><td>56.0</td><td>47.4</td><td>38.8</td><td>29.7</td><td>20.5</td><td>38.5</td><td>171.0</td></tr><tr><th>G-TAD[38]</th><td>54.5</td><td>47.6</td><td>40.3</td><td>30.8</td><td>23.4</td><td>39.3</td><td>639.8</td></tr><tr><th>TAL[6]</th><td>53.2</td><td>48.5</td><td>42.8</td><td>33.8</td><td>20.8</td><td>39.8</td><td>-</td></tr><tr><th>TCANet[28]</th><td>60.6</td><td>53.2</td><td>44.6</td><td>36.8</td><td>26.7</td><td>44.3</td><td>-</td></tr><tr><th>CSA+BMN[30]</th><td>64.4</td><td>58.0</td><td>49.2</td><td>38.2</td><td>27.8</td><td>47.5</td><td>-</td></tr><tr><th>P-GCN[43]</th><td>63.6</td><td>57.8</td><td>49.1</td><td>-</td><td>-</td><td>-</td><td>4.4</td></tr><tr><th>RTD-Net[31]</th><td>68.3</td><td>62.3</td><td>51.9</td><td>38.8</td><td>23.7</td><td>49.0</td><td>-</td></tr><tr><th>VSGN[45]</th><td>66.7</td><td>60.4</td><td>52.4</td><td>41.0</td><td>30.4</td><td>50.2</td><td>-</td></tr><tr><th></th><th>ContextLoc[48]</th><td>68.3</td><td>63.8</td><td>54.3</td><td>41.8</td><td>26.2</td><td>50.9</td><td>3.1</td></tr><tr><th rowspan=\"6\">One-stage</th><th>SSAD[20]</th><td>43.0</td><td>35.0</td><td>24.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>SSN[41]</th><td>51.9</td><td>41.0</td><td>29.9</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>A2Net[40]</th><td>58.6</td><td>54.1</td><td>45.5</td><td>32.5</td><td>17.2</td><td>41.6</td><td>30.4</td></tr><tr><th>AFSD[18]</th><td>67.3</td><td>62.4</td><td>55.5</td><td>43.7</td><td>31.1</td><td>52.0</td><td>5.1</td></tr><tr><th>TadTr[23]</th><td>62.4</td><td>57.4</td><td>49.2</td><td>37.8</td><td>26.3</td><td>46.6</td><td>0.75</td></tr><tr><th>ReAct</th><td>69.2</td><td>65.0</td><td>57.1</td><td>47.8</td><td>35.6</td><td>55.0</td><td>0.68</td></tr></tbody></table>", "caption": "Table 1: Comparison with the state-of-the-art methods on THUMOS14 dataset. We report the mean Average Precision (mAP) in different thresholds and the floating-point operations (FLOPs, G).", "list_citation_info": ["[45] Zhao, C., Thabet, A.K., Ghanem, B.: Video self-stitching graph network for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13658\u201313667 (2021)", "[43] Zeng, R., Huang, W., Tan, M., Rong, Y., Zhao, P., Huang, J., Gan, C.: Graph convolutional networks for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7094\u20137103 (2019)", "[38] Xu, M., Zhao, C., Rojas, D.S., Thabet, A., Ghanem, B.: G-tad: Sub-graph localization for temporal action detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10156\u201310165 (2020)", "[20] Lin, T., Zhao, X., Shou, Z.: Single shot temporal action detection. In: Proceedings of the 25th ACM international conference on Multimedia. pp. 988\u2013996 (2017)", "[41] Yu, T., Ren, Z., Li, Y., Yan, E., Xu, N., Yuan, J.: Temporal structure mining for weakly supervised action detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5522\u20135531 (2019)", "[23] Liu, X., Wang, Q., Hu, Y., Tang, X., Bai, S., Bai, X.: End-to-end temporal action detection with transformer. arXiv preprint arXiv:2106.10271 (2021)", "[21] Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: Bsn: Boundary sensitive network for temporal action proposal generation. In: Proceedings of the European conference on computer vision (ECCV). pp. 3\u201319 (2018)", "[19] Lin, T., Liu, X., Li, X., Ding, E., Wen, S.: Bmn: Boundary-matching network for temporal action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3889\u20133898 (2019)", "[48] Zhu, Z., Tang, W., Wang, L., Zheng, N., Hua, G.: Enriching local and global contexts for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13516\u201313525 (2021)", "[28] Qing, Z., Su, H., Gan, W., Wang, D., Wu, W., Wang, X., Qiao, Y., Yan, J., Gao, C., Sang, N.: Temporal context aggregation network for temporal action proposal refinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 485\u2013494 (2021)", "[40] Yang, L., Peng, H., Zhang, D., Fu, J., Han, J.: Revisiting anchor mechanisms for temporal action localization. IEEE Transactions on Image Processing 29, 8535\u20138548 (2020)", "[30] Sridhar, D., Quader, N., Muralidharan, S., Li, Y., Dai, P., Lu, J.: Class semantics-based attention for action detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13739\u201313748 (2021)", "[6] Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng, J., Sukthankar, R.: Rethinking the faster r-cnn architecture for temporal action localization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1130\u20131139 (2018)", "[18] Lin, C., Xu, C., Luo, D., Wang, Y., Tai, Y., Wang, C., Li, J., Huang, F., Fu, Y.: Learning salient boundary feature for anchor-free temporal action localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3320\u20133329 (2021)", "[31] Tan, J., Tang, J., Wang, L., Wu, G.: Relaxed transformer decoders for direct action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13526\u201313535 (2021)"]}, {"table": "<table><tbody><tr><th>Type</th><th>Method</th><td>0.5</td><td>0.75</td><td>0.95</td><td>Avg.</td><td>FLOPs(G)</td></tr><tr><th rowspan=\"6\">Actioness</th><th>BSN[21]</th><td>46.5</td><td>30.0</td><td>8.0</td><td>28.2</td><td>-</td></tr><tr><th>SSN[41]</th><td>43.2</td><td>28.7</td><td>5.6</td><td>28.3</td><td>-</td></tr><tr><th>BMN[19]</th><td>50.1</td><td>34.8</td><td>8.3</td><td>33.9</td><td>45.6</td></tr><tr><th>G-TAD[38]</th><td>50.4</td><td>34.6</td><td>9.0</td><td>34.1</td><td>45.7</td></tr><tr><th>BU-TAL[46]</th><td>43.5</td><td>33.9</td><td>9.2</td><td>34.3</td><td>-</td></tr><tr><th>VSGN[45]</th><td>52.3</td><td>35.2</td><td>8.3</td><td>34.7</td><td>-</td></tr><tr><th rowspan=\"4\">Anchor-based</th><th>TAL[6]</th><td>38.2</td><td>18.3</td><td>1.3</td><td>20.2</td><td>-</td></tr><tr><th>PGCN[43]</th><td>48.3</td><td>33.2</td><td>3.3</td><td>31.1</td><td>5.0</td></tr><tr><th>TCANet[28]</th><td>52.3</td><td>36.7</td><td>6.9</td><td>35.5</td><td>-</td></tr><tr><th>AFSD[18]</th><td>52.4</td><td>35.2</td><td>6.5</td><td>34.3</td><td>15.3</td></tr><tr><th rowspan=\"3\">DETR-based</th><th>RTD-Net[31]</th><td>47.2</td><td>30.7</td><td>8.6</td><td>30.8</td><td>-</td></tr><tr><th>TadTr[23]</th><td>49.1</td><td>32.6</td><td>8.5</td><td>32.3</td><td>0.38</td></tr><tr><th>ReAct</th><td>49.6</td><td>33.0</td><td>8.6</td><td>32.6</td><td>0.38</td></tr></tbody></table>", "caption": "Table 2: Comparison with the state-of-the-art methods on ActivityNet-1.3 dataset.", "list_citation_info": ["[41] Yu, T., Ren, Z., Li, Y., Yan, E., Xu, N., Yuan, J.: Temporal structure mining for weakly supervised action detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5522\u20135531 (2019)", "[45] Zhao, C., Thabet, A.K., Ghanem, B.: Video self-stitching graph network for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13658\u201313667 (2021)", "[38] Xu, M., Zhao, C., Rojas, D.S., Thabet, A., Ghanem, B.: G-tad: Sub-graph localization for temporal action detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10156\u201310165 (2020)", "[46] Zhao, P., Xie, L., Ju, C., Zhang, Y., Wang, Y., Tian, Q.: Bottom-up temporal action localization with mutual regularization. In: European Conference on Computer Vision. pp. 539\u2013555. Springer (2020)", "[43] Zeng, R., Huang, W., Tan, M., Rong, Y., Zhao, P., Huang, J., Gan, C.: Graph convolutional networks for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7094\u20137103 (2019)", "[23] Liu, X., Wang, Q., Hu, Y., Tang, X., Bai, S., Bai, X.: End-to-end temporal action detection with transformer. arXiv preprint arXiv:2106.10271 (2021)", "[21] Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: Bsn: Boundary sensitive network for temporal action proposal generation. In: Proceedings of the European conference on computer vision (ECCV). pp. 3\u201319 (2018)", "[19] Lin, T., Liu, X., Li, X., Ding, E., Wen, S.: Bmn: Boundary-matching network for temporal action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3889\u20133898 (2019)", "[28] Qing, Z., Su, H., Gan, W., Wang, D., Wu, W., Wang, X., Qiao, Y., Yan, J., Gao, C., Sang, N.: Temporal context aggregation network for temporal action proposal refinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 485\u2013494 (2021)", "[6] Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng, J., Sukthankar, R.: Rethinking the faster r-cnn architecture for temporal action localization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1130\u20131139 (2018)", "[18] Lin, C., Xu, C., Luo, D., Wang, Y., Tai, Y., Wang, C., Li, J., Huang, F., Fu, Y.: Learning salient boundary feature for anchor-free temporal action localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3320\u20133329 (2021)", "[31] Tan, J., Tang, J., Wang, L., Wu, G.: Relaxed transformer decoders for direct action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13526\u201313535 (2021)"]}], "citation_info_to_title": {"[38] Xu, M., Zhao, C., Rojas, D.S., Thabet, A., Ghanem, B.: G-tad: Sub-graph localization for temporal action detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10156\u201310165 (2020)": "G-tad: Sub-graph localization for temporal action detection", "[18] Lin, C., Xu, C., Luo, D., Wang, Y., Tai, Y., Wang, C., Li, J., Huang, F., Fu, Y.: Learning salient boundary feature for anchor-free temporal action localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3320\u20133329 (2021)": "Learning Salient Boundary Feature for Anchor-Free Temporal Action Localization", "[41] Yu, T., Ren, Z., Li, Y., Yan, E., Xu, N., Yuan, J.: Temporal structure mining for weakly supervised action detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 5522\u20135531 (2019)": "Temporal structure mining for weakly supervised action detection", "[45] Zhao, C., Thabet, A.K., Ghanem, B.: Video self-stitching graph network for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13658\u201313667 (2021)": "Video self-stitching graph network for temporal action localization", "[31] Tan, J., Tang, J., Wang, L., Wu, G.: Relaxed transformer decoders for direct action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13526\u201313535 (2021)": "Relaxed Transformer Decoders for Direct Action Proposal Generation", "[30] Sridhar, D., Quader, N., Muralidharan, S., Li, Y., Dai, P., Lu, J.: Class semantics-based attention for action detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13739\u201313748 (2021)": "Class Semantics-Based Attention for Action Detection", "[20] Lin, T., Zhao, X., Shou, Z.: Single shot temporal action detection. In: Proceedings of the 25th ACM international conference on Multimedia. pp. 988\u2013996 (2017)": "Single shot temporal action detection", "[28] Qing, Z., Su, H., Gan, W., Wang, D., Wu, W., Wang, X., Qiao, Y., Yan, J., Gao, C., Sang, N.: Temporal context aggregation network for temporal action proposal refinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 485\u2013494 (2021)": "Temporal Context Aggregation Network for Temporal Action Proposal Refinement", "[19] Lin, T., Liu, X., Li, X., Ding, E., Wen, S.: Bmn: Boundary-matching network for temporal action proposal generation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3889\u20133898 (2019)": "Bmn: Boundary-matching network for temporal action proposal generation", "[21] Lin, T., Zhao, X., Su, H., Wang, C., Yang, M.: Bsn: Boundary sensitive network for temporal action proposal generation. In: Proceedings of the European conference on computer vision (ECCV). pp. 3\u201319 (2018)": "Bsn: Boundary sensitive network for temporal action proposal generation", "[48] Zhu, Z., Tang, W., Wang, L., Zheng, N., Hua, G.: Enriching local and global contexts for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 13516\u201313525 (2021)": "Enriching local and global contexts for temporal action localization", "[43] Zeng, R., Huang, W., Tan, M., Rong, Y., Zhao, P., Huang, J., Gan, C.: Graph convolutional networks for temporal action localization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 7094\u20137103 (2019)": "Graph Convolutional Networks for Temporal Action Localization", "[40] Yang, L., Peng, H., Zhang, D., Fu, J., Han, J.: Revisiting anchor mechanisms for temporal action localization. IEEE Transactions on Image Processing 29, 8535\u20138548 (2020)": "Revisiting anchor mechanisms for temporal action localization", "[23] Liu, X., Wang, Q., Hu, Y., Tang, X., Bai, S., Bai, X.: End-to-end temporal action detection with transformer. arXiv preprint arXiv:2106.10271 (2021)": "End-to-end temporal action detection with transformer", "[46] Zhao, P., Xie, L., Ju, C., Zhang, Y., Wang, Y., Tian, Q.: Bottom-up temporal action localization with mutual regularization. In: European Conference on Computer Vision. pp. 539\u2013555. Springer (2020)": "Bottom-up temporal action localization with mutual regularization", "[6] Chao, Y.W., Vijayanarasimhan, S., Seybold, B., Ross, D.A., Deng, J., Sukthankar, R.: Rethinking the faster r-cnn architecture for temporal action localization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1130\u20131139 (2018)": "Rethinking the faster r-cnn architecture for temporal action localization"}, "source_title_to_arxiv_id": {"Learning Salient Boundary Feature for Anchor-Free Temporal Action Localization": "2103.13137", "Relaxed Transformer Decoders for Direct Action Proposal Generation": "2102.01894", "End-to-end temporal action detection with transformer": "2106.10271"}}