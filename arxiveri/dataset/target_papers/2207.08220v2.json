{"title": "Fast-MoCo: Boost Momentum-based Contrastive Learning with Combinatorial Patches", "abstract": "Contrastive-based self-supervised learning methods achieved great success in\nrecent years. However, self-supervision requires extremely long training epochs\n(e.g., 800 epochs for MoCo v3) to achieve promising results, which is\nunacceptable for the general academic community and hinders the development of\nthis topic. This work revisits the momentum-based contrastive learning\nframeworks and identifies the inefficiency in which two augmented views\ngenerate only one positive pair. We propose Fast-MoCo - a novel framework that\nutilizes combinatorial patches to construct multiple positive pairs from two\naugmented views, which provides abundant supervision signals that bring\nsignificant acceleration with neglectable extra computational cost. Fast-MoCo\ntrained with 100 epochs achieves 73.5% linear evaluation accuracy, similar to\nMoCo v3 (ResNet-50 backbone) trained with 800 epochs. Extra training (200\nepochs) further improves the result to 75.1%, which is on par with\nstate-of-the-art methods. Experiments on several downstream tasks also confirm\nthe effectiveness of Fast-MoCo.", "authors": ["Yuanzheng Ci", "Chen Lin", "Lei Bai", "Wanli Ouyang"], "published_date": "2022_07_17", "pdf_url": "http://arxiv.org/pdf/2207.08220v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td><p>Method</p></td><td><p>100 ep.</p></td><td><p>200 ep.</p></td><td><p>400 ep.</p></td><td><p>800 ep.</p></td><td><p>1000 ep.</p></td></tr><tr><td><p>\\hlineB2.5SimCLR [6]</p></td><td><p>64.8</p></td><td><p>67.0</p></td><td><p>68.3</p></td><td><p>69.1</p></td><td><p>-</p></td></tr><tr><td><p>MoCo v2 [7]</p></td><td><p>-</p></td><td><p>67.5</p></td><td><p>-</p></td><td><p>71.1</p></td><td><p>-</p></td></tr><tr><td><p>BYOL [15]</p></td><td><p>66.5</p></td><td><p>70.6</p></td><td><p>73.2</p></td><td><p>-</p></td><td><p>74.3</p></td></tr><tr><td><p>SwAV [3]</p></td><td><p>-</p></td><td><p>-</p></td><td><p>70.1</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>BarlowTwins [33]</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>73.2</p></td></tr><tr><td><p>SimSiam [8]</p></td><td><p>68.1</p></td><td><p>70.0</p></td><td><p>70.8</p></td><td><p>71.3</p></td><td><p>-</p></td></tr><tr><td><p>MoCo v3 [9]</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>73.8</p></td><td><p>-</p></td></tr><tr><td><p>NNCLR [11]</p></td><td><p>69.4</p></td><td><p>70.7</p></td><td><p>74.2</p></td><td><p>74.9</p></td><td><p>75.4</p></td></tr><tr><td><p>OBoW [13]</p></td><td><p>-</p></td><td><p>73.8</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td></tr><tr><td>Fast-MoCo</td><td>73.5</td><td>75.1</td><td>75.5</td><td><p>-</p></td><td><p>-</p></td></tr><tr><td><p>SwAV [3] (w/ multi-crop)</p></td><td><p>72.1</p></td><td><p>73.9</p></td><td><p>-</p></td><td><p>75.3</p></td><td><p>-</p></td></tr><tr><td><p>DINO [4] (w/ multi-crop)</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td><p>75.3</p></td><td><p>-</p></td></tr><tr><td><p>NNCLR [11] (w/ multi-crop)</p></td><td><p>-</p></td><td><p>-</p></td><td><p>-</p></td><td>75.6</td><td><p>-</p></td></tr></tbody></table>", "caption": "Table 1: ImageNet-1k linear evaluation results for existing methods and our Fast-MoCo using ResNet-50. Best results are in bold. Fast-MoCo can achieve similar performance as MoCo v3 with only 100 epochs. When trained for 200 epochs, Fast-MoCo performances better than MoCo v3 trained for 800 epochs and is comparable with state-of-the-arts (multi-crop is not used in Fast-MoCo for a fair comparison).", "list_citation_info": ["[3] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems 33, 9912\u20139924 (2020)", "[15] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems 33, 21271\u201321284 (2020)", "[6] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597\u20131607. PMLR (2020)", "[11] Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P., Zisserman, A.: With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9588\u20139597 (2021)", "[8] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)", "[13] Gidaris, S., Bursuc, A., Puy, G., Komodakis, N., Cord, M., Perez, P.: Obow: Online bag-of-visual-words generation for self-supervised learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6830\u20136840 (2021)", "[7] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)", "[4] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9650\u20139660 (2021)", "[33] Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: Self-supervised learning via redundancy reduction. In: International Conference on Machine Learning. pp. 12310\u201312320. PMLR (2021)", "[9] Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9640\u20139649 (2021)"]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"2\">1%</td><td colspan=\"2\">10%</td></tr><tr><td><p>Top-1</p></td><td><p>Top-5</p></td><td><p>Top-1</p></td><td><p>Top-5</p></td></tr><tr><td><p>\\hlineB2.5</p><p>Supervised</p></td><td><p>25.4</p></td><td><p>48.4</p></td><td><p>56.4</p></td><td><p>80.4</p></td></tr><tr><td><p>InstDisc [31]</p></td><td><p>-</p></td><td><p>39.2</p></td><td><p>-</p></td><td><p>77.4</p></td></tr><tr><td><p>PIRL [24]</p></td><td><p>-</p></td><td><p>57.2</p></td><td><p>-</p></td><td><p>83.8</p></td></tr><tr><td><p>SimCLR [6]</p></td><td><p>48.3</p></td><td><p>75.5</p></td><td><p>65.6</p></td><td><p>87.8</p></td></tr><tr><td><p>BYOL [15]</p></td><td><p>53.2</p></td><td><p>78.4</p></td><td><p>68.8</p></td><td><p>89.0</p></td></tr><tr><td><p>Barlow Twins [33]</p></td><td><p>55.0</p></td><td><p>79.2</p></td><td><p>69.7</p></td><td><p>89.3</p></td></tr><tr><td><p>NNCLR [11]</p></td><td><p>56.4</p></td><td><p>80.7</p></td><td><p>69.8</p></td><td><p>89.3</p></td></tr><tr><td>Fast-MoCo</td><td>56.5</td><td>81.1</td><td>70.3</td><td>89.4</td></tr><tr><td><p>SwAV [3] (w/ multi-crop)</p></td><td><p>53.9</p></td><td><p>78.5</p></td><td><p>70.2</p></td><td><p>89.9</p></td></tr></tbody></table>", "caption": "Table 2: Semi-supervised learning results on ImageNet-1K with ResNet-50 backbone. We report Top-1 and Top-5 accuracies for models finetuned with 1\\% and 10\\% labeled data. Detailed configuration can be found in supplementary material.", "list_citation_info": ["[3] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems 33, 9912\u20139924 (2020)", "[15] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems 33, 21271\u201321284 (2020)", "[6] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597\u20131607. PMLR (2020)", "[11] Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P., Zisserman, A.: With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9588\u20139597 (2021)", "[24] Misra, I., Maaten, L.v.d.: Self-supervised learning of pretext-invariant representations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6707\u20136717 (2020)", "[33] Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: Self-supervised learning via redundancy reduction. In: International Conference on Machine Learning. pp. 12310\u201312320. PMLR (2021)", "[31] Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-parametric instance discrimination. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3733\u20133742 (2018)"]}, {"table": "<table><tbody><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">VOC det</td><td colspan=\"3\">COCO det</td><td colspan=\"3\">COCO seg</td></tr><tr><td><p>AP_{all}</p></td><td><p>AP_{50}</p></td><td><p>AP_{75}</p></td><td><p>AP_{all}^{bb}</p></td><td><p>AP_{50}^{bb}</p></td><td><p>AP_{75}^{bb}</p></td><td><p>AP_{all}^{mk}</p></td><td><p>AP_{50}^{mk}</p></td><td><p>AP_{75}^{mk}</p></td></tr><tr><td><p>\\hlineB2.5</p><p>Supervised</p></td><td><p>53.5</p></td><td><p>81.3</p></td><td><p>58.8</p></td><td><p>38.2</p></td><td><p>58.2</p></td><td><p>41.2</p></td><td><p>33.3</p></td><td><p>54.7</p></td><td><p>35.2</p></td></tr><tr><td><p>MoCo V2 [7]</p></td><td><p>57.4</p></td><td><p>82.5</p></td><td><p>64.0</p></td><td><p>39.3</p></td><td><p>58.9</p></td><td><p>42.5</p></td><td><p>34.4</p></td><td><p>55.8</p></td><td><p>36.5</p></td></tr><tr><td><p>SimSiam [8]</p></td><td><p>57</p></td><td><p>82.4</p></td><td><p>63.7</p></td><td><p>39.2</p></td><td>59.3</td><td><p>42.1</p></td><td><p>34.4</p></td><td>56.0</td><td><p>36.7</p></td></tr><tr><td><p>Barlow Twins [33]</p></td><td><p>56.8</p></td><td><p>82.6</p></td><td><p>63.4</p></td><td><p>39.2</p></td><td><p>59.0</p></td><td><p>42.5</p></td><td><p>34.3</p></td><td>56.0</td><td><p>36.5</p></td></tr><tr><td>Fast-MoCo</td><td>57.7</td><td>82.7</td><td>64.4</td><td>39.5</td><td><p>59.2</p></td><td>42.6</td><td>34.6</td><td><p>55.9</p></td><td>36.9</td></tr><tr><td><p>SwAV [3] (w/ multi-crop)</p></td><td><p>56.1</p></td><td><p>82.6</p></td><td><p>62.7</p></td><td><p>38.4</p></td><td><p>58.6</p></td><td><p>41.3</p></td><td><p>33.8</p></td><td><p>55.2</p></td><td><p>35.9</p></td></tr></tbody></table>", "caption": "Table 3: VOC and COCO object detection (det) and instance segmentation (seg) results. We report results measured by Average Precision (AP) using ResNet50 with the C4 backbone variant [14]. For VOC dataset, we train on trainval07+12 and evaluate on test07 by running three trials and report the averaged results.", "list_citation_info": ["[3] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems 33, 9912\u20139924 (2020)", "[8] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)", "[7] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)", "[33] Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: Self-supervised learning via redundancy reduction. In: International Conference on Machine Learning. pp. 12310\u201312320. PMLR (2021)", "[14] Girshick, R., Radosavovic, I., Gkioxari, G., Doll\u00e1r, P., He, K.: Detectron (2018)"]}, {"table": "<table><tbody><tr><th>Config</th><td colspan=\"2\">Values</td></tr><tr><th><p>\\hlineB2.5Epochs</p></th><td><p>\\{100,200,400\\}</p></td><td><p>20</p></td></tr><tr><th><p>Optimizer</p></th><td colspan=\"2\">SGD</td></tr><tr><th><p>Optimizer momentum</p></th><td colspan=\"2\">0.9</td></tr><tr><th><p>Weight decay</p></th><td colspan=\"2\">1e-4</td></tr><tr><th><p>Gradient clipping</p></th><td colspan=\"2\">1.0</td></tr><tr><th><p>Learning rate schedule</p></th><td colspan=\"2\">Cosine</td></tr><tr><th><p>Initial Learning rate</p></th><td><p>0.1</p></td><td><p>0.15</p></td></tr><tr><th><p>Final Learning rate</p></th><td colspan=\"2\">0.0</td></tr><tr><th><p>Warmup epochs</p></th><td colspan=\"2\">1</td></tr><tr><th><p>Warmup initial learning rate</p></th><td><p>0.025</p></td><td><p>0.0375</p></td></tr><tr><th><p>Batch size</p></th><td colspan=\"2\">512</td></tr><tr><th><p>Temperature \\tau</p></th><td colspan=\"2\">1.0</td></tr><tr><th><p>Exponential moving average momentum \\alpha</p></th><td colspan=\"2\">0.99</td></tr><tr><th><p>Augmentation</p></th><td colspan=\"2\">As in [8]</td></tr></tbody></table>", "caption": "Table 5: Self-supervised pretraining setup.", "list_citation_info": ["[8] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)"]}, {"table": "<table><tbody><tr><th>Config</th><td colspan=\"2\">Values</td></tr><tr><th><p>\\hlineB2.5Pre-trainig epochs</p></th><td><p>\\{100,200,400\\}</p></td><td><p>20</p></td></tr><tr><th><p>Fine-tuning epochs</p></th><td><p>90</p></td><td><p>10</p></td></tr><tr><th><p>Optimizer</p></th><td colspan=\"2\">LARS</td></tr><tr><th><p>Optimizer momentum</p></th><td colspan=\"2\">0.9</td></tr><tr><th><p>Learning rate schedule</p></th><td colspan=\"2\">Cosine</td></tr><tr><th><p>Initial Learning rate</p></th><td><p>0.8</p></td><td><p>1.6</p></td></tr><tr><th><p>Final Learning rate</p></th><td colspan=\"2\">0.0</td></tr><tr><th><p>Batch size</p></th><td colspan=\"2\">4096</td></tr><tr><th><p>Augmentation</p></th><td colspan=\"2\">As in [9]</td></tr></tbody></table>", "caption": "Table 6: Linear evaluation setup.", "list_citation_info": ["[9] Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9640\u20139649 (2021)"]}, {"table": "<table><tbody><tr><td><p>Method</p></td><td><p>Top-1 (e100)</p></td></tr><tr><td><p>\\hlineB2.5NNCLR [11]</p></td><td><p>69.4</p></td></tr><tr><td><p>NNCLR + combinatorial patches</p></td><td><p>72.5 (+3.1)</p></td></tr><tr><td><p>BYOL [15]</p></td><td><p>66.5</p></td></tr><tr><td><p>BYOL + combinatorial patches</p></td><td><p>73.6 (+7.1)</p></td></tr><tr><td><p>SimSiam [8]</p></td><td><p>68.1</p></td></tr><tr><td><p>SimSiam + combinatorial patches</p></td><td><p>71.7 (+3.6)</p></td></tr></tbody></table>", "caption": "Table 7: ImageNet linear evaluation performance of our method on other frameworks with ResNet-50. Results of compared methods w/o combinatorial patches are from their original paper.", "list_citation_info": ["[8] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)", "[15] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems 33, 21271\u201321284 (2020)", "[11] Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P., Zisserman, A.: With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9588\u20139597 (2021)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">Top-1</th></tr><tr><th><p>b1024</p></th><th><p>b512</p></th><th><p>b256</p></th></tr></thead><tbody><tr><td><p>\\hlineB2.5SimSiam [8] (100 ep.)</p></td><td>68.0</td><td>68.1</td><td><p>68.1</p></td></tr><tr><td><p>BYOL [15] (300 ep.)</p></td><td>72.3</td><td>72.2</td><td><p>71.9</p></td></tr><tr><td><p>Barlow Twins [33] (300 ep.)</p></td><td>71.7</td><td>71.4</td><td><p>70.7</p></td></tr><tr><td><p>Fast-MoCo (100 ep.)</p></td><td>73.6</td><td>73.5</td><td><p>72.5</p></td></tr></tbody></table>", "caption": "Table 8: Comparison of robustness to batch size from 1024 to 256. Result of the compared methods are from their original paper.", "list_citation_info": ["[8] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)", "[15] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems 33, 21271\u201321284 (2020)", "[33] Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: Self-supervised learning via redundancy reduction. In: International Conference on Machine Learning. pp. 12310\u201312320. PMLR (2021)"]}], "citation_info_to_title": {"[8] Chen, X., He, K.: Exploring simple siamese representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 15750\u201315758 (2021)": "Exploring Simple Siamese Representation Learning", "[31] Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via non-parametric instance discrimination. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3733\u20133742 (2018)": "Unsupervised feature learning via non-parametric instance discrimination", "[11] Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P., Zisserman, A.: With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9588\u20139597 (2021)": "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations", "[3] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems 33, 9912\u20139924 (2020)": "Unsupervised learning of visual features by contrasting cluster assignments", "[13] Gidaris, S., Bursuc, A., Puy, G., Komodakis, N., Cord, M., Perez, P.: Obow: Online bag-of-visual-words generation for self-supervised learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6830\u20136840 (2021)": "Obow: Online bag-of-visual-words generation for self-supervised learning", "[4] Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9650\u20139660 (2021)": "Emerging properties in self-supervised vision transformers", "[6] Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597\u20131607. PMLR (2020)": "A simple framework for contrastive learning of visual representations", "[7] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)": "Improved Baselines with Momentum Contrastive Learning", "[14] Girshick, R., Radosavovic, I., Gkioxari, G., Doll\u00e1r, P., He, K.: Detectron (2018)": "Detectron", "[24] Misra, I., Maaten, L.v.d.: Self-supervised learning of pretext-invariant representations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6707\u20136717 (2020)": "Self-supervised learning of pretext-invariant representations", "[33] Zbontar, J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: Self-supervised learning via redundancy reduction. In: International Conference on Machine Learning. pp. 12310\u201312320. PMLR (2021)": "Barlow twins: Self-supervised learning via redundancy reduction", "[9] Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9640\u20139649 (2021)": "An empirical study of training self-supervised vision transformers", "[15] Grill, J.B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al.: Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems 33, 21271\u201321284 (2020)": "Bootstrap your own latent-a new approach to self-supervised learning"}, "source_title_to_arxiv_id": {"Emerging properties in self-supervised vision transformers": "2104.14294", "An empirical study of training self-supervised vision transformers": "2104.02057"}}