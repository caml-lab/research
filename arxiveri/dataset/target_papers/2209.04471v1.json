{"title": "MCIBI++: Soft Mining Contextual Information Beyond Image for Semantic Segmentation", "abstract": "Co-occurrent visual pattern makes context aggregation become an essential\nparadigm for semantic segmentation.The existing studies focus on modeling the\ncontexts within image while neglecting the valuable semantics of the\ncorresponding category beyond image. To this end, we propose a novel soft\nmining contextual information beyond image paradigm named MCIBI++ to further\nboost the pixel-level representations. Specifically, we first set up a\ndynamically updated memory module to store the dataset-level distribution\ninformation of various categories and then leverage the information to yield\nthe dataset-level category representations during network forward. After that,\nwe generate a class probability distribution for each pixel representation and\nconduct the dataset-level context aggregation with the class probability\ndistribution as weights. Finally, the original pixel representations are\naugmented with the aggregated dataset-level and the conventional image-level\ncontextual information. Moreover, in the inference phase, we additionally\ndesign a coarse-to-fine iterative inference strategy to further boost the\nsegmentation results. MCIBI++ can be effortlessly incorporated into the\nexisting segmentation frameworks and bring consistent performance improvements.\nAlso, MCIBI++ can be extended into the video semantic segmentation framework\nwith considerable improvements over the baseline. Equipped with MCIBI++, we\nachieved the state-of-the-art performance on seven challenging image or video\nsemantic segmentation benchmarks.", "authors": ["Zhenchao Jin", "Dongdong Yu", "Zehuan Yuan", "Lequan Yu"], "published_date": "2022_09_09", "pdf_url": "http://arxiv.org/pdf/2209.04471v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Momentum m</th><th>Backbone</th><th>Stride</th><th>mIoU (\\%)</th></tr></thead><tbody><tr><td>0.01</td><td>ResNet-50</td><td>8\\times</td><td>42.06</td></tr><tr><td>0.05</td><td>ResNet-50</td><td>8\\times</td><td>42.89</td></tr><tr><td>0.1</td><td>ResNet-50</td><td>8\\times</td><td>43.39</td></tr><tr><td>0.3</td><td>ResNet-50</td><td>8\\times</td><td>42.71</td></tr><tr><td>0.5</td><td>ResNet-50</td><td>8\\times</td><td>42.72</td></tr><tr><td>0.7</td><td>ResNet-50</td><td>8\\times</td><td>43.23</td></tr><tr><td>0.9</td><td>ResNet-50</td><td>8\\times</td><td>42.95</td></tr><tr><td>0.9-Poly</td><td>ResNet-50</td><td>8\\times</td><td>42.45</td></tr></tbody></table>", "caption": "TABLE IV: Ablation study of momentum m of \\mathcal{M} on ADE20K val set.0.9-Poly means adopting the polynomial annealing policy to schedule m [44].", "list_citation_info": ["[44] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, \u201cMining contextual information beyond image for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7231\u20137241."]}, {"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td>Stride</td><td>mIoU (\\%)</td></tr><tr><td>CCNet [28]</td><td>ResNet-101</td><td>8\\times</td><td>45.76</td></tr><tr><td>OCNet [27]</td><td>ResNet-101</td><td>8\\times</td><td>45.45</td></tr><tr><td>ACNet [24]</td><td>ResNet-101</td><td>8\\times</td><td>45.90</td></tr><tr><td>DMNet [84]</td><td>ResNet-101</td><td>8\\times</td><td>45.50</td></tr><tr><td>EncNet [85]</td><td>ResNet-101</td><td>8\\times</td><td>44.65</td></tr><tr><td>PSPNet [4]</td><td>ResNet-101</td><td>8\\times</td><td>43.29</td></tr><tr><td>PSANet [59]</td><td>ResNet-101</td><td>8\\times</td><td>43.77</td></tr><tr><td>APCNet [86]</td><td>ResNet-101</td><td>8\\times</td><td>45.38</td></tr><tr><td>OCRNet [26]</td><td>ResNet-101</td><td>8\\times</td><td>45.28</td></tr><tr><td>UperNet [35]</td><td>ResNet-101</td><td>8\\times</td><td>44.85</td></tr><tr><td>CPNet [25]</td><td>ResNet-101</td><td>8\\times</td><td>46.27</td></tr><tr><td>ISNet [23]</td><td>ResNet-101</td><td>8\\times</td><td>47.31</td></tr><tr><td>MaskFormer [22]</td><td>ResNet-101</td><td>8\\times</td><td>47.20</td></tr><tr><td>PSPNet [4]</td><td>ResNet-269</td><td>8\\times</td><td>44.94</td></tr><tr><td>OCRNet [26]</td><td>HRNetV2-W48</td><td>4\\times</td><td>45.66</td></tr><tr><td>ISNet [23]</td><td>ResNeSt-101</td><td>8\\times</td><td>47.55</td></tr><tr><td>DeepLabV3 [6]</td><td>ResNeSt-101</td><td>8\\times</td><td>46.91</td></tr><tr><td>DeepLabV3 [6]</td><td>ResNeSt-200</td><td>8\\times</td><td>48.36</td></tr><tr><td>SETR-MLA [21]</td><td>ViT-Large</td><td>16\\times</td><td>50.28</td></tr><tr><td>UperNet [20]</td><td>Swin-Large</td><td>32\\times</td><td>53.50</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNet-101</td><td>8\\times</td><td>47.22</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNeSt-101</td><td>8\\times</td><td>47.36</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ViT-Large</td><td>16\\times</td><td>50.80</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNet-101</td><td>8\\times</td><td>47.93</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNeSt-101</td><td>8\\times</td><td>48.56</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>Swin-Large</td><td>32\\times</td><td>54.52</td></tr></tbody></table>", "caption": "TABLE X: The comparison with the state-of-the-art methods on ADE20K (val).Multi-scale and flipping testing are adopted for fair comparison.Bold denotes the best score.", "list_citation_info": ["[35] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, \u201cUnified perceptual parsing for scene understanding,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 418\u2013434.", "[25] C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, and N. Sang, \u201cContext prior for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009416\u201312\u2009425.", "[85] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal, \u201cContext encoding for semantic segmentation,\u201d in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2018, pp. 7151\u20137160.", "[28] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, \u201cCcnet: Criss-cross attention for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 603\u2013612.", "[59] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia, \u201cPsanet: Point-wise spatial attention network for scene parsing,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 267\u2013283.", "[23] Z. Jin, B. Liu, Q. Chu, and N. Yu, \u201cIsnet: Integrate image-level and semantic-level context for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7189\u20137198.", "[44] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, \u201cMining contextual information beyond image for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7231\u20137241.", "[22] B. Cheng, A. Schwing, and A. Kirillov, \u201cPer-pixel classification is not all you need for semantic segmentation,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 17\u2009864\u201317\u2009875, 2021.", "[21] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr et al., \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 6881\u20136890.", "[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \u201cRethinking atrous convolution for semantic image segmentation,\u201d arXiv preprint arXiv:1706.05587, 2017.", "[4] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2881\u20132890.", "[20] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10\u2009012\u201310\u2009022.", "[27] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, \u201cOcnet: Object context for semantic segmentation,\u201d International Journal of Computer Vision, vol. 129, no. 8, pp. 2375\u20132398, 2021.", "[24] J. Fu, J. Liu, Y. Wang, Y. Li, Y. Bao, J. Tang, and H. Lu, \u201cAdaptive context network for scene parsing,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6748\u20136757.", "[26] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in European conference on computer vision. Springer, 2020, pp. 173\u2013190.", "[86] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, \u201cAdaptive pyramid context network for semantic segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7519\u20137528.", "[84] J. He, Z. Deng, and Y. Qiao, \u201cDynamic multi-scale filters for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 3562\u20133572."]}, {"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td>Stride</td><td>mIoU (\\%)</td></tr><tr><td>Attention [51]</td><td>ResNet-101</td><td>-</td><td>42.92</td></tr><tr><td>DeepLab [5]</td><td>ResNet-101</td><td>-</td><td>44.80</td></tr><tr><td>MMAN [88]</td><td>ResNet-101</td><td>-</td><td>46.81</td></tr><tr><td>CE2P [16]</td><td>ResNet-101</td><td>16\\times</td><td>53.10</td></tr><tr><td>BraidNet [89]</td><td>ResNet-101</td><td>8\\times</td><td>54.54</td></tr><tr><td>DeepLabV3 [6]</td><td>ResNet-101</td><td>8\\times</td><td>54.58</td></tr><tr><td>OCNet [27]</td><td>ResNet-101</td><td>8\\times</td><td>54.72</td></tr><tr><td>CCNet [28]</td><td>ResNet-101</td><td>8\\times</td><td>55.47</td></tr><tr><td>OCRNet [26]</td><td>ResNet-101</td><td>8\\times</td><td>55.60</td></tr><tr><td>CorrPM [90]</td><td>ResNet-101</td><td>8\\times</td><td>55.33</td></tr><tr><td>ISNet [23]</td><td>ResNet-101</td><td>8\\times</td><td>55.41</td></tr><tr><td>ISNet [23]</td><td>ResNeSt-101</td><td>8\\times</td><td>56.81</td></tr><tr><td>HRNet [19]</td><td>HRNetV2-W48</td><td>4\\times</td><td>55.90</td></tr><tr><td>OCRNet [26]</td><td>HRNetV2-W48</td><td>4\\times</td><td>56.65</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNet-101</td><td>8\\times</td><td>55.42</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNeSt-101</td><td>8\\times</td><td>56.34</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>HRNetV2-W48</td><td>4\\times</td><td>56.99</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNet-101</td><td>8\\times</td><td>56.32</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNeSt-101</td><td>8\\times</td><td>57.08</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>Swin-Large</td><td>32\\times</td><td>59.91</td></tr></tbody></table>", "caption": "TABLE XI: Segmentation results on the validation set of LIP.Test time augmentation is utilized for fair comparison.", "list_citation_info": ["[23] Z. Jin, B. Liu, Q. Chu, and N. Yu, \u201cIsnet: Integrate image-level and semantic-level context for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7189\u20137198.", "[28] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, \u201cCcnet: Criss-cross attention for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 603\u2013612.", "[19] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., \u201cDeep high-resolution representation learning for visual recognition,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.", "[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, \u201cDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 4, pp. 834\u2013848, 2017.", "[90] Z. Zhang, C. Su, L. Zheng, X. Xie, and Y. Li, \u201cOn the correlation among edge, pose and parsing,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.", "[51] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, \u201cAttention to scale: Scale-aware semantic image segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3640\u20133649.", "[44] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, \u201cMining contextual information beyond image for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7231\u20137241.", "[16] T. Ruan, T. Liu, Z. Huang, Y. Wei, S. Wei, and Y. Zhao, \u201cDevil in the details: Towards accurate single and multiple human parsing,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 4814\u20134821.", "[89] X. Liu, M. Zhang, W. Liu, J. Song, and T. Mei, \u201cBraidnet: Braiding semantics and details for accurate human parsing,\u201d in Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 338\u2013346.", "[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \u201cRethinking atrous convolution for semantic image segmentation,\u201d arXiv preprint arXiv:1706.05587, 2017.", "[88] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, and Y. Yang, \u201cMacro-micro adversarial network for human parsing,\u201d in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 418\u2013434.", "[27] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, \u201cOcnet: Object context for semantic segmentation,\u201d International Journal of Computer Vision, vol. 129, no. 8, pp. 2375\u20132398, 2021.", "[26] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in European conference on computer vision. Springer, 2020, pp. 173\u2013190."]}, {"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td>Stride</td><td>mIoU (\\%)</td></tr><tr><td>DANet [8]</td><td>ResNet-101</td><td>8\\times</td><td>39.70</td></tr><tr><td>OCRNet [26]</td><td>ResNet-101</td><td>8\\times</td><td>39.50</td></tr><tr><td>SVCNet [91]</td><td>ResNet-101</td><td>8\\times</td><td>39.60</td></tr><tr><td>EMANet [62]</td><td>ResNet-101</td><td>8\\times</td><td>39.90</td></tr><tr><td>ACNet [24]</td><td>ResNet-101</td><td>8\\times</td><td>40.10</td></tr><tr><td>ISNet [23]</td><td>ResNet-101</td><td>8\\times</td><td>41.60</td></tr><tr><td>MaskFormer [22]</td><td>ResNet-101</td><td>8\\times</td><td>39.30</td></tr><tr><td>ISNet [23]</td><td>ResNeSt-101</td><td>8\\times</td><td>42.08</td></tr><tr><td>OCRNet [26]</td><td>HRNetV2-W48</td><td>4\\times</td><td>40.50</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNet-101</td><td>8\\times</td><td>41.49</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNeSt-101</td><td>8\\times</td><td>42.15</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ViT-Large</td><td>16\\times</td><td>44.89</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNet-101</td><td>8\\times</td><td>41.84</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNeSt-101</td><td>8\\times</td><td>42.71</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>Swin-Large</td><td>32\\times</td><td>50.27</td></tr></tbody></table>", "caption": "TABLE XII: Comparison with the state-of-the-art segmentation frameworks on the test set of COCO-Stuff.Multi-scale and flipping testing are employed for fair comparison.The best score is marked in bold.", "list_citation_info": ["[91] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, \u201cSemantic correlation promoted shape-variant context for segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8885\u20138894.", "[23] Z. Jin, B. Liu, Q. Chu, and N. Yu, \u201cIsnet: Integrate image-level and semantic-level context for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7189\u20137198.", "[44] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, \u201cMining contextual information beyond image for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7231\u20137241.", "[22] B. Cheng, A. Schwing, and A. Kirillov, \u201cPer-pixel classification is not all you need for semantic segmentation,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 17\u2009864\u201317\u2009875, 2021.", "[62] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, \u201cExpectation-maximization attention networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9167\u20139176.", "[24] J. Fu, J. Liu, Y. Wang, Y. Li, Y. Bao, J. Tang, and H. Lu, \u201cAdaptive context network for scene parsing,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6748\u20136757.", "[26] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in European conference on computer vision. Springer, 2020, pp. 173\u2013190.", "[8] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \u201cDual attention network for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3146\u20133154."]}, {"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td>Stride</td><td>mIoU (\\%)</td></tr><tr><td>CCNet [28]</td><td>ResNet-101</td><td>8\\times</td><td>81.90</td></tr><tr><td>PSPNet [4]</td><td>ResNet-101</td><td>8\\times</td><td>78.40</td></tr><tr><td>PSANet [59]</td><td>ResNet-101</td><td>8\\times</td><td>80.10</td></tr><tr><td>OCNet [27]</td><td>ResNet-101</td><td>8\\times</td><td>80.10</td></tr><tr><td>OCRNet [26]</td><td>ResNet-101</td><td>8\\times</td><td>81.80</td></tr><tr><td>DANet [8]</td><td>ResNet-101</td><td>8\\times</td><td>81.50</td></tr><tr><td>ACFNet [61]</td><td>ResNet-101</td><td>8\\times</td><td>81.80</td></tr><tr><td>ANNet [60]</td><td>ResNet-101</td><td>8\\times</td><td>81.30</td></tr><tr><td>ACNet [24]</td><td>ResNet-101</td><td>8\\times</td><td>82.30</td></tr><tr><td>CPNet [25]</td><td>ResNet-101</td><td>8\\times</td><td>81.30</td></tr><tr><td>SFNet [92]</td><td>ResNet-101</td><td>8\\times</td><td>81.80</td></tr><tr><td>SPNet [93]</td><td>ResNet-101</td><td>8\\times</td><td>82.00</td></tr><tr><td>DenseASPP [58]</td><td>DenseNet-161</td><td>8\\times</td><td>80.60</td></tr><tr><td>Dynamic [94]</td><td>Layer33-PSP</td><td>8\\times</td><td>80.70</td></tr><tr><td>HRNet [19]</td><td>HRNetV2-W48</td><td>4\\times</td><td>81.60</td></tr><tr><td>OCRNet [26]</td><td>HRNetV2-W48</td><td>4\\times</td><td>82.40</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNet-101</td><td>8\\times</td><td>82.03</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>ResNeSt-101</td><td>8\\times</td><td>81.59</td></tr><tr><td>DeepLabV3+MCIBI [44]</td><td>HRNetV2-W48</td><td>4\\times</td><td>82.55</td></tr><tr><td>DeepLabV3+MCIBI++ (ours)</td><td>ResNet-101</td><td>8\\times</td><td>82.20</td></tr><tr><td>DeepLabV3+MCIBI++ (ours)</td><td>ResNeSt-101</td><td>8\\times</td><td>81.70</td></tr><tr><td>DeepLabV3+MCIBI++ (ours)</td><td>HRNetV2-W48</td><td>4\\times</td><td>82.74</td></tr></tbody></table>", "caption": "TABLE XIII: Comparison of performance on the test set of Cityscapes with state-of-the-art approaches (trained on trainval set).Multi-scale and flipping testing are used for fair comparison.Bold denotes the best score.", "list_citation_info": ["[25] C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, and N. Sang, \u201cContext prior for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009416\u201312\u2009425.", "[28] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, \u201cCcnet: Criss-cross attention for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 603\u2013612.", "[59] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia, \u201cPsanet: Point-wise spatial attention network for scene parsing,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 267\u2013283.", "[19] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., \u201cDeep high-resolution representation learning for visual recognition,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.", "[94] Y. Li, L. Song, Y. Chen, Z. Li, X. Zhang, X. Wang, and J. Sun, \u201cLearning dynamic routing for semantic segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 8553\u20138562.", "[44] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, \u201cMining contextual information beyond image for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7231\u20137241.", "[61] F. Zhang, Y. Chen, Z. Li, Z. Hong, J. Liu, F. Ma, J. Han, and E. Ding, \u201cAcfnet: Attentional class feature network for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6798\u20136807.", "[58] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, \u201cDenseaspp for semantic segmentation in street scenes,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3684\u20133692.", "[4] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2881\u20132890.", "[60] Z. Zhu, M. Xu, S. Bai, T. Huang, and X. Bai, \u201cAsymmetric non-local neural networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 593\u2013602.", "[92] X. Li, A. You, Z. Zhu, H. Zhao, M. Yang, K. Yang, S. Tan, and Y. Tong, \u201cSemantic flow for fast and accurate scene parsing,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 775\u2013793.", "[27] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, \u201cOcnet: Object context for semantic segmentation,\u201d International Journal of Computer Vision, vol. 129, no. 8, pp. 2375\u20132398, 2021.", "[24] J. Fu, J. Liu, Y. Wang, Y. Li, Y. Bao, J. Tang, and H. Lu, \u201cAdaptive context network for scene parsing,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6748\u20136757.", "[26] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in European conference on computer vision. Springer, 2020, pp. 173\u2013190.", "[93] Q. Hou, L. Zhang, M.-M. Cheng, and J. Feng, \u201cStrip pooling: Rethinking spatial pooling for scene parsing,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 4003\u20134012.", "[8] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \u201cDual attention network for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3146\u20133154."]}, {"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td>Stride</td><td>mIoU (\\%)</td></tr><tr><td>EncNet [85]</td><td>ResNet-101</td><td>8\\times</td><td>51.70</td></tr><tr><td>DANet [8]</td><td>ResNet-101</td><td>8\\times</td><td>52.60</td></tr><tr><td>ANNet [60]</td><td>ResNet-101</td><td>8\\times</td><td>52.80</td></tr><tr><td>SVCNet [91]</td><td>ResNet-101</td><td>8\\times</td><td>53.20</td></tr><tr><td>CPNet [25]</td><td>ResNet-101</td><td>8\\times</td><td>53.90</td></tr><tr><td>APCNet [86]</td><td>ResNet-101</td><td>8\\times</td><td>54.70</td></tr><tr><td>ACNet [24]</td><td>ResNet-101</td><td>8\\times</td><td>54.10</td></tr><tr><td>EMANet [62]</td><td>ResNet-101</td><td>8\\times</td><td>53.10</td></tr><tr><td>SFNet [92]</td><td>ResNet-101</td><td>8\\times</td><td>53.80</td></tr><tr><td>OCRNet [26]</td><td>ResNet-101</td><td>8\\times</td><td>54.80</td></tr><tr><td>SPNet [93]</td><td>ResNet-101</td><td>8\\times</td><td>54.50</td></tr><tr><td>HRNet [19]</td><td>HRNetV2-W48</td><td>4\\times</td><td>54.00</td></tr><tr><td>OCRNet [26]</td><td>HRNetV2-W48</td><td>4\\times</td><td>56.20</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNet-101</td><td>8\\times</td><td>56.82</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNeSt-101</td><td>8\\times</td><td>57.92</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>Swin-Large</td><td>32\\times</td><td>64.01</td></tr></tbody></table>", "caption": "TABLE XIV: Quantitative evaluation on the PASCAL-Context validation set.Multi-scale and flipping testing are used for fair comparison.", "list_citation_info": ["[91] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, \u201cSemantic correlation promoted shape-variant context for segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8885\u20138894.", "[25] C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, and N. Sang, \u201cContext prior for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009416\u201312\u2009425.", "[85] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal, \u201cContext encoding for semantic segmentation,\u201d in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2018, pp. 7151\u20137160.", "[93] Q. Hou, L. Zhang, M.-M. Cheng, and J. Feng, \u201cStrip pooling: Rethinking spatial pooling for scene parsing,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 4003\u20134012.", "[19] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., \u201cDeep high-resolution representation learning for visual recognition,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.", "[26] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in European conference on computer vision. Springer, 2020, pp. 173\u2013190.", "[62] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, \u201cExpectation-maximization attention networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9167\u20139176.", "[60] Z. Zhu, M. Xu, S. Bai, T. Huang, and X. Bai, \u201cAsymmetric non-local neural networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 593\u2013602.", "[92] X. Li, A. You, Z. Zhu, H. Zhao, M. Yang, K. Yang, S. Tan, and Y. Tong, \u201cSemantic flow for fast and accurate scene parsing,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 775\u2013793.", "[24] J. Fu, J. Liu, Y. Wang, Y. Li, Y. Bao, J. Tang, and H. Lu, \u201cAdaptive context network for scene parsing,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6748\u20136757.", "[8] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \u201cDual attention network for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3146\u20133154.", "[86] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, \u201cAdaptive pyramid context network for semantic segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7519\u20137528."]}, {"table": "<table><tbody><tr><td>Method</td><td>Backbone</td><td>Stride</td><td>mIoU (\\%)</td></tr><tr><td>FCN [11] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>70.59</td></tr><tr><td>SemanticFPN [95] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>72.51</td></tr><tr><td>PointRend [15] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>72.31</td></tr><tr><td>ANNet [60] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.15</td></tr><tr><td>APCNet [86] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.99</td></tr><tr><td>OCRNet [26] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.82</td></tr><tr><td>CCNet [28] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.02</td></tr><tr><td>CE2P [16] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>77.77</td></tr><tr><td>DANet [8] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>77.97</td></tr><tr><td>DeepLabV3 [6] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>79.52</td></tr><tr><td>DeepLabV3Plus [7] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>79.19</td></tr><tr><td>DMNet [84] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>79.15</td></tr><tr><td>ISANet [96] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.60</td></tr><tr><td>UperNet [35] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>79.13</td></tr><tr><td>PSPNet [4] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>79.04</td></tr><tr><td>PSANet [59] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.97</td></tr><tr><td>Nonlocal [9] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.89</td></tr><tr><td>GCNet [3] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.81</td></tr><tr><td>EncNet [85] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>77.61</td></tr><tr><td>EMANet [62] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>76.43</td></tr><tr><td>DNLNet [97] (our impl.)</td><td>ResNet-101</td><td>8\\times</td><td>78.37</td></tr><tr><td>OCRNet [26] (our impl.)</td><td>HRNetV2-W48</td><td>4\\times</td><td>77.60</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNet-50</td><td>8\\times</td><td>79.48</td></tr><tr><td>UperNet+MCIBI++ (ours)</td><td>ResNet-101</td><td>8\\times</td><td>80.42</td></tr></tbody></table>", "caption": "TABLE XV: The semantic segmentation result on the validation set of PASCAL VOC 2012.Single-scale testing is utilized for fair comparison.", "list_citation_info": ["[26] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in European conference on computer vision. Springer, 2020, pp. 173\u2013190.", "[97] M. Yin, Z. Yao, Y. Cao, X. Li, Z. Zhang, S. Lin, and H. Hu, \u201cDisentangled non-local neural networks,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 191\u2013207.", "[11] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks for semantic segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431\u20133440.", "[9] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7794\u20137803.", "[62] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, \u201cExpectation-maximization attention networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9167\u20139176.", "[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \u201cRethinking atrous convolution for semantic image segmentation,\u201d arXiv preprint arXiv:1706.05587, 2017.", "[60] Z. Zhu, M. Xu, S. Bai, T. Huang, and X. Bai, \u201cAsymmetric non-local neural networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 593\u2013602.", "[96] L. Huang, Y. Yuan, J. Guo, C. Zhang, X. Chen, and J. Wang, \u201cInterlaced sparse self-attention for semantic segmentation,\u201d arXiv preprint arXiv:1907.12273, 2019.", "[35] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, \u201cUnified perceptual parsing for scene understanding,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 418\u2013434.", "[3] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, \u201cGcnet: Non-local networks meet squeeze-excitation networks and beyond,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0\u20130.", "[4] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2881\u20132890.", "[86] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, \u201cAdaptive pyramid context network for semantic segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7519\u20137528.", "[85] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal, \u201cContext encoding for semantic segmentation,\u201d in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2018, pp. 7151\u20137160.", "[28] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, \u201cCcnet: Criss-cross attention for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 603\u2013612.", "[59] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia, \u201cPsanet: Point-wise spatial attention network for scene parsing,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 267\u2013283.", "[95] A. Kirillov, R. Girshick, K. He, and P. Dollar, \u201cPanoptic feature pyramid networks,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2019. [Online]. Available: http://dx.doi.org/10.1109/CVPR.2019.00656", "[7] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, \u201cEncoder-decoder with atrous separable convolution for semantic image segmentation,\u201d in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 801\u2013818.", "[15] A. Kirillov, Y. Wu, K. He, and R. Girshick, \u201cPointrend: Image segmentation as rendering,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 9799\u20139808.", "[16] T. Ruan, T. Liu, Z. Huang, Y. Wei, S. Wei, and Y. Zhao, \u201cDevil in the details: Towards accurate single and multiple human parsing,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 4814\u20134821.", "[8] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \u201cDual attention network for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3146\u20133154.", "[84] J. He, Z. Deng, and Y. Qiao, \u201cDynamic multi-scale filters for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 3562\u20133572."]}, {"table": "<table><tbody><tr><th>Method</th><th>Backbone</th><td>Pixel Accuracy (\\%)</td><td>Mean Class Accuracy (\\%)</td><td>mIoU (\\%)</td><td>TC (\\%)</td><td>mVC{}_{8} (\\%)</td><td>mVC{}_{16} (\\%)</td></tr><tr><th>Image Semantic Segmentation Framework</th><th></th><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>DeepLabV3Plus [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>34.67</td><td>65.45</td><td>83.24</td><td>78.24</td></tr><tr><th>UperNet [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>36.46</td><td>63.10</td><td>82.55</td><td>76.08</td></tr><tr><th>PSPNet [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>36.47</td><td>65.89</td><td>84.16</td><td>79.63</td></tr><tr><th>OCRNet [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>36.68</td><td>66.21</td><td>83.97</td><td>79.04</td></tr><tr><th>UperNet+MCIBI [44, 45]</th><th>ResNet-101</th><td>73.85</td><td>53.42</td><td>42.11</td><td>68.15</td><td>82.11</td><td>76.52</td></tr><tr><th>UperNet+MCIBI [44, 45]</th><th>Swin-Large</th><td>81.22</td><td>64.65</td><td>55.18</td><td>75.75</td><td>89.52</td><td>86.13</td></tr><tr><th>UperNet+MCIBI++ (ours)</th><th>ResNet-101</th><td>75.15 (+1.30)</td><td>55.64 (+2.22)</td><td>43.21 (+1.10)</td><td>70.06 (+1.91)</td><td>83.62 (+1.51)</td><td>78.48 (+1.96)</td></tr><tr><th>UperNet+MCIBI++ (ours)</th><th>Swin-Large</th><td>81.70 (+0.48)</td><td>65.43 (+0.78)</td><td>56.04 (+0.86)</td><td>76.40 (+0.65)</td><td>90.07 (+0.55)</td><td>87.01 (+0.88)</td></tr><tr><th>Video Semantic Segmentation Framework</th><th></th><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>ETC+PSPNet [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>36.55</td><td>67.94</td><td>84.10</td><td>79.22</td></tr><tr><th>NetWarp+PSPNet [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>36.95</td><td>67.85</td><td>84.36</td><td>79.42</td></tr><tr><th>ETC+OCRNet [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>37.46</td><td>68.99</td><td>84.10</td><td>79.10</td></tr><tr><th>NetWarp+OCRNet [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>37.52</td><td>68.89</td><td>84.00</td><td>78.97</td></tr><tr><th>TCB{}_{\\emph{st-ppm}} [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>37.46</td><td>70.30</td><td>86.95</td><td>82.12</td></tr><tr><th>TCB{}_{\\emph{st-ocr}} [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>37.40</td><td>72.20</td><td>86.88</td><td>82.04</td></tr><tr><th>TCB{}_{\\emph{st-ocr~{}mem}} [43]</th><th>ResNet-101</th><td>-</td><td>-</td><td>37.82</td><td>73.63</td><td>87.86</td><td>83.99</td></tr><tr><th>UperNet+Video-MCIBI [44, 45]</th><th>Swin-Large</th><td>81.97</td><td>68.58</td><td>57.57</td><td>80.90</td><td>90.67</td><td>87.39</td></tr><tr><th>UperNet+Video-MCIBI++ (ours)</th><th>Swin-Large</th><td>82.20 (+0.23)</td><td>69.00 (+0.42)</td><td>58.88 (+1.31)</td><td>81.21 (+0.31)</td><td>91.53 (+0.86)</td><td>88.50 (+1.11)</td></tr></tbody></table>", "caption": "TABLE XVII: Comparison on the VSPW validation set. mVC{}_{C} means we use a clip with C frames.MCIBI and MCIBI++ represent for the image semantic segmentation methods proposed in ths paper whileVideo-MCIBI and Video-MCIBI++ denote the video semantic segmentation approaches.", "list_citation_info": ["[43] J. Miao, Y. Wei, Y. Wu, C. Liang, G. Li, and Y. Yang, \u201cVspw: A large-scale dataset for video scene parsing in the wild,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 4133\u20134143.", "[44] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, \u201cMining contextual information beyond image for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7231\u20137241."]}], "citation_info_to_title": {"[8] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \u201cDual attention network for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3146\u20133154.": "Dual attention network for scene segmentation", "[97] M. Yin, Z. Yao, Y. Cao, X. Li, Z. Zhang, S. Lin, and H. Hu, \u201cDisentangled non-local neural networks,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 191\u2013207.": "Disentangled non-local neural networks", "[92] X. Li, A. You, Z. Zhu, H. Zhao, M. Yang, K. Yang, S. Tan, and Y. Tong, \u201cSemantic flow for fast and accurate scene parsing,\u201d in European Conference on Computer Vision. Springer, 2020, pp. 775\u2013793.": "Semantic flow for fast and accurate scene parsing", "[89] X. Liu, M. Zhang, W. Liu, J. Song, and T. Mei, \u201cBraidnet: Braiding semantics and details for accurate human parsing,\u201d in Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 338\u2013346.": "Braidnet: Braiding Semantics and Details for Accurate Human Parsing", "[16] T. Ruan, T. Liu, Z. Huang, Y. Wei, S. Wei, and Y. Zhao, \u201cDevil in the details: Towards accurate single and multiple human parsing,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 4814\u20134821.": "Devil in the details: Towards accurate single and multiple human parsing", "[93] Q. Hou, L. Zhang, M.-M. Cheng, and J. Feng, \u201cStrip pooling: Rethinking spatial pooling for scene parsing,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 4003\u20134012.": "Strip pooling: Rethinking spatial pooling for scene parsing", "[25] C. Yu, J. Wang, C. Gao, G. Yu, C. Shen, and N. Sang, \u201cContext prior for scene segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009416\u201312\u2009425.": "Context prior for scene segmentation", "[84] J. He, Z. Deng, and Y. Qiao, \u201cDynamic multi-scale filters for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 3562\u20133572.": "Dynamic multi-scale filters for semantic segmentation", "[21] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr et al., \u201cRethinking semantic segmentation from a sequence-to-sequence perspective with transformers,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 6881\u20136890.": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers", "[28] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, \u201cCcnet: Criss-cross attention for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 603\u2013612.": "Ccnet: Criss-cross attention for semantic segmentation", "[6] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, \u201cRethinking atrous convolution for semantic image segmentation,\u201d arXiv preprint arXiv:1706.05587, 2017.": "Rethinking atrous convolution for semantic image segmentation", "[59] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia, \u201cPsanet: Point-wise spatial attention network for scene parsing,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 267\u2013283.": "Psanet: Point-wise spatial attention network for scene parsing", "[94] Y. Li, L. Song, Y. Chen, Z. Li, X. Zhang, X. Wang, and J. Sun, \u201cLearning dynamic routing for semantic segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 8553\u20138562.": "Learning dynamic routing for semantic segmentation", "[44] Z. Jin, T. Gong, D. Yu, Q. Chu, J. Wang, C. Wang, and J. Shao, \u201cMining contextual information beyond image for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7231\u20137241.": "Mining contextual information beyond image for semantic segmentation", "[35] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, \u201cUnified perceptual parsing for scene understanding,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 418\u2013434.": "Unified perceptual parsing for scene understanding", "[4] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2881\u20132890.": "Pyramid Scene Parsing Network", "[43] J. Miao, Y. Wei, Y. Wu, C. Liang, G. Li, and Y. Yang, \u201cVspw: A large-scale dataset for video scene parsing in the wild,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 4133\u20134143.": "Vspw: A large-scale dataset for video scene parsing in the wild", "[88] Y. Luo, Z. Zheng, L. Zheng, T. Guan, J. Yu, and Y. Yang, \u201cMacro-micro adversarial network for human parsing,\u201d in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 418\u2013434.": "Macro-micro adversarial network for human parsing", "[24] J. Fu, J. Liu, Y. Wang, Y. Li, Y. Bao, J. Tang, and H. Lu, \u201cAdaptive context network for scene parsing,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6748\u20136757.": "Adaptive context network for scene parsing", "[86] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, \u201cAdaptive pyramid context network for semantic segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 7519\u20137528.": "Adaptive Pyramid Context Network for Semantic Segmentation", "[7] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, \u201cEncoder-decoder with atrous separable convolution for semantic image segmentation,\u201d in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 801\u2013818.": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "[3] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, \u201cGcnet: Non-local networks meet squeeze-excitation networks and beyond,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019, pp. 0\u20130.": "GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond", "[20] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10\u2009012\u201310\u2009022.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[11] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks for semantic segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431\u20133440.": "Fully Convolutional Networks for Semantic Segmentation", "[96] L. Huang, Y. Yuan, J. Guo, C. Zhang, X. Chen, and J. Wang, \u201cInterlaced sparse self-attention for semantic segmentation,\u201d arXiv preprint arXiv:1907.12273, 2019.": "Interlaced Sparse Self-Attention for Semantic Segmentation", "[19] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., \u201cDeep high-resolution representation learning for visual recognition,\u201d IEEE transactions on pattern analysis and machine intelligence, 2020.": "Deep high-resolution representation learning for visual recognition", "[51] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, \u201cAttention to scale: Scale-aware semantic image segmentation,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3640\u20133649.": "Attention to scale: Scale-aware semantic image segmentation", "[22] B. Cheng, A. Schwing, and A. Kirillov, \u201cPer-pixel classification is not all you need for semantic segmentation,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 17\u2009864\u201317\u2009875, 2021.": "Per-pixel classification is not all you need for semantic segmentation", "[90] Z. Zhang, C. Su, L. Zheng, X. Xie, and Y. Li, \u201cOn the correlation among edge, pose and parsing,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.": "On the correlation among edge, pose and parsing", "[91] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, \u201cSemantic correlation promoted shape-variant context for segmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8885\u20138894.": "Semantic correlation promoted shape-variant context for segmentation", "[23] Z. Jin, B. Liu, Q. Chu, and N. Yu, \u201cIsnet: Integrate image-level and semantic-level context for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 7189\u20137198.": "Isnet: Integrate image-level and semantic-level context for semantic segmentation", "[60] Z. Zhu, M. Xu, S. Bai, T. Huang, and X. Bai, \u201cAsymmetric non-local neural networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 593\u2013602.": "Asymmetric non-local neural networks for semantic segmentation", "[9] X. Wang, R. Girshick, A. Gupta, and K. He, \u201cNon-local neural networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7794\u20137803.": "Non-local neural networks", "[85] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal, \u201cContext encoding for semantic segmentation,\u201d in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2018, pp. 7151\u20137160.": "Context encoding for semantic segmentation", "[27] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang, \u201cOcnet: Object context for semantic segmentation,\u201d International Journal of Computer Vision, vol. 129, no. 8, pp. 2375\u20132398, 2021.": "Ocnet: Object context for semantic segmentation", "[95] A. Kirillov, R. Girshick, K. He, and P. Dollar, \u201cPanoptic feature pyramid networks,\u201d 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2019. [Online]. Available: http://dx.doi.org/10.1109/CVPR.2019.00656": "Panoptic feature pyramid networks", "[15] A. Kirillov, Y. Wu, K. He, and R. Girshick, \u201cPointrend: Image segmentation as rendering,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 9799\u20139808.": "Pointrend: Image segmentation as rendering", "[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, \u201cDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 4, pp. 834\u2013848, 2017.": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "[58] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, \u201cDenseaspp for semantic segmentation in street scenes,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3684\u20133692.": "Denseaspp for semantic segmentation in street scenes", "[26] Y. Yuan, X. Chen, and J. Wang, \u201cObject-contextual representations for semantic segmentation,\u201d in European conference on computer vision. Springer, 2020, pp. 173\u2013190.": "Object-contextual representations for semantic segmentation", "[61] F. Zhang, Y. Chen, Z. Li, Z. Hong, J. Liu, F. Ma, J. Han, and E. Ding, \u201cAcfnet: Attentional class feature network for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6798\u20136807.": "Acfnet: Attentional Class Feature Network for Semantic Segmentation", "[62] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, \u201cExpectation-maximization attention networks for semantic segmentation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9167\u20139176.": "Expectation-maximization attention networks for semantic segmentation"}, "source_title_to_arxiv_id": {"Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers": "2012.15840", "Mining contextual information beyond image for semantic segmentation": "2108.11819", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030", "Isnet: Integrate image-level and semantic-level context for semantic segmentation": "2108.12382"}}