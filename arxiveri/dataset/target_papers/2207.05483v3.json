{"title": "CorrI2P: Deep Image-to-Point Cloud Registration via Dense Correspondence", "abstract": "Motivated by the intuition that the critical step of localizing a 2D image in\nthe corresponding 3D point cloud is establishing 2D-3D correspondence between\nthem, we propose the first feature-based dense correspondence framework for\naddressing the image-to-point cloud registration problem, dubbed CorrI2P, which\nconsists of three modules, i.e., feature embedding, symmetric overlapping\nregion detection, and pose estimation through the established correspondence.\nSpecifically, given a pair of a 2D image and a 3D point cloud, we first\ntransform them into high-dimensional feature space and feed the resulting\nfeatures into a symmetric overlapping region detector to determine the region\nwhere the image and point cloud overlap each other. Then we use the features of\nthe overlapping regions to establish the 2D-3D correspondence before running\nEPnP within RANSAC to estimate the camera's pose. Experimental results on KITTI\nand NuScenes datasets show that our CorrI2P outperforms state-of-the-art\nimage-to-point cloud registration methods significantly. We will make the code\npublicly available.", "authors": ["Siyu Ren", "Yiming Zeng", "Junhui Hou", "Xiaodong Chen"], "published_date": "2022_07_12", "pdf_url": "http://arxiv.org/pdf/2207.05483v3", "list_table_and_caption": [{"table": "<table><tbody><tr><th rowspan=\"2\"></th><td colspan=\"2\">KITTI</td><td colspan=\"2\">NuScenes</td></tr><tr><td>RTE \\downarrow (m)</td><td>RRE \\downarrow ({}^{\\circ})</td><td>RTE \\downarrow (m)</td><td>RRE \\downarrow ({}^{\\circ})</td></tr><tr><th>Grid Cls. + EPnP [19]</th><td>1.07\\pm 0.61</td><td>6.48\\pm 1.66</td><td>2.35\\pm 1.12</td><td>7.20\\pm 1.65</td></tr><tr><th>DeepI2P (3D) [19]</th><td>1.27\\pm 0.80</td><td>6.26\\pm 2.29</td><td>2.00\\pm 1.08</td><td>7.18\\pm 1.92</td></tr><tr><th>DeepI2P (2D) [19]</th><td>1.46\\pm 0.96</td><td>4.27\\pm 2.74</td><td>2.19\\pm 1.16</td><td>3.54\\pm 2.51</td></tr><tr><th>Ours</th><td>\\boldsymbol{0.74\\pm 0.65}</td><td>\\boldsymbol{2.07\\pm 1.64}</td><td>\\boldsymbol{1.83\\pm 1.06}</td><td>\\boldsymbol{2.65\\pm 1.93}</td></tr></tbody></table>", "caption": "TABLE I: Comparison of the registration accuracy (mean \\pm std) of different methods on the KITTI and NuScenes datasets. \u201c\\downarrow\u201d means that the smaller, the better. The best results are highlighted in bold.", "list_citation_info": ["[19] J. Li and G. H. Lee, \u201cDeepi2p: Image-to-point cloud registration via deep classification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15\u2009960\u201315\u2009969."]}, {"table": "<table><tbody><tr><th></th><th>Method</th><td>Recall</td><td>Precision</td><td>F2-Score</td></tr><tr><th rowspan=\"4\">PC</th><th>ISS [28]</th><td>0.044</td><td>0.268</td><td>0.076</td></tr><tr><th>Random</th><td>0.199</td><td>0.196</td><td>0.197</td></tr><tr><th>DeepI2P [19]</th><td>0.935</td><td>0.946</td><td>0.938</td></tr><tr><th>Ours</th><td>0.975</td><td>0.911</td><td>0.941</td></tr><tr><th rowspan=\"3\">IMG</th><th>SIFT [27]</th><td>0.091</td><td>0.585</td><td>0.156</td></tr><tr><th>Random</th><td>0.329</td><td>0.599</td><td>0.424</td></tr><tr><th>Ours</th><td>0.783</td><td>0.903</td><td>0.838</td></tr></tbody></table>", "caption": "TABLE II: The performance of the overlap detection on the KITTI dataset. The best results are highlighted in bold.", "list_citation_info": ["[19] J. Li and G. H. Lee, \u201cDeepi2p: Image-to-point cloud registration via deep classification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15\u2009960\u201315\u2009969.", "[27] D. G. Lowe, \u201cObject recognition from local scale-invariant features,\u201d in Proceedings of the seventh IEEE international conference on computer vision, vol. 2. Ieee, 1999, pp. 1150\u20131157.", "[28] Y. Zhong, \u201cIntrinsic shape signatures: A shape descriptor for 3d object recognition,\u201d in 2009 IEEE 12th international conference on computer vision workshops, ICCV Workshops. IEEE, 2009, pp. 689\u2013696."]}], "citation_info_to_title": {"[19] J. Li and G. H. Lee, \u201cDeepi2p: Image-to-point cloud registration via deep classification,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15\u2009960\u201315\u2009969.": "Deepi2p: Image-to-point cloud registration via deep classification", "[28] Y. Zhong, \u201cIntrinsic shape signatures: A shape descriptor for 3d object recognition,\u201d in 2009 IEEE 12th international conference on computer vision workshops, ICCV Workshops. IEEE, 2009, pp. 689\u2013696.": "Intrinsic shape signatures: A shape descriptor for 3d object recognition", "[27] D. G. Lowe, \u201cObject recognition from local scale-invariant features,\u201d in Proceedings of the seventh IEEE international conference on computer vision, vol. 2. Ieee, 1999, pp. 1150\u20131157.": "Object recognition from local scale-invariant features"}, "source_title_to_arxiv_id": {"Deepi2p: Image-to-point cloud registration via deep classification": "2104.03501"}}