{"title": "Differentiable Point-Based Radiance Fields for Efficient View Synthesis", "abstract": "We propose a differentiable rendering algorithm for efficient novel view\nsynthesis. By departing from volume-based representations in favor of a learned\npoint representation, we improve on existing methods more than an order of\nmagnitude in memory and runtime, both in training and inference. The method\nbegins with a uniformly-sampled random point cloud and learns per-point\nposition and view-dependent appearance, using a differentiable splat-based\nrenderer to evolve the model to match a set of input images. Our method is up\nto 300x faster than NeRF in both training and inference, with only a marginal\nsacrifice in quality, while using less than 10~MB of memory for a static scene.\nFor dynamic scenes, our method trains two orders of magnitude faster than\nSTNeRF and renders at near interactive rate, while maintaining high image\nquality and temporal coherence even without imposing any temporal-coherency\nregularizers.", "authors": ["Qiang Zhang", "Seung-Hwan Baek", "Szymon Rusinkiewicz", "Felix Heide"], "published_date": "2022_05_28", "pdf_url": "http://arxiv.org/pdf/2205.14330v3", "list_table_and_caption": [{"table": "<table><thead><tr><th rowspan=\"2\">Synthetic Dataset</th><th rowspan=\"2\">Pretraining</th><th rowspan=\"2\">Training</th><th rowspan=\"2\">Inference</th><th rowspan=\"2\">Model Size</th><th colspan=\"3\">Rendering Quality</th></tr><tr><th>PSNR\\uparrow</th><th>SSIM\\uparrow</th><th>LPIPS\\downarrow</th></tr></thead><tbody><tr><td>NeRF [Mildenhall et al., 2020]</td><td>None</td><td>20 h</td><td>1/12 fps</td><td>14 MB</td><td>31.0 dB</td><td>0.947</td><td>0.081</td></tr><tr><td>IBRNet [Wang et al., 2021]</td><td>1 day</td><td>30 min</td><td>1/25 fps</td><td>15 MB</td><td>28.1 dB</td><td>0.942</td><td>0.072</td></tr><tr><td>MVSNeRF [Chenet al., 2021b]</td><td>20 h</td><td>15 min</td><td>1/14 fps</td><td>14 MB</td><td>27.0 dB</td><td>0.931</td><td>0.168</td></tr><tr><td>Plenoxels [Yu et al., 2021a]</td><td>None</td><td>11 min</td><td>15 fps</td><td>1.1 GB</td><td>31.7 dB</td><td>0.958</td><td>0.050</td></tr><tr><td>Plenoxels_s [Yu et al., 2021a]</td><td>None</td><td>8.5 min</td><td>18 fps</td><td>234 MB</td><td>28.5 dB</td><td>0.926</td><td>0.100</td></tr><tr><td>Pulsar [Lassner andZollhofer, 2021]</td><td>None</td><td>95 min</td><td>4 fps</td><td>228 MB</td><td>26.9 dB</td><td>0.923</td><td>0.184</td></tr><tr><td>PBNR [Kopanas et al., 2021]</td><td>None</td><td>3 h</td><td>4 fps</td><td>2.96 GB</td><td>27.4 dB</td><td>0.932</td><td>0.164</td></tr><tr><td>Ours</td><td>None</td><td>3 min</td><td>32 fps</td><td>9 MB</td><td>30.3 dB</td><td>0.945</td><td>0.078</td></tr></tbody></table>", "caption": "Table 1. Static Novel View Synthesis Evaluation on the Synthetic Blender Dataset. We evaluate the proposed method and comparable baseline approaches for novel view synthesis on the static Blender scenes from [Mildenhall et al., 2020]. The proposed model does not require an extra dataset for pretraining and improves on existing methods in training, inference speed, model size, at cost of only a small reduction in quality. Specifically, although the concurrent Plenoxels [2021a] achieves better quality, our model is two magnitudes smaller than theirs. We also compare here to the Plenoxels_s model from [Yu et al., 2021a] (Plenoxels with smaller volume resolution), which achieves worse rendering quality with larger model size.", "list_citation_info": ["Kopanas et al. [2021] Georgios Kopanas, Julien Philip, Thomas Leimk\u00fchler, and George Drettakis. 2021. Point-Based Neural Rendering with Per-View Optimization. In Computer Graphics Forum, Vol. 40. Wiley Online Library, 29\u201343.", "Mildenhall et al. [2020] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Proc. ECCV. Springer, 405\u2013421.", "Wang et al. [2021] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. 2021. IBRNet: Learning Multi-View Image-Based Rendering. In Proc. CVPR.", "Yu et al. [2021a] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks. arXiv:2112.05131.", "Chen et al. [2021b] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. 2021b. MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo. In Proc. ICCV.", "Lassner and Zollhofer [2021] Christoph Lassner and Michael Zollhofer. 2021. Pulsar: Efficient sphere-based neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1440\u20131449."]}, {"table": "<table><thead><tr><th rowspan=\"2\">STNeRF</th><th rowspan=\"2\">Train</th><th rowspan=\"2\">Render</th><th rowspan=\"2\">Model</th><th colspan=\"3\">Rendering Quality</th></tr><tr><th>PSNR\\uparrow</th><th>SSIM\\uparrow</th><th>LPIPS\\downarrow</th></tr></thead><tbody><tr><td>NeRF</td><td>40 h</td><td>1/25 fps</td><td>14 MB</td><td>23.7 dB</td><td>0.853</td><td>0.304</td></tr><tr><td>NeRF-t</td><td>100 h</td><td>1/26 fps</td><td>16 MB</td><td>28.9 dB</td><td>0.913</td><td>0.259</td></tr><tr><td>STNeRF</td><td>50 h</td><td>1/30 fps</td><td>12 MB</td><td>32.1 dB</td><td>0.918</td><td>0.224</td></tr><tr><td>Ours</td><td>30 min</td><td>25 fps</td><td>110 MB</td><td>34.6 dB</td><td>0.927</td><td>0.207</td></tr></tbody></table>", "caption": "Table 2. Quantitative Evaluation on the STNeRF Dataset. We report rendering quality, memory footprint, rendering and training time for novel video view synthesis on the STNeRF datset[2021]. Compared to STNeRF[2021] and the NeRF variants suggested in the same work, the training speed and inference speed of the proposed method is two orders of magnitude higher. ", "list_citation_info": ["Zhang et al. [2021] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. 2021. Editable Free-Viewpoint Video using a Layered Neural Representation. ACM Trans. Graphics 40, 4 (2021), 1\u201318."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Module</th><th colspan=\"3\">Rendering Quality</th></tr><tr><th>PSNR\\uparrow</th><th>SSIM\\uparrow</th><th>LPIPS\\downarrow</th></tr></thead><tbody><tr><th>w/o sh</th><td>27.1 dB</td><td>0.932</td><td>0.155</td></tr><tr><th>w/o hybrid</th><td>26.6 dB</td><td>0.925</td><td>0.163</td></tr><tr><th>w/o filter</th><td>29.1 dB</td><td>0.941</td><td>0.103</td></tr><tr><th>Ours</th><td>30.3 dB</td><td>0.945</td><td>0.078</td></tr></tbody></table><p>.</p>", "caption": "Table 3. Model Ablation Experiments. We evaluate the rendering quality of our method on the Blender dataset [2020] when gradually removing components from the rendering pipeline. Specifically, we ablate the spherical harmonics model per point, the coarse-to-fine strategy, and the filtering function for the training. The experiments results validate that all components contribute to the rendering quality.", "list_citation_info": ["Mildenhall et al. [2020] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Proc. ECCV. Springer, 405\u2013421."]}, {"table": "<table><thead><tr><th>STNeRF</th><th>Train</th><th>Render</th><th>Model</th></tr></thead><tbody><tr><td>NeRF</td><td>50 h</td><td>1/15 fps</td><td>14 MB</td></tr><tr><td>NeRF-t</td><td>75 h</td><td>1/15 fps</td><td>16 MB</td></tr><tr><td>STNeRF</td><td>86 h</td><td>1/18 fps</td><td>12 MB</td></tr><tr><td>Ours</td><td>1 h</td><td>28 fps</td><td>240 MB</td></tr></tbody></table>", "caption": "Table 5. Quantitative Evaluation on the DSC Dataset. We report the training time, rendering speed and the model size required to represent scenes from the DSC[Vlasic et al., 2009] dataset with the proposed method. Note that we train a radiance point cloud for each frame here and we report the total memory consumption.", "list_citation_info": ["Vlasic et al. [2009] Daniel Vlasic, Pieter Peers, Ilya Baran, Paul Debevec, Jovan Popovi\u0107, Szymon Rusinkiewicz, and Wojciech Matusik. 2009. Dynamic Shape Capture using Multi-View Photometric Stereo. ACM Trans. Graphics 28, 5 (Dec. 2009)."]}], "citation_info_to_title": {"Yu et al. [2021a] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks. arXiv:2112.05131.": "Plenoxels: Radiance Fields without Neural Networks", "Chen et al. [2021b] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. 2021b. MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo. In Proc. ICCV.": "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo", "Vlasic et al. [2009] Daniel Vlasic, Pieter Peers, Ilya Baran, Paul Debevec, Jovan Popovi\u0107, Szymon Rusinkiewicz, and Wojciech Matusik. 2009. Dynamic Shape Capture using Multi-View Photometric Stereo. ACM Trans. Graphics 28, 5 (Dec. 2009).": "Dynamic Shape Capture using Multi-View Photometric Stereo", "Wang et al. [2021] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. 2021. IBRNet: Learning Multi-View Image-Based Rendering. In Proc. CVPR.": "IBRNet: Learning Multi-View Image-Based Rendering", "Zhang et al. [2021] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. 2021. Editable Free-Viewpoint Video using a Layered Neural Representation. ACM Trans. Graphics 40, 4 (2021), 1\u201318.": "Editable Free-Viewpoint Video using a Layered Neural Representation", "Mildenhall et al. [2020] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Proc. ECCV. Springer, 405\u2013421.": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "Kopanas et al. [2021] Georgios Kopanas, Julien Philip, Thomas Leimk\u00fchler, and George Drettakis. 2021. Point-Based Neural Rendering with Per-View Optimization. In Computer Graphics Forum, Vol. 40. Wiley Online Library, 29\u201343.": "Point-Based Neural Rendering with Per-View Optimization", "Lassner and Zollhofer [2021] Christoph Lassner and Michael Zollhofer. 2021. Pulsar: Efficient sphere-based neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1440\u20131449.": "Pulsar: Efficient sphere-based neural rendering"}, "source_title_to_arxiv_id": {"Plenoxels: Radiance Fields without Neural Networks": "2112.05131", "IBRNet: Learning Multi-View Image-Based Rendering": "2102.13090"}}