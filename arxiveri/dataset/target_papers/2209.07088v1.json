{"title": "Self-distilled Feature Aggregation for Self-supervised Monocular Depth Estimation", "abstract": "Self-supervised monocular depth estimation has received much attention\nrecently in computer vision. Most of the existing works in literature aggregate\nmulti-scale features for depth prediction via either straightforward\nconcatenation or element-wise addition, however, such feature aggregation\noperations generally neglect the contextual consistency between multi-scale\nfeatures. Addressing this problem, we propose the Self-Distilled Feature\nAggregation (SDFA) module for simultaneously aggregating a pair of low-scale\nand high-scale features and maintaining their contextual consistency. The SDFA\nemploys three branches to learn three feature offset maps respectively: one\noffset map for refining the input low-scale feature and the other two for\nrefining the input high-scale feature under a designed self-distillation\nmanner. Then, we propose an SDFA-based network for self-supervised monocular\ndepth estimation, and design a self-distilled training strategy to train the\nproposed network with the SDFA module. Experimental results on the KITTI\ndataset demonstrate that the proposed method outperforms the comparative\nstate-of-the-art methods in most cases. The code is available at\nhttps://github.com/ZM-Zhou/SDFA-Net_pytorch.", "authors": ["Zhengming Zhou", "Qiulei Dong"], "published_date": "2022_09_15", "pdf_url": "http://arxiv.org/pdf/2209.07088v1", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Method</td><td>PP</td><td>Sup.</td><td>Data.</td><td>Resolution</td><td>Abs Rel \\downarrow</td><td>Sq Rel \\downarrow</td><td>RMSE \\downarrow</td><td>logRMSE \\downarrow</td><td>A1 \\uparrow</td><td>A2 \\uparrow</td><td>A3 \\uparrow</td></tr><tr><td colspan=\"12\">Raw Eigen test set</td></tr><tr><td>R-MSFM6 [59]</td><td></td><td>M</td><td>K</td><td>320x1024</td><td>0.108</td><td>0.748</td><td>4.470</td><td>0.185</td><td>0.889</td><td>0.963</td><td>0.982</td></tr><tr><td>PackNet [20]</td><td></td><td>M</td><td>K</td><td>384x1280</td><td>0.107</td><td>0.802</td><td>4.538</td><td>0.186</td><td>0.889</td><td>0.962</td><td>0.981</td></tr><tr><td>SGDepth [34]</td><td></td><td>M(s)</td><td>K</td><td>384x1280</td><td>0.107</td><td>0.768</td><td>4.468</td><td>0.186</td><td>0.891</td><td>0.963</td><td>0.982</td></tr><tr><td>FeatureNet [46]</td><td></td><td>M</td><td>K</td><td>320x1024</td><td>0.104</td><td>0.729</td><td>4.481</td><td>0.179</td><td>0.893</td><td>0.965</td><td>0.984</td></tr><tr><td>monoResMatch [48]</td><td>\u2713</td><td>S(d)</td><td>K</td><td>384x1280</td><td>0.111</td><td>0.867</td><td>4.714</td><td>0.199</td><td>0.864</td><td>0.954</td><td>0.979</td></tr><tr><td>Monodepth2 [15]</td><td>\u2713</td><td>S</td><td>K</td><td>320x1024</td><td>0.105</td><td>0.822</td><td>4.692</td><td>0.199</td><td>0.876</td><td>0.954</td><td>0.977</td></tr><tr><td>DepthHints [54]</td><td>\u2713</td><td>S(d)</td><td>K</td><td>320x1024</td><td>0.096</td><td>0.710</td><td>4.393</td><td>0.185</td><td>0.890</td><td>0.962</td><td>0.981</td></tr><tr><td>DBoosterNet-e [18]</td><td></td><td>S</td><td>K</td><td>384x1280</td><td>0.095</td><td>0.636</td><td>4.105</td><td>0.178</td><td>0.890</td><td>0.963</td><td>0.984</td></tr><tr><td>SingleNet [3]</td><td>\u2713</td><td>S</td><td>K</td><td>320x1024</td><td>0.094</td><td>0.681</td><td>4.392</td><td>0.185</td><td>0.892</td><td>0.962</td><td>0.981</td></tr><tr><td>FAL-Net [16]</td><td>\u2713</td><td>S</td><td>K</td><td>384x1280</td><td>0.093</td><td>0.564</td><td>3.973</td><td>0.174</td><td>0.898</td><td>0.967</td><td>0.985</td></tr><tr><td>Edge-of-depth [60]</td><td>\u2713</td><td>S(s,d)</td><td>K</td><td>320x1024</td><td>0.091</td><td>0.646</td><td>4.244</td><td>0.177</td><td>0.898</td><td>0.966</td><td>0.983</td></tr><tr><td>PLADE-Net [17]</td><td>\u2713</td><td>S</td><td>K</td><td>384x1280</td><td>0.089</td><td>0.590</td><td>4.008</td><td>0.172</td><td>0.900</td><td>0.967</td><td>0.985</td></tr><tr><td>EPCDepth [41]</td><td>\u2713</td><td>S(d)</td><td>K</td><td>320x1024</td><td>0.091</td><td>0.646</td><td>4.207</td><td>0.176</td><td>0.901</td><td>0.966</td><td>0.983</td></tr><tr><td>SDFA-Net(Ours)</td><td></td><td>S</td><td>K</td><td>384x1280</td><td>0.090</td><td>0.538</td><td>3.896</td><td>0.169</td><td>0.906</td><td>0.969</td><td>0.985</td></tr><tr><td>SDFA-Net(Ours)</td><td>\u2713</td><td>S</td><td>K</td><td>384x1280</td><td>0.089</td><td>0.531</td><td>3.864</td><td>0.168</td><td>0.907</td><td>0.969</td><td>0.985</td></tr><tr><td>PackNet [20]</td><td></td><td>M</td><td>CS+K</td><td>384x1280</td><td>0.104</td><td>0.758</td><td>4.386</td><td>0.182</td><td>0.895</td><td>0.964</td><td>0.982</td></tr><tr><td>SemanticGuide [21]</td><td></td><td>M(s)</td><td>CS+K</td><td>384x1280</td><td>0.100</td><td>0.761</td><td>4.270</td><td>0.175</td><td>0.902</td><td>0.965</td><td>0.982</td></tr><tr><td>monoResMatch [48]</td><td>\u2713</td><td>S(d)</td><td>CS+K</td><td>384x1280</td><td>0.096</td><td>0.673</td><td>4.351</td><td>0.184</td><td>0.890</td><td>0.961</td><td>0.981</td></tr><tr><td>DBoosterNet-e [18]</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.086</td><td>0.538</td><td>3.852</td><td>0.168</td><td>0.905</td><td>0.969</td><td>0.985</td></tr><tr><td>FAL-Net [16]</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.088</td><td>0.547</td><td>4.004</td><td>0.175</td><td>0.898</td><td>0.966</td><td>0.984</td></tr><tr><td>PLADE-Net [17]</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.087</td><td>0.550</td><td>3.837</td><td>0.167</td><td>0.908</td><td>0.970</td><td>0.985</td></tr><tr><td>SDFA-Net(Ours)</td><td></td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.085</td><td>0.531</td><td>3.888</td><td>0.167</td><td>0.911</td><td>0.969</td><td>0.985</td></tr><tr><td>SDFA-Net(Ours)</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.084</td><td>0.523</td><td>3.833</td><td>0.166</td><td>0.913</td><td>0.970</td><td>0.985</td></tr><tr><td colspan=\"12\">Improved Eigen test set</td></tr><tr><td>DepthHints [54]</td><td>\u2713</td><td>S(d)</td><td>K</td><td>320x1024</td><td>0.074</td><td>0.364</td><td>3.202</td><td>0.114</td><td>0.936</td><td>0.989</td><td>0.997</td></tr><tr><td>WaveletMonoDepth [44]</td><td>\u2713</td><td>S(d)</td><td>K</td><td>320x1024</td><td>0.074</td><td>0.357</td><td>3.170</td><td>0.114</td><td>0.936</td><td>0.989</td><td>0.997</td></tr><tr><td>DBoosterNet-e [18]</td><td></td><td>S</td><td>K</td><td>384x1280</td><td>0.070</td><td>0.298</td><td>2.916</td><td>0.109</td><td>0.940</td><td>0.991</td><td>0.998</td></tr><tr><td>FAL-Net [16]</td><td>\u2713</td><td>S</td><td>K</td><td>384x1280</td><td>0.071</td><td>0.281</td><td>2.912</td><td>0.108</td><td>0.943</td><td>0.991</td><td>0.998</td></tr><tr><td>PLADE-Net [17]</td><td>\u2713</td><td>S</td><td>K</td><td>384x1280</td><td>0.066</td><td>0.272</td><td>2.918</td><td>0.104</td><td>0.945</td><td>0.992</td><td>0.998</td></tr><tr><td>SDFA-Net(Ours)</td><td>\u2713</td><td>S</td><td>K</td><td>384x1280</td><td>0.074</td><td>0.228</td><td>2.547</td><td>0.101</td><td>0.956</td><td>0.995</td><td>0.999</td></tr><tr><td>PackNet [20]</td><td></td><td>M</td><td>CS+K</td><td>384x1280</td><td>0.071</td><td>0.359</td><td>3.153</td><td>0.109</td><td>0.944</td><td>0.990</td><td>0.997</td></tr><tr><td>DBoosterNet-e [18]</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.062</td><td>0.242</td><td>2.617</td><td>0.096</td><td>0.955</td><td>0.994</td><td>0.998</td></tr><tr><td>FAL-Net [16]</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.068</td><td>0.276</td><td>2.906</td><td>0.106</td><td>0.944</td><td>0.991</td><td>0.998</td></tr><tr><td>PLADE-Net [17]</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.065</td><td>0.253</td><td>2.710</td><td>0.100</td><td>0.950</td><td>0.992</td><td>0.998</td></tr><tr><td>SDFA-Net(Ours)</td><td>\u2713</td><td>S</td><td>CS+K</td><td>384x1280</td><td>0.069</td><td>0.207</td><td>2.472</td><td>0.096</td><td>0.963</td><td>0.995</td><td>0.999</td></tr></tbody></table>", "caption": "Table 1: Quantitative comparison on both the raw and improved KITTI Eigen test sets.\\downarrow/\\uparrow denotes that lower / higher is better.The best and the second best results are in bold and underlined in each metric.", "list_citation_info": ["[34] Klingner, M., Term\u00f6hlen, J.A., Mikolajczyk, J., Fingscheidt, T.: Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance. In: ECCV. pp. 582\u2013600 (2020)", "[17] GonzalezBello, J.L., Kim, M.: Plade-net: Towards pixel-level accuracy for self-supervised single-view depth estimation with neural positional encoding and distilled matting loss. In: CVPR. pp. 6851\u20136860 (2021)", "[59] Zhou, Z., Fan, X., Shi, P., Xin, Y.: R-msfm: Recurrent multi-scale feature modulation for monocular depth estimating. In: ICCV. pp. 12777\u201312786 (2021)", "[18] GonzalezBello, J.L., Kim, M.: Self-supervised deep monocular depth estimation with ambiguity boosting. IEEE TPAMI (2021)", "[60] Zhu, S., Brazil, G., Liu, X.: The edge of depth: Explicit constraints between segmentation and depth. In: CVPR. pp. 13116\u201313125 (2020)", "[3] Chen, Z., Ye, X., Yang, W., Xu, Z., Tan, X., Zou, Z., Ding, E., Zhang, X., Huang, L.: Revealing the reciprocal relations between self-supervised stereo and monocular depth estimation. In: ICCV. pp. 15529\u201315538 (2021)", "[41] Peng, R., Wang, R., Lai, Y., Tang, L., Cai, Y.: Excavating the potential capacity of self-supervised monocular depth estimation. In: ICCV. pp. 15560\u201315569 (2021)", "[21] Guizilini, V., Hou, R., Li, J., Ambrus, R., Gaidon, A.: Semantically-guided representation learning for self-supervised monocular depth. In: International Conference on Learning Representations (ICLR) (2020)", "[44] Ramamonjisoa, M., Firman, M., Watson, J., Lepetit, V., Turmukhambetov, D.: Single image depth prediction with wavelet decomposition. In: CVPR. pp. 11089\u201311098 (2021)", "[15] Godard, C., Mac Aodha, O., Firman, M., Brostow, G.J.: Digging into self-supervised monocular depth estimation. In: ICCV. pp. 3828\u20133838 (2019)", "[20] Guizilini, V., Ambrus, R., Pillai, S., Raventos, A., Gaidon, A.: 3d packing for self-supervised monocular depth estimation. In: CVPR. pp. 2485\u20132494 (2020)", "[48] Tosi, F., Aleotti, F., Poggi, M., Mattoccia, S.: Learning monocular depth estimation infusing traditional stereo knowledge. In: CVPR. pp. 9799\u20139809 (2019)", "[16] GonzalezBello, J.L., Kim, M.: Forget about the lidar: Self-supervised depth estimators with med probability volumes. Advances in Neural Information Processing Systems 33, 12626\u201312637 (2020)", "[54] Watson, J., Firman, M., Brostow, G.J., Turmukhambetov, D.: Self-supervised monocular depth hints. In: ICCV (2019)", "[46] Shu, C., Yu, K., Duan, Z., Yang, K.: Feature-metric loss for self-supervised learning of depth and egomotion. In: ECCV. pp. 572\u2013588 (2020)"]}], "citation_info_to_title": {"[21] Guizilini, V., Hou, R., Li, J., Ambrus, R., Gaidon, A.: Semantically-guided representation learning for self-supervised monocular depth. In: International Conference on Learning Representations (ICLR) (2020)": "Semantically-guided representation learning for self-supervised monocular depth", "[15] Godard, C., Mac Aodha, O., Firman, M., Brostow, G.J.: Digging into self-supervised monocular depth estimation. In: ICCV. pp. 3828\u20133838 (2019)": "Digging into self-supervised monocular depth estimation", "[18] GonzalezBello, J.L., Kim, M.: Self-supervised deep monocular depth estimation with ambiguity boosting. IEEE TPAMI (2021)": "Self-supervised deep monocular depth estimation with ambiguity boosting", "[44] Ramamonjisoa, M., Firman, M., Watson, J., Lepetit, V., Turmukhambetov, D.: Single image depth prediction with wavelet decomposition. In: CVPR. pp. 11089\u201311098 (2021)": "Single image depth prediction with wavelet decomposition", "[20] Guizilini, V., Ambrus, R., Pillai, S., Raventos, A., Gaidon, A.: 3d packing for self-supervised monocular depth estimation. In: CVPR. pp. 2485\u20132494 (2020)": "3D Packing for Self-Supervised Monocular Depth Estimation", "[3] Chen, Z., Ye, X., Yang, W., Xu, Z., Tan, X., Zou, Z., Ding, E., Zhang, X., Huang, L.: Revealing the reciprocal relations between self-supervised stereo and monocular depth estimation. In: ICCV. pp. 15529\u201315538 (2021)": "Revealing the reciprocal relations between self-supervised stereo and monocular depth estimation", "[48] Tosi, F., Aleotti, F., Poggi, M., Mattoccia, S.: Learning monocular depth estimation infusing traditional stereo knowledge. In: CVPR. pp. 9799\u20139809 (2019)": "Learning Monocular Depth Estimation Infusing Traditional Stereo Knowledge", "[17] GonzalezBello, J.L., Kim, M.: Plade-net: Towards pixel-level accuracy for self-supervised single-view depth estimation with neural positional encoding and distilled matting loss. In: CVPR. pp. 6851\u20136860 (2021)": "Plade-net: Towards pixel-level accuracy for self-supervised single-view depth estimation with neural positional encoding and distilled matting loss", "[16] GonzalezBello, J.L., Kim, M.: Forget about the lidar: Self-supervised depth estimators with med probability volumes. Advances in Neural Information Processing Systems 33, 12626\u201312637 (2020)": "Forget about the lidar: Self-supervised depth estimators with med probability volumes", "[54] Watson, J., Firman, M., Brostow, G.J., Turmukhambetov, D.: Self-supervised monocular depth hints. In: ICCV (2019)": "Self-supervised Monocular Depth Hints", "[46] Shu, C., Yu, K., Duan, Z., Yang, K.: Feature-metric loss for self-supervised learning of depth and egomotion. In: ECCV. pp. 572\u2013588 (2020)": "Feature-metric loss for self-supervised learning of depth and egomotion", "[34] Klingner, M., Term\u00f6hlen, J.A., Mikolajczyk, J., Fingscheidt, T.: Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance. In: ECCV. pp. 582\u2013600 (2020)": "Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance", "[41] Peng, R., Wang, R., Lai, Y., Tang, L., Cai, Y.: Excavating the potential capacity of self-supervised monocular depth estimation. In: ICCV. pp. 15560\u201315569 (2021)": "Excavating the potential capacity of self-supervised monocular depth estimation", "[59] Zhou, Z., Fan, X., Shi, P., Xin, Y.: R-msfm: Recurrent multi-scale feature modulation for monocular depth estimating. In: ICCV. pp. 12777\u201312786 (2021)": "R-msfm: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating", "[60] Zhu, S., Brazil, G., Liu, X.: The edge of depth: Explicit constraints between segmentation and depth. In: CVPR. pp. 13116\u201313125 (2020)": "The title of this paper is The edge of depth: Explicit constraints between segmentation and depth"}, "source_title_to_arxiv_id": {"Feature-metric loss for self-supervised learning of depth and egomotion": "2007.10603", "Excavating the potential capacity of self-supervised monocular depth estimation": "2109.12484"}}