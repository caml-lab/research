{"title": "A Thousand Words Are Worth More Than a Picture: Natural Language-Centric Outside-Knowledge Visual Question Answering", "abstract": "Outside-knowledge visual question answering (OK-VQA) requires the agent to\ncomprehend the image, make use of relevant knowledge from the entire web, and\ndigest all the information to answer the question. Most previous works address\nthe problem by first fusing the image and question in the multi-modal space,\nwhich is inflexible for further fusion with a vast amount of external\nknowledge. In this paper, we call for a paradigm shift for the OK-VQA task,\nwhich transforms the image into plain text, so that we can enable knowledge\npassage retrieval, and generative question-answering in the natural language\nspace. This paradigm takes advantage of the sheer volume of gigantic knowledge\nbases and the richness of pre-trained language models. A\nTransform-Retrieve-Generate framework (TRiG) framework is proposed, which can\nbe plug-and-played with alternative image-to-text models and textual knowledge\nbases. Experimental results show that our TRiG framework outperforms all\nstate-of-the-art supervised methods by at least 11.1% absolute margin.", "authors": ["Feng Gao", "Qing Ping", "Govind Thattai", "Aishwarya Reganti", "Ying Nian Wu", "Prem Natarajan"], "published_date": "2022_01_14", "pdf_url": "http://arxiv.org/pdf/2201.05299v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Model</th><td>EM</td><td>VQA Score</td></tr><tr><th>SOTA Methods</th><td></td><td></td></tr><tr><th>KRISP[38]</th><td></td><td>32.31</td></tr><tr><th>ConceptBert[12]</th><td></td><td>33.66</td></tr><tr><th>CBM[46]</th><td></td><td>38.60</td></tr><tr><th>KRISP w/ VQA2.0 pretrained</th><td></td><td>38.70</td></tr><tr><th>MAVEx[60]</th><td></td><td>38.70</td></tr><tr><th>RVLESK[48]</th><td></td><td>39.04</td></tr><tr><th>Weakly Supervised VRR[36]</th><td></td><td>39.20</td></tr><tr><th>MAVEx w/(Ensemble 5) [60]</th><td></td><td>39.40</td></tr><tr><th>Ours</th><td></td><td></td></tr><tr><th>TRiG w/ Q+C+DL+O, G</th><td>53.62%</td><td>49.24</td></tr><tr><th>TRiG w/ Q+C+DL+O, BS</th><td>53.59%</td><td>49.35</td></tr><tr><th>TRiG w/ Q+C+DL+O, G, \\mathbf{E}^{*}</th><td>54.73%</td><td>50.50</td></tr></tbody></table>", "caption": "Table 2: Comparison of supervised-learning methods on the OK-VQA dataset. In TRiG Model, Q: Question, C: Caption, DL: Dense Labels, O: OCR Text, G: Greedy Decode, BS: Beam-Search, \\mathbf{E}^{*}: Ensembles of the 6 TRiG models.", "list_citation_info": ["[46] Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, and Eneko Agirre. Image captioning for effective use of language models in knowledge-based visual question answering. arXiv preprint arXiv:2109.08029, 2021.", "[48] Violetta Shevchenko, Damien Teney, Anthony Dick, and Anton van den Hengel. Reasoning over vision and language: Exploring the benefits of supplemental knowledge. arXiv preprint arXiv:2101.06013, 2021.", "[60] Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. Multi-modal answer validation for knowledge-based vqa. arXiv preprint arXiv:2103.12248, 2021.", "[38] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14111\u201314121, 2021.", "[12] Fran\u00e7ois Garderes, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. Conceptbert: Concept-aware representation for visual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 489\u2013498, 2020.", "[36] Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. Weakly-supervised visual-retriever-reader for knowledge-based question answering. arXiv preprint arXiv:2109.04014, 2021."]}, {"table": "<table><tbody><tr><th>Model</th><td>#Params</td><td>VQA Score</td></tr><tr><th>SOTA Prompt Method[63]</th><td></td><td></td></tr><tr><th>PICa w/16 RP C+T</th><td>175B</td><td>43.30</td></tr><tr><th>PICa w/16 SP C+T</th><td>175B</td><td>46.50</td></tr><tr><th>PICa w/16 SP C+T, 3\\times\\text{E}</th><td>175B</td><td>47.70</td></tr><tr><th>PICa w/16 SP C+T, 5\\times\\text{E}</th><td>175B</td><td>48.00</td></tr><tr><th>Ours</th><td></td><td></td></tr><tr><th>TRiG w/ Q+C+DL+O, G</th><td>0.77B</td><td>49.24</td></tr><tr><th>TRiG w/ Q+C+DL+O, BS</th><td>0.77B</td><td>49.35</td></tr><tr><th>TRiG w/ Q+C+DL+O, G, \\mathbf{E}^{*}</th><td>0.77B</td><td>50.50</td></tr></tbody></table>", "caption": "Table 3: Comparison of Proposed TRiG with SOTA Prompt-Based Method on the OK-VQA Dataset. In[63], RP: Random Prompt, SP Selected Prompt, C: Caption, T: Image-Tagging, E: Prompt Ensemble. In TRiG model, Q: Question, C: Caption, DL: Dense Labels, O: OCR Text, G: Greedy Decode, BS: Beam-Search, \\mathbf{E}^{*}: Ensembles of the 6 TRiG models.", "list_citation_info": ["[63] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. arXiv preprint arXiv:2109.05014, 2021."]}, {"table": "<table><thead><tr><th>Inputs</th><th>VQA Score</th></tr></thead><tbody><tr><th>Question + K + C</th><td>42.54</td></tr><tr><th>Question + K + C + L</th><td>42.94</td></tr><tr><th>Question + K + C + L + O</th><td>43.53</td></tr><tr><th>Question + K + C + DL + O</th><td>49.35</td></tr></tbody></table>", "caption": "Table 4: Ablation Study of the Different Variants of Text Input into the Generative QA Model (K: Knowledge passages, C: Caption, L = Bottom-Up Labels[1], DL = Dense Labels, O = OCR Text).", "list_citation_info": ["[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086, 2018."]}], "citation_info_to_title": {"[38] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14111\u201314121, 2021.": "Krisp: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa", "[60] Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. Multi-modal answer validation for knowledge-based vqa. arXiv preprint arXiv:2103.12248, 2021.": "Multi-modal answer validation for knowledge-based vqa", "[63] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. arXiv preprint arXiv:2109.05014, 2021.": "An empirical study of GPT-3 for few-shot knowledge-based VQA", "[48] Violetta Shevchenko, Damien Teney, Anthony Dick, and Anton van den Hengel. Reasoning over vision and language: Exploring the benefits of supplemental knowledge. arXiv preprint arXiv:2101.06013, 2021.": "Reasoning over vision and language: Exploring the benefits of supplemental knowledge", "[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086, 2018.": "Bottom-up and top-down attention for image captioning and visual question answering", "[12] Fran\u00e7ois Garderes, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. Conceptbert: Concept-aware representation for visual question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 489\u2013498, 2020.": "Conceptbert: Concept-aware representation for visual question answering", "[46] Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa, and Eneko Agirre. Image captioning for effective use of language models in knowledge-based visual question answering. arXiv preprint arXiv:2109.08029, 2021.": "Image captioning for effective use of language models in knowledge-based visual question answering", "[36] Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. Weakly-supervised visual-retriever-reader for knowledge-based question answering. arXiv preprint arXiv:2109.04014, 2021.": "Weakly-supervised visual-retriever-reader for knowledge-based question answering"}, "source_title_to_arxiv_id": {"Image captioning for effective use of language models in knowledge-based visual question answering": "2109.08029"}}