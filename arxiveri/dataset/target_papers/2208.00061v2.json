{"title": "UAVM: Towards Unifying Audio and Visual Models", "abstract": "Conventional audio-visual models have independent audio and video branches.\nIn this work, we unify the audio and visual branches by designing a Unified\nAudio-Visual Model (UAVM). The UAVM achieves a new state-of-the-art\naudio-visual event classification accuracy of 65.8% on VGGSound. More\ninterestingly, we also find a few intriguing properties of UAVM that the\nmodality-independent counterparts do not have.", "authors": ["Yuan Gong", "Alexander H. Liu", "Andrew Rouditchenko", "James Glass"], "published_date": "2022_07_29", "pdf_url": "http://arxiv.org/pdf/2208.00061v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th>VGGSound (Top-1 Accuracy, %)</th><td>Audio</td><td>Video</td><td>Fusion</td></tr><tr><th>Chen et al. [19]</th><td>48.8</td><td>-</td><td>-</td></tr><tr><th>AudioSlowFast [27]</th><td>50.1</td><td>-</td><td>-</td></tr><tr><th>MBT [22]</th><td>52.3{}^{\\ast}</td><td>51.2{}^{\\ast}</td><td>64.1</td></tr><tr><th>Our Cross-Modal Attention Model</th><td>-</td><td>-</td><td>62.9\\pm0.2</td></tr><tr><th>Our Modal-Independent Model</th><td>56.5\\pm0.1{}^{\\ast}</td><td>49.7\\pm0.2{}^{\\ast}</td><td>65.7\\pm0.2</td></tr><tr><th>Our UAVM Model</th><td>56.5\\pm0.1{}^{\\dagger}</td><td>49.9\\pm0.2{}^{\\dagger}</td><td>65.8\\pm0.1</td></tr><tr><th>Full AudioSet (mAP)</th><td>Audio</td><td>Video</td><td>Fusion</td></tr><tr><th>GBlend [28]</th><td>32.4{}^{\\ast}</td><td>18.8{}^{\\ast}</td><td>41.8</td></tr><tr><th>Attn Audio-Visual [29]</th><td>38.4{}^{\\ast}</td><td>25.7{}^{\\ast}</td><td>46.2</td></tr><tr><th>Perceiver [12]</th><td>38.4{}^{\\ast}</td><td>25.8{}^{\\ast}</td><td>44.2</td></tr><tr><th>MBT [22] (w/ 500k training samples)</th><td>44.3{}^{\\ast}</td><td>32.3{}^{\\ast}</td><td>52.1</td></tr><tr><th>Our Cross-Modal Attention Model</th><td>-</td><td>-</td><td>50.4\\pm0.1</td></tr><tr><th>Our Modal-Independent Model</th><td>45.5\\pm0.0{}^{\\ast}</td><td>26.8\\pm0.1{}^{\\ast}</td><td>48.1\\pm0.1</td></tr><tr><th>Our UAVM Model</th><td>45.6\\pm0.0{}^{\\dagger}</td><td>27.4\\pm0.1{}^{\\dagger}</td><td>48.0\\pm0.0</td></tr></tbody></table>", "caption": "TABLE I: Model Performance Comparison on VGGSound and AudioSet. ({}^{\\ast} single-modal model trained independently. <br/>{}^{\\dagger} modality-missing results of a multi-modal model. )", "list_citation_info": ["[19] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, \u201cVggsound: A large-scale audio-visual dataset,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 721\u2013725.", "[22] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun, \u201cAttention bottlenecks for multimodal fusion,\u201d Advances in Neural Information Processing Systems, pp. 14\u2009200\u201314\u2009213, 2021.", "[29] H. M. Fayek and A. Kumar, \u201cLarge scale audiovisual learning of sounds with weakly labeled data,\u201d in International Joint Conferences on Artificial Intelligence, 2021, pp. 558\u2013565.", "[27] E. Kazakos, A. Nagrani, A. Zisserman, and D. Damen, \u201cSlow-fast auditory streams for audio recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 855\u2013859.", "[28] W. Wang, D. Tran, and M. Feiszli, \u201cWhat makes training multi-modal classification networks hard?\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009695\u201312\u2009705.", "[12] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira, \u201cPerceiver: General perception with iterative attention,\u201d in International conference on machine learning, 2021, pp. 4651\u20134664."]}], "citation_info_to_title": {"[28] W. Wang, D. Tran, and M. Feiszli, \u201cWhat makes training multi-modal classification networks hard?\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 12\u2009695\u201312\u2009705.": "What makes training multi-modal classification networks hard?", "[22] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun, \u201cAttention bottlenecks for multimodal fusion,\u201d Advances in Neural Information Processing Systems, pp. 14\u2009200\u201314\u2009213, 2021.": "Attention bottlenecks for multimodal fusion", "[12] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira, \u201cPerceiver: General perception with iterative attention,\u201d in International conference on machine learning, 2021, pp. 4651\u20134664.": "Perceiver: General perception with iterative attention", "[27] E. Kazakos, A. Nagrani, A. Zisserman, and D. Damen, \u201cSlow-fast auditory streams for audio recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 855\u2013859.": "Slow-fast auditory streams for audio recognition", "[29] H. M. Fayek and A. Kumar, \u201cLarge scale audiovisual learning of sounds with weakly labeled data,\u201d in International Joint Conferences on Artificial Intelligence, 2021, pp. 558\u2013565.": "Large scale audiovisual learning of sounds with weakly labeled data", "[19] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, \u201cVggsound: A large-scale audio-visual dataset,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 721\u2013725.": "Vggsound: A large-scale audio-visual dataset"}, "source_title_to_arxiv_id": {"Vggsound: A large-scale audio-visual dataset": "2004.14368"}}