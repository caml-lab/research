{"title": "You Should Look at All Objects", "abstract": "Feature pyramid network (FPN) is one of the key components for object\ndetectors. However, there is a long-standing puzzle for researchers that the\ndetection performance of large-scale objects are usually suppressed after\nintroducing FPN. To this end, this paper first revisits FPN in the detection\nframework and reveals the nature of the success of FPN from the perspective of\noptimization. Then, we point out that the degraded performance of large-scale\nobjects is due to the arising of improper back-propagation paths after\nintegrating FPN. It makes each level of the backbone network only has the\nability to look at the objects within a certain scale range. Based on these\nanalysis, two feasible strategies are proposed to enable each level of the\nbackbone to look at all objects in the FPN-based detection frameworks.\nSpecifically, one is to introduce auxiliary objective functions to make each\nbackbone level directly receive the back-propagation signals of various-scale\nobjects during training. The other is to construct the feature pyramid in a\nmore reasonable way to avoid the irrational back-propagation paths. Extensive\nexperiments on the COCO benchmark validate the soundness of our analysis and\nthe effectiveness of our methods. Without bells and whistles, we demonstrate\nthat our method achieves solid improvements (more than 2%) on various detection\nframeworks: one-stage, two-stage, anchor-based, anchor-free and\ntransformer-based detectors.", "authors": ["Zhenchao Jin", "Dongdong Yu", "Luchuan Song", "Zehuan Yuan", "Lequan Yu"], "published_date": "2022_07_16", "pdf_url": "http://arxiv.org/pdf/2207.07889v1", "list_table_and_caption": [{"table": "<table><tbody><tr><th>Method</th><th>Backbone</th><th>Schedule</th><td>AP</td><td>AP{}_{50}</td><td>AP{}_{75}</td><td>AP{}_{S}</td><td>AP{}_{M}</td><td>AP{}_{L}</td></tr><tr><th>one-stage</th><th></th><th></th><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>RetinaNet [22]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>38.5</td><td>57.6</td><td>41.0</td><td>21.7</td><td>42.8</td><td>50.4</td></tr><tr><th>RetinaNet [22]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>41.2 (+2.7)</td><td>60.8</td><td>43.8</td><td>24.1</td><td>45.2</td><td>55.2</td></tr><tr><th>FreeAnchor [46]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>40.3</td><td>59.0</td><td>43.1</td><td>21.8</td><td>44.0</td><td>54.2</td></tr><tr><th>FreeAnchor [46]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>43.2 (+2.9)</td><td>62.0</td><td>46.3</td><td>24.4</td><td>47.4</td><td>57.6</td></tr><tr><th>ATSS [45]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>41.5</td><td>59.9</td><td>45.2</td><td>24.2</td><td>45.9</td><td>53.3</td></tr><tr><th>ATSS [45]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>43.8 (+2.3)</td><td>62.1</td><td>47.3</td><td>26.8</td><td>48.0</td><td>57.2</td></tr><tr><th>two-stage</th><th></th><th></th><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>Faster R-CNN [31]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>39.4</td><td>60.1</td><td>43.1</td><td>22.4</td><td>43.7</td><td>51.1</td></tr><tr><th>Faster R-CNN [31]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>42.2 (+2.8)</td><td>63.0</td><td>45.8</td><td>25.5</td><td>46.1</td><td>55.8</td></tr><tr><th>Mask R-CNN [9]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>40.0</td><td>60.5</td><td>44.0</td><td>22.6</td><td>44.0</td><td>52.6</td></tr><tr><th>Mask R-CNN [9]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>43.3 (+3.3)</td><td>63.7</td><td>47.6</td><td>25.7</td><td>47.1</td><td>56.6</td></tr><tr><th>Cascade R-CNN [2]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>42.0</td><td>60.4</td><td>45.7</td><td>23.4</td><td>45.8</td><td>55.7</td></tr><tr><th>Cascade R-CNN [2]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>44.5 (+2.5)</td><td>63.1</td><td>48.4</td><td>26.1</td><td>48.5</td><td>57.8</td></tr><tr><th>Cascade Mask R-CNN [2]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>42.9</td><td>61.0</td><td>46.6</td><td>24.4</td><td>46.5</td><td>57.0</td></tr><tr><th>Cascade Mask R-CNN [2]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>45.4 (+2.5)</td><td>63.8</td><td>49.4</td><td>27.5</td><td>49.3</td><td>59.5</td></tr><tr><th>anchor-free</th><th></th><th></th><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>FCOS [39]</th><th>ResNet-50-FPN</th><th>1\\times</th><td>36.6</td><td>56.0</td><td>38.8</td><td>21.0</td><td>40.6</td><td>47.0</td></tr><tr><th>FCOS [39]</th><th>ResNet-50-FPN-CFG</th><th>1\\times</th><td>39.6 (+3.0)</td><td>58.8</td><td>42.3</td><td>22.9</td><td>43.4</td><td>51.9</td></tr><tr><th>Sparse R-CNN [37]</th><th>ResNet-50-FPN</th><th>1\\times</th><td>37.9</td><td>56.0</td><td>40.5</td><td>20.7</td><td>40.0</td><td>53.5</td></tr><tr><th>Sparse R-CNN [37]</th><th>ResNet-50-FPN-CFG</th><th>1\\times</th><td>40.1 (+2.2)</td><td>58.7</td><td>42.6</td><td>22.2</td><td>42.6</td><td>55.6</td></tr><tr><th>FSAF [49]</th><th>ResNet-101-FPN</th><th>1\\times</th><td>39.3</td><td>58.6</td><td>42.1</td><td>22.1</td><td>43.4</td><td>51.2</td></tr><tr><th>FSAF [49]</th><th>ResNet-101-FPN-CFG</th><th>1\\times</th><td>42.2 (+2.9)</td><td>62.0</td><td>44.8</td><td>24.3</td><td>45.9</td><td>56.2</td></tr><tr><th>transformer</th><th></th><th></th><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>Mask R-CNN [27]</th><th>Swin-T-FPN</th><th>1\\times</th><td>42.7</td><td>65.2</td><td>46.8</td><td>26.5</td><td>45.9</td><td>56.6</td></tr><tr><th>Mask R-CNN [27]</th><th>Swin-T-FPN-CFG</th><th>1\\times</th><td>46.0 (+3.3)</td><td>67.0</td><td>50.5</td><td>28.8</td><td>49.7</td><td>59.1</td></tr><tr><th>strong baseline</th><th></th><th></th><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><th>Cascade Mask R-CNN [2]</th><th>ResNeXt-101-64x4d-FPN</th><th>3\\times</th><td>46.6</td><td>65.1</td><td>50.6</td><td>29.3</td><td>50.5</td><td>60.1</td></tr><tr><th>Cascade Mask R-CNN [2]</th><th>ResNeXt-101-64x4d-FPN-CFG</th><th>3\\times</th><td>50.1 (+3.5)</td><td>68.6</td><td>54.5</td><td>32.7</td><td>53.7</td><td>64.3</td></tr></tbody></table>", "caption": "Table 3: The improvements on AP after integrating the cascade feature grouping module into various detection frameworks.The 1\\times, 3\\times training schedules follow the settings explained in MMDetection [4].FPN-CFG denotes for applying the cascade feature grouping module into FPN.", "list_citation_info": ["[46] Zhang, X., Wan, F., Liu, C., Ji, X., Ye, Q.: Learning to match anchors for visual object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)", "[2] Cai, Z., Vasconcelos, N.: Cascade r-cnn: High quality object detection and instance segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019)", "[9] He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961\u20132969 (2017)", "[39] Tian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9627\u20139636 (2019)", "[4] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., et al.: Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019)", "[31] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28, 91\u201399 (2015)", "[22] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980\u20132988 (2017)", "[27] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)", "[45] Zhang, S., Chi, C., Yao, Y., Lei, Z., Li, S.Z.: Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9759\u20139768 (2020)", "[37] Sun, P., Zhang, R., Jiang, Y., Kong, T., Xu, C., Zhan, W., Tomizuka, M., Li, L., Yuan, Z., Wang, C., et al.: Sparse r-cnn: End-to-end object detection with learnable proposals. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14454\u201314463 (2021)", "[49] Zhu, C., He, Y., Savvides, M.: Feature selective anchor-free module for single-shot object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 840\u2013849 (2019)"]}], "citation_info_to_title": {"[46] Zhang, X., Wan, F., Liu, C., Ji, X., Ye, Q.: Learning to match anchors for visual object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)": "Learning to match anchors for visual object detection", "[9] He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961\u20132969 (2017)": "Mask R-CNN", "[2] Cai, Z., Vasconcelos, N.: Cascade r-cnn: High quality object detection and instance segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019)": "Cascade R-CNN: High Quality Object Detection and Instance Segmentation", "[37] Sun, P., Zhang, R., Jiang, Y., Kong, T., Xu, C., Zhan, W., Tomizuka, M., Li, L., Yuan, Z., Wang, C., et al.: Sparse r-cnn: End-to-end object detection with learnable proposals. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14454\u201314463 (2021)": "Sparse r-cnn: End-to-end object detection with learnable proposals", "[45] Zhang, S., Chi, C., Yao, Y., Lei, Z., Li, S.Z.: Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9759\u20139768 (2020)": "Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection", "[39] Tian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9627\u20139636 (2019)": "FCOS: Fully Convolutional One-Stage Object Detection", "[4] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., et al.: Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019)": "Mmdetection: Open mmlab detection toolbox and benchmark", "[22] Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980\u20132988 (2017)": "Focal loss for dense object detection", "[49] Zhu, C., He, Y., Savvides, M.: Feature selective anchor-free module for single-shot object detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 840\u2013849 (2019)": "Feature Selective Anchor-Free Module for Single-Shot Object Detection", "[31] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28, 91\u201399 (2015)": "Faster r-cnn: Towards real-time object detection with region proposal networks", "[27] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)": "Swin transformer: Hierarchical vision transformer using shifted windows"}, "source_title_to_arxiv_id": {"Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}