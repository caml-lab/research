{"title": "Densely Constrained Depth Estimator for Monocular 3D Object Detection", "abstract": "Estimating accurate 3D locations of objects from monocular images is a\nchallenging problem because of lacking depth. Previous work shows that\nutilizing the object's keypoint projection constraints to estimate multiple\ndepth candidates boosts the detection performance. However, the existing\nmethods can only utilize vertical edges as projection constraints for depth\nestimation. So these methods only use a small number of projection constraints\nand produce insufficient depth candidates, leading to inaccurate depth\nestimation. In this paper, we propose a method that utilizes dense projection\nconstraints from edges of any direction. In this way, we employ much more\nprojection constraints and produce considerable depth candidates. Besides, we\npresent a graph matching weighting module to merge the depth candidates. The\nproposed method DCD (Densely Constrained Detector) achieves state-of-the-art\nperformance on the KITTI and WOD benchmarks. Code is released at\nhttps://github.com/BraveGroup/DCD.", "authors": ["Yingyan Li", "Yuntao Chen", "Jiawei He", "Zhaoxiang Zhang"], "published_date": "2022_07_20", "pdf_url": "http://arxiv.org/pdf/2207.10047v3", "list_table_and_caption": [{"table": "<table><tbody><tr><td rowspan=\"2\">Methods</td><td rowspan=\"2\">Reference</td><td rowspan=\"2\">Category</td><td colspan=\"3\">AP_{3D|R40|IoU@0.7}</td><td colspan=\"3\">AP_{BEV|R40|IoU@0.7}</td></tr><tr><td>Easy</td><td>Mod.</td><td>Hard</td><td>Easy</td><td>Mod.</td><td>Hard</td></tr><tr><td>PatchNet [26]</td><td>ECCV20</td><td rowspan=\"3\">Pretrained Depth</td><td>15.68</td><td>11.12</td><td>10.17</td><td>22.97</td><td>16.86</td><td>14.97</td></tr><tr><td>D4LCN [8]</td><td>CVPR20</td><td>16.65</td><td>11.72</td><td>9.51</td><td>22.51</td><td>16.02</td><td>12.55</td></tr><tr><td>DDMP-3D [40]</td><td>CVPR21</td><td>19.71</td><td>12.78</td><td>9.80</td><td>28.08</td><td>17.89</td><td>13.44</td></tr><tr><td>CaDDN [31]</td><td>CVPR21</td><td>LiDAR Auxiliary</td><td>19.17</td><td>13.41</td><td>11.46</td><td>27.94</td><td>18.91</td><td>17.19</td></tr><tr><td>RTM3D [18]</td><td>ECCV20</td><td rowspan=\"6\">Directly Regress</td><td>14.41</td><td>10.34</td><td>8.77</td><td>19.17</td><td>14.20</td><td>11.99</td></tr><tr><td>Movi3D [36]</td><td>ECCV20</td><td>15.19</td><td>10.90</td><td>9.26</td><td>22.76</td><td>17.03</td><td>14.85</td></tr><tr><td>Ground-Aware [21]</td><td>RAL21</td><td>21.65</td><td>13.25</td><td>9.91</td><td>29.81</td><td>17.98</td><td>13.08</td></tr><tr><td>MonoDLE [28]</td><td>CVPR21</td><td>17.23</td><td>12.26</td><td>10.29</td><td>24.79</td><td>18.89</td><td>16.00</td></tr><tr><td>MonoRCNN [35]</td><td>ICCV21</td><td>18.36</td><td>12.65</td><td>10.03</td><td>25.48</td><td>18.11</td><td>14.10</td></tr><tr><td>MonoEF [56]</td><td>CVPR21</td><td>21.29</td><td>13.87</td><td>11.71</td><td>29.03</td><td>19.70</td><td>17.26</td></tr><tr><td>MonoRUn [6]</td><td>CVPR21</td><td rowspan=\"5\">Geometric-based</td><td>19.65</td><td>12.30</td><td>10.58</td><td>27.94</td><td>17.34</td><td>15.24</td></tr><tr><td>AutoShape [23]</td><td>ICCV21</td><td>22.47</td><td>14.17</td><td>11.36</td><td>30.66</td><td>20.08</td><td>15.59</td></tr><tr><td>GUPNet [25]</td><td>ICCV21</td><td>22.20</td><td>15.02</td><td>13.12</td><td>30.29</td><td>21.19</td><td>18.20</td></tr><tr><td>MonoFlex(Baseline) [51]</td><td>CVPR21</td><td>19.94</td><td>13.89</td><td>12.07</td><td>28.23</td><td>19.75</td><td>16.89</td></tr><tr><td>DCD(Ours)</td><td>ECCV22</td><td>23.81</td><td>15.90</td><td>13.21</td><td>32.55</td><td>21.50</td><td>18.25</td></tr></tbody></table>", "caption": "Table 1: The result on the KITTI test server compared with other public methods in recent years.", "list_citation_info": ["[51] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3d object detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 3289\u20133298. Computer Vision Foundation / IEEE (2021)", "[40] Wang, L., Du, L., Ye, X., Fu, Y., Guo, G., Xue, X., Feng, J., Zhang, L.: Depth-conditioned dynamic message propagation for monocular 3d object detection. In: CVPR. pp. 454\u2013463 (2021)", "[36] Simonelli, A., Bulo, S.R., Porzi, L., Ricci, E., Kontschieder, P.: Towards generalization across depth for monocular 3d object detection. In: ECCV. pp. 767\u2013782. Springer (2020)", "[21] Liu, Y., Yixuan, Y., Liu, M.: Ground-aware monocular 3d object detection for autonomous driving. IEEE Robotics and Automation Letters 6(2), 919\u2013926 (2021)", "[8] Ding, M., Huo, Y., Yi, H., Wang, Z., Shi, J., Lu, Z., Luo, P.: Learning depth-guided convolutions for monocular 3d object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 1000\u20131001 (2020)", "[26] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking pseudo-lidar representation. In: European Conference on Computer Vision. pp. 311\u2013327. Springer (2020)", "[28] Ma, X., Zhang, Y., Xu, D., Zhou, D., Yi, S., Li, H., Ouyang, W.: Delving into localization errors for monocular 3d object detection. In: CVPR. pp. 4721\u20134730 (2021)", "[35] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposition for monocular 3d object detection. arXiv preprint arXiv:2104.03775 (2021)", "[6] Chen, H., Huang, Y., Tian, W., Gao, Z., Xiong, L.: Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10379\u201310388 (2021)", "[23] Liu, Z., Zhou, D., Lu, F., Fang, J., Zhang, L.: Autoshape: Real-time shape-aware monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15641\u201315650 (2021)", "[25] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3111\u20133121 (2021)", "[31] Reading, C., Harakeh, A., Chae, J., Waslander, S.L.: Categorical depth distribution network for monocular 3d object detection. arXiv preprint arXiv:2103.01100 (2021)", "[56] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: Monocular 3d object detection: An extrinsic parameter free approach. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7556\u20137566 (2021)", "[18] Li, P., Zhao, H., Liu, P., Cao, F.: Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving. arXiv preprint arXiv:2001.03343 2 (2020)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th colspan=\"3\">Pedestrian</th><th colspan=\"3\">Cyclist</th></tr><tr><th>Easy</th><th>Mod.</th><th>Hard</th><th>Easy</th><th>Mod.</th><th>Hard</th></tr></thead><tbody><tr><th>M3D-RPN [2]</th><td>4.92</td><td>3.48</td><td>2.94</td><td>0.94</td><td>0.65</td><td>0.47</td></tr><tr><th>MonoPair [7]</th><td>10.02</td><td>6.68</td><td>5.53</td><td>3.79</td><td>2.12</td><td>1.83</td></tr><tr><th>MonoFlex [51]</th><td>9.43</td><td>6.31</td><td>5.26</td><td>4.17</td><td>2.35</td><td>2.04</td></tr><tr><th>AutoShape [23]</th><td>5.46</td><td>3.74</td><td>3.03</td><td>5.99</td><td>3.06</td><td>2.70</td></tr><tr><th>Ours</th><td>10.37</td><td>6.73</td><td>6.28</td><td>4.72</td><td>2.74</td><td>2.41</td></tr></tbody></table>", "caption": "Table 2: The AP_{3D|R40} results for Pedestrian and Cyclist on KITTI test set. We use the bounding box corners as the input of DGDE.", "list_citation_info": ["[2] Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9287\u20139296 (2019)", "[51] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3d object detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 3289\u20133298. Computer Vision Foundation / IEEE (2021)", "[7] Chen, Y., Tai, L., Sun, K., Li, M.: Monopair: Monocular 3d object detection using pairwise spatial relationships. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12093\u201312102 (2020)", "[23] Liu, Z., Zhou, D., Lu, F., Fang, J., Zhang, L.: Autoshape: Real-time shape-aware monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15641\u201315650 (2021)"]}, {"table": "<table><tbody><tr><th>Difficulty</th><th>Method</th><td>3D AP (IoU@0.7)</td><td>3D APH (IoU@0.7)</td><td>3D AP (IoU@0.5)</td><td>3D APH (IoU@0.5)</td></tr><tr><th rowspan=\"6\">LEVEL_1/LEVEL_2</th><th>M3D-RPN\u2021 [2]</th><td>0.35/0.33</td><td>0.34/0.33</td><td>3.79/3.61</td><td>3.63/3.46</td></tr><tr><th>PatchNet\u2020[27]</th><td>0.39/0.38</td><td>0.37/0.36</td><td>2.92/2.42</td><td>2.74/2.28</td></tr><tr><th>PCT [41]</th><td>0.89/0.66</td><td>0.88/0.66</td><td>4.20/4.03</td><td>4.15/3.99</td></tr><tr><th>CaDDN [31]</th><td>5.03/4.49</td><td>4.99/4.45</td><td>17.54/16.51</td><td>17.31/16.28</td></tr><tr><th>MonoFlex{}^{\\ast} [51]</th><td>11.70/10.96</td><td>11.64/10.90</td><td>32.26/30.31</td><td>32.06/30.12</td></tr><tr><th>DCD(Ours)</th><td>12.57/11.78</td><td>12.50/11.72</td><td>33.44/31.43</td><td>33.24/31.25</td></tr></tbody></table>", "caption": "Table 3: The result on WOD [38] val set. Italics: These methods utilize the whole train set, while the others use 1/3 amount of images in train set. \u2021: M3D-RPN is re-implemented by [31]. \u2020: PatchNet is re-implemented by [41]. {}^{\\ast}: MonoFlex is our baseline and re-implemented ourselves.", "list_citation_info": ["[2] Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9287\u20139296 (2019)", "[51] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3d object detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 3289\u20133298. Computer Vision Foundation / IEEE (2021)", "[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)", "[27] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking pseudo-lidar representation. In: European Conference on Computer Vision. pp. 311\u2013327. Springer (2020)", "[41] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3d object detection. Advances in Neural Information Processing Systems 34 (2021)", "[31] Reading, C., Harakeh, A., Chae, J., Waslander, S.L.: Categorical depth distribution network for monocular 3d object detection. arXiv preprint arXiv:2103.01100 (2021)"]}, {"table": "<table><thead><tr><th rowspan=\"2\"></th><th rowspan=\"2\">Weighting Method</th><th rowspan=\"2\">DGDE</th><th rowspan=\"2\">#Keypoints</th><th rowspan=\"2\">#Depth Candidates</th><th></th><th>AP_{3D|R40|IoU@0.7}</th><th></th></tr><tr><th>Easy</th><th>Mod</th><th>Hard</th></tr></thead><tbody><tr><td>(a)</td><td>Uncertainty (Baseline) [51]</td><td></td><td>10</td><td>5</td><td>21.63</td><td>15.87</td><td>13.38</td></tr><tr><td>(b)</td><td>Uncertainty</td><td>\u2713</td><td>10</td><td>45</td><td>21.72</td><td>16.09</td><td>13.35</td></tr><tr><td>(c)</td><td>Uncertainty</td><td>\u2713</td><td>73</td><td>1500</td><td>22.84</td><td>16.53</td><td>13.77</td></tr><tr><td>(d)</td><td>GMW</td><td></td><td>10</td><td>5</td><td>22.58</td><td>16.14</td><td>13.63</td></tr><tr><td>(e)</td><td>GMW</td><td>\u2713</td><td>10</td><td>45</td><td>23.30</td><td>16.91</td><td>14.93</td></tr><tr><td>(f)</td><td>GMW</td><td>\u2713</td><td>73</td><td>1500</td><td>23.94</td><td>17.38</td><td>15.32</td></tr></tbody></table>", "caption": "Table 4: Quantitative results using the state-of-the-art method MonoFlex [51] as baseline. This table shows the effectiveness of DGDE and GMW. The Sec. 4.4 explains the strategy of choosing 1500 depth candidates.", "list_citation_info": ["[51] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3d object detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 3289\u20133298. Computer Vision Foundation / IEEE (2021)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Method</th><th rowspan=\"2\">#Keypoints</th><th rowspan=\"2\">Depth Estimatior</th><th rowspan=\"2\">#Depth Candidates</th><th colspan=\"3\">AP_{3D|R40|IoU@0.7}</th></tr><tr><th>Easy</th><th>Mod</th><th>Hard</th></tr></thead><tbody><tr><td>Autoshape [23]</td><td>73</td><td>Combined</td><td>1</td><td>22.37</td><td>16.48</td><td>14.58</td></tr><tr><td>DCD (Ours)</td><td>73</td><td>DGDE</td><td>1500</td><td>23.94</td><td>17.38</td><td>15.32</td></tr></tbody></table>", "caption": "Table 7: We re-implement the AutoShape\u2019s depth estimation and weighting regression methods on our baseline. The combined depth estimator combines all keypoint projection constraints as input and produces one depth as output. ", "list_citation_info": ["[23] Liu, Z., Zhou, D., Lu, F., Fang, J., Zhang, L.: Autoshape: Real-time shape-aware monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15641\u201315650 (2021)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Difficulty</th><th rowspan=\"2\">Method</th><th colspan=\"4\">3D mAP</th><th colspan=\"4\">3D mAPH</th></tr><tr><th>Overall</th><th>0-30m</th><th>30-50m</th><th>50-\\infty</th><th>Overall</th><th>0-30m</th><th>30-50m</th><th>50m-\\infty</th></tr></thead><tbody><tr><th rowspan=\"4\">LEVEL_1@IoU 0.7</th><th>M3D-RPN\u2021 [2]</th><td>0.35</td><td>1.12</td><td>0.18</td><td>0.02</td><td>0.34</td><td>1.10</td><td>0.18</td><td>0.02</td></tr><tr><th>PatchNet\u2020[27]</th><td>0.39</td><td>1.67</td><td>0.13</td><td>0.03</td><td>0.37</td><td>1.63</td><td>0.12</td><td>0.03</td></tr><tr><th>PCT [41]</th><td>0.89</td><td>3.18</td><td>0.27</td><td>0.07</td><td>0.88</td><td>3.15</td><td>0.27</td><td>0.07</td></tr><tr><th>CaDDN [31]</th><td>5.03</td><td>14.54</td><td>1.47</td><td>0.10</td><td>4.99</td><td>14.43</td><td>1.45</td><td>0.10</td></tr><tr><th></th><th>MonoFlex{}^{\\ast} [51]</th><td>11.70</td><td>30.64</td><td>5.29</td><td>1.05</td><td>11.64</td><td>30.48</td><td>5.27</td><td>1.04</td></tr><tr><th></th><th>DCD(Ours)</th><td>12.57</td><td>32.47</td><td>5.94</td><td>1.24</td><td>12.50</td><td>32.30</td><td>5.91</td><td>1.23</td></tr><tr><th rowspan=\"4\">LEVEL_2@IoU 0.7</th><th>M3D-RPN\u2021 [2]</th><td>0.33</td><td>1.12</td><td>0.18</td><td>0.02</td><td>0.33</td><td>1.10</td><td>0.17</td><td>0.02</td></tr><tr><th>PatchNet\u2020[27]</th><td>0.38</td><td>1.67</td><td>0.13</td><td>0.03</td><td>0.36</td><td>1.63</td><td>0.11</td><td>0.03</td></tr><tr><th>PCT [41]</th><td>0.66</td><td>3.18</td><td>0.27</td><td>0.07</td><td>0.66</td><td>3.15</td><td>0.26</td><td>0.07</td></tr><tr><th>CaDDN [31]</th><td>4.49</td><td>14.50</td><td>1.42</td><td>0.09</td><td>4.45</td><td>14.38</td><td>1.41</td><td>0.09</td></tr><tr><th></th><th>MonoFlex{}^{\\ast} [51]</th><td>10.96</td><td>30.54</td><td>5.14</td><td>0.91</td><td>10.90</td><td>30.37</td><td>5.11</td><td>0.91</td></tr><tr><th></th><th>DCD(Ours)</th><td>11.78</td><td>32.30</td><td>5.76</td><td>1.08</td><td>11.72</td><td>32.19</td><td>5.73</td><td>1.08</td></tr></tbody></table>", "caption": "Table 8: The IoU@0.7 result on WOD [38] val set. Italics: These methods utilize the whole train set, while the others uses 1/3 amount of images in train set. \u2021: M3D-RPN is re-implemented by [31]. \u2020: PatchNet is re-implemented by [41]. {}^{\\ast}: MonoFlex is our baseline and re-implemented ourselves.", "list_citation_info": ["[2] Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9287\u20139296 (2019)", "[51] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3d object detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 3289\u20133298. Computer Vision Foundation / IEEE (2021)", "[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)", "[27] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking pseudo-lidar representation. In: European Conference on Computer Vision. pp. 311\u2013327. Springer (2020)", "[41] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3d object detection. Advances in Neural Information Processing Systems 34 (2021)", "[31] Reading, C., Harakeh, A., Chae, J., Waslander, S.L.: Categorical depth distribution network for monocular 3d object detection. arXiv preprint arXiv:2103.01100 (2021)"]}, {"table": "<table><thead><tr><th rowspan=\"2\">Difficulty</th><th rowspan=\"2\">Method</th><th colspan=\"4\">3D mAP</th><th colspan=\"4\">3D mAPH</th></tr><tr><th>Overall</th><th>0-30m</th><th>30-50m</th><th>50-\\infty</th><th>Overall</th><th>0-30m</th><th>30-50m</th><th>50m-\\infty</th></tr></thead><tbody><tr><th rowspan=\"4\">LEVEL_1@IoU 0.5</th><th>M3D-RPN\u2021 [2]</th><td>3.79</td><td>11.14</td><td>2.16</td><td>0.26</td><td>3.63</td><td>10.70</td><td>2.09</td><td>0.21</td></tr><tr><th>PatchNet\u2020[27]</th><td>2.92</td><td>10.03</td><td>1.09</td><td>0.23</td><td>2.74</td><td>9.75</td><td>0.96</td><td>0.18</td></tr><tr><th>PCT [41]</th><td>4.20</td><td>14.70</td><td>1.78</td><td>0.39</td><td>4.15</td><td>14.54</td><td>1.75</td><td>0.39</td></tr><tr><th>CaDDN [31]</th><td>17.54</td><td>45.00</td><td>9.24</td><td>0.64</td><td>17.31</td><td>44.46</td><td>9.11</td><td>0.62</td></tr><tr><th></th><th>MonoFlex{}^{\\ast} [51]</th><td>32.26</td><td>61.13</td><td>25.85</td><td>9.03</td><td>32.06</td><td>60.75</td><td>25.71</td><td>8.95</td></tr><tr><th></th><th>DCD(Ours)</th><td>33.44</td><td>62.70</td><td>26.35</td><td>10.16</td><td>33.24</td><td>62.35</td><td>26.21</td><td>10.09</td></tr><tr><th rowspan=\"4\">LEVEL_2@IoU 0.5</th><th>M3D-RPN\u2021 [2]</th><td>3.61</td><td>11.12</td><td>2.12</td><td>0.24</td><td>3.46</td><td>10.67</td><td>2.04</td><td>0.20</td></tr><tr><th>PatchNet\u2020[27]</th><td>2.42</td><td>10.01</td><td>1.07</td><td>0.22</td><td>2.28</td><td>9.73</td><td>0.94</td><td>0.16</td></tr><tr><th>PCT [41]</th><td>4.03</td><td>14.67</td><td>1.74</td><td>0.36</td><td>3.99</td><td>14.51</td><td>1.71</td><td>0.35</td></tr><tr><th>CaDDN [31]</th><td>16.51</td><td>44.87</td><td>8.99</td><td>0.58</td><td>16.28</td><td>44.33</td><td>8.86</td><td>0.55</td></tr><tr><th></th><th>MonoFlex{}^{\\ast} [51]</th><td>30.31</td><td>60.91</td><td>25.11</td><td>7.92</td><td>30.12</td><td>60.54</td><td>24.97</td><td>7.85</td></tr><tr><th></th><th>DCD(Ours)</th><td>31.43</td><td>62.48</td><td>25.60</td><td>8.92</td><td>31.25</td><td>62.13</td><td>25.46</td><td>8.86</td></tr></tbody></table>", "caption": "Table 9: The IoU@0.5 result on WOD[38] val set. Italics: These methods utilize the whole train set, while the others uses 1/3 amount of images in train set. \u2021: M3D-RPN is re-implemented by [31]. \u2020: PatchNet is re-implemented by [41]. {}^{\\ast}: MonoFlex is our baseline and re-implemented ourselves.", "list_citation_info": ["[2] Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9287\u20139296 (2019)", "[51] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3d object detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 3289\u20133298. Computer Vision Foundation / IEEE (2021)", "[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)", "[27] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking pseudo-lidar representation. In: European Conference on Computer Vision. pp. 311\u2013327. Springer (2020)", "[41] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3d object detection. Advances in Neural Information Processing Systems 34 (2021)", "[31] Reading, C., Harakeh, A., Chae, J., Waslander, S.L.: Categorical depth distribution network for monocular 3d object detection. arXiv preprint arXiv:2103.01100 (2021)"]}], "citation_info_to_title": {"[2] Brazil, G., Liu, X.: M3d-rpn: Monocular 3d region proposal network for object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9287\u20139296 (2019)": "M3d-rpn: Monocular 3d region proposal network for object detection", "[36] Simonelli, A., Bulo, S.R., Porzi, L., Ricci, E., Kontschieder, P.: Towards generalization across depth for monocular 3d object detection. In: ECCV. pp. 767\u2013782. Springer (2020)": "Towards generalization across depth for monocular 3d object detection", "[40] Wang, L., Du, L., Ye, X., Fu, Y., Guo, G., Xue, X., Feng, J., Zhang, L.: Depth-conditioned dynamic message propagation for monocular 3d object detection. In: CVPR. pp. 454\u2013463 (2021)": "Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection", "[21] Liu, Y., Yixuan, Y., Liu, M.: Ground-aware monocular 3d object detection for autonomous driving. IEEE Robotics and Automation Letters 6(2), 919\u2013926 (2021)": "Ground-aware Monocular 3D Object Detection for Autonomous Driving", "[23] Liu, Z., Zhou, D., Lu, F., Fang, J., Zhang, L.: Autoshape: Real-time shape-aware monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15641\u201315650 (2021)": "Autoshape: Real-time shape-aware monocular 3d object detection", "[56] Zhou, Y., He, Y., Zhu, H., Wang, C., Li, H., Jiang, Q.: Monocular 3d object detection: An extrinsic parameter free approach. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7556\u20137566 (2021)": "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach", "[18] Li, P., Zhao, H., Liu, P., Cao, F.: Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving. arXiv preprint arXiv:2001.03343 2 (2020)": "Rtm3d: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving", "[31] Reading, C., Harakeh, A., Chae, J., Waslander, S.L.: Categorical depth distribution network for monocular 3d object detection. arXiv preprint arXiv:2103.01100 (2021)": "Categorical depth distribution network for monocular 3d object detection", "[35] Shi, X., Ye, Q., Chen, X., Chen, C., Chen, Z., Kim, T.K.: Geometry-based distance decomposition for monocular 3d object detection. arXiv preprint arXiv:2104.03775 (2021)": "Geometry-based distance decomposition for monocular 3d object detection", "[28] Ma, X., Zhang, Y., Xu, D., Zhou, D., Yi, S., Li, H., Ouyang, W.: Delving into localization errors for monocular 3d object detection. In: CVPR. pp. 4721\u20134730 (2021)": "Delving into localization errors for monocular 3d object detection", "[25] Lu, Y., Ma, X., Yang, L., Zhang, T., Liu, Y., Chu, Q., Yan, J., Ouyang, W.: Geometry uncertainty projection network for monocular 3d object detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3111\u20133121 (2021)": "Geometry Uncertainty Projection Network for Monocular 3D Object Detection", "[38] Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)": "Scalability in perception for autonomous driving: Waymo open dataset", "[26] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking pseudo-lidar representation. In: European Conference on Computer Vision. pp. 311\u2013327. Springer (2020)": "Rethinking pseudo-lidar representation", "[41] Wang, L., Zhang, L., Zhu, Y., Zhang, Z., He, T., Li, M., Xue, X.: Progressive coordinate transforms for monocular 3d object detection. Advances in Neural Information Processing Systems 34 (2021)": "Progressive coordinate transforms for monocular 3d object detection", "[8] Ding, M., Huo, Y., Yi, H., Wang, Z., Shi, J., Lu, Z., Luo, P.: Learning depth-guided convolutions for monocular 3d object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. pp. 1000\u20131001 (2020)": "Learning depth-guided convolutions for monocular 3d object detection", "[6] Chen, H., Huang, Y., Tian, W., Gao, Z., Xiong, L.: Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10379\u201310388 (2021)": "Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation", "[27] Ma, X., Liu, S., Xia, Z., Zhang, H., Zeng, X., Ouyang, W.: Rethinking pseudo-lidar representation. In: European Conference on Computer Vision. pp. 311\u2013327. Springer (2020)": "Rethinking pseudo-lidar representation", "[7] Chen, Y., Tai, L., Sun, K., Li, M.: Monopair: Monocular 3d object detection using pairwise spatial relationships. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12093\u201312102 (2020)": "Monopair: Monocular 3d object detection using pairwise spatial relationships", "[51] Zhang, Y., Lu, J., Zhou, J.: Objects are different: Flexible monocular 3d object detection. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 3289\u20133298. Computer Vision Foundation / IEEE (2021)": "Flexible Monocular 3D Object Detection"}, "source_title_to_arxiv_id": {"Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection": "2103.16470", "Categorical depth distribution network for monocular 3d object detection": "2103.01100", "Progressive coordinate transforms for monocular 3d object detection": "2108.05793"}}