{"title": "Enabling and Accelerating Dynamic Vision Transformer Inference for Real-Time Applications", "abstract": "Many state-of-the-art deep learning models for computer vision tasks are\nbased on the transformer architecture. Such models can be computationally\nexpensive and are typically statically set to meet the deployment scenario.\nHowever, in real-time applications, the resources available for every inference\ncan vary considerably and be smaller than what state-of-the-art models require.\nWe can use dynamic models to adapt the model execution to meet real-time\napplication resource constraints. While prior dynamic work primarily minimized\nresource utilization for less complex input images, we adapt vision\ntransformers to meet system dynamic resource constraints, independent of the\ninput image. We find that unlike early transformer models, recent\nstate-of-the-art vision transformers heavily rely on convolution layers. We\nshow that pretrained models are fairly resilient to skipping computation in the\nconvolution and self-attention layers, enabling us to create a low-overhead\nsystem for dynamic real-time inference without extra training. Finally, we\nexplore compute organization and memory sizes to find settings to efficiency\nexecute dynamic vision transformers. We find that wider vector sizes produce a\nbetter energy-accuracy tradeoff across dynamic configurations despite limiting\nthe granularity of dynamic execution, but scaling accelerator resources for\nlarger models does not significantly improve the latency-area-energy-tradeoffs.\nOur accelerator saves 20% of execution time and 30% of energy with a 4% drop in\naccuracy with pretrained SegFormer B2 model in our dynamic inference approach\nand 57% of execution time for the ResNet-50 backbone with a 4.5% drop in\naccuracy with the Once-For-All approach.", "authors": ["Kavya Sreedhar", "Jason Clemons", "Rangharajan Venkatesan", "Stephen W. Keckler", "Mark Horowitz"], "published_date": "2022_12_06", "pdf_url": "http://arxiv.org/pdf/2212.02687v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td>Model</td><td>Parameters (Millions)</td><td>Dataset</td><td>Image size</td><td>GFLOPs</td><td>Latency (ms)</td><td>FPS</td><td>mIoU / AP</td><td>Task</td></tr><tr><td>SegFormer B2 ADE  [71]</td><td>27.6</td><td>ADE20K [84]</td><td>512 by 512</td><td>62.6</td><td>58</td><td>17.2</td><td>0.4651</td><td>SS</td></tr><tr><td>SegFormer B2 Cityscapes [71]</td><td>27.6</td><td>Cityscapes [10]</td><td>1024 by 2048</td><td>705</td><td>415</td><td>2.4</td><td>0.8098</td><td>SS</td></tr><tr><td>Swin Tiny [37]</td><td>60</td><td>ADE20K [84]</td><td>512 by 512</td><td>237</td><td>215</td><td>4.7</td><td>0.4451</td><td>SS</td></tr><tr><td>DETR [7]</td><td>41</td><td>COCO [34]</td><td>640 by 480</td><td>86</td><td>162</td><td>6.2</td><td>0.401</td><td>OD</td></tr><tr><td>Deformable DETR [86]</td><td>40</td><td>COCO [34]</td><td>640 by 480</td><td>173</td><td>119</td><td>5.8</td><td>0.445</td><td>OD</td></tr></tbody></table>", "caption": "TABLE I: State-of-the-art vision transformer model summary for a batch size of 1 and profiling on a NVIDIA TITAN V GPU with clocks locked to 1005 MHz. SS = semantic segmentation. OD = object detection. Accuracy metrics: mIoU for the ADE20K dataset [84] (SS) and AP, with IoU from 0.5 to 0.95 in increments of 0.05 for the COCO-2017 dataset [34] (OD).", "list_citation_info": ["[84] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, \u201cScene parsing through ade20k dataset,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 633\u2013641.", "[10] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset for semantic urban scene understanding,\u201d in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "[7] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in European conference on computer vision. Springer, 2020, pp. 213\u2013229.", "[86] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable detr: Deformable transformers for end-to-end object detection,\u201d arXiv preprint arXiv:2010.04159, 2020.", "[34] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in European conference on computer vision. Springer, 2014, pp. 740\u2013755.", "[37] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10\u2009012\u201310\u2009022.", "[71] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, \u201cSegformer: Simple and efficient design for semantic segmentation with transformers,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021."]}, {"table": "<table><thead><tr><th></th><th></th><th></th><th>PE Array</th><th>Normalized</th></tr><tr><th>Accelerator</th><th>WM (kB)</th><th>AM (kB)</th><th>Area (mm^{2})</th><th>Energy</th></tr></thead><tbody><tr><td>OFA1</td><td>1024</td><td>64</td><td>8.33</td><td>16.5</td></tr><tr><td>OFA2</td><td>128</td><td>64</td><td>2.26</td><td>14.3</td></tr><tr><td>OFA3</td><td>64</td><td>32</td><td>1.66</td><td>14.6</td></tr></tbody></table>", "caption": "TABLE IV: Accelerator parameterizations with K0 = 32 and C0 = 32 considered for OFA ResNet-50 models [6].", "list_citation_info": ["[6] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, \u201cOnce-for-all: Train one network and specialize it for efficient deployment,\u201d arXiv preprint arXiv:1908.09791, 2019."]}], "citation_info_to_title": {"[71] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, \u201cSegformer: Simple and efficient design for semantic segmentation with transformers,\u201d Advances in Neural Information Processing Systems, vol. 34, 2021.": "Segformer: Simple and efficient design for semantic segmentation with transformers", "[34] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in context,\u201d in European conference on computer vision. Springer, 2014, pp. 740\u2013755.": "Microsoft coco: Common objects in context", "[6] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, \u201cOnce-for-all: Train one network and specialize it for efficient deployment,\u201d arXiv preprint arXiv:1908.09791, 2019.": "Once-for-all: Train one network and specialize it for efficient deployment", "[7] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \u201cEnd-to-end object detection with transformers,\u201d in European conference on computer vision. Springer, 2020, pp. 213\u2013229.": "End-to-end object detection with transformers", "[86] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \u201cDeformable detr: Deformable transformers for end-to-end object detection,\u201d arXiv preprint arXiv:2010.04159, 2020.": "Deformable DETR: Deformable Transformers for End-to-End Object Detection", "[37] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10\u2009012\u201310\u2009022.": "Swin transformer: Hierarchical vision transformer using shifted windows", "[84] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, \u201cScene parsing through ade20k dataset,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 633\u2013641.": "Scene parsing through ade20k dataset", "[10] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset for semantic urban scene understanding,\u201d in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.": "The title of the paper is The cityscapes dataset for semantic urban scene understanding"}, "source_title_to_arxiv_id": {"Once-for-all: Train one network and specialize it for efficient deployment": "1908.09791", "Swin transformer: Hierarchical vision transformer using shifted windows": "2103.14030"}}