{"title": "Data-free Dense Depth Distillation", "abstract": "We study data-free knowledge distillation (KD) for monocular depth estimation\n(MDE), which learns a lightweight model for real-world depth perception tasks\nby compressing it from a trained teacher model while lacking training data in\nthe target domain. Owing to the essential difference between image\nclassification and dense regression, previous methods of data-free KD are not\napplicable to MDE. To strengthen its applicability in real-world tasks, in this\npaper, we propose to apply KD with out-of-distribution simulated images. The\nmajor challenges to be resolved are i) lacking prior information about object\ndistribution of real-world training data, and ii) domain shift between\nsimulated and real-world images. To cope with these difficulties, we propose a\ntailored framework for depth distillation. The framework generates new training\nsamples for maximally covering distributed patterns of objects in the target\ndomain and utilizes a transformation network to efficiently adapt them to the\nfeature statistics preserved in the teacher model. Through extensive\nexperiments on various depth estimation models and two different datasets, we\nshow that our method outperforms the baseline KD by a good margin and even\nachieves slightly better performance with as few as 1/6 of training images,\ndemonstrating a clear superiority.", "authors": ["Junjie Hu", "Chenyou Fan", "Mete Ozay", "Hualie Jiang", "Tin Lun Lam"], "published_date": "2022_08_26", "pdf_url": "http://arxiv.org/pdf/2208.12464v2", "list_table_and_caption": [{"table": "<table><tr><td></td><td><p>Dataset (\\mathcal{X^{\\prime}})</p></td><td colspan=\"3\">Properties of \\mathcal{X^{\\prime}}</td><td><p>\\delta_{1}</p></td></tr><tr><td><p>(a)</p></td><td><p>NYU-v2 [46]</p></td><td><p>indoor scene</p></td><td><p>real world</p></td><td><p>50K</p></td><td><p>0.808</p></td></tr><tr><td><p>(b)</p></td><td><p>ScanNet [7]</p></td><td><p>indoor scene</p></td><td><p>real world</p></td><td><p>50K</p></td><td><p>0.787</p></td></tr><tr><td><p>(c)</p></td><td><p>ImageNet [8]</p></td><td><p>single object</p></td><td><p>real-wrold</p></td><td><p>50K</p></td><td><p>0.685</p></td></tr><tr><td><p>(d)</p></td><td><p>Random noises</p></td><td><p>-</p></td><td><p>-</p></td><td><p>50K</p></td><td><p>0.194</p></td></tr><tr><td><p>(e)</p></td><td><p>KITTI [49]</p></td><td><p>outdoor scene</p></td><td><p>real world</p></td><td><p>50K</p></td><td><p>0.705</p></td></tr><tr><td><p>(f)</p></td><td><p>SceneNet [37]</p></td><td><p>indoor scene</p></td><td><p>simulation</p></td><td><p>50K</p></td><td><p>0.712</p></td></tr><tr><td><p>(g)</p></td><td><p>SceneNet [37]</p></td><td><p>indoor scene</p></td><td><p>simulation</p></td><td><p>300K</p></td><td>0.742</td></tr></table>", "caption": "TABLE I: Results of the student model employed on the NYU-v2 test set. The student model is trained via knowledge distillation with different OOD data. Except for (g), all datasets have approximately 50K images.", "list_citation_info": ["[37] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison, \u201cScenenet rgb-d: 5m photorealistic images of synthetic indoor trajectories with ground truth,\u201d arXiv preprint arXiv:1612.05079, 2016.", "[49] J. Uhrig, N. Schneider, L. Schneider, U. Franke, T. Brox, and A. Geiger, \u201cSparsity invariant cnns,\u201d in International Conference on 3D Vision (3DV), 2017, pp. 11\u201320.", "[7] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5828\u20135839.", "[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248\u2013255.", "[46] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, \u201cIndoor segmentation and support inference from rgbd images,\u201d in European Conference on Computer Vision (ECCV), vol. 7576, 2012, pp. 746\u2013760."]}, {"table": "<table><tr><td colspan=\"2\">  Teacher (Backbone)\\rightarrow Student (Backbone)</td><td colspan=\"2\"> ResNet-34 [18]\\rightarrow ResNet-34 </td><td colspan=\"2\"> ResNet-34 [18]\\rightarrow MobileNet-v2 </td><td colspan=\"2\"> ResNet-50 [25]\\rightarrow ResNet-18 </td><td colspan=\"2\"> ResNet-50 [20]\\rightarrow ResNet-18 </td><td colspan=\"2\"> SeNet-154 [4]\\rightarrow ResNet-34 </td></tr><tr><td colspan=\"2\">Parameter Reduction</td><td colspan=\"2\">None</td><td colspan=\"2\">21.9 M \\rightarrow 1.7 M</td><td colspan=\"2\">63.6 M \\rightarrow 13.7 M</td><td colspan=\"2\">67.6 M \\rightarrow 14.9 M</td><td colspan=\"2\">258.4 M \\rightarrow 38.7 M</td></tr><tr><td><p>Method</p></td><td><p>Data</p></td><td><p>REL \\downarrow</p></td><td><p>\\delta_{1} \\uparrow</p></td><td><p>REL \\downarrow</p></td><td><p>\\delta_{1} \\uparrow</p></td><td><p>REL \\downarrow</p></td><td><p>\\delta_{1} \\uparrow</p></td><td><p>REL</p></td><td><p>\\delta_{1} \\uparrow</p></td><td><p>REL \\downarrow</p></td><td><p>\\delta_{1} \\uparrow</p></td></tr><tr><td><p>Teacher</p></td><td rowspan=\"2\"><p>NYU-v2</p></td><td><p>0.133</p></td><td><p>0.829</p></td><td><p>0.133</p></td><td><p>0.829</p></td><td><p>0.134</p></td><td><p>0.824</p></td><td><p>0.126</p></td><td><p>0.843</p></td><td><p>0.111</p></td><td><p>0.878</p></td></tr><tr><td><p>Student</p></td><td><p>0.133</p></td><td><p>0.829</p></td><td><p>0.145</p></td><td><p>0.802</p></td><td><p>0.145</p></td><td><p>0.805</p></td><td><p>0.137</p></td><td><p>0.826</p></td><td><p>0.125</p></td><td><p>0.843</p></td></tr><tr><td><p>Random noises</p></td><td rowspan=\"2\"><p>None</p></td><td><p>0.426</p></td><td><p>0.193</p></td><td><p>0.431</p></td><td><p>0.194</p></td><td><p>0.517</p></td><td><p>0.102</p></td><td><p>0.511</p></td><td><p>0.112</p></td><td><p>0.514</p></td><td><p>0.107</p></td></tr><tr><td><p>DFAD [12]</p></td><td><p>0.285</p></td><td><p>0.402</p></td><td><p>0.306</p></td><td><p>0.329</p></td><td><p>0.300</p></td><td><p>0.382</p></td><td><p>0.341</p></td><td><p>0.338</p></td><td><p>0.347</p></td><td><p>0.278</p></td></tr><tr><td><p>KD-OOD [16]</p></td><td rowspan=\"2\"><p>SceneNet \\mathcal{X}^{\\prime}_{1}</p></td><td><p>0.164</p></td><td><p>0.753</p></td><td><p>0.175</p></td><td><p>0.712</p></td><td><p>0.188</p></td><td><p>0.660</p></td><td><p>0.175</p></td><td><p>0.710</p></td><td><p>0.174</p></td><td><p>0.695</p></td></tr><tr><td><p>Ours</p></td><td>0.155</td><td>0.774</td><td>0.168</td><td>0.742</td><td>0.173</td><td>0.701</td><td>0.167</td><td>0.722</td><td>0.156</td><td>0.759</td></tr><tr><td><p>KD-OOD [16]</p></td><td rowspan=\"2\"><p>SceneNet \\mathcal{X}^{\\prime}_{2}</p></td><td><p>0.158</p></td><td><p>0.761</p></td><td><p>0.165</p></td><td><p>0.742</p></td><td><p>0.180</p></td><td><p>0.676</p></td><td><p>0.172</p></td><td><p>0.713</p></td><td><p>0.161</p></td><td><p>0.738</p></td></tr><tr><td><p>Ours</p></td><td>0.151</td><td>0.789</td><td>0.157</td><td>0.778</td><td>0.165</td><td>0.726</td><td>0.157</td><td>0.760</td><td>0.151</td><td>0.776</td></tr></table>", "caption": "TABLE II: Quantitative results on the NYU-v2 dataset.", "list_citation_info": ["[25] L. Iro, R. Christian, B. Vasileios, T. Federico, and N. Nassir, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in International Conference on 3D Vision (3DV), 2016, pp. 239\u2013248.", "[18] J. Hu, C. Fan, H. Jiang, X. Guo, Y. Gao, X. Lu, and T. L. Lam, \u201cBoosting light-weight depth estimation via knowledge distillation,\u201d arXiv preprint arXiv:2105.06143, 2021.", "[20] J. Hu, M. Ozay, Y. Zhang, and T. Okatani, \u201cRevisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2019, pp. 1043\u20131051.", "[12] G. Fang, J. Song, C. Shen, X. Wang, D. Chen, D. Chen, and M. Song, \u201cData-free adversarial distillation,\u201d ArXiv, vol. abs/1912.11006, 2019.", "[4] X. Chen, X. Chen, and Z.-J. Zha, \u201cStructure-aware residual pyramid network for monocular depth estimation,\u201d in International Joint Conferences on Artificial Intelligence, 2019, pp. 694\u2013700.", "[16] G. E. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d ArXiv, vol. abs/1503.02531, 2015."]}, {"table": "<table><tr><td>Method</td><td>Data</td><td>REL \\downarrow</td><td>\\delta_{1} \\uparrow</td></tr><tr><td>Teacher Model [18]</td><td rowspan=\"2\">ScanNet</td><td>0.150</td><td>0.790</td></tr><tr><td>Student Model [18]</td><td>0.165</td><td>0.764</td></tr><tr><td>Random noise</td><td rowspan=\"2\">None</td><td>0.539</td><td>0.079</td></tr><tr><td>DFAD [12]</td><td>0.335</td><td>0.368</td></tr><tr><td>KD-OOD [16]</td><td rowspan=\"2\">SceneNet \\mathcal{X}^{\\prime}_{1}</td><td>0.224</td><td>0.541</td></tr><tr><td>Ours</td><td>0.196</td><td>0.646</td></tr><tr><td>KD-OOD [16]</td><td rowspan=\"2\">SceneNet \\mathcal{X}^{\\prime}_{2}</td><td>0.200</td><td>0.618</td></tr><tr><td>Ours</td><td>0.185</td><td>0.693</td></tr></table>", "caption": "TABLE IV: The results provided by the models on the ScanNet dataset.", "list_citation_info": ["[16] G. E. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d ArXiv, vol. abs/1503.02531, 2015.", "[18] J. Hu, C. Fan, H. Jiang, X. Guo, Y. Gao, X. Lu, and T. L. Lam, \u201cBoosting light-weight depth estimation via knowledge distillation,\u201d arXiv preprint arXiv:2105.06143, 2021.", "[12] G. Fang, J. Song, C. Shen, X. Wang, D. Chen, D. Chen, and M. Song, \u201cData-free adversarial distillation,\u201d ArXiv, vol. abs/1912.11006, 2019."]}], "citation_info_to_title": {"[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248\u2013255.": "ImageNet: A Large-Scale Hierarchical Image Database", "[25] L. Iro, R. Christian, B. Vasileios, T. Federico, and N. Nassir, \u201cDeeper depth prediction with fully convolutional residual networks,\u201d in International Conference on 3D Vision (3DV), 2016, pp. 239\u2013248.": "Deeper depth prediction with fully convolutional residual networks", "[4] X. Chen, X. Chen, and Z.-J. Zha, \u201cStructure-aware residual pyramid network for monocular depth estimation,\u201d in International Joint Conferences on Artificial Intelligence, 2019, pp. 694\u2013700.": "Structure-aware residual pyramid network for monocular depth estimation", "[18] J. Hu, C. Fan, H. Jiang, X. Guo, Y. Gao, X. Lu, and T. L. Lam, \u201cBoosting light-weight depth estimation via knowledge distillation,\u201d arXiv preprint arXiv:2105.06143, 2021.": "Boosting light-weight depth estimation via knowledge distillation", "[16] G. E. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d ArXiv, vol. abs/1503.02531, 2015.": "Distilling the knowledge in a neural network", "[12] G. Fang, J. Song, C. Shen, X. Wang, D. Chen, D. Chen, and M. Song, \u201cData-free adversarial distillation,\u201d ArXiv, vol. abs/1912.11006, 2019.": "Data-free adversarial distillation", "[20] J. Hu, M. Ozay, Y. Zhang, and T. Okatani, \u201cRevisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2019, pp. 1043\u20131051.": "Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries", "[7] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \u201cScannet: Richly-annotated 3d reconstructions of indoor scenes,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5828\u20135839.": "Scannet: Richly-annotated 3d reconstructions of indoor scenes", "[49] J. Uhrig, N. Schneider, L. Schneider, U. Franke, T. Brox, and A. Geiger, \u201cSparsity invariant cnns,\u201d in International Conference on 3D Vision (3DV), 2017, pp. 11\u201320.": "Sparsity Invariant CNNs", "[37] J. McCormac, A. Handa, S. Leutenegger, and A. J. Davison, \u201cScenenet rgb-d: 5m photorealistic images of synthetic indoor trajectories with ground truth,\u201d arXiv preprint arXiv:1612.05079, 2016.": "Scenenet rgb-d: 5m photorealistic images of synthetic indoor trajectories with ground truth", "[46] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, \u201cIndoor segmentation and support inference from rgbd images,\u201d in European Conference on Computer Vision (ECCV), vol. 7576, 2012, pp. 746\u2013760.": "Indoor segmentation and support inference from rgbd images"}, "source_title_to_arxiv_id": {"Boosting light-weight depth estimation via knowledge distillation": "2105.06143"}}