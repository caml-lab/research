{"title": "Refine and Represent: Region-to-Object Representation Learning", "abstract": "Recent works in self-supervised learning have demonstrated strong performance\non scene-level dense prediction tasks by pretraining with object-centric or\nregion-based correspondence objectives. In this paper, we present\nRegion-to-Object Representation Learning (R2O) which unifies region-based and\nobject-centric pretraining. R2O operates by training an encoder to dynamically\nrefine region-based segments into object-centric masks and then jointly learns\nrepresentations of the contents within the mask. R2O uses a \"region refinement\nmodule\" to group small image regions, generated using a region-level prior,\ninto larger regions which tend to correspond to objects by clustering\nregion-level features. As pretraining progresses, R2O follows a\nregion-to-object curriculum which encourages learning region-level features\nearly on and gradually progresses to train object-centric representations.\nRepresentations learned using R2O lead to state-of-the art performance in\nsemantic segmentation for PASCAL VOC (+0.7 mIOU) and Cityscapes (+0.4 mIOU) and\ninstance segmentation on MS COCO (+0.3 mask AP). Further, after pretraining on\nImageNet, R2O pretrained models are able to surpass existing state-of-the-art\nin unsupervised object segmentation on the Caltech-UCSD Birds 200-2011 dataset\n(+2.9 mIoU) without any further training. We provide the code/models from this\nwork at https://github.com/KKallidromitis/r2o.", "authors": ["Akash Gokul", "Konstantinos Kallidromitis", "Shufan Li", "Yusuke Kato", "Kazuki Kozuka", "Trevor Darrell", "Colorado J Reed"], "published_date": "2022_08_25", "pdf_url": "http://arxiv.org/pdf/2208.11821v2", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><th colspan=\"2\">COCO 1\\times</th><th colspan=\"2\">COCO 2\\times</th></tr><tr><th>Method</th><th>AP<sup>bb</sup></th><th>AP<sup>mk</sup></th><th>AP<sup>bb</sup></th><th>AP<sup>mk</sup></th></tr><tr><td>Supervised</td><td>38.9</td><td>35.4</td><td>40.6</td><td>36.8</td></tr><tr><td>MoCo v2 [10]</td><td>38.9</td><td>35.4</td><td>40.9</td><td>37.0</td></tr><tr><td>BYOL<sup>\u2020</sup> [23]</td><td>40.6</td><td>37.5</td><td>42.0</td><td>38.7</td></tr><tr><td>DenseCL [56]</td><td>40.3</td><td>36.4</td><td>41.2</td><td>37.3</td></tr><tr><td>ReSim [60]</td><td>39.3</td><td>35.7</td><td>41.1</td><td>37.1</td></tr><tr><td>PixPro [63]</td><td>41.4</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DetCon<sub>B</sub><sup>\u2020</sup> [28]</td><td>41.5</td><td>38.0</td><td>42.1</td><td>38.9</td></tr><tr><td>DetCo [61]</td><td>39.4</td><td>34.4</td><td>41.4</td><td>35.8</td></tr><tr><td>LEWEL [31]</td><td>41.3</td><td>37.4</td><td>42.2</td><td>38.2</td></tr><tr><td>R2O</td><td>41.7</td><td>38.3</td><td>42.3</td><td>39.0</td></tr></tbody></table>", "caption": "Table 1: Performance on COCO object detection and instance segmentation following ImageNet pretraining. All methods pretrained a ResNet-50 which later served as the backbone of a Mask R-CNN R50-FPN finetuned on train2017 for 12 epochs (1\\times schedule) or 24 epochs (2\\times schedule). We report Average Precision on bounding box (AP<sup>bb</sup>) and mask (AP<sup>mk</sup>) predictions for val2017. <sup>\u2020</sup>: Results from re-implementation.", "list_citation_info": ["Wang et al. [2021] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.", "H\u00e9naff et al. [2021] O. J. H\u00e9naff, S. Koppula, J.-B. Alayrac, A. van den Oord, O. Vinyals, and J. Carreira. Efficient visual pretraining with contrastive detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10086\u201310096, 2021.", "Xiao et al. [2021] T. Xiao, C. J. Reed, X. Wang, K. Keutzer, and T. Darrell. Region similarity representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10539\u201310548, 2021.", "Huang et al. [2022] L. Huang, S. You, M. Zheng, F. Wang, C. Qian, and T. Yamasaki. Learning where to learn in cross-view self-supervised learning. arXiv preprint arXiv:2203.14898, 2022.", "Xie et al. [2021a] E. Xie, J. Ding, W. Wang, X. Zhan, H. Xu, P. Sun, Z. Li, and P. Luo. Detco: Unsupervised contrastive learning for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8392\u20138401, 2021a.", "Grill et al. [2020] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "Chen et al. [2020b] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.", "Xie et al. [2021c] Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693, 2021c."]}, {"table": "<table><thead><tr><th>Method</th><th>PASCAL VOC</th><th>Cityscapes</th></tr></thead><tbody><tr><th>Supervised</th><td>72.4</td><td>74.7</td></tr><tr><th>MoCo v2 [10]</th><td>73.9</td><td>75.6</td></tr><tr><th>BYOL<sup>\u2020</sup> [23]</th><td>75.0</td><td>75.8</td></tr><tr><th>DenseCL [56]</th><td>73.8</td><td>76.1</td></tr><tr><th>DetCon<sub>B</sub><sup>\u2020</sup> [28]</th><td>76.0</td><td>76.2</td></tr><tr><th>ReSim [60]</th><td>74.3</td><td>75.5</td></tr><tr><th>PixPro [63]</th><td>74.2</td><td>75.9</td></tr><tr><th>DetCo [61]</th><td>74.3</td><td>74.9</td></tr><tr><th>LEWEL [31]</th><td>75.5</td><td>75.4</td></tr><tr><th>R2O</th><td>76.7</td><td>76.6</td></tr></tbody></table>", "caption": "Table 2: Performance on PASCAL VOC and Cityscapes semantic segmentation (mIOU) following ImageNet pretraining. Unless otherwise stated, results are computing using pretrained weights officially released by the authors. <sup>\u2020</sup>: Results from re-implementation of pretraining method.", "list_citation_info": ["H\u00e9naff et al. [2021] O. J. H\u00e9naff, S. Koppula, J.-B. Alayrac, A. van den Oord, O. Vinyals, and J. Carreira. Efficient visual pretraining with contrastive detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10086\u201310096, 2021.", "Wang et al. [2021] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.", "Xiao et al. [2021] T. Xiao, C. J. Reed, X. Wang, K. Keutzer, and T. Darrell. Region similarity representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10539\u201310548, 2021.", "Huang et al. [2022] L. Huang, S. You, M. Zheng, F. Wang, C. Qian, and T. Yamasaki. Learning where to learn in cross-view self-supervised learning. arXiv preprint arXiv:2203.14898, 2022.", "Xie et al. [2021a] E. Xie, J. Ding, W. Wang, X. Zhan, H. Xu, P. Sun, Z. Li, and P. Luo. Detco: Unsupervised contrastive learning for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8392\u20138401, 2021a.", "Grill et al. [2020] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "Chen et al. [2020b] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.", "Xie et al. [2021c] Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693, 2021c."]}, {"table": "<table><tbody><tr><th>Method</th><td>CUB-200-2011</td></tr><tr><th>Self-Supervised Pretraining Methods</th><td></td></tr><tr><th>MoCo v2 [10]</th><td>63.5</td></tr><tr><th>BYOL<sup>\u2020</sup> [23]</th><td>65.1</td></tr><tr><th>Unsupervised Object Segmentation Methods</th><td></td></tr><tr><th>PerturbGAN [5]</th><td>38.0</td></tr><tr><th>ReDO [7]</th><td>42.6</td></tr><tr><th>OneGAN [4]</th><td>55.5</td></tr><tr><th>Voynov et al. [54]</th><td>68.3</td></tr><tr><th>Melas-Kyriazi et al. [39]</th><td>66.4</td></tr><tr><th>R2O</th><td>71.2</td></tr></tbody></table>", "caption": "Table 3: Performance on CUB-200-2011 segmentation. Results report the mean intersection-over-union (mIOU) for foreground-background segmentations of images in a Caltech-UCSD Birds 200-2011 (CUB-200-2011) test set [7]. Unlike [5, 7, 4, 54], we are able to produce state-of-the-art quality segmentations (+2.9 mIOU) using our ImageNet pretrained encoder and without finetuning on CUB-200-2011. <sup>\u2020</sup>: Results from re-implementation.", "list_citation_info": ["Bielski and Favaro [2019] A. Bielski and P. Favaro. Emergence of object segmentation in perturbed generative models. Advances in Neural Information Processing Systems, 32, 2019.", "Melas-Kyriazi et al. [2021] L. Melas-Kyriazi, C. Rupprecht, I. Laina, and A. Vedaldi. Finding an unsupervised image segmenter in each of your deep generative models. arXiv preprint arXiv:2105.08127, 2021.", "Voynov et al. [2021] A. Voynov, S. Morozov, and A. Babenko. Object segmentation without labels with large-scale generative models. In International Conference on Machine Learning, pages 10596\u201310606. PMLR, 2021.", "Chen et al. [2019] M. Chen, T. Arti\u00e8res, and L. Denoyer. Unsupervised object segmentation by redrawing. Advances in neural information processing systems, 32, 2019.", "Grill et al. [2020] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "Chen et al. [2020b] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.", "Benny and Wolf [2020] Y. Benny and L. Wolf. Onegan: Simultaneous unsupervised learning of conditional image generation, foreground segmentation, and fine-grained clustering. In European Conference on Computer Vision, pages 514\u2013530. Springer, 2020."]}, {"table": "<table><thead><tr><th>Method</th><th>PASCAL VOC</th><th>Cityscapes</th></tr></thead><tbody><tr><th>Supervised</th><td>72.4</td><td>74.7</td></tr><tr><th>MoCo v2 [10]</th><td>73.9 \\pm 0.12</td><td>75.6 \\pm 0.09</td></tr><tr><th>BYOL<sup>\u2020</sup> [23]</th><td>75.0 \\pm 0.22</td><td>75.8 \\pm 0.31</td></tr><tr><th>DenseCL [56]</th><td>73.8 \\pm 0.2</td><td>76.1 \\pm 0.12</td></tr><tr><th>DetCon<sub>B</sub><sup>\u2020</sup> [28]</th><td>76.0 \\pm 0.14</td><td>76.2 \\pm 0.09</td></tr><tr><th>ReSim [60]</th><td>74.3 \\pm 0.28</td><td>75.5 \\pm 0.15</td></tr><tr><th>PixPro [63]</th><td>74.2 \\pm 0.44</td><td>75.9 \\pm 0.37</td></tr><tr><th>DetCo [61]</th><td>74.3 \\pm 0.07</td><td>74.9 \\pm 0.46</td></tr><tr><th>LEWEL [31]</th><td>75.5 \\pm 0.23</td><td>75.4 \\pm 0.07</td></tr><tr><th>R2O</th><td>76.7 \\pm 0.09</td><td>76.6 \\pm 0.05</td></tr></tbody></table>", "caption": "Table A1: Performance on PASCAL VOC and Cityscapes semantic segmentation (mIoU) with standard deviation across 3 seeds. <sup>\u2020</sup>: Results from re-implementation of pretraining method.", "list_citation_info": ["H\u00e9naff et al. [2021] O. J. H\u00e9naff, S. Koppula, J.-B. Alayrac, A. van den Oord, O. Vinyals, and J. Carreira. Efficient visual pretraining with contrastive detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10086\u201310096, 2021.", "Wang et al. [2021] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.", "Xiao et al. [2021] T. Xiao, C. J. Reed, X. Wang, K. Keutzer, and T. Darrell. Region similarity representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10539\u201310548, 2021.", "Huang et al. [2022] L. Huang, S. You, M. Zheng, F. Wang, C. Qian, and T. Yamasaki. Learning where to learn in cross-view self-supervised learning. arXiv preprint arXiv:2203.14898, 2022.", "Xie et al. [2021a] E. Xie, J. Ding, W. Wang, X. Zhan, H. Xu, P. Sun, Z. Li, and P. Luo. Detco: Unsupervised contrastive learning for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8392\u20138401, 2021a.", "Grill et al. [2020] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.", "Chen et al. [2020b] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.", "Xie et al. [2021c] Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693, 2021c."]}, {"table": "<table><thead><tr><th>Objective</th><th>AP<sup>bb</sup></th><th>AP<sup>mk</sup></th></tr></thead><tbody><tr><th>MAE [27]</th><td>34.8</td><td>31.7</td></tr><tr><th>R2O</th><td>40.5</td><td>36.6</td></tr></tbody></table>", "caption": "Table A4: Examining COCO object detection and instance segmentation performance when pretraining a ViT. We report Average Precision on bounding box (AP<sup>bb</sup>) and mask (AP<sup>mk</sup>) predictions for val2017", "list_citation_info": ["He et al. [2022] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022."]}], "citation_info_to_title": {"H\u00e9naff et al. [2021] O. J. H\u00e9naff, S. Koppula, J.-B. Alayrac, A. van den Oord, O. Vinyals, and J. Carreira. Efficient visual pretraining with contrastive detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10086\u201310096, 2021.": "Efficient visual pretraining with contrastive detection", "Xie et al. [2021c] Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu. Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16684\u201316693, 2021c.": "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning", "Xie et al. [2021a] E. Xie, J. Ding, W. Wang, X. Zhan, H. Xu, P. Sun, Z. Li, and P. Luo. Detco: Unsupervised contrastive learning for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8392\u20138401, 2021a.": "Detco: Unsupervised contrastive learning for object detection", "Chen et al. [2020b] X. Chen, H. Fan, R. Girshick, and K. He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.": "Improved baselines with momentum contrastive learning", "Xiao et al. [2021] T. Xiao, C. J. Reed, X. Wang, K. Keutzer, and T. Darrell. Region similarity representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10539\u201310548, 2021.": "Region similarity representation learning", "Grill et al. [2020] J.-B. Grill, F. Strub, F. Altch\u00e9, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.": "Bootstrap your own latent-a new approach to self-supervised learning", "Bielski and Favaro [2019] A. Bielski and P. Favaro. Emergence of object segmentation in perturbed generative models. Advances in Neural Information Processing Systems, 32, 2019.": "Emergence of object segmentation in perturbed generative models", "Voynov et al. [2021] A. Voynov, S. Morozov, and A. Babenko. Object segmentation without labels with large-scale generative models. In International Conference on Machine Learning, pages 10596\u201310606. PMLR, 2021.": "Object Segmentation Without Labels with Large-Scale Generative Models", "Chen et al. [2019] M. Chen, T. Arti\u00e8res, and L. Denoyer. Unsupervised object segmentation by redrawing. Advances in neural information processing systems, 32, 2019.": "Unsupervised Object Segmentation by Redrawing", "He et al. [2022] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.": "Masked autoencoders are scalable vision learners", "Benny and Wolf [2020] Y. Benny and L. Wolf. Onegan: Simultaneous unsupervised learning of conditional image generation, foreground segmentation, and fine-grained clustering. In European Conference on Computer Vision, pages 514\u2013530. Springer, 2020.": "Onegan: Simultaneous unsupervised learning of conditional image generation, foreground segmentation, and fine-grained clustering", "Wang et al. [2021] X. Wang, R. Zhang, C. Shen, T. Kong, and L. Li. Dense contrastive learning for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3024\u20133033, 2021.": "Dense contrastive learning for self-supervised visual pre-training", "Huang et al. [2022] L. Huang, S. You, M. Zheng, F. Wang, C. Qian, and T. Yamasaki. Learning where to learn in cross-view self-supervised learning. arXiv preprint arXiv:2203.14898, 2022.": "Learning where to learn in cross-view self-supervised learning", "Melas-Kyriazi et al. [2021] L. Melas-Kyriazi, C. Rupprecht, I. Laina, and A. Vedaldi. Finding an unsupervised image segmenter in each of your deep generative models. arXiv preprint arXiv:2105.08127, 2021.": "Finding an unsupervised image segmenter in each of your deep generative models"}, "source_title_to_arxiv_id": {"Efficient visual pretraining with contrastive detection": "2103.10957", "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning": "2011.10043", "Masked autoencoders are scalable vision learners": "2111.06377", "Learning where to learn in cross-view self-supervised learning": "2203.14898"}}