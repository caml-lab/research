{"title": "CLIP model is an Efficient Continual Learner", "abstract": "The continual learning setting aims to learn new tasks over time without\nforgetting the previous ones. The literature reports several significant\nefforts to tackle this problem with limited or no access to previous task data.\nAmong such efforts, typical solutions offer sophisticated techniques involving\nmemory replay, knowledge distillation, model regularization, and dynamic\nnetwork expansion. The resulting methods have a retraining cost at each\nlearning task, dedicated memory requirements, and setting-specific design\nchoices. In this work, we show that a frozen CLIP (Contrastive Language-Image\nPretraining) model offers astounding continual learning performance without any\nfine-tuning (zero-shot evaluation). We evaluate CLIP under a variety of\nsettings including class-incremental, domain-incremental and task-agnostic\nincremental learning on five popular benchmarks (ImageNet-100 & 1K, CORe50,\nCIFAR-100, and TinyImageNet). Without any bells and whistles, the CLIP model\noutperforms the state-of-the-art continual learning approaches in the majority\nof the settings. We show the effect on the CLIP model's performance by varying\ntext inputs with simple prompt templates. To the best of our knowledge, this is\nthe first work to report the CLIP zero-shot performance in a continual setting.\nWe advocate the use of this strong yet embarrassingly simple baseline for\nfuture comparisons in the continual learning tasks.", "authors": ["Vishal Thengane", "Salman Khan", "Munawar Hayat", "Fahad Khan"], "published_date": "2022_10_06", "pdf_url": "http://arxiv.org/pdf/2210.03114v1", "list_table_and_caption": [{"table": "<table><thead><tr><th></th><th colspan=\"2\">10 steps</th><th colspan=\"2\">20 steps</th><th colspan=\"2\">50 steps</th></tr><tr><th>Methods</th><th>Avg</th><th>Last</th><th>Avg</th><th>Last</th><th>Avg</th><th>Last</th></tr></thead><tbody><tr><th>iCaRL (Rebuffi et al., 2017)</th><td>65.27</td><td>50.74</td><td>61.20</td><td>43.74</td><td>56.08</td><td>36.62</td></tr><tr><th>UCIR (Hou et al., 2019)</th><td>58.66</td><td>43.39</td><td>58.17</td><td>40.63</td><td>56.86</td><td>37.09</td></tr><tr><th>BiC (Wu et al., 2019)</th><td>68.80</td><td>53.54</td><td>66.48</td><td>47.02</td><td>62.09</td><td>41.04</td></tr><tr><th>RPSNet (Rajasegaran et al., 2019b)</th><td>68.60</td><td>57.05</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>WA (Zhao et al., 2020)</th><td>69.46</td><td>53.78</td><td>67.33</td><td>47.31</td><td>64.32</td><td>42.14</td></tr><tr><th>PODNet (Douillard et al., 2020)</th><td>58.03</td><td>41.05</td><td>53.97</td><td>35.02</td><td>51.19</td><td>32.99</td></tr><tr><th>DER (w/o P) (Yan et al., 2021)</th><td>75.36</td><td>65.22</td><td>74.09</td><td>62.48</td><td>72.41</td><td>59.08</td></tr><tr><th>DER (Yan et al., 2021)</th><td>74.64</td><td>64.35</td><td>73.98</td><td>62.55</td><td>72.05</td><td>59.76</td></tr><tr><th>DyTox (Douillard et al., 2022)</th><td>67.33</td><td>51.68</td><td>67.30</td><td>48.45</td><td>64.39</td><td>43.47</td></tr><tr><th>DyTox+ (Douillard et al., 2022)</th><td>74.10</td><td>62.34</td><td>71.62</td><td>57.43</td><td>68.90</td><td>51.09</td></tr><tr><th>Continual-CLIP</th><td>75.17</td><td>66.72</td><td>75.95</td><td>66.72</td><td>76.49</td><td>66.72</td></tr></tbody></table>", "caption": "Table 1: Comparison of state-of-the-art CL methods on CIFAR100 benchmark in class-incremental setting, in terms of the average and last task accuracy values.", "list_citation_info": ["Wu et al. (2019) Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 374\u2013382, 2019.", "Hou et al. (2019) Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 831\u2013839, 2019.", "Douillard et al. (2022) Arthur Douillard, Alexandre Ram\u00e9, Guillaume Couairon, and Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9285\u20139295, June 2022.", "Yan et al. (2021) Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3014\u20133023, June 2021.", "Zhao et al. (2020) Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13208\u201313217, 2020.", "Douillard et al. (2020) Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In In Proceedings of the European Conference on Computer Vision, pp. 86\u2013102. Springer, 2020.", "Rajasegaran et al. (2019b) Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An adaptive random path selection approach for incremental learning. arXiv preprint arXiv:1906.01120, 2019b.", "Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2001\u20132010, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">ImageNet100-B0</th><th colspan=\"2\">ImageNet1K</th><th colspan=\"2\">ImageNet100-B50</th></tr><tr><th>Methods</th><th>Avg</th><th>Last</th><th>Avg</th><th>Last</th><th>Avg</th><th>Last</th></tr></thead><tbody><tr><th>iCaRL (Rebuffi et al., 2017)</th><td>-</td><td>-</td><td>38.40</td><td>22.70</td><td>-</td><td>-</td></tr><tr><th>UCIR (Hou et al., 2019)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>68.09</td><td>57.30</td></tr><tr><th>WA (Zhao et al., 2020)</th><td>-</td><td>-</td><td>65.67</td><td>55.60</td><td>-</td><td>-</td></tr><tr><th>TPCIL (Tao et al., 2020)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>74.81</td><td>66.91</td></tr><tr><th>PODNet (Douillard et al., 2020)</th><td>-</td><td>-</td><td>-</td><td>-</td><td>74.33</td><td>-</td></tr><tr><th>Simple-DER (Li et al., 2021b)</th><td>-</td><td>-</td><td>66.63</td><td>59.24</td><td>-</td><td>-</td></tr><tr><th>DER (w/o P) (Yan et al., 2021)</th><td>77.18</td><td>66.70</td><td>68.84</td><td>60.16</td><td>78.20</td><td>74.92</td></tr><tr><th>DER (Yan et al., 2021)</th><td>76.12</td><td>66.06</td><td>66.73</td><td>58.62</td><td>77.13</td><td>72.06</td></tr><tr><th>DyTox (Douillard et al., 2022)</th><td>73.96</td><td>62.20</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>DyTox+ (Douillard et al., 2022)</th><td>77.15</td><td>67.70</td><td>70.88</td><td>60.00</td><td>-</td><td>-</td></tr><tr><th>Continual-CLIP</th><td>85.00</td><td>75.42</td><td>75.51</td><td>67.71</td><td>79.69</td><td>75.42</td></tr></tbody></table>", "caption": "Table 2: Comparison of state-of-the-art CL methods on different ImageNet benchmarks, in class-incremental settings with 10 splits, in terms of average and last accuracy values.", "list_citation_info": ["Hou et al. (2019) Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 831\u2013839, 2019.", "Douillard et al. (2022) Arthur Douillard, Alexandre Ram\u00e9, Guillaume Couairon, and Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9285\u20139295, June 2022.", "Tao et al. (2020) Xiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Xing Wei, and Yihong Gong. Topology-preserving class-incremental learning. In In Proceedings of the European Conference on Computer Vision, pp. 254\u2013270. Springer, 2020.", "Yan et al. (2021) Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3014\u20133023, June 2021.", "Zhao et al. (2020) Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13208\u201313217, 2020.", "Li et al. (2021b) Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang, and Wei-Shi Zheng. Preserving earlier knowledge in continual learning with the help of all previous feature extractors, 2021b. URL https://arxiv.org/abs/2104.13614.", "Douillard et al. (2020) Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In In Proceedings of the European Conference on Computer Vision, pp. 86\u2013102. Springer, 2020.", "Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2001\u20132010, 2017."]}, {"table": "<table><thead><tr><th></th><th colspan=\"2\">5 steps</th><th colspan=\"2\">10 steps</th><th colspan=\"2\">20 steps</th></tr><tr><th>Methods</th><th>Avg</th><th>Last</th><th>Avg</th><th>Last</th><th>Avg</th><th>Last</th></tr></thead><tbody><tr><th>EWC (Kirkpatrick et al., 2017)</th><td>19.01</td><td>6.00</td><td>15.82</td><td>3.79</td><td>12.35</td><td>4.73</td></tr><tr><th>LwF (Li &amp; Hoiem, 2017)</th><td>22.31</td><td>7.34</td><td>17.34</td><td>4.73</td><td>12.48</td><td>4.26</td></tr><tr><th>LwF-MC (Rebuffi et al., 2017)</th><td>29.09</td><td>15.63</td><td>23.03</td><td>13.25</td><td>17.31</td><td>7.95</td></tr><tr><th>iCaRL (Rebuffi et al., 2017)</th><td>34.27</td><td>23.22</td><td>30.94</td><td>20.82</td><td>27.83</td><td>20.16</td></tr><tr><th>iCaRL-NCM (Rebuffi et al., 2017)</th><td>45.95</td><td>34.60</td><td>43.22</td><td>33.22</td><td>37.85</td><td>27.54</td></tr><tr><th>EEIL (Castro et al., 2018)</th><td>47.17</td><td>35.12</td><td>45.03</td><td>34.64</td><td>40.41</td><td>29.72</td></tr><tr><th>UCIR (Hou et al., 2019)</th><td>50.30</td><td>39.42</td><td>48.58</td><td>37.29</td><td>42.84</td><td>30.85</td></tr><tr><th>MUC (Liu et al., 2020)</th><td>32.23</td><td>19.20</td><td>26.67</td><td>15.33</td><td>21.89</td><td>10.32</td></tr><tr><th>PASS (Zhu et al., 2021)</th><td>49.54</td><td>41.64</td><td>47.19</td><td>39.27</td><td>42.01</td><td>32.93</td></tr><tr><th>DyTox (Douillard et al., 2022)</th><td>55.58</td><td>47.23</td><td>52.26</td><td>42.79</td><td>46.18</td><td>36.21</td></tr><tr><th>Continual-CLIP</th><td>70.49</td><td>66.43</td><td>70.55</td><td>66.43</td><td>70.51</td><td>66.43</td></tr></tbody></table>", "caption": "Table 3: Comparison of state-of-the-art CL methods on different TinyImageNet splits in class-incremental settings with 50 base classes, in terms of the average and last accuracy values.", "list_citation_info": ["Castro et al. (2018) Francisco M Castro, Manuel J Mar\u00edn-Jim\u00e9nez, Nicol\u00e1s Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In In Proceedings of the European conference on computer vision, pp. 233\u2013248, 2018.", "Hou et al. (2019) Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 831\u2013839, 2019.", "Li & Hoiem (2017) Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.", "Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.", "Douillard et al. (2022) Arthur Douillard, Alexandre Ram\u00e9, Guillaume Couairon, and Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9285\u20139295, June 2022.", "Liu et al. (2020) Yu Liu, Sarah Parisot, Gregory Slabaugh, Xu Jia, Ales Leonardis, and Tinne Tuytelaars. More classifiers, less forgetting: A generic multi-classifier paradigm for incremental learning. In In Proceedings of the European Conference on Computer Vision, pp. 699\u2013716. Springer, 2020.", "Zhu et al. (2021) Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and self-supervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5871\u20135880, 2021.", "Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2001\u20132010, 2017."]}, {"table": "<table><thead><tr><th>Datasets</th><th>Methods</th><th>Overall Acc</th><th>Next-domain</th><th>In-domain</th><th>Backward</th><th>Forward</th></tr></thead><tbody><tr><th rowspan=\"2\">CLEAR-10</th><td>Top-1 team</td><td>92.70</td><td>92.50</td><td>93.40</td><td>94.20</td><td>90.90</td></tr><tr><td>Continual-CLIP</td><td>93.79</td><td>93.51</td><td>93.58</td><td>93.83</td><td>93.33</td></tr><tr><th rowspan=\"2\">CLEAR-100</th><td>Top-1 team</td><td>91.46</td><td>91.25</td><td>91.99</td><td>93.40</td><td>89.20</td></tr><tr><td>Continual-CLIP</td><td>93.63</td><td>93.50</td><td>93.50</td><td>93.66</td><td>93.33</td></tr></tbody></table>", "caption": "Table 4: Domain incremental setting comparison with winning team of the recent CVPR 2022 Continual LEArning on Real-World Imagery (CLEAR) Challenge (AIcrowd, 2022; Lin et al., 2021).", "list_citation_info": ["AIcrowd (2022) AIcrowd. Cvpr 2022 clear challenge: Leaderboards, 2022. URL https://www.aicrowd.com/challenges/cvpr-2022-clear-challenge/leaderboards."]}, {"table": "<table><thead><tr><th>Methods</th><th>Test Acc (%)</th></tr></thead><tbody><tr><th>Na\u00efve (Lomonaco &amp; Maltoni, 2017)</th><td>54.69</td></tr><tr><th>EWC (Kirkpatrick et al., 2017)</th><td>57.40</td></tr><tr><th>LwF (Zhao et al., 2020)</th><td>59.42</td></tr><tr><th>Cumulative (Lomonaco &amp; Maltoni, 2017)</th><td>65.15</td></tr><tr><th>GDM (Parisi et al., 2018)</th><td>74.87</td></tr><tr><th>Continual-CLIP</th><td>84.73</td></tr></tbody></table>", "caption": "Table 5: Core50 dataset comparisons with other baselines in domain incremental setting. The values for compared methods are from (Lomonaco &amp; Maltoni, 2017; Parisi et al., 2018).", "list_citation_info": ["Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.", "Parisi et al. (2018) German I Parisi, Jun Tani, Cornelius Weber, and Stefan Wermter. Lifelong learning of spatiotemporal representations with dual-memory recurrent self-organization. Frontiers in neurorobotics, 12:78, 2018.", "Lomonaco & Maltoni (2017) Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pp. 17\u201326. PMLR, 2017.", "Zhao et al. (2020) Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13208\u201313217, 2020."]}, {"table": "<table><thead><tr><th>Methods</th><th>Test Acc (%)</th></tr></thead><tbody><tr><th>Encoders and Ensemble (Shanahan et al., 2021)</th><td>39.0</td></tr><tr><th>Continual-CLIP</th><td>66.72</td></tr></tbody></table>", "caption": "Table 6: Comparisons in Task-agnostic setting on Core50 dataset. ", "list_citation_info": ["Shanahan et al. (2021) Murray Shanahan, Christos Kaplanis, and Jovana Mitrovi\u0107. Encoders and ensembles for task-free continual learning. arXiv preprint arXiv:2105.13327, 2021."]}, {"table": "<table><thead><tr><th>Class Names Type</th><th>Avg Acc (%)</th><th>Last Acc (%)</th></tr></thead><tbody><tr><th>(a) ImageNet default</th><td>72.96</td><td>64.44</td></tr><tr><th>(b) Radford et al. (2021) curated</th><td>74.81</td><td>66.58</td></tr><tr><th>(c) First synonym from each subset</th><td>71.65</td><td>63.97</td></tr></tbody></table>", "caption": "Table 7: Effect of using different class names on the Continual-CLIP accuracy on ImageNet-1k with a prompt \u201ca photo of a {}.\u201d", "list_citation_info": ["Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021."]}], "citation_info_to_title": {"Li & Hoiem (2017) Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947, 2017.": "Learning without forgetting", "Lomonaco & Maltoni (2017) Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pp. 17\u201326. PMLR, 2017.": "Core50: a new dataset and benchmark for continuous object recognition", "Li et al. (2021b) Zhuoyun Li, Changhong Zhong, Sijia Liu, Ruixuan Wang, and Wei-Shi Zheng. Preserving earlier knowledge in continual learning with the help of all previous feature extractors, 2021b. URL https://arxiv.org/abs/2104.13614.": "Preserving earlier knowledge in continual learning with the help of all previous feature extractors", "Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.": "Overcoming catastrophic forgetting in neural networks", "Shanahan et al. (2021) Murray Shanahan, Christos Kaplanis, and Jovana Mitrovi\u0107. Encoders and ensembles for task-free continual learning. arXiv preprint arXiv:2105.13327, 2021.": "Encoders and ensembles for task-free continual learning", "Liu et al. (2020) Yu Liu, Sarah Parisot, Gregory Slabaugh, Xu Jia, Ales Leonardis, and Tinne Tuytelaars. More classifiers, less forgetting: A generic multi-classifier paradigm for incremental learning. In In Proceedings of the European Conference on Computer Vision, pp. 699\u2013716. Springer, 2020.": "More classifiers, less forgetting: A generic multi-classifier paradigm for incremental learning", "Zhao et al. (2020) Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia. Maintaining discrimination and fairness in class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13208\u201313217, 2020.": "Maintaining discrimination and fairness in class incremental learning", "Zhu et al. (2021) Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu. Prototype augmentation and self-supervision for incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5871\u20135880, 2021.": "Prototype Augmentation and Self-Supervision for Incremental Learning", "Hou et al. (2019) Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incrementally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 831\u2013839, 2019.": "Learning a unified classifier incrementally via rebalancing", "Douillard et al. (2022) Arthur Douillard, Alexandre Ram\u00e9, Guillaume Couairon, and Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9285\u20139295, June 2022.": "Dytox: Transformers for continual learning with dynamic token expansion", "Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2001\u20132010, 2017.": "icarl: Incremental classifier and representation learning", "Tao et al. (2020) Xiaoyu Tao, Xinyuan Chang, Xiaopeng Hong, Xing Wei, and Yihong Gong. Topology-preserving class-incremental learning. In In Proceedings of the European Conference on Computer Vision, pp. 254\u2013270. Springer, 2020.": "Topology-Preserving Class-Incremental Learning", "Castro et al. (2018) Francisco M Castro, Manuel J Mar\u00edn-Jim\u00e9nez, Nicol\u00e1s Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In In Proceedings of the European conference on computer vision, pp. 233\u2013248, 2018.": "End-to-end incremental learning", "Rajasegaran et al. (2019b) Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, Ling Shao, and Ming-Hsuan Yang. An adaptive random path selection approach for incremental learning. arXiv preprint arXiv:1906.01120, 2019b.": "An adaptive random path selection approach for incremental learning", "AIcrowd (2022) AIcrowd. Cvpr 2022 clear challenge: Leaderboards, 2022. URL https://www.aicrowd.com/challenges/cvpr-2022-clear-challenge/leaderboards.": "AIcrowd: CVPR 2022 Clear Challenge Leaderboards", "Parisi et al. (2018) German I Parisi, Jun Tani, Cornelius Weber, and Stefan Wermter. Lifelong learning of spatiotemporal representations with dual-memory recurrent self-organization. Frontiers in neurorobotics, 12:78, 2018.": "Lifelong learning of spatiotemporal representations with dual-memory recurrent self-organization", "Douillard et al. (2020) Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. In In Proceedings of the European Conference on Computer Vision, pp. 86\u2013102. Springer, 2020.": "Podnet: Pooled outputs distillation for small-tasks incremental learning", "Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.": "Learning transferable visual models from natural language supervision", "Yan et al. (2021) Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3014\u20133023, June 2021.": "Der: Dynamically expandable representation for class incremental learning", "Wu et al. (2019) Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Large scale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 374\u2013382, 2019.": "Large Scale Incremental Learning"}, "source_title_to_arxiv_id": {"Encoders and ensembles for task-free continual learning": "2105.13327", "Dytox: Transformers for continual learning with dynamic token expansion": "2111.11326", "Podnet: Pooled outputs distillation for small-tasks incremental learning": "2004.13513", "Der: Dynamically expandable representation for class incremental learning": "2103.16788"}}