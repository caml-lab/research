{"title": "Thinking Hallucination for Video Captioning", "abstract": "With the advent of rich visual representations and pre-trained language\nmodels, video captioning has seen continuous improvement over time. Despite the\nperformance improvement, video captioning models are prone to hallucination.\nHallucination refers to the generation of highly pathological descriptions that\nare detached from the source material. In video captioning, there are two kinds\nof hallucination: object and action hallucination. Instead of endeavoring to\nlearn better representations of a video, in this work, we investigate the\nfundamental sources of the hallucination problem. We identify three main\nfactors: (i) inadequate visual features extracted from pre-trained models, (ii)\nimproper influences of source and target contexts during multi-modal fusion,\nand (iii) exposure bias in the training strategy. To alleviate these problems,\nwe propose two robust solutions: (a) the introduction of auxiliary heads\ntrained in multi-label settings on top of the extracted visual features and (b)\nthe addition of context gates, which dynamically select the features during\nfusion. The standard evaluation metrics for video captioning measures\nsimilarity with ground truth captions and do not adequately capture object and\naction relevance. To this end, we propose a new metric, COAHA (caption object\nand action hallucination assessment), which assesses the degree of\nhallucination. Our method achieves state-of-the-art performance on the\nMSR-Video to Text (MSR-VTT) and the Microsoft Research Video Description Corpus\n(MSVD) datasets, especially by a massive margin in CIDEr score.", "authors": ["Nasib Ullah", "Partha Pratim Mohanta"], "published_date": "2022_09_28", "pdf_url": "http://arxiv.org/pdf/2209.13853v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Models</th><th></th><th>MSVD</th><th></th><th></th><th></th><th>MSR-VTT</th><th></th><th></th></tr><tr><th></th><th>B@4</th><th>M</th><th>R</th><th>C</th><th>B@4</th><th>M</th><th>R</th><th>C</th></tr></thead><tbody><tr><th>SA-LSTM [43]</th><td>45.3</td><td>31.9</td><td>64.2</td><td>76.2</td><td>36.3</td><td>25.5</td><td>58.3</td><td>39.9</td></tr><tr><th>h-RNN [44]</th><td>44.3</td><td>31.1</td><td>-</td><td>62.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><th>hLSTMat [30]</th><td>53.0</td><td>33.6</td><td>-</td><td>73.8</td><td>38.3</td><td>26.3</td><td>-</td><td>-</td></tr><tr><th>RecNet [2]</th><td>52.3</td><td>34.1</td><td>69.8</td><td>80.3</td><td>39.1</td><td>26.6</td><td>59.3</td><td>42.7</td></tr><tr><th>M3 [39]</th><td>52.8</td><td>33.3</td><td>-</td><td>-</td><td>38.1</td><td>26.6</td><td>-</td><td>-</td></tr><tr><th>PickNet [9]</th><td>52.3</td><td>33.3</td><td>69.6</td><td>76.5</td><td>41.3</td><td>27.7</td><td>59.8</td><td>44.1</td></tr><tr><th>MARN [23]</th><td>48.6</td><td>35.1</td><td>71.9</td><td>92.2</td><td>40.4</td><td>28.1</td><td>60.7</td><td>47.1</td></tr><tr><th>GRU-EVE [1]</th><td>47.9</td><td>35.0</td><td>71.5</td><td>78.1</td><td>38.3</td><td>28.4</td><td>60.7</td><td>48.1</td></tr><tr><th>POS+CG [37]</th><td>52.5</td><td>34.1</td><td>71.3</td><td>88.7</td><td>42.0</td><td>28.2</td><td>61.6</td><td>48.7</td></tr><tr><th>OA-BTG [45]</th><td>56.9</td><td>36.2</td><td>-</td><td>90.6</td><td>41.4</td><td>28.2</td><td>-</td><td>46.9</td></tr><tr><th>STG-KD [20]</th><td>52.2</td><td>36.9</td><td>73.9</td><td>93.0</td><td>40.5</td><td>28.3</td><td>60.9</td><td>47.1</td></tr><tr><th>SAAT [46]</th><td>46.5</td><td>33.5</td><td>69.4</td><td>81.0</td><td>40.5</td><td>28.2</td><td>60.9</td><td>49.1</td></tr><tr><th>ORG-TRL [47]</th><td>54.3</td><td>36.4</td><td>73.9</td><td>95.2</td><td>43.6</td><td>28.8</td><td>62.1</td><td>50.9</td></tr><tr><th>SGN [29]</th><td>52.8</td><td>35.5</td><td>72.9</td><td>94.3</td><td>40.8</td><td>28.3</td><td>60.8</td><td>49.5</td></tr><tr><th>Ours</th><td>53.3</td><td>36.5</td><td>74.0</td><td>99.9</td><td>41.1</td><td>28.9</td><td>61.9</td><td>51.7</td></tr></tbody></table>", "caption": "Table 1: Performance comparison on MSVD and MSR-VTT benchmarks. B4, M, R, and C denote BLEU-4, METEOR, ROUGE_L, and CIDEr, respectively.", "list_citation_info": ["[44] Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using hierarchical recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016)", "[23] Pei, W., Zhang, J., Wang, X., Ke, L., Shen, X., Tai, Y.: Memory-attended recurrent network for video captioning. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 8347\u20138356. Computer Vision Foundation / IEEE (2019)", "[29] Ryu, H., Kang, S., Kang, H., Yoo, C.D.: Semantic grouping network for video captioning. Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021 (2021)", "[45] Zhang, J., Peng, Y.: Object-aware aggregation with bidirectional temporal graph for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2019)", "[39] Wang, J., Wang, W., Huang, Y., Wang, L., Tan, T.: M3: Multimodal memory modelling for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018)", "[2] Bairui, W., Lin, M., Wei, Z., Wei, L.: Reconstruction network for video captioning. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018)", "[47] Ziqi, Z., Shi, Y., Yuan, C., Li, B., Wang, P., Hu, W., Zha, Z.: Object relational graph with teacher- recommended learning for video captioning. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)", "[9] Chen, Y., Wang, S., Zhang, W., Huang, Q.: Less is more: Picking informative frames for video captioning. In Proceedings of the European Conference on Computer Vision. (2018)", "[20] Pan, B., Cai, H., Huang, D., Lee, K., Gaidon, A., Adeli, E., Niebles, J.C.: Spatio-temporal graph for video captioning with knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)", "[30] Song, J., Gao, L., Guo, Z., Liu, W., Zhang, D., Shen, H.T.: Hierarchical lstm with adjusted temporal attention for video captioning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (2017)", "[1] Aafaq, N., Akhtar, N., Liu, W., Gilani, S.Z., Mian, A.: Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, pages 12487\u201312496 (2019)", "[46] Zheng, Q., Wang, C., Tao, D.: Syntax-aware action targeting for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)", "[37] Wang, B., Ma, L., Zhang, W., Jiang, W., Wang, J., Liu, W.: Controllable video captioning with POS sequence guidance based on gated fusion network. In The IEEE International Conference on Computer Vision (ICCV) (2019)", "[43] Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C.J., Larochelle, H., Courville, A.C.: Describing videos by exploiting temporal structure. In: 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015. pp. 4507\u20134515. IEEE Computer Society (2015). https://doi.org/10.1109/ICCV.2015.512, https://doi.org/10.1109/ICCV.2015.512"]}], "citation_info_to_title": {"[20] Pan, B., Cai, H., Huang, D., Lee, K., Gaidon, A., Adeli, E., Niebles, J.C.: Spatio-temporal graph for video captioning with knowledge distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)": "Spatio-temporal graph for video captioning with knowledge distillation", "[9] Chen, Y., Wang, S., Zhang, W., Huang, Q.: Less is more: Picking informative frames for video captioning. In Proceedings of the European Conference on Computer Vision. (2018)": "Less is more: Picking informative frames for video captioning", "[2] Bairui, W., Lin, M., Wei, Z., Wei, L.: Reconstruction network for video captioning. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018)": "Reconstruction Network for Video Captioning", "[23] Pei, W., Zhang, J., Wang, X., Ke, L., Shen, X., Tai, Y.: Memory-attended recurrent network for video captioning. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. pp. 8347\u20138356. Computer Vision Foundation / IEEE (2019)": "Memory-attended recurrent network for video captioning", "[45] Zhang, J., Peng, Y.: Object-aware aggregation with bidirectional temporal graph for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2019)": "Object-aware aggregation with bidirectional temporal graph for video captioning", "[44] Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using hierarchical recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016)": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks", "[43] Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C.J., Larochelle, H., Courville, A.C.: Describing videos by exploiting temporal structure. In: 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015. pp. 4507\u20134515. IEEE Computer Society (2015). https://doi.org/10.1109/ICCV.2015.512, https://doi.org/10.1109/ICCV.2015.512": "Describing videos by exploiting temporal structure", "[39] Wang, J., Wang, W., Huang, Y., Wang, L., Tan, T.: M3: Multimodal memory modelling for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018)": "M3: Multimodal memory modelling for video captioning", "[37] Wang, B., Ma, L., Zhang, W., Jiang, W., Wang, J., Liu, W.: Controllable video captioning with POS sequence guidance based on gated fusion network. In The IEEE International Conference on Computer Vision (ICCV) (2019)": "Controllable video captioning with POS sequence guidance based on gated fusion network", "[47] Ziqi, Z., Shi, Y., Yuan, C., Li, B., Wang, P., Hu, W., Zha, Z.: Object relational graph with teacher- recommended learning for video captioning. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)": "Object relational graph with teacher-recommended learning for video captioning", "[29] Ryu, H., Kang, S., Kang, H., Yoo, C.D.: Semantic grouping network for video captioning. Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021 (2021)": "Semantic grouping network for video captioning", "[1] Aafaq, N., Akhtar, N., Liu, W., Gilani, S.Z., Mian, A.: Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, pages 12487\u201312496 (2019)": "Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning", "[30] Song, J., Gao, L., Guo, Z., Liu, W., Zhang, D., Shen, H.T.: Hierarchical lstm with adjusted temporal attention for video captioning. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (2017)": "Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning", "[46] Zheng, Q., Wang, C., Tao, D.: Syntax-aware action targeting for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020)": "Syntax-aware action targeting for video captioning"}, "source_title_to_arxiv_id": {"Semantic grouping network for video captioning": "2102.00831"}}