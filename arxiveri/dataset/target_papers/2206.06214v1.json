{"title": "Learning a Degradation-Adaptive Network for Light Field Image Super-Resolution", "abstract": "Recent years have witnessed the great advances of deep neural networks (DNNs)\nin light field (LF) image super-resolution (SR). However, existing DNN-based LF\nimage SR methods are developed on a single fixed degradation (e.g., bicubic\ndownsampling), and thus cannot be applied to super-resolve real LF images with\ndiverse degradations. In this paper, we propose the first method to handle LF\nimage SR with multiple degradations. In our method, a practical LF degradation\nmodel that considers blur and noise is developed to approximate the degradation\nprocess of real LF images. Then, a degradation-adaptive network (LF-DAnet) is\ndesigned to incorporate the degradation prior into the SR process. By training\non LF images with multiple synthetic degradations, our method can learn to\nadapt to different degradations while incorporating the spatial and angular\ninformation. Extensive experiments on both synthetically degraded and\nreal-world LFs demonstrate the effectiveness of our method. Compared with\nexisting state-of-the-art single and LF image SR methods, our method achieves\nsuperior SR performance under a wide range of degradations, and generalizes\nbetter to real LF images. Codes and models are available at\nhttps://github.com/YingqianWang/LF-DAnet.", "authors": ["Yingqian Wang", "Zhengyu Liang", "Longguang Wang", "Jungang Yang", "Wei An", "Yulan Guo"], "published_date": "2022_06_13", "pdf_url": "http://arxiv.org/pdf/2206.06214v1", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>Kernel</td><td colspan=\"3\">HCInew</td><td colspan=\"3\">HCIold</td><td colspan=\"3\">STFgantry</td></tr><tr><td colspan=\"2\">Noise level</td><td>0</td><td>15</td><td>50</td><td>0</td><td>15</td><td>50</td><td>0</td><td>15</td><td>50</td></tr><tr><td>Bicubic</td><td rowspan=\"8\">0</td><td>27.71/0.852</td><td>25.90/0.789</td><td>19.53/0.492</td><td>32.58/0.934</td><td>28.55/0.857</td><td>20.05/0.501</td><td>26.09/0.845</td><td>24.68/0.789</td><td>19.18/0.516</td></tr><tr><td>DistgSSR [28]</td><td>31.38/0.922</td><td>24.88/0.722</td><td>15.59/0.284</td><td>37.56/0.973</td><td>26.17/0.751</td><td>15.43/0.256</td><td>31.66/0.953</td><td>24.37/0.754</td><td>15.53/0.319</td></tr><tr><td>LFT [30]</td><td>31.43/0.921</td><td>24.99/0.729</td><td>15.89/0.279</td><td>37.63/0.974</td><td>26.48/0.765</td><td>15.96/0.258</td><td>31.80/0.954</td><td>24.39/0.758</td><td>15.74/0.317</td></tr><tr><td>SRMD [49]</td><td>29.55/0.886</td><td>27.88/0.851</td><td>25.37/0.806</td><td>35.04/0.953</td><td>31.56/0.919</td><td>28.26/0.883</td><td>28.85/0.911</td><td>26.73/0.869</td><td>23.60/0.795</td></tr><tr><td>DASR [31]</td><td>29.31/0.886</td><td>27.78/0.852</td><td>24.10/0.785</td><td>34.54/0.950</td><td>31.45/0.919</td><td>22.70/0.829</td><td>26.99/0.897</td><td>26.07/0.866</td><td>21.92/0.768</td></tr><tr><td>BSRNet [62]</td><td>28.42/0.865</td><td>24.98/0.831</td><td>19.32/0.748</td><td>32.73/0.933</td><td>28.22/0.895</td><td>17.97/0.748</td><td>26.55/0.880</td><td>22.55/0.829</td><td>17.46/0.714</td></tr><tr><td>Real-ESRNet [63]</td><td>28.05/0.862</td><td>26.99/0.839</td><td>23.65/0.789</td><td>31.80/0.931</td><td>30.11/0.905</td><td>24.14/0.842</td><td>24.78/0.871</td><td>24.51/0.850</td><td>19.45/0.754</td></tr><tr><td>LF-DAnet (ours)</td><td>30.43/0.907</td><td>29.55/0.886</td><td>28.23/0.859</td><td>36.44/0.967</td><td>34.63/0.951</td><td>32.42/0.929</td><td>29.77/0.932</td><td>28.62/0.912</td><td>26.99/0.878</td></tr><tr><td>Bicubic</td><td rowspan=\"8\">1.5</td><td>27.02/0.836</td><td>25.42/0.773</td><td>19.41/0.478</td><td>31.63/0.923</td><td>28.16/0.846</td><td>19.99/0.491</td><td>25.15/0.821</td><td>24.00/0.764</td><td>18.96/0.493</td></tr><tr><td>DistgSSR [28]</td><td>28.60/0.876</td><td>24.46/0.699</td><td>15.60/0.273</td><td>33.64/0.949</td><td>25.97/0.739</td><td>15.43/0.251</td><td>27.16/0.883</td><td>23.59/0.714</td><td>15.57/0.302</td></tr><tr><td>LFT [30]</td><td>28.57/0.875</td><td>24.60/0.708</td><td>15.89/0.268</td><td>33.62/0.949</td><td>26.25/0.753</td><td>15.96/0.252</td><td>27.13/0.882</td><td>23.69/0.721</td><td>15.70/0.295</td></tr><tr><td>SRMD [49]</td><td>29.58/0.886</td><td>27.39/0.840</td><td>25.01/0.798</td><td>35.00/0.953</td><td>31.02/0.912</td><td>27.94/0.879</td><td>28.87/0.910</td><td>26.05/0.851</td><td>23.06/0.776</td></tr><tr><td>DASR [31]</td><td>29.46/0.884</td><td>27.34/0.840</td><td>24.09/0.781</td><td>34.87/0.952</td><td>30.95/0.911</td><td>23.44/0.831</td><td>27.83/0.902</td><td>25.84/0.850</td><td>21.95/0.755</td></tr><tr><td>BSRNet [62]</td><td>28.38/0.861</td><td>24.79/0.824</td><td>19.36/0.746</td><td>32.77/0.932</td><td>28.11/0.892</td><td>18.00/0.749</td><td>26.67/0.877</td><td>22.34/0.815</td><td>17.39/0.706</td></tr><tr><td>Real-ESRNet [63]</td><td>28.17/0.862</td><td>26.68/0.830</td><td>23.50/0.783</td><td>32.11/0.932</td><td>29.85/0.900</td><td>24.13/0.840</td><td>25.18/0.872</td><td>24.30/0.834</td><td>19.41/0.741</td></tr><tr><td>LF-DAnet (ours)</td><td>30.15/0.900</td><td>28.98/0.872</td><td>27.65/0.845</td><td>36.10/0.963</td><td>33.87/0.942</td><td>31.81/0.920</td><td>29.47/0.924</td><td>27.91/0.894</td><td>26.25/0.857</td></tr><tr><td>Bicubic</td><td rowspan=\"8\">3</td><td>25.52/0.803</td><td>24.32/0.741</td><td>19.09/0.454</td><td>29.59/0.898</td><td>27.12/0.822</td><td>19.82/0.476</td><td>23.21/0.766</td><td>22.45/0.711</td><td>18.41/0.450</td></tr><tr><td>DistgSSR [28]</td><td>25.79/0.811</td><td>23.30/0.656</td><td>15.47/0.254</td><td>29.92/0.904</td><td>25.19/0.710</td><td>15.38/0.241</td><td>23.55/0.780</td><td>21.83/0.639</td><td>15.32/0.265</td></tr><tr><td>LFT [30]</td><td>25.73/0.810</td><td>23.41/0.665</td><td>15.75/0.248</td><td>29.83/0.904</td><td>25.42/0.724</td><td>15.91/0.242</td><td>23.47/0.779</td><td>21.89/0.647</td><td>15.43/0.257</td></tr><tr><td>SRMD [49]</td><td>29.20/0.876</td><td>26.32/0.816</td><td>24.30/0.782</td><td>34.39/0.948</td><td>29.87/0.896</td><td>27.36/0.871</td><td>28.29/0.898</td><td>24.51/0.807</td><td>22.08/0.742</td></tr><tr><td>DASR [31]</td><td>28.62/0.867</td><td>26.26/0.815</td><td>23.70/0.767</td><td>33.72/0.942</td><td>29.82/0.896</td><td>23.75/0.829</td><td>27.71/0.887</td><td>24.48/0.807</td><td>21.50/0.723</td></tr><tr><td>BSRNet [62]</td><td>27.60/0.843</td><td>24.13/0.804</td><td>19.33/0.740</td><td>31.96/0.921</td><td>27.72/0.883</td><td>18.05/0.750</td><td>26.05/0.849</td><td>21.62/0.776</td><td>17.23/0.691</td></tr><tr><td>Real-ESRNet [63]</td><td>27.33/0.845</td><td>25.67/0.807</td><td>23.04/0.769</td><td>31.45/0.919</td><td>29.11/0.889</td><td>23.93/0.835</td><td>25.24/0.856</td><td>23.35/0.789</td><td>19.14/0.712</td></tr><tr><td>LF-DAnet (ours)</td><td>29.43/0.884</td><td>27.76/0.845</td><td>26.54/0.821</td><td>35.10/0.955</td><td>32.34/0.924</td><td>30.54/0.904</td><td>28.51/0.904</td><td>26.22/0.853</td><td>24.72/0.814</td></tr><tr><td>Bicubic</td><td rowspan=\"8\">4.5</td><td>24.36/0.779</td><td>23.41/0.718</td><td>18.79/0.438</td><td>28.05/0.879</td><td>26.19/0.803</td><td>19.63/0.465</td><td>21.80/0.725</td><td>21.26/0.672</td><td>17.90/0.420</td></tr><tr><td>DistgSSR [28]</td><td>24.38/0.781</td><td>22.48/0.631</td><td>15.33/0.242</td><td>28.08/0.880</td><td>24.50/0.690</td><td>15.31/0.235</td><td>21.83/0.728</td><td>20.67/0.595</td><td>15.04/0.243</td></tr><tr><td>LFT [30]</td><td>24.39/0.781</td><td>22.57/0.640</td><td>15.61/0.237</td><td>28.08/0.880</td><td>24.71/0.705</td><td>15.84/0.235</td><td>21.84/0.728</td><td>20.72/0.602</td><td>15.14/0.233</td></tr><tr><td>SRMD [49]</td><td>26.32/0.818</td><td>25.09/0.792</td><td>23.65/0.769</td><td>30.62/0.908</td><td>28.61/0.882</td><td>26.66/0.864</td><td>24.34/0.780</td><td>22.80/0.753</td><td>21.28/0.716</td></tr><tr><td>DASR [31]</td><td>25.34/0.799</td><td>24.89/0.788</td><td>23.11/0.755</td><td>29.33/0.895</td><td>28.39/0.880</td><td>23.94/0.827</td><td>22.99/0.761</td><td>22.65/0.749</td><td>20.76/0.697</td></tr><tr><td>BSRNet [62]</td><td>26.31/0.816</td><td>23.40/0.784</td><td>19.26/0.734</td><td>30.35/0.902</td><td>27.10/0.874</td><td>18.03/0.749</td><td>24.23/0.795</td><td>20.83/0.738</td><td>17.06/0.680</td></tr><tr><td>Real-ESRNet [63]</td><td>26.28/0.816</td><td>24.69/0.787</td><td>22.55/0.758</td><td>30.04/0.900</td><td>28.08/0.878</td><td>23.66/0.830</td><td>23.97/0.810</td><td>22.17/0.743</td><td>18.90/0.693</td></tr><tr><td>LF-DAnet (ours)</td><td>28.00/0.854</td><td>26.55/0.820</td><td>25.58/0.801</td><td>33.39/0.937</td><td>30.88/0.906</td><td>29.45/0.890</td><td>26.59/0.860</td><td>24.64/0.808</td><td>23.30/0.771</td></tr></table><p>  Note: 1) For the methods in [62, 63], we used the models trained with a pixel-wise L1 loss (i.e., BSRNet and Real-ESRNet) for comparison since they</p><p>  can achieve higher PSNR and SSIM values as compared to their GAN-based version (i.e., BSRGAN and Real-ESRGAN). 2) SRMD and our LF-DAnet</p><p>  are non-blind SR methods while DASR, BSRNet and Real-ESRNet are blind SR methods.</p>", "caption": "TABLE I: PSNR and SSIM results achieved by different methods on the HCInew [68], HCIold [69] and STFgantry [70] datasets under synthetic degradations (with different blur kernel widths and noise levels). Note that, the degradation degenerates to the bicubic downsampling degradation when kernel width and noise level equal to 0. Best results are in bold faces.", "list_citation_info": ["[68] K. Honauer, O. Johannsen, D. Kondermann, and B. Goldluecke, \u201cA dataset and evaluation methodology for depth estimation on 4d light fields,\u201d in Asian Conference on Computer Vision (ACCV), 2016, pp. 19\u201334.", "[70] V. Vaish and A. Adams, \u201cThe (new) stanford light field archive,\u201d Computer Graphics Laboratory, Stanford University, vol. 6, no. 7, 2008.", "[69] S. Wanner, S. Meister, and B. Goldluecke, \u201cDatasets and benchmarks for densely sampled 4d light fields.\u201d in Vision, Modelling and Visualization (VMV), vol. 13, 2013, pp. 225\u2013226.", "[31] L. Wang, Y. Wang, X. Dong, Q. Xu, J. Yang, W. An, and Y. Guo, \u201cUnsupervised degradation representation learning for blind super-resolution,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 10\u2009581\u201310\u2009590.", "[62] K. Zhang, J. Liang, L. Van Gool, and R. Timofte, \u201cDesigning a practical degradation model for deep blind image super-resolution,\u201d in International Conference on Computer Vision (ICCV), 2021, pp. 4791\u20134800.", "[28] Y. Wang, L. Wang, G. Wu, J. Yang, W. An, J. Yu, and Y. Guo, \u201cDisentangling light fields for super-resolution and disparity estimation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.", "[49] K. Zhang, W. Zuo, and L. Zhang, \u201cLearning a single convolutional super-resolution network for multiple degradations,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3262\u20133271.", "[63] X. Wang, L. Xie, C. Dong, and Y. Shan, \u201cReal-esrgan: Training real-world blind super-resolution with pure synthetic data,\u201d in International Conference on Computer Vision Workshops (ICCVW), 2021, pp. 1905\u20131914.", "[30] Z. Liang, Y. Wang, L. Wang, J. Yang, and S. Zhou, \u201cLight field image super-resolution with transformers,\u201d IEEE Signal Processing Letters, 2022."]}, {"table": "<table><tr><td></td><td rowspan=\"2\">#Param.</td><td rowspan=\"2\">FLOPs</td><td rowspan=\"2\">Time</td><td colspan=\"3\">PSNR/SSIM</td></tr><tr><td></td><td>HCInew</td><td>HCIold</td><td>STFgantry</td></tr><tr><td>SRMD [49]</td><td>1.50M</td><td>39.76G</td><td>0.070s</td><td>27.18/0.838</td><td>31.16/0.913</td><td>25.78/0.840</td></tr><tr><td>DASR [31]</td><td>5.80M</td><td>82.03G</td><td>0.051s</td><td>26.74/0.831</td><td>29.47/0.896</td><td>24.92/0.829</td></tr><tr><td>BSRNet [62]</td><td>16.70M</td><td>459.6G</td><td>0.119s</td><td>24.03/0.807</td><td>26.17/0.856</td><td>21.98/0.793</td></tr><tr><td>Real-ESRNet [63]</td><td>16.70M</td><td>459.6G</td><td>0.119s</td><td>25.92/0.821</td><td>28.52/0.888</td><td>22.82/0.809</td></tr><tr><td>DistgSSR [28]</td><td>3.53M</td><td>65.41G</td><td>0.037s</td><td>22.79/0.611</td><td>24.97/0.642</td><td>22.06/0.623</td></tr><tr><td>LFT [30]</td><td>1.11M</td><td>29.45G</td><td>0.070s</td><td>22.92/0.612</td><td>25.23/0.647</td><td>22.14/0.623</td></tr><tr><td>LF-DAnet (ours)</td><td>3.80M</td><td>65.93G</td><td>0.039s</td><td>28.75/0.869</td><td>33.69/0.939</td><td>27.61/0.885</td></tr></table>", "caption": "TABLE II: Comparisons of the number of parameters (#Param.), FLOPs and running time for 4\\timesSR. Note that, FLOPs and running time are calculated on an input LF with an angular resolution of 5\\times5 and a spatial resolution of 32\\times32. PSNR and SSIM scores are averaged over 9 degradations (kernel width=(0, 1.5, 3), noise level=(0, 15, 50)) in Table I. Best results are in bold faces.", "list_citation_info": ["[31] L. Wang, Y. Wang, X. Dong, Q. Xu, J. Yang, W. An, and Y. Guo, \u201cUnsupervised degradation representation learning for blind super-resolution,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 10\u2009581\u201310\u2009590.", "[62] K. Zhang, J. Liang, L. Van Gool, and R. Timofte, \u201cDesigning a practical degradation model for deep blind image super-resolution,\u201d in International Conference on Computer Vision (ICCV), 2021, pp. 4791\u20134800.", "[28] Y. Wang, L. Wang, G. Wu, J. Yang, W. An, J. Yu, and Y. Guo, \u201cDisentangling light fields for super-resolution and disparity estimation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.", "[49] K. Zhang, W. Zuo, and L. Zhang, \u201cLearning a single convolutional super-resolution network for multiple degradations,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3262\u20133271.", "[63] X. Wang, L. Xie, C. Dong, and Y. Shan, \u201cReal-esrgan: Training real-world blind super-resolution with pure synthetic data,\u201d in International Conference on Computer Vision Workshops (ICCVW), 2021, pp. 1905\u20131914.", "[30] Z. Liang, Y. Wang, L. Wang, J. Yang, and S. Zhou, \u201cLight field image super-resolution with transformers,\u201d IEEE Signal Processing Letters, 2022."]}], "citation_info_to_title": {"[68] K. Honauer, O. Johannsen, D. Kondermann, and B. Goldluecke, \u201cA dataset and evaluation methodology for depth estimation on 4d light fields,\u201d in Asian Conference on Computer Vision (ACCV), 2016, pp. 19\u201334.": "A dataset and evaluation methodology for depth estimation on 4d light fields", "[70] V. Vaish and A. Adams, \u201cThe (new) stanford light field archive,\u201d Computer Graphics Laboratory, Stanford University, vol. 6, no. 7, 2008.": "The (new) Stanford Light Field Archive", "[63] X. Wang, L. Xie, C. Dong, and Y. Shan, \u201cReal-esrgan: Training real-world blind super-resolution with pure synthetic data,\u201d in International Conference on Computer Vision Workshops (ICCVW), 2021, pp. 1905\u20131914.": "Real-esrgan: Training real-world blind super-resolution with pure synthetic data", "[28] Y. Wang, L. Wang, G. Wu, J. Yang, W. An, J. Yu, and Y. Guo, \u201cDisentangling light fields for super-resolution and disparity estimation,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.": "Disentangling light fields for super-resolution and disparity estimation", "[62] K. Zhang, J. Liang, L. Van Gool, and R. Timofte, \u201cDesigning a practical degradation model for deep blind image super-resolution,\u201d in International Conference on Computer Vision (ICCV), 2021, pp. 4791\u20134800.": "Designing a practical degradation model for deep blind image super-resolution", "[31] L. Wang, Y. Wang, X. Dong, Q. Xu, J. Yang, W. An, and Y. Guo, \u201cUnsupervised degradation representation learning for blind super-resolution,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 10\u2009581\u201310\u2009590.": "Unsupervised degradation representation learning for blind super-resolution", "[69] S. Wanner, S. Meister, and B. Goldluecke, \u201cDatasets and benchmarks for densely sampled 4d light fields.\u201d in Vision, Modelling and Visualization (VMV), vol. 13, 2013, pp. 225\u2013226.": "Datasets and benchmarks for densely sampled 4d light fields", "[49] K. Zhang, W. Zuo, and L. Zhang, \u201cLearning a single convolutional super-resolution network for multiple degradations,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3262\u20133271.": "Learning a single convolutional super-resolution network for multiple degradations", "[30] Z. Liang, Y. Wang, L. Wang, J. Yang, and S. Zhou, \u201cLight field image super-resolution with transformers,\u201d IEEE Signal Processing Letters, 2022.": "Light field image super-resolution with transformers"}, "source_title_to_arxiv_id": {"Disentangling light fields for super-resolution and disparity estimation": "2202.10603", "Light field image super-resolution with transformers": "2108.07597"}}