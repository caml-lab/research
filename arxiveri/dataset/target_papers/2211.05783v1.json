{"title": "Unifying Flow, Stereo and Depth Estimation", "abstract": "We present a unified formulation and model for three motion and 3D perception\ntasks: optical flow, rectified stereo matching and unrectified stereo depth\nestimation from posed images. Unlike previous specialized architectures for\neach specific task, we formulate all three tasks as a unified dense\ncorrespondence matching problem, which can be solved with a single model by\ndirectly comparing feature similarities. Such a formulation calls for\ndiscriminative feature representations, which we achieve using a Transformer,\nin particular the cross-attention mechanism. We demonstrate that\ncross-attention enables integration of knowledge from another image via\ncross-view interactions, which greatly improves the quality of the extracted\nfeatures. Our unified model naturally enables cross-task transfer since the\nmodel architecture and parameters are shared across tasks. We outperform RAFT\nwith our unified model on the challenging Sintel dataset, and our final model\nthat uses a few additional task-specific refinement steps outperforms or\ncompares favorably to recent state-of-the-art methods on 10 popular flow,\nstereo and depth datasets, while being simpler and more efficient in terms of\nmodel design and inference speed.", "authors": ["Haofei Xu", "Jing Zhang", "Jianfei Cai", "Hamid Rezatofighi", "Fisher Yu", "Dacheng Tao", "Andreas Geiger"], "published_date": "2022_11_10", "pdf_url": "http://arxiv.org/pdf/2211.05783v1", "list_table_and_caption": [{"table": "<table><tr><td>Method</td><td>#refine.</td><td colspan=\"4\">Things (val, clean)</td><td colspan=\"4\">Sintel (train, clean)</td><td colspan=\"4\">Sintel (train, final)</td><td>Param(M)</td><td>Time(ms)</td></tr><tr><td></td><td></td><td>EPE</td><td>s_{0-10}</td><td>s_{10-40}</td><td>s_{40+}</td><td>EPE</td><td>s_{0-10}</td><td>s_{10-40}</td><td>s_{40+}</td><td>EPE</td><td>s_{0-10}</td><td>s_{10-40}</td><td>s_{40+}</td><td></td><td></td></tr><tr><td rowspan=\"6\">RAFT [21]</td><td>0</td><td>14.28</td><td>1.47</td><td>3.62</td><td>40.48</td><td>4.04</td><td>0.77</td><td>4.30</td><td>26.66</td><td>5.45</td><td>0.99</td><td>6.30</td><td>35.19</td><td rowspan=\"6\">5.3</td><td>25 (14)</td></tr><tr><td>3</td><td>6.27</td><td>0.69</td><td>1.67</td><td>17.63</td><td>1.92</td><td>0.47</td><td>2.32</td><td>11.37</td><td>3.25</td><td>0.65</td><td>4.00</td><td>20.04</td><td>39 (21)</td></tr><tr><td>7</td><td>4.66</td><td>0.55</td><td>1.38</td><td>12.87</td><td>1.61</td><td>0.39</td><td>1.90</td><td>9.61</td><td>2.80</td><td>0.53</td><td>3.30</td><td>17.76</td><td>58 (31)</td></tr><tr><td>11</td><td>4.31</td><td>0.53</td><td>1.33</td><td>11.79</td><td>1.55</td><td>0.41</td><td>1.73</td><td>9.19</td><td>2.72</td><td>0.52</td><td>3.12</td><td>17.43</td><td>78 (41)</td></tr><tr><td>23</td><td>4.22</td><td>0.53</td><td>1.32</td><td>11.52</td><td>1.47</td><td>0.36</td><td>1.63</td><td>9.00</td><td>2.69</td><td>0.52</td><td>3.05</td><td>17.28</td><td>133 (71)</td></tr><tr><td>31</td><td>4.25</td><td>0.53</td><td>1.31</td><td>11.63</td><td>1.41</td><td>0.32</td><td>1.55</td><td>8.83</td><td>2.69</td><td>0.52</td><td>3.00</td><td>17.45</td><td>170 (91)</td></tr><tr><td rowspan=\"2\">GMFlow</td><td>0</td><td>3.48</td><td>0.67</td><td>1.31</td><td>8.97</td><td>1.50</td><td>0.46</td><td>1.77</td><td>8.26</td><td>2.96</td><td>0.72</td><td>3.45</td><td>17.70</td><td>4.7</td><td>57 (26)</td></tr><tr><td>1</td><td>2.80</td><td>0.53</td><td>1.01</td><td>7.31</td><td>1.08</td><td>0.30</td><td>1.25</td><td>6.26</td><td>2.48</td><td>0.51</td><td>2.81</td><td>15.67</td><td>4.7</td><td>151 (66)</td></tr></table>", "caption": "TABLE III: RAFT\u2019s iterative refinement architecture vs. our GMFlow model. The models are trained on Chairs and Things training sets. We use RAFT\u2019s officially released model for evaluation. The inference time is measured on a single V100 and A100 (in parentheses) GPU at Sintel resolution (436\\times 1024). Our method gains more speedup than RAFT (2.29\\times vs. 1.87\\times, i.e., ours: 151\\to 66, RAFT: 170\\to 91) on the high-end A100 GPU since our method doesn\u2019t require a large number of sequential computation.", "list_citation_info": ["[21] Z. Teed and J. Deng, \u201cRaft: Recurrent all-pairs field transforms for optical flow,\u201d in ECCV. Springer, 2020, pp. 402\u2013419."]}, {"table": "<table><tr><td>Method</td><td colspan=\"3\">Sintel (clean)</td><td colspan=\"3\">Sintel (final)</td></tr><tr><td></td><td>all</td><td>matched</td><td>unmatched</td><td>all</td><td>matched</td><td>unmatched</td></tr><tr><td>FlowNet2 [16]</td><td>4.16</td><td>1.56</td><td>25.40</td><td>5.74</td><td>2.75</td><td>30.11</td></tr><tr><td>PWC-Net+ [90]</td><td>3.45</td><td>1.41</td><td>20.12</td><td>4.60</td><td>2.25</td><td>23.70</td></tr><tr><td>HD{}^{3} [18]</td><td>4.79</td><td>1.62</td><td>30.63</td><td>4.67</td><td>2.17</td><td>24.99</td></tr><tr><td>VCN [91]</td><td>2.81</td><td>1.11</td><td>16.68</td><td>4.40</td><td>2.22</td><td>22.24</td></tr><tr><td>DICL [92]</td><td>2.63</td><td>0.97</td><td>16.24</td><td>3.60</td><td>1.66</td><td>19.44</td></tr><tr><td>RAFT [21]</td><td>1.94</td><td>-</td><td>-</td><td>3.18</td><td>-</td><td>-</td></tr><tr><td>GMFlow [38]</td><td>1.74</td><td>0.65</td><td>10.56</td><td>2.90</td><td>1.32</td><td>15.80</td></tr><tr><td>RAFT{}^{\\dagger} [21]</td><td>1.61</td><td>0.62</td><td>9.65</td><td>2.86</td><td>1.41</td><td>14.68</td></tr><tr><td>GMA{}^{\\dagger} [31]</td><td>1.39</td><td>0.58</td><td>7.96</td><td>2.47</td><td>1.24</td><td>12.50</td></tr><tr><td>GMFlowNet [93]</td><td>1.39</td><td>0.52</td><td>8.49</td><td>2.65</td><td>1.27</td><td>13.88</td></tr><tr><td>DIP{}^{\\dagger} [94]</td><td>1.44</td><td>0.52</td><td>8.92</td><td>2.83</td><td>1.28</td><td>15.49</td></tr><tr><td>AGFlow{}^{\\dagger} [95]</td><td>1.43</td><td>0.56</td><td>8.54</td><td>2.47</td><td>1.22</td><td>12.64</td></tr><tr><td>CRAFT{}^{\\dagger} [81]</td><td>1.44</td><td>0.61</td><td>8.20</td><td>2.42</td><td>1.16</td><td>12.64</td></tr><tr><td>FlowFormer [82]</td><td>1.20</td><td>0.41</td><td>7.63</td><td>2.12</td><td>0.99</td><td>11.37</td></tr><tr><td>GMFlow+</td><td>1.03</td><td>0.34</td><td>6.68</td><td>2.37</td><td>1.10</td><td>12.74</td></tr></table>", "caption": "TABLE IX: Comparisons on Sintel test test for optical flow. {}^{\\dagger} represents the method uses last frame\u2019s flow prediction as initialization for subsequent refinement, while other methods all use two frames only.", "list_citation_info": ["[16] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, \u201cFlownet 2.0: Evolution of optical flow estimation with deep networks,\u201d in CVPR, 2017, pp. 2462\u20132470.", "[91] G. Yang and D. Ramanan, \u201cVolumetric correspondence networks for optical flow,\u201d NeurIPS, vol. 32, pp. 794\u2013805, 2019.", "[31] S. Jiang, D. Campbell, Y. Lu, H. Li, and R. Hartley, \u201cLearning to estimate hidden motions with global motion aggregation,\u201d in ICCV, October 2021, pp. 9772\u20139781.", "[82] Z. Huang, X. Shi, C. Zhang, Q. Wang, K. C. Cheung, H. Qin, J. Dai, and H. Li, \u201cFlowFormer: A transformer architecture for optical flow,\u201d ECCV, 2022.", "[90] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, \u201cModels matter, so does training: An empirical study of cnns for optical flow estimation,\u201d TPAMI, vol. 42, no. 6, pp. 1408\u20131423, 2019.", "[18] Z. Yin, T. Darrell, and F. Yu, \u201cHierarchical discrete distribution decomposition for match density estimation,\u201d in CVPR, 2019, pp. 6044\u20136053.", "[94] Z. Zheng, N. Nie, Z. Ling, P. Xiong, J. Liu, H. Wang, and J. Li, \u201cDip: Deep inverse patchmatch for high-resolution optical flow,\u201d in CVPR, 2022, pp. 8925\u20138934.", "[21] Z. Teed and J. Deng, \u201cRaft: Recurrent all-pairs field transforms for optical flow,\u201d in ECCV. Springer, 2020, pp. 402\u2013419.", "[81] X. Sui, S. Li, X. Geng, Y. Wu, X. Xu, Y. Liu, R. Goh, and H. Zhu, \u201cCraft: Cross-attentional flow transformer for robust optical flow,\u201d in CVPR, 2022, pp. 17\u2009602\u201317\u2009611.", "[92] J. Wang, Y. Zhong, Y. Dai, K. Zhang, P. Ji, and H. Li, \u201cDisplacement-invariant matching cost learning for accurate optical flow estimation,\u201d NeurIPS, vol. 33, 2020.", "[93] S. Zhao, L. Zhao, Z. Zhang, E. Zhou, and D. Metaxas, \u201cGlobal matching with overlapping attention for optical flow estimation,\u201d in CVPR, 2022, pp. 17\u2009592\u201317\u2009601.", "[38] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, and D. Tao, \u201cGmflow: Learning optical flow via global matching,\u201d in CVPR, 2022, pp. 8121\u20138130.", "[95] A. Luo, F. Yang, K. Luo, X. Li, H. Fan, and S. Liu, \u201cLearning optical flow with adaptive graph reasoning,\u201d in AAAI, 2022."]}, {"table": "<table><tr><td>Method</td><td>FlowNet2 [16]</td><td>PWC-Net+ [90]</td><td>RAFT [21]</td><td>GMFlow+</td></tr><tr><td>All</td><td>11.48</td><td>7.72</td><td>5.10</td><td>4.49</td></tr><tr><td>Noc</td><td>6.94</td><td>4.91</td><td>3.07</td><td>2.40</td></tr></table>", "caption": "TABLE X: Comparisons on KITTI test set for optical flow. The metric is F1-all. \u201cAll\u201d denotes the evaluation results on all pixels with ground truth, and \u201cNoc\u201d denotes non-occluded pixels only.", "list_citation_info": ["[16] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, \u201cFlownet 2.0: Evolution of optical flow estimation with deep networks,\u201d in CVPR, 2017, pp. 2462\u20132470.", "[21] Z. Teed and J. Deng, \u201cRaft: Recurrent all-pairs field transforms for optical flow,\u201d in ECCV. Springer, 2020, pp. 402\u2013419.", "[90] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, \u201cModels matter, so does training: An empirical study of cnns for optical flow estimation,\u201d TPAMI, vol. 42, no. 6, pp. 1408\u20131423, 2019."]}, {"table": "<table><tr><td>Model</td><td>D1-all (All)</td><td>D1-all (Noc)</td><td>Time (s)</td></tr><tr><td>LEAStereo [96]</td><td>1.65</td><td>1.51</td><td>0.30</td></tr><tr><td>CREStereo [54]</td><td>1.69</td><td>1.54</td><td>0.41</td></tr><tr><td>GANet-deep [25]</td><td>1.81</td><td>1.63</td><td>1.80</td></tr><tr><td>CFNet [53]</td><td>1.88</td><td>1.73</td><td>0.18</td></tr><tr><td>AANet+ [19]</td><td>2.03</td><td>1.85</td><td>0.06</td></tr><tr><td>PSMNet [24]</td><td>2.32</td><td>2.14</td><td>0.41</td></tr><tr><td>GMStereo</td><td>1.77</td><td>1.61</td><td>0.17</td></tr></table>", "caption": "TABLE XII: Stereo performance on KITTI 2015 test set. ", "list_citation_info": ["[25] F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, \u201cGa-net: Guided aggregation net for end-to-end stereo matching,\u201d in CVPR, 2019, pp. 185\u2013194.", "[19] H. Xu and J. Zhang, \u201cAanet: Adaptive aggregation network for efficient stereo matching,\u201d in CVPR, 2020, pp. 1959\u20131968.", "[53] Z. Shen, Y. Dai, and Z. Rao, \u201cCfnet: Cascade and fused cost volume for robust stereo matching,\u201d in CVPR, 2021, pp. 13\u2009906\u201313\u2009915.", "[96] X. Cheng, Y. Zhong, M. Harandi, Y. Dai, X. Chang, H. Li, T. Drummond, and Z. Ge, \u201cHierarchical neural architecture search for deep stereo matching,\u201d NeurIPS, vol. 33, pp. 22\u2009158\u201322\u2009169, 2020.", "[24] J.-R. Chang and Y.-S. Chen, \u201cPyramid stereo matching network,\u201d in CVPR, 2018, pp. 5410\u20135418.", "[54] J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan, and S. Liu, \u201cPractical stereo matching via cascaded recurrent network with adaptive correlation,\u201d in CVPR, 2022, pp. 16\u2009263\u201316\u2009272."]}, {"table": "<table><tr><td>Model</td><td>bad 2.0</td><td>bad 4.0</td><td>AvgErr</td><td>RMS</td><td>Time (s)</td></tr><tr><td>CREStereo [54]</td><td>3.71</td><td>2.04</td><td>1.15</td><td>7.70</td><td>3.55 (F)</td></tr><tr><td>RAFT-Stereo [26]</td><td>4.74</td><td>2.75</td><td>1.27</td><td>8.41</td><td>11.6 (F)</td></tr><tr><td>LEAStereo [96]</td><td>7.15</td><td>2.75</td><td>1.43</td><td>8.11</td><td>2.90 (H)</td></tr><tr><td>HSMNet [97]</td><td>10.2</td><td>4.83</td><td>2.07</td><td>10.3</td><td>0.51 (F)</td></tr><tr><td>CFNet [53]</td><td>10.1</td><td>6.49</td><td>3.49</td><td>15.4</td><td>0.69 (H)</td></tr><tr><td>GMStereo</td><td>7.14</td><td>2.96</td><td>1.31</td><td>6.45</td><td>0.73 (F)</td></tr></table>", "caption": "TABLE XIII: Stereo performance on Middlebury test set. \u201cF\u201d and \u201cH\u201d denote the results are generated using the full and half resolution images, respectively.", "list_citation_info": ["[53] Z. Shen, Y. Dai, and Z. Rao, \u201cCfnet: Cascade and fused cost volume for robust stereo matching,\u201d in CVPR, 2021, pp. 13\u2009906\u201313\u2009915.", "[96] X. Cheng, Y. Zhong, M. Harandi, Y. Dai, X. Chang, H. Li, T. Drummond, and Z. Ge, \u201cHierarchical neural architecture search for deep stereo matching,\u201d NeurIPS, vol. 33, pp. 22\u2009158\u201322\u2009169, 2020.", "[54] J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan, and S. Liu, \u201cPractical stereo matching via cascaded recurrent network with adaptive correlation,\u201d in CVPR, 2022, pp. 16\u2009263\u201316\u2009272.", "[97] G. Yang, J. Manela, M. Happold, and D. Ramanan, \u201cHierarchical deep stereo matching on high-resolution images,\u201d in CVPR, 2019, pp. 5515\u20135524.", "[26] L. Lipson, Z. Teed, and J. Deng, \u201cRaft-stereo: Multilevel recurrent field transforms for stereo matching,\u201d in 3DV. IEEE, 2021, pp. 218\u2013227."]}, {"table": "<table><tr><td>Model</td><td>bad 1.0</td><td>bad 2.0</td><td>bad 4.0</td></tr><tr><td>GANet [25]</td><td>6.56</td><td>1.10</td><td>0.54</td></tr><tr><td>AANet [19]</td><td>5.01</td><td>1.66</td><td>0.75</td></tr><tr><td>CFNet [53]</td><td>3.31</td><td>0.77</td><td>0.31</td></tr><tr><td>RAFT-Stereo [26]</td><td>2.44</td><td>0.44</td><td>0.15</td></tr><tr><td>CREStereo [54]</td><td>0.98</td><td>0.22</td><td>0.10</td></tr><tr><td>GMStereo</td><td>1.83</td><td>0.25</td><td>0.08</td></tr></table>", "caption": "TABLE XIV: Stereo performance on ETH3D two-view stereo test set. ", "list_citation_info": ["[19] H. Xu and J. Zhang, \u201cAanet: Adaptive aggregation network for efficient stereo matching,\u201d in CVPR, 2020, pp. 1959\u20131968.", "[53] Z. Shen, Y. Dai, and Z. Rao, \u201cCfnet: Cascade and fused cost volume for robust stereo matching,\u201d in CVPR, 2021, pp. 13\u2009906\u201313\u2009915.", "[25] F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, \u201cGa-net: Guided aggregation net for end-to-end stereo matching,\u201d in CVPR, 2019, pp. 185\u2013194.", "[26] L. Lipson, Z. Teed, and J. Deng, \u201cRaft-stereo: Multilevel recurrent field transforms for stereo matching,\u201d in 3DV. IEEE, 2021, pp. 218\u2013227.", "[54] J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan, and S. Liu, \u201cPractical stereo matching via cascaded recurrent network with adaptive correlation,\u201d in CVPR, 2022, pp. 16\u2009263\u201316\u2009272."]}, {"table": "<table><tr><td>Model</td><td>all:10</td><td>all:5</td><td>all:3</td><td>Time (ms)</td></tr><tr><td>ACVNet [22]</td><td>4.06</td><td>6.46</td><td>10.10</td><td>236</td></tr><tr><td>Odepth{}^{\\dagger}</td><td>3.78</td><td>7.55</td><td>12.33</td><td>199</td></tr><tr><td>LRM{}^{\\dagger}</td><td>2.47</td><td>4.71</td><td>8.44</td><td>191</td></tr><tr><td>MSCLab{}^{\\dagger}</td><td>2.39</td><td>6.29</td><td>11.65</td><td>150</td></tr><tr><td>GMStereo</td><td>1.61</td><td>3.19</td><td>6.86</td><td>190</td></tr></table>", "caption": "TABLE XV: Argoverse Stereo Challenge held on CVPR 2022 Autonomous Driving Workshop. {}^{\\dagger} denotes anonymous submission.", "list_citation_info": ["[22] G. Xu, J. Cheng, P. Guo, and X. Yang, \u201cAttention concatenation volume for accurate and efficient stereo matching,\u201d in CVPR, 2022, pp. 12\u2009981\u201312\u2009990."]}, {"table": "<table><tr><td>Model</td><td>Abs Rel</td><td>Sq Rel</td><td>RMSE</td><td>RMSE log</td><td>Time (s)</td></tr><tr><td>DeMoN [37]</td><td>0.231</td><td>0.520</td><td>0.761</td><td>0.289</td><td>0.69</td></tr><tr><td>BA-Net [20]</td><td>0.161</td><td>0.092</td><td>0.346</td><td>0.214</td><td>0.38</td></tr><tr><td>DeepV2D [64]</td><td>0.057</td><td>0.010</td><td>0.168</td><td>0.080</td><td>0.69</td></tr><tr><td>GMDepth</td><td>0.059</td><td>0.019</td><td>0.179</td><td>0.082</td><td>0.04</td></tr></table>", "caption": "TABLE XVI: Depth performance on ScanNet test set. ", "list_citation_info": ["[64] Z. Teed and J. Deng, \u201cDeepv2d: Video to depth with differentiable structure from motion,\u201d in ICLR, 2020.", "[20] C. Tang and P. Tan, \u201cBa-net: Dense bundle adjustment networks,\u201d in ICLR, 2019.", "[37] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox, \u201cDemon: Depth and motion network for learning monocular stereo,\u201d in CVPR, 2017, pp. 5038\u20135047."]}, {"table": "<table><tr><td>Dataset</td><td>Model</td><td>Abs Rel</td><td>Sq Rel</td><td>RMSE</td><td>RMSE log</td></tr><tr><td rowspan=\"5\">RGBD-SLAM</td><td>DeMoN [37]</td><td>0.157</td><td>0.524</td><td>1.780</td><td>0.202</td></tr><tr><td>DeepMVS [104]</td><td>0.294</td><td>0.430</td><td>0.868</td><td>0.351</td></tr><tr><td>DPSNet [63]</td><td>0.154</td><td>0.215</td><td>0.723</td><td>0.226</td></tr><tr><td>IIB [29]</td><td>0.095</td><td>-</td><td>0.550</td><td>-</td></tr><tr><td>GMDepth</td><td>0.101</td><td>0.177</td><td>0.556</td><td>0.167</td></tr><tr><td rowspan=\"5\">SUN3D</td><td>DeMoN [37]</td><td>0.214</td><td>1.120</td><td>2.421</td><td>0.206</td></tr><tr><td>DeepMVS [104]</td><td>0.282</td><td>0.435</td><td>0.944</td><td>0.363</td></tr><tr><td>DPSNet [63]</td><td>0.147</td><td>0.107</td><td>0.427</td><td>0.191</td></tr><tr><td>IIB [29]</td><td>0.099</td><td>-</td><td>0.293</td><td>-</td></tr><tr><td>GMDepth</td><td>0.112</td><td>0.068</td><td>0.336</td><td>0.146</td></tr><tr><td rowspan=\"5\">Scenes11</td><td>DeMoN [37]</td><td>0.556</td><td>3.402</td><td>2.603</td><td>0.391</td></tr><tr><td>DeepMVS [104]</td><td>0.210</td><td>0.373</td><td>0.891</td><td>0.270</td></tr><tr><td>DPSNet [63]</td><td>0.056</td><td>0.144</td><td>0.714</td><td>0.140</td></tr><tr><td>IIB [29]</td><td>0.056</td><td>-</td><td>0.523</td><td>-</td></tr><tr><td>GMDepth</td><td>0.050</td><td>0.069</td><td>0.491</td><td>0.106</td></tr></table>", "caption": "TABLE XVII: Depth performance on RGBD-SLAM, SUN3D and Scenes11 test datasets. ", "list_citation_info": ["[104] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, \u201cDeepmvs: Learning multi-view stereopsis,\u201d in CVPR, 2018, pp. 2821\u20132830.", "[63] S. Im, H.-G. Jeon, S. Lin, and I. S. Kweon, \u201cDpsnet: End-to-end deep plane sweep stereo,\u201d in ICLR, 2019.", "[29] W. Yifan, C. Doersch, R. Arandjelovi\u0107, J. Carreira, and A. Zisserman, \u201cInput-level inductive biases for 3d reconstruction,\u201d in CVPR, 2022, pp. 6176\u20136186.", "[37] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox, \u201cDemon: Depth and motion network for learning monocular stereo,\u201d in CVPR, 2017, pp. 5038\u20135047."]}], "citation_info_to_title": {"[19] H. Xu and J. Zhang, \u201cAanet: Adaptive aggregation network for efficient stereo matching,\u201d in CVPR, 2020, pp. 1959\u20131968.": "Aanet: Adaptive aggregation network for efficient stereo matching", "[53] Z. Shen, Y. Dai, and Z. Rao, \u201cCfnet: Cascade and fused cost volume for robust stereo matching,\u201d in CVPR, 2021, pp. 13\u2009906\u201313\u2009915.": "Cfnet: Cascade and Fused Cost Volume for Robust Stereo Matching", "[25] F. Zhang, V. Prisacariu, R. Yang, and P. H. Torr, \u201cGa-net: Guided aggregation net for end-to-end stereo matching,\u201d in CVPR, 2019, pp. 185\u2013194.": "Ga-net: Guided aggregation net for end-to-end stereo matching", "[26] L. Lipson, Z. Teed, and J. Deng, \u201cRaft-stereo: Multilevel recurrent field transforms for stereo matching,\u201d in 3DV. IEEE, 2021, pp. 218\u2013227.": "Raft-stereo: Multilevel recurrent field transforms for stereo matching", "[91] G. Yang and D. Ramanan, \u201cVolumetric correspondence networks for optical flow,\u201d NeurIPS, vol. 32, pp. 794\u2013805, 2019.": "Volumetric correspondence networks for optical flow", "[37] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox, \u201cDemon: Depth and motion network for learning monocular stereo,\u201d in CVPR, 2017, pp. 5038\u20135047.": "Demon: Depth and motion network for learning monocular stereo", "[94] Z. Zheng, N. Nie, Z. Ling, P. Xiong, J. Liu, H. Wang, and J. Li, \u201cDip: Deep inverse patchmatch for high-resolution optical flow,\u201d in CVPR, 2022, pp. 8925\u20138934.": "Dip: Deep Inverse Patchmatch for High-Resolution Optical Flow", "[24] J.-R. Chang and Y.-S. Chen, \u201cPyramid stereo matching network,\u201d in CVPR, 2018, pp. 5410\u20135418.": "Pyramid stereo matching network", "[95] A. Luo, F. Yang, K. Luo, X. Li, H. Fan, and S. Liu, \u201cLearning optical flow with adaptive graph reasoning,\u201d in AAAI, 2022.": "Learning Optical Flow with Adaptive Graph Reasoning", "[31] S. Jiang, D. Campbell, Y. Lu, H. Li, and R. Hartley, \u201cLearning to estimate hidden motions with global motion aggregation,\u201d in ICCV, October 2021, pp. 9772\u20139781.": "Learning to estimate hidden motions with global motion aggregation", "[29] W. Yifan, C. Doersch, R. Arandjelovi\u0107, J. Carreira, and A. Zisserman, \u201cInput-level inductive biases for 3d reconstruction,\u201d in CVPR, 2022, pp. 6176\u20136186.": "Input-level inductive biases for 3d reconstruction", "[21] Z. Teed and J. Deng, \u201cRaft: Recurrent all-pairs field transforms for optical flow,\u201d in ECCV. Springer, 2020, pp. 402\u2013419.": "Raft: Recurrent all-pairs field transforms for optical flow", "[22] G. Xu, J. Cheng, P. Guo, and X. Yang, \u201cAttention concatenation volume for accurate and efficient stereo matching,\u201d in CVPR, 2022, pp. 12\u2009981\u201312\u2009990.": "Attention concatenation volume for accurate and efficient stereo matching", "[93] S. Zhao, L. Zhao, Z. Zhang, E. Zhou, and D. Metaxas, \u201cGlobal matching with overlapping attention for optical flow estimation,\u201d in CVPR, 2022, pp. 17\u2009592\u201317\u2009601.": "Global matching with overlapping attention for optical flow estimation", "[96] X. Cheng, Y. Zhong, M. Harandi, Y. Dai, X. Chang, H. Li, T. Drummond, and Z. Ge, \u201cHierarchical neural architecture search for deep stereo matching,\u201d NeurIPS, vol. 33, pp. 22\u2009158\u201322\u2009169, 2020.": "Hierarchical neural architecture search for deep stereo matching", "[97] G. Yang, J. Manela, M. Happold, and D. Ramanan, \u201cHierarchical deep stereo matching on high-resolution images,\u201d in CVPR, 2019, pp. 5515\u20135524.": "Hierarchical deep stereo matching on high-resolution images", "[90] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, \u201cModels matter, so does training: An empirical study of cnns for optical flow estimation,\u201d TPAMI, vol. 42, no. 6, pp. 1408\u20131423, 2019.": "Models matter, so does training: An empirical study of cnns for optical flow estimation", "[20] C. Tang and P. Tan, \u201cBa-net: Dense bundle adjustment networks,\u201d in ICLR, 2019.": "Ba-net: Dense bundle adjustment networks", "[63] S. Im, H.-G. Jeon, S. Lin, and I. S. Kweon, \u201cDpsnet: End-to-end deep plane sweep stereo,\u201d in ICLR, 2019.": "Dpsnet: End-to-end deep plane sweep stereo", "[81] X. Sui, S. Li, X. Geng, Y. Wu, X. Xu, Y. Liu, R. Goh, and H. Zhu, \u201cCraft: Cross-attentional flow transformer for robust optical flow,\u201d in CVPR, 2022, pp. 17\u2009602\u201317\u2009611.": "Craft: Cross-attentional flow transformer for robust optical flow", "[16] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox, \u201cFlownet 2.0: Evolution of optical flow estimation with deep networks,\u201d in CVPR, 2017, pp. 2462\u20132470.": "Flownet 20: Evolution of optical flow estimation with deep networks", "[92] J. Wang, Y. Zhong, Y. Dai, K. Zhang, P. Ji, and H. Li, \u201cDisplacement-invariant matching cost learning for accurate optical flow estimation,\u201d NeurIPS, vol. 33, 2020.": "Displacement-invariant matching cost learning for accurate optical flow estimation", "[64] Z. Teed and J. Deng, \u201cDeepv2d: Video to depth with differentiable structure from motion,\u201d in ICLR, 2020.": "Deepv2d: Video to depth with differentiable structure from motion", "[18] Z. Yin, T. Darrell, and F. Yu, \u201cHierarchical discrete distribution decomposition for match density estimation,\u201d in CVPR, 2019, pp. 6044\u20136053.": "Hierarchical discrete distribution decomposition for match density estimation", "[38] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, and D. Tao, \u201cGmflow: Learning optical flow via global matching,\u201d in CVPR, 2022, pp. 8121\u20138130.": "Gmflow: Learning optical flow via global matching", "[54] J. Li, P. Wang, P. Xiong, T. Cai, Z. Yan, L. Yang, J. Liu, H. Fan, and S. Liu, \u201cPractical stereo matching via cascaded recurrent network with adaptive correlation,\u201d in CVPR, 2022, pp. 16\u2009263\u201316\u2009272.": "Practical stereo matching via cascaded recurrent network with adaptive correlation", "[104] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang, \u201cDeepmvs: Learning multi-view stereopsis,\u201d in CVPR, 2018, pp. 2821\u20132830.": "DeepMVS: Learning Multi-View Stereopsis", "[82] Z. Huang, X. Shi, C. Zhang, Q. Wang, K. C. Cheung, H. Qin, J. Dai, and H. Li, \u201cFlowFormer: A transformer architecture for optical flow,\u201d ECCV, 2022.": "FlowFormer: A transformer architecture for optical flow"}, "source_title_to_arxiv_id": {"Gmflow: Learning optical flow via global matching": "2111.13680", "FlowFormer: A transformer architecture for optical flow": "2203.16194"}}