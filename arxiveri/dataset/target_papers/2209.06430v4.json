{"title": "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment", "abstract": "The pre-trained image-text models, like CLIP, have demonstrated the strong\npower of vision-language representation learned from a large scale of\nweb-collected image-text data. In light of the well-learned visual features,\nsome existing works transfer image representation to video domain and achieve\ngood results. However, how to utilize image-language pre-trained model (e.g.,\nCLIP) for video-language pre-training (post-pretraining) is still under\nexplored. In this paper, we investigate two questions: 1) what are the factors\nhindering post-pretraining CLIP to further improve the performance on\nvideo-language tasks? and 2) how to mitigate the impact of these factors?\nThrough a series of comparative experiments and analyses, we find that the data\nscale and domain gap between language sources have great impacts. Motivated by\nthese, we propose a Omnisource Cross-modal Learning method equipped with a\nVideo Proxy mechanism on the basis of CLIP, namely CLIP-ViP. Extensive results\nshow that our approach improves the performance of CLIP on video-text retrieval\nby a large margin. Our model also achieves SOTA results on a variety of\ndatasets, including MSR-VTT, DiDeMo, LSMDC, and ActivityNet. We will release\nour code and pre-trained CLIP-ViP models at\nhttps://github.com/microsoft/XPretrain/tree/main/CLIP-ViP.", "authors": ["Hongwei Xue", "Yuchong Sun", "Bei Liu", "Jianlong Fu", "Ruihua Song", "Houqiang Li", "Jiebo Luo"], "published_date": "2022_09_14", "pdf_url": "http://arxiv.org/pdf/2209.06430v4", "list_table_and_caption": [{"table": "<table><thead><tr><th>NMI Score</th><th>MSR-VTT [47]</th><th>DiDemo [1]</th><th>Mean</th></tr></thead><tbody><tr><th>HD-VILA{}_{sub}</th><td>0.831</td><td>0.684</td><td>0.758</td></tr><tr><th>HD-VILA{}_{cap}</th><td>0.317</td><td>0.621</td><td>0.469</td></tr><tr><th>WebVid [2]</th><td>0.420</td><td>0.488</td><td>0.454</td></tr><tr><th>COCO [25]</th><td>0.373</td><td>0.758</td><td>0.566</td></tr><tr><th>CC12M [38]</th><td>0.445</td><td>0.673</td><td>0.559</td></tr></tbody></table>", "caption": "Table 1: Normalized Mutual Information (NMI) score of language features extracted on series of data and downstream tasks. We choose MSR-VTT and DiDemo as downstream tasks. Larger value indicates larger domain gap.", "list_citation_info": ["[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.", "[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.", "[47] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, pages 5288\u20135296, 2016.", "[38] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.", "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, pages 5803\u20135812, 2017."]}, {"table": "<table><tbody><tr><th>Method</th><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><th>ClipBERT [21]</th><td>22.0</td><td>46.8</td><td>59.9</td><td>6.0</td></tr><tr><th>VLM [45]</th><td>28.1</td><td>55.5</td><td>67.4</td><td>4.0</td></tr><tr><th>MMT [9]</th><td>26.6</td><td>57.1</td><td>69.6</td><td>4.0</td></tr><tr><th>Support Set [34]</th><td>30.1</td><td>58.5</td><td>69.3</td><td>3.0</td></tr><tr><th>Frozen [2]</th><td>31.0</td><td>59.5</td><td>70.5</td><td>3.0</td></tr><tr><th>VideoCLIP [46]</th><td>30.9</td><td>55.4</td><td>66.8</td><td>-</td></tr><tr><th>HD-VILA [48]</th><td>35.6</td><td>65.3</td><td>78.0</td><td>3.0</td></tr><tr><th>Florence [52]</th><td>37.6</td><td>63.8</td><td>72.6</td><td>-</td></tr><tr><th>All-in-One [40]</th><td>37.9</td><td>68.1</td><td>77.1</td><td>-</td></tr><tr><th>BridgeFormer [11]</th><td>37.6</td><td>64.8</td><td>75.1</td><td>3.0</td></tr><tr><th>CLIP-ViT-B/32</th><td colspan=\"4\"></td></tr><tr><th>CLIP4Clip [29]</th><td>44.5</td><td>71.4</td><td>81.6</td><td>2.0</td></tr><tr><th>CenterCLIP [57]</th><td>44.2</td><td>71.6</td><td>82.1</td><td>2.0</td></tr><tr><th>XPool [12]</th><td>46.9</td><td>72.8</td><td>82.2</td><td>2.0</td></tr><tr><th>CLIP2Video [7]</th><td>45.6</td><td>72.6</td><td>81.7</td><td>2.0</td></tr><tr><th>CLIP2Video\u2020[3]</th><td>47.2</td><td>73.0</td><td>83.0</td><td>2.0</td></tr><tr><th>CLIP2TV [10]</th><td>46.1</td><td>72.5</td><td>82.9</td><td>2.0</td></tr><tr><th>DRL [43]</th><td>47.4</td><td>74.6</td><td>83.8</td><td>2.0</td></tr><tr><th>CAMoE* [6]</th><td>47.3</td><td>74.2</td><td>84.5</td><td>2.0</td></tr><tr><th>Ours</th><td>50.1</td><td>74.8</td><td>84.6</td><td>1.0</td></tr><tr><th>Ours*</th><td>55.9</td><td>77.0</td><td>86.8</td><td>1.0</td></tr><tr><th>CLIP-ViT-B/16</th><td colspan=\"4\"></td></tr><tr><th>CenterCLIP [57]</th><td>48.4</td><td>73.8</td><td>82.0</td><td>2.0</td></tr><tr><th>CLIP2TV [10]</th><td>49.3</td><td>74.7</td><td>83.6</td><td>2.0</td></tr><tr><th>DRL [43]</th><td>50.2</td><td>76.5</td><td>84.7</td><td>1.0</td></tr><tr><th>DRL\u2020[43]</th><td>53.3</td><td>80.3</td><td>87.6</td><td>1.0</td></tr><tr><th>Ours</th><td>54.2</td><td>77.2</td><td>84.8</td><td>1.0</td></tr><tr><th>Ours*</th><td>57.7</td><td>80.5</td><td>88.2</td><td>1.0</td></tr></tbody></table>", "caption": "Table 5: Comparison of text-to-video retrieval in MSR-VTT [47]. * and \u2020 respectively denotes that the method uses DSL [6] and QB-Norm [3] as post-processing operations.", "list_citation_info": ["[6] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290, 2021.", "[52] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.", "[9] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214\u2013229, 2020.", "[47] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, pages 5288\u20135296, 2016.", "[12] Satya Krishna Gorti, No\u00ebl Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu. X-pool: Cross-modal language-video attention for text-video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5006\u20135015, 2022.", "[48] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5036\u20135045, 2022.", "[40] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.", "[45] Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. Vlm: Task-agnostic video-language model pre-training for video understanding. arXiv preprint arXiv:2105.09996, 2021.", "[43] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xian-Sheng Hua. Disentangled representation learning for text-video retrieval. arXiv preprint arXiv:2203.07111, 2022.", "[11] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022.", "[21] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 7331\u20137341, 2021.", "[3] Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu, and Samuel Albanie. Cross modal retrieval with querybank normalisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5194\u20135205, 2022.", "[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.", "[10] Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610, 2021.", "[7] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.", "[46] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In EMNLP, pages 6787\u20136800, 2021.", "[57] Shuai Zhao, Linchao Zhu, Xiaohan Wang, and Yi Yang. Centerclip: Token clustering for efficient text-video retrieval. arXiv preprint arXiv:2205.00823, 2022.", "[29] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[34] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In ICLR, 2021."]}, {"table": "<table><tbody><tr><th>Method</th><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><th>ClipBERT [21]</th><td>20.4</td><td>48.0</td><td>60.8</td><td>6.0</td></tr><tr><th>Frozen [2]</th><td>31.0</td><td>59.8</td><td>72.4</td><td>3.0</td></tr><tr><th>HD-VILA [48]</th><td>28.8</td><td>57.4</td><td>69.1</td><td>4.0</td></tr><tr><th>All-in-One [40]</th><td>32.7</td><td>61.4</td><td>73.5</td><td>3.0</td></tr><tr><th>BridgeFormer [11]</th><td>37.0</td><td>62.2</td><td>73.9</td><td>3.0</td></tr><tr><th>CLIP-ViT-B/32</th><td colspan=\"4\"></td></tr><tr><th>CLIP4Clip [29]</th><td>43.4</td><td>70.2</td><td>80.6</td><td>2.0</td></tr><tr><th>CLIP2TV [10]</th><td>45.5</td><td>69.7</td><td>80.6</td><td>2.0</td></tr><tr><th>DRL [43]</th><td>47.9</td><td>73.8</td><td>82.7</td><td>2.0</td></tr><tr><th>CAMoE* [6]</th><td>43.8</td><td>71.4</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>48.6</td><td>77.1</td><td>84.4</td><td>2.0</td></tr><tr><th>Ours*</th><td>53.8</td><td>79.6</td><td>86.5</td><td>1.0</td></tr><tr><th>CLIP-ViT-B/16</th><td colspan=\"4\"></td></tr><tr><th>DRL [43]</th><td>49.0</td><td>76.5</td><td>84.5</td><td>2.0</td></tr><tr><th>Ours</th><td>50.5</td><td>78.4</td><td>87.1</td><td>1.0</td></tr><tr><th>Ours*</th><td>55.3</td><td>82.0</td><td>89.3</td><td>1.0</td></tr></tbody></table>", "caption": "Table 6: Comparison of text-to-video retrieval in didemo [1]. * denotes that the method uses post-processing operations DSL [6].", "list_citation_info": ["[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.", "[6] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290, 2021.", "[10] Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610, 2021.", "[48] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5036\u20135045, 2022.", "[11] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022.", "[21] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 7331\u20137341, 2021.", "[40] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.", "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, pages 5803\u20135812, 2017.", "[29] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[43] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xian-Sheng Hua. Disentangled representation learning for text-video retrieval. arXiv preprint arXiv:2203.07111, 2022."]}, {"table": "<table><tbody><tr><th>Method</th><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><th>ClipBERT [21]</th><td>21.3</td><td>49.0</td><td>63.5</td><td>6.0</td></tr><tr><th>MMT [9]</th><td>28.7</td><td>61.4</td><td>-</td><td>3.3</td></tr><tr><th>Support Set [34]</th><td>29.2</td><td>61.6</td><td>-</td><td>3.0</td></tr><tr><th>Frozen [2]</th><td>28.8</td><td>60.9</td><td>-</td><td>3.0</td></tr><tr><th>HD-VILA [48]</th><td>28.5</td><td>57.4</td><td>-</td><td>4.0</td></tr><tr><th>All-in-One [40]</th><td>22.4</td><td>53.7</td><td>67.7</td><td>5.0</td></tr><tr><th>CLIP-ViT-B/32</th><td colspan=\"4\"></td></tr><tr><th>CLIP4Clip [29]</th><td>40.5</td><td>72.4</td><td>-</td><td>2.0</td></tr><tr><th>CenterCLIP [57]</th><td>43.9</td><td>74.6</td><td>85.8</td><td>2.0</td></tr><tr><th>CLIP2TV [10]</th><td>44.1</td><td>75.2</td><td>-</td><td>2.0</td></tr><tr><th>DRL [43]</th><td>44.2</td><td>74.5</td><td>86.1</td><td>2.0</td></tr><tr><th>CAMoE* [6]</th><td>51.0</td><td>77.7</td><td>-</td><td>-</td></tr><tr><th>Ours</th><td>51.1</td><td>78.4</td><td>88.3</td><td>1.0</td></tr><tr><th>Ours*</th><td>59.1</td><td>83.9</td><td>91.3</td><td>1.0</td></tr><tr><th>CLIP-ViT-B/16</th><td colspan=\"4\"></td></tr><tr><th>CenterCLIP [57]</th><td>46.2</td><td>77.0</td><td>87.6</td><td>2.0</td></tr><tr><th>DRL [43]</th><td>46.2</td><td>77.3</td><td>88.2</td><td>2.0</td></tr><tr><th>Ours</th><td>53.4</td><td>81.4</td><td>90.0</td><td>1.0</td></tr><tr><th>Ours*</th><td>61.4</td><td>85.7</td><td>92.6</td><td>1.0</td></tr></tbody></table>", "caption": "Table 7: Comparison of text-to-video retrieval in ActivityNet [19]. * denotes that the method uses post-processing operations DSL [6].", "list_citation_info": ["[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.", "[6] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290, 2021.", "[9] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214\u2013229, 2020.", "[10] Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610, 2021.", "[19] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706\u2013715, 2017.", "[34] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.", "[48] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5036\u20135045, 2022.", "[21] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 7331\u20137341, 2021.", "[57] Shuai Zhao, Linchao Zhu, Xiaohan Wang, and Yi Yang. Centerclip: Token clustering for efficient text-video retrieval. arXiv preprint arXiv:2205.00823, 2022.", "[40] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.", "[29] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[43] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xian-Sheng Hua. Disentangled representation learning for text-video retrieval. arXiv preprint arXiv:2203.07111, 2022."]}, {"table": "<table><tbody><tr><th>Method</th><td>R@1 \\uparrow</td><td>R@5 \\uparrow</td><td>R@10 \\uparrow</td><td>MdR \\downarrow</td></tr><tr><th>MMT [9]</th><td>12.9</td><td>29.9</td><td>40.1</td><td>19.3</td></tr><tr><th>Frozen [2]</th><td>15.0</td><td>30.8</td><td>40.3</td><td>20.0</td></tr><tr><th>HD-VILA [48]</th><td>17.4</td><td>34.1</td><td>44.1</td><td>15.0</td></tr><tr><th>BridgeFormer [11]</th><td>17.9</td><td>35.4</td><td>44.5</td><td>15.0</td></tr><tr><th>CLIP-ViT-B/32</th><td colspan=\"4\"></td></tr><tr><th>CLIP4Clip [29]</th><td>21.6</td><td>41.8</td><td>49.8</td><td>11.0</td></tr><tr><th>CenterCLIP [57]</th><td>21.7</td><td>39.8</td><td>49.8</td><td>11.0</td></tr><tr><th>XPool [12]</th><td>22.7</td><td>42.6</td><td>51.2</td><td>10.0</td></tr><tr><th>DRL [43]</th><td>24.9</td><td>45.7</td><td>55.3</td><td>7.0</td></tr><tr><th>CAMoE* [6]</th><td>25.9</td><td>46.1</td><td>53.7</td><td>-</td></tr><tr><th>Ours</th><td>25.6</td><td>45.3</td><td>54.4</td><td>8.0</td></tr><tr><th>Ours*</th><td>26.0</td><td>46.4</td><td>54.9</td><td>8.0</td></tr><tr><th>CLIP-ViT-B/16</th><td colspan=\"4\"></td></tr><tr><th>CLIP4Clip by [57]</th><td>24.1</td><td>45.0</td><td>55.1</td><td>8.0</td></tr><tr><th>CenterCLIP [57]</th><td>24.2</td><td>46.2</td><td>55.9</td><td>8.0</td></tr><tr><th>DRL [43]</th><td>26.5</td><td>47.6</td><td>56.8</td><td>7.0</td></tr><tr><th>Ours</th><td>29.4</td><td>50.6</td><td>59.0</td><td>5.0</td></tr><tr><th>Ours*</th><td>30.7</td><td>51.4</td><td>60.6</td><td>5.0</td></tr></tbody></table>", "caption": "Table 8: Comparison of text-to-video retrieval in LSMDC [37]. * denotes that the method uses post-processing operations DSL [6].", "list_citation_info": ["[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.", "[6] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290, 2021.", "[37] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, H. Larochelle, Aaron C. Courville, and Bernt Schiele. Movie description. IJCV, pages 94\u2013120, 2016.", "[9] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214\u2013229, 2020.", "[12] Satya Krishna Gorti, No\u00ebl Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu. X-pool: Cross-modal language-video attention for text-video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5006\u20135015, 2022.", "[48] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5036\u20135045, 2022.", "[11] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022.", "[57] Shuai Zhao, Linchao Zhu, Xiaohan Wang, and Yi Yang. Centerclip: Token clustering for efficient text-video retrieval. arXiv preprint arXiv:2205.00823, 2022.", "[29] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.", "[43] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xian-Sheng Hua. Disentangled representation learning for text-video retrieval. arXiv preprint arXiv:2203.07111, 2022."]}], "citation_info_to_title": {"[12] Satya Krishna Gorti, No\u00ebl Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu. X-pool: Cross-modal language-video attention for text-video retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5006\u20135015, 2022.": "X-pool: Cross-modal language-video attention for text-video retrieval", "[57] Shuai Zhao, Linchao Zhu, Xiaohan Wang, and Yi Yang. Centerclip: Token clustering for efficient text-video retrieval. arXiv preprint arXiv:2205.00823, 2022.": "Centerclip: Token clustering for efficient text-video retrieval", "[47] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, pages 5288\u20135296, 2016.": "Msr-vtt: A large video description dataset for bridging video and language", "[6] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen. Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss. arXiv preprint arXiv:2109.04290, 2021.": "Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss", "[52] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.": "Florence: A new foundation model for computer vision", "[46] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In EMNLP, pages 6787\u20136800, 2021.": "Videoclip: Contrastive pre-training for zero-shot video-text understanding", "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, pages 5803\u20135812, 2017.": "Localizing moments in video with natural language", "[37] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, H. Larochelle, Aaron C. Courville, and Bernt Schiele. Movie description. IJCV, pages 94\u2013120, 2016.": "Movie description", "[38] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "[7] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021.": "Clip2video: Mastering video-text retrieval via image clip", "[3] Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu, and Samuel Albanie. Cross modal retrieval with querybank normalisation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5194\u20135205, 2022.": "Cross modal retrieval with querybank normalisation", "[9] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In ECCV, pages 214\u2013229, 2020.": "Multi-modal transformer for video retrieval", "[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In ICCV, 2021.": "Frozen in time: A joint video and image encoder for end-to-end retrieval", "[48] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5036\u20135045, 2022.": "Advancing high-resolution video-language representation with large-scale video transcriptions", "[43] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xian-Sheng Hua. Disentangled representation learning for text-video retrieval. arXiv preprint arXiv:2203.07111, 2022.": "Disentangled representation learning for text-video retrieval", "[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.": "Microsoft COCO: Common Objects in Context", "[45] Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh, Christoph Feichtenhofer, Florian Metze, and Luke Zettlemoyer. Vlm: Task-agnostic video-language model pre-training for video understanding. arXiv preprint arXiv:2105.09996, 2021.": "Vlm: Task-agnostic video-language model pre-training for video understanding", "[19] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706\u2013715, 2017.": "Dense-captioning events in videos", "[40] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.": "All in one: Exploring unified video-language pre-training", "[29] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.": "Clip4clip: An empirical study of clip for end to end video clip retrieval", "[21] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, pages 7331\u20137341, 2021.": "Less is more: Clipbert for video-and-language learning via sparse sampling", "[10] Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610, 2021.": "Clip2tv: An empirical study on transformer-based methods for video-text retrieval", "[34] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.": "Support-set bottlenecks for video-text representation learning", "[11] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridging video-text retrieval with multiple choice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16167\u201316176, 2022.": "Bridging video-text retrieval with multiple choice questions"}, "source_title_to_arxiv_id": {"Cross modal retrieval with querybank normalisation": "2112.12777", "Frozen in time: A joint video and image encoder for end-to-end retrieval": "2104.00650", "Advancing high-resolution video-language representation with large-scale video transcriptions": "2111.10337", "Disentangled representation learning for text-video retrieval": "2203.07111", "Clip4clip: An empirical study of clip for end to end video clip retrieval": "2104.08860"}}