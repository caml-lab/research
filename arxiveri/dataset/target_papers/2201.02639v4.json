{"title": "MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound", "abstract": "As humans, we navigate a multimodal world, building a holistic understanding\nfrom all our senses. We introduce MERLOT Reserve, a model that represents\nvideos jointly over time -- through a new training objective that learns from\naudio, subtitles, and video frames. Given a video, we replace snippets of text\nand audio with a MASK token; the model learns by choosing the correct\nmasked-out snippet. Our objective learns faster than alternatives, and performs\nwell at scale: we pretrain on 20 million YouTube videos.\n  Empirical results show that MERLOT Reserve learns strong multimodal\nrepresentations. When finetuned, it sets state-of-the-art on Visual Commonsense\nReasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%,\nand 1.5% respectively. Ablations show that these tasks benefit from audio\npretraining -- even VCR, a QA task centered around images (without sound).\nMoreover, our objective enables out-of-the-box prediction, revealing strong\nmultimodal commonsense understanding. In a fully zero-shot setting, our model\nobtains competitive results on four video tasks, even outperforming supervised\napproaches on the recently proposed Situated Reasoning (STAR) benchmark.\n  We analyze why audio enables better vision-language representations,\nsuggesting significant opportunities for future research. We conclude by\ndiscussing ethical and societal implications of multimodal pretraining.", "authors": ["Rowan Zellers", "Jiasen Lu", "Ximing Lu", "Youngjae Yu", "Yanpeng Zhao", "Mohammadreza Salehi", "Aditya Kusupati", "Jack Hessel", "Ali Farhadi", "Yejin Choi"], "published_date": "2022_01_07", "pdf_url": "http://arxiv.org/pdf/2201.02639v4", "list_table_and_caption": [{"table": "<table><tbody><tr><td></td><td><p>Configuration <br/>for one epoch of pretraining</p></td><td><p>VCR Q\\rightarrowA</p></td><td><p>val (%)</p></td></tr><tr><td rowspan=\"3\">V+T</td><td><p>Mask LM [29, 106, 128]</p></td><td><p>67.2</p></td><td></td></tr><tr><td><p>VirTex-style [27]</p></td><td><p>67.8</p></td><td></td></tr><tr><td><p><img/> Contrastive Span</p></td><td>69.7</td><td></td></tr><tr><td rowspan=\"4\">V+T+A</td><td><p><img/> Audio as target</p></td><td><p>70.4</p></td><td></td></tr><tr><td><p><img/> Audio as input and target</p></td><td>70.7</td><td></td></tr><tr><td><p>Audio as input and target, w/o strict localization</p></td><td><p>70.6</p></td><td></td></tr><tr><td><p><img/>Reserve-B</p></td><td>71.9</td><td></td></tr></tbody></table>", "caption": "Table 1: Ablation study of our contrastive span objective. It outperforms prior work in a Vision+Text setting, with a 1% boost when audio is added. Our full setup, adding written text, improves another 1%. <img/> denotes part of our full model. ", "list_citation_info": ["[27] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11162\u201311173, 2021.", "[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">Accuracy (%)</th></tr></thead><tbody><tr><th>Model</th><td>Voice</td><td>Image+Voice</td><td>Image</td></tr><tr><th><img/>Reserve-L</th><th>10.8</th><th>9.6</th><th>10.7</th></tr><tr><th>CLIP ViT-B/16 [92]</th><td></td><td></td><td>86.0</td></tr></tbody></table>", "caption": "Table 2: Zero-shot person (face/voice) recognition accuracy on VoxCeleb2 [87] and VGGFace2 [17], using different modalities. While <img/>Reserve can perform person recognition from several modalities, its performance is much lower than the recognition-optimized CLIP model in the image-to-name setting. We hypothesize that this is due to a similarity between this setting and CLIP\u2019s pretraining data \u2013 news articles often include celebrity images, paired with their names. ", "list_citation_info": ["[87] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017.", "[92] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.", "[17] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), pages 67\u201374. IEEE, 2018."]}, {"table": "<table><thead><tr><th></th><th colspan=\"3\">GFlops, from</th><th><p>VCR</p></th></tr><tr><th><p>Model</p></th><th>Image <br/>Encoder</th><th>Joint<br/>Encoder</th><th>Total</th><th>Q\\rightarrowAR <br/>Acc(%)</th></tr></thead><tbody><tr><td><p>UNITER-Base[21]</p></td><td>1766</td><td>28</td><td>1794</td><td><p>58.2</p></td></tr><tr><td><p>UNITER-Large[21]</p></td><td>1767</td><td>99</td><td>1867</td><td><p>62.8</p></td></tr><tr><td><p>MERLOT [128]</p></td><td>236</td><td>67</td><td>303</td><td><p>65.1</p></td></tr><tr><td><p><img/>Reserve-B</p></td><td>99</td><td>46</td><td>146</td><td><p>62.6</p></td></tr><tr><td><p><img/>Reserve-L</p></td><td>176</td><td>165</td><td>341</td><td>71.5</td></tr></tbody></table>", "caption": "Table 3: Efficiency metrics of our model versus others, measured in terms of (giga) floating point operations required to process a single image, question, and answer candidate on VCR. We compare with the overall VCR performance on the combined Q\\rightarrowAR metric. Our <img/>Reserve family of models are significantly more efficient than prior work, with <img/>Reserve-L being roughly on par with MERLOT [128] in terms of FLOPs, yet improving accuracy by over 6%.", "list_citation_info": ["[128] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. arXiv preprint arXiv:2106.02636, 2021.", "[21] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer, 2020."]}, {"table": "<table><tbody><tr><td></td><th></th><th><p>Base</p></th><th><p>Large</p></th></tr><tr><td rowspan=\"5\">VCR</td><td><p>Batch Size</p></td><td colspan=\"2\">32</td></tr><tr><td><p>Training Epochs</p></td><td colspan=\"2\">5</td></tr><tr><td><p>Image Size</p></td><td colspan=\"2\">288\\times512</td></tr><tr><td><p>Learning Rates Tried</p></td><td>1e-5, 2e-5, 3e-5</td><td>8e-6, 1e-5, 1.2e-5 </td></tr><tr><td><p>Learning Rate</p></td><td><p>2e-5</p></td><td><p>8e-6</p></td></tr><tr><td rowspan=\"5\">TVQA</td><td><p>Batch Size</p></td><td colspan=\"2\">32</td></tr><tr><td><p>Training Epochs</p></td><td colspan=\"2\">3</td></tr><tr><td><p>Image Size</p></td><td colspan=\"2\">288\\times512</td></tr><tr><td><p>Learning Rates Tried</p></td><td>5e-6, 1e-5</td><td>5e-6, 1e-5</td></tr><tr><td><p>Learning Rate</p></td><td colspan=\"2\">5e-6</td></tr><tr><td rowspan=\"5\">Kinetics-600</td><td><p>Batch Size</p></td><td colspan=\"2\">64</td></tr><tr><td><p>Training Epochs</p></td><td colspan=\"2\">15</td></tr><tr><td><p>Image Size</p></td><td colspan=\"2\">288\\times512</td></tr><tr><td><p>Learning Rate</p></td><td><p>1e-5</p></td><td><p>5e-6</p></td></tr><tr><td><p>Data Augmentation</p></td><td colspan=\"2\">From [2]</td></tr></tbody></table>", "caption": "Table 5: Hyperparameters for finetuning on downstream tasks. Note that for Kinetics-600, we tried to mimic VATT\u2019s setup [2], including adopting their training-epoch regime and their data augmentation strategies. Our data augmentation strategies were much simpler for VCR and TVQA (random cropping, and for VCR sometimes horizontally flipping the image); we suspect that our VCR/TVQA results could be made higher if data augmentation was further explored.", "list_citation_info": ["[2] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT: transformers for multimodal self-supervised learning from raw video, audio and text. arXiv preprint arXiv:2104.11178, 2021."]}, {"table": "<table><tbody><tr><th></th><th></th><td colspan=\"3\">Overall</td><td colspan=\"3\">Unseen Kitchen</td><td colspan=\"3\">Tail Classes</td></tr><tr><th></th><th>Model</th><td>Verb</td><td>Noun</td><td>Act</td><td>Verb</td><td>Noun</td><td>Act</td><td>Verb</td><td>Noun</td><td>Act</td></tr><tr><th rowspan=\"10\">Validation</th><th>RULSTM [38]</th><td>27.8</td><td>30.8</td><td>14.0</td><td>28.8</td><td>27.2</td><td>14.2</td><td>19.8</td><td>22.0</td><td>11.1</td></tr><tr><th>AVT+ (TSN) [46]</th><td>25.5</td><td>31.8</td><td>14.8</td><td>25.5</td><td>23.6</td><td>11.5</td><td>18.5</td><td>25.8</td><td>12.6</td></tr><tr><th>AVT+ [46]</th><td>28.2</td><td>32.0</td><td>15.9</td><td>19.5</td><td>23.9</td><td>11.9</td><td>21.1</td><td>25.8</td><td>14.1</td></tr><tr><th>Chance</th><td>6.4</td><td>2.0</td><td>0.2</td><td>14.4</td><td>2.9</td><td>0.5</td><td>1.6</td><td>0.2</td><td>0.1</td></tr><tr><th>CLIP (VIT-B/16) [92]</th><td>13.3</td><td>14.5</td><td>2.0</td><td>12.3</td><td>8.4</td><td>2.1</td><td>14.3</td><td>14.3</td><td>1.7</td></tr><tr><th>CLIP (RN50x16) [92]</th><td>16.5</td><td>12.8</td><td>2.2</td><td>13.4</td><td>7.0</td><td>1.2</td><td>17.1</td><td>12.6</td><td>2.5</td></tr><tr><th><img/>Reserve-B</th><td>17.9</td><td>15.6</td><td>2.7</td><td>11.0</td><td>15.7</td><td>4.4</td><td>18.0</td><td>12.7</td><td>2.0</td></tr><tr><th><img/>Reserve-L</th><td>15.6</td><td>19.3</td><td>4.5</td><td>14.1</td><td>18.4</td><td>3.4</td><td>14.7</td><td>18.5</td><td>4.4</td></tr><tr><th><img/>Reserve-B (+audio)</th><td>20.9</td><td>17.5</td><td>3.7</td><td>15.5</td><td>20.1</td><td>4.3</td><td>20.7</td><td>14.5</td><td>3.2</td></tr><tr><th><img/>Reserve-L (+audio)</th><td>23.2</td><td>23.7</td><td>4.8</td><td>20.3</td><td>21.0</td><td>5.9</td><td>22.7</td><td>21.6</td><td>4.0</td></tr><tr><th rowspan=\"3\">Test</th><th>RULSTM [38]</th><td>25.3</td><td>26.7</td><td>11.2</td><td>19.4</td><td>26.9</td><td>9.7</td><td>17.6</td><td>16.0</td><td>7.9</td></tr><tr><th>AVT+ [46]</th><td>25.6</td><td>28.8</td><td>12.6</td><td>20.9</td><td>22.3</td><td>8.8</td><td>19.0</td><td>22.0</td><td>10.1</td></tr><tr><th><img/>Reserve-L (+audio)</th><td>24.0</td><td>25.5</td><td>5.8</td><td>22.7</td><td>26.4</td><td>7.0</td><td>23.7</td><td>24.2</td><td>4.7</td></tr></tbody></table>", "caption": "Table 6: <img/>Reserve gets competitive results on EPIC Kitchen Action Anticipation challenge with zero-shot, over methods from prior work.  ", "list_citation_info": ["[38] Antonino Furnari and Giovanni Maria Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020.", "[46] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. arXiv preprint arXiv:2106.02036, 2021.", "[92] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021."]}, {"table": "<p>Accuracy (%)ModelPromptingESC50US8KVoxCeleb2AudioClip68.668.8<img/>Reserve-LText-only.41.660.210.8Image-only.42.854.313.3Image and text.52.262.309.6</p>", "caption": "Table 7: Zero-shot audio classification accuracies (%) on ESC50 [90], US8K [98], and VoxCeleb2 [87]. We compare our model with AudioClip [54], which was pretrained on supervised data from AudioSet [43]. Our <img/>Reserve performs well across the board, especially when given both the image and the text as a prompt \u2013 demonstrating its OCR capability.", "list_citation_info": ["[98] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A dataset and taxonomy for urban sound research. In Proceedings of the 22nd ACM international conference on Multimedia, pages 1041\u20131044, 2014.", "[43] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In Proc. IEEE ICASSP 2017, New Orleans, LA, 2017.", "[90] Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, pages 1015\u20131018. ACM Press.", "[87] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017.", "[54] Andrey Guzhov, Federico Raue, J\u00f6rn Hees, and Andreas Dengel. Audioclip: Extending clip to image, text and audio. arXiv preprint arXiv:2106.13043, 2021."]}], "citation_info_to_title": {"[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.": "Bert: Pre-training of deep bidirectional transformers for language understanding", "[27] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11162\u201311173, 2021.": "Virtex: Learning visual representations from textual annotations", "[21] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer, 2020.": "Uniter: Universal image-text representation learning", "[92] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.": "Learning transferable visual models from natural language supervision", "[38] Antonino Furnari and Giovanni Maria Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2020.": "Rolling-unrolling lstms for action anticipation from first-person video", "[90] Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, pages 1015\u20131018. ACM Press.": "ESC: Dataset for Environmental Sound Classification", "[54] Andrey Guzhov, Federico Raue, J\u00f6rn Hees, and Andreas Dengel. Audioclip: Extending clip to image, text and audio. arXiv preprint arXiv:2106.13043, 2021.": "Audioclip: Extending clip to image, text and audio", "[128] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. arXiv preprint arXiv:2106.02636, 2021.": "Merlot: Multimodal Neural Script Knowledge Models", "[2] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. VATT: transformers for multimodal self-supervised learning from raw video, audio and text. arXiv preprint arXiv:2104.11178, 2021.": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text", "[17] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), pages 67\u201374. IEEE, 2018.": "Vggface2: A dataset for recognising faces across pose and age", "[87] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017.": "Voxceleb: a large-scale speaker identification dataset", "[98] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. A dataset and taxonomy for urban sound research. In Proceedings of the 22nd ACM international conference on Multimedia, pages 1041\u20131044, 2014.": "A dataset and taxonomy for urban sound research", "[46] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. arXiv preprint arXiv:2106.02036, 2021.": "Anticipative Video Transformer", "[43] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In Proc. IEEE ICASSP 2017, New Orleans, LA, 2017.": "Audio set: An ontology and human-labeled dataset for audio events"}, "source_title_to_arxiv_id": {"Vggface2: A dataset for recognising faces across pose and age": "1710.08092"}}