{"title": "Semantically Grounded Visual Embeddings for Zero-Shot Learning", "abstract": "Zero-shot learning methods rely on fixed visual and semantic embeddings,\nextracted from independent vision and language models, both pre-trained for\nother large-scale tasks. This is a weakness of current zero-shot learning\nframeworks as such disjoint embeddings fail to adequately associate visual and\ntextual information to their shared semantic content. Therefore, we propose to\nlearn semantically grounded and enriched visual information by computing a\njoint image and text model with a two-stream network on a proxy task. To\nimprove this alignment between image and textual representations, provided by\nattributes, we leverage ancillary captions to provide grounded semantic\ninformation. Our method, dubbed joint embeddings for zero-shot learning is\nevaluated on several benchmark datasets, improving the performance of existing\nstate-of-the-art methods in both standard ($+1.6$\\% on aPY, $+2.6\\%$ on FLO)\nand generalized ($+2.1\\%$ on AWA$2$, $+2.2\\%$ on CUB) zero-shot recognition.", "authors": ["Shah Nawaz", "Jacopo Cavazza", "Alessio Del Bue"], "published_date": "2022_01_03", "pdf_url": "http://arxiv.org/pdf/2201.00577v2", "list_table_and_caption": [{"table": "<table><tbody><tr><th></th><th></th><th></th><td colspan=\"4\">AWA2 [28]</td><td colspan=\"4\">aPY [61]</td></tr><tr><th></th><th></th><th></th><td>T1</td><td>u</td><td>s</td><td>H</td><td>T1</td><td>u</td><td>s</td><td>H</td></tr><tr><th colspan=\"3\">Baseline \\colon CDL [25]</th><td>69.9</td><td>28.1</td><td>73.5</td><td>40.6</td><td>43.0</td><td>19.8</td><td>48.6</td><td>28.1</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>65.6</td><td>24.2</td><td>71.7</td><td>36.2</td><td>44.1</td><td>18.1</td><td>46.1</td><td>26.0</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>67.1</td><td>26.2</td><td>76.1</td><td>39.0</td><td>44.8</td><td>19.4</td><td>47.3</td><td>27.5</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>69.2</td><td>27.1</td><td>74.6</td><td>39.8</td><td>45.6</td><td>20.1</td><td>49.8</td><td>28.6</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">Flickr30K</th><th><p>d=2048</p></th><td>68.2</td><td>27.9</td><td>77.1</td><td>41.0</td><td>45.2</td><td>22.3</td><td>50.4</td><td>30.9</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>68.1</td><td>27.1</td><td>69.3</td><td>39.0</td><td>45.1</td><td>18.6</td><td>48.6</td><td>26.9</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>69.3</td><td>28.4</td><td>75.6</td><td>41.3</td><td>46.3</td><td>23.2</td><td>50.2</td><td>31.7</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>69.5</td><td>29.0</td><td>75.6</td><td>41.9</td><td>45.3</td><td>20.4</td><td>49.9</td><td>28.7</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">MSCOCO</th><th><p>d=2048</p></th><td>70.1</td><td>28.4</td><td>72.1</td><td>40.7</td><td>44.1</td><td>19.4</td><td>49.0</td><td>27.8</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>68.3</td><td>26.1</td><td>71.7</td><td>38.3</td><td>42.1</td><td>20.2</td><td>46.7</td><td>28.2</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>69.9</td><td>28.1</td><td>73.5</td><td>40.7</td><td>43.3</td><td>21.8</td><td>49.2</td><td>30.2</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>69.3</td><td>28.1</td><td>77.1</td><td>41.2</td><td>44.8</td><td>22.8</td><td>50.4</td><td>31.4</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">ConcCapt</th><th><p>d=2048</p></th><td>71.0</td><td>29.3</td><td>77.1</td><td>42.5</td><td>45.9</td><td>24.5</td><td>51.1</td><td>33.1</td></tr><tr><th colspan=\"3\">Baseline \\colon f-CLSWGAN [56]</th><td>68.2</td><td>57.9</td><td>61.4</td><td>59.6</td><td>40.5</td><td>25.8</td><td>59.5</td><td>36.0</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>67.0</td><td>55.4</td><td>58.2</td><td>56.8</td><td>38.6</td><td>28.1</td><td>18.0</td><td>22.0</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>65.6</td><td>59.0</td><td>62.1</td><td>60.5</td><td>39.3</td><td>24.5</td><td>50.6</td><td>33.0</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>68.0</td><td>55.2</td><td>65.8</td><td>60.1</td><td>38.7</td><td>27.9</td><td>44.9</td><td>34.4</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">Flickr30K</th><th><p>d=2048</p></th><td>65.0</td><td>55.8</td><td>67.6</td><td>61.1</td><td>37.6</td><td>23.4</td><td>63.2</td><td>34.1</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>72.7</td><td>61.5</td><td>62.3</td><td>61.9</td><td>39.3</td><td>31.4</td><td>27.2</td><td>29.2</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>70.7</td><td>63.4</td><td>64.0</td><td>63.8</td><td>41.7</td><td>26.5</td><td>48.5</td><td>34.3</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>70.4</td><td>62.8</td><td>63.8</td><td>63.3</td><td>38.0</td><td>25.7</td><td>49.9</td><td>34.0</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">MSCOCO</th><th><p>d=2048</p></th><td>70.3</td><td>60.8</td><td>69.1</td><td>64.7</td><td>38.4</td><td>28.2</td><td>50.6</td><td>36.2</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>68.5</td><td>59.0</td><td>65.0</td><td>61.9</td><td>39.0</td><td>24.4</td><td>29.2</td><td>26.6</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>70.0</td><td>62.1</td><td>65.4</td><td>63.7</td><td>38.3</td><td>26.4</td><td>41.4</td><td>32.2</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>68.4</td><td>60.0</td><td>65.8</td><td>62.6</td><td>40.1</td><td>26.4</td><td>57.3</td><td>36.2</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">ConcCapt</th><th><p>d=2048</p></th><td>67.0</td><td>61.2</td><td>61.5</td><td>61.3</td><td>41.0</td><td>27.4</td><td>67.3</td><td>39.0</td></tr><tr><th colspan=\"3\">Baseline \\colon tf-VAEGAN [35]</th><td>72.2</td><td>59.8</td><td>75.1</td><td>66.6</td><td>40.8</td><td>30.8</td><td>54.6</td><td>39.3</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>70.0</td><td>57.8</td><td>68.2</td><td>62.6</td><td>36.4</td><td>25.5</td><td>42.2</td><td>31.8</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>67.4</td><td>57.3</td><td>73.8</td><td>64.5</td><td>38.9</td><td>26.8</td><td>50.9</td><td>35.0</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>69.9</td><td>63.1</td><td>71.5</td><td>67.0</td><td>41.4</td><td>29.2</td><td>53.0</td><td>37.6</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">Flickr30K</th><th><p>d=2048</p></th><td>69.9</td><td>63.1</td><td>61.5</td><td>67.0</td><td>40.2</td><td>29.9</td><td>47.1</td><td>36.8</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>70.5</td><td>56.9</td><td>73.4</td><td>64.0</td><td>43.2</td><td>30.0</td><td>62.3</td><td>40.4</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>72.3</td><td>60.6</td><td>64.0</td><td>66.0</td><td>43.7</td><td>30.6</td><td>59.7</td><td>40.5</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>71.8</td><td>62.8</td><td>63.8</td><td>66.8</td><td>40.8</td><td>30.8</td><td>53.6</td><td>38.5</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">MSCOCO</th><th><p>d=2048</p></th><td>76.3</td><td>62.3</td><td>78.3</td><td>69.4</td><td>41.8</td><td>27.3</td><td>70.6</td><td>39.4</td></tr><tr><th></th><th></th><th><p>d=256</p></th><td>71.9</td><td>61.0</td><td>73.6</td><td>66.8</td><td>41.8</td><td>29.3</td><td>51.9</td><td>37.5</td></tr><tr><th></th><th></th><th><p>d=512</p></th><td>69.4</td><td>58.8</td><td>71.2</td><td>64.4</td><td>39.6</td><td>29.8</td><td>52.5</td><td>38.0</td></tr><tr><th></th><th></th><th><p>d=1024</p></th><td>68.1</td><td>59.0</td><td>71.8</td><td>64.8</td><td>40.3</td><td>31.9</td><td>54.7</td><td>40.3</td></tr><tr><th rowspan=\"-4\">JE-ZSL (ours)</th><th rowspan=\"-4\">ConcCapt</th><th><p>d=2048</p></th><td>70.6</td><td>63.0</td><td>75.6</td><td>68.7</td><td>42.1</td><td>32.1</td><td>49.5</td><td>39.0</td></tr></tbody></table>", "caption": "Table 1: Joint Embeddings for zero-shot learning (JE-ZSL) - ablation study on the effect of the dimensionality. We mark in blue and bold any improvement scored by joint embeddings over the respective backbone zero-shot learning method used as baseline.", "list_citation_info": ["[35] Sanath Narayan, Akshita Gupta, Fahad Shahbaz Khan, Cees GM Snoek, and Ling Shao. Latent embedding feedback and discriminative features for zero-shot classification. In The European Conference on Computer Vision (ECCV), 2020.", "[28] CH. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.", "[61] Y. Yu. aPascal-aYahoo Image Data Collection. Technical report, University of Illinois at Urbana-Champaign, 2009.", "[56] Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5542\u20135551, 2018.", "[25] Huajie Jiang, Ruiping Wang, Shiguang Shan, and Xilin Chen. Learning class prototypes via structure alignment for zero-shot recognition. In European conference on computer vision (ECCV), 2018."]}, {"table": "<table><tbody><tr><td></td><td></td><td colspan=\"4\">AWA2 [28]</td><td colspan=\"4\">CUB [51]</td><td colspan=\"4\">FLO [37]</td><td colspan=\"4\">aPY [61]</td></tr><tr><td></td><td></td><td>T1</td><td>u</td><td>s</td><td>H</td><td>T1</td><td>u</td><td>s</td><td>H</td><td>T1</td><td>u</td><td>s</td><td>H</td><td>T1</td><td>u</td><td>s</td><td>H</td></tr><tr><td rowspan=\"7\">Compatibility Func</td><td>LATEM [53]</td><td>55.1</td><td>7.3</td><td>71.7</td><td>13.3</td><td>49.6</td><td>15.2</td><td>57.3</td><td>24</td><td>40.4</td><td>6.6</td><td>47.6</td><td>11.5</td><td>36.8</td><td>5.7</td><td>65.6</td><td>10.4</td></tr><tr><td>SynC [8]</td><td>49.3</td><td>8.9</td><td>87.3</td><td>16.2</td><td>53.0</td><td>11.5</td><td>70.9</td><td>19.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>23.9</td><td>7.4</td><td>66.3</td><td>13.3</td></tr><tr><td>SJE [2]</td><td>65.6</td><td>11.3</td><td>74.6</td><td>19.6</td><td>53.9</td><td>23.5</td><td>59.2</td><td>33.6</td><td>53.4</td><td>13.9</td><td>47.6</td><td>21.5</td><td>31.7</td><td>1.3</td><td>71.4</td><td>2.6</td></tr><tr><td>DeViSE [16]</td><td>54.2</td><td>13.4</td><td>68.7</td><td>22.4</td><td>52.0</td><td>23.8</td><td>53.0</td><td>32.8</td><td>45.9</td><td>9.9</td><td>44.2</td><td>16.2</td><td>37.0</td><td>3.5</td><td>78.4</td><td>6.7</td></tr><tr><td>KerZSL [64]</td><td>70.5</td><td>18.9</td><td>82.7</td><td>30.8</td><td>51.7</td><td>21.6</td><td>52.8</td><td>30.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>45.3</td><td>10.5</td><td>76.2</td><td>18.5</td></tr><tr><td>CDL [25]</td><td>69.9</td><td>28.1</td><td>73.5</td><td>40.6</td><td>54.5</td><td>23.5</td><td>55.2</td><td>32.9</td><td>59.6</td><td>32.0</td><td>64.8</td><td>42.8</td><td>43.0</td><td>19.8</td><td>48.6</td><td>28.1</td></tr><tr><td>JE-ZSL (ours)</td><td>71.0</td><td>29.3</td><td>77.1</td><td>42.5</td><td>54.1</td><td>25.6</td><td>70.2</td><td>37.5</td><td>62.2</td><td>43.5</td><td>62.2</td><td>51.2</td><td>45.9</td><td>24.5</td><td>51.1</td><td>33.1</td></tr><tr><td rowspan=\"10\">Feature Generation</td><td>f-CLSWGAN [56]</td><td>68.2</td><td>57.9</td><td>61.4</td><td>59.6</td><td>57.3</td><td>43.7</td><td>57.7</td><td>49.7</td><td>67.2</td><td>59.0</td><td>73.8</td><td>65.6</td><td>40.5</td><td>25.8</td><td>59.5</td><td>36.0</td></tr><tr><td>Cycle-WGAN [14]</td><td>66.8</td><td>63.4</td><td>59.6</td><td>59.8</td><td>58.6</td><td>59.3</td><td>47.9</td><td>53.0</td><td>70.3</td><td>61.6</td><td>69.2</td><td>65.2</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>f-VAEGAN-D2 [58]</td><td>71.1</td><td>57.6</td><td>70.6</td><td>63.5</td><td>61.0</td><td>48.4</td><td>60.1</td><td>53.6</td><td>67.7</td><td>56.8</td><td>74.9</td><td>64.6</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>CADA-VAE [44]</td><td>68.2</td><td>55.8</td><td>75.0</td><td>63.9</td><td>57.3</td><td>51.6</td><td>53.5</td><td>52.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GDAN [23]</td><td>68.1</td><td>32.1</td><td>67.5</td><td>43.5</td><td>58.8</td><td>39.3</td><td>66.7</td><td>49.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>38.3</td><td>30.4</td><td>75.0</td><td>43.4</td></tr><tr><td>COSMO [5]</td><td>-</td><td>52.8</td><td>80.0</td><td>63.6</td><td>-</td><td>44.4</td><td>57.8</td><td>50.2</td><td>-</td><td>59.6</td><td>81.4</td><td>68.8</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PGN [62]</td><td>71.2</td><td>48.0</td><td>83.6</td><td>61.0</td><td>68.3</td><td>48.5</td><td>57.2</td><td>52.5</td><td>81.4</td><td>63.6</td><td>77.8</td><td>70.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>0-VAEGAN [20]</td><td>69.9</td><td>56.2</td><td>71.7</td><td>63.0</td><td>54.9</td><td>41.1</td><td>48.5</td><td>44.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>36.3</td><td>31.7</td><td>53.2</td><td>39.7</td></tr><tr><td>tf-VAEGAN [35]</td><td>72.2</td><td>59.8</td><td>75.1</td><td>66.6</td><td>64.9</td><td>52.8</td><td>64.7</td><td>58.1</td><td>70.8</td><td>62.5</td><td>84.1</td><td>71.7</td><td>40.8</td><td>30.8</td><td>54.6</td><td>39.3</td></tr><tr><td>JE-ZSL (ours)</td><td>70.6</td><td>63.0</td><td>75.6</td><td>68.7</td><td>70.2</td><td>60.0</td><td>60.7</td><td>60.3</td><td>75.6</td><td>67.6</td><td>84.7</td><td>75.1</td><td>42.1</td><td>32.1</td><td>49.5</td><td>39.0</td></tr></tbody></table>", "caption": "Table 2: Comparison of our proposed JE-ZSL against state-of-the-art methods based on either compatibility functions or feature generation. Improvements scored by JE-ZSL over each of this class of methods is highlighted in blue and bolded.", "list_citation_info": ["[8] Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei Sha. Synthesized classifiers for zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "[62] Yunlong Yu, Zhong Ji, Jungong Han, and Zhongfei Zhang. Episode-based prototype generating network for zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[64] Hongguang Zhang and Piotr Koniusz. Zero-shot kernel learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.", "[16] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc\u2019Aurelio Ranzato, and Tomas Mikolov. DeVISE: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems (NeurIPS), 2013.", "[44] Edgar Schonfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized zero- and few-shot learning via aligned variational autoencoders. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[23] He Huang, Changhu Wang, Philip S Yu, and Chang-Dong Wang. Generative dual adversarial network for generalized zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[28] CH. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.", "[53] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele. Latent embeddings for zero-shot classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "[58] Yongqin Xian, Saurabh Sharma, Bernt Schiele, and Zeynep Akata. F-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.", "[5] Yuval Atzmon and Gal Chechik. Adaptive confidence smoothing for generalized zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.", "[2] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for fine-grained image classification. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.", "[25] Huajie Jiang, Ruiping Wang, Shiguang Shan, and Xilin Chen. Learning class prototypes via structure alignment for zero-shot recognition. In European conference on computer vision (ECCV), 2018.", "[51] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.", "[37] M-E. Nilsback and A. Zisserman. A visual vocabulary for flower classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2006.", "[35] Sanath Narayan, Akshita Gupta, Fahad Shahbaz Khan, Cees GM Snoek, and Ling Shao. Latent embedding feedback and discriminative features for zero-shot classification. In The European Conference on Computer Vision (ECCV), 2020.", "[14] Rafael Felix, Vijay BG Kumar, Ian Reid, and Gustavo Carneiro. Multi-modal cycle-consistent generalized zero-shot learning. In The European Conference on Computer Vision (ECCV), 2018.", "[61] Y. Yu. aPascal-aYahoo Image Data Collection. Technical report, University of Illinois at Urbana-Champaign, 2009.", "[20] Zongyan Han, Zhenyong Fu, and Jian Yang. Learning the redundancy-free features for generalized zero-shot object recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.", "[56] Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5542\u20135551, 2018."]}], "citation_info_to_title": {"[58] Yongqin Xian, Saurabh Sharma, Bernt Schiele, and Zeynep Akata. F-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.": "F-VAEGAN-D2: A Feature Generating Framework for Any-Shot Learning", "[62] Yunlong Yu, Zhong Ji, Jungong Han, and Zhongfei Zhang. Episode-based prototype generating network for zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "Episode-based Prototype Generating Network for Zero-Shot Learning", "[51] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.": "Caltech-UCSD Birds 200", "[28] CH. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.": "Learning to detect unseen object classes by between-class attribute transfer", "[14] Rafael Felix, Vijay BG Kumar, Ian Reid, and Gustavo Carneiro. Multi-modal cycle-consistent generalized zero-shot learning. In The European Conference on Computer Vision (ECCV), 2018.": "Multi-modal cycle-consistent generalized zero-shot learning", "[53] Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele. Latent embeddings for zero-shot classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.": "Latent embeddings for zero-shot classification", "[20] Zongyan Han, Zhenyong Fu, and Jian Yang. Learning the redundancy-free features for generalized zero-shot object recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.": "Learning the redundancy-free features for generalized zero-shot object recognition", "[61] Y. Yu. aPascal-aYahoo Image Data Collection. Technical report, University of Illinois at Urbana-Champaign, 2009.": "aPascal-aYahoo Image Data Collection", "[56] Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5542\u20135551, 2018.": "Feature generating networks for zero-shot learning", "[5] Yuval Atzmon and Gal Chechik. Adaptive confidence smoothing for generalized zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Adaptive confidence smoothing for generalized zero-shot learning", "[25] Huajie Jiang, Ruiping Wang, Shiguang Shan, and Xilin Chen. Learning class prototypes via structure alignment for zero-shot recognition. In European conference on computer vision (ECCV), 2018.": "Learning Class Prototypes via Structure Alignment for Zero-Shot Recognition", "[23] He Huang, Changhu Wang, Philip S Yu, and Chang-Dong Wang. Generative dual adversarial network for generalized zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Generative Dual Adversarial Network for Generalized Zero-Shot Learning", "[8] Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei Sha. Synthesized classifiers for zero-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.": "Synthesized classifiers for zero-shot learning", "[2] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for fine-grained image classification. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.": "Evaluation of Output Embeddings for Fine-Grained Image Classification", "[35] Sanath Narayan, Akshita Gupta, Fahad Shahbaz Khan, Cees GM Snoek, and Ling Shao. Latent embedding feedback and discriminative features for zero-shot classification. In The European Conference on Computer Vision (ECCV), 2020.": "Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification", "[16] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc\u2019Aurelio Ranzato, and Tomas Mikolov. DeVISE: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems (NeurIPS), 2013.": "DeVISE: A deep visual-semantic embedding model", "[44] Edgar Schonfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized zero- and few-shot learning via aligned variational autoencoders. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.": "Generalized zero- and few-shot learning via aligned variational autoencoders", "[37] M-E. Nilsback and A. Zisserman. A visual vocabulary for flower classification. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2006.": "A visual vocabulary for flower classification", "[64] Hongguang Zhang and Piotr Koniusz. Zero-shot kernel learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.": "Zero-shot kernel learning"}, "source_title_to_arxiv_id": {"Generative Dual Adversarial Network for Generalized Zero-Shot Learning": "1811.04857"}}