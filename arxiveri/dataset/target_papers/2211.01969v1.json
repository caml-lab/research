{"title": "Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing", "abstract": "This paper presents a framework for jointly grounding objects that follow\ncertain semantic relationship constraints given in a scene graph. A typical\nnatural scene contains several objects, often exhibiting visual relationships\nof varied complexities between them. These inter-object relationships provide\nstrong contextual cues toward improving grounding performance compared to a\ntraditional object query-only-based localization task. A scene graph is an\nefficient and structured way to represent all the objects and their semantic\nrelationships in the image. In an attempt towards bridging these two modalities\nrepresenting scenes and utilizing contextual information for improving object\nlocalization, we rigorously study the problem of grounding scene graphs on\nnatural images. To this end, we propose a novel graph neural network-based\napproach referred to as Visio-Lingual Message PAssing Graph Neural Network\n(VL-MPAG Net). In VL-MPAG Net, we first construct a directed graph with object\nproposals as nodes and an edge between a pair of nodes representing a plausible\nrelation between them. Then a three-step inter-graph and intra-graph message\npassing is performed to learn the context-dependent representation of the\nproposals and query objects. These object representations are used to score the\nproposals to generate object localization. The proposed method significantly\noutperforms the baselines on four public datasets.", "authors": ["Aditay Tripathi", "Anand Mishra", "Anirban Chakraborty"], "published_date": "2022_11_03", "pdf_url": "http://arxiv.org/pdf/2211.01969v1", "list_table_and_caption": [{"table": "<table><thead><tr><th>Model</th><th colspan=\"2\">COCO-stuff</th><th colspan=\"2\">VG-FO</th><th colspan=\"2\">SG</th></tr><tr><th></th><th>R@1</th><th>R@5</th><th>R@1</th><th>R@5</th><th>R@1</th><th>R@5</th></tr></thead><tbody><tr><td>Edges removed</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>   node-only (Detection)</td><td>21.0</td><td>47.9</td><td>30.1</td><td>62.8</td><td>23.4</td><td>-</td></tr><tr><td>   node-only (Localization)</td><td>33.9</td><td>57.2</td><td>29.9</td><td>53.5</td><td>34.7</td><td>62.5</td></tr><tr><td>Flattened triplets</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>   MDETR [17]</td><td>30.1</td><td>47.9</td><td>25.4</td><td>44.8</td><td>15.9</td><td>29.9</td></tr><tr><td>Structured Graph Query</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>   CRF-Based{}^{*} [16]</td><td>-</td><td>-</td><td>-</td><td>-</td><td>23.9</td><td>-</td></tr><tr><td>   Ours (VL-MPAG Net)</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>    1-layer</td><td>35.5</td><td>57.9</td><td>32.7</td><td>61.6</td><td>35.9</td><td>64.2</td></tr><tr><td>    2-layers</td><td>36.3</td><td>58.4</td><td>36.0</td><td>63.3</td><td>36.9</td><td>65.6</td></tr></tbody></table>", "caption": "Table 2: Results for scene graph grounding task on COCO-stuff val and VG-FO for completely overlapping train-test categories setting. {}^{*}Due to the unavailability of the implementation of [16] at the time of submission of this paper, we only compare with reported results in their paper.", "list_citation_info": ["[16] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3668\u20133678, 2015.", "[17] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr - modulated detection for end-to-end multi-modal understanding. ArXiv, abs/2104.12763, 2021."]}, {"table": "<table><tbody><tr><th rowspan=\"2\">Model</th><td colspan=\"2\">Subject</td><td colspan=\"2\">Object</td></tr><tr><td>    R@1</td><td>    R@5</td><td>    R@1</td><td>    R@5</td></tr><tr><th>SSAS [19]</th><td>21.5</td><td>-</td><td>24.2</td><td>-</td></tr><tr><th>VRD-LP [25]</th><td>31.5</td><td>38.8</td><td>34.9</td><td>40.3</td></tr><tr><th>CPARR [12]</th><td>49.8</td><td>69.4</td><td>52.4</td><td>70.2</td></tr><tr><th>Ours (VL-MPAG Net)</th><td>51.6</td><td>79.3</td><td>51.7</td><td>76.1</td></tr></tbody></table>", "caption": "Table 4:  Comparison of VL-MPAG Net against the referring relations baselines for the scenario when graph contains only two nodes on VRD dataset.", "list_citation_info": ["[25] Cewu Lu, Ranjay Krishna, Michael S. Bernstein, and Li Fei-Fei. Visual relationship detection with language priors. In ECCV, 2016.", "[19] Ranjay Krishna, Ines Chami, Michael S. Bernstein, and Li Fei-Fei. Referring relationships. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6867\u20136876, 2018.", "[12] Chuanzi He, Haidong Zhu, Jiyang Gao, Kan Chen, and R. Nevatia. Cparr: Category-based proposal analysis for referring relationships. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 4074\u20134083, 2020."]}, {"table": "<table><thead><tr><th rowspan=\"2\">Model</th><th colspan=\"2\">      Seen Categories</th><th colspan=\"2\">      Unseen Categories</th></tr><tr><th>    R@1</th><th>    R@5</th><th>    R@1</th><th>    R@5</th></tr></thead><tbody><tr><th>Node only (Localization)</th><td>33.2</td><td>56.6</td><td>19.6</td><td>43.1</td></tr><tr><th>MDETR [17]</th><td>26.2</td><td>47.1</td><td>26.4</td><td>45.7</td></tr><tr><th>Ours(VL-MPAG Net)</th><td></td><td></td><td></td><td></td></tr><tr><th>     1-layer</th><td>38.0</td><td>64.9</td><td>27.5</td><td>54.5</td></tr><tr><th>     2-layers</th><td>39.9</td><td>66.9</td><td>29.0</td><td>53.6</td></tr></tbody></table>", "caption": "Table 5:  The proposed model outperforms the baselines for \u2018seen\u2019 and \u2018unseen\u2019 object categories on VG-PO dataset.", "list_citation_info": ["[17] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr - modulated detection for end-to-end multi-modal understanding. ArXiv, abs/2104.12763, 2021."]}], "citation_info_to_title": {"[19] Ranjay Krishna, Ines Chami, Michael S. Bernstein, and Li Fei-Fei. Referring relationships. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6867\u20136876, 2018.": "Referring relationships", "[25] Cewu Lu, Ranjay Krishna, Michael S. Bernstein, and Li Fei-Fei. Visual relationship detection with language priors. In ECCV, 2016.": "Visual Relationship Detection with Language Priors", "[12] Chuanzi He, Haidong Zhu, Jiyang Gao, Kan Chen, and R. Nevatia. Cparr: Category-based proposal analysis for referring relationships. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 4074\u20134083, 2020.": "Cparr: Category-based proposal analysis for referring relationships", "[16] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3668\u20133678, 2015.": "Image retrieval using scene graphs", "[17] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr - modulated detection for end-to-end multi-modal understanding. ArXiv, abs/2104.12763, 2021.": "Mdetr - Modulated Detection for End-to-End Multi-Modal Understanding"}, "source_title_to_arxiv_id": {"Mdetr - Modulated Detection for End-to-End Multi-Modal Understanding": "2104.12763"}}