{"title": "Self Pre-training with Masked Autoencoders for Medical Image Analysis", "abstract": "Masked Autoencoder (MAE) has recently been shown to be effective in\npre-training Vision Transformers (ViT) for natural image analysis. By\nperforming the pretext task of reconstructing the original image from only\npartial observations, the encoder, which is a ViT, is encouraged to aggregate\ncontextual information to infer content in masked image regions. We believe\nthat this context aggregation ability is also essential to the medical image\ndomain where each anatomical structure is functionally and mechanically\nconnected to other structures and regions. However, there is no ImageNet-scale\nmedical image dataset for pre-training. Thus, in this paper, we investigate a\nself pre-training paradigm with MAE for medical images, i.e., models are\npre-trained on the same target dataset. To validate the MAE self pre-training,\nwe consider three diverse medical image tasks including chest X-ray disease\nclassification, CT abdomen multi-organ segmentation and MRI brain tumor\nsegmentation. It turns out MAE self pre-training benefits all the tasks\nmarkedly. Specifically, the mAUC on lung disease classification is increased by\n9.4%. The average DSC on brain tumor segmentation is improved from 77.4% to\n78.9%. Most interestingly, on the small-scale multi-organ segmentation dataset\n(N=30), the average DSC improves from 78.8% to 83.5% and the HD95 is reduced by\n60%, indicating its effectiveness in limited data scenarios. The segmentation\nand classification results reveal the promising potential of MAE self\npre-training for medical image analysis.", "authors": ["Lei Zhou", "Huidong Liu", "Joseph Bae", "Junjun He", "Dimitris Samaras", "Prateek Prasanna"], "published_date": "2022_03_10", "pdf_url": "http://arxiv.org/pdf/2203.05573v1", "list_table_and_caption": [{"table": "<table><tr><td rowspan=\"2\">Architectures</td><td colspan=\"2\">Pre-training</td><td rowspan=\"2\">Epochs</td><td rowspan=\"2\">mAUC</td></tr><tr><td>Method</td><td>Dataset</td></tr><tr><td>CXR14-R50 [23]</td><td>supervised</td><td>ImageNet-1K</td><td>-</td><td>74.5%</td></tr><tr><td>ChestNet [22]</td><td>supervised</td><td>ImageNet-1K</td><td>-</td><td>78.1%</td></tr><tr><td>CheXNet [18][25]</td><td>supervised</td><td>ImageNet-1K</td><td>-</td><td>78.9%</td></tr><tr><td>ResNet18 [13]</td><td>MoCo [12]</td><td>Self</td><td>-</td><td>78.6%</td></tr><tr><td>ResNet50</td><td>MoCo-v2 [6]</td><td>Self</td><td>-</td><td>79.4%</td></tr><tr><td>Enc_{t} [25]</td><td>LSAE</td><td>self</td><td>-</td><td>79.0%</td></tr><tr><td>ViT-B/16</td><td>None</td><td>None</td><td>100</td><td>74.4%</td></tr><tr><td>ViT-B/16</td><td>None</td><td>None</td><td>400</td><td>74.9%</td></tr><tr><td>ViT-B/16</td><td>supervised</td><td>ImageNet-1K</td><td>100</td><td>80.7%</td></tr><tr><td>ViT-B/16</td><td>MAE</td><td>Self</td><td>100</td><td>81.5%</td></tr></table>", "caption": "Table 1: Lung Disease Classification on ChestX-ray14.", "list_citation_info": ["[25] Zhou, L., Bae, J., Liu, H., Singh, G., Green, J., Gupta, A., Samaras, D., Prasanna, P.: Lung swapping autoencoder: Learning a disentangled structure-texture representation of chest radiographs. arXiv preprint arXiv:2201.07344 (2022)", "[6] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)", "[12] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9729\u20139738 (2020)", "[22] Wang, H., Xia, Y.: Chestnet: A deep neural network for classification of thoracic diseases on chest radiography. arXiv preprint arXiv:1807.03058 (2018)", "[18] Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K., et al.: Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225 (2017)", "[23] Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2097\u20132106 (2017)", "[13] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778 (2016)"]}, {"table": "<table><tr><td>Framework</td><td>Avg DSC/HD95</td><td>Aorta</td><td>Gallbladder</td><td>Kidney(L)</td><td>Kidney(R)</td><td>Liver</td><td>Pancreas</td><td>Spleen</td><td>Stomach</td></tr><tr><td>V-Net [15]</td><td>68.81/-</td><td>75.34</td><td>51.87</td><td>77.10</td><td>80.75</td><td>87.84</td><td>40.05</td><td>80.56</td><td>56.98</td></tr><tr><td>DARR [9]</td><td>69.77/-</td><td>74.74</td><td>53.77</td><td>72.31</td><td>73.24</td><td>94.08</td><td>54.18</td><td>89.90</td><td>45.96</td></tr><tr><td>U-Net(R50) [19]</td><td>74.68/36.87</td><td>84.18</td><td>62.84</td><td>79.19</td><td>71.29</td><td>93.35</td><td>48.23</td><td>84.41</td><td>73.92</td></tr><tr><td>AttnUNet(R50) [20]</td><td>75.57/36.97</td><td>55.92</td><td>63.91</td><td>79.20</td><td>72.71</td><td>93.56</td><td>49.37</td><td>87.19</td><td>74.95</td></tr><tr><td>TransUNet [5]</td><td>77.48/31.69</td><td>87.23</td><td>63.13</td><td>81.87</td><td>77.02</td><td>94.08</td><td>55.86</td><td>85.08</td><td>75.62</td></tr><tr><td>UNETR</td><td>78.83/25.59</td><td>85.46</td><td>70.88</td><td>83.03</td><td>82.02</td><td>95.83</td><td>50.99</td><td>88.26</td><td>72.74</td></tr><tr><td>UNETR+ImageNet</td><td>79.67/24.28</td><td>86.07</td><td>74.29</td><td>82.44</td><td>81.65</td><td>95.84</td><td>58.08</td><td>87.74</td><td>69.98</td></tr><tr><td>UNETR+MAE</td><td>83.52/10.24</td><td>88.92</td><td>75.25</td><td>86.37</td><td>84.00</td><td>95.95</td><td>65.02</td><td>90.56</td><td>80.89</td></tr></table>", "caption": "Table 2: Abdomen Multi-organ Segmentation on BTCV", "list_citation_info": ["[20] Schlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glocker, B., Rueckert, D.: Attention gated networks: Learning to leverage salient regions in medical images. Medical image analysis 53, 197\u2013207 (2019)", "[15] Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 2016 fourth international conference on 3D vision (3DV). pp. 565\u2013571. IEEE (2016)", "[9] Fu, S., Lu, Y., Wang, Y., Zhou, Y., Shen, W., Fishman, E., Yuille, A.: Domain adaptive relational reasoning for 3d multi-organ segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 656\u2013666. Springer (2020)", "[19] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234\u2013241. Springer (2015)", "[5] Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021)"]}], "citation_info_to_title": {"[23] Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2097\u20132106 (2017)": "Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases", "[20] Schlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glocker, B., Rueckert, D.: Attention gated networks: Learning to leverage salient regions in medical images. Medical image analysis 53, 197\u2013207 (2019)": "Attention gated networks: Learning to leverage salient regions in medical images", "[6] Chen, X., Fan, H., Girshick, R., He, K.: Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297 (2020)": "Improved Baselines with Momentum Contrastive Learning", "[5] Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou, Y.: Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 (2021)": "Transunet: Transformers make strong encoders for medical image segmentation", "[25] Zhou, L., Bae, J., Liu, H., Singh, G., Green, J., Gupta, A., Samaras, D., Prasanna, P.: Lung swapping autoencoder: Learning a disentangled structure-texture representation of chest radiographs. arXiv preprint arXiv:2201.07344 (2022)": "Lung swapping autoencoder: Learning a disentangled structure-texture representation of chest radiographs", "[15] Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: 2016 fourth international conference on 3D vision (3DV). pp. 565\u2013571. IEEE (2016)": "V-net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation", "[19] Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234\u2013241. Springer (2015)": "U-net: Convolutional networks for biomedical image segmentation", "[22] Wang, H., Xia, Y.: Chestnet: A deep neural network for classification of thoracic diseases on chest radiography. arXiv preprint arXiv:1807.03058 (2018)": "Chestnet: A deep neural network for classification of thoracic diseases on chest radiography", "[18] Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K., et al.: Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225 (2017)": "Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning", "[9] Fu, S., Lu, Y., Wang, Y., Zhou, Y., Shen, W., Fishman, E., Yuille, A.: Domain adaptive relational reasoning for 3d multi-organ segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 656\u2013666. Springer (2020)": "Domain Adaptive Relational Reasoning for 3D Multi-Organ Segmentation", "[13] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778 (2016)": "Deep residual learning for image recognition", "[12] He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9729\u20139738 (2020)": "Momentum contrast for unsupervised visual representation learning"}, "source_title_to_arxiv_id": {"Transunet: Transformers make strong encoders for medical image segmentation": "2102.04306", "Lung swapping autoencoder: Learning a disentangled structure-texture representation of chest radiographs": "2201.07344"}}